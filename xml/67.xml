<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:36:29Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|66001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4344</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4344</id><created>2014-09-15</created><authors><author><keyname>Rorabaugh</keyname><forenames>Danny</forenames></author></authors><title>A bound on a convexity measure for point sets</title><categories>cs.CG math.CO</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A planar point set is in convex position precisely when it has a convex
polygonization, that is, a polygonization with maximum interior angle measure
at most \pi. We can thus talk about the convexity of a set of points in terms
of the minimum, taken over all polygonizations, of the maximum interior angle.
The main result presented here is a nontrivial combinatorial upper bound of
this min-max value in terms of the number of points in the set. Motivated by a
particular construction, we also pose a natural conjecture for the best upper
bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4349</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4349</id><created>2014-09-15</created><authors><author><keyname>Aflalo</keyname><forenames>Yonathan</forenames></author><author><keyname>Brezis</keyname><forenames>Haim</forenames></author><author><keyname>Kimmel</keyname><forenames>Ron</forenames></author></authors><title>On the optimality of shape and data representation in the spectral
  domain</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proof of the optimality of the eigenfunctions of the Laplace-Beltrami
operator (LBO) in representing smooth functions on surfaces is provided and
adapted to the field of applied shape and data analysis. It is based on the
Courant-Fischer min-max principle adapted to our case. % The theorem we present
supports the new trend in geometry processing of treating geometric structures
by using their projection onto the leading eigenfunctions of the decomposition
of the LBO. Utilisation of this result can be used for constructing numerically
efficient algorithms to process shapes in their spectrum. We review a couple of
applications as possible practical usage cases of the proposed optimality
criteria. % We refer to a scale invariant metric, which is also invariant to
bending of the manifold. This novel pseudo-metric allows constructing an LBO by
which a scale invariant eigenspace on the surface is defined. We demonstrate
the efficiency of an intermediate metric, defined as an interpolation between
the scale invariant and the regular one, in representing geometric structures
while capturing both coarse and fine details. Next, we review a numerical
acceleration technique for classical scaling, a member of a family of
flattening methods known as multidimensional scaling (MDS). There, the
optimality is exploited to efficiently approximate all geodesic distances
between pairs of points on a given surface, and thereby match and compare
between almost isometric surfaces. Finally, we revisit the classical principal
component analysis (PCA) definition by coupling its variational form with a
Dirichlet energy on the data manifold. By pairing the PCA with the LBO we can
handle cases that go beyond the scope defined by the observation set that is
handled by regular PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4354</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4354</id><created>2014-09-15</created><authors><author><keyname>Raja</keyname><forenames>S. V. Kasmir</forenames></author><author><keyname>Rajitha</keyname><forenames>V.</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Meenakshi</forenames></author></authors><title>A Binary Schema and Computational Algorithms to Process Vowel-based
  Euphonic Conjunctions for Word Searches</title><categories>cs.CL</categories><journal-ref>International Journal of Applied Engineering Research, ISSN
  0973-4562, Vol. 9, No. 20, 2014, pp 7127-7142</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comprehensively searching for words in Sanskrit E-text is a non-trivial
problem because words could change their forms in different contexts. One such
context is sandhi or euphonic conjunctions, which cause a word to change owing
to the presence of adjacent letters or words. The change wrought by these
possible conjunctions can be so significant in Sanskrit that a simple search
for the word in its given form alone can significantly reduce the success level
of the search. This work presents a representational schema that represents
letters in a binary format and reduces Paninian rules of euphonic conjunctions
to simple bit set-unset operations. The work presents an efficient algorithm to
process vowel-based sandhis using this schema. It further presents another
algorithm that uses the sandhi processor to generate the possible transformed
word forms of a given word to use in a comprehensive word search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4355</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4355</id><created>2014-09-15</created><updated>2015-03-06</updated><authors><author><keyname>Ross</keyname><forenames>Neil J.</forenames></author></authors><title>Optimal ancilla-free Clifford+V approximation of z-rotations</title><categories>quant-ph cs.ET</categories><comments>14 pages. Extends previous version from Pauli+V to Clifford+V. arXiv
  admin note: text overlap with arXiv:1403.2975</comments><journal-ref>Quantum Information and Computation, Vol. 15, No 11-12, pp.
  932-950 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new efficient algorithm to approximate z-rotations by
ancilla-free Clifford+V circuits, up to a given precision epsilon. Our
algorithm is optimal in the presence of an oracle for integer factoring: it
outputs the shortest Clifford+V circuit solving the given problem instance. In
the absence of such an oracle, our algorithm is still near-optimal, producing
circuits of V-count m + O(log(log(1/epsilon))), where m is the V-count of the
third-to-optimal solution. A restricted version of the algorithm approximates
z-rotations in the Pauli+V gate set. Our method is based on previous work by
the author and Selinger on the optimal ancilla-free approximation of
z-rotations using Clifford+T gates and on previous work by Bocharov, Gurevich,
and Svore on the asymptotically optimal ancilla-free approximation of
z-rotations using Clifford+V gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4360</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4360</id><created>2014-09-15</created><authors><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Smith</keyname><forenames>Andrew M.</forenames></author><author><keyname>Vorobeychik</keyname><forenames>Yevgeniy</forenames></author><author><keyname>Mayo</keyname><forenames>Jackson</forenames></author><author><keyname>Armstrong</keyname><forenames>Robert C.</forenames></author></authors><title>Characterizing short-term stability for Boolean networks over any
  distribution of transfer functions</title><categories>nlin.CD cs.DM nlin.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a characterization of short-term stability of random Boolean
networks under \emph{arbitrary} distributions of transfer functions. Given any
distribution of transfer functions for a random Boolean network, we present a
formula that decides whether short-term chaos (damage spreading) will happen.
We provide a formal proof for this formula, and empirically show that its
predictions are accurate. Previous work only works for special cases of
balanced families. It has been observed that these characterizations fail for
unbalanced families, yet such families are widespread in real biological
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4364</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4364</id><created>2014-09-15</created><authors><author><keyname>Raja</keyname><forenames>S. V. Kasmir</forenames></author><author><keyname>Rajitha</keyname><forenames>V.</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Meenakshi</forenames></author></authors><title>Computational Algorithms Based on the Paninian System to Process
  Euphonic Conjunctions for Word Searches</title><categories>cs.CL</categories><journal-ref>International Journal of Computer Science and Information
  Security, Vol. 12, No. 8, 2014, pp. 64-76</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching for words in Sanskrit E-text is a problem that is accompanied by
complexities introduced by features of Sanskrit such as euphonic conjunctions
or sandhis. A word could occur in an E-text in a transformed form owing to the
operation of rules of sandhi. Simple word search would not yield these
transformed forms of the word. Further, there is no search engine in the
literature that can comprehensively search for words in Sanskrit E-texts taking
euphonic conjunctions into account. This work presents an optimal binary
representational schema for letters of the Sanskrit alphabet along with
algorithms to efficiently process the sandhi rules of Sanskrit grammar. The
work further presents an algorithm that uses the sandhi processing algorithm to
perform a comprehensive word search on E-text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4377</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4377</id><created>2014-09-15</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>A quantum mechanical version of Price's theorem for Gaussian states</title><categories>quant-ph cs.SY math-ph math.MP math.PR</categories><comments>11 pages, to appear in the Proceedings of the Australian Control
  Conference, 17-18 November 2014, Canberra, Australia</comments><msc-class>81S25, 81S30, 81P16, 81S05, 81Q15, 35Q40, 81Q93</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with integro-differential identities which are known
in statistical signal processing as Price's theorem for expectations of
nonlinear functions of jointly Gaussian random variables. We revisit these
relations for classical variables by using the Frechet differentiation with
respect to covariance matrices, and then show that Price's theorem carries over
to a quantum mechanical setting. The quantum counterpart of the theorem is
established for Gaussian quantum states in the framework of the Weyl functional
calculus for quantum variables satisfying the Heisenberg canonical commutation
relations. The quantum mechanical version of Price's theorem relates the
Frechet derivative of the generalized moment of such variables with respect to
the real part of their quantum covariance matrix with other moments. As an
illustrative example, we consider these relations for quadratic-exponential
moments which are relevant to risk-sensitive quantum control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4379</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4379</id><created>2014-09-15</created><authors><author><keyname>Fawzi</keyname><forenames>Hamza</forenames></author><author><keyname>Saunderson</keyname><forenames>James</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author></authors><title>Equivariant semidefinite lifts of regular polygons</title><categories>math.OC cs.CC cs.CG math.CO</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a polytope P in $\mathbb{R}^n$, we say that P has a positive
semidefinite lift (psd lift) of size d if one can express P as the linear
projection of an affine slice of the positive semidefinite cone
$\mathbf{S}^d_+$. If a polytope P has symmetry, we can consider equivariant psd
lifts, i.e. those psd lifts that respect the symmetry of P. One of the simplest
families of polytopes with interesting symmetries are regular polygons in the
plane, which have played an important role in the study of linear programming
lifts (or extended formulations). In this paper we study equivariant psd lifts
of regular polygons. We first show that the standard Lasserre/sum-of-squares
hierarchy for the regular N-gon requires exactly ceil(N/4) iterations and thus
yields an equivariant psd lift of size linear in N. In contrast we show that
one can construct an equivariant psd lift of the regular 2^n-gon of size 2n-1,
which is exponentially smaller than the psd lift of the sum-of-squares
hierarchy. Our construction relies on finding a sparse sum-of-squares
certificate for the facet-defining inequalities of the regular 2^n-gon, i.e.,
one that only uses a small (logarithmic) number of monomials. Since any
equivariant LP lift of the regular 2^n-gon must have size 2^n, this gives the
first example of a polytope with an exponential gap between sizes of
equivariant LP lifts and equivariant psd lifts. Finally we prove that our
construction is essentially optimal by showing that any equivariant psd lift of
the regular N-gon must have size at least logarithmic in N.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4388</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4388</id><created>2014-09-15</created><authors><author><keyname>Kamal</keyname><forenames>Hany</forenames></author><author><keyname>Picone</keyname><forenames>Marco</forenames></author><author><keyname>Amoretti</keyname><forenames>Michele</forenames></author></authors><title>A Survey and Taxonomy of Urban Traffic Management: Towards Vehicular
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Urban Traffic Management (UTM) topics have been tackled since long time,
mainly by civil engineers and by city planners. The introduction of new
communication technologies - such as cellular systems, satellite positioning
systems and inter-vehicle communications - has significantly changed the way
researchers deal with UTM issues. In this survey, we provide a review and a
classification of how UTM has been addressed in the literature. We start from
the recent achievements of &quot;classical&quot; approaches to urban traffic estimation
and optimization, including methods based on the analysis of data collected by
fixed sensors (e.g., cameras and radars), as well as methods based on
information provided by mobile phones, such as Floating Car Data (FCD).
Afterwards, we discuss urban traffic optimization, presenting the most recent
works on traffic signal control and vehicle routing control. Then, after
recalling the main concepts of Vehicular Ad-Hoc Networks (VANETs), we classify
the different VANET-based approaches to UTM, according to three categories
(&quot;pure&quot; VANETs, hybrid vehicular-sensor networks and hybrid vehicular-cellular
networks), while illustrating the major research issues for each of them. The
main objective of this survey is to provide a comprehensive view on UTM to
researchers with focus on VANETs, in order to pave the way for the design and
development of novel techniques for mitigating urban traffic problems, based on
inter-vehicle communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4391</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4391</id><created>2014-09-15</created><authors><author><keyname>Touchette</keyname><forenames>Dave</forenames></author></authors><title>Direct Sum Theorem for Bounded Round Quantum Communication Complexity</title><categories>quant-ph cs.CC cs.IT math.IT</categories><comments>25 pages, no figure, part of prelims taken from arXiv:1404.3733</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a direct sum theorem for bounded round entanglement-assisted quantum
communication complexity. To do so, we use the fully quantum definition for
information cost and complexity that we recently introduced, and use both the
fact that information is a lower bound on the communication, and the fact that
a direct sum property holds for quantum information complexity. We then give a
protocol for compressing a single copy of a protocol down to its quantum
information cost, up to terms depending on the number of rounds and the allowed
increase in error. Two important tools to derive this protocol are a smooth
conditional min-entropy bound for a one-shot quantum state redistribution
protocol, and the quantum substate theorem of Jain, Radhakrishnan and Sen
(FOCS'02) to transform this bound into a von Neumann conditional entropy bound.
This result further establishes the newly introduced notions of quantum
information cost and complexity as the correct quantum generalisations of the
classical ones in the standard communication complexity setting. Finding such a
quantum generalization of information complexity was one of the open problem
recently raised by Braverman (STOC'12).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4393</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4393</id><created>2014-09-15</created><authors><author><keyname>Egan</keyname><forenames>Malcolm</forenames></author></authors><title>Low-High SNR Transition in Multiuser MIMO</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiuser MIMO (MU-MIMO) plays a key role in the widely adopted 3GPP LTE
standard for wireless cellular networks. While exact and asymptotic sum-rate
results are well known, the problem of obtaining intuitive analytical results
for medium signal-to-noise ratios (SNRs) is still not solved. In this paper, we
propose the bend point, which quantifies the transition between low and high
SNR; i.e., the beginning of the high SNR region. We derive the bend point for
MU-MIMO with zero-forcing precoding and show that it is intimately related to
the intercept of the high SNR asymptote with the zero sum-rate line. Using this
result, we obtain a new approximation of the sum-rate at the bend
point---providing a useful rule of thumb for the effect of increasing the
number of antennas at medium SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4398</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4398</id><created>2014-09-15</created><updated>2015-01-14</updated><authors><author><keyname>Choi</keyname><forenames>Jaehyung</forenames></author><author><keyname>Mullhaupt</keyname><forenames>Andrew P.</forenames></author></authors><title>Application of K\&quot;ahler manifold to signal processing and Bayesian
  inference</title><categories>math.DG cs.IT math.IT math.ST stat.TH</categories><comments>8 pages, submitted to the Proceedings of MaxEnt 14</comments><journal-ref>AIP Conf. Proc. 1641, 113 (2015)</journal-ref><doi>10.1063/1.4905970</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the information geometry of linear systems and its application to
Bayesian inference, and the simplification available in the K\&quot;ahler manifold
case. We find conditions for the information geometry of linear systems to be
K\&quot;ahler, and the relation of the K\&quot;ahler potential to information geometric
quantities such as $\alpha $-divergence, information distance and the dual
$\alpha $-connection structure. The K\&quot;ahler structure simplifies the
calculation of the metric tensor, connection, Ricci tensor and scalar
curvature, and the $\alpha $-generalization of the geometric objects. The
Laplace--Beltrami operator is also simplified in the K\&quot;ahler geometry. One of
the goals in information geometry is the construction of Bayesian priors
outperforming the Jeffreys prior, which we use to demonstrate the utility of
the K\&quot;ahler structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4403</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4403</id><created>2014-09-13</created><authors><author><keyname>Hou</keyname><forenames>Lei</forenames></author><author><keyname>Pan</keyname><forenames>Xue</forenames></author><author><keyname>Guo</keyname><forenames>Qiang</forenames></author><author><keyname>Liu</keyname><forenames>Jian-Guo</forenames></author></authors><title>Memory effect of the online user preference</title><categories>physics.soc-ph cs.SI</categories><comments>22 pages, 5 figures, Scientific Reports 2014 Accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mechanism of the online user preference evolution is of great
significance for understanding the online user behaviors and improving the
quality of online services. Since users are allowed to rate on objects in many
online systems, ratings can well reflect the users' preference. With two
benchmark datasets from online systems, we uncover the memory effect in users'
selecting behavior which is the sequence of qualities of selected objects and
the rating behavior which is the sequence of ratings delivered by each user.
Furthermore, the memory duration is presented to describe the length of a
memory, which exhibits the power-law distribution, i.e., the probability of the
occurring of long-duration memory is much higher than that of the random case
which follows the exponential distribution. We present a preference model in
which a Markovian process is utilized to describe the users' selecting
behavior, and the rating behavior depends on the selecting behavior. With only
one parameter for each of the user's selecting and rating behavior, the
preference model could regenerate any duration distribution ranging from the
power-law form (strong memory) to the exponential form (weak memory).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4409</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4409</id><created>2014-09-15</created><authors><author><keyname>Nyagudi</keyname><forenames>Nyagudi Musandu</forenames></author></authors><title>Tackling the &quot;Gremlins Paradox&quot; : Autonomous Hot Swap Healing Protocol
  for Chronic Performance Problem Aversion in On-board Mission Critical Systems
  for Lethal Autonomous Weapons</title><categories>cs.CR</categories><comments>14 pages, 2 figures</comments><msc-class>93C85</msc-class><acm-class>C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives insights into the issue of Chronic Performance Problem
Aversion in On-board Mission Critical Systems for Lethal Autonomous Weapons
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4433</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4433</id><created>2014-09-15</created><updated>2015-12-10</updated><authors><author><keyname>Gajarsk&#xfd;</keyname><forenames>Jakub</forenames><affiliation>Masaryk University, Brno</affiliation></author><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames><affiliation>Masaryk University, Brno</affiliation></author><author><keyname>Obdr&#x17e;&#xe1;lek</keyname><forenames>Jan</forenames><affiliation>Masaryk University, Brno</affiliation></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames><affiliation>Masaryk University, Brno</affiliation></author></authors><title>Faster Existential FO Model Checking on Posets</title><categories>cs.LO cs.DM</categories><comments>Paper as accepted to the LMCS journal. An extended abstract of an
  earlier version of this paper has appeared at ISAAC'14. Main changes to the
  previous version are improvements in the Multicoloured Clique part (Section
  4)</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:8) 2015</journal-ref><doi>10.2168/LMCS-11(4:8)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the model checking problem for the existential fragment of
first-order (FO) logic on partially ordered sets is fixed-parameter tractable
(FPT) with respect to the formula and the width of a poset (the maximum size of
an antichain). While there is a long line of research into FO model checking on
graphs, the study of this problem on posets has been initiated just recently by
Bova, Ganian and Szeider (CSL-LICS 2014), who proved that the existential
fragment of FO has an FPT algorithm for a poset of fixed width. We improve upon
their result in two ways: (1) the runtime of our algorithm is
O(f(|{\phi}|,w).n^2) on n-element posets of width w, compared to O(g(|{\phi}|).
n^{h(w)}) of Bova et al., and (2) our proofs are simpler and easier to follow.
We complement this result by showing that, under a certain
complexity-theoretical assumption, the existential FO model checking problem
does not have a polynomial kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4438</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4438</id><created>2014-09-15</created><authors><author><keyname>Gulati</keyname><forenames>Manoj</forenames></author><author><keyname>Ram</keyname><forenames>Shobha Sundar</forenames></author><author><keyname>Singh</keyname><forenames>Amarjeet</forenames></author></authors><title>An In Depth Study into Using EMI Signatures for Appliance Identification</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy conservation is a key factor towards long term energy sustainability.
Real-time end user energy feedback, using disaggregated electric load
composition, can play a pivotal role in motivating consumers towards energy
conservation. Recent works have explored using high frequency conducted
electromagnetic interference (EMI) on power lines as a single point sensing
parameter for monitoring common home appliances. However, key questions
regarding the reliability and feasibility of using EMI signatures for
non-intrusive load monitoring over multiple appliances across different sensing
paradigms remain unanswered. This work presents some of the key challenges
towards using EMI as a unique and time invariant feature for load
disaggregation. In-depth empirical evaluations of a large number of appliances
in different sensing configurations are carried out, in both laboratory and
real world settings. Insights into the effects of external parameters such as
line impedance, background noise and appliance coupling on the EMI behavior of
an appliance are realized through simulations and measurements. A generic
approach for simulating the EMI behavior of an appliance that can then be used
to do a detailed analysis of real world phenomenology is presented. The
simulation approach is validated with EMI data from a router. Our EMI dataset -
High Frequency EMI Dataset (HFED) is also released.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4447</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4447</id><created>2014-09-15</created><authors><author><keyname>Zhao</keyname><forenames>Changhong</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author></authors><title>Optimal Sizing of Voltage Control Devices for Distribution Circuit with
  Intermittent Load</title><categories>cs.SY</categories><comments>10 pages, 7 figures, submitted to HICSS'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider joint control of a switchable capacitor and a D-STATCOM for
voltage regulation in a distribution circuit with intermittent load. The
control problem is formulated as a two-timescale optimal power flow problem
with chance constraints, which minimizes power loss while limiting the
probability of voltage violations due to fast changes in load. The control
problem forms the basis of an optimization problem which determines the sizes
of the control devices by minimizing sum of the expected power loss cost and
the capital cost. We develop computationally efficient heuristics to solve the
optimal sizing problem and implement real-time control. Numerical experiments
on a circuit with high-performance computing (HPC) load show that the proposed
sizing and control schemes significantly improve the reliability of voltage
regulation on the expense of only a moderate increase in cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4450</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4450</id><created>2014-09-15</created><authors><author><keyname>Ciampaglia</keyname><forenames>Giovanni Luca</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>The production of information in the attention economy</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 3 figures, 1 table</comments><report-no>Giovanni Luca Ciampaglia, Alessandro Flammini &amp; Filippo Menczer
  Scientific Reports 5, Article number: 9452 (2015)</report-no><doi>10.1038/srep09452</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online traces of human activity offer novel opportunities to study the
dynamics of complex knowledge exchange networks, and in particular how the
relationship between demand and supply of information is mediated by
competition for our limited individual attention. The emergent patterns of
collective attention determine what new information is generated and consumed.
Can we measure the relationship between demand and supply for new information
about a topic? Here we propose a normalization method to compare attention
bursts statistics across topics that have an heterogeneous distribution of
attention. Through analysis of a massive dataset on traffic to Wikipedia, we
find that the production of new knowledge is associated to significant shifts
of collective attention, which we take as a proxy for its demand. What we
observe is consistent with a scenario in which the allocation of attention
toward a topic stimulates the demand for information about it, and in turn the
supply of further novel information. Our attempt to quantify demand and supply
of information, and our finding about their temporal ordering, may lead to the
development of the fundamental laws of the attention economy, and a better
understanding of the social exchange of knowledge in online and offline
information networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4451</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4451</id><created>2014-09-15</created><authors><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Bent</keyname><forenames>Russell</forenames></author><author><keyname>Bienstock</keyname><forenames>Daniel</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Dvijotham</forenames></author></authors><title>Efficient Synchronization Stability Metrics for Fault Clearing</title><categories>cs.SY math.OC</categories><comments>7 pages, 4 figures</comments><report-no>LA-UR-14-20227</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct methods can provide rapid screening of the dynamical security of large
numbers fault and contingency scenarios by avoiding extensive time simulation.
We introduce a computationally-efficient direct method based on optimization
that leverages efficient cutting plane techniques. The method considers both
unstable equilibrium points and the effects of additional relay tripping on
dynamical security\cite{01SH}. Similar to other direct methods, our approach
yields conservative results for dynamical security, however, the optimization
formulation potentially lends itself to the inclusion of additional constraints
to reduce this conservatism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4469</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4469</id><created>2014-09-15</created><authors><author><keyname>Nikonov</keyname><forenames>Dmitri E.</forenames></author><author><keyname>Young</keyname><forenames>Ian A.</forenames></author><author><keyname>Bourianoff</keyname><forenames>George I.</forenames></author></authors><title>Convolutional Networks for Image Processing by Coupled Oscillator Arrays</title><categories>nlin.PS cond-mat.dis-nn cs.CV</categories><comments>23 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A coupled oscillator array is shown to approximate convolutions with Gabor
filters for image processing tasks. Pixelated image fragments and filter
functions are converted to voltages, differenced, and input into a
corresponding array of weakly coupled Voltage Controlled Oscillators (VCOs).
This is referred to as Frequency Shift Keying (FSK). Upon synchronization of
the array, the common node amplitude provides a metric for the degree of match
between the image fragment and the filter function. The optimal oscillator
parameters for synchronization are determined and favor a moderate value of the
Q-factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4470</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4470</id><created>2014-09-15</created><authors><author><keyname>Fanaei</keyname><forenames>Mohammad</forenames></author><author><keyname>Tahmasbi-Sarvestani</keyname><forenames>Amin</forenames></author><author><keyname>Fallah</keyname><forenames>Yaser P.</forenames></author><author><keyname>Bansal</keyname><forenames>Gaurav</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Kenney</keyname><forenames>John B.</forenames></author></authors><title>Adaptive Content Control for Communication amongst Cooperative Automated
  Vehicles</title><categories>cs.NI</categories><comments>7 Pages, 10 Figures, Sixth International Symposium on Wireless
  Vehicular Communications (WiVEC'2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative automated vehicles exchange information to assist each other in
creating a more precise and extended view of their surroundings, with the aim
of improving automated-driving decisions. This paper addresses the need for
scalable communication among these vehicles. To this end, a general
communication framework is proposed through which automated cars exchange
information derived from multi-resolution maps created using their local
sensing modalities. This method can extend the region visible to a car beyond
the area directly sensed by its own sensors. An adaptive, probabilistic,
distance-dependent strategy is proposed that controls the content of the
messages exchanged among vehicles based on performance measures associated with
the load on the communication channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4476</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4476</id><created>2014-09-15</created><updated>2014-09-17</updated><authors><author><keyname>Mota</keyname><forenames>Francisco</forenames></author></authors><title>Projective Root-Locus: An Extension of Root-Locus Plot to the Projective
  Plane</title><categories>cs.SY</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an extension of the classical Root-Locus (RL) method
where the points are calculated in the real projective plane instead of the
conventional affine real plane; we denominate this extension of the Root-Locus
as &quot;Projective Root-Locus (PjRL)&quot;. To plot the PjRL we use the concept of
&quot;Gnomonic Projection&quot; in order to have a representation of the projective real
plane as a simi-sphere of radius one in ${\mathbb R}^3$. We will see that the
PjRL reduces to the RL in the affine $XY$ plane, but also we can plot the RL
onto another affine component of the projective plane, like $ZY$ affine plane
for instance, to obtain what we denominate complementary plots of the
conventional RL. We also show that with the PjRL the points at infinity of the
RL can be computed as solutions of a set algebraic equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4481</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4481</id><created>2014-09-15</created><authors><author><keyname>Bera</keyname><forenames>Aniket</forenames></author><author><keyname>Wolinski</keyname><forenames>David</forenames></author><author><keyname>Pettr&#xe9;</keyname><forenames>Julien</forenames></author><author><keyname>Manocha</keyname><forenames>Dinesh</forenames></author></authors><title>Real-time Crowd Tracking using Parameter Optimized Mixture of Motion
  Models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel, real-time algorithm to track the trajectory of each
pedestrian in moderately dense crowded scenes. Our formulation is based on an
adaptive particle-filtering scheme that uses a combination of various
multi-agent heterogeneous pedestrian simulation models. We automatically
compute the optimal parameters for each of these different models based on
prior tracked data and use the best model as motion prior for our
particle-filter based tracking algorithm. We also use our &quot;mixture of motion
models&quot; for adaptive particle selection and accelerate the performance of the
online tracking algorithm. The motion model parameter estimation is formulated
as an optimization problem, and we use an approach that solves this
combinatorial optimization problem in a model independent manner and hence
scalable to any multi-agent pedestrian motion model. We evaluate the
performance of our approach on different crowd video datasets and highlight the
improvement in accuracy over homogeneous motion models and a baseline
mean-shift based tracker. In practice, our formulation can compute trajectories
of tens of pedestrians on a multi-core desktop CPU in in real time and offer
higher accuracy as compared to prior real time pedestrian tracking algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4484</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4484</id><created>2014-09-15</created><authors><author><keyname>Collevecchio</keyname><forenames>Andrea</forenames></author><author><keyname>Garoni</keyname><forenames>Timothy M.</forenames></author><author><keyname>Hyndman</keyname><forenames>Timothy</forenames></author><author><keyname>Tokarev</keyname><forenames>Daniel</forenames></author></authors><title>The worm algorithm for the Ising model is rapidly mixing</title><categories>math-ph cond-mat.stat-mech cs.DM math.CO math.MP math.PR</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove rapid mixing of the Prokofiev-Svistunov (or worm) algorithm for the
zero-field ferromagnetic Ising model, on all finite graphs and at all
temperatures. As a corollary, we show how to rigorously construct simple and
efficient approximation schemes for the Ising susceptibility and two-point
correlation function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4489</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4489</id><created>2014-09-15</created><authors><author><keyname>Sreekumar</keyname><forenames>Sreejith</forenames></author><author><keyname>Dey</keyname><forenames>Bikash K.</forenames></author><author><keyname>Pillai</keyname><forenames>Sibi Raj B.</forenames></author></authors><title>Distributed Rate Adaptation and Power Control in Fading Multiple Access
  Channels</title><categories>cs.IT math.IT</categories><comments>26 pages, 11 pictures, presented in parts at ISIT2013 and ITW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, the capacity region of a coherent fading multiple access
channel (MAC) is analyzed in two popular contexts. In the first, a centralized
system with full channel state information at the transmitters (CSIT) is
assumed, and the communication parameters like transmit power and data-rate are
jointly chosen for every fading vector realization. On the other hand, in
fast-fading links with distributed CSIT, the lack of full CSI is compensated by
performing ergodic averaging over sufficiently many channel realizations.
Notice that the distributed CSI may necessitate decentralized power-control for
optimal data-transfer. Apart from these two models, the case of slow-fading
links and distributed CSIT, though relevant to many systems, has received much
less attention.
  In this paper, a block-fading AWGN MAC with full CSI at the receiver and
distributed CSI at the transmitters is considered. The links undergo
independent fading, but otherwise have arbitrary fading distributions. The
channel statistics and respective long-term average transmit powers are known
to all parties. We first consider the case where each encoder has knowledge
only of its own link quality, and not of others. For this model, we compute the
adaptive capacity region, i.e. the collection of average rate-tuples under
block-wise coding/decoding such that the rate-tuple for every fading
realization is inside the instantaneous MAC capacity region. The key step in
our solution is an optimal rate allocation function for any given set of
distributed power control laws at the transmitters. This also allows us to
characterize the optimal power control for a wide class of fading models.
Further extensions are also proposed to account for more general CSI
availability at the transmitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4499</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4499</id><created>2014-09-16</created><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author></authors><title>Toward Fully-Shared Access: Designing ISP Service Plans Leveraging
  Excess Bandwidth Allocation</title><categories>cs.NI</categories><comments>4 pages, 2 figures, ICTC 2014 (Oct. 2014, Busan, Korea)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shaping subscriber traffic based on token bucket filter (TBF) by Internet
service providers (ISPs) results in waste of network resources in shared access
when there are few active subscribers, because it cannot allocate excess
bandwidth in the long term. New traffic control schemes have been recently
proposed to allocate excess bandwidth among active subscribers proportional to
their token generation rates. In this paper we report the current status of our
research on designing flexible yet practical service plans exploiting excess
bandwidth allocation enabled by the new traffic control schemes in shared
access networks, which are attractive to both ISP and its subscribers in terms
of revenue and quality of service (QoS) and serve as a stepping stone to
fully-shared access in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4503</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4503</id><created>2014-09-16</created><updated>2015-03-01</updated><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Christin</keyname><forenames>Nicolas</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel</forenames></author><author><keyname>Sinha</keyname><forenames>Arunesh</forenames></author></authors><title>Audit Games with Multiple Defender Resources</title><categories>cs.GT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern organizations (e.g., hospitals, social networks, government agencies)
rely heavily on audit to detect and punish insiders who inappropriately access
and disclose confidential information. Recent work on audit games models the
strategic interaction between an auditor with a single audit resource and
auditees as a Stackelberg game, augmenting associated well-studied security
games with a configurable punishment parameter. We significantly generalize
this audit game model to account for multiple audit resources where each
resource is restricted to audit a subset of all potential violations, thus
enabling application to practical auditing scenarios. We provide an FPTAS that
computes an approximately optimal solution to the resulting non-convex
optimization problem. The main technical novelty is in the design and
correctness proof of an optimization transformation that enables the
construction of this FPTAS. In addition, we experimentally demonstrate that
this transformation significantly speeds up computation of solutions for a
class of audit games and security games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4504</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4504</id><created>2014-09-16</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Zhu</keyname><forenames>Hua</forenames></author></authors><title>Voting for Deceptive Opinion Spam Detection</title><categories>cs.CL cs.SI</categories><comments>arXiv admin note: text overlap with arXiv:1204.2804 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consumers' purchase decisions are increasingly influenced by user-generated
online reviews. Accordingly, there has been growing concern about the potential
for posting deceptive opinion spam fictitious reviews that have been
deliberately written to sound authentic, to deceive the readers. Existing
approaches mainly focus on developing automatic supervised learning based
methods to help users identify deceptive opinion spams.
  This work, we used the LSI and Sprinkled LSI technique to reduce the
dimension for deception detection. We make our contribution to demonstrate what
LSI is capturing in latent semantic space and reveal how deceptive opinions can
be recognized automatically from truthful opinions. Finally, we proposed a
voting scheme which integrates different approaches to further improve the
classification performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4507</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4507</id><created>2014-09-16</created><authors><author><keyname>Sayed</keyname><forenames>Awny</forenames></author><author><keyname>Almaqrashi</keyname><forenames>Amal</forenames></author></authors><title>Scalable and Efficient Self-Join Processing technique in RDF data</title><categories>cs.DB</categories><comments>8-pages, 5-figures, International Journal of Computer Science Issues
  (IJCSI), Volume 11, Issue 2. April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient management of RDF data plays an important role in successfully
understanding and fast querying data. Although the current approaches of
indexing in RDF Triples such as property tables and vertically partitioned
solved many issues; however, they still suffer from the performance in the
complex self-join queries and insert data in the same table. As an improvement
in this paper, we propose an alternative solution to facilitate flexibility and
efficiency in that queries and try to reach to the optimal solution to decrease
the self-joins as much as possible, this solution based on the idea of
&quot;Recursive Mapping of Twin Tables&quot;. Our main goal of Recursive Mapping of Twin
Tables (RMTT) approach is divided the main RDF Triple into two tables which
have the same structure of RDF Triple and insert the RDF data recursively. Our
experimental results compared the performance of join queries in vertically
partitioned approach and the RMTT approach using very large RDF data, like DBLP
and DBpedia datasets. Our experimental results with a number of complex
submitted queries shows that our approach is highly scalable compared with
RDF-3X approach and RMTT reduces the number of self-joins especially in complex
queries 3-4 times than RDF-3X approach
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4511</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4511</id><created>2014-09-16</created><authors><author><keyname>Segal-Halevi</keyname><forenames>Erel</forenames></author><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Aumann</keyname><forenames>Yonatan</forenames></author></authors><title>Fair and Square: Cake-cutting in Two Dimensions</title><categories>cs.GT cs.CG</categories><comments>Submitted to ITCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of fairly dividing a two dimensional heterogeneous
good among multiple players. Applications include division of land as well as
ad space in print and electronic media. Classical cake cutting protocols
primarily consider a one-dimensional resource, or allocate each player multiple
infinitesimally small &quot;pieces&quot;. In practice, however, the two dimensional
\emph{shape} of the allotted piece is of crucial importance in many
applications (e.g. squares or bounded aspect-ratio rectangles are most useful
for building houses, as well as advertisements). We thus introduce and study
the problem of fair two-dimensional division wherein the allotted plots must be
of some restricted two-dimensional geometric shape(s). Adding this geometric
constraint re-opens most questions and challenges related to cake-cutting.
Indeed, even the elementary \emph{proportionality} fairness criteria can no
longer be guaranteed in all cases. In this paper we thus examine the
\emph{level} of proportionality that \emph{can} be guaranteed, providing both
impossibility results (for proportionality that cannot be guaranteed), and
algorithmic constructions (for proportionality that can be guaranteed). We
focus primarily on the case when the cake is a rectilinear polygon and the
allotted plots must be squares or bounded aspect-ratio rectangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4526</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4526</id><created>2014-09-16</created><updated>2015-03-24</updated><authors><author><keyname>Smith</keyname><forenames>Benjamin</forenames><affiliation>INRIA Saclay - Ile de France, LIX</affiliation></author></authors><title>The Q-curve construction for endomorphism-accelerated elliptic curves</title><categories>cs.CR math.NT</categories><comments>To appear in the Journal of Cryptology. arXiv admin note: text
  overlap with arXiv:1305.5400</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a detailed account of the use of $\mathbb{Q}$-curve reductions to
construct elliptic curves over $\mathbb{F}\_{p^2}$ with efficiently computable
endomorphisms, which can be used to accelerate elliptic curve-based
cryptosystems in the same way as Gallant--Lambert--Vanstone (GLV) and
Galbraith--Lin--Scott (GLS) endomorphisms. Like GLS (which is a degenerate case
of our construction), we offer the advantage over GLV of selecting from a much
wider range of curves, and thus finding secure group orders when \(p\) is fixed
for efficient implementation. Unlike GLS, we also offer the possibility of
constructing twist-secure curves. We construct several one-parameter families
of elliptic curves over $\mathbb{F}\_{p^2}$ equipped with efficient
endomorphisms for every $p \textgreater{} 3$, and exhibit examples of
twist-secure curves over $\mathbb{F}\_{p^2}$ for the efficient Mersenne prime
$p = 2^{127}-1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4545</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4545</id><created>2014-09-16</created><authors><author><keyname>Tron</keyname><forenames>Emanuele</forenames></author></authors><title>A Note on Rectangle Covering with Congruent Disks</title><categories>cs.CG</categories><comments>7 pages, 4 figure</comments><msc-class>52C15, 05B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we prove that, if $S_n$ is the greatest area of a rectangle
which can be covered with $n$ unit disks, then $2\leq S_n/n&lt;3 \sqrt{3}/2$, and
these are the best constants; moreover, for $\Delta(n):=(3\sqrt{3}/2)n-S_n$, we
have $0.727384&lt;\liminf\Delta(n)/\sqrt{n}&lt;2.121321$ and
$0.727384&lt;\limsup\Delta(n)/\sqrt{n}&lt;4.165064$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4559</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4559</id><created>2014-09-16</created><authors><author><keyname>Korchiyne</keyname><forenames>Redouan</forenames></author><author><keyname>Farssi</keyname><forenames>Sidi Mohamed</forenames></author><author><keyname>Sbihi</keyname><forenames>Abderrahmane</forenames></author><author><keyname>Touahni</keyname><forenames>Rajaa</forenames></author><author><keyname>Alaoui</keyname><forenames>Mustapha Tahiri</forenames></author></authors><title>A Combined Method Of Fractal And GLCM Features For MRI And CT Scan
  Images Classification</title><categories>cs.CV</categories><comments>13 pages, 6 figures, Signal &amp; Image Processing : An International
  Journal(SIPIJ)Vol.5, No.4, August 2014</comments><doi>10.5121/sipij.2014.5409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractal analysis has been shown to be useful in image processing for
characterizing shape and gray-scale complexity. The fractal feature is a
compact descriptor used to give a numerical measure of the degree of
irregularity of the medical images. This descriptor property does not give
ownership of the local image structure. In this paper, we present a combination
of this parameter based on Box Counting with GLCM Features. This powerful
combination has proved good results especially in classification of medical
texture from MRI and CT Scan images of trabecular bone. This method has the
potential to improve clinical diagnostics tests for osteoporosis pathologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4560</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4560</id><created>2014-09-16</created><authors><author><keyname>Wang</keyname><forenames>Kehao</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Pricing Mobile Data Offloading: A Distributed Market Framework</title><categories>cs.NI cs.GT</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile data offloading is an emerging technology to avoid congestion in
cellular networks and improve the level of user satisfaction. In this paper, we
develop a distributed market framework to price the offloading service, and
conduct a detailed analysis of the incentives for offloading service providers
and conflicts arising from the interactions of different participators.
Specifically, we formulate a multi-leader multi-follower Stackelberg game
(MLMF-SG) to model the interactions between the offloading service providers
and the offloading service consumers in the considered market framework, and
investigate the cases where the offloading capacity of APs is unlimited and
limited, respectively. For the case without capacity limit, we decompose the
followers' game of the MLMF-SG (FG-MLMF-SG) into a number of simple follower
games (FGs), and prove the existence and uniqueness of the equilibrium of the
FGs from which the existence and uniqueness of the FG-MLMF-SG also follows. For
the leaders' game of the MLMF-SG, we also prove the existence and uniqueness of
the equilibrium. For the case with capacity limit, by considering a symmetric
strategy profile, we establish the existence and uniqueness of the equilibrium
of the corresponding MLMF-SG, and present a distributed algorithm that allows
the leaders to achieve the equilibrium. Finally, extensive numerical
experiments demonstrate that the Stackelberg equilibrium is very close to the
corresponding social optimum for both considered cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4561</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4561</id><created>2014-09-16</created><authors><author><keyname>Marinescu</keyname><forenames>Andrei</forenames></author><author><keyname>Dusparic</keyname><forenames>Ivana</forenames></author><author><keyname>Taylor</keyname><forenames>Adam</forenames></author><author><keyname>Cahill</keyname><forenames>Vinny</forenames></author><author><keyname>Clarke</keyname><forenames>Siobh&#xe1;n</forenames></author></authors><title>Decentralised Multi-Agent Reinforcement Learning for Dynamic and
  Uncertain Environments</title><categories>cs.MA</categories><comments>7 pages, 7 figures, 1 Table, 1 algorithm, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-Agent Reinforcement Learning (MARL) is a widely used technique for
optimization in decentralised control problems. However, most applications of
MARL are in static environments, and are not suitable when agent behaviour and
environment conditions are dynamic and uncertain. Addressing uncertainty in
such environments remains a challenging problem for MARL-based systems. The
dynamic nature of the environment causes previous knowledge of how agents
interact to become outdated. Advanced knowledge of potential changes through
prediction significantly supports agents converging to near-optimal control
solutions. In this paper we propose P-MARL, a decentralised MARL algorithm
enhanced by a prediction mechanism that provides accurate information regarding
up-coming changes in the environment. This prediction is achieved by employing
an Artificial Neural Network combined with a Self-Organising Map that detects
and matches changes in the environment. The proposed algorithm is validated in
a realistic smart-grid scenario, and provides a 92% Pareto efficient solution
to an electric vehicle charging problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4565</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4565</id><created>2014-09-16</created><authors><author><keyname>Napoli</keyname><forenames>Christian</forenames></author><author><keyname>Pappalardo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Tramontana</keyname><forenames>Emiliano</forenames></author></authors><title>Improving files availability for BitTorrent using a diffusion model</title><categories>cs.NI cs.NE</categories><msc-class>68M12, 68M14, 62M45, 68T05</msc-class><acm-class>C.1.4; C.2.1; C.2.4; C.2.4; F.1.1; I.2.6; H.3.5</acm-class><journal-ref>IEEE 23rd International WETICE Conference, pp. 191-196, 2014</journal-ref><doi>10.1109/WETICE.2014.65</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The BitTorrent mechanism effectively spreads file fragments by copying the
rarest fragments first. We propose to apply a mathematical model for the
diffusion of fragments on a P2P in order to take into account both the effects
of peer distances and the changing availability of peers while time goes on.
Moreover, we manage to provide a forecast on the availability of a torrent
thanks to a neural network that models the behaviour of peers on the P2P
system. The combination of the mathematical model and the neural network
provides a solution for choosing file fragments that need to be copied first,
in order to ensure their continuous availability, counteracting possible
disconnections by some peers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4566</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4566</id><created>2014-09-16</created><authors><author><keyname>Yildiz</keyname><forenames>Olcay Taner</forenames></author><author><keyname>Alpaydin</keyname><forenames>Ethem</forenames></author></authors><title>Multivariate Comparison of Classification Algorithms</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical tests that compare classification algorithms are univariate and
use a single performance measure, e.g., misclassification error, $F$ measure,
AUC, and so on. In multivariate tests, comparison is done using multiple
measures simultaneously. For example, error is the sum of false positives and
false negatives and a univariate test on error cannot make a distinction
between these two sources, but a 2-variate test can. Similarly, instead of
combining precision and recall in $F$ measure, we can have a 2-variate test on
(precision, recall). We use Hotelling's multivariate $T^2$ test for comparing
two algorithms, and when we have three or more algorithms we use the
multivariate analysis of variance (MANOVA) followed by pairwise post hoc tests.
In our experiments, we see that multivariate tests have higher power than
univariate tests, that is, they can detect differences that univariate tests
cannot. We also discuss how multivariate analysis allows us to automatically
extract performance measures that best distinguish the behavior of multiple
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4575</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4575</id><created>2014-09-16</created><updated>2016-01-07</updated><authors><author><keyname>Zhang</keyname><forenames>Shubao</forenames></author></authors><title>Stable Cosparse Recovery via \ell_q-analysis Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the $\ell_q$-analysis optimization $(0&lt;q\leq1)$
problem for cosparse signal recovery. Our results show that the nonconvex
$\ell_q$-analysis optimization with $q&lt;1$ has better properties in terms of
stability and robustness than the convex $\ell_1$-analysis optimization. In
addition, we develop an iteratively reweighted method to solve this problem
under the variational framework. Theoretical analysis demonstrates that our
method is capable of pursuing a local minima close to the global minima. The
empirical results show that the nonconvex approach performs better than its
convex counterpart. It is also illustrated that our method outperforms the
other state-of-the-art methods for cosparse signal recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4583</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4583</id><created>2014-09-16</created><updated>2015-05-08</updated><authors><author><keyname>Kasparian</keyname><forenames>Azniv</forenames></author><author><keyname>Velikova</keyname><forenames>Evgeniya</forenames></author></authors><title>Tangent Codes</title><categories>cs.IT math.AG math.IT</categories><msc-class>Primary: 94B27, 14G50, Secondary: 14G17, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present article studies the finite Zariski tangent spaces to an affine
variety X as linear codes, in order to characterize their typical or
exceptional properties by global geometric conditions on X. The discussion
concerns the generic minimum distance of a tangent code to X, its lower
semi-continuity under a deformation of X, as well as the existence of Zariski
tangent spaces to X with exceptional minimum distance. Tangent codes are shown
to admit simultaneous decoding. The duals of the tangent codes to X are
realized by gradients of polynomials from the ideal of X. We provide
constructions of affine varieties with near MDS, cyclic or Hamming tangent
codes. Puncturing, shortening and extending finite Zariski tangent spaces are
related to the corresponding operations on affine varieties. The (u|u+v)
construction of tangent codes is associated with a fibered product of
varieties. Explicit constructions realize linear Hamming isometries as
differentials of morphisms of affine varieties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4587</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4587</id><created>2014-09-16</created><authors><author><keyname>Abdmouleh</keyname><forenames>Med Karim</forenames></author><author><keyname>Khalfallah</keyname><forenames>Ali</forenames></author><author><keyname>Bouhlel</keyname><forenames>Med Salim</forenames></author></authors><title>A new Watermarking Technique for Medical Image using Hierarchical
  Encryption</title><categories>cs.MM cs.CR</categories><comments>6 pages, 9 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues 11(4)
  (2014) 27-32</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, characterized by the innovation of technology and the
digital revolution, the field of media has become important. The transfer and
exchange of multimedia data and duplication have become major concerns of
researchers. Consequently, protecting copyrights and ensuring service safety is
needed. Cryptography has a specific role, is to protect secret files against
unauthorized access. In this paper, a hierarchical cryptosystem algorithm based
on Logistic Map chaotic systems is proposed. The results show that the proposed
method improves the security of the image. Experimental results on a database
of 200 medical images show that the proposed method significantly gives better
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4601</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4601</id><created>2014-09-16</created><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Pinsker</keyname><forenames>Michael</forenames></author><author><keyname>Pongr&#xe1;cz</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>Projective clone homomorphisms</title><categories>math.LO cs.CC cs.LO math.RA</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that a countable $\omega$-categorical structure interprets all
finite structures primitively positively if and only if its polymorphism clone
maps to the clone of projections on a two-element set via a continuous clone
homomorphism. We investigate the relationship between the existence of a clone
homomorphism to the projection clone, and the existence of such a homomorphism
which is continuous and thus meets the above criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4605</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4605</id><created>2014-09-16</created><authors><author><keyname>Herman</keyname><forenames>Ivo</forenames></author><author><keyname>Martinec</keyname><forenames>Dan</forenames></author><author><keyname>Hur&#xe1;k</keyname><forenames>Zden&#x11b;k</forenames></author><author><keyname>Sebek</keyname><forenames>Michael</forenames></author></authors><title>Nonzero bound on Fiedler eigenvalue causes exponential growth of
  H-infinity norm of vehicular platoon</title><categories>cs.SY</categories><comments>Conditionally accepted to IEEE Transactions on Automatic Control</comments><journal-ref>IEEE Transactions on Automatic Control, vol. 60, issue 8, pp.
  2248-2253, 2016,</journal-ref><doi>10.1109/TAC.2014.2366980</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider platoons composed of identical vehicles and controlled in a
distributed way, that is, each vehicle has its own onboard controller. The
regulation errors in spacing to the immediately preceeding and following
vehicles are weighted differently by the onboard controller, which thus
implements an asymmetric bidirectional control scheme. The weights can vary
along the platoon. We prove that such platoons have a nonzero uniform bound on
the second smallest eigenvalue of the graph Laplacian matrix - the Fiedler
eigenvalue. Furthermore, it is shown that existence of this bound always
signals undesirable scaling properties of the platoon. Namely, the H-infinity
norm of the transfer function of the platoon grows exponentially with the
number of vehicles regardless of the controllers used. Hence the benefits of a
uniform gap in the spectrum of a Laplacian with an asymetric distributed
controller are paid for by poor scaling as the number of vehicles grows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4613</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4613</id><created>2014-09-16</created><authors><author><keyname>Schlesinger</keyname><forenames>M. I.</forenames></author><author><keyname>Vodolazskiy</keyname><forenames>E. V.</forenames></author><author><keyname>Yakovenko</keyname><forenames>V. M.</forenames></author></authors><title>Similarity of closed polygonal curves in Frechet metric</title><categories>cs.CG</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article analyzes similarity of closed polygonal curves in Frechet metric,
which is stronger than the well-known Hausdorff metric and therefore is more
appropriate in some applications. An algorithm that determines whether the
Frechet distance between two closed polygonal curves with m and n vertices is
less than a given number is described. The described algorithm takes O(mn) time
whereas the previously known algorithms take O(mn log(mn)) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4614</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4614</id><created>2014-09-16</created><updated>2015-09-19</updated><authors><author><keyname>Ahmed</keyname><forenames>Bilal</forenames></author></authors><title>Lexical Normalisation of Twitter Data</title><categories>cs.CL</categories><comments>Removed typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter with over 500 million users globally, generates over 100,000 tweets
per minute . The 140 character limit per tweet, perhaps unintentionally,
encourages users to use shorthand notations and to strip spellings to their
bare minimum &quot;syllables&quot; or elisions e.g. &quot;srsly&quot;. The analysis of twitter
messages which typically contain misspellings, elisions, and grammatical
errors, poses a challenge to established Natural Language Processing (NLP)
tools which are generally designed with the assumption that the data conforms
to the basic grammatical structure commonly used in English language. In order
to make sense of Twitter messages it is necessary to first transform them into
a canonical form, consistent with the dictionary or grammar. This process,
performed at the level of individual tokens (&quot;words&quot;), is called lexical
normalisation. This paper investigates various techniques for lexical
normalisation of Twitter data and presents the findings as the techniques are
applied to process raw data from Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4617</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4617</id><created>2014-09-16</created><authors><author><keyname>Hochreiter</keyname><forenames>Ronald</forenames></author><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author></authors><title>The Role of Emotions in Propagating Brands in Social Networks</title><categories>cs.SI cs.CL stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key aspect of word of mouth marketing are emotions. Emotions in texts help
propagating messages in conventional advertising. In word of mouth scenarios,
emotions help to engage consumers and incite to propagate the message further.
While the function of emotions in offline marketing in general and word of
mouth marketing in particular is rather well understood, online marketing can
only offer a limited view on the function of emotions. In this contribution we
seek to close this gap. We therefore investigate how emotions function in
social media. To do so, we collected more than 30,000 brand marketing messages
from the Google+ social networking site. Using state of the art computational
linguistics classifiers, we compute the sentiment of these messages. Starting
out with Poisson regression-based baseline models, we seek to replicate earlier
findings using this large data set. We extend upon earlier research by
computing multi-level mixed effects models that compare the function of
emotions across different industries. We find that while the well known notion
of activating emotions propagating messages holds in general for our data as
well. But there are significant differences between the observed industries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4618</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4618</id><created>2014-09-16</created><updated>2015-05-11</updated><authors><author><keyname>Anjam</keyname><forenames>Immanuel</forenames></author><author><keyname>Valdman</keyname><forenames>Jan</forenames></author></authors><title>Fast MATLAB assembly of FEM matrices in 2D and 3D: Edge elements</title><categories>cs.MS math.NA</categories><comments>12 pages, 5 figures, ESCO 2014 conference</comments><msc-class>97N80, 65M60</msc-class><doi>10.1016/j.amc.2015.03.105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an effective and flexible way to assemble finite element stiffness
and mass matrices in MATLAB. We apply this for problems discretized by edge
finite elements. Typical edge finite elements are Raviart-Thomas elements used
in discretizations of H(div) spaces and Nedelec elements in discretizations of
H(curl) spaces. We explain vectorization ideas and comment on a freely
available MATLAB code which is fast and scalable with respect to time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4621</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4621</id><created>2014-09-16</created><updated>2015-07-18</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Pritam</forenames></author><author><keyname>Ghosh</keyname><forenames>Subir Kumar</forenames></author><author><keyname>Roy</keyname><forenames>Bodhayan</forenames></author></authors><title>Approximability of Guarding Weak Visibility Polygons</title><categories>cs.CG</categories><comments>24 pages, 20 figures, 28 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The art gallery problem enquires about the least number of guards that are
sufficient to ensure that an art gallery, represented by a polygon $P$, is
fully guarded. In 1998, the problems of finding the minimum number of point
guards, vertex guards, and edge guards required to guard $P$ were shown to be
APX-hard by Eidenbenz, Widmayer and Stamm. In 1987, Ghosh presented
approximation algorithms for vertex guards and edge guards that achieved a
ratio of $\mathcal{O}(\log n)$, which was improved upto $\mathcal{O}(\log\log
OPT)$ by King and Kirkpatrick in 2011. It has been conjectured that
constant-factor approximation algorithms exist for these problems. We settle
the conjecture for the special class of polygons that are weakly visible from
an edge and contain no holes by presenting a 6-approximation algorithm for
finding the minimum number of vertex guards that runs in $\mathcal{O}(n^2)$
time. On the other hand, for weak visibility polygons with holes, we present a
reduction from the Set Cover problem to show that there cannot exist a
polynomial time algorithm for the vertex guard problem with an approximation
ratio better than $((1 - \epsilon)/12)\ln n$ for any $\epsilon&gt;0$, unless NP=P.
We also show that, for the special class of polygons without holes that are
orthogonal as well as weakly visible from an edge, the approximation ratio can
be improved to 3. Finally, we consider the point guard problem and show that it
is NP-hard in the case of polygons weakly visible from an edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4626</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4626</id><created>2014-09-14</created><authors><author><keyname>Pluzhnik</keyname><forenames>Evgeniy</forenames></author><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author><author><keyname>Payain</keyname><forenames>Simon</forenames></author></authors><title>Laboratory Test Bench for Research Network and Cloud Computing</title><categories>cs.DC</categories><comments>5 pages</comments><journal-ref>Int'l J. of Communications, Network and System Sciences, 7:7,
  243-247</journal-ref><doi>10.4236/ijcns.2014.77026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At present moment, there is a great interest in development of information
systems operating in cloud infrastructures. Generally, many of tasks remain
unresolved such as tasks of optimization of large databases in a hybrid cloud
infrastructure, quality of service (QoS) at different levels of cloud services,
dynamic control of distribution of cloud resources in application systems and
many others. Research and development of new solutions can be limited in case
of using emulators or international commercial cloud services, due to the
closed architecture and limited opportunities for experimentation. Article
provides answers to questions on the establishment of a pilot cloud practically
&quot;at home&quot; with the ability to adjust the width of the emulation channel and
delays in data transmission. It also describes architecture and configuration
of the experimental setup. The proposed modular structure can be expanded by
available computing power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4627</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4627</id><created>2014-09-16</created><authors><author><keyname>Budikova</keyname><forenames>Petra</forenames></author><author><keyname>Botorek</keyname><forenames>Jan</forenames></author><author><keyname>Batko</keyname><forenames>Michal</forenames></author><author><keyname>Zezula</keyname><forenames>Pavel</forenames></author></authors><title>DISA at ImageCLEF 2014 Revised: Search-based Image Annotation with DeCAF
  Features</title><categories>cs.IR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper constitutes an extension to the report on DISA-MU team
participation in the ImageCLEF 2014 Scalable Concept Image Annotation Task as
published in [3]. Specifically, we introduce a new similarity search component
that was implemented into the system, report on the results achieved by
utilizing this component, and analyze the influence of different similarity
search parameters on the annotation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4629</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4629</id><created>2014-09-16</created><authors><author><keyname>Gacek</keyname><forenames>Andrew</forenames></author><author><keyname>Backes</keyname><forenames>John</forenames></author><author><keyname>Cofer</keyname><forenames>Darren</forenames></author><author><keyname>Slind</keyname><forenames>Konrad</forenames></author><author><keyname>Whalen</keyname><forenames>Mike</forenames></author></authors><title>Resolute: An Assurance Case Language for Architecture Models</title><categories>cs.SE</categories><comments>9 pages, accepted to HILT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arguments about the safety, security, and correctness of a complex system are
often made in the form of an assurance case. An assurance case is a structured
argument, often represented with a graphical interface, that presents and
supports claims about a system's behavior. The argument may combine different
kinds of evidence to justify its top level claim. While assurance cases deliver
some level of guarantee of a system's correctness, they lack the rigor that
proofs from formal methods typically provide. Furthermore, changes in the
structure of a model during development may result in inconsistencies between a
design and its assurance case. Our solution is a framework for automatically
generating assurance cases based on 1) a system model specified in an
architectural design language, 2) a set of logical rules expressed in a domain
specific language that we have developed, and 3) the results of other formal
analyses that have been run on the model. We argue that the rigor of these
automatically generated assurance cases exceeds those of traditional assurance
case arguments because of their more formal logical foundation and direct
connection to the architectural model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4637</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4637</id><created>2014-09-16</created><authors><author><keyname>Koenighofer</keyname><forenames>Robert</forenames></author><author><keyname>Toegl</keyname><forenames>Ronald</forenames></author><author><keyname>Bloem</keyname><forenames>Roderick</forenames></author></authors><title>Automatic Error Localization for Software using Deductive Verification</title><categories>cs.LO cs.SE</categories><comments>This is an extended version of [8], featuring an additional appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even competent programmers make mistakes. Automatic verification can detect
errors, but leaves the frustrating task of finding the erroneous line of code
to the user. This paper presents an automatic approach for identifying
potential error locations in software. It is based on a deductive verification
engine, which detects errors in functions annotated with pre- and
post-conditions. Using an automatic theorem prover, our approach finds
expressions in the code that can be modified such that the program satisfies
its specification. Scalability is achieved by analyzing each function in
isolation. We have implemented our approach in the widely used Frama-C
framework and present first experimental results. This is an extended version
of [8], featuring an additional appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4639</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4639</id><created>2014-09-16</created><authors><author><keyname>Heidari</keyname><forenames>Rahmat</forenames></author><author><keyname>Seron</keyname><forenames>Maria</forenames></author><author><keyname>Braslavsky</keyname><forenames>Julio</forenames></author></authors><title>Ultimate boundedness of droop controlled Microgrids with secondary loops</title><categories>cs.SY</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study theoretical properties of inverter-based microgrids
controlled via primary and secondary loops. Stability of these microgrids has
been the subject of a number of recent studies. Conventional approaches based
on standard hierarchical control rely on time-scale separation between primary
and secondary control loops to show local stability of equilibria. In this
paper we show that (i) frequency regulation can be ensured without assuming
time-scale separation and, (ii) ultimate boundedness of the trajectories
starting inside a region of the state space can be guaranteed under a condition
on the inverters power injection errors. The trajectory ultimate bound can be
computed by simple iterations of a nonlinear mapping and provides a certificate
of the overall performance of the controlled microgrid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4653</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4653</id><created>2014-09-16</created><authors><author><keyname>Bianculli</keyname><forenames>Domenico</forenames></author><author><keyname>Ghezzi</keyname><forenames>Carlo</forenames></author><author><keyname>Krstic</keyname><forenames>Srdan</forenames></author><author><keyname>Pietro</keyname><forenames>Pierluigi San</forenames></author></authors><title>Offline Trace Checking of Quantitative Properties of Service-Based
  Applications</title><categories>cs.SE</categories><comments>19 pages, 7 figures, Extended version of the SOCA 2014 paper</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service-based applications are often developed as compositions of partner
services. A service integrator needs precise methods to specify the quality
attributes expected by each partner service, as well as effective techniques to
verify these attributes. In previous work, we identified the most common
specification patterns related to provisioning service-based applications and
developed an expressive specification language (SOLOIST) that supports them.
SOLOIST is an extension of metric temporal logic with aggregate temporal
modalities that can be used to write quantitative temporal properties.
  In this paper we address the problem of performing offline checking of
service execution traces against quantitative requirements specifications
written in SOLOIST. We present a translation of SOLOIST into CLTLB(D), a
variant of linear temporal logic, and reduce the trace checking of SOLOIST to
bounded satisfiability checking of CLTLB(D), which is supported by ZOT, an
SMT-based verification toolkit. We detail the results of applying the proposed
offline trace checking procedure to different types of traces, and compare its
performance with previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4671</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4671</id><created>2014-09-12</created><updated>2014-10-01</updated><authors><author><keyname>Masood</keyname><forenames>Mudassir</forenames></author><author><keyname>Afify</keyname><forenames>Laila H.</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>Efficient Coordinated Recovery of Sparse Channels in Massive MIMO</title><categories>stat.AP cs.IT math.IT</categories><comments>16 pages, 12 figures</comments><doi>10.1109/TSP.2014.2369005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of estimating sparse channels in massive
MIMO-OFDM systems. Most wireless channels are sparse in nature with large delay
spread. In addition, these channels as observed by multiple antennas in a
neighborhood have approximately common support. The sparsity and common support
properties are attractive when it comes to the efficient estimation of large
number of channels in massive MIMO systems. Moreover, to avoid pilot
contamination and to achieve better spectral efficiency, it is important to use
a small number of pilots. We present a novel channel estimation approach which
utilizes the sparsity and common support properties to estimate sparse channels
and require a small number of pilots. Two algorithms based on this approach
have been developed which perform Bayesian estimates of sparse channels even
when the prior is non-Gaussian or unknown. Neighboring antennas share among
each other their beliefs about the locations of active channel taps to perform
estimation. The coordinated approach improves channel estimates and also
reduces the required number of pilots. Further improvement is achieved by the
data-aided version of the algorithm. Extensive simulation results are provided
to demonstrate the performance of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4687</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4687</id><created>2014-09-16</created><authors><author><keyname>Hummel</keyname><forenames>Patrick</forenames></author><author><keyname>McAfee</keyname><forenames>R. Preston</forenames></author></authors><title>Position Auctions with Externalities and Brand Effects</title><categories>cs.GT</categories><comments>19 pages. A shorter version of this paper will appear in WINE 2014</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents models for predicted click-through rates in position
auctions that take into account two possibilities that are not normally
considered---that the identities of ads shown in other positions may affect the
probability that an ad in a particular position receives a click
(externalities) and that some ads may be less adversely affected by being shown
in a lower position than others (brand effects). We present a general axiomatic
methodology for how click probabilities are affected by the qualities of the
ads in the other positions, and illustrate that using these axioms will
increase revenue as long as higher quality ads tend to be ranked ahead of lower
quality ads. We also present appropriate algorithms for selecting the optimal
allocation of ads when predicted click-through rates are governed by either the
models of externalities or brand effects that we consider. Finally, we analyze
the performance of a greedy algorithm of ranking the ads by their expected
cost-per-1000-impressions bids when the true click-through rates are governed
by our model of predicted click-through rates with brand effects and illustrate
that such an algorithm will potentially cost as much as half of the total
possible social welfare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4689</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4689</id><created>2014-09-16</created><updated>2015-05-24</updated><authors><author><keyname>Lederer</keyname><forenames>Johannes</forenames></author><author><keyname>Guadarrama</keyname><forenames>Sergio</forenames></author></authors><title>Compute Less to Get More: Using ORC to Improve Sparse Filtering</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse Filtering is a popular feature learning algorithm for image
classification pipelines. In this paper, we connect the performance of Sparse
Filtering with spectral properties of the corresponding feature matrices. This
connection provides new insights into Sparse Filtering; in particular, it
suggests early stopping of Sparse Filtering. We therefore introduce the Optimal
Roundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We
show that this stopping criterion is related with pre-processing procedures
such as Statistical Whitening and demonstrate that it can make image
classification with Sparse Filtering considerably faster and more accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4695</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4695</id><created>2014-09-16</created><authors><author><keyname>Tagarelli</keyname><forenames>Andrea</forenames></author><author><keyname>Interdonato</keyname><forenames>Roberto</forenames></author></authors><title>Lurking in Social Networks: Topology-based Analysis and Ranking Methods</title><categories>cs.SI physics.soc-ph</categories><comments>24 pages, 10 figures, 16 tables</comments><journal-ref>Social Network Analysis and Mining. August 2014, 4:230</journal-ref><doi>10.1007/s13278-014-0230-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The massive presence of silent members in online communities, the so-called
lurkers, has long attracted the attention of researchers in social science,
cognitive psychology, and computer-human interaction. However, the study of
lurking phenomena represents an unexplored opportunity of research in data
mining, information retrieval and related fields. In this paper, we take a
first step towards the formal specification and analysis of lurking in social
networks. We address the new problem of lurker ranking and propose the first
centrality methods specifically conceived for ranking lurkers in social
networks. Our approach utilizes only the network topology without probing into
text contents or user relationships related to media. Using Twitter, Flickr,
FriendFeed and GooglePlus as cases in point, our methods' performance was
evaluated against data-driven rankings as well as existing centrality methods,
including the classic PageRank and alpha-centrality. Empirical evidence has
shown the significance of our lurker ranking approach, and its uniqueness in
effectively identifying and ranking lurkers in an online social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4696</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4696</id><created>2014-09-16</created><updated>2015-04-15</updated><authors><author><keyname>Karwa</keyname><forenames>Vishesh</forenames></author><author><keyname>Slavkovi&#x107;</keyname><forenames>Aleksandra B.</forenames></author><author><keyname>Krivitsky</keyname><forenames>Pavel</forenames></author></authors><title>Differentially Private Exponential Random Graphs</title><categories>stat.OT cs.CR stat.ME</categories><comments>minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose methods to release and analyze synthetic graphs in order to
protect privacy of individual relationships captured by the social network.
Proposed techniques aim at fitting and estimating a wide class of exponential
random graph models (ERGMs) in a differentially private manner, and thus offer
rigorous privacy guarantees. More specifically, we use the randomized response
mechanism to release networks under $\epsilon$-edge differential privacy. To
maintain utility for statistical inference, treating the original graph as
missing, we propose a way to use likelihood based inference and Markov chain
Monte Carlo (MCMC) techniques to fit ERGMs to the produced synthetic networks.
We demonstrate the usefulness of the proposed techniques on a real data
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4698</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4698</id><created>2014-09-16</created><authors><author><keyname>Hong</keyname><forenames>Charmgil</forenames></author><author><keyname>Batal</keyname><forenames>Iyad</forenames></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author></authors><title>A Mixtures-of-Experts Framework for Multi-Label Classification</title><categories>cs.LG</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a novel probabilistic approach for multi-label classification that
is based on the mixtures-of-experts architecture combined with recently
introduced conditional tree-structured Bayesian networks. Our approach captures
different input-output relations from multi-label data using the efficient
tree-structured classifiers, while the mixtures-of-experts architecture aims to
compensate for the tree-structured restrictions and build a more accurate
model. We develop and present algorithms for learning the model from data and
for performing multi-label predictions on future data instances. Experiments on
multiple benchmark datasets demonstrate that our approach achieves highly
competitive results and outperforms the existing state-of-the-art multi-label
classification methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4711</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4711</id><created>2014-09-16</created><authors><author><keyname>Chlebus</keyname><forenames>Bogdan S.</forenames></author><author><keyname>G&#x105;sieniec</keyname><forenames>Leszek</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author><author><keyname>Shvartsman</keyname><forenames>Alexander A.</forenames></author></authors><title>Doing-it-All with Bounded Work and Communication</title><categories>cs.DC</categories><comments>62 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Do-All problem, where $p$ cooperating processors need to
complete $t$ similar and independent tasks in an adversarial setting. Here we
deal with a synchronous message passing system with processors that are subject
to crash failures. Efficiency of algorithms in this setting is measured in
terms of work complexity (also known as total available processor steps) and
communication complexity (total number of point-to-point messages). When work
and communication are considered to be comparable resources, then the overall
efficiency is meaningfully expressed in terms of effort defined as work +
communication. We develop and analyze a constructive algorithm that has work
${\cal O}( t + p \log p\, (\sqrt{p\log p}+\sqrt{t\log t}\, ) )$ and a
nonconstructive algorithm that has work ${\cal O}(t +p \log^2 p)$. The latter
result is close to the lower bound $\Omega(t + p \log p/ \log \log p)$ on work.
The effort of each of these algorithms is proportional to its work when the
number of crashes is bounded above by $c\,p$, for some positive constant $c &lt;
1$. We also present a nonconstructive algorithm that has effort ${\cal O}(t + p
^{1.77})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4712</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4712</id><created>2014-09-16</created><authors><author><keyname>Forni</keyname><forenames>Fulvio</forenames></author><author><keyname>Sepulchre</keyname><forenames>Rodolphe</forenames></author></authors><title>Differential analysis of nonlinear systems: revisiting the pendulum
  example</title><categories>cs.SY math.DS</categories><comments>12 pages, 14 figures. Tutorial paper for the 53rd IEEE Conference on
  Decision and Control, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential analysis aims at inferring global properties of nonlinear
behaviors from the local analysis of the linearized dynamics. The paper
motivates and illustrates the use of differential analysis on the nonlinear
pendulum model, an archetype example of nonlinear behavior. Special emphasis is
put on recent work by the authors in this area, which includes a differential
Lyapunov framework for contraction analysis and the concept of differential
positivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4714</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4714</id><created>2014-09-16</created><updated>2015-03-06</updated><authors><author><keyname>Kulig</keyname><forenames>Andrzej</forenames></author><author><keyname>Drozdz</keyname><forenames>Stanislaw</forenames></author><author><keyname>Kwapien</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Oswiecimka</keyname><forenames>Pawel</forenames></author></authors><title>Modeling the average shortest path length in growth of word-adjacency
  networks</title><categories>cs.CL physics.soc-ph</categories><comments>Accepted for publication in Physical Review E</comments><journal-ref>Phys. Rev. E. 91, 032810 (2015)</journal-ref><doi>10.1103/PhysRevE.91.032810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate properties of evolving linguistic networks defined by the
word-adjacency relation. Such networks belong to the category of networks with
accelerated growth but their shortest path length appears to reveal the network
size dependence of different functional form than the ones known so far. We
thus compare the networks created from literary texts with their artificial
substitutes based on different variants of the Dorogovtsev-Mendes model and
observe that none of them is able to properly simulate the novel asymptotics of
the shortest path length. Then, we identify the local chain-like linear growth
induced by grammar and style as a missing element in this model and extend it
by incorporating such effects. It is in this way that a satisfactory agreement
with the empirical result is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4727</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4727</id><created>2014-09-11</created><authors><author><keyname>Mustafidah</keyname><forenames>Hindayati</forenames></author><author><keyname>Hartati</keyname><forenames>Sri</forenames></author><author><keyname>Wardoyo</keyname><forenames>Retantyo</forenames></author><author><keyname>Harjoko</keyname><forenames>Agus</forenames></author></authors><title>Selection of Most Appropriate Backpropagation Training Algorithm in Data
  Pattern Recognition</title><categories>cs.NE</categories><comments>4 pages, 2 figures, 6 tables, Published with International Journal of
  Computer Trends and Technology (IJCTT) 14(2):92-95, Aug 2014</comments><doi>10.14445/22312803/IJCTT-V14P120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are several training algorithms for backpropagation method in neural
network. Not all of these algorithms have the same accuracy level demonstrated
through the percentage level of suitability in recognizing patterns in the
data. In this research tested 12 training algorithms specifically in recognize
data patterns of test validity. The basic network parameters used are the
maximum allowable epoch = 1000, target error = 10-3, and learning rate = 0.05.
Of the twelve training algorithms each performed 20 times looping. The test
results obtained that the percentage rate of the great match is trainlm
algorithm with alpha 5% have adequate levels of suitability of 87.5% at the
level of significance of 0.000. This means the most appropriate training
algorithm in recognizing the the data pattern of test validity is the trainlm
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4739</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4739</id><created>2014-09-16</created><updated>2015-02-10</updated><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Karray</keyname><forenames>Mohamed Kadhem</forenames><affiliation>FT R\&amp;D</affiliation></author><author><keyname>Keeler</keyname><forenames>Holger Paul</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Wireless networks appear Poissonian due to strong shadowing</title><categories>cs.NI math.PR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geographic locations of cellular base stations sometimes can be well fitted
with spatial homogeneous Poisson point processes. In this paper we make a
complementary observation: In the presence of the log-normal shadowing of
sufficiently high variance, the statistics of the propagation loss of a single
user with respect to different network stations are invariant with respect to
their geographic positioning, whether regular or not, for a wide class of
empirically homogeneous networks. Even in perfectly hexagonal case they appear
as though they were realized in a Poisson network model, i.e., form an
inhomogeneous Poisson point process on the positive half-line with a power-law
density characterized by the path-loss exponent. At the same time, the
conditional distances to the corresponding base stations, given their observed
propagation losses, become independent and log-normally distributed, which can
be seen as a decoupling between the real and model geometry. The result applies
also to Suzuki (Rayleigh-log-normal) propagation model. We use
Kolmogorov-Smirnov test to empirically study the quality of the Poisson
approximation and use it to build a linear-regression method for the
statistical estimation of the value of the path-loss exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4740</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4740</id><created>2014-09-16</created><authors><author><keyname>Asghar</keyname><forenames>Ahmad Bilal</forenames></author><author><keyname>Smith</keyname><forenames>Stephen L.</forenames></author></authors><title>Robot Monitoring for the Detection and Confirmation of Stochastic Events</title><categories>cs.RO</categories><comments>Extended version of IEEE CDC 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a robot patrolling problem in which events arrive
randomly over time at the vertices of a graph. When an event arrives it remains
active for a random amount of time. If that time active exceeds a certain
threshold, then we say that the event is a true event; otherwise it is a false
event. The robot(s) can traverse the graph to detect newly arrived events, and
can revisit these events in order to classify them as true or false. The goal
is to plan robot paths that maximize the number of events that are correctly
classified, with the constraint that there are no false positives. We show that
the offline version of this problem is NP-hard. We then consider a simple
patrolling policy based on the traveling salesman tour, and characterize the
probability of correctly classifying an event. We investigate the problem when
multiple robots follow the same path, and we derive the optimal (and not
necessarily uniform) spacing between robots on the path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4744</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4744</id><created>2014-09-16</created><updated>2014-11-16</updated><authors><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>An Efficient List Decoder Architecture for Polar Codes</title><categories>cs.AR cs.IT math.IT</categories><comments>12 pages, accepted by IEEE TVLSI Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long polar codes can achieve the symmetric capacity of arbitrary binary-input
discrete memoryless channels under a low complexity successive cancelation (SC)
decoding algorithm. However, for polar codes with short and moderate code
length, the decoding performance of the SC algorithm is inferior. The cyclic
redundancy check (CRC) aided successive cancelation list (SCL) decoding
algorithm has better error performance than the SC algorithm for short or
moderate polar codes. In this paper, we propose an efficient list decoder
architecture for the CRC aided SCL algorithm, based on both algorithmic
reformulations and architectural techniques. In particular, an area efficient
message memory architecture is proposed to reduce the area of the proposed
decoder architecture. An efficient path pruning unit suitable for large list
size is also proposed. For a polar code of length 1024 and rate $\frac{1}{2}$,
when list size $L=2$ and 4, the proposed list decoder architecture is
implemented under a TSMC 90nm CMOS technology. Compared with the list decoders
in the literature, our decoder achieves 1.33 to 1.96 times hardware efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4747</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4747</id><created>2014-09-16</created><authors><author><keyname>Rabenoro</keyname><forenames>Tsirizo</forenames><affiliation>SAMM</affiliation></author><author><keyname>Lacaille</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>SAMM</affiliation></author><author><keyname>Cottrell</keyname><forenames>Marie</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Anomaly Detection Based on Indicators Aggregation</title><categories>stat.ML cs.LG</categories><comments>International Joint Conference on Neural Networks (IJCNN 2014),
  Beijing : China (2014). arXiv admin note: substantial text overlap with
  arXiv:1407.0880</comments><proxy>ccsd</proxy><doi>10.1109/IJCNN.2014.6889841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic anomaly detection is a major issue in various areas. Beyond mere
detection, the identification of the source of the problem that produced the
anomaly is also essential. This is particularly the case in aircraft engine
health monitoring where detecting early signs of failure (anomalies) and
helping the engine owner to implement efficiently the adapted maintenance
operations (fixing the source of the anomaly) are of crucial importance to
reduce the costs attached to unscheduled maintenance. This paper introduces a
general methodology that aims at classifying monitoring signals into normal
ones and several classes of abnormal ones. The main idea is to leverage expert
knowledge by generating a very large number of binary indicators. Each
indicator corresponds to a fully parametrized anomaly detector built from
parametric anomaly scores designed by experts. A feature selection method is
used to keep only the most discriminant indicators which are used at inputs of
a Naive Bayes classifier. This give an interpretable classifier based on
interpretable anomaly detectors whose parameters have been optimized indirectly
by the selection process. The proposed methodology is evaluated on simulated
data designed to reproduce some of the anomaly types observed in real world
engines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4752</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4752</id><created>2014-09-16</created><updated>2015-03-25</updated><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Schaefer</keyname><forenames>Rafael F.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On the Continuity of the Secrecy Capacity of Compound and Arbitrarily
  Varying Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>revised. Section VI added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wiretap channel models secure communication of two users in the presence
of a non-legitimate eavesdropper who must be kept ignorant of transmitted
messages. The performance of such a system is usually characterized by its
secrecy capacity determining the maximum transmission rate of secure
communication. In this paper, the issue of whether the secrecy capacity is a
continuous function of the system parameters or not is examined. In particular,
this is done for channel uncertainty modeled via compound channels and
arbitrarily varying channels, in which the legitimate users know only that the
true channel realization is from a pre-specified uncertainty set. In the former
model, this realization remains constant for the whole duration of
transmission, while in the latter the realization varies from channel use to
channel use in an unknown and arbitrary manner. These models not only capture
the case of channel uncertainty, but are also suitable to model scenarios in
which a malicious adversary influences or jams the legitimate transmission. The
secrecy capacity of the compound wiretap channel is shown to be robust in the
sense that it is a continuous function of the uncertainty set. Thus, small
variations in the uncertainty set lead to small variations in secrecy capacity.
On the other hand, the deterministic secrecy capacity of the arbitrarily
varying wiretap channel is shown to be discontinuous in the uncertainty set
meaning that small variations can lead to dramatic losses in capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4757</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4757</id><created>2014-09-16</created><authors><author><keyname>Ishiguro</keyname><forenames>Katsuhiko</forenames></author><author><keyname>Sato</keyname><forenames>Issei</forenames></author><author><keyname>Ueda</keyname><forenames>Naonori</forenames></author></authors><title>Collapsed Variational Bayes Inference of Infinite Relational Model</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Infinite Relational Model (IRM) is a probabilistic model for relational
data clustering that partitions objects into clusters based on observed
relationships. This paper presents Averaged CVB (ACVB) solutions for IRM,
convergence-guaranteed and practically useful fast Collapsed Variational Bayes
(CVB) inferences. We first derive ordinary CVB and CVB0 for IRM based on the
lower bound maximization. CVB solutions yield deterministic iterative
procedures for inferring IRM given the truncated number of clusters. Our
proposal includes CVB0 updates of hyperparameters including the concentration
parameter of the Dirichlet Process, which has not been studied in the
literature. To make the CVB more practically useful, we further study the CVB
inference in two aspects. First, we study the convergence issues and develop a
convergence-guaranteed algorithm for any CVB-based inferences called ACVB,
which enables automatic convergence detection and frees non-expert
practitioners from difficult and costly manual monitoring of inference
processes. Second, we present a few techniques for speeding up IRM inferences.
In particular, we describe the linear time inference of CVB0, allowing the IRM
for larger relational data uses. The ACVB solutions of IRM showed comparable or
better performance compared to existing inference methods in experiments, and
provide deterministic, faster, and easier convergence detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4760</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4760</id><created>2014-09-16</created><authors><author><keyname>Tavakoli</keyname><forenames>Hassan</forenames></author></authors><title>A Fast Convergence Density Evolution Algorithm for Optimal Rate LDPC
  Codes in BEC</title><categories>cs.IT math.IT</categories><comments>This Paper is a draft of final paper which represented in 7th
  International Symposium on Telecommunications (IST'2014)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We derive a new fast convergent Density Evolution algorithm for finding
optimal rate Low-Density Parity-Check (LDPC) codes used over the binary erasure
channel (BEC). The fast convergence property comes from the modified Density
Evolution (DE), a numerical method for analyzing the behavior of iterative
decoding convergence of a LDPC code. We have used the method of [16] for
designing of a LDPC code with optimal rate. This has been done for a given
parity check node degree distribution, erasure probability and specified DE
constraint. The fast behavior of DE and found optimal rate with this method
compare with the previous DE constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4761</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4761</id><created>2014-09-16</created><authors><author><keyname>Tavakoli</keyname><forenames>Hassan</forenames></author></authors><title>Reducing the Complexity of the Linear Programming Decoding</title><categories>cs.IT math.IT</categories><comments>This Paper is a draft of final paper which represented in 7th
  International Symposium on Telecommunications (IST'2014)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we show how the complexity of Linear Programming (LP) decoder
can decrease. We use the degree 3 check equation to model all variation check
degrees. The complexity of LP decoding is directed relative to the number of
constraint. Number of constraint for original LP decoder is O(n*(2^n)). Our
method decrease the number of the constraint to O(n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4762</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4762</id><created>2014-09-16</created><authors><author><keyname>Tavakoli</keyname><forenames>Hassan</forenames></author></authors><title>Source and Channel Optimal Rate LDPC Code Design for one Sender in
  BE-MAC with Source Correlation</title><categories>cs.IT math.IT</categories><comments>This Paper is a draft of final paper which represented in 7th
  International Symposium on Telecommunications (IST'2014)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present an extension of the semidefinite programming
formulation of the optimal rate code design in single link Binary Erasure
Channel (BEC) proposed by the authors to the Binary Erasure Multiple Access
Channel (BE-MAC) with two sources correlation. This new way can be easily
extended to the multiple access senders. Simulation results show the efficiency
and effectiveness of the new approach in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4800</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4800</id><created>2014-09-16</created><authors><author><keyname>Bermejo-Vega</keyname><forenames>Juan</forenames></author><author><keyname>Lin</keyname><forenames>Cedric Yen-Yu</forenames></author><author><keyname>Nest</keyname><forenames>Maarten Van den</forenames></author></authors><title>The computational power of normalizer circuits over black-box groups</title><categories>quant-ph cs.CC</categories><comments>40 pages + appendices</comments><report-no>MIT-CTP/4584</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a precise connection between Clifford circuits, Shor's
factoring algorithm and several other famous quantum algorithms with
exponential quantum speed-ups for solving Abelian hidden subgroup problems. We
show that all these different forms of quantum computation belong to a common
new restricted model of quantum operations that we call \emph{black-box
normalizer circuits}. To define these, we extend the previous model of
normalizer circuits [arXiv:1201.4867v1,arXiv:1210.3637,arXiv:1409.3208], which
are built of quantum Fourier transforms, group automorphism and quadratic phase
gates associated to an Abelian group $G$. In previous works, the group $G$ is
always given in an explicitly decomposed form. In our model, we remove this
assumption and allow $G$ to be a black-box group. While standard normalizer
circuits were shown to be efficiently classically simulable
[arXiv:1201.4867v1,arXiv:1210.3637,arXiv:1409.3208], we find that normalizer
circuits are powerful enough to factorize and solve classically-hard problems
in the black-box setting. We further set upper limits to their computational
power by showing that decomposing finite Abelian groups is complete for the
associated complexity class. In particular, solving this problem renders
black-box normalizer circuits efficiently classically simulable by exploiting
the generalized stabilizer formalism in
[arXiv:1201.4867v1,arXiv:1210.3637,arXiv:1409.3208]. Lastly, we employ our
connection to draw a few practical implications for quantum algorithm design:
namely, we give a no-go theorem for finding new quantum algorithms with
black-box normalizer circuits, a universality result for low-depth normalizer
circuits, and identify two other complete problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4802</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4802</id><created>2014-09-16</created><updated>2015-01-01</updated><authors><author><keyname>Karawia</keyname><forenames>A. A.</forenames></author></authors><title>On Solving Pentadiagonal Linear Systems via Transformations</title><categories>math.NA cs.NA cs.SC</categories><msc-class>15A15, 15A23, 68W30, 11Y05, 33F10</msc-class><acm-class>F.2.1; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many authors studied numeric algorithms for solving the linear systems of the
pentadiagonal type. The well-known Fast Pentadiagonal System Solver algorithm
is an example of such algorithms. The current article are described new numeric
and symbolic algorithms for solving pentadiagonal linear systems via
transformations. New algorithms are natural generalization of the work
presented in [Moawwad El- Mikkawy and Faiz Atlan, Algorithms for Solving Linear
Systems of Equations of Tridiagonal Type via Transformations, Applied
Mathematics, 2014, 5, 413-422]. The symbolic algorithms remove the cases where
the numeric algorithms fail. The computational cost of our algorithms is given.
Some examples are given in order to illustrate the effectiveness of the
proposed algorithms. All of the experiments are performed on a computer with
the aid of programs written in MATLAB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4812</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4812</id><created>2014-09-16</created><updated>2015-01-13</updated><authors><author><keyname>Guar&#xed;n-Zapata</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Gomez</keyname><forenames>Juan</forenames></author></authors><title>Evaluation of the Spectral Finite Element Method With the Theory of
  Phononic Crystals</title><categories>cs.CE math.NA physics.comp-ph</categories><comments>20 pages, 8 figures, Preprint of an article published in Journal of
  Computational Acoustics</comments><msc-class>65N30, 74B05, 74J05, 74J20, 74S05</msc-class><journal-ref>Journal of Computational Acoustics, Vol. 23 (2015) 1550004 (17
  pages)</journal-ref><doi>10.1142/S0218396X15500046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluated the performance of the classical and spectral finite element
method in the simulation of elastodynamic problems. We used as a quality
measure their ability to capture the actual dispersive behavior of the
material. Four different materials are studied: a homogeneous non-dispersive
material, a bilayer material, and composite materials consisting of an aluminum
matrix and brass inclusions or voids. To obtain the dispersion properties,
spatial periodicity is assumed so the analysis is conducted using Floquet-Bloch
principles. The effects in the dispersion properties of the lumping process for
the mass matrices resulting from the classical finite element method are also
investigated, since that is a common practice when the problem is solved with
explicit time marching schemes. At high frequencies the predictions with the
spectral technique exactly match the analytical dispersion curves, while the
classical method does not. This occurs even at the same computational demands.
At low frequencies however, the results from both the classical (consistent or
mass-lumped) and spectral finite element coincide with the analytically
determined curves. Surprisingly, at low frequencies even the results obtained
with the artificial diagonal mass matrix from the classical technique exactly
match the analytic dispersion curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4813</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4813</id><created>2014-09-16</created><authors><author><keyname>Zhang</keyname><forenames>Xiao</forenames></author><author><keyname>Martin</keyname><forenames>Travis</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Identification of core-periphery structure in networks</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><doi>10.1103/PhysRevE.91.032803</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Many networks can be usefully decomposed into a dense core plus an outlying,
loosely-connected periphery. Here we propose an algorithm for performing such a
decomposition on empirical network data using methods of statistical inference.
Our method fits a generative model of core-periphery structure to observed data
using a combination of an expectation--maximization algorithm for calculating
the parameters of the model and a belief propagation algorithm for calculating
the decomposition itself. We find the method to be efficient, scaling easily to
networks with a million or more nodes and we test it on a range of networks,
including real-world examples as well as computer-generated benchmarks, for
which it successfully identifies known core-periphery structure with low error
rate. We also demonstrate that the method is immune from the detectability
transition observed in the related community detection problem, which prevents
the detection of community structure when that structure is too weak. There is
no such transition for core-periphery structure, which is detectable, albeit
with some statistical error, no matter how weak it is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4814</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4814</id><created>2014-09-16</created><authors><author><keyname>Simard</keyname><forenames>Patrice</forenames></author><author><keyname>Chickering</keyname><forenames>David</forenames></author><author><keyname>Lakshmiratan</keyname><forenames>Aparna</forenames></author><author><keyname>Charles</keyname><forenames>Denis</forenames></author><author><keyname>Bottou</keyname><forenames>Leon</forenames></author><author><keyname>Suarez</keyname><forenames>Carlos Garcia Jurado</forenames></author><author><keyname>Grangier</keyname><forenames>David</forenames></author><author><keyname>Amershi</keyname><forenames>Saleema</forenames></author><author><keyname>Verwey</keyname><forenames>Johan</forenames></author><author><keyname>Suh</keyname><forenames>Jina</forenames></author></authors><title>ICE: Enabling Non-Experts to Build Models Interactively for Large-Scale
  Lopsided Problems</title><categories>cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quick interaction between a human teacher and a learning machine presents
numerous benefits and challenges when working with web-scale data. The human
teacher guides the machine towards accomplishing the task of interest. The
learning machine leverages big data to find examples that maximize the training
value of its interaction with the teacher. When the teacher is restricted to
labeling examples selected by the machine, this problem is an instance of
active learning. When the teacher can provide additional information to the
machine (e.g., suggestions on what examples or predictive features should be
used) as the learning task progresses, then the problem becomes one of
interactive learning.
  To accommodate the two-way communication channel needed for efficient
interactive learning, the teacher and the machine need an environment that
supports an interaction language. The machine can access, process, and
summarize more examples than the teacher can see in a lifetime. Based on the
machine's output, the teacher can revise the definition of the task or make it
more precise. Both the teacher and the machine continuously learn and benefit
from the interaction.
  We have built a platform to (1) produce valuable and deployable models and
(2) support research on both the machine learning and user interface challenges
of the interactive learning problem. The platform relies on a dedicated,
low-latency, distributed, in-memory architecture that allows us to construct
web-scale learning machines with quick interaction speed. The purpose of this
paper is to describe this architecture and demonstrate how it supports our
research efforts. Preliminary results are presented as illustrations of the
architecture but are not the primary focus of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4822</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4822</id><created>2014-09-16</created><authors><author><keyname>Zhang</keyname><forenames>Zheng</forenames><affiliation>Abe</affiliation></author><author><keyname>Yang</keyname><forenames>Xiu</forenames><affiliation>Abe</affiliation></author><author><keyname>Marucci</keyname><forenames>Giovanni</forenames><affiliation>Abe</affiliation></author><author><keyname>Maffezzoni</keyname><forenames>Paolo</forenames><affiliation>Abe</affiliation></author><author><keyname>Ibrahim</keyname><affiliation>Abe</affiliation></author><author><keyname>Elfadel</keyname><forenames>M.</forenames></author><author><keyname>Karniadakis</keyname><forenames>George Em</forenames></author><author><keyname>Daniel</keyname><forenames>Luca</forenames></author></authors><title>Stochastic Testing Simulator for Integrated Circuits and MEMS:
  Hierarchical and Sparse Techniques</title><categories>cs.CE</categories><comments>Accepted to IEEE Custom Integrated Circuits Conference in June 2014.
  arXiv admin note: text overlap with arXiv:1407.3023</comments><journal-ref>Z. Zhang, X. Yang, G. Marucci, P. Maffezzoni, I. M. Elfadel, G. E.
  Karniadakis and L. Daniel, &quot;Stochastic Testing Simulator for Integrated
  Circuits and MEMS: Hierarchical and Sparse Techniques&quot;, in Proc. CICC, Sept.
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process variations are a major concern in today's chip design since they can
significantly degrade chip performance. To predict such degradation, existing
circuit and MEMS simulators rely on Monte Carlo algorithms, which are typically
too slow. Therefore, novel fast stochastic simulators are highly desired. This
paper first reviews our recently developed stochastic testing simulator that
can achieve speedup factors of hundreds to thousands over Monte Carlo. Then, we
develop a fast hierarchical stochastic spectral simulator to simulate a complex
circuit or system consisting of several blocks. We further present a fast
simulation approach based on anchored ANOVA (analysis of variance) for some
design problems with many process variations. This approach can reduce the
simulation cost and can identify which variation sources have strong impacts on
the circuit's performance. The simulation results of some circuit and MEMS
examples are reported to show the effectiveness of our simulator
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4824</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4824</id><created>2014-09-16</created><authors><author><keyname>Zhang</keyname><forenames>Zheng</forenames><affiliation>Abe</affiliation></author><author><keyname>Ibrahim</keyname><affiliation>Abe</affiliation></author><author><keyname>Elfadel</keyname><forenames>M.</forenames></author><author><keyname>Daniel</keyname><forenames>Luca</forenames></author></authors><title>Uncertainty Quantification for Integrated Circuits: Stochastic Spectral
  Methods</title><categories>cs.CE cs.NA math.NA</categories><comments>published in Proc. ICCCAD 2013</comments><journal-ref>Int. Conf. Computer-Aided Design, pp. 803-810, San Jose, CA, Nov.
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to significant manufacturing process variations, the performance of
integrated circuits (ICs) has become increasingly uncertain. Such uncertainties
must be carefully quantified with efficient stochastic circuit simulators. This
paper discusses the recent advances of stochastic spectral circuit simulators
based on generalized polynomial chaos (gPC). Such techniques can handle both
Gaussian and non-Gaussian random parameters, showing remarkable speedup over
Monte Carlo for circuits with a small or medium number of parameters. We focus
on the recently developed stochastic testing and the application of
conventional stochastic Galerkin and stochastic collocation schemes to
nonlinear circuit problems. The uncertainty quantification algorithms for
static, transient and periodic steady-state simulations are presented along
with some practical simulation results. Some open problems in this field are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4826</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4826</id><created>2014-09-16</created><authors><author><keyname>Zhang</keyname><forenames>Zheng</forenames><affiliation>Abe</affiliation></author><author><keyname>El-Moselhy</keyname><forenames>Tarek A.</forenames><affiliation>Abe</affiliation></author><author><keyname>Maffezzoni</keyname><forenames>Paolo</forenames><affiliation>Abe</affiliation></author><author><keyname>Ibrahim</keyname><affiliation>Abe</affiliation></author><author><keyname>Elfadel</keyname><forenames>M.</forenames></author><author><keyname>Daniel</keyname><forenames>Luca</forenames></author></authors><title>Efficient Uncertainty Quantification for the Periodic Steady State of
  Forced and Autonomous Circuits</title><categories>cs.CE</categories><comments>Published by IEEE Trans Circuits and Systems II: Express Briefs in
  2013</comments><journal-ref>IEEE Trans. Circuits and Systems II: Express Briefs, vol. 60,
  no.10, pp. 687-691, (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This brief paper proposes an uncertainty quantification method for the
periodic steady-state (PSS) analysis with both Gaussian and non-Gaussian
variations. Our stochastic testing formulation for the PSS problem provides
superior efficiency over both Monte Carlo methods and existing spectral
methods. The numerical implementation of a stochastic shooting Newton solver is
presented for both forced and autonomous circuits. Simulation results on some
analog/RF circuits are reported to show the effectiveness of our proposed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4828</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4828</id><created>2014-09-16</created><authors><author><keyname>Chen</keyname><forenames>Ho-Lin</forenames></author><author><keyname>Doty</keyname><forenames>David</forenames></author><author><keyname>Holden</keyname><forenames>Dhiraj</forenames></author><author><keyname>Thachuk</keyname><forenames>Chris</forenames></author><author><keyname>Woods</keyname><forenames>Damien</forenames></author><author><keyname>Yang</keyname><forenames>Chun-Tao</forenames></author></authors><title>Fast algorithmic self-assembly of simple shapes using random agitation</title><categories>cs.DS cs.CC cs.CG</categories><comments>Conference version at DNA20</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the power of uncontrolled random molecular movement in the nubot
model of self-assembly. The nubot model is an asynchronous nondeterministic
cellular automaton augmented with rigid-body movement rules (push/pull,
deterministically and programmatically applied to specific monomers) and random
agitations (nondeterministically applied to every monomer and direction with
equal probability all of the time). Previous work on the nubot model showed how
to build simple shapes such as lines and squares quickly---in expected time
that is merely logarithmic of their size. These results crucially make use of
the programmable rigid-body movement rule: the ability for a single monomer to
control the movement of a large objects quickly, and only at a time and place
of the programmers' choosing. However, in engineered molecular systems,
molecular motion is largely uncontrolled and fundamentally random. This raises
the question of whether similar results can be achieved in a more restrictive,
and perhaps easier to justify, model where uncontrolled random movements, or
agitations, are happening throughout the self-assembly process and are the only
form of rigid-body movement. We show that this is indeed the case: we give a
polylogarithmic expected time construction for squares using agitation, and a
sublinear expected time construction to build a line. Such results are
impossible in an agitation-free (and movement-free) setting and thus show the
benefits of exploiting uncontrolled random movement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4829</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4829</id><created>2014-09-16</created><authors><author><keyname>Zhang</keyname><forenames>Zheng</forenames><affiliation>Abe</affiliation></author><author><keyname>El-Moselhy</keyname><forenames>Tarek A.</forenames><affiliation>Abe</affiliation></author><author><keyname>Ibrahim</keyname><affiliation>Abe</affiliation></author><author><keyname>Elfadel</keyname><forenames>M.</forenames></author><author><keyname>Daniel</keyname><forenames>Luca</forenames></author></authors><title>Calculation of Generalized Polynomial-Chaos Basis Functions and Gauss
  Quadrature Rules in Hierarchical Uncertainty Quantification</title><categories>cs.CE cs.NA math.NA</categories><comments>Published by IEEE Trans CAD in May 2014</comments><journal-ref>IEEE Trans. Computer-Aided Design of Integrated Circuits and
  Systems, vol. 33, no. 5, pp. 728-740, May 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic spectral methods are efficient techniques for uncertainty
quantification. Recently they have shown excellent performance in the
statistical analysis of integrated circuits. In stochastic spectral methods,
one needs to determine a set of orthonormal polynomials and a proper numerical
quadrature rule. The former are used as the basis functions in a generalized
polynomial chaos expansion. The latter is used to compute the integrals
involved in stochastic spectral methods. Obtaining such information requires
knowing the density function of the random input {\it a-priori}. However,
individual system components are often described by surrogate models rather
than density functions. In order to apply stochastic spectral methods in
hierarchical uncertainty quantification, we first propose to construct
physically consistent closed-form density functions by two monotone
interpolation schemes. Then, by exploiting the special forms of the obtained
density functions, we determine the generalized polynomial-chaos basis
functions and the Gauss quadrature rules that are required by a stochastic
spectral simulator. The effectiveness of our proposed algorithm is verified by
both synthetic and practical circuit examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4831</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4831</id><created>2014-09-16</created><authors><author><keyname>Zhang</keyname><forenames>Zheng</forenames><affiliation>Abe</affiliation></author><author><keyname>El-Moselhy</keyname><forenames>Tarek A.</forenames><affiliation>Abe</affiliation></author><author><keyname>Ibrahim</keyname><affiliation>Abe</affiliation></author><author><keyname>Elfadel</keyname><forenames>M.</forenames></author><author><keyname>Daniel</keyname><forenames>Luca</forenames></author></authors><title>Stochastic Testing Method for Transistor-Level Uncertainty
  Quantification Based on Generalized Polynomial Chaos</title><categories>cs.CE</categories><comments>published by IEEE Trans CAD in Oct 2013</comments><journal-ref>IEEE Trans. Computer-Aided Design of Integrated Circuits and
  Systems, vol. 32, no. 10, pp. 1533-1545, Oct. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncertainties have become a major concern in integrated circuit design. In
order to avoid the huge number of repeated simulations in conventional Monte
Carlo flows, this paper presents an intrusive spectral simulator for
statistical circuit analysis. Our simulator employs the recently developed
generalized polynomial chaos expansion to perform uncertainty quantification of
nonlinear transistor circuits with both Gaussian and non-Gaussian random
parameters. We modify the nonintrusive stochastic collocation (SC) method and
develop an intrusive variant called stochastic testing (ST) method to
accelerate the numerical simulation. Compared with the stochastic Galerkin (SG)
method, the resulting coupled deterministic equations from our proposed ST
method can be solved in a decoupled manner at each time point. At the same
time, ST uses fewer samples and allows more flexible time step size controls
than directly using a nonintrusive SC solver. These two properties make ST more
efficient than SG and than existing SC methods, and more suitable for
time-domain circuit simulation. Simulation results of several digital, analog
and RF circuits are reported. Since our algorithm is based on generic
mathematical models, the proposed ST algorithm can be applied to many other
engineering problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4835</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4835</id><created>2014-09-16</created><authors><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Vijay-Shanker</keyname><forenames>K.</forenames></author></authors><title>Taking into Account the Differences between Actively and Passively
  Acquired Data: The Case of Active Learning with Support Vector Machines for
  Imbalanced Datasets</title><categories>cs.LG cs.CL stat.ML</categories><comments>4 pages, 5 figures; appeared in Proceedings of Human Language
  Technologies: The 2009 Annual Conference of the North American Chapter of the
  Association for Computational Linguistics, Companion Volume: Short Papers,
  pages 137-140, Boulder, Colorado, June 2009. Association for Computational
  Linguistics</comments><acm-class>I.2.6; I.2.7; I.5.1; I.5.4</acm-class><journal-ref>Proceedings of HLT: The 2009 Annual Conference of the North
  American Chapter of the Association for Computational Linguistics, Short
  Papers, pages 137-140, Boulder, Colorado, June 2009. Association for
  Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actively sampled data can have very different characteristics than passively
sampled data. Therefore, it's promising to investigate using different
inference procedures during AL than are used during passive learning (PL). This
general idea is explored in detail for the focused case of AL with
cost-weighted SVMs for imbalanced data, a situation that arises for many HLT
tasks. The key idea behind the proposed InitPA method for addressing imbalance
is to base cost models during AL on an estimate of overall corpus imbalance
computed via a small unbiased sample rather than the imbalance in the labeled
training data, which is the leading method used during PL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4842</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4842</id><created>2014-09-16</created><authors><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Jia</keyname><forenames>Yangqing</forenames></author><author><keyname>Sermanet</keyname><forenames>Pierre</forenames></author><author><keyname>Reed</keyname><forenames>Scott</forenames></author><author><keyname>Anguelov</keyname><forenames>Dragomir</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Vanhoucke</keyname><forenames>Vincent</forenames></author><author><keyname>Rabinovich</keyname><forenames>Andrew</forenames></author></authors><title>Going Deeper with Convolutions</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a deep convolutional neural network architecture codenamed
&quot;Inception&quot;, which was responsible for setting the new state of the art for
classification and detection in the ImageNet Large-Scale Visual Recognition
Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the
improved utilization of the computing resources inside the network. This was
achieved by a carefully crafted design that allows for increasing the depth and
width of the network while keeping the computational budget constant. To
optimize quality, the architectural decisions were based on the Hebbian
principle and the intuition of multi-scale processing. One particular
incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22
layers deep network, the quality of which is assessed in the context of
classification and detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4843</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4843</id><created>2014-09-16</created><updated>2015-04-19</updated><authors><author><keyname>Chen</keyname><forenames>Jiecao</forenames></author><author><keyname>Zhang</keyname><forenames>Qin</forenames></author></authors><title>Improved Algorithms for Distributed Entropy Monitoring</title><categories>cs.DS</categories><comments>19 pages (include reference)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern data management systems often need to deal with massive, dynamic and
inherently distributed data sources. We collect the data using a distributed
network, and at the same time try to maintain a global view of the data at a
central coordinator using a minimal amount of communication. Such applications
have been captured by the distributed monitoring model which has attracted a
lot of attention in recent years. In this paper we investigate the monitoring
of the entropy functions, which are very useful in network monitoring
applications such as detecting distributed denial-of-service attacks. Our
results significantly improve the previous best results by Arackaparambil et
al. [2]. Our technical contribution also includes implementing the celebrated
AMS-sampling method (by Alon et al. [1]) in the distributed monitoring model,
which could be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4845</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4845</id><created>2014-09-16</created><authors><author><keyname>Zeng</keyname><forenames>Li</forenames></author><author><keyname>Liu</keyname><forenames>Renren</forenames></author><author><keyname>Zhang</keyname><forenames>Leo Yu</forenames></author><author><keyname>Liu</keyname><forenames>Yuansheng</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-Wo</forenames></author></authors><title>Cryptanalyzing an image encryption algorithm based on scrambling and
  Veginere cipher</title><categories>cs.CR</categories><comments>11 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, an image encryption algorithm based on scrambling and Vegin`ere
cipher has been proposed. However, it was soon cryptanalyzed by Zhang et al.
using a combination of chosen-plaintext attack and differential attack. This
paper briefly reviews the two attack methods proposed by Zhang et al. and
outlines the mathematical interpretations of them. Based on their work, we
present an improved chosen-plaintext attack to further reduce the number of
chosen-plaintexts required, which is proved to be optimal. Moreover, it is
found that an elaborately designed known-plaintex attack can efficiently
compromise the image cipher under study. This finding is verified by both
mathematical analysis and numerical simulations. The cryptanalyzing techniques
described in this paper may provide some insights for designing secure and
efficient multimedia ciphers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4863</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4863</id><created>2014-09-16</created><authors><author><keyname>Rossi</keyname><forenames>Federico</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>On the fundamental limitations of performance for distributed
  decision-making in robotic networks</title><categories>cs.MA cs.RO</categories><comments>Will be presented at CDC2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies fundamental limitations of performance for distributed
decision-making in robotic networks. The class of decision-making problems we
consider encompasses a number of prototypical problems such as average-based
consensus as well as distributed optimization, leader election, majority
voting, MAX, MIN, and logical formulas. We first propose a formal model for
distributed computation on robotic networks that is based on the concept of I/O
automata and is inspired by the Computer Science literature on distributed
computing clusters. Then, we present a number of bounds on time, message, and
byte complexity, which we use to discuss the relative performance of a number
of approaches for distributed decision-making. From a methodological
standpoint, our work sheds light on the relation between the tools developed by
the Computer Science and Controls communities on the topic of distributed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4883</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4883</id><created>2014-09-17</created><authors><author><keyname>Ridgway</keyname><forenames>James</forenames></author><author><keyname>Stannett</keyname><forenames>Mike</forenames></author></authors><title>Developing a Video Steganography Toolkit</title><categories>cs.MM</categories><comments>14 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although techniques for separate image and audio steganography are widely
known, relatively little has been described concerning the hiding of
information within video streams (&quot;video steganography&quot;). In this paper we
review the current state of the art in this field, and describe the key issues
we have encountered in developing a practical video steganography system. A
supporting video is also available online at
http://www.youtube.com/watch?v=YhnlHmZolRM
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4898</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4898</id><created>2014-09-17</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Khan</keyname><forenames>Gohar Feroz</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>The Generation of Large Networks from Web-of-Science Data</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the 1990s, one of us developed a series of freeware routines
(http://www.leydesdorff.net/indicators) that enable the user to organize
downloads from the Web-of-Science (Thomson Reuters) into a relational database,
and then to export matrices for further analysis in various formats (for
example, for co-author analysis). The basic format of the matrices displays
each document as a case in a row that can be attributed different variables in
the columns. One limitation to this approach was hitherto that relational
databases typically have an upper limit for the number of variables, such as
256 or 1024. In this brief communication, we report on a way to circumvent this
limitation by using txt2Pajek.exe, available as freeware from
http://www.pfeffer.at/txt2pajek/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4899</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4899</id><created>2014-09-17</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>Is the new citation-rank approach P100' in bibliometrics really new?</title><categories>cs.DL</categories><comments>11 pages, 4 figures, 5 tables</comments><journal-ref>Journal of Informetrics 8, 997-1004 (2014)</journal-ref><doi>10.1016/j.joi.2014.10.001</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The percentile-based rating scale P100 describes the citation impact in terms
of the distribution of unique citation values. This approach has recently been
refined by considering also the frequency of papers with the same citation
counts. Here I compare the resulting P100' with P100 for an empirical dataset
and a simple fictitious model dataset. It is shown that P100' is not much
different from standard percentile-based ratings in terms of citation
frequencies. A new indicator P100'' is introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4919</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4919</id><created>2014-09-17</created><authors><author><keyname>Rim</keyname><forenames>Kwangmyong</forenames></author><author><keyname>Choe</keyname><forenames>Yonghua</forenames></author></authors><title>Software Cognitive Complexity Measure Based on Scope of Variables</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define a Mathematical model of program structure.
Mathematical model of program structure defined here provides unified
mathematical treatment of program structure, which reveals that a program is a
large and finite set of embedded binary relations between current statement and
previous ones. Then, a program is considered as a composed listing and a
logical combination of multiple statements according to the certain composing
rules. We also define the Scope Information Complexity Number (SICN) and
present the cognitive complexity based on functional decomposition of software,
including theoretical validation through nine Weyuker's properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4920</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4920</id><created>2014-09-17</created><authors><author><keyname>Yu</keyname><forenames>Jihong</forenames></author><author><keyname>Chen</keyname><forenames>Lin</forenames></author></authors><title>Stability Analysis of Frame Slotted Aloha Protocol</title><categories>cs.IT math.IT</categories><comments>14 pages, submitted to IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frame Slotted Aloha (FSA) protocol has been widely applied in Radio Frequency
Identification (RFID) systems as the de facto standard in tag identification.
However, very limited work has been done on the stability of FSA despite its
fundamental importance both on the theoretical characterisation of FSA
performance and its effective operation in practical systems. In order to
bridge this gap, we devote this paper to investigating the stability properties
of FSA by focusing on two physical layer models of practical importance, the
models with single packet reception and multipacket reception capabilities.
Technically, we model the FSA system backlog as a Markov chain with its states
being backlog size at the beginning of each frame. The objective is to analyze
the ergodicity of the Markov chain and demonstrate its properties in different
regions, particularly the instability region. By employing drift analysis, we
obtain the closed-form conditions for the stability of FSA and show that the
stability region is maximised when the frame length equals the backlog size in
the single packet reception model and when the ratio of the backlog size to
frame length equals in order of magnitude the maximum multipacket reception
capacity in the multipacket reception model. Furthermore, to characterise
system behavior in the instability region, we mathematically demonstrate the
existence of transience of the backlog Markov chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4925</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4925</id><created>2014-09-17</created><updated>2015-01-17</updated><authors><author><keyname>David</keyname><forenames>Cristina</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Lewis</keyname><forenames>Matt</forenames></author></authors><title>Second-Order Propositional Satisfiability</title><categories>cs.LO</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamentally, every static program analyser searches for a proof through a
combination of heuristics providing candidate solutions and a candidate
validation technique. Essentially, the heuristic reduces a second-order problem
to a first-order/propositional one, while the validation is often just a call
to a SAT/SMT solver. This results in a monolithic design of such analyses that
conflates the formulation of the problem with the solving process.
Consequently, any change to the latter causes changes to the whole analysis.
This design is dictated by the state of the art in solver technology. While
SAT/SMT solvers have experienced tremendous progress, there are barely any
second-order solvers. This paper takes a step towards addressing this situation
by proposing a decidable fragment of second-order logic that is still
expressive enough to capture numerous program analysis problems (e.g. safety
proving, bug finding, termination and non-termination proving,
superoptimisation). We refer to the satisfiability problem for this fragment as
Second-Order SAT and show it is NEXPTIME-complete. Finally, we build a decision
procedure for Second-Order SAT based on program synthesis and present
experimental evidence that our approach is tractable for program analysis
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4926</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4926</id><created>2014-09-17</created><updated>2016-02-17</updated><authors><author><keyname>Batselier</keyname><forenames>Kim</forenames></author><author><keyname>Wong</keyname><forenames>Ngai</forenames></author></authors><title>Symmetric Tensor Decomposition by an Iterative Eigendecomposition
  Algorithm</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an iterative algorithm, called the symmetric tensor eigen-rank-one
iterative decomposition (STEROID), for decomposing a symmetric tensor into a
real linear combination of symmetric rank-1 unit-norm outer factors using only
eigendecompositions and least-squares fitting. Originally designed for a
symmetric tensor with an order being a power of two, STEROID is shown to be
applicable to any order through an innovative tensor embedding technique.
Numerical examples demonstrate the high efficiency and accuracy of the proposed
scheme even for large scale problems. Furthermore, we show how STEROID readily
solves a problem in nonlinear block-structured system identification and
nonlinear state-space identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4928</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4928</id><created>2014-09-17</created><authors><author><keyname>Dr&#xe9;meau</keyname><forenames>Ang&#xe9;lique</forenames></author><author><keyname>Sch&#xfc;lke</keyname><forenames>Christophe</forenames></author><author><keyname>Xu</keyname><forenames>Yingying</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Statistical inference with probabilistic graphical models</title><categories>cs.LG stat.ML</categories><comments>Chapter of &quot;Statistical Physics, Optimization, Inference, and
  Message-Passing Algorithms&quot;, Eds.: F. Krzakala, F. Ricci-Tersenghi, L.
  Zdeborova, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University
  Press, to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These are notes from the lecture of Devavrat Shah given at the autumn school
&quot;Statistical Physics, Optimization, Inference, and Message-Passing Algorithms&quot;,
that took place in Les Houches, France from Monday September 30th, 2013, till
Friday October 11th, 2013. The school was organized by Florent Krzakala from
UPMC &amp; ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, Lenka
Zdeborova from CEA Saclay &amp; CNRS, and Riccardo Zecchina from Politecnico
Torino. This lecture of Devavrat Shah (MIT) covers the basics of inference and
learning. It explains how inference problems are represented within structures
known as graphical models. The theoretical basis of the belief propagation
algorithm is then explained and derived. This lecture sets the stage for
generalizations and applications of message passing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4935</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4935</id><created>2014-09-17</created><authors><author><keyname>Goyal</keyname><forenames>Prachi</forenames></author><author><keyname>Misra</keyname><forenames>Pranabendu</forenames></author><author><keyname>Panolan</keyname><forenames>Fahad</forenames></author><author><keyname>Philip</keyname><forenames>Geevarghese</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Finding Even Subgraphs Even Faster</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problems of the following kind have been the focus of much recent research in
the realm of parameterized complexity: Given an input graph (digraph) on $n$
vertices and a positive integer parameter $k$, find if there exist $k$ edges
(arcs) whose deletion results in a graph that satisfies some specified parity
constraints. In particular, when the objective is to obtain a connected graph
in which all the vertices have even degrees---where the resulting graph is
\emph{Eulerian}---the problem is called Undirected Eulerian Edge Deletion. The
corresponding problem in digraphs where the resulting graph should be strongly
connected and every vertex should have the same in-degree as its out-degree is
called Directed Eulerian Edge Deletion. Cygan et al. [\emph{Algorithmica,
2014}] showed that these problems are fixed parameter tractable (FPT), and gave
algorithms with the running time $2^{O(k \log k)}n^{O(1)}$. They also asked, as
an open problem, whether there exist FPT algorithms which solve these problems
in time $2^{O(k)}n^{O(1)}$. In this paper we answer their question in the
affirmative: using the technique of computing \emph{representative families of
co-graphic matroids} we design algorithms which solve these problems in time
$2^{O(k)}n^{O(1)}$. The crucial insight we bring to these problems is to view
the solution as an independent set of a co-graphic matroid. We believe that
this view-point/approach will be useful in other problems where one of the
constraints that need to be satisfied is that of connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4936</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4936</id><created>2014-09-17</created><authors><author><keyname>Bagnall</keyname><forenames>Anthony</forenames></author><author><keyname>Younsi</keyname><forenames>Reda</forenames></author></authors><title>Ensembles of Random Sphere Cover Classifiers</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and evaluate alternative ensemble schemes for a new instance based
learning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fuses
instances into spheres, then bases classification on distance to spheres rather
than distance to instances. The randomised nature of RSC makes it ideal for use
in ensembles. We propose two ensemble methods tailored to the RSC classifier;
$\alpha \beta$RSE, an ensemble based on instance resampling and $\alpha$RSSE, a
subspace ensemble. We compare $\alpha \beta$RSE and $\alpha$RSSE to tree based
ensembles on a set of UCI datasets and demonstrates that RSC ensembles perform
significantly better than some of these ensembles, and not significantly worse
than the others. We demonstrate via a case study on six gene expression data
sets that $\alpha$RSSE can outperform other subspace ensemble methods on high
dimensional data when used in conjunction with an attribute filter. Finally, we
perform a set of Bias/Variance decomposition experiments to analyse the source
of improvement in comparison to a base classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4955</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4955</id><created>2014-09-17</created><authors><author><keyname>Hwang</keyname><forenames>Hsien-Kuei</forenames></author><author><keyname>Panholzer</keyname><forenames>Alois</forenames></author><author><keyname>Rolin</keyname><forenames>Nicolas</forenames></author><author><keyname>Tsai</keyname><forenames>Tsung-Hsi</forenames></author><author><keyname>Chen</keyname><forenames>Wei-Mei</forenames></author></authors><title>Probabilistic analysis of the (1+1)-evolutionary algorithm</title><categories>math.PR cs.DS</categories><comments>53 pages with 8 figures and 4 appendices</comments><msc-class>60C05, 68W40 (Primary), 60F06, 65Q30 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a detailed analysis of the cost used by the (1+1)-evolutionary
algorithm. The problem has been approached in the evolutionary algorithm
literature under various views, formulation and degree of rigor. Our asymptotic
approximations for the mean and the variance represent the strongest of their
kind. The approach we develop is also applicable to characterize the limit laws
and is based on asymptotic resolution of the underlying recurrence. While most
approximations have their simple formal nature, we elaborate on the delicate
error analysis required for rigorous justifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4958</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4958</id><created>2014-09-17</created><authors><author><keyname>Wang</keyname><forenames>Yi</forenames></author></authors><title>Tensity Research Based on the Information of Eye Movement</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User's mental state is concerned gradually, during the interaction course of
human robot. As the measurement and identification method of psychological
state, tension, has certain practical significance role. At presents there is
no suitable method of measuring the tension. Firstly, sum up some availability
of eye movement index. And then parameters extraction on eye movement
characteristics of normal illumination is studied, including the location of
the face, eyes location, access to the pupil diameter, the eye pupil center
characteristic parameters. And with the judgment of the tension in eye images,
extract exact information of gaze direction. Finally, through the experiment to
prove the proposed method is effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4972</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4972</id><created>2014-09-17</created><authors><author><keyname>Bhattacharjee</keyname><forenames>Tapomayukh</forenames></author><author><keyname>Rehg</keyname><forenames>James M.</forenames></author><author><keyname>Kemp</keyname><forenames>Charles C.</forenames></author></authors><title>Inferring Object Properties from Incidental Contact with a Tactile
  Sensing Forearm</title><categories>cs.RO</categories><comments>This is the initial submitted version of our journal paper. We
  uploaded it in arXiv so that other people can cite our work while the journal
  review process is going on. It has 12 pages, 18 figures, 3 tables, and 1
  Pseudocode</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whole-arm tactile sensing enables a robot to sense properties of contact
across its entire arm. By using this large sensing area, a robot has the
potential to acquire useful information from incidental contact that occurs
while performing a task. Within this paper, we demonstrate that data-driven
methods can be used to infer mechanical properties of objects from incidental
contact with a robot's forearm. We collected data from a tactile-sensing
forearm as it made contact with various objects during a simple reaching
motion. We then used hidden Markov models (HMMs) to infer two object properties
(rigid vs. soft and fixed vs. movable) based on low-dimensional features of
time-varying tactile sensor data (maximum force, contact area, and contact
motion). A key issue is the extent to which data-driven methods can generalize
to robot actions that differ from those used during training. To investigate
this issue, we developed an idealized mechanical model of a robot with a
compliant joint making contact with an object. This model provides intuition
for the classification problem. We also conducted tests in which we varied the
robot arm's velocity and joint stiffness. We found that, in contrast to our
previous methods [1], multivariate HMMs achieved high cross-validation accuracy
and successfully generalized what they had learned to new robot motions with
distinct velocities and joint stiffnesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4977</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4977</id><created>2014-09-17</created><authors><author><keyname>Ghoshal</keyname><forenames>Pratik</forenames></author><author><keyname>Nasre</keyname><forenames>Meghana</forenames></author><author><keyname>Nimbhorkar</keyname><forenames>Prajakta</forenames></author></authors><title>Rank Maximal Matchings -- Structure and Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G = (A U P, E) be a bipartite graph where A denotes a set of agents, P
denotes a set of posts and ranks on the edges denote preferences of the agents
over posts. A matching M in G is rank-maximal if it matches the maximum number
of applicants to their top-rank post, subject to this, the maximum number of
applicants to their second rank post and so on.
  In this paper, we develop a switching graph characterization of rank-maximal
matchings, which is a useful tool that encodes all rank-maximal matchings in an
instance. The characterization leads to simple and efficient algorithms for
several interesting problems. In particular, we give an efficient algorithm to
compute the set of rank-maximal pairs in an instance. We show that the problem
of counting the number of rank-maximal matchings is #P-Complete and also give
an FPRAS for the problem. Finally, we consider the problem of deciding whether
a rank-maximal matching is popular among all the rank-maximal matchings in a
given instance, and give an efficient algorithm for the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4988</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4988</id><created>2014-09-17</created><authors><author><keyname>Bianchi</keyname><forenames>Filippo Maria</forenames></author><author><keyname>Maiorino</keyname><forenames>Enrico</forenames></author><author><keyname>Livi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Rizzi</keyname><forenames>Antonello</forenames></author><author><keyname>Sadeghian</keyname><forenames>Alireza</forenames></author></authors><title>An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for
  Clusters Mining and Knowledge Discovery</title><categories>cs.LG cs.DC cs.MA</categories><doi>10.1007/s00500-015-1876-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a multi-agent algorithm able to automatically discover relevant
regularities in a given dataset, determining at the same time the set of
configurations of the adopted parametric dissimilarity measure yielding compact
and separated clusters. Each agent operates independently by performing a
Markovian random walk on a suitable weighted graph representation of the input
dataset. Such a weighted graph representation is induced by the specific
parameter configuration of the dissimilarity measure adopted by the agent,
which searches and takes decisions autonomously for one cluster at a time.
Results show that the algorithm is able to discover parameter configurations
that yield a consistent and interpretable collection of clusters. Moreover, we
demonstrate that our algorithm shows comparable performances with other similar
state-of-the-art algorithms when facing specific clustering problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4991</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4991</id><created>2014-09-17</created><updated>2015-02-23</updated><authors><author><keyname>Eikel</keyname><forenames>Martina</forenames></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author><author><keyname>Setzer</keyname><forenames>Alexander</forenames></author></authors><title>RoBuSt: A Crash-Failure-Resistant Distributed Storage System</title><categories>cs.DS cs.DC</categories><comments>Revised full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present the first distributed storage system that is provably
robust against crash failures issued by an adaptive adversary, i.e., for each
batch of requests the adversary can decide based on the entire system state
which servers will be unavailable for that batch of requests. Despite up to
$\gamma n^{1/\log\log n}$ crashed servers, with $\gamma&gt;0$ constant and $n$
denoting the number of servers, our system can correctly process any batch of
lookup and write requests (with at most a polylogarithmic number of requests
issued at each non-crashed server) in at most a polylogarithmic number of
communication rounds, with at most polylogarithmic time and work at each server
and only a logarithmic storage overhead.
  Our system is based on previous work by Eikel and Scheideler (SPAA 2013), who
presented IRIS, a distributed information system that is provably robust
against the same kind of crash failures. However, IRIS is only able to serve
lookup requests. Handling both lookup and write requests has turned out to
require major changes in the design of IRIS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4992</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4992</id><created>2014-09-17</created><authors><author><keyname>Kadir</keyname><forenames>Ashraful</forenames></author><author><keyname>Sandberg</keyname><forenames>Mattias</forenames></author><author><keyname>Szepessy</keyname><forenames>Anders</forenames></author></authors><title>An adaptive mass algorithm for Car-Parrinello and Ehrenfest ab initio
  molecular dynamics</title><categories>physics.chem-ph cs.NA</categories><msc-class>65P10, 81-08, 81Q15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ehrenfest and Car-Parrinello molecular dynamics are computational
alternatives to approximate Born-Oppenheimer molecular dynamics without solving
the electron eigenvalue problem at each time-step. A non-trivial issue is to
choose the artificial electron mass parameter appearing in the Car-Parrinello
method to achieve both good accuracy and high computational efficiency. In this
paper, we propose an algorithm, motivated by the Landau-Zener probability, to
systematically choose an artificial mass dynamically, which makes the
Car-Parrinello and Ehrenfest molecular dynamics methods dependent only on the
problem data. Numerical experiments for simple model problems show that the
time-dependent adaptive artificial mass parameter improves the efficiency of
the Car-Parrinello and Ehrenfest molecular dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4995</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4995</id><created>2014-09-17</created><authors><author><keyname>He</keyname><forenames>Xixi</forenames></author><author><keyname>Li</keyname><forenames>Xirong</forenames></author><author><keyname>Yang</keyname><forenames>Gang</forenames></author><author><keyname>Xu</keyname><forenames>Jieping</forenames></author><author><keyname>Jin</keyname><forenames>Qin</forenames></author></authors><title>Adaptive Tag Selection for Image Annotation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Not all tags are relevant to an image, and the number of relevant tags is
image-dependent. Although many methods have been proposed for image
auto-annotation, the question of how to determine the number of tags to be
selected per image remains open. The main challenge is that for a large tag
vocabulary, there is often a lack of ground truth data for acquiring optimal
cutoff thresholds per tag. In contrast to previous works that pre-specify the
number of tags to be selected, we propose in this paper adaptive tag selection.
The key insight is to divide the vocabulary into two disjoint subsets, namely a
seen set consisting of tags having ground truth available for optimizing their
thresholds and a novel set consisting of tags without any ground truth. Such a
division allows us to estimate how many tags shall be selected from the novel
set according to the tags that have been selected from the seen set. The
effectiveness of the proposed method is justified by our participation in the
ImageCLEF 2014 image annotation task. On a set of 2,065 test images with ground
truth available for 207 tags, the benchmark evaluation shows that compared to
the popular top-$k$ strategy which obtains an F-score of 0.122, adaptive tag
selection achieves a higher F-score of 0.223. Moreover, by treating the
underlying image annotation system as a black box, the new method can be used
as an easy plug-in to boost the performance of existing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5000</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5000</id><created>2014-09-17</created><authors><author><keyname>Fioriti</keyname><forenames>Vincenzo</forenames></author><author><keyname>Chinnici</keyname><forenames>Marta</forenames></author></authors><title>Identifying sparse and dense sub-graphs in large graphs with a fast
  algorithm</title><categories>cs.DS cs.SI</categories><doi>10.1209/0295-5075/108/50006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the nodes of small sub-graphs with no a priori information is a
hard problem. In this work, we want to find each node of a sparse sub-graph
embedded in both dynamic and static background graphs, of larger average
degree. We show that exploiting the summability over several background
realizations of the Estrada-Benzi communicability and the Krylov approximation
of the matrix exponential, it is possible to recover the sub-graph with a fast
algorithm with computational complexity O(N n). Relaxing the problem to
complete sub-graphs, the same performance is obtained with a single background.
The worst case complexity for the single background is O(n log(n)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5015</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5015</id><created>2014-09-17</created><authors><author><keyname>Fatimah</keyname><forenames>Binish</forenames></author><author><keyname>Joshi</keyname><forenames>S. D.</forenames></author></authors><title>Exact Least Squares Algorithm for Signal Matched Multirate Whitening
  Filter Bank: Part I</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define a concept of signal matched multirate whitening
filter bank which provides an optimum coding gain. This is achieved by
whitening the outputs, of the analysis filter bank, within as well as across
the channels, by solving a constrained projection problem. We also present a
fast time and order recursive least squares algorithm to obtain the vector
output of the proposed analysis filter bank. The recursive algorithm, developed
here, gives rise to a lattice-like structure. Since the proposed signal matched
analysis filter bank coefficients are not available directly, an order
recursive algorithm is also presented for estimating these from the lattice
parameters. Simulation results are presented to validate the theory. It is also
observed that the proposed algorithm can be used to whiten
Gaussian/non-Gaussian processes with minimum as well as non-minimum phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5021</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5021</id><created>2014-09-17</created><updated>2015-03-18</updated><authors><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author></authors><title>CryptGraph: Privacy Preserving Graph Analytics on Encrypted Graph</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many graph mining and analysis services have been deployed on the cloud,
which can alleviate users from the burden of implementing and maintaining graph
algorithms. However, putting graph analytics on the cloud can invade users'
privacy. To solve this problem, we propose CryptGraph, which runs graph
analytics on encrypted graph to preserve the privacy of both users' graph data
and the analytic results. In CryptGraph, users encrypt their graphs before
uploading them to the cloud. The cloud runs graph analysis on the encrypted
graphs and obtains results which are also in encrypted form that the cloud
cannot decipher. During the process of computing, the encrypted graphs are
never decrypted on the cloud side. The encrypted results are sent back to users
and users perform the decryption to obtain the plaintext results. In this
process, users' graphs and the analytics results are both encrypted and the
cloud knows neither of them. Thereby, users' privacy can be strongly protected.
Meanwhile, with the help of homomorphic encryption, the results analyzed from
the encrypted graphs are guaranteed to be correct. In this paper, we present
how to encrypt a graph using homomorphic encryption and how to query the
structure of an encrypted graph by computing polynomials. To solve the problem
that certain operations are not executable on encrypted graphs, we propose hard
computation outsourcing to seek help from users. Using two graph algorithms as
examples, we show how to apply our methods to perform analytics on encrypted
graphs. Experiments on two datasets demonstrate the correctness and feasibility
of our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5022</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5022</id><created>2014-09-17</created><updated>2014-12-03</updated><authors><author><keyname>De Boer</keyname><forenames>Frank</forenames><affiliation>CWI</affiliation></author><author><keyname>Jaghoori</keyname><forenames>Mahdi</forenames><affiliation>Leiden University</affiliation></author><author><keyname>Laneve</keyname><forenames>Cosimo</forenames><affiliation>University of Bologna</affiliation></author><author><keyname>Zavattaro</keyname><forenames>Gianluigi</forenames><affiliation>University of Bologna</affiliation></author></authors><title>Decidability Problems for Actor Systems</title><categories>cs.PL cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  4, 2014) lmcs:1091</journal-ref><doi>10.2168/LMCS-10(4:5)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a nominal actor-based language and study its expressive power.
We have identified the presence/absence of fields as a crucial feature: the
dynamic creation of names in combination with fields gives rise to Turing
completeness. On the other hand, restricting to stateless actors gives rise to
systems for which properties such as termination are decidable. This
decidability result still holds for actors with states when the number of
actors is bounded and the state is read-only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5024</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5024</id><created>2014-08-31</created><authors><author><keyname>Seth</keyname><forenames>Agrima</forenames></author><author><keyname>Mishra</keyname><forenames>Deepak</forenames></author></authors><title>Comparative Study of Geometric and Image Based Modelling and Rendering
  Techniques</title><categories>cs.GR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a comparative study of the traditional 3D computer graphics technique
of geometric modelling and image-based rendering techniques that were surveyed
and implemented.We have discussed the classifications and representative
methods of both the techniques. The study has shown that there is a strong
continuum between both the techniques and a hybrid of the two is most suitable
for further implementations.This hybridisation study is underway to create
models of real life situations and provide disaster management training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5034</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5034</id><created>2014-09-17</created><authors><author><keyname>Zaidi</keyname><forenames>Faraz</forenames></author><author><keyname>Muelder</keyname><forenames>Chris</forenames></author><author><keyname>Sallaberry</keyname><forenames>Arnaud</forenames></author></authors><title>Analysis and Visualization of Dynamic Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Book chapter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter provides an overview of the different techniques and methods
that exist for the analysis and visualization of dynamic networks. Basic
definitions and formal notations are discussed and important references are
cited.
  A major reason for the popularity of the field of dynamic networks is its
applicability in a number of diverse fields. The field of dynamic networks is
in its infancy and there are so many avenues that need to be explored. From
developing network generation models to developing temporal metrics and
measures, from structural analysis to visual analysis, there is room for
further exploration in almost every dimension where dynamic networks are
studied. Recently, with the availability of dynamic data from various fields,
the empirical study and experimentation with real data sets has also helped
maturate the field. Furthermore, researchers have started to develop
foundations and theories based on these datasets which in turn has resulted
lots of activity among research communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5040</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5040</id><created>2014-09-17</created><authors><author><keyname>Gilbert</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Simonetto</keyname><forenames>Paolo</forenames></author><author><keyname>Zaidi</keyname><forenames>Faraz</forenames></author><author><keyname>Jourdan</keyname><forenames>Fabien</forenames></author><author><keyname>Bourqui</keyname><forenames>Romain</forenames></author></authors><title>Communities and Hierarchical Structures in Dynamic Social Networks:
  Analysis and Visualization</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Social Network Analysis and Mining, Springer, 2011, 1, 83-95</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Detection of community structures in social networks has attracted lots of
attention in the domain of sociology and behavioral sciences. Social networks
also exhibit dynamic nature as these networks change continuously with the
passage of time. Social networks might also present a hierarchical structure
led by individuals that play important roles in a society such as Managers and
Decision Makers. Detection and Visualization of these networks changing over
time is a challenging problem where communities change as a function of events
taking place in the society and the role people play in it.
  In this paper we address these issues by presenting a system to analyze
dynamic social networks. The proposed system is based on dynamic graph
discretization and graph clustering. The system allows detection of major
structural changes taking place in social communities over time and reveals
hierarchies by identifying influential people in a social networks. We use two
different data sets for the empirical evaluation and observe that our system
helps to discover interesting facts about the social and hierarchical
structures present in these social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5052</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5052</id><created>2014-09-17</created><updated>2014-09-18</updated><authors><author><keyname>Welch</keyname><forenames>Philip</forenames></author></authors><title>Discrete Transfinite Computation</title><categories>math.LO cs.LO</categories><comments>A survey of transfinite computational models to appear as a chapter
  in &quot; Turing's Ideas: their significance and impact &quot;, Eds G. Sommaruga &amp; T.
  Strahm, Birkh\&quot;auser, 2015</comments><msc-class>03E15, 03D75, 03D70, 03D65, 03D60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe various computational models based initially, but not
exclusively, on that of the Turing machine, that are generalized to allow for
transfinitely many computational steps. Variants of such machines are
considered that have longer tapes than the standard model, or that work on
ordinals rather than numbers. We outline the connections between such models
and the older theories of recursion in higher types, generalized recursion
theory, and recursion on ordinals such as $\alpha$-recursion. We conclude that,
in particular, polynomial time computation on $\omega$-strings is well modelled
by several convergent conceptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5054</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5054</id><created>2014-09-16</created><authors><author><keyname>Daham</keyname><forenames>Bnar Faisal</forenames></author><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author><author><keyname>Abdual-Rahman</keyname><forenames>Suha A.</forenames></author></authors><title>TCP Performance for Kurd Messenger Application Using Bio-computing</title><categories>cs.NI</categories><comments>15 pages, 11 figures, 6 Tabels. arXiv admin note: substantial text
  overlap with arXiv:1206.0893</comments><journal-ref>Zanco, Journal of Pure and Applied Sciences, Salahaddin
  University, Hawler, IRAQ, Vol. 21 No. 2, 2009, Pages 123-139</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work was conducted to design, implement, and evaluate a new model of
measuring Transmission Control Protocol (TCP) performance of real time network.
The proposed model Biological Kurd Messenger (BIOKM) has two main goals: First
is to run the model efficiently, second is to obtain high TCP performance via
real time network using bio-computing technique, especially molecular
calculation because it provides wisdom results and it can exploit all
facilities of phylogentic analysis. To measure TCP performance two protocols
were selected Internet Relay Chat Daemon (IRCD) and File Transfer Protocol
(FTP), the BIOKM model consists of two applications Kurd Messenger Server Side
(KMSS) and Kurd Messenger Client Side (KMCS) written in Java programming
language by implementing algorithms of BIOKM Server and Client application. The
paper also includes the implementation of hybridized model algorithm based on
Neighbor-Joining (NJ) method to measure TCP performance, then implementing
algorithm of Little law (steady state) for single server queue as a comparison
with bio-computing algorithm. The results obtained by using bio-computing and
little law techniques show very good performance and the two techniques result
are very close to each other this is because of local implementation. The main
tools which have been used in this work can be divided into software and
hardware tools.
  Keywords: Biological Kurd Messenger (BIOKM), Kurd Messenger Phylogenetic
tree, Hybridized Model, Little Law, TCP Performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5079</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5079</id><created>2014-09-16</created><authors><author><keyname>Ahmed</keyname><forenames>Bilal</forenames></author></authors><title>Predictive Capacity of Meteorological Data - Will it rain tomorrow</title><categories>cs.LG</categories><comments>7 pages, 2 Result Sets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the availability of high precision digital sensors and cheap storage
medium, it is not uncommon to find large amounts of data collected on almost
all measurable attributes, both in nature and man-made habitats. Weather in
particular has been an area of keen interest for researchers to develop more
accurate and reliable prediction models. This paper presents a set of
experiments which involve the use of prevalent machine learning techniques to
build models to predict the day of the week given the weather data for that
particular day i.e. temperature, wind, rain etc., and test their reliability
across four cities in Australia {Brisbane, Adelaide, Perth, Hobart}. The
results provide a comparison of accuracy of these machine learning techniques
and their reliability to predict the day of the week by analysing the weather
data. We then apply the models to predict weather conditions based on the
available data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5089</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5089</id><created>2014-09-16</created><authors><author><keyname>Adje</keyname><forenames>Assal&#xe9;</forenames><affiliation>Toulouse</affiliation></author><author><keyname>Garoche</keyname><forenames>Pierre-Lo&#xef;c</forenames><affiliation>Toulouse</affiliation></author></authors><title>Automatic Synthesis of Piecewise Linear Quadratic Invariants for
  Programs</title><categories>math.OC cs.SY</categories><comments>18p</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the various critical systems that worth to be formally analyzed, a wide
set consists of controllers for dynamical systems. Those programs typically
execute an infinite loop in which simple com putations update internal states
and produce commands to update the system state. Those systems are yet hardly
analyzable by available static analysis method, since, even if performing
mainly linear computations, the computation of a safe set of reachable states
often requires quadratic invariants. In this paper we consider the general
setting of a piecewise affine program; that is a program performing different
affine updates on the system depending on some conditions. This typically
encompasses linear controllers with saturations or controllers with different
behaviors and performances activated on some safety conditions. Our analysis is
inspired by works performed a decade ago by Johansson et al, and Morari et al,
in the control community. We adapted their method focused on the analysis of
stability in continuous-time or discrete-time settings to fit the static
analysis paradigm and the computation of invariants, that is over-approximation
of reachable sets using piecewise quadratic Lyapunov functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5092</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5092</id><created>2014-09-17</created><authors><author><keyname>El-alaouy</keyname><forenames>El-arbi</forenames></author><author><keyname>Rhoulami</keyname><forenames>Khadija</forenames></author></authors><title>Survey Management Web Platform Applied to Morocco Household Survey Panel</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CSPro (Census and Survey Processing System) is a software package used
recently in many large scale surveys for data collection. This software often
used as desktop software has been used for the first time as a web service for
resolving some problems encountered in the first wave of the Morocco Household
Panel Survey (MHSP). The article will outline the Survey Management Web
Platform that has been developed based on web 2.0 technologies for both
integrating the CSPro web service control and centralizing the data files
collection from survey fields.
  Keywords: CSPro, Data collection, Survey Panel, Web Platform, Web Service,
UML
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5099</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5099</id><created>2014-09-16</created><authors><author><keyname>Fatimah</keyname><forenames>Binish</forenames></author><author><keyname>Joshi</keyname><forenames>S. D.</forenames></author></authors><title>Exact Least Squares Algorithm for Signal Matched Synthesis Filter Bank:
  Part II</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the companion paper, we proposed a concept of signal matched whitening
filter bank and developed a time and order recursive, fast least squares
algorithm for the same. Objective of part II of the paper is two fold: first is
to define a concept of signal matched synthesis filter bank, hence combining
definitions of part I and part II we obtain a filter bank matched to a given
signal. We also develop a fast time and order recursive, least squares
algorithm for obtaining the same. The synthesis filters, obtained here,
reconstruct the given signal only and not every signal from the finite energy
signal space (i.e. belonging to L^2(R)), as is usually done. The recursions, so
obtained, result in a lattice-like structure. Since the filter parameters are
not directly available, we also present an order recursive algorithm for the
computation of signal matched synthesis filter bank coefficients from the
lattice parameters. The second objective is to explore the possibility of using
synthesis side for modeling of a given stochastic process. Simulation results
have also been presented to validate the theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5114</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5114</id><created>2014-09-17</created><updated>2014-10-10</updated><authors><author><keyname>Ouyang</keyname><forenames>Shuxin</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy</forenames></author><author><keyname>Song</keyname><forenames>Yi-Zhe</forenames></author><author><keyname>Li</keyname><forenames>Xueming</forenames></author></authors><title>A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and
  Low-resolution</title><categories>cs.CV</categories><comments>survey paper(35 pages)</comments><acm-class>A.1; I.4.9; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous face recognition (HFR) refers to matching face imagery across
different domains. It has received much interest from the research community as
a result of its profound implications in law enforcement. A wide variety of new
invariant features, cross-modality matching models and heterogeneous datasets
being established in recent years. This survey provides a comprehensive review
of established techniques and recent developments in HFR. Moreover, we offer a
detailed account of datasets and benchmarks commonly used for evaluation. We
finish by assessing the state of the field and discussing promising directions
for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5140</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5140</id><created>2014-09-17</created><authors><author><keyname>Liu</keyname><forenames>Xishuo</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author></authors><title>The ADMM penalized decoder for LDPC codes</title><categories>cs.IT math.IT</categories><comments>This work was supported by the National Science Foundation (NSF)
  under Grants CCF-1217058 and by a Natural Sciences and Engineering Research
  Council of Canada (NSERC) Discovery Research Grant. This paper was submitted
  to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear programming (LP) decoding for low-density parity-check (LDPC) codes
proposed by Feldman et al. is shown to have theoretical guarantees in several
regimes and empirically is not observed to suffer from an error floor. However
at low signal-to-noise ratios (SNRs), LP decoding is observed to have worse
error performance than belief propagation (BP) decoding. In this paper, we seek
to improve LP decoding at low SNRs while still achieving good high SNR
performance. We first present a new decoding framework obtained by trying to
solve a non-convex optimization problem using the alternating direction method
of multipliers (ADMM). This non-convex problem is constructed by adding a
penalty term to the LP decoding objective. The goal of the penalty term is to
make &quot;pseudocodewords&quot;, which are the non-integer vertices of the LP relaxation
to which the LP decoder fails, more costly. We name this decoder class the
&quot;ADMM penalized decoder&quot;. In our simulation results, the ADMM penalized decoder
with $\ell_1$ and $\ell_2$ penalties outperforms both BP and LP decoding at all
SNRs. For high SNR regimes where it is infeasible to simulate, we use an
instanton analysis and show that the ADMM penalized decoder has better high SNR
performance than BP decoding. We also develop a reweighted LP decoder using
linear approximations to the objective with an $\ell_1$ penalty. We show that
this decoder has an improved theoretical recovery threshold compared to LP
decoding. In addition, we show that the empirical gain of the reweighted LP
decoder is significant at low SNRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5141</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5141</id><created>2014-09-17</created><updated>2015-07-27</updated><authors><author><keyname>Liu</keyname><forenames>Xishuo</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author></authors><title>ADMM LP decoding of non-binary LDPC codes in $\mathbb{F}_{2^m}$</title><categories>cs.IT math.IT</categories><comments>This work was supported by the National Science Foundation (NSF)
  under Grants CCF-1217058 and by a Natural Sciences and Engineering Research
  Council of Canada (NSERC) Discovery Research Grant. This paper was submitted
  to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop efficient decoders for non-binary low-density
parity-check (LDPC) codes using the alternating direction method of multipliers
(ADMM). We apply ADMM to two decoding problems. The first problem is linear
programming (LP) decoding. In order to develop an efficient algorithm, we focus
on non-binary codes in fields of characteristic two. This allows us to
transform each constraint in $\mathbb{F}_{2^m}$ to a set of constraints in
$\mathbb{F}_{2}$ that has a factor graph representation. Applying ADMM to the
LP decoding problem results in two types of non-trivial sub-routines. The first
type requires us to solve an unconstrained quadratic program. We solve this
problem efficiently by leveraging new results obtained from studying the above
factor graphs. The second type requires Euclidean projection onto polytopes
that are studied in the literature, a projection that can be solved efficiently
using off-the-shelf techniques, which scale linearly in the dimension of the
vector to project. ADMM LP decoding scales linearly with block length, linearly
with check degree, and quadratically with field size. The second problem we
consider is a penalized LP decoding problem. This problem is obtained by
incorporating a penalty term into the LP decoding objective. The purpose of the
penalty term is to make non-integer solutions (pseudocodewords) more expensive
and hence to improve decoding performance. The ADMM algorithm for the penalized
LP problem requires Euclidean projection onto a polytope formed by embedding
the constraints specified by the non-binary single parity-check code, which can
be solved by applying the ADMM technique to the resulting quadratic program.
Empirically, this decoder achieves a much reduced error rate than LP decoding
at low signal-to-noise ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5162</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5162</id><created>2014-09-17</created><updated>2015-03-04</updated><authors><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Limited Feedback Hybrid Precoding for Multi-User Millimeter Wave Systems</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Antenna arrays will be an important ingredient in millimeter wave (mmWave)
cellular systems. A natural application of antenna arrays is simultaneous
transmission to multiple users. Unfortunately, the hardware constraints in
mmWave systems make it difficult to apply conventional lower frequency
multiuser MIMO precoding techniques at mmWave. This paper develops low
complexity hybrid analog/digital precoding for downlink multiuser mmWave
systems. Hybrid precoding involves a combination of analog and digital
processing that is inspired by the power consumption of complete radio
frequency and mixed signal hardware. The proposed algorithm configures hybrid
precoders at the transmitter and analog combiners at multiple receivers with a
small training and feedback overhead. The performance of the proposed algorithm
is analyzed in the large dimensional regime and in single path channels. When
the analog and digital precoding vectors are selected from quantized codebooks,
the rate loss due to the joint quantization is characterized and insights are
given into the performance of hybrid beamforming compared with analog-only
beamforming solutions. Analytical and simulation results show that the proposed
techniques offer higher sum rates compared with analog-only beamforming
solutions, and approach the performance of the unconstrained digital
beamforming with relatively small codebooks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5164</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5164</id><created>2014-09-17</created><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Maceli</keyname><forenames>Peter</forenames></author><author><keyname>Zhong</keyname><forenames>Mingxian</forenames></author></authors><title>Three-coloring graphs with no induced seven-vertex path I : the
  triangle-free case</title><categories>math.CO cs.DM</categories><comments>53 Pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give a polynomial time algorithm which determines if a
given triangle-free graph with no induced seven-vertex path is 3-colorable, and
gives an explicit coloring if one exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5165</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5165</id><created>2014-09-17</created><authors><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Vijay-Shanker</keyname><forenames>K.</forenames></author></authors><title>A Method for Stopping Active Learning Based on Stabilizing Predictions
  and the Need for User-Adjustable Stopping</title><categories>cs.LG cs.CL stat.ML</categories><comments>9 pages, 3 figures, 5 tables; appeared in Proceedings of the
  Thirteenth Conference on Computational Natural Language Learning
  (CoNLL-2009), June 2009</comments><acm-class>I.2.6; I.2.7; I.5.1; I.5.4; G.3</acm-class><journal-ref>In Proceedings of the Thirteenth Conference on Computational
  Natural Language Learning (CoNLL-2009), pages 39-47, Boulder, Colorado, June
  2009. Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A survey of existing methods for stopping active learning (AL) reveals the
needs for methods that are: more widely applicable; more aggressive in saving
annotations; and more stable across changing datasets. A new method for
stopping AL based on stabilizing predictions is presented that addresses these
needs. Furthermore, stopping methods are required to handle a broad range of
different annotation/performance tradeoff valuations. Despite this, the
existing body of work is dominated by conservative methods with little (if any)
attention paid to providing users with control over the behavior of stopping
methods. The proposed method is shown to fill a gap in the level of
aggressiveness available for stopping AL and supports providing users with
control over stopping behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5166</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5166</id><created>2014-09-17</created><authors><author><keyname>Qin</keyname><forenames>Hu</forenames></author><author><keyname>Zhang</keyname><forenames>Zizhen</forenames></author><author><keyname>Xie</keyname><forenames>Yubin</forenames></author><author><keyname>Lim</keyname><forenames>Andrew</forenames></author></authors><title>A Tabu Search Algorithm for the Multi-period Inspector Scheduling
  Problem</title><categories>cs.AI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a multi-period inspector scheduling problem (MPISP),
which is a new variant of the multi-trip vehicle routing problem with time
windows (VRPTW). In the MPISP, each inspector is scheduled to perform a route
in a given multi-period planning horizon. At the end of each period, each
inspector is not required to return to the depot but has to stay at one of the
vertices for recuperation. If the remaining time of the current period is
insufficient for an inspector to travel from his/her current vertex $A$ to a
certain vertex B, he/she can choose either waiting at vertex A until the start
of the next period or traveling to a vertex C that is closer to vertex B.
Therefore, the shortest transit time between any vertex pair is affected by the
length of the period and the departure time. We first describe an approach of
computing the shortest transit time between any pair of vertices with an
arbitrary departure time. To solve the MPISP, we then propose several local
search operators adapted from classical operators for the VRPTW and integrate
them into a tabu search framework. In addition, we present a constrained
knapsack model that is able to produce an upper bound for the problem. Finally,
we evaluate the effectiveness of our algorithm with extensive experiments based
on a set of test instances. Our computational results indicate that our
approach generates high-quality solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5177</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5177</id><created>2014-09-17</created><authors><author><keyname>Noor-A-Rahim</keyname><forenames>Md.</forenames></author><author><keyname>Nguyen</keyname><forenames>Khoa D.</forenames></author><author><keyname>Lechner</keyname><forenames>Gottfried</forenames></author></authors><title>Delay-Exponent of Bilayer Anytime Code</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation in ITW-2014. 5 Pages, 3 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the design and the delay-exponent of anytime codes
over a three terminal relay network. We propose a bilayer anytime code based on
anytime spatially coupled low-density parity-check (LDPC) codes and investigate
the anytime characteristics through density evolution analysis. By using
mathematical induction technique, we find analytical expressions of the
delay-exponent for the proposed code. Through comparison, we show that the
analytical delay-exponent has a close match with the delay-exponent obtained
from numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5181</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5181</id><created>2014-09-17</created><updated>2015-02-06</updated><authors><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author><author><keyname>Pi</keyname><forenames>Zhouyue</forenames></author><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author></authors><title>TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type
  Photoplethysmographic Signals During Intensive Physical Exercise</title><categories>cs.CY</categories><comments>Matlab codes and data are available at:
  https://sites.google.com/site/researchbyzhang/</comments><journal-ref>IEEE Transactions on Biomedical Engineering, vol. 62, no. 2, pp.
  522-531, February 2015</journal-ref><doi>10.1109/TBME.2014.2359372</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heart rate monitoring using wrist-type photoplethysmographic (PPG) signals
during subjects' intensive exercise is a difficult problem, since the signals
are contaminated by extremely strong motion artifacts caused by subjects' hand
movements. So far few works have studied this problem. In this work, a general
framework, termed TROIKA, is proposed, which consists of signal decomposiTion
for denoising, sparse signal RecOnstructIon for high-resolution spectrum
estimation, and spectral peaK trAcking with verification. The TROIKA framework
has high estimation accuracy and is robust to strong motion artifacts. Many
variants can be straightforwardly derived from this framework. Experimental
results on datasets recorded from 12 subjects during fast running at the peak
speed of 15 km/hour showed that the average absolute error of heart rate
estimation was 2.34 beat per minute (BPM), and the Pearson correlation between
the estimates and the ground-truth of heart rate was 0.992. This framework is
of great values to wearable devices such as smart-watches which use PPG signals
to monitor heart rate for fitness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5185</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5185</id><created>2014-09-18</created><updated>2014-09-25</updated><authors><author><keyname>Lee</keyname><forenames>Chen-Yu</forenames></author><author><keyname>Xie</keyname><forenames>Saining</forenames></author><author><keyname>Gallagher</keyname><forenames>Patrick</forenames></author><author><keyname>Zhang</keyname><forenames>Zhengyou</forenames></author><author><keyname>Tu</keyname><forenames>Zhuowen</forenames></author></authors><title>Deeply-Supervised Nets</title><categories>stat.ML cs.LG cs.NE</categories><comments>Patent disclosure, UCSD Docket No. SD2014-313, filed on May 22, 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Our proposed deeply-supervised nets (DSN) method simultaneously minimizes
classification error while making the learning process of hidden layers direct
and transparent. We make an attempt to boost the classification performance by
studying a new formulation in deep networks. Three aspects in convolutional
neural networks (CNN) style architectures are being looked at: (1) transparency
of the intermediate layers to the overall classification; (2)
discriminativeness and robustness of learned features, especially in the early
layers; (3) effectiveness in training due to the presence of the exploding and
vanishing gradients. We introduce &quot;companion objective&quot; to the individual
hidden layers, in addition to the overall objective at the output layer (a
different strategy to layer-wise pre-training). We extend techniques from
stochastic gradient methods to analyze our algorithm. The advantage of our
method is evident and our experimental result on benchmark datasets shows
significant performance gain over existing methods (e.g. all state-of-the-art
results on MNIST, CIFAR-10, CIFAR-100, and SVHN).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5187</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5187</id><created>2014-09-18</created><updated>2015-05-27</updated><authors><author><keyname>Liu</keyname><forenames>Ying</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Do</keyname><forenames>Younghae</forenames></author></authors><title>Core-like groups result in invalidation of identifying super-spreader by
  k-shell decomposition</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the most influential spreaders is an important issue in
understanding and controlling spreading processes on complex networks. Recent
studies showed that nodes located in the core of a network as identified by the
k-shell decomposition are the most influential spreaders. However, through a
great deal of numerical simulations, we observe that not in all real networks
do nodes in high shells are very influential: in some networks the core nodes
are the most influential which we call true core, while in others nodes in high
shells, even the innermost core, are not good spreaders which we call core-like
group. By analyzing the k-core structure of the networks, we find that the true
core of a network links diversely to the shells of the network, while the
core-like group links very locally within the group. For nodes in the core-like
group, the k-shell index cannot reflect their location importance in the
network. We further introduce a measure based on the link diversity of shells
to effectively distinguish the true core and core-like group, and identify
core-like groups throughout the networks. Our findings help to better
understand the structural features of real networks and influential nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5188</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5188</id><created>2014-09-18</created><authors><author><keyname>Wang</keyname><forenames>Ruxin</forenames></author><author><keyname>Han</keyname><forenames>Congying</forenames></author><author><keyname>Wu</keyname><forenames>Yanping</forenames></author><author><keyname>Guo</keyname><forenames>Tiande</forenames></author></authors><title>Fingerprint Classification Based on Depth Neural Network</title><categories>cs.CV</categories><comments>14 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fingerprint classification is an effective technique for reducing the
candidate numbers of fingerprints in the stage of matching in automatic
fingerprint identification system (AFIS). In recent years, deep learning is an
emerging technology which has achieved great success in many fields, such as
image processing, natural language processing and so on. In this paper, we only
choose the orientation field as the input feature and adopt a new method
(stacked sparse autoencoders) based on depth neural network for fingerprint
classification. For the four-class problem, we achieve a classification of 93.1
percent using the depth network structure which has three hidden layers (with
1.8% rejection) in the NIST-DB4 database. And then we propose a novel method
using two classification probabilities for fuzzy classification which can
effectively enhance the accuracy of classification. By only adjusting the
probability threshold, we get the accuracy of classification is 96.1% (setting
threshold is 0.85), 97.2% (setting threshold is 0.90) and 98.0% (setting
threshold is 0.95). Using the fuzzy method, we obtain higher accuracy than
other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5189</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5189</id><created>2014-09-18</created><updated>2015-03-26</updated><authors><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Frank</keyname><forenames>Michael</forenames></author><author><keyname>Itzhakov</keyname><forenames>Avraham</forenames></author><author><keyname>Miller</keyname><forenames>Alice</forenames></author></authors><title>Solving Graph Coloring Problems with Abstraction and Symmetry</title><categories>cs.AI cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a general methodology, based on abstraction and
symmetry, that applies to solve hard graph edge-coloring problems and
demonstrates its use to provide further evidence that the Ramsey number
$R(4,3,3)=30$. The number $R(4,3,3)$ is often presented as the unknown Ramsey
number with the best chances of being found &quot;soon&quot;. Yet, its precise value has
remained unknown for more than 50 years. We illustrate our approach by showing
that: (1) there are precisely 78{,}892 $(3,3,3;13)$ Ramsey colorings; and (2)
if there exists a $(4,3,3;30)$ Ramsey coloring then it is (13,8,8) regular.
Specifically each node has 13 edges in the first color, 8 in the second, and 8
in the third. We conjecture that these two results will help provide a proof
that no $(4,3,3;30)$ Ramsey coloring exists implying that $R(4,3,3)=30$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5200</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5200</id><created>2014-09-18</created><authors><author><keyname>Bhagat</keyname><forenames>Smriti</forenames></author><author><keyname>Kim</keyname><forenames>Anthony</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Weinsberg</keyname><forenames>Udi</forenames></author></authors><title>The Shapley Value in Knapsack Budgeted Games</title><categories>cs.GT</categories><comments>A short version to appear in the 10th Conference on Web and Internet
  Economics (WINE 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the study of computing the Shapley value for a new class of
cooperative games that we call budgeted games, and investigate in particular
knapsack budgeted games, a version modeled after the classical knapsack
problem. In these games, the &quot;value&quot; of a set $S$ of agents is determined only
by a critical subset $T\subseteq S$ of the agents and not the entirety of $S$
due to a budget constraint that limits how large $T$ can be. We show that the
Shapley value can be computed in time faster than by the na\&quot;ive exponential
time algorithm when there are sufficiently many agents, and also provide an
algorithm that approximates the Shapley value within an additive error. For a
related budgeted game associated with a greedy heuristic, we show that the
Shapley value can be computed in pseudo-polynomial time. Furthermore, we
generalize our proof techniques and propose what we term algorithmic
representation framework that captures a broad class of cooperative games with
the property of efficient computation of the Shapley value. The main idea is
that the problem of determining the efficient computation can be reduced to
that of finding an alternative representation of the games and an associated
algorithm for computing the underlying value function with small time and space
complexities in the representation size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5209</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5209</id><created>2014-09-18</created><updated>2015-06-28</updated><authors><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Pedestrian Detection with Spatially Pooled Features and Structured
  Ensemble Learning</title><categories>cs.CV cs.LG</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many typical applications of object detection operate within a prescribed
false-positive range. In this situation the performance of a detector should be
assessed on the basis of the area under the ROC curve over that range, rather
than over the full curve, as the performance outside the range is irrelevant.
This measure is labelled as the partial area under the ROC curve (pAUC). We
propose a novel ensemble learning method which achieves a maximal detection
rate at a user-defined range of false positive rates by directly optimizing the
partial AUC using structured learning.
  In order to achieve a high object detection performance, we propose a new
approach to extract low-level visual features based on spatial pooling.
Incorporating spatial pooling improves the translational invariance and thus
the robustness of the detection process. Experimental results on both synthetic
and real-world data sets demonstrate the effectiveness of our approach, and we
show that it is possible to train state-of-the-art pedestrian detectors using
the proposed structured ensemble learning method with spatially pooled
features. The result is the current best reported performance on the
Caltech-USA pedestrian detection dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5214</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5214</id><created>2014-09-18</created><authors><author><keyname>Gamarnik</keyname><forenames>David</forenames></author><author><keyname>Hemery</keyname><forenames>Mathieu</forenames></author><author><keyname>Hetterich</keyname><forenames>Samuel</forenames></author></authors><title>Local Algorithms for Graphs</title><categories>cond-mat.dis-nn cs.DS</categories><comments>Chapter of &quot;Statistical Physics, Optimization, Inference, and
  Message-Passing Algorithms&quot;, Eds.: F. Krzakala, F. Ricci-Tersenghi, L.
  Zdeborova, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University
  Press, to appear)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We are going to analyze local algorithms over sparse random graphs. These
algorithms are based on local information where local regards to a decision
made by the exploration of a small neighbourhood of a certain vertex plus a
believe of the structure of the whole graph and maybe added some randomness.
This kind of algorithms can be a natural response to the given problem or an
efficient approximation such as the Belief Propagation Algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5223</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5223</id><created>2014-09-18</created><authors><author><keyname>Ruijl</keyname><forenames>Ben</forenames></author><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Vermaseren</keyname><forenames>Jos</forenames></author><author><keyname>Herik</keyname><forenames>Jaap van den</forenames></author></authors><title>Why Local Search Excels in Expression Simplification</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simplifying expressions is important to make numerical integration of large
expressions from High Energy Physics tractable. To this end, Horner's method
can be used. Finding suitable Horner schemes is assumed to be hard, due to the
lack of local heuristics. Recently, MCTS was reported to be able to find near
optimal schemes. However, several parameters had to be fine-tuned manually. In
this work, we investigate the state space properties of Horner schemes and find
that the domain is relatively flat and contains only a few local minima. As a
result, the Horner space is appropriate to be explored by Stochastic Local
Search (SLS), which has only two parameters: the number of iterations
(computation time) and the neighborhood structure. We found a suitable
neighborhood structure, leaving only the allowed computation time as a
parameter. We performed a range of experiments. The results obtained by SLS are
similar or better than those obtained by MCTS. Furthermore, we show that SLS
obtains the good results at least 10 times faster. Using SLS, we can speed up
numerical integration of many real-world large expressions by at least a factor
of 24. For High Energy Physics this means that numerical integrations that took
weeks can now be done in hours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5224</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5224</id><created>2014-09-18</created><authors><author><keyname>Riverso</keyname><forenames>Stefano</forenames></author><author><keyname>Boem</keyname><forenames>Francesca</forenames></author><author><keyname>Ferrari-Trecate</keyname><forenames>Giancarlo</forenames></author><author><keyname>Parisini</keyname><forenames>Thomas</forenames></author></authors><title>Plug-and-play fault diagnosis and control-reconfiguration for a class of
  nonlinear large-scale constrained systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a novel Plug-and-Play (PnP) architecture for the
control and monitoring of Large-Scale Systems (LSSs). The proposed approach
integrates a distributed Model Predictive Control (MPC) strategy with a
distributed Fault Detection (FD) architecture and methodology in a PnP
framework. The basic concept is to use the FD scheme as an autonomous decision
support system: once a fault is detected, the faulty subsystem can be unplugged
to avoid the propagation of the fault in the interconnected LSS. Analogously,
once the issue has been solved, the disconnected subsystem can be
re-plugged-in. PnP design of local controllers and detectors allow these
operations to be performed safely, i.e. without spoiling stability and
constraint satisfaction for the whole LSS. The PnP distributed MPC is derived
for a class of nonlinear LSS and an integrated PnP distributed FD architecture
is proposed. Simulation results show the effectiveness and the potential of the
general methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5230</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5230</id><created>2014-09-18</created><authors><author><keyname>Shi</keyname><forenames>Baoguang</forenames></author><author><keyname>Bai</keyname><forenames>Xiang</forenames></author><author><keyname>Liu</keyname><forenames>Wenyu</forenames></author><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author></authors><title>Deep Regression for Face Alignment</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a deep regression approach for face alignment. The
deep architecture consists of a global layer and multi-stage local layers. We
apply the back-propagation algorithm with the dropout strategy to jointly
optimize the regression parameters. We show that the resulting deep regressor
gradually and evenly approaches the true facial landmarks stage by stage,
avoiding the tendency to yield over-strong early stage regressors while
over-weak later stage regressors. Experimental results show that our approach
achieves the state-of-the-art
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5241</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5241</id><created>2014-09-18</created><updated>2014-10-23</updated><authors><author><keyname>Fernando</keyname><forenames>Basura</forenames></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames></author><author><keyname>Sebban</keyname><forenames>Marc</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Subspace Alignment For Domain Adaptation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new domain adaptation (DA) algorithm where the
source and target domains are represented by subspaces spanned by eigenvectors.
Our method seeks a domain invariant feature space by learning a mapping
function which aligns the source subspace with the target one. We show that the
solution of the corresponding optimization problem can be obtained in a simple
closed form, leading to an extremely fast algorithm. We present two approaches
to determine the only hyper-parameter in our method corresponding to the size
of the subspaces. In the first approach we tune the size of subspaces using a
theoretical bound on the stability of the obtained result. In the second
approach, we use maximum likelihood estimation to determine the subspace size,
which is particularly useful for high dimensional data. Apart from PCA, we
propose a subspace creation method that outperform partial least squares (PLS)
and linear discriminant analysis (LDA) in domain adaptation. We test our method
on various datasets and show that, despite its intrinsic simplicity, it
outperforms state of the art DA methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5253</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5253</id><created>2014-09-18</created><authors><author><keyname>Gemmetto</keyname><forenames>Valerio</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author></authors><title>Multiplexity versus correlation: the role of local constraints in real
  multiplexes</title><categories>physics.soc-ph cs.SI</categories><comments>32 pages, 6 figures</comments><journal-ref>Scientific Reports 5, 9120 (2015)</journal-ref><doi>10.1038/srep09120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several real-world systems can be represented as multi-layer complex
networks, i.e. in terms of a superposition of various graphs, each related to a
different mode of connection between nodes. Hence, the definition of proper
mathematical quantities aiming at capturing the level of complexity of those
systems is required. Various attempts have been made to measure the empirical
dependencies between the layers of a multiplex, for both binary and weighted
networks. In the simplest case, such dependencies are measured via
correlation-based metrics: we show that this is equivalent to the use of
completely homogeneous benchmarks specifying only global constraints, such as
the total number of links in each layer. However, these approaches do not take
into account the heterogeneity in the degree and strength distributions, which
are instead a fundamental feature of real-world multiplexes. In this work, we
compare the observed dependencies between layers with the expected values
obtained from reference models that appropriately control for the observed
heterogeneity in the degree and strength distributions. This leads to novel
multiplexity measures that we test on different datasets, i.e. the
International Trade Network (ITN) and the European Airport Network (EAN). Our
findings confirm that the use of homogeneous benchmarks can lead to misleading
results, and furthermore highlight the important role played by the
distribution of hubs across layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5257</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5257</id><created>2014-09-18</created><authors><author><keyname>Salsano</keyname><forenames>Stefano</forenames></author><author><keyname>Blefari-Melazzi</keyname><forenames>Nicola</forenames></author><author><keyname>Presti</keyname><forenames>Francesco Lo</forenames></author><author><keyname>Siracusano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Ventre</keyname><forenames>Pier Luigi</forenames></author></authors><title>Generalized Virtual Networking: an enabler for Service Centric
  Networking and Network Function Virtualization</title><categories>cs.NI</categories><journal-ref>Networks 2014, 16th International Telecommunications Network
  Strategy and Planning Symposium, 17-19 September 2014, Funchal, Portugal</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the Generalized Virtual Networking (GVN) concept.
GVN provides a framework to influence the routing of packets based on service
level information that is carried in the packets. It is based on a protocol
header inserted between the Network and Transport layers, therefore it can be
seen as a layer 3.5 solution. Technically, GVN is proposed as a new transport
layer protocol in the TCP/IP protocol suite. An IP router that is not GVN
capable will simply process the IP destination address as usual. Similar
concepts have been proposed in other works, and referred to as Service Oriented
Networking, Service Centric Networking, Application Delivery Networking, but
they are now generalized in the proposed GVN framework. In this respect, the
GVN header is a generic container that can be adapted to serve the needs of
arbitrary service level routing solutions. The GVN header can be managed by GVN
capable end-hosts and applications or can be pushed/popped at the edge of a GVN
capable network (like a VLAN tag). In this position paper, we show that
Generalized Virtual Networking is a powerful enabler for SCN (Service Centric
Networking) and NFV (Network Function Virtualization) and how it couples with
the SDN (Software Defined Networking) paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5260</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5260</id><created>2014-09-18</created><authors><author><keyname>Scott</keyname><forenames>Michael James</forenames></author><author><keyname>Ghinea</keyname><forenames>Gheorghita</forenames></author><author><keyname>Hamilton</keyname><forenames>Ian</forenames></author></authors><title>Promoting Inclusive Design Practice at the Global Game Jam: A Pilot
  Evaluation</title><categories>cs.CY</categories><comments>Presented at the 2014 IEEE Frontiers in Education Conference, 12
  Pages, 1 Figure</comments><journal-ref>Frontiers in Education Conference (FIE), 2014 IEEE, 1-4</journal-ref><doi>10.1109/FIE.2014.7044162</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Games are a popular form of entertainment. However, many computer games
present unnecessary barriers to players with sensory, motor and cognitive
impairments. In order to overcome such pitfalls, an awareness of their impact
and a willingness to apply inclusive design practice is often necessary. The
Global Game Jam offers a potential avenue to promote inclusive design practices
to students of game development. As such, this paper evaluates the impact of an
initiative to promote inclusive design practices during the 2014 Global Game
Jam. An attitude questionnaire was distributed to both participants and
non-participants at one event venue. The results indicate that, having enrolled
in the initiative, students' attitudes improved. Furthermore, all attendees
reported they were likely to pursue further learning opportunities and consider
accessibility issues in their future games. This suggests that the Global Game
Jam, and other similar events, present an attractive avenue to promote
inclusive design practice within the context of digital game development.
However, further analysis of submitted games, additional qualitative inquiry
and a large-scale trial are needed to determine impact on practice and to form
recommendations for future events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5276</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5276</id><created>2014-09-18</created><updated>2016-01-21</updated><authors><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Mladen</forenames></author></authors><title>Codes in $ A_n $ Lattices: Geometry of $ B_h $ Sets and Difference Sets</title><categories>math.CO cs.DM cs.IT math.IT</categories><comments>17 pages (single-column), 8 figures. v2: Terminology slightly changed
  (in Theorem 2.1), Remark 2.2 added, several references added. v3: Section 3
  on Bh sequences added, title changed accordingly, minor changes in the rest
  of the paper, references updated. v4: Material reorganized, title changed,
  Section 1.2.2 added, discussion following Thm 2.2 added with an improved
  bound in eq. (2.15)</comments><msc-class>05B10, 11B13, 11B75, 05B45, 94B25, 52C22, 52C07, 52C17, 11H31,
  68P30, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by several communication scenarios, such as the permutation
channel, the $ 0 $-insertion/deletion channel, and $ q $-ary asymmetric
channels, we investigate properties of (linear) codes in $ A_n $ lattices. In
particular, we demonstrate a connection between such codes and notions of
difference sets and $ B_h $ sets in Abelian groups. It is shown that the $ A_n
$ lattice admits a linear perfect code of radius $ 1 $ if and only if there
exists an Abelian planar difference set of cardinality $ n + 1 $. Similarly, a
direct link is given between linear codes of radius $ r $ in the $ A_n $
lattice and $ B_{2r} $ sets of cardinality $ n + 1 $. $ B_{2r+1} $ sets are
also represented geometrically in a slightly different way. Apart from
providing a geometric intuition about $ B_h $ sets, this interpretation enables
simple derivations of bounds on their parameters, which are either equivalent
to, or improve upon the known bounds. In connection to the above, more general
(non-planar) Abelian difference sets and perfect codes of radius $ r $ are also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5282</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5282</id><created>2014-09-18</created><authors><author><keyname>Vickers</keyname><forenames>Paul</forenames></author><author><keyname>Laing</keyname><forenames>Christopher</forenames></author><author><keyname>Debashi</keyname><forenames>Mohamed</forenames></author><author><keyname>Fairfax</keyname><forenames>Tom</forenames></author></authors><title>Sonification Aesthetics and Listening for Network Situational Awareness</title><categories>cs.HC cs.CY</categories><comments>Workshop paper presented at SoniHED --- Conference on Sonification of
  Health and Environmental Data, York, UK, 12 September, 2014</comments><doi>10.13140/2.1.4225.6648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper looks at the problem of using sonification to enable network
administrators to maintaining situational awareness about their network
environment. Network environments generate a lot of data and the need for
continuous monitoring means that sonification systems must be designed in such
a way as to maximise acceptance while minimising annoyance and listener
fatigue. It will be argued that solutions based on the concept of the
soundscape offer an ecological advantage over other sonification designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5287</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5287</id><created>2014-09-18</created><authors><author><keyname>Benamara</keyname><forenames>O.</forenames></author><author><keyname>Merazka</keyname><forenames>F.</forenames></author></authors><title>An improvement of a cryptanalysis algorithm</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present experiments in order to show how some pseudo random
number generators can improve the effectiveness of a statistical cryptanalysis
algorithm. We deduce mainly that a better generator enhance the accuracy of the
cryptanalysis algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5292</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5292</id><created>2014-09-18</created><authors><author><keyname>Zamani</keyname><forenames>Mohammad</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery</forenames></author></authors><title>Minimum-Energy Distributed Filtering</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses the problem of distributed filtering with guaranteed
convergence properties using minimum-energy filtering and $H_\infty$ filtering
methodologies. A linear state space plant model is considered observed by a
network of communicating sensors, in which individual sensor measurements may
lead to an unobservable filtering problem. However, each filter locally shares
estimates, that are subject to disturbances, with its respective neighboring
filters to produce an estimate of the plant state. The minimum-energy strategy
of the proposed local filter leads to a locally optimal time-varying filter
gain facilitating the transient and the asymptotic convergence of the
estimation error, with guaranteed $H_\infty$ performance. The filters are
implementable using only the local measurements and information from the
neighboring filters subject to disturbances. A key idea of the proposed
algorithm is to locally approximate the neighboring estimates, that are not
directly accessible, considering them as disturbance contaminated versions of
the plant state. The proposed algorithm imposes minimal communication load on
the network and is scalable to larger sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5298</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5298</id><created>2014-09-18</created><authors><author><keyname>Zhou</keyname><forenames>Bin</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author><author><keyname>Zhe</keyname><forenames>He</forenames></author></authors><title>Degree-layer theory of network topology</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The network topology can be described by the number of nodes and the
interconnections among them. The degree of a node in a network is the number of
connections it has to other nodes and the degree distribution is the
probability distribution of these degrees over the whole network. Therefore,
the degree is very important structural parameter of network topology. However,
given the number of nodes and the degree of each node in a network, the
topology of the network cannot be determined. Therefore, we propose the
degree-layer theory of network topology to describe deeply the network
topology. First, we propose the concept of degree-tree with the breadth-first
search tree. The degrees of all nodes are layered and have a hierarchical
structure. Second,the degree-layer theory is described in detail. Two new
concepts are defined in the theory. An index is proposed to quantitatively
distinguish the two network topologies. It also can quantitatively measure the
stability of network topology built by a model mechanism. One theorem is given
and proved, furthermore, and one corollary is derived directly from the
theorem. Third, the applications of the degree-layer theory are discussed in
the ER random network, WS small world network and BA scale-free network, and
the influences of the degree distribution on the stability of network topology
are studied in the three networks. In conclusion, the degree-layer theory is
helpful for accurately describing the network topology, and provides a new
starting point for researching the similarity and isomorphism between two
network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5306</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5306</id><created>2014-09-18</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author></authors><title>Qualitative Analysis of Concurrent Mean-payoff Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider concurrent games played by two-players on a finite-state graph,
where in every round the players simultaneously choose a move, and the current
state along with the joint moves determine the successor state. We study a
fundamental objective, namely, mean-payoff objective, where a reward is
associated to each transition, and the goal of player 1 is to maximize the
long-run average of the rewards, and the objective of player 2 is strictly the
opposite. The path constraint for player 1 could be qualitative, i.e., the
mean-payoff is the maximal reward, or arbitrarily close to it; or quantitative,
i.e., a given threshold between the minimal and maximal reward. We consider the
computation of the almost-sure (resp. positive) winning sets, where player 1
can ensure that the path constraint is satisfied with probability 1 (resp.
positive probability). Our main results for qualitative path constraints are as
follows: (1) we establish qualitative determinacy results that show that for
every state either player 1 has a strategy to ensure almost-sure (resp.
positive) winning against all player-2 strategies, or player 2 has a spoiling
strategy to falsify almost-sure (resp. positive) winning against all player-1
strategies; (2) we present optimal strategy complexity results that precisely
characterize the classes of strategies required for almost-sure and positive
winning for both players; and (3) we present quadratic time algorithms to
compute the almost-sure and the positive winning sets, matching the best known
bound of algorithms for much simpler problems (such as reachability
objectives). For quantitative constraints we show that a polynomial time
solution for the almost-sure or the positive winning set would imply a solution
to a long-standing open problem (the value problem for turn-based deterministic
mean-payoff games) that is not known to be solvable in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5308</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5308</id><created>2014-09-18</created><updated>2014-12-01</updated><authors><author><keyname>El-Kebir</keyname><forenames>Mohammed</forenames></author><author><keyname>Klau</keyname><forenames>Gunnar W.</forenames></author></authors><title>Solving the Maximum-Weight Connected Subgraph Problem to Optimality</title><categories>cs.DS</categories><comments>11th DIMACS implementation challenge</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected node-weighted graph, the Maximum-Weight Connected
Subgraph problem (MWCS) is to identify a subset of nodes of maximalsum of
weights that induce a connected subgraph. MWCS is closely related to the
well-studied Prize Collecting Steiner Tree problem and has many applications in
different areas, including computational biology, network design and computer
vision. The problem is NP-hard and even hard to approximate within a constant
factor. In this work we describe an algorithmic scheme for solving MWCS to
provable optimality, which is based on preprocessing rules, new results on
decomposing an instance into its biconnected and triconnected components and a
branch-and-cut approach combined with a primal heuristic. We demonstrate the
performance of our method on the benchmark instances of the 11th DIMACS
implementation challenge consisting of MWCS as well as transformed PCST
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5313</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5313</id><created>2014-09-18</created><updated>2014-09-22</updated><authors><author><keyname>Machens</keyname><forenames>Holger</forenames></author></authors><title>Sandboxing for Software Transactional Memory with Deferred Updates</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Software transactional memory implementations which allow transactions to
work on inconsistent states of shared data, risk to cause application visible
errors such as memory access violations or endless loops. Hence, many
implementations rely on repeated incremental validation of every read of the
transaction to always guarantee for a consistent view of shared data. Because
this eager validation technique generates significant processing costs several
proposals have been published to establish a sandbox for transactions, which
transparently prevents or suppresses those errors and thereby allows to reduce
the frequency of in-flight validations.
  The most comprehensive sandboxing concept of transactions in software
transactional memory based on deferred updates and considering unmanaged
languages, integrates multiple techniques such as signal interposition,
out-of-band validation and static and dynamic instrumentation. The latter
comprises the insertion of a validation barrier in front of every direct write
which addresses the execution stack of the thread and potentially results from
unvalidated reads.
  This paper basically results from a review of this sandboxing approach, which
revealed some improvements for sandboxing on C/C++. Based on knowledge about
the runtime environment and the compiler an error model has been developed to
identify critical paths to application visible errors. This analysis lead to a
concept for stack protection with less frequent validation, an alternative
out-of-band validation technique and revealed additional risks of so-called
waivered regions without instrumentation inside transactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5317</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5317</id><created>2014-09-18</created><authors><author><keyname>MacLean</keyname><forenames>Scott</forenames></author><author><keyname>Labahn</keyname><forenames>George</forenames></author></authors><title>A Bayesian model for recognizing handwritten mathematical expressions</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing handwritten mathematics is a challenging classification problem,
requiring simultaneous identification of all the symbols comprising an input as
well as the complex two-dimensional relationships between symbols and
subexpressions. Because of the ambiguity present in handwritten input, it is
often unrealistic to hope for consistently perfect recognition accuracy. We
present a system which captures all recognizable interpretations of the input
and organizes them in a parse forest from which individual parse trees may be
extracted and reported. If the top-ranked interpretation is incorrect, the user
may request alternates and select the recognition result they desire. The tree
extraction step uses a novel probabilistic tree scoring strategy in which a
Bayesian network is constructed based on the structure of the input, and each
joint variable assignment corresponds to a different parse tree. Parse trees
are then reported in order of decreasing probability. Two accuracy evaluations
demonstrate that the resulting recognition system is more accurate than
previous versions (which used non-probabilistic methods) and other academic
math recognizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5318</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5318</id><created>2014-09-18</created><authors><author><keyname>Fournet</keyname><forenames>Julie</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author></authors><title>Contact patterns among high school students</title><categories>physics.soc-ph cs.SI</categories><comments>Supplementary Information at
  http://s3-eu-west-1.amazonaws.com/files.figshare.com/1677807/File_S1.pdf</comments><journal-ref>PLoS ONE 9(9):e107878 (2014)</journal-ref><doi>10.1371/journal.pone.0107878</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face-to-face contacts between individuals contribute to shape social networks
and play an important role in determining how infectious diseases can spread
within a population. It is thus important to obtain accurate and reliable
descriptions of human contact patterns occurring in various day-to-day life
contexts. Recent technological advances and the development of wearable sensors
able to sense proximity patterns have made it possible to gather data giving
access to time-varying contact networks of individuals in specific
environments. Here we present and analyze two such data sets describing with
high temporal resolution the contact patterns of students in a high school. We
define contact matrices describing the contact patterns between students of
different classes and show the importance of the class structure. We take
advantage of the fact that the two data sets were collected in the same setting
during several days in two successive years to perform a longitudinal analysis
on two very different timescales. We show the high stability of the contact
patterns across days and across years: the statistical distributions of numbers
and durations of contacts are the same in different periods, and we observe a
very high similarity of the contact matrices measured in different days or
different years. The rate of change of the contacts of each individual from one
day to the next is also similar in different years. We discuss the interest of
the present analysis and data sets for various fields, including in social
sciences in order to better understand and model human behavior and
interactions in different contexts, and in epidemiology in order to inform
models describing the spread of infectious diseases and design targeted
containment strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5320</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5320</id><created>2014-09-16</created><updated>2014-12-05</updated><authors><author><keyname>Hao</keyname><forenames>He</forenames></author><author><keyname>Sanandaji</keyname><forenames>Borhan M.</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author><author><keyname>Vincent</keyname><forenames>Tyrone L.</forenames></author></authors><title>Potentials and Economics of Residential Thermal Loads Providing
  Regulation Reserve</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Residential Thermostatically Controlled Loads (TCLs) such as Air Conditioners
(ACs), heat pumps, water heaters, and refrigerators have an enormous thermal
storage potential for providing regulation reserve to the grid. In this paper,
we study the potential resource and economic analysis of TCLs providing
frequency regulation service. In particular, we show that the potential
resource of TCLs in California is more than enough for both current and
predicted near-future regulation requirements for the California power system.
Moreover, we estimate the cost and revenue of TCLs, discuss the qualification
requirements, recommended policy changes, and participation incentive methods,
and compare TCLs with other energy storage technologies. We show that TCLs are
potentially more cost-effective than other energy storage technologies such as
flywheels, Li-ion, advanced lead acid, and Zinc Bromide batteries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5326</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5326</id><created>2014-09-18</created><authors><author><keyname>Tomsett</keyname><forenames>Richard J.</forenames></author><author><keyname>Ainsworth</keyname><forenames>Matt</forenames></author><author><keyname>Thiele</keyname><forenames>Alexander</forenames></author><author><keyname>Sanayei</keyname><forenames>Mehdi</forenames></author><author><keyname>Chen</keyname><forenames>Xing</forenames></author><author><keyname>Gieselmann</keyname><forenames>Alwin</forenames></author><author><keyname>Whittington</keyname><forenames>Miles A.</forenames></author><author><keyname>Cunningham</keyname><forenames>Mark O.</forenames></author><author><keyname>Kaiser</keyname><forenames>Marcus</forenames></author></authors><title>Virtual Electrode Recording Tool for EXtracellular potentials (VERTEX):
  Comparing multi-electrode recordings from simulated and biological mammalian
  cortical tissue</title><categories>q-bio.NC cs.AI cs.NE</categories><comments>appears in Brain Struct Funct 2014</comments><doi>10.1007/s00429-014-0793-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local field potentials (LFPs) sampled with extracellular electrodes are
frequently used as a measure of population neuronal activity. However, relating
such measurements to underlying neuronal behaviour and connectivity is
non-trivial. To help study this link, we developed the Virtual Electrode
Recording Tool for EXtracellular potentials (VERTEX). We first identified a
reduced neuron model that retained the spatial and frequency filtering
characteristics of extracellular potentials from neocortical neurons. We then
developed VERTEX as an easy-to-use Matlab tool for simulating LFPs from large
populations (&gt;100 000 neurons). A VERTEX-based simulation successfully
reproduced features of the LFPs from an in vitro multi-electrode array
recording of macaque neocortical tissue. Our model, with virtual electrodes
placed anywhere in 3D, allows direct comparisons with the in vitro recording
setup. We envisage that VERTEX will stimulate experimentalists, clinicians, and
computational neuroscientists to use models to understand the mechanisms
underlying measured brain dynamics in health and disease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5327</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5327</id><created>2014-09-17</created><authors><author><keyname>Walton</keyname><forenames>N. S.</forenames></author></authors><title>Store-Forward and its implications for Proportional Scheduling</title><categories>cs.NI cs.PF math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Proportional Scheduler was recently proposed as a scheduling algorithm
for multi-hop switch networks. For these networks, the BackPressure scheduler
is the classical benchmark. For networks with fixed routing, the Proportional
Scheduler is maximum stable, myopic and, furthermore, will alleviate certain
scaling issued found in BackPressure for large networks. Nonetheless, the
equilibrium and delay properties of the Proportional Scheduler has not been
fully characterized.
  In this article, we postulate on the equilibrium behaviour of the
Proportional Scheduler though the analysis of an analogous rule called the
Store-Forward allocation. It has been shown that Store-Forward has
asymptotically allocates according to the Proportional Scheduler. Further, for
Store-Forward networks, numerous equilibrium quantities are explicitly
calculable. For FIFO networks under Store-Forward, we calculate the policies
stationary distribution and end-to-end route delay. We discuss network
topologies when the stationary distribution is product-form, a phenomenon which
we call \emph{product form resource pooling}. We extend this product form
notion to independent set scheduling on perfect graphs, where we show that
non-neighbouring queues are statistically independent. Finally, we analyse the
large deviations behaviour of the equilibrium distribution of Store-Forward
networks in order to construct Lyapunov functions for FIFO switch networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5330</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5330</id><created>2014-09-18</created><authors><author><keyname>Fang</keyname><forenames>Jian</forenames></author><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Learning and approximation capability of orthogonal super greedy
  algorithm</title><categories>cs.LG</categories><comments>30 pages,14 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the approximation capability of orthogonal super greedy
algorithms (OSGA) and its applications in supervised learning. OSGA is
concerned with selecting more than one atoms in each iteration step, which, of
course, greatly reduces the computational burden when compared with the
conventional orthogonal greedy algorithm (OGA). We prove that even for function
classes that are not the convex hull of the dictionary, OSGA does not degrade
the approximation capability of OGA provided the dictionary is incoherent.
Based on this, we deduce a tight generalization error bound for OSGA learning.
Our results show that in the realm of supervised learning, OSGA provides a
possibility to further reduce the computational burden of OGA in the premise of
maintaining its prominent generalization capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5340</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5340</id><created>2014-09-18</created><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author></authors><title>Belief revision by examples</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common assumption in belief revision is that the reliability of the
information sources is either given, derived from temporal information, or the
same for all. This article does not describe a new semantics for integration
but the problem of obtaining the reliability of the sources given the result of
a previous merging. As an example, the relative reliability of two sensors can
be assessed given some certain observation, and allows for subsequent mergings
of data coming from them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5366</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5366</id><created>2014-09-18</created><authors><author><keyname>Cord-Landwehr</keyname><forenames>Andreas</forenames></author><author><keyname>M&#xe4;cker</keyname><forenames>Alexander</forenames></author><author><keyname>der Heide</keyname><forenames>Friedhelm Meyer auf</forenames></author></authors><title>Quality of Service in Network Creation Games</title><categories>cs.GT</categories><comments>An extended abstract of this paper has been accepted for publication
  in the proceedings of the 10th International Conference on Web and Internet
  Economics (WINE)</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network creation games model the creation and usage costs of networks formed
by n selfish nodes. Each node v can buy a set of edges, each for a fixed price
\alpha &gt; 0. Its goal is to minimize its private costs, i.e., the sum (SUM-game,
Fabrikant et al., PODC 2003) or maximum (MAX-game, Demaine et al., PODC 2007)
of distances from $v$ to all other nodes plus the prices of the bought edges.
The above papers show the existence of Nash equilibria as well as upper and
lower bounds for the prices of anarchy and stability. In several subsequent
papers, these bounds were improved for a wide range of prices \alpha. In this
paper, we extend these models by incorporating quality-of-service aspects: Each
edge cannot only be bought at a fixed quality (edge length one) for a fixed
price \alpha. Instead, we assume that quality levels (i.e., edge lengths) are
varying in a fixed interval [\beta,B], 0 &lt; \beta &lt;= B. A node now cannot only
choose which edge to buy, but can also choose its quality x, for the price
p(x), for a given price function p. For both games and all price functions, we
show that Nash equilibria exist and that the price of stability is either
constant or depends only on the interval size of available edge lengths. Our
main results are bounds for the price of anarchy. In case of the SUM-game, we
show that they are tight if price functions decrease sufficiently fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5368</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5368</id><created>2014-09-18</created><authors><author><keyname>Courcelle</keyname><forenames>Bruno</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Durand</keyname><forenames>Ir&#xe8;ne A.</forenames><affiliation>LaBRI</affiliation></author></authors><title>Fly-automata, model-checking and recognizability</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Recognizability Theorem states that if a set of finite graphs is
definable by a monadic second-order (MSO) sentence, then it is recognizable
with respect to the graph algebra upon which the definition of clique-width is
based. Recognizability is an algebraic notion, defined in terms of congruences
that can also be formulated by means of finite automata on the terms that
describe the considered graphs. This theorem entails that the verification of
MSO graph properties, or equivalently, the model-checking problem for MSO logic
over finite binary relational structures, is fixed-parameter tractable (FPT)
for the parameter consisting of the formula that expresses the property and the
clique-width (or the tree-width) of the input graph or structure. The
corresponding algorithms can be implemented by means of fly-automata whose
transitions are computed on the fly and not tabulated. We review two versions
of recognizability, we present fly-automata by means of examples showing that
they can also compute values attached to graphs. We show that fly-automata with
infinite sets of states yield a simple proof of the strong version of the
Recognizability Theorem. This proof has not been published previously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5370</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5370</id><created>2014-09-18</created><updated>2015-01-12</updated><authors><author><keyname>Gluskin</keyname><forenames>Emanuel</forenames></author></authors><title>On the physical and circuit-theoretic significance of the Memristor</title><categories>cs.ET</categories><comments>9 pages/ The present version is strongly extended in the sense of the
  circuit theory discussion</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is noticed that the inductive and capacitive features of the memristor
reflect (and are a quintessence of) such features of any resistor. The very
presence in the resistive characteristic v = f(i) of the voltage and current
state variables, associated by their electrodynamics sense with electrical and
magnetic fields, forces any resister to cause to accumulate some magnetic and
electrostatic fields and energies around itself. The present version is
strongly extended in the sense of the circuit theory discussion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5383</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5383</id><created>2014-09-18</created><authors><author><keyname>Abshoff</keyname><forenames>Sebastian</forenames></author><author><keyname>Cord-Landwehr</keyname><forenames>Andreas</forenames></author><author><keyname>Jung</keyname><forenames>Daniel</forenames></author><author><keyname>Skopalik</keyname><forenames>Alexander</forenames></author></authors><title>Multilevel Network Games</title><categories>cs.GT</categories><comments>An extended abstract of this paper has been accepted for publication
  in the proceedings of the 10th International Conference on Web and Internet
  Economics (WINE)</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multilevel network game, where nodes can improve their
communication costs by connecting to a high-speed network. The $n$ nodes are
connected by a static network and each node can decide individually to become a
gateway to the high-speed network. The goal of a node $v$ is to minimize its
private costs, i.e., the sum (SUM-game) or maximum (MAX-game) of communication
distances from $v$ to all other nodes plus a fixed price $\alpha &gt; 0$ if it
decides to be a gateway. Between gateways the communication distance is $0$,
and gateways also improve other nodes' distances by behaving as shortcuts. For
the SUM-game, we show that for $\alpha \leq n-1$, the price of anarchy is
$\Theta(n/\sqrt{\alpha})$ and in this range equilibria always exist. In range
$\alpha \in (n-1,n(n-1))$ the price of anarchy is $\Theta(\sqrt{\alpha})$, and
for $\alpha \geq n(n-1)$ it is constant. For the MAX-game, we show that the
price of anarchy is either $\Theta(1 + n/\sqrt{\alpha})$, for $\alpha\geq 1$,
or else $1$. Given a graph with girth of at least $4\alpha$, equilibria always
exist. Concerning the dynamics, both the SUM-game and the MAX-game are not
potential games. For the SUM-game, we even show that it is not weakly acyclic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5400</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5400</id><created>2014-09-18</created><authors><author><keyname>Weyand</keyname><forenames>Tobias</forenames></author><author><keyname>Leibe</keyname><forenames>Bastian</forenames></author></authors><title>Visual Landmark Recognition from Internet Photo Collections: A
  Large-Scale Evaluation</title><categories>cs.CV</categories><doi>10.1016/j.cviu.2015.02.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of a visual landmark recognition system is to identify photographed
buildings or objects in query photos and to provide the user with relevant
information on them. With their increasing coverage of the world's landmark
buildings and objects, Internet photo collections are now being used as a
source for building such systems in a fully automatic fashion. This process
typically consists of three steps: clustering large amounts of images by the
objects they depict; determining object names from user-provided tags; and
building a robust, compact, and efficient recognition index. To this date,
however, there is little empirical information on how well current approaches
for those steps perform in a large-scale open-set mining and recognition task.
Furthermore, there is little empirical information on how recognition
performance varies for different types of landmark objects and where there is
still potential for improvement. With this paper, we intend to fill these gaps.
Using a dataset of 500k images from Paris, we analyze each component of the
landmark recognition pipeline in order to answer the following questions: How
many and what kinds of objects can be discovered automatically? How can we best
use the resulting image clusters to recognize the object in a query? How can
the object be efficiently represented in memory for recognition? How reliably
can semantic information be extracted? And finally: What are the limiting
factors in the resulting pipeline from query to semantics? We evaluate how
different choices of methods and parameters for the individual pipeline steps
affect overall system performance and examine their effects for different query
categories such as buildings, paintings or sculptures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5402</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5402</id><created>2014-09-18</created><authors><author><keyname>Zhao</keyname><forenames>Huasha</forenames></author><author><keyname>Jiang</keyname><forenames>Biye</forenames></author><author><keyname>Canny</keyname><forenames>John</forenames></author></authors><title>SAME but Different: Fast and High-Quality Gibbs Parameter Estimation</title><categories>cs.LG stat.ML</categories><comments>10 pages, 5 figures</comments><acm-class>K.3.2; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gibbs sampling is a workhorse for Bayesian inference but has several
limitations when used for parameter estimation, and is often much slower than
non-sampling inference methods. SAME (State Augmentation for Marginal
Estimation) \cite{Doucet99,Doucet02} is an approach to MAP parameter estimation
which gives improved parameter estimates over direct Gibbs sampling. SAME can
be viewed as cooling the posterior parameter distribution and allows annealed
search for the MAP parameters, often yielding very high quality (lower loss)
estimates. But it does so at the expense of additional samples per iteration
and generally slower performance. On the other hand, SAME dramatically
increases the parallelism in the sampling schedule, and is an excellent match
for modern (SIMD) hardware. In this paper we explore the application of SAME to
graphical model inference on modern hardware. We show that combining SAME with
factored sample representation (or approximation) gives throughput competitive
with the fastest symbolic methods, but with potentially better quality. We
describe experiments on Latent Dirichlet Allocation, achieving speeds similar
to the fastest reported methods (online Variational Bayes) and lower
cross-validated loss than other LDA implementations. The method is simple to
implement and should be applicable to many other models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5403</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5403</id><created>2014-09-18</created><updated>2014-10-01</updated><authors><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Iandola</keyname><forenames>Forrest</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Deformable Part Models are Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deformable part models (DPMs) and convolutional neural networks (CNNs) are
two widely used tools for visual recognition. They are typically viewed as
distinct approaches: DPMs are graphical models (Markov random fields), while
CNNs are &quot;black-box&quot; non-linear classifiers. In this paper, we show that a DPM
can be formulated as a CNN, thus providing a novel synthesis of the two ideas.
Our construction involves unrolling the DPM inference algorithm and mapping
each step to an equivalent (and at times novel) CNN layer. From this
perspective, it becomes natural to replace the standard image features used in
DPM with a learned feature extractor. We call the resulting model DeepPyramid
DPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantly
outperforms DPMs based on histograms of oriented gradients features (HOG) and
slightly outperforms a comparable version of the recently introduced R-CNN
detection system, while running an order of magnitude faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5414</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5414</id><created>2014-09-18</created><updated>2015-04-18</updated><authors><author><keyname>Upadhyay</keyname><forenames>Jalaj</forenames></author></authors><title>Differentially Private Linear Algebra in the Streaming Model</title><categories>cs.DS cs.CR</categories><comments>25 pages (comments are welcome); corrected few typos and included a
  more detailed proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical linear algebra plays an important role in computer science. In this
paper, we initiate the study of performing linear algebraic tasks while
preserving privacy when the data is streamed online. Our main focus is the
space requirement of the privacy-preserving data-structures. We give the first
{\em sketch-based} algorithm for differential privacy. We give optimal, up to
logarithmic factor, space data-structures that can compute low rank
approximation, linear regression, and matrix multiplication, while preserving
differential privacy with better additive error bounds compared to the known
results. Notably, we match the best known space bound in the non-private
setting by Kane and Nelson (J. ACM, 61(1):4).
  Our mechanism for differentially private low-rank approximation {\em reuses}
the random Gaussian matrix in a specific way to provide a single-pass
mechanism. We prove that the resulting distribution also preserve differential
privacy. This can be of independent interest. We do not make any assumptions,
like singular value separation or normalized row assumption, as made in the
earlier works. The mechanisms for matrix multiplication and linear regression
can be seen as the private analogues of the known non-private algorithms. All
our mechanisms, in the form presented, can also be computed in the distributed
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5429</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5429</id><created>2014-09-18</created><updated>2014-11-19</updated><authors><author><keyname>Lichtman</keyname><forenames>Marc</forenames></author></authors><title>Antifragile Electronic Warfare</title><categories>cs.NI</categories><comments>3 pages 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter introduces the concept of antifragile electronic warfare (EW),
which we define as the ability to allow a communications link to improve
performance due to the presence of a jammer. This concept should not be
confused with jamming countermeasures (a.k.a. anti-jamming or electronic
protection). Rather, antifragile EW can be thought of as the next step beyond
simply avoiding or mitigating jamming. After introducing the concept we narrow
down the subset of jammers this concept can be applied to, and provide a brief
example of an antifragile EW strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5443</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5443</id><created>2014-09-18</created><updated>2014-09-27</updated><authors><author><keyname>Kolias</keyname><forenames>Vasilis</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Ioannis</forenames></author><author><keyname>Kayafas</keyname><forenames>Eleftherios</forenames></author></authors><title>Exploratory Analysis of a Terabyte Scale Web Corpus</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a preliminary analysis over the largest publicly
accessible web dataset: the Common Crawl Corpus. We measure nine web
characteristics from two levels of granularity using MapReduce and we comment
on the initial observations over a fraction of it. To the best of our knowledge
two of the characteristics, the language distribution and the HTML version of
pages have not been analyzed in previous work, while the specific dataset has
been only analyzed on page level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5452</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5452</id><created>2014-09-18</created><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Devanny</keyname><forenames>William E.</forenames></author><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Simons</keyname><forenames>Joseph A.</forenames></author><author><keyname>Trott</keyname><forenames>Lowell</forenames></author></authors><title>Windows into Geometric Events: Data Structures for Time-Windowed
  Querying of Temporal Point Sets</title><categories>cs.DS</categories><comments>CCCG 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study geometric data structures for sets of point-based temporal events,
answering time-windowed queries, i.e., given a contiguous time interval we
answer common geometric queries about the point events with time stamps in this
interval. The geometric queries we consider include queries based on the
skyline, convex hull, and proximity relations of the point set. We provide
space efficient data structures which answer queries in polylogarithmic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5462</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5462</id><created>2014-09-18</created><authors><author><keyname>Zhou</keyname><forenames>Wei</forenames></author><author><keyname>Labahn</keyname><forenames>George</forenames></author></authors><title>Fast and deterministic computation of the determinant of a polynomial
  matrix</title><categories>cs.SC</categories><comments>10 pages</comments><msc-class>15A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a square, nonsingular matrix of univariate polynomials
$\mathbf{F}\in\mathbb{K}[x]^{n\times n}$ over a field $\mathbb{K}$, we give a
deterministic algorithm for finding the determinant of $\mathbf{F}$. The
complexity of the algorithm is $\bigO \left(n^{\omega}s\right)$ field
operations where $s$ is the average column degree or the average row degree of
$\mathbf{F}$. Here $\bigO$ notation is Big-$O$ with log factors omitted and
$\omega$ is the exponent of matrix multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5466</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5466</id><created>2014-09-18</created><authors><author><keyname>Biniaz</keyname><forenames>Ahmad</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Higher-Order Triangular-Distance Delaunay Graphs: Graph-Theoretical
  Properties</title><categories>cs.CG</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an extension of the triangular-distance Delaunay graphs
(TD-Delaunay) on a set $P$ of points in the plane. In TD-Delaunay, the convex
distance is defined by a fixed-oriented equilateral triangle $\triangledown$,
and there is an edge between two points in $P$ if and only if there is an empty
homothet of $\triangledown$ having the two points on its boundary. We consider
higher-order triangular-distance Delaunay graphs, namely $k$-TD, which contains
an edge between two points if the interior of the homothet of $\triangledown$
having the two points on its boundary contains at most $k$ points of $P$. We
consider the connectivity, Hamiltonicity and perfect-matching admissibility of
$k$-TD. Finally we consider the problem of blocking the edges of $k$-TD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5474</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5474</id><created>2014-09-18</created><authors><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames></author><author><keyname>B&#xe9;dorf</keyname><forenames>Jeroen</forenames></author></authors><title>Computational Gravitational Dynamics with Modern Numerical Accelerators</title><categories>astro-ph.IM astro-ph.GA cs.OH</categories><comments>Accepted for publication in IEEE Computer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the recent optimizations of gravitational $N$-body kernels for
running them on graphics processing units (GPUs), on single hosts and massive
parallel platforms. For each of the two main $N$-body techniques, direct
summation and tree-codes, we discuss the optimization strategy, which is
different for each algorithm. Because both the accuracy as well as the
performance characteristics differ, hybridizing the two algorithms is essential
when simulating a large $N$-body system with high-density structures containing
few particles, and with low-density structures containing many particles. We
demonstrate how this can be realized by splitting the underlying Hamiltonian,
and we subsequently demonstrate the efficiency and accuracy of the hybrid code
by simulating a group of 11 merging galaxies with massive black holes in the
nuclei.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5486</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5486</id><created>2014-09-18</created><authors><author><keyname>Sadigh</keyname><forenames>Dorsa</forenames></author><author><keyname>Kim</keyname><forenames>Eric S.</forenames></author><author><keyname>Coogan</keyname><forenames>Samuel</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>A Learning Based Approach to Control Synthesis of Markov Decision
  Processes for Linear Temporal Logic Specifications</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to synthesize a control policy for a Markov decision process (MDP)
such that the resulting traces of the MDP satisfy a linear temporal logic (LTL)
property. We construct a product MDP that incorporates a deterministic Rabin
automaton generated from the desired LTL property. The reward function of the
product MDP is defined from the acceptance condition of the Rabin automaton.
This construction allows us to apply techniques from learning theory to the
problem of synthesis for LTL specifications even when the transition
probabilities are not known a priori. We prove that our method is guaranteed to
find a controller that satisfies the LTL property with probability one if such
a policy exists, and we suggest empirically with a case study in traffic
control that our method produces reasonable control strategies even when the
LTL property cannot be satisfied with probability one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5491</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5491</id><created>2014-09-18</created><authors><author><keyname>Silva-Garc&#xed;a</keyname><forenames>V. M.</forenames></author><author><keyname>Flores-Carapia</keyname><forenames>R.</forenames></author><author><keyname>Renter&#xed;a-M&#xe1;arquez</keyname><forenames>C.</forenames></author><author><keyname>Luna-Benoso</keyname><forenames>B.</forenames></author><author><keyname>V&#xe1;zquez</keyname><forenames>C. A. Jim&#xe9;nez</forenames></author></authors><title>Images encryption using AES and variable permutations</title><categories>cs.CR</categories><comments>13 pages</comments><acm-class>D.3.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This work proposes a different procedure to encrypt images of 256 grey levels
and colour, using the symmetric system Advanced Encryption Standard with a
variable permutation in the first round, after the x-or operation. Variable
permutation means using a different one for each input block of 128 bits. In
this vein, an algorithm is constructed that defines a Bijective function
between sets Nm = {n in N, 0 &lt;= n &lt; fac(m)} with n &gt;= 2 and Pm = {pi, pi is a
permutation of 0, 1, ..., m-1}. This algorithm calculates permutations on 128
positions with 127 known constants. The transcendental numbers are used to
select the 127 constants in a pseudo-random way. The proposed encryption
quality is evaluated by the following criteria: Correlation; horizontal,
vertical and diagonal, Entropy and Discrete Fourier Transform. The latter uses
the NIST standard 800-22. Also, a sensitivity analysis was performed in
encrypted figures. Furthermore, an additional test is proposed which considers
the distribution of 256 shades of the three colours; red, green and blue for
colour images. On the other hand, it is important to mention that the images
are encrypted without loss of information because many banking companies and
some safety area countries do not allow the figures to go through a compression
process with information loss. i.e., it is forbidden to use formats such as
JPEG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5495</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5495</id><created>2014-09-18</created><updated>2015-01-23</updated><authors><author><keyname>Hu</keyname><forenames>Hanzhang</forenames></author><author><keyname>Grubb</keyname><forenames>Alexander</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author></authors><title>Efficient Feature Group Sequencing for Anytime Linear Prediction</title><categories>cs.LG</categories><comments>work submitted to AISTATS2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a regularized linear learning algorithm to sequence groups of
features, where each group incurs test-time cost or computation. Specifically,
we develop a simple extension to Orthogonal Matching Pursuit (OMP) that
respects the structure of groups of features with variable costs, and we prove
that it achieves near-optimal anytime linear prediction at each budget
threshold where a new group is selected. Our algorithm and analysis extends to
generalized linear models with multi-dimensional responses. We demonstrate the
scalability of the resulting approach on large real-world data-sets with many
feature groups associated with test-time computational costs. Our method
improves over Group Lasso and Group OMP in the anytime performance of linear
predictions, measured in timeliness, an anytime prediction performance metric,
while providing rigorous performance guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5502</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5502</id><created>2014-09-18</created><authors><author><keyname>Kalinin</keyname><forenames>Alexander</forenames></author><author><keyname>Savchenko</keyname><forenames>George</forenames></author></authors><title>Using crowdsourcing system for creating site-specific statistical
  machine translation engine</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crowdsourcing translation approach is an effective tool for globalization
of site content, but it is also an important source of parallel linguistic
data. For the given site, processed with a crowdsourcing system, a
sentence-aligned corpus can be fetched, which covers a very narrow domain of
terminology and language patterns - a site-specific domain. These data can be
used for training and estimation of site-specific statistical machine
translation engine
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5505</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5505</id><created>2014-09-18</created><authors><author><keyname>Carmi</keyname><forenames>Avishy Y.</forenames></author><author><keyname>Moskovich</keyname><forenames>Daniel</forenames></author></authors><title>Low-Dimensional Topology of Information Fusion</title><categories>cs.IT math.IT</categories><comments>8 pages. Conference proceedings version. Will be superceded by a
  journal version</comments><msc-class>94A15, 57M25</msc-class><acm-class>H.1.1</acm-class><journal-ref>Proceedings of BICT '14, 8th International Conference on
  Bio-inspired Information and Communications Technologies, Boston, MA, USA,
  Dec. 01 - 03, 2014, pp. 251-258</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an axiomatic characterization of information fusion, on the basis
of which we define an information fusion network. Our construction is
reminiscent of tangle diagrams in low dimensional topology. Information fusion
networks come equipped with a natural notion of equivalence. Equivalent
networks `contain the same information', but differ locally. When fusing
streams of information, an information fusion network may adaptively optimize
itself inside its equivalence class. This provides a fault tolerance mechanism
for such networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5506</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5506</id><created>2014-09-18</created><updated>2015-06-14</updated><authors><author><keyname>&#x15e;tef&#x103;nescu</keyname><forenames>R&#x103;zvan</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Efficient approximation of sparse Jacobians for time-implicit reduced
  order models</title><categories>math.NA cs.NA</categories><comments>37 pages, 14 figures - proper orthogonal decomposition, DEIM,
  implicit reduced-order models, shallow water equations, finite difference
  methods. arXiv admin note: text overlap with arXiv:1402.2018</comments><report-no>CSTR - 19/2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a sparse matrix discrete interpolation method to
effectively compute matrix approximations in the reduced order modeling
framework. The sparse algorithm developed herein relies on the discrete
empirical interpolation method and uses only samples of the nonzero entries of
the matrix series. The proposed approach can approximate very large matrices,
unlike the current matrix discrete empirical interpolation method which is
limited by its large computational memory requirements. The empirical
interpolation indexes obtained by the sparse algorithm slightly differ from the
ones computed by the matrix discrete empirical interpolation method as a
consequence of the singular vectors round-off errors introduced by the economy
or full singular value decomposition (SVD) algorithms when applied to the full
matrix snapshots. When appropriately padded with zeros the economy SVD
factorization of the nonzero elements of the snapshots matrix is a valid
economy SVD for the full snapshots matrix. Numerical experiments are performed
with the 1D Burgers and 2D Shallow Water Equations test problems where the
quadratic reduced nonlinearities are computed via tensorial calculus. The
sparse matrix approximation strategy is compared against five existing methods
for computing reduced Jacobians: a) matrix discrete empirical interpolation
method, b) discrete empirical interpolation method, c) tensorial calculus, d)
full Jacobian projection onto the reduced basis subspace, and e) directional
derivatives of the model along the reduced basis functions. The sparse matrix
method outperforms all other algorithms. The use of traditional matrix discrete
empirical interpolation method is not possible for very large instances due to
its excessive memory requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5512</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5512</id><created>2014-09-19</created><authors><author><keyname>Li</keyname><forenames>Liangyue</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author><author><keyname>Cao</keyname><forenames>Nan</forenames></author><author><keyname>Ehrlich</keyname><forenames>Kate</forenames></author><author><keyname>Lin</keyname><forenames>Yu-Ru</forenames></author><author><keyname>Buchler</keyname><forenames>Norbou</forenames></author></authors><title>Replacing the Irreplaceable: Fast Algorithms for Team Member
  Recommendation</title><categories>cs.SI cs.DS</categories><comments>Initially submitted to KDD 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of Team Member Replacement: given a team
of people embedded in a social network working on the same task, find a good
candidate who can fit in the team after one team member becomes unavailable. We
conjecture that a good team member replacement should have good skill matching
as well as good structure matching. We formulate this problem using the concept
of graph kernel. To tackle the computational challenges, we propose a family of
fast algorithms by (a) designing effective pruning strategies, and (b)
exploring the smoothness between the existing and the new team structures. We
conduct extensive experimental evaluations on real world datasets to
demonstrate the effectiveness and efficiency. Our algorithms (a) perform
significantly better than the alternative choices in terms of both precision
and recall; and (b) scale sub-linearly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5515</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5515</id><created>2014-09-19</created><authors><author><keyname>Zamani</keyname><forenames>Mohammad</forenames></author><author><keyname>Shames</keyname><forenames>Iman</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery</forenames></author></authors><title>Minimum-Energy Distributed Consensus of Uncertain Agents</title><categories>cs.SY</categories><comments>The paper is accepted for presentation at the 53rd IEEE Conference on
  Decision and Control (IEEE CDC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a consensus algorithm for a multi-agent system where each
agent has access to its imperfect own state and neighboring state measurements.
The measurements are subject to deterministic disturbances and the proposed
algorithm provides a minimum-energy estimate of the measured states which is
instrumental in achieving consensus by the nodes. It is shown that the proposed
consensus algorithm converges exponentially in the absence of disturbances, and
its performance under bounded continuous disturbances is investigated as well.
The convergence performance of the proposed method is further studied using
simulations where we show that consensus is achieved despite using large
measurement errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5519</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5519</id><created>2014-09-19</created><authors><author><keyname>Wen</keyname><forenames>Guanghui</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery</forenames></author></authors><title>Distributed Consensus of Linear Multi-Agent Systems with Switching
  Directed Topologies</title><categories>cs.SY</categories><comments>The paper will be presented at the 2014 Australian Control Conference
  (AUCC 2014), Canberra, Australia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the distributed consensus problem for a linear
multi-agent system with switching directed communication topologies. By
appropriately introducing a linear transformation, the consensus problem is
equivalently converted to a stabilization problem for a class of switched
linear systems. Some sufficient consensus conditions are then derived by using
tools from the matrix theory and stability analysis of switched systems. It is
proved that consensus in such a multi-agent system can be ensured if each agent
is stabilizable and each possible directed topology contains a directed
spanning tree. Finally, a numerical simulation is given for illustration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5524</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5524</id><created>2014-09-19</created><authors><author><keyname>Han</keyname><forenames>Shuguang</forenames></author><author><keyname>He</keyname><forenames>Daqing</forenames></author><author><keyname>Yue</keyname><forenames>Zhen</forenames></author></authors><title>Benchmarking the Privacy-Preserving People Search</title><categories>cs.IR cs.CY</categories><comments>4 pages, 5 figures</comments><acm-class>H.2.8; H.3.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  People search is an important topic in information retrieval. Many previous
studies on this topic employed social networks to boost search performance by
incorporating either local network features (e.g. the common connections
between the querying user and candidates in social networks), or global network
features (e.g. the PageRank), or both. However, the available social network
information can be restricted because of the privacy settings of involved
users, which in turn would affect the performance of people search. Therefore,
in this paper, we focus on the privacy issues in people search. We propose
simulating different privacy settings with a public social network due to the
unavailability of privacy-concerned networks. Our study examines the influences
of privacy concerns on the local and global network features, and their impacts
on the performance of people search. Our results show that: 1) the privacy
concerns of different people in the networks have different influences. People
with higher association (i.e. higher degree in a network) have much greater
impacts on the performance of people search; 2) local network features are more
sensitive to the privacy concerns, especially when such concerns come from high
association peoples in the network who are also related to the querying user.
As the first study on this topic, we hope to generate further discussions on
these issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5531</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5531</id><created>2014-09-19</created><updated>2014-11-28</updated><authors><author><keyname>Coecke</keyname><forenames>Bob</forenames></author><author><keyname>Fritz</keyname><forenames>Tobias</forenames></author><author><keyname>Spekkens</keyname><forenames>Robert W.</forenames></author></authors><title>A mathematical theory of resources</title><categories>quant-ph cs.IT cs.SY math.IT</categories><comments>32 pages, many figures. v2 and v3: minor revisions</comments><doi>10.1016/j.ic.2016.02.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many different fields of science, it is useful to characterize physical
states and processes as resources. Chemistry, thermodynamics, Shannon's theory
of communication channels, and the theory of quantum entanglement are prominent
examples. Questions addressed by a theory of resources include: Which resources
can be converted into which other ones? What is the rate at which arbitrarily
many copies of one resource can be converted into arbitrarily many copies of
another? Can a catalyst help in making an impossible transformation possible?
How does one quantify the resource? Here, we propose a general mathematical
definition of what constitutes a resource theory. We prove some general
theorems about how resource theories can be constructed from theories of
processes wherein there is a special class of processes that are implementable
at no cost and which define the means by which the costly states and processes
can be interconverted one to another. We outline how various existing resource
theories fit into our framework. Our abstract characterization of resource
theories is a first step in a larger project of identifying universal features
and principles of resource theories. In this vein, we identify a few general
results concerning resource convertibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5532</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5532</id><created>2014-09-19</created><authors><author><keyname>Yang</keyname><forenames>Minho</forenames></author><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Kim</keyname><forenames>Dong Ku</forenames></author></authors><title>Linear Degrees of Freedom of MIMO Broadcast Channels with Reconfigurable
  Antennas in the Absence of CSIT</title><categories>cs.IT math.IT</categories><comments>25 pages, 7 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The K-user multiple-input and multiple-output (MIMO) broadcast channel (BC)
with no channel state information at the transmitter (CSIT) is considered,
where each receiver is assumed to be equipped with reconfigurable antennas
capable of choosing a subset of receiving modes from several preset modes.
Under general antenna configurations, the sum linear degrees of freedom (LDoF)
of the K-user MIMO BC with reconfigurable antennas is completely characterized,
which corresponds to the maximum sum DoF achievable by linear coding
strategies. The LDoF region is further characterized for a class of antenna
configurations. Similar analysis is extended to the K-user MIMO interference
channels with reconfigurable antennas and the sum LDoF is characterized for a
class of antenna configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5543</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5543</id><created>2014-09-19</created><updated>2015-08-04</updated><authors><author><keyname>Cheng</keyname><forenames>Fan</forenames></author><author><keyname>Geng</keyname><forenames>Yanlin</forenames></author></authors><title>Higher Order Derivatives in Costa's Entropy Power Inequality</title><categories>cs.IT math.IT</categories><comments>Second version submitted. https://sites.google.com/site/chengfancuhk/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X$ be an arbitrary continuous random variable and $Z$ be an independent
Gaussian random variable with zero mean and unit variance. For $t~&gt;~0$, Costa
proved that $e^{2h(X+\sqrt{t}Z)}$ is concave in $t$, where the proof hinged on
the first and second order derivatives of $h(X+\sqrt{t}Z)$. Specifically, these
two derivatives are signed, i.e., $\frac{\partial}{\partial t}h(X+\sqrt{t}Z)
\geq 0$ and $\frac{\partial^2}{\partial t^2}h(X+\sqrt{t}Z) \leq 0$. In this
paper, we show that the third order derivative of $h(X+\sqrt{t}Z)$ is
nonnegative, which implies that the Fisher information $J(X+\sqrt{t}Z)$ is
convex in $t$. We further show that the fourth order derivative of
$h(X+\sqrt{t}Z)$ is nonpositive. Following the first four derivatives, we make
two conjectures on $h(X+\sqrt{t}Z)$: the first is that
$\frac{\partial^n}{\partial t^n} h(X+\sqrt{t}Z)$ is nonnegative in $t$ if $n$
is odd, and nonpositive otherwise; the second is that $\log J(X+\sqrt{t}Z)$ is
convex in $t$. The first conjecture can be rephrased in the context of
completely monotone functions: $J(X+\sqrt{t}Z)$ is completely monotone in $t$.
The history of the first conjecture may date back to a problem in mathematical
physics studied by McKean in 1966. Apart from these results, we provide a
geometrical interpretation to the covariance-preserving transformation and
study the concavity of $h(\sqrt{t}X+\sqrt{1-t}Z)$, revealing its connection
with Costa's EPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5546</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5546</id><created>2014-09-19</created><authors><author><keyname>Imran</keyname><forenames>Asif</forenames></author><author><keyname>Nahar</keyname><forenames>Nadia</forenames></author><author><keyname>Sakib</keyname><forenames>Kazi</forenames></author></authors><title>Watchword-Oriented and Time-Stamped Algorithms for Tamper-Proof Cloud
  Provenance Cognition</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provenance is derivative journal information about the origin and activities
of system data and processes. For a highly dynamic system like the cloud,
provenance can be accurately detected and securely used in cloud digital
forensic investigation activities. This paper proposes watchword oriented
provenance cognition algorithm for the cloud environment. Additionally
time-stamp based buffer verifying algorithm is proposed for securing the access
to the detected cloud provenance. Performance analysis of the novel algorithms
proposed here yields a desirable detection rate of 89.33% and miss rate of
8.66%. The securing algorithm successfully rejects 64% of malicious requests,
yielding a cumulative frequency of 21.43 for MR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5552</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5552</id><created>2014-09-19</created><authors><author><keyname>Imran</keyname><forenames>Asif</forenames></author><author><keyname>Dey</keyname><forenames>Emon Kumar</forenames></author><author><keyname>Sakib</keyname><forenames>Kazi</forenames></author></authors><title>Active-Threaded Algorithms for Provenance Cognition in the Cloud
  preserving Low Overhead and Fault Tolerance</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provenance is the derivation history of information about the origin of data
and processes. For a highly dynamic system such as the cloud, provenance must
be effectively detected to be used as proves to ensure accountability during
digital forensic investigations. This paper proposes active-threaded provenance
cognition algorithms that ensure effective and high speed detection of
provenance information in the activity layer of the cloud. The algorithms also
support encapsulation of the provenance information on specific targets.
Performance evaluation of the proposed algorithms reveal mean delay of 8.198
seconds that is below the pre-defined benchmark of 10 seconds. Standard
deviation and cumulative frequencies for delays are found to be 1.434 and 45.1%
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5557</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5557</id><created>2014-09-19</created><authors><author><keyname>Tramel</keyname><forenames>Eric W.</forenames></author><author><keyname>Kumar</keyname><forenames>Santhosh</forenames></author><author><keyname>Giurgiu</keyname><forenames>Andrei</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Statistical Estimation: From Denoising to Sparse Regression and Hidden
  Cliques</title><categories>cs.IT math.IT stat.ML</categories><comments>Chapter of &quot;Statistical Physics, Optimization, Inference, and
  Message-Passing Algorithms&quot;, Eds.: F. Krzakala, F. Ricci-Tersenghi, L.
  Zdeborova, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University
  Press, to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These notes review six lectures given by Prof. Andrea Montanari on the topic
of statistical estimation for linear models. The first two lectures cover the
principles of signal recovery from linear measurements in terms of minimax
risk. Subsequent lectures demonstrate the application of these principles to
several practical problems in science and engineering. Specifically, these
topics include denoising of error-laden signals, recovery of compressively
sensed signals, reconstruction of low-rank matrices, and also the discovery of
hidden cliques within large networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5561</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5561</id><created>2014-09-19</created><updated>2015-01-18</updated><authors><author><keyname>Fukunaga</keyname><forenames>Takuro</forenames></author></authors><title>Approximating the generalized terminal backup problem via half-integral
  multiflow relaxation</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network design problem called the generalized terminal backup
problem. Whereas earlier work investigated the edge-connectivity constraints
only, we consider both edge- and node-connectivity constraints for this
problem. A major contribution of this paper is the development of a strongly
polynomial-time 4/3-approximation algorithm for the problem. Specifically, we
show that a linear programming relaxation of the problem is half-integral, and
that the half-integral optimal solution can be rounded to a 4/3-approximate
solution. We also prove that the linear programming relaxation of the problem
with the edge-connectivity constraints is equivalent to minimizing the cost of
half-integral multiflows that satisfy flow demands given from terminals. This
observation presents a strongly polynomial-time algorithm for computing a
minimum cost half-integral multiflow under flow demand constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5567</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5567</id><created>2014-09-19</created><authors><author><keyname>Lu</keyname><forenames>Yanchao</forenames></author><author><keyname>Wu</keyname><forenames>Donghong</forenames></author><author><keyname>He</keyname><forenames>Bingsheng</forenames></author><author><keyname>Tang</keyname><forenames>Xueyan</forenames></author><author><keyname>Xu</keyname><forenames>Jianliang</forenames></author><author><keyname>Guo</keyname><forenames>Minyi</forenames></author></authors><title>Rank-Aware Dynamic Migrations and Adaptive Demotions for DRAM Power
  Management</title><categories>cs.PF cs.AR cs.OS</categories><comments>19 pages</comments><report-no>SJTU-CSE-13-2</report-no><acm-class>B.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern DRAM architectures allow a number of low-power states on individual
memory ranks for advanced power management. Many previous studies have taken
advantage of demotions on low-power states for energy saving. However, most of
the demotion schemes are statically performed on a limited number of
pre-selected low-power states, and are suboptimal for different workloads and
memory architectures. Even worse, the idle periods are often too short for
effective power state transitions, especially for memory intensive
applications. Wrong decisions on power state transition incur significant
energy and delay penalties. In this paper, we propose a novel memory system
design named RAMZzz with rank-aware energy saving optimizations including
dynamic page migrations and adaptive demotions. Specifically, we group the
pages with similar access locality into the same rank with dynamic page
migrations. Ranks have their hotness: hot ranks are kept busy for high
utilization and cold ranks can have more lengthy idle periods for power state
transitions. We further develop adaptive state demotions by considering all
low-power states for each rank and a prediction model to estimate the
power-down timeout among states. We experimentally compare our algorithm with
other energy saving policies with cycle-accurate simulation. Experiments with
benchmark workloads show that RAMZzz achieves significant improvement on
energy-delay2 and energy consumption over other energy saving techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5583</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5583</id><created>2014-09-19</created><updated>2015-02-26</updated><authors><author><keyname>Amir</keyname><forenames>Mohamed</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Elfouly</keyname><forenames>Tarek</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>On the degrees of freedom of the MIMO Wiretap and MIMO broadcast
  channels with unknown eavesdroppers</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1404.5007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the secure degrees of freedom region of the point to point and
the two-transmitter Gaussian broadcast wiretap channels with multiple antennas
at the legitimate receivers and the eavesdroppers with unknown eavesdroppers'
CSI. The existence of unknown number of eavesdroppers is assumed but with
maximum number of antennas at any eavesdropper is limited to a known value
$N_E$. The channel matrices between the transmitters and the receiver is
available everywhere, while the legitimate pair have no information about the
eavesdroppers channels. A new upperbound is established and The DoF region is
characterized. It is important to note that the same problem was studied with
arbitrarily varying eavesdropper channels, our region is larger than their
upperbound. It is important to note that the same problem has been studied
recently with arbitrarily varying eavesdropper channels and an upperbound has
been derived. However, our achievable sum secure DoF exceeds their previously
derived upperbound. Consequently, we revisited the uppperbound derivation and
we re-derived a new mathematically robust upperbound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5595</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5595</id><created>2014-09-19</created><authors><author><keyname>Rainone</keyname><forenames>M.</forenames></author><author><keyname>Fonda</keyname><forenames>C.</forenames></author><author><keyname>Canessa</keyname><forenames>E.</forenames></author></authors><title>IMAGINARY Math Exhibition using Low-cost 3D Printers</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We have made an attempt to reproduce 17 objects of the IMAGINARY Open
Mathematics Exhibition (www.imaginary.org) using low-cost, desktop 3D printers.
The IMAGINARY open math is an international project by the Mathematisches
Forschungsinstitut Oberwolfach in Germany and includes galleries of volumetric
objects that are unique, have aesthetic appeal and mathematical meaning. We
illustrate here the printing of these diverse learning materials using new 3D
affordable printing technologies based on Fused Deposition Modelling (FDM) and
the use of biodegradable plastic PLA. The final goal is to support museums,
schools and higher education institutions in countries with lower scientific
infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5606</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5606</id><created>2014-09-19</created><authors><author><keyname>Lee</keyname><forenames>Jaeseok</forenames></author><author><keyname>Kwon</keyname><forenames>Suhyuk</forenames></author><author><keyname>Choi</keyname><forenames>Jun Won</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author></authors><title>Greedy Sparse Signal Recovery with Tree Pruning</title><categories>cs.IT math.IT</categories><comments>29 pages, 8 figures, draftcls, 11pts</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recently, greedy algorithm has received much attention as a cost-effective
means to reconstruct the sparse signals from compressed measurements. Much of
previous work has focused on the investigation of a single candidate to
identify the support (index set of nonzero elements) of the sparse signals.
Well-known drawback of the greedy approach is that the chosen candidate is
often not the optimal solution due to the myopic decision in each iteration. In
this paper, we propose a greedy sparse recovery algorithm investigating
multiple promising candidates via the tree search. Two key ingredients of the
proposed algorithm, referred to as the matching pursuit with a tree pruning
(TMP), to achieve efficiency in the tree search are the {\it pre-selection} to
put a restriction on columns of the sensing matrix to be investigated and the
{\it tree pruning} to eliminate unpromising paths from the search tree. In our
performance guarantee analysis and empirical simulations, we show that TMP is
effective in recovering sparse signals in both noiseless and noisy scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5616</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5616</id><created>2014-09-19</created><authors><author><keyname>Deng</keyname><forenames>Zhaohong</forenames></author><author><keyname>Choi</keyname><forenames>Kup-Sze</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Shitong</forenames></author></authors><title>A Survey on Soft Subspace Clustering</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering (SC) is a promising clustering technology to identify
clusters based on their associations with subspaces in high dimensional spaces.
SC can be classified into hard subspace clustering (HSC) and soft subspace
clustering (SSC). While HSC algorithms have been extensively studied and well
accepted by the scientific community, SSC algorithms are relatively new but
gaining more attention in recent years due to better adaptability. In the
paper, a comprehensive survey on existing SSC algorithms and the recent
development are presented. The SSC algorithms are classified systematically
into three main categories, namely, conventional SSC (CSSC), independent SSC
(ISSC) and extended SSC (XSSC). The characteristics of these algorithms are
highlighted and the potential future development of SSC is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5622</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5622</id><created>2014-09-19</created><authors><author><keyname>Jelenkovi&#x107;</keyname><forenames>Predrag R.</forenames></author><author><keyname>Skiani</keyname><forenames>Evangelia D.</forenames></author></authors><title>Instability of Sharing Systems in the Presence of Retransmissions</title><categories>cs.PF math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retransmissions represent a primary failure recovery mechanism on all layers
of communication network architecture. Similarly, fair sharing, e.g. processor
sharing (PS), is a widely accepted approach to resource allocation among
multiple users. Recent work has shown that retransmissions in failure-prone,
e.g. wireless ad hoc, networks can cause heavy tails and long delays. In this
paper, we discover a new phenomenon showing that PS-based scheduling induces
complete instability with zero throughput in the presence of retransmissions,
regardless of how low the traffic load may be. This phenomenon occurs even when
the job sizes are bounded/fragmented, e.g. deterministic. Our analytical
results are further validated via simulation experiments. Moreover, our work
demonstrates that scheduling one job at a time, such as first-come-first-serve,
achieves stability and should be preferred in these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5623</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5623</id><created>2014-09-19</created><updated>2014-11-27</updated><authors><author><keyname>R&#xf6;nnqvist</keyname><forenames>Samuel</forenames></author><author><keyname>Wang</keyname><forenames>Xiaolu</forenames></author><author><keyname>Sarlin</keyname><forenames>Peter</forenames></author></authors><title>Interactive Visual Exploration of Topic Models using Graphs</title><categories>cs.IR cs.CL</categories><comments>Online demo at http://risklab.fi/demo/topics/. appears in Proceedings
  of the 2014 Eurographics Conference on Visualization (EuroVis)</comments><acm-class>I.5.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Probabilistic topic modeling is a popular and powerful family of tools for
uncovering thematic structure in large sets of unstructured text documents.
While much attention has been directed towards the modeling algorithms and
their various extensions, comparatively few studies have concerned how to
present or visualize topic models in meaningful ways. In this paper, we present
a novel design that uses graphs to visually communicate topic structure and
meaning. By connecting topic nodes via descriptive keyterms, the graph
representation reveals topic similarities, topic meaning and shared, ambiguous
keyterms. At the same time, the graph can be used for information retrieval
purposes, to find documents by topic or topic subsets. To exemplify the utility
of the design, we illustrate its use for organizing and exploring corpora of
financial patents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5627</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5627</id><created>2014-09-19</created><updated>2015-02-25</updated><authors><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Guo</keyname><forenames>Heng</forenames></author></authors><title>The complexity of approximating complex-valued Ising and Tutte partition
  functions</title><categories>cs.CC quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of approximately evaluating the Ising and Tutte
partition functions with complex parameters. Our results are partly motivated
by the study of the quantum complexity classes BQP and IQP. Recent results show
how to encode quantum computations as evaluations of classical partition
functions. These results rely on interesting and deep results about quantum
computation in order to obtain hardness results about the difficulty of
(classically) evaluating the partition functions for certain fixed parameters.
  The motivation for this paper is to study more comprehensively the complexity
of (classically) approximating the Ising and Tutte partition functions with
complex parameters. Partition functions are combinatorial in nature and
quantifying their approximation complexity does not require a detailed
understanding of quantum computation. Using combinatorial arguments, we give
the first full classification of the complexity of multiplicatively
approximating the norm and additively approximating the argument of the Ising
partition function for complex edge interactions (as well as of approximating
the partition function according to a natural complex metric). We also study
the norm approximation problem in the presence of external fields, for which we
give a complete dichotomy when the parameters are roots of unity. Previous
results were known just for a few such points, and we strengthen these results
from BQP-hardness to #P-hardness. Moreover, we show that computing the sign of
the Tutte polynomial is #P-hard at certain points related to the simulation of
BQP. Using our classifications, we then revisit the connections to quantum
computation, drawing conclusions that are a little different from (and
incomparable to) ones in the quantum literature, but along similar lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5641</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5641</id><created>2014-09-19</created><authors><author><keyname>Kosolobov</keyname><forenames>Dmitry</forenames></author></authors><title>Lempel-Ziv Factorization May Be Harder Than Computing All Runs</title><categories>cs.DS</categories><comments>12 pages, 3 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of computing the Lempel-Ziv factorization and the set of all
runs (= maximal repetitions) is studied in the decision tree model of
computation over ordered alphabet. It is known that both these problems can be
solved by RAM algorithms in $O(n\log\sigma)$ time, where $n$ is the length of
the input string and $\sigma$ is the number of distinct letters in it. We prove
an $\Omega(n\log\sigma)$ lower bound on the number of comparisons required to
construct the Lempel-Ziv factorization and thereby conclude that a popular
technique of computation of runs using the Lempel-Ziv factorization cannot
achieve an $o(n\log\sigma)$ time bound. In contrast with this, we exhibit an
$O(n)$ decision tree algorithm finding all runs in a string. Therefore, in the
decision tree model the runs problem is easier than the Lempel-Ziv
factorization. Thus we support the conjecture that there is a linear RAM
algorithm finding all runs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5659</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5659</id><created>2014-09-19</created><authors><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Multiple-access Fading Channel with Wireless Power Transfer and Energy
  Harvesting</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure</comments><doi>10.1109/LCOMM.2014.2355198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the achievable average rates of a multiple-access system, which
consists of N energy-harvesting users (EHUs) that transmit information over a
block fading multiple-access channel (MAC) and a base station (BS) that
broadcasts radio frequency (RF) energy to the EHUs for wireless power transfer.
The information (over the uplink) and power (over the downlink) can be
transmitted either in time division duplex or frequency division duplex. For
the case when the EHUs battery capacities and the number of transmission slots
are both infinite, we determine the optimal power allocation for the BS and the
optimal rates and power allocations for the EHUs that maximize the achievable
rate region of the MAC. The resulting online solution is asymptotically
optimal, and also applicable for a finite number of transmission slots and
finite battery capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5671</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5671</id><created>2014-09-12</created><authors><author><keyname>Gol</keyname><forenames>Ebru Aydin</forenames></author><author><keyname>Bartocci</keyname><forenames>Ezio</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion
  Systems</title><categories>cs.AI cs.CE cs.LG cs.LO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a technique to detect and generate patterns in a network of
locally interacting dynamical systems. Central to our approach is a novel
spatial superposition logic, whose semantics is defined over the quad-tree of a
partitioned image. We show that formulas in this logic can be efficiently
learned from positive and negative examples of several types of patterns. We
also demonstrate that pattern detection, which is implemented as a model
checking algorithm, performs very well for test data sets different from the
learning sets. We define a quantitative semantics for the logic and integrate
the model checking algorithm with particle swarm optimization in a
computational framework for synthesis of parameters leading to desired patterns
in reaction-diffusion systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5677</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5677</id><created>2014-09-19</created><updated>2015-04-14</updated><authors><author><keyname>Winkel</keyname><forenames>Mathias</forenames></author><author><keyname>Speck</keyname><forenames>Robert</forenames></author><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author></authors><title>A high-order Boris integrator</title><categories>math.NA cs.NA</categories><journal-ref>Journal of Computational Physics 295, pp. 456-474, 2015</journal-ref><doi>10.1016/j.jcp.2015.04.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces the high-order Boris-SDC method for integrating the
equations of motion for electrically charged particles in an electric and
magnetic field. Boris-SDC relies on a combination of the Boris-integrator with
spectral deferred corrections (SDC). SDC can be considered as preconditioned
Picard iteration to compute the stages of a collocation method. In this
interpretation, inverting the preconditioner corresponds to a sweep with a
low-order method. In Boris-SDC, the Boris method, a second-order Lorentz force
integrator based on velocity-Verlet, is used as a sweeper/preconditioner. The
presented method provides a generic way to extend the classical Boris
integrator, which is widely used in essentially all particle-based plasma
physics simulations involving magnetic fields, to a high-order method.
Stability, convergence order and conservation properties of the method are
demonstrated for different simulation setups. Boris-SDC reproduces the expected
high order of convergence for a single particle and for the center-of-mass of a
particle cloud in a Penning trap and shows good long-term energy stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5681</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5681</id><created>2014-09-19</created><authors><author><keyname>Vester</keyname><forenames>Steen</forenames></author></authors><title>Model-checking Quantitative Alternating-time Temporal Logic on
  One-counter Game Models</title><categories>cs.LO</categories><comments>22 pages, 12 figures</comments><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider quantitative extensions of the alternating-time temporal logics
ATL/ATLs called quantitative alternating-time temporal logics (QATL/QATLs) in
which the value of a counter can be compared to constants using equality,
inequality and modulo constraints. We interpret these logics in one-counter
game models which are infinite duration games played on finite control graphs
where each transition can increase or decrease the value of an unbounded
counter. That is, the state-space of these games are, generally, infinite. We
consider the model-checking problem of the logics QATL and QATLs on one-counter
game models with VASS semantics for which we develop algorithms and provide
matching lower bounds. Our algorithms are based on reductions of the
model-checking problems to model-checking games. This approach makes it quite
simple for us to deal with extensions of the logical languages as well as the
infinite state spaces. The framework generalizes on one hand qualitative
problems such as ATL/ATLs model-checking of finite-state systems,
model-checking of the branching-time temporal logics CTL and CTLs on
one-counter processes and the realizability problem of LTL specifications. On
the other hand the model-checking problem for QATL/QATLs generalizes
quantitative problems such as the fixed-initial credit problem for energy games
(in the case of QATL) and energy parity games (in the case of QATLs). Our
results are positive as we show that the generalizations are not too costly
with respect to complexity. As a byproduct we obtain new results on the
complexity of model-checking CTLs in one-counter processes and show that
deciding the winner in one-counter games with LTL objectives is
2ExpSpace-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5686</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5686</id><created>2014-09-19</created><authors><author><keyname>Deng</keyname><forenames>Zhaohong</forenames></author><author><keyname>Jiang</keyname><forenames>Yizhang</forenames></author><author><keyname>Chung</keyname><forenames>Fu-Lai</forenames></author><author><keyname>Choi</keyname><forenames>Kup-Sze</forenames></author><author><keyname>Wang</keyname><forenames>Shitong</forenames></author></authors><title>Transfer Prototype-based Fuzzy Clustering</title><categories>cs.LG</categories><comments>The manuscript has been submitted to IEEE Trans. Fuzzy Systmes in
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional prototype based clustering methods, such as the well-known
fuzzy c-mean (FCM) algorithm, usually need sufficient data to find a good
clustering partition. If the available data is limited or scarce, most of the
existing prototype based clustering algorithms will no longer be effective.
While the data for the current clustering task may be scarce, there is usually
some useful knowledge available in the related scenes/domains. In this study,
the concept of transfer learning is applied to prototype based fuzzy clustering
(PFC). Specifically, the idea of leveraging knowledge from the source domain is
exploited to develop a set of transfer prototype based fuzzy clustering (TPFC)
algorithms. Three prototype based fuzzy clustering algorithms, namely, FCM,
fuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have been
chosen to incorporate with knowledge leveraging mechanism to develop the
corresponding transfer clustering algorithms. Novel objective functions are
proposed to integrate the knowledge of source domain with the data of target
domain for clustering in the target domain. The proposed algorithms have been
validated on different synthetic and real-world datasets and the results
demonstrate their effectiveness when compared with both the original prototype
based fuzzy clustering algorithms and the related clustering algorithms like
multi-task clustering and co-clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5698</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5698</id><created>2014-09-19</created><updated>2015-01-16</updated><authors><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Gabrielli</keyname><forenames>Andrea</forenames></author><author><keyname>Labini</keyname><forenames>Francesco Sylos</forenames></author></authors><title>The Scientific Competitiveness of Nations</title><categories>physics.soc-ph cs.DL</categories><journal-ref>PLoS ONE 9(12): e113470 (2014)</journal-ref><doi>10.1371/journal.pone.0113470</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use citation data of scientific articles produced by individual nations in
different scientific domains to determine the structure and efficiency of
national research systems. We characterize the scientific fitness of each
nation (that is, the competitiveness of its research system) and the complexity
of each scientific domain by means of a non-linear iterative algorithm able to
assess quantitatively the advantage of scientific diversification. We find that
technological leading nations, beyond having the largest production of
scientific papers and the largest number of citations, do not specialize in a
few scientific domains. Rather, they diversify as much as possible their
research system. On the other side, less developed nations are competitive only
in scientific domains where also many other nations are present.
Diversification thus represents the key element that correlates with scientific
and technological competitiveness. A remarkable implication of this structure
of the scientific competition is that the scientific domains playing the role
of &quot;markers&quot; of national scientific competitiveness are those not necessarily
of high technological requirements, but rather addressing the most
&quot;sophisticated&quot; needs of the society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5705</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5705</id><created>2014-09-19</created><updated>2015-09-07</updated><authors><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Kim</keyname><forenames>Jin Kyu</forenames></author><author><keyname>Zhou</keyname><forenames>Yi</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Kumar</keyname><forenames>Abhimanu</forenames></author><author><keyname>Yu</keyname><forenames>Yaoliang</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author></authors><title>Distributed Machine Learning via Sufficient Factor Broadcasting</title><categories>cs.LG cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix-parametrized models, including multiclass logistic regression and
sparse coding, are used in machine learning (ML) applications ranging from
computer vision to computational biology. When these models are applied to
large-scale ML problems starting at millions of samples and tens of thousands
of classes, their parameter matrix can grow at an unexpected rate, resulting in
high parameter synchronization costs that greatly slow down distributed
learning. To address this issue, we propose a Sufficient Factor Broadcasting
(SFB) computation model for efficient distributed learning of a large family of
matrix-parameterized models, which share the following property: the parameter
update computed on each data sample is a rank-1 matrix, i.e., the outer product
of two &quot;sufficient factors&quot; (SFs). By broadcasting the SFs among worker
machines and reconstructing the update matrices locally at each worker, SFB
improves communication efficiency --- communication costs are linear in the
parameter matrix's dimensions, rather than quadratic --- without affecting
computational correctness. We present a theoretical convergence analysis of
SFB, and empirically corroborate its efficiency on four different
matrix-parametrized ML models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5715</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5715</id><created>2014-09-19</created><updated>2014-09-22</updated><authors><author><keyname>Schulte</keyname><forenames>Stefan</forenames></author><author><keyname>Janiesch</keyname><forenames>Christian</forenames></author><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author><author><keyname>Weber</keyname><forenames>Ingo</forenames></author><author><keyname>Hoenisch</keyname><forenames>Philipp</forenames></author></authors><title>Elastic Business Process Management: State of the Art and Open
  Challenges for BPM in the Cloud</title><categories>cs.DC</categories><comments>Please cite as: S. Schulte, C. Janiesch, S. Venugopal, I. Weber, and
  P. Hoenisch (2015). Elastic Business Process Management: State of the Art and
  Open Challenges for BPM in the Cloud. Future Generation Computer Systems,
  Volume NN, Number N, NN-NN., http://dx.doi.org/10.1016/j.future.2014.09.005</comments><acm-class>C.2.4</acm-class><doi>10.1016/j.future.2014.09.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of cloud computing, organizations are nowadays able to react
rapidly to changing demands for computational resources. Not only individual
applications can be hosted on virtual cloud infrastructures, but also complete
business processes. This allows the realization of so-called elastic processes,
i.e., processes which are carried out using elastic cloud resources. Despite
the manifold benefits of elastic processes, there is still a lack of solutions
supporting them.
  In this paper, we identify the state of the art of elastic Business Process
Management with a focus on infrastructural challenges. We conceptualize an
architecture for an elastic Business Process Management System and discuss
existing work on scheduling, resource allocation, monitoring, decentralized
coordination, and state management for elastic processes. Furthermore, we
present two representative elastic Business Process Management Systems which
are intended to counter these challenges. Based on our findings, we identify
open issues and outline possible research directions for the realization of
elastic processes and elastic Business Process Management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5716</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5716</id><created>2014-09-19</created><authors><author><keyname>Lameiro</keyname><forenames>Christian</forenames></author><author><keyname>Santamar&#xed;a</keyname><forenames>Ignacio</forenames></author><author><keyname>Schreier</keyname><forenames>Peter J.</forenames></author></authors><title>Benefits of improper signaling for underlay cognitive radio</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we study the potential benefits of improper signaling for a
secondary user (SU) in underlay cognitive radio networks. We consider a basic
yet illustrative scenario in which the primary user (PU) always transmit proper
Gaussian signals and has a minimum rate constraint. After parameterizing the SU
transmit signal in terms of its power and circularity coefficient (which
measures the degree of impropriety), we prove that the SU improves its rate by
transmitting improper signals only when the ratio of the squared modulus
between the SU-PU interference link and the SU direct link exceeds a given
threshold. As a by-product of this analysis, we obtain the optimal circularity
coefficient that must be used by the SU depending on its power budget. Some
simulation results show that the SU benefits from the transmission of improper
signals especially when the PU is not highly loaded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5718</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5718</id><created>2014-09-18</created><updated>2015-12-08</updated><authors><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author></authors><title>Convolutional Neural Networks over Tree Structures for Programming
  Language Processing</title><categories>cs.LG cs.NE cs.SE</categories><comments>Accepted at AAAI-16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming language processing (similar to natural language processing) is a
hot research topic in the field of software engineering; it has also aroused
growing interest in the artificial intelligence community. However, different
from a natural language sentence, a program contains rich, explicit, and
complicated structural information. Hence, traditional NLP models may be
inappropriate for programs. In this paper, we propose a novel tree-based
convolutional neural network (TBCNN) for programming language processing, in
which a convolution kernel is designed over programs' abstract syntax trees to
capture structural information. TBCNN is a generic architecture for programming
language processing; our experiments show its effectiveness in two different
program analysis tasks: classifying programs according to functionality, and
detecting code snippets of certain patterns. TBCNN outperforms baseline
methods, including several neural models for NLP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5719</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5719</id><created>2014-09-19</created><authors><author><keyname>L&#xf3;pez-Ib&#xe1;&#xf1;ez</keyname><forenames>Manuel</forenames><affiliation>IRIDIA</affiliation></author><author><keyname>Liefooghe</keyname><forenames>Arnaud</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Verel</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LISIC</affiliation></author></authors><title>Local Optimal Sets and Bounded Archiving on Multi-objective
  NK-Landscapes with Correlated Objectives</title><categories>cs.AI</categories><comments>appears in Parallel Problem Solving from Nature - PPSN XIII,
  Ljubljana : Slovenia (2014)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-319-10762-2_61</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The properties of local optimal solutions in multi-objective combinatorial
optimization problems are crucial for the effectiveness of local search
algorithms, particularly when these algorithms are based on Pareto dominance.
Such local search algorithms typically return a set of mutually nondominated
Pareto local optimal (PLO) solutions, that is, a PLO-set. This paper
investigates two aspects of PLO-sets by means of experiments with Pareto local
search (PLS). First, we examine the impact of several problem characteristics
on the properties of PLO-sets for multi-objective NK-landscapes with correlated
objectives. In particular, we report that either increasing the number of
objectives or decreasing the correlation between objectives leads to an
exponential increment on the size of PLO-sets, whereas the variable correlation
has only a minor effect. Second, we study the running time and the quality
reached when using bounding archiving methods to limit the size of the archive
handled by PLS, and thus, the maximum size of the PLO-set found. We argue that
there is a clear relationship between the running time of PLS and the
difficulty of a problem instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5729</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5729</id><created>2014-09-19</created><authors><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Hyperspectral and Multispectral Image Fusion based on a Sparse
  Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a variational based approach to fusing hyperspectral and
multispectral images. The fusion process is formulated as an inverse problem
whose solution is the target image assumed to live in a much lower dimensional
subspace. A sparse regularization term is carefully designed, relying on a
decomposition of the scene on a set of dictionaries. The dictionary atoms and
the corresponding supports of active coding coefficients are learned from the
observed images. Then, conditionally on these dictionaries and supports, the
fusion problem is solved via alternating optimization with respect to the
target image (using the alternating direction method of multipliers) and the
coding coefficients. Simulation results demonstrate the efficiency of the
proposed algorithm when compared with the state-of-the-art fusion methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5743</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5743</id><created>2014-09-19</created><updated>2014-10-13</updated><authors><author><keyname>Rucco</keyname><forenames>Matteo</forenames></author><author><keyname>Rodrigues</keyname><forenames>David M. S.</forenames></author><author><keyname>Merelli</keyname><forenames>Emanuela</forenames></author><author><keyname>Johnson</keyname><forenames>Jeffrey H.</forenames></author><author><keyname>Falsetti</keyname><forenames>Lorenzo</forenames></author><author><keyname>Nitti</keyname><forenames>Cinzia</forenames></author><author><keyname>Salvi</keyname><forenames>Aldo</forenames></author></authors><title>Neural Hypernetwork Approach for Pulmonary Embolism diagnosis</title><categories>physics.med-ph cs.LG physics.data-an q-bio.QM stat.ML</categories><comments>16 pages, 6 figures, 5 tables</comments><acm-class>J.2; J.3; I.5; C.1.3; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces an integrative approach based on Q-analysis with machine
learning. The new approach, called Neural Hypernetwork, has been applied to a
case study of pulmonary embolism diagnosis. The objective of the application of
neural hyper-network to pulmonary embolism (PE) is to improve diagnose for
reducing the number of CT-angiography needed. Hypernetworks, based on
topological simplicial complex, generalize the concept of two-relation to
many-body relation. Furthermore, Hypernetworks provide a significant
generalization of network theory, enabling the integration of relational
structure, logic and analytic dynamics. Another important results is that
Q-analysis stays close to the data, while other approaches manipulate data,
projecting them into metric spaces or applying some filtering functions to
highlight the intrinsic relations. A pulmonary embolism (PE) is a blockage of
the main artery of the lung or one of its branches, frequently fatal. Our study
uses data on 28 diagnostic features of 1,427 people considered to be at risk of
PE. The resulting neural hypernetwork correctly recognized 94% of those
developing a PE. This is better than previous results that have been obtained
with other methods (statistical selection of features, partial least squares
regression, topological data analysis in a metric space).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5744</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5744</id><created>2014-09-19</created><authors><author><keyname>Badawy</keyname><forenames>Ahmed</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Trinchero</keyname><forenames>Daniele</forenames></author><author><keyname>Fouly</keyname><forenames>Tarek El</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>A Simple AoA Estimation Scheme</title><categories>cs.SY cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an intuitive, simple and hardware friendly, yet surprisingly novel
and efficient, received signal's angle of arrival (AoA) estimation scheme. Our
intuitive, two-phases cross-correlation based scheme relies on a switched beam
antenna array, which is used to collect an omni-directional signal using few
elements of the antenna array in the first phase. In the second phase, the
scheme switches the main beam of the antenna array to scan the angular region
of interest. The collected signal from each beam (direction or angle) is cross
correlated with the omni-directional signal. The cross-correlation coefficient
will be the highest at the correct AoA and relatively negligible elsewhere. The
proposed scheme simplicity stems from its low computational complexity (only
cross-correlation and comparison operations are required) and its independence
of the transmitted signal structure (does not require information about the
transmitted signal). The proposed scheme requires a receiver with switched beam
antenna array, which can be attached to a single radio frequency chain through
phase shifters, hence, its hardware friendliness. The high efficiency of our
system can be observed by comparing its performance with the literature's best
performing MUSIC algorithm. The comparison demonstrates that our scheme
outperforms the MUSIC algorithm, specially at low SNR levels. Moreover, the
number of sources that can be detected using our scheme is bound by the number
of switched beams, rather than the number of antenna elements in the case of
the MUSIC algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5752</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5752</id><created>2014-09-19</created><authors><author><keyname>Derbel</keyname><forenames>Bilel</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Brockhoff</keyname><forenames>Dimo</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Liefooghe</keyname><forenames>Arnaud</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Verel</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LISIC</affiliation></author></authors><title>On the Impact of Multiobjective Scalarizing Functions</title><categories>cs.AI</categories><comments>appears in Parallel Problem Solving from Nature - PPSN XIII,
  Ljubljana : Slovenia (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a renewed interest in decomposition-based approaches
for evolutionary multiobjective optimization. However, the impact of the choice
of the underlying scalarizing function(s) is still far from being well
understood. In this paper, we investigate the behavior of different scalarizing
functions and their parameters. We thereby abstract firstly from any specific
algorithm and only consider the difficulty of the single scalarized problems in
terms of the search ability of a (1+lambda)-EA on biobjective NK-landscapes.
Secondly, combining the outcomes of independent single-objective runs allows
for more general statements on set-based performance measures. Finally, we
investigate the correlation between the opening angle of the scalarizing
function's underlying contour lines and the position of the final solution in
the objective space. Our analysis is of fundamental nature and sheds more light
on the key characteristics of multiobjective scalarizing functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5757</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5757</id><created>2014-09-19</created><authors><author><keyname>Asai</keyname><forenames>Ryo</forenames></author><author><keyname>Vladimirov</keyname><forenames>Andrey</forenames></author></authors><title>Intel Cilk Plus for Complex Parallel Algorithms: &quot;Enormous Fast Fourier
  Transform&quot; (EFFT) Library</title><categories>cs.MS cs.DC cs.DS cs.PF</categories><comments>17 pages. Submitted to Parallel Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we demonstrate the methodology for parallelizing the
computation of large one-dimensional discrete fast Fourier transforms (DFFTs)
on multi-core Intel Xeon processors. DFFTs based on the recursive Cooley-Tukey
method have to control cache utilization, memory bandwidth and vector hardware
usage, and at the same time scale across multiple threads or compute nodes. Our
method builds on single-threaded Intel Math Kernel Library (MKL) implementation
of DFFT, and uses the Intel Cilk Plus framework for thread parallelism. We
demonstrate the ability of Intel Cilk Plus to handle parallel recursion with
nested loop-centric parallelism without tuning the code to the number of cores
or cache metrics. The result of our work is a library called EFFT that performs
1D DFTs of size 2^N for N&gt;=21 faster than the corresponding Intel MKL parallel
DFT implementation by up to 1.5x, and faster than FFTW by up to 2.5x. The code
of EFFT is available for free download under the GPLv3 license. This work
provides a new efficient DFFT implementation, and at the same time demonstrates
an educational example of how computer science problems with complex parallel
patterns can be optimized for high performance using the Intel Cilk Plus
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5758</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5758</id><created>2014-09-19</created><authors><author><keyname>Bevacqua</keyname><forenames>Elisabetta</forenames><affiliation>CERV, Lab-STICC</affiliation></author><author><keyname>Igor</keyname><forenames>Sankovic</forenames><affiliation>CERV, Lab-STICC</affiliation></author><author><keyname>Ayoub</keyname><forenames>Maatalaoui</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>N&#xe9;d&#xe9;lec</keyname><forenames>A.</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>De Loor</keyname><forenames>Pierre</forenames><affiliation>CERV, Lab-STICC</affiliation></author></authors><title>Effects of Coupling in Human-Virtual Agent Body Interaction</title><categories>cs.HC cs.AI cs.GR</categories><comments>appears in Intelligent Virtual Agents, 14th International Conference,
  IVA 2014, Boston : \'Etats-Unis (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a study of the dynamic coupling between a user and a
virtual character during body interaction. Coupling is directly linked with
other dimensions, such as co-presence, engagement, and believability, and was
measured in an experiment that allowed users to describe their subjective
feelings about those dimensions of interest. The experiment was based on a
theatrical game involving the imitation of slow upper-body movements and the
proposal of new movements by the user and virtual agent. The agent's behaviour
varied in autonomy: the agent could limit itself to imitating the user's
movements only, initiate new movements, or combine both behaviours. After the
game, each participant completed a questionnaire regarding their engagement in
the interaction, their subjective feeling about the co-presence of the agent,
etc. Based on four main dimensions of interest, we tested several hypotheses
against our experimental results, which are discussed here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5760</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5760</id><created>2014-09-02</created><authors><author><keyname>Kumar</keyname><forenames>Surender</forenames></author><author><keyname>Prateek</keyname><forenames>Manish</forenames></author><author><keyname>Bhushan</keyname><forenames>Bharat</forenames></author></authors><title>Distance based (DBCP) Cluster Protocol for Heterogeneous Wireless Sensor
  Network</title><categories>cs.NI</categories><comments>6 Pages, 4 Figures,
  http://www.ijcaonline.org/archives/volume76/number9/2013</comments><doi>10.5120/13278-0877</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Clustering is an important concept to reduce the energy consumption and
prolonging the life of a wireless sensor network. In heterogeneous wireless
sensor network some of the nodes are equipped with more energy than the other
nodes.
  Many routing algorithms are proposed for heterogeneous wireless sensor
network. Stable Election Protocol (SEP) is one of the important protocol in
this category. In this research paper a novel energy efficient distance based
cluster protocol (DBCP) is proposed for single hop heterogeneous wireless
sensor network to increase the life and energy efficiency of a sensor network.
DBCP use the average distance of the sensor from the base station as the major
issue for the selection of a cluster head in the sensor network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5763</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5763</id><created>2014-09-19</created><updated>2014-09-26</updated><authors><author><keyname>Xu</keyname><forenames>Jin</forenames></author><author><keyname>He</keyname><forenames>Haibo</forenames></author><author><keyname>Man</keyname><forenames>Hong</forenames></author></authors><title>Active Dictionary Learning in Sparse Representation Based Classification</title><categories>cs.CV</categories><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representation, which uses dictionary atoms to reconstruct input
vectors, has been studied intensively in recent years. A proper dictionary is a
key for the success of sparse representation. In this paper, an active
dictionary learning (ADL) method is introduced, in which classification error
and reconstruction error are considered as the active learning criteria in
selection of the atoms for dictionary construction. The learned dictionaries
are caculated in sparse representation based classification (SRC). The
classification accuracy and reconstruction error are used to evaluate the
proposed dictionary learning method. The performance of the proposed dictionary
learning method is compared with other methods, including unsupervised
dictionary learning and whole-training-data dictionary. The experimental
results based on the UCI data sets and face data set demonstrate the efficiency
of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5774</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5774</id><created>2014-09-03</created><authors><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Soria</keyname><forenames>Daniele</forenames></author><author><keyname>Gibson</keyname><forenames>Jack E.</forenames></author><author><keyname>Hubbard</keyname><forenames>Richard B.</forenames></author></authors><title>Attributes for Causal Inference in Longitudinal Observational Databases</title><categories>cs.CE cs.LG</categories><comments>The 26th IEEE International Symposium on Computer-Based Medical
  Systems, Porto, pp. 548 - 549, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pharmaceutical industry is plagued by the problem of side effects that
can occur anytime a prescribed medication is ingested. There has been a recent
interest in using the vast quantities of medical data available in longitudinal
observational databases to identify causal relationships between drugs and
medical events. Unfortunately the majority of existing post marketing
surveillance algorithms measure how dependant or associated an event is on the
presence of a drug rather than measuring causality. In this paper we
investigate potential attributes that can be used in causal inference to
identify side effects based on the Bradford-Hill causality criteria. Potential
attributes are developed by considering five of the causality criteria and
feature selection is applied to identify the most suitable of these attributes
for detecting side effects. We found that attributes based on the specificity
criterion may improve side effect signalling algorithms but the experiment and
dosage criteria attributes investigated in this paper did not offer sufficient
additional information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5783</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5783</id><created>2014-09-19</created><authors><author><keyname>Butler</keyname><forenames>Brian K.</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author></authors><title>LDPC Code Density Evolution in the Error Floor Region</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper explores density evolution (DE) for low-density parity-check
(LDPC) codes at signal-to-noise-ratios (SNRs) that are significantly above the
decoding threshold. The focus is on the additive white Gaussian noise channel
and LDPC codes in which the variable nodes have regular degree.
  Prior work, using DE, produced results in the error floor region which were
asymptotic in the belief-propagation decoder's log-likelihood ratio (LLR)
values. We develop expressions which closely approximate the LLR growth
behavior at moderate LLR magnitudes. We then produce bounds on the mean
extrinsic check-node LLR values required, as a function of SNR, such that the
growth rate of the LLRs exceeds that of a particular trapping set's internal
LLRs such that its error floor contribution may be eliminated. We find that our
predictions for the mean LLRs to be accurate in the error floor region, but the
predictions for the LLR variance to be lacking beyond several initial
iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5786</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5786</id><created>2014-09-22</created><updated>2015-10-02</updated><authors><author><keyname>Peng</keyname><forenames>Xi</forenames></author><author><keyname>Yan</keyname><forenames>Rui</forenames></author><author><keyname>Zhao</keyname><forenames>Bo</forenames></author><author><keyname>Tang</keyname><forenames>Huajin</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author></authors><title>Fast Low-rank Representation based Spatial Pyramid Matching for Image
  Classification</title><categories>cs.CV</categories><comments>accepted into knowledge based systems, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial Pyramid Matching (SPM) and its variants have achieved a lot of
success in image classification. The main difference among them is their
encoding schemes. For example, ScSPM incorporates Sparse Code (SC) instead of
Vector Quantization (VQ) into the framework of SPM. Although the methods
achieve a higher recognition rate than the traditional SPM, they consume more
time to encode the local descriptors extracted from the image. In this paper,
we propose using Low Rank Representation (LRR) to encode the descriptors under
the framework of SPM. Different from SC, LRR considers the group effect among
data points instead of sparsity. Benefiting from this property, the proposed
method (i.e., LrrSPM) can offer a better performance. To further improve the
generalizability and robustness, we reformulate the rank-minimization problem
as a truncated projection problem. Extensive experimental studies show that
LrrSPM is more efficient than its counterparts (e.g., ScSPM) while achieving
competitive recognition rates on nine image data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5816</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5816</id><created>2014-09-19</created><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author><author><keyname>Derka</keyname><forenames>Martin</forenames></author></authors><title>1-String CZ-Representation of Planar Graphs</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove that every planar 4-connected graph has a
CZ-representation---a string representation using paths in a rectangular grid
that contain at most one vertical segment. Furthermore, two paths representing
vertices $u,v$ intersect precisely once whenever there is an edge between $u$
and $v$. The required size of the grid is $n \times 2n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5826</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5826</id><created>2014-09-19</created><updated>2015-01-10</updated><authors><author><keyname>Brock</keyname><forenames>Benjamin</forenames></author><author><keyname>Belt</keyname><forenames>Andrew</forenames></author><author><keyname>Billings</keyname><forenames>Jay Jay</forenames></author><author><keyname>Guidry</keyname><forenames>Mike</forenames></author></authors><title>Explicit Integration with GPU Acceleration for Large Kinetic Networks</title><categories>physics.comp-ph astro-ph.SR cs.OH</categories><comments>20 pages, 8 figures, submitted to Journal of Computational Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the first implementation of recently-developed fast explicit
kinetic integration algorithms on modern graphics processing unit (GPU)
accelerators. Taking as a generic test case a Type Ia supernova explosion with
an extremely stiff thermonuclear network having 150 isotopic species and 1604
reactions coupled to hydrodynamics using operator splitting, we demonstrate the
capability to solve of order 100 realistic kinetic networks in parallel in the
same time that standard implicit methods can solve a single such network on a
CPU. This orders-of-magnitude decrease in compute time for solving systems of
realistic kinetic networks implies that important coupled, multiphysics
problems in various scientific and technical fields that were intractible, or
could be simulated only with highly schematic kinetic networks, are now
computationally feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5828</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5828</id><created>2014-09-19</created><updated>2015-08-03</updated><authors><author><keyname>Mlika</keyname><forenames>Zoubeir</forenames></author><author><keyname>Goonewardena</keyname><forenames>Mathew</forenames></author><author><keyname>Ajib</keyname><forenames>Wessam</forenames></author><author><keyname>Elbiaze</keyname><forenames>Halima</forenames></author></authors><title>User-Base Station Association in HetSNets: Complexity and Efficient
  Algorithms</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the problem of user association to small-cell base
stations (SBSs) in a heterogeneous and small-cell network (HetSNet). Two
optimization problems are investigated, which are maximizing the set of
associated users to the SBSs (the unweighted problem) and maximizing the set of
weighted associated users to the SBSs (the weighted problem), under
signal-to-interference-plus-noise ratio (SINR) constraints. Both problems are
formulated as linear integer programs. The weighted problem is known to be
NP-hard and, in this paper, the unweighted problem is proved to be NP-hard as
well. Therefore, this paper develops two heuristic polynomial-time algorithms
to solve both problems. The computational complexity of the proposed algorithms
is evaluated and is shown to be far more efficient than the complexity of the
optimal brute-force (BF) algorithm. Moreover, the paper benchmarks the
performance of the proposed algorithms against the BF algorithm, the
branch-and-bound (B\&amp;B) algorithm and standard algorithms, through numerical
simulations. The results demonstrate the close-to-optimal performance of the
proposed algorithms. They also show that the weighted problem can be solved to
provide solutions that are fair between users or to balance the load among
SBSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5834</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5834</id><created>2014-09-19</created><authors><author><keyname>Globerson</keyname><forenames>Amir</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author><author><keyname>Sontag</keyname><forenames>David</forenames></author><author><keyname>Yildirim</keyname><forenames>Cafer</forenames></author></authors><title>Tight Error Bounds for Structured Prediction</title><categories>cs.LG cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured prediction tasks in machine learning involve the simultaneous
prediction of multiple labels. This is typically done by maximizing a score
function on the space of labels, which decomposes as a sum of pairwise
elements, each depending on two specific labels. Intuitively, the more pairwise
terms are used, the better the expected accuracy. However, there is currently
no theoretical account of this intuition. This paper takes a significant step
in this direction.
  We formulate the problem as classifying the vertices of a known graph
$G=(V,E)$, where the vertices and edges of the graph are labelled and correlate
semi-randomly with the ground truth. We show that the prospects for achieving
low expected Hamming error depend on the structure of the graph $G$ in
interesting ways. For example, if $G$ is a very poor expander, like a path,
then large expected Hamming error is inevitable. Our main positive result shows
that, for a wide class of graphs including 2D grid graphs common in machine
vision applications, there is a polynomial-time algorithm with small and
information-theoretically near-optimal expected error. Our results provide a
first step toward a theoretical justification for the empirical success of the
efficient approximate inference algorithms that are used for structured
prediction in models where exact inference is intractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5839</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5839</id><created>2014-09-19</created><authors><author><keyname>Long</keyname><forenames>Ying</forenames></author><author><keyname>Liu</keyname><forenames>Xingjian</forenames></author><author><keyname>Zhou</keyname><forenames>Jiangping</forenames></author><author><keyname>Gu</keyname><forenames>Yizhen</forenames></author></authors><title>Profiling underprivileged residents with mid-term public transit
  smartcard data of Beijing</title><categories>cs.OH</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobility of economically underprivileged residents in China has seldom been
well profiled due to privacy issue and the characteristics of Chinese over
poverty. In this paper, we identify and characterize underprivileged residents
in Beijing using ubiquitous public transport smartcard transactions in 2008 and
2010, respectively. We regard these frequent bus/metro riders (FRs) in China,
especially in Beijing, as economically underprivileged residents. Our argument
is tested against (1) the household travel survey in 2010, (2) a small-scale
survey in 2012, as well as (3) our interviews with local residents in Beijing.
Cardholders' job and residence locations are identified using Smart Card Data
(SCD) in 2008 and 2010. Our analysis is restricted to cardholders that use the
same cards in both years. We then classify all identified FRs into 20 groups by
residence changes (change, no change), workplace changes (change, no change,
finding a job, losing a job, and all-time employed) during 2008-2010 and
housing place in 2010 (within the fourth ring road or not). The underprivileged
degree of each FR is then evaluated using the 2014 SCD. To the best of our
knowledge, this is one of the first studies for understanding long- or mid-term
urban dynamics using immediate &quot;big data&quot;, and also for profiling
underprivileged residents in Beijing in a fine-scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5841</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5841</id><created>2014-09-19</created><authors><author><keyname>Noyen</keyname><forenames>Kay</forenames></author><author><keyname>Volland</keyname><forenames>Dirk</forenames></author><author><keyname>W&#xf6;rner</keyname><forenames>Dominic</forenames></author><author><keyname>Fleisch</keyname><forenames>Elgar</forenames></author></authors><title>When Money Learns to Fly: Towards Sensing as a Service Applications
  Using Bitcoin</title><categories>cs.CY</categories><comments>6 pages, 1 figure, 1 table</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Sensing-as-a-Service (S2aaS) is an emerging Internet of Things (IOT) business
model pattern. To be technically feasible and to effectively allow for broad
adoption, S2aaS implementations have to overcome manifold systemic hurdles,
specifically regarding payment and sensor identification. In an effort to
overcome these hurdles, we propose Bitcoin as protocol for S2aaS networks. To
lay the groundwork and start the conversation about disruptive changes that
Bitcoin technology could bring to S2aaS concepts and IOT in general, we
identify and discuss the core characteristics that could drive those changes.
We present a conceptual example and describe the basic process of exchanging
data for cash using Bitcoin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5844</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5844</id><created>2014-09-19</created><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Key Capacity for Product Sources with Application to Stationary Gaussian
  Processes</title><categories>cs.IT cs.CR math.IT</categories><comments>Presented in part at 2014 IEEE International Symposium on Information
  Theory (ISIT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for product sources, rate splitting is optimal for secret key
agreement using limited one-way communication between two terminals. This
yields an alternative proof of the tensorization property of a strong data
processing inequality originally studied by Erkip and Cover and amended
recently by Anantharam et al. We derive a &quot;water-filling&quot; solution of the
communication-rate--key-rate tradeoff for a wide class of discrete memoryless
vector Gaussian sources which subsumes the case without an eavesdropper.
Moreover, we derive an explicit formula for the maximum &quot;secret key per bit of
communication&quot; for all discrete memoryless vector Gaussian sources using a
tensorization property. Finally, a one-shot information spectrum achievability
bound for key generation is proved from which we characterize the
communication-rate--key-rate tradeoff for stationary Gaussian processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5845</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5845</id><created>2014-09-19</created><updated>2015-03-13</updated><authors><author><keyname>Henderson</keyname><forenames>Marie</forenames></author><author><keyname>Page</keyname><forenames>Howard Philip</forenames></author></authors><title>Planning Security Services for IT Systems</title><categories>cs.CR cs.CY</categories><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Often the hardest job is to get business representatives to look at security
as something that makes managing their risks and achieving their objectives
easier, with security compliance as just part of that journey. This paper
addresses that by making planning for security services a 'business tool'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5865</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5865</id><created>2014-09-20</created><authors><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author></authors><title>Homotopy Bisimilarity for Higher-Dimensional Automata</title><categories>cs.LO math.CT</categories><comments>Heavily revised version of arXiv:1209.4927</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new category of higher-dimensional automata in which the
morphisms are functional homotopy simulations, i.e. functional simulations up
to concurrency of independent events. For this, we use unfoldings of
higher-dimensional automata into higher-dimensional trees. Using a notion of
open maps in this category, we define homotopy bisimilarity. We show that
homotopy bisimilarity is equivalent to a straight-forward generalization of
standard bisimilarity to higher dimensions, and that it is finer than split
bisimilarity and incomparable with history-preserving bisimilarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5872</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5872</id><created>2014-09-20</created><authors><author><keyname>Schrammel</keyname><forenames>Peter</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Brain</keyname><forenames>Martin</forenames></author><author><keyname>Martins</keyname><forenames>Ruben</forenames></author><author><keyname>Teige</keyname><forenames>Tino</forenames></author><author><keyname>Bienm&#xfc;ller</keyname><forenames>Tom</forenames></author></authors><title>Incremental Bounded Model Checking for Embedded Software (extended
  version)</title><categories>cs.SE</categories><comments>extended version of paper submitted to EMSOFT'14</comments><acm-class>D.2.4; D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program analysis is on the brink of mainstream in embedded systems
development. Formal verification of behavioural requirements, finding runtime
errors and automated test case generation are some of the most common
applications of automated verification tools based on Bounded Model Checking.
Existing industrial tools for embedded software use an off-the-shelf Bounded
Model Checker and apply it iteratively to verify the program with an increasing
number of unwindings. This approach unnecessarily wastes time repeating work
that has already been done and fails to exploit the power of incremental SAT
solving. This paper reports on the extension of the software model checker CBMC
to support incremental Bounded Model Checking and its successful integration
with the industrial embedded software verification tool BTC EmbeddedTester. We
present an extensive evaluation over large industrial embedded programs, which
shows that incremental Bounded Model Checking cuts runtimes by one order of
magnitude in comparison to the standard non-incremental approach, enabling the
application of formal verification to large and complex embedded software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5886</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5886</id><created>2014-09-20</created><authors><author><keyname>Joudeh</keyname><forenames>Hamdi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>AMMSE Optimization for Multiuser MISO Systems with Imperfect CSIT and
  Perfect CSIR</title><categories>cs.IT math.IT</categories><comments>IEEE Global Communications Conference (GLOBECOM) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the design of robust linear precoders for MU-MISO
systems where users have perfect Channel State Information (CSI) while the BS
has partial CSI. In particular, the BS has access to imperfect estimates of the
channel vectors, in addition to the covariance matrices of the estimation error
vectors. A closed-form expression for the Average Minimum Mean Square Error
(AMMSE) is obtained using the second order Taylor Expansion. This approximation
is used to formulate two fairness-based robust design problems: a maximum
AMMSE-constrained problem and a power-constrained problem. We propose an
algorithm based on convex optimization techniques to address the first problem,
while the second problem is tackled by exploiting the close relationship
between the two problems, in addition to their monotonic natures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5887</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5887</id><created>2014-09-20</created><authors><author><keyname>Sinha</keyname><forenames>Tanmay</forenames></author><author><keyname>Li</keyname><forenames>Nan</forenames></author><author><keyname>Jermann</keyname><forenames>Patrick</forenames></author><author><keyname>Dillenbourg</keyname><forenames>Pierre</forenames></author></authors><title>Capturing &quot;attrition intensifying&quot; structural traits from didactic
  interaction sequences of MOOC learners</title><categories>cs.CY cs.LG cs.SI</categories><comments>&quot;Shared Task&quot; submission for EMNLP 2014 Workshop on Modeling Large
  Scale Social Interaction in Massively Open Online Courses</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is an attempt to discover hidden structural configurations in
learning activity sequences of students in Massive Open Online Courses (MOOCs).
Leveraging combined representations of video clickstream interactions and forum
activities, we seek to fundamentally understand traits that are predictive of
decreasing engagement over time. Grounded in the interdisciplinary field of
network science, we follow a graph based approach to successfully extract
indicators of active and passive MOOC participation that reflect persistence
and regularity in the overall interaction footprint. Using these rich
educational semantics, we focus on the problem of predicting student attrition,
one of the major highlights of MOOC literature in the recent years. Our results
indicate an improvement over a baseline ngram based approach in capturing
&quot;attrition intensifying&quot; features from the learning activities that MOOC
learners engage in. Implications for some compelling future research are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5900</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5900</id><created>2014-09-20</created><authors><author><keyname>Feldman</keyname><forenames>Moran</forenames></author></authors><title>Maximizing Symmetric Submodular Functions</title><categories>cs.DS</categories><comments>21 pages</comments><msc-class>68Q25, 68R05, 68W25, 90C27</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetric submodular functions are an important family of submodular
functions capturing many interesting functions including cut functions of
graphs and hypergraphs. Maximization of such functions subject to various
constraints receives little attention by current research, unlike similar
minimization problems which have been widely studied. In this work, we identify
a few submodular maximization problems for which one can get a better
approximation for symmetric objectives than the state of the art approximation
for general submodular functions.
  We first consider the problem of maximizing a non-negative symmetric
submodular function $f\colon 2^\mathcal{N} \to \mathbb{R}^+$ subject to a
down-monotone solvable polytope $\mathcal{P} \in [0, 1]^\mathcal{N}$. For this
problem we describe an algorithm producing a fractional solution of value at
least $0.432 \cdot f(OPT)$, where $OPT$ is the optimal \emph{integral}
solution. Our second result considers the problem $\max \{f(S) : |S| = k\}$ for
a non-negative symmetric submodular function $f\colon 2^\mathcal{N} \to
\mathbb{R}^+$. For this problem, we give an approximation ratio that depends on
the value $k / |\mathcal{N}|$ and is always at least $0.432$. Our method can
also be applied to non-negative non-symmetric submodular function, in which
case it produces $1/e - o(1)$ approximation, improving over the best known
result for this problem. Finally, we describe a deterministic linear-time
$1/2$-approximation algorithm for unconstrained maximization of a non-negative
symmetric submodular function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5907</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5907</id><created>2014-09-20</created><updated>2014-10-08</updated><authors><author><keyname>Batra</keyname><forenames>Nipun</forenames></author><author><keyname>Gulati</keyname><forenames>Manoj</forenames></author><author><keyname>Jain</keyname><forenames>Puneet</forenames></author><author><keyname>Whitehouse</keyname><forenames>Kamin</forenames></author><author><keyname>Singh</keyname><forenames>Amarjeet</forenames></author></authors><title>Poster Abstract: Bits and Watts: Improving energy disaggregation
  performance using power line communication modems</title><categories>cs.OH</categories><doi>10.1145/2674061.2675039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-intrusive load monitoring (NILM) or energy disaggregation, aims to
disaggregate a household's electricity consumption into constituent appliances.
More than three decades of work in NILM has resulted in the development of
several novel algorithmic approaches. However, despite these advancements, two
core challenges still exist: i) disaggregating low power consumption appliances
and ii) distinguishing between multiple instances of similar appliances. These
challenges are becoming increasingly important due to an increasing number of
appliances and increased usage of electronics in homes. Previous approaches
have attempted to solve these problems using expensive hardware involving high
sampling rates better suited to laboratory settings, or using additional number
of sensors, limiting the ease of deployment. In this work, we explore using
commercial-off-the-shelf (COTS) power line communication (PLC) modems as an
inexpensive and easy to deploy alternative solution to these problems. We use
the reduction in bandwidth between two PLC modems, caused due to the change in
PLC modulation scheme when different appliances are operated as a signature for
an appliance. Since the noise generated in the powerline is dependent both on
type and location of an appliance, we believe that our technique based on PLC
modems can be a promising addition for solving NILM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5908</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5908</id><created>2014-09-20</created><updated>2014-11-09</updated><authors><author><keyname>Kelly</keyname><forenames>Jack</forenames></author><author><keyname>Batra</keyname><forenames>Nipun</forenames></author><author><keyname>Parson</keyname><forenames>Oliver</forenames></author><author><keyname>Dutta</keyname><forenames>Haimonti</forenames></author><author><keyname>Knottenbelt</keyname><forenames>William</forenames></author><author><keyname>Rogers</keyname><forenames>Alex</forenames></author><author><keyname>Singh</keyname><forenames>Amarjeet</forenames></author><author><keyname>Srivastava</keyname><forenames>Mani</forenames></author></authors><title>Demo Abstract: NILMTK v0.2: A Non-intrusive Load Monitoring Toolkit for
  Large Scale Data Sets</title><categories>cs.OH</categories><comments>1st ACM International Conference on Embedded Systems For
  Energy-Efficient Buildings, 2014</comments><doi>10.1145/2674061.2675024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this demonstration, we present an open source toolkit for evaluating
non-intrusive load monitoring research; a field which aims to disaggregate a
household's total electricity consumption into individual appliances. The
toolkit contains: a number of importers for existing public data sets, a set of
preprocessing and statistics functions, a benchmark disaggregation algorithm
and a set of metrics to evaluate the performance of such algorithms.
Specifically, this release of the toolkit has been designed to enable the use
of large data sets by only loading individual chunks of the whole data set into
memory at once for processing, before combining the results of each chunk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5909</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5909</id><created>2014-09-20</created><authors><author><keyname>Damanik</keyname><forenames>David</forenames><affiliation>Rice University</affiliation></author></authors><title>Finite Automata With Restricted Two-Way Motion</title><categories>cs.FL</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider finite two-way automata and measure the use of two-way motion by
counting the number of left moves in accepting computations. Restriction of the
automata according to this measure allows us to study in detail the use of
two-way motion for the acceptance of regular languages in terms of state
complexity. The two-way spectrum of a given regular language is introduced.
This quantity reflects the change of size of minimal accepting devices if the
use of two-way motion is increased incrementally. We give examples for spectra,
prove uniform upper and lower bounds and study their sharpness. We also have
state complexity results for two-way automata with uniformly bounded use of
two-way motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5911</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5911</id><created>2014-09-20</created><updated>2015-01-07</updated><authors><author><keyname>Cao</keyname><forenames>X.</forenames></author><author><keyname>Saez</keyname><forenames>Y.</forenames></author><author><keyname>Pesti</keyname><forenames>G.</forenames></author><author><keyname>Kish</keyname><forenames>L. B.</forenames></author></authors><title>On KLJN-based secure key distribution in vehicular communication
  networks</title><categories>cs.CR</categories><comments>Accepted for publication</comments><journal-ref>Fluct. Noise Lett., Vol. 14, No. 1 (2015) 1550008 (11 pages)</journal-ref><doi>10.1142/S021947751550008X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a former paper [Fluct. Noise Lett., 13 (2014) 1450020] we introduced a
vehicular communication system with unconditionally secure key exchange based
on the Kirchhoff-Law-Johnson-Noise (KLJN) key distribution scheme. In this
paper, we address the secure KLJN key donation to vehicles. This KLJN key
donation solution is performed lane-by-lane by using roadside key provider
equipment embedded in the pavement. A method to compute the lifetime of the
KLJN key is also given. This key lifetime depends on the car density and gives
an upper limit of the lifetime of the KLJN key for vehicular communication
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5913</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5913</id><created>2014-09-20</created><authors><author><keyname>Hattab</keyname><forenames>Ghaith</forenames></author><author><keyname>Ibnkahla</keyname><forenames>Mohammed</forenames></author></authors><title>Multiband Spectrum Access: Great Promises for Future Cognitive Radio
  Networks</title><categories>cs.IT math.IT</categories><comments>22 pages, 13 figures; published in the Proceedings of the IEEE
  Journal, Special Issue on Future Radio Spectrum Access, March 2014</comments><journal-ref>Proceedings of the IEEE, vol.102, no.3,pp.282-306, March 2014</journal-ref><doi>10.1109/JPROC.2014.2303977</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio has been widely considered as one of the prominent solutions
to tackle the spectrum scarcity. While the majority of existing research has
focused on single-band cognitive radio, multiband cognitive radio represents
great promises towards implementing efficient cognitive networks compared to
single-based networks. Multiband cognitive radio networks (MB-CRNs) are
expected to significantly enhance the network's throughput and provide better
channel maintenance by reducing handoff frequency. Nevertheless, the wideband
front-end and the multiband spectrum access impose a number of challenges yet
to overcome. This paper provides an in-depth analysis on the recent
advancements in multiband spectrum sensing techniques, their limitations, and
possible future directions to improve them. We study cooperative communications
for MB-CRNs to tackle a fundamental limit on diversity and sampling. We also
investigate several limits and tradeoffs of various design parameters for
MB-CRNs. In addition, we explore the key MB-CRNs performance metrics that
differ from the conventional metrics used for single-band based networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5922</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5922</id><created>2014-09-20</created><authors><author><keyname>Stolee</keyname><forenames>Derrick</forenames></author></authors><title>Automated Discharging Arguments for Density Problems in Grids</title><categories>cs.DM math.CO</categories><comments>This is an extended abstract, with 10 pages, 2 appendices, 5 tables,
  and 2 figures</comments><msc-class>68R10, 05C69, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discharging arguments demonstrate a connection between local structure and
global averages. This makes it an effective tool for proving lower bounds on
the density of special sets in infinite grids. However, the minimum density of
an identifying code in the hexagonal grid remains open, with an upper bound of
$\frac{3}{7} \approx 0.428571$ and a lower bound of $\frac{5}{12}\approx
0.416666$. We present a new, experimental framework for producing discharging
arguments using an algorithm. This algorithm replaces the lengthy case analysis
of human-written discharging arguments with a linear program that produces the
best possible lower bound using the specified set of discharging rules. We use
this framework to present a lower bound of $\frac{23}{55} \approx 0.418181$ on
the density of an identifying code in the hexagonal grid, and also find several
sharp lower bounds for variations on identifying codes in the hexagonal,
square, and triangular grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5930</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5930</id><created>2014-09-20</created><updated>2014-11-10</updated><authors><author><keyname>Varn</keyname><forenames>Dowman P.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Chaotic Crystallography: How the physics of information reveals
  structural order in materials</title><categories>cond-mat.mtrl-sci cond-mat.dis-nn cs.FL cs.IT math.IT nlin.CD</categories><comments>9 pages, two figures, 1 table;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/ChemOpinion.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review recent progress in applying information- and computation-theoretic
measures to describe material structure that transcends previous methods based
on exact geometric symmetries. We discuss the necessary theoretical background
for this new toolset and show how the new techniques detect and describe novel
material properties. We discuss how the approach relates to well known
crystallographic practice and examine how it provides novel interpretations of
familiar structures. Throughout, we concentrate on disordered materials that,
while important, have received less attention both theoretically and
experimentally than those with either periodic or aperiodic order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5932</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5932</id><created>2014-09-20</created><authors><author><keyname>Zhang</keyname><forenames>L.</forenames></author><author><keyname>Cai</keyname><forenames>Y.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Zhao</keyname><forenames>M.</forenames></author></authors><title>Robust Multi-Branch Tomlinson-Harashima Precoding in Cooperative MIMO
  Relay Systems</title><categories>cs.IT math.IT</categories><comments>14 pages, 9 figures, IEEE Transactions on Communications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the design of robust transceivers with
Tomlinson-Harashima precoding (THP) for multiple-input multiple-output (MIMO)
relay systems with amplify-and-forward (AF) protocols based on a multi-branch
(MB) strategy. The MB strategy employs successive interference cancellation
(SIC) on several parallel branches which are equipped with different ordering
patterns so that each branch produces transmit signals by exploiting a certain
ordering pattern. For each parallel branch, the proposed robust nonlinear
transceiver design consists of THP at the source along with a linear precoder
at the relay and a linear minimum-mean-squared-error (MMSE) receiver at the
destination. By taking the channel uncertainties into account, the source and
relay precoders are jointly optimised to minimise the mean-squared-error (MSE).
We then employ a diagonalization method along with some attributes of
matrix-monotone functions to convert the optimization problem with matrix
variables into an optimization problem with scalar variables. We resort to an
iterative method to obtain the solution for the relay and the source precoders
via Karush-Kuhn-Tucker (KKT) conditions. An appropriate selection rule is
developed to choose the nonlinear transceiver corresponding to the best branch
for data transmission. Simulation results demonstrate that the proposed MB-THP
scheme is capable of alleviating the effects of channel state information (CSI)
errors and improving the robustness of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5937</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5937</id><created>2014-09-20</created><updated>2015-02-07</updated><authors><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Distributed Robust Learning</title><categories>stat.ML cs.LG</categories><comments>18 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework for distributed robust statistical learning on {\em
big contaminated data}. The Distributed Robust Learning (DRL) framework can
reduce the computational time of traditional robust learning methods by several
orders of magnitude. We analyze the robustness property of DRL, showing that
DRL not only preserves the robustness of the base robust learning method, but
also tolerates contaminations on a constant fraction of results from computing
nodes (node failures). More precisely, even in presence of the most adversarial
outlier distribution over computing nodes, DRL still achieves a breakdown point
of at least $ \lambda^*/2 $, where $ \lambda^* $ is the break down point of
corresponding centralized algorithm. This is in stark contrast with naive
division-and-averaging implementation, which may reduce the breakdown point by
a factor of $ k $ when $ k $ computing nodes are used. We then specialize the
DRL framework for two concrete cases: distributed robust principal component
analysis and distributed robust regression. We demonstrate the efficiency and
the robustness advantages of DRL through comprehensive simulations and
predicting image tags on a large-scale image set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5942</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5942</id><created>2014-09-21</created><authors><author><keyname>Sobh</keyname><forenames>Tarek S.</forenames></author><author><keyname>Khalil</keyname><forenames>Awad H.</forenames></author></authors><title>IP Tracing and Active Network Response</title><categories>cs.CR cs.NI</categories><comments>11 pages, 2 figures, 12th International Conference on Artificial
  Intelligence Applications (ICAIA 2004), Cairo, Egypt, February 18-20, 2004</comments><acm-class>C.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active security is mainly concerned with performing one or more security
functions when a host in a communication network is subject to an attack. Such
security functions include appropriate actions against attackers. To properly
afford active security actions a set of software subsystems should be
integrated together so that they can automatically detect and appropriately
address any vulnerability in the underlying network. This work presents
integrated model for active security response model. The proposed model
introduces Active Response Mechanism (ARM) for tracing anonymous attacks in the
network back to their source. This work is motivated by the increased frequency
and sophistication of denial-of-service attacks and by the difficulty in
tracing packets with incorrect, or &quot;spoofed&quot;, source addresses. This paper
presents within the proposed model two tracing approaches based on:
  1.Sleepy Watermark Tracing (SWT) for unauthorized access attacks.
  2.Probabilistic Packet Marking (PPM) in the network for Denial of Service
(DoS) and Distributed Denial of Service (DDoS) attacks. On the basis of the
proposed model a cooperative network security tools such as firewall, intrusion
detection system with IP tracing mechanism has been designed for taking a rapid
active response against real IPs for attackers. The proposed model is able to
detect network vulnerabilities, trace attack source IP and reconfigure the
attacked subnetworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5944</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5944</id><created>2014-09-21</created><updated>2014-11-19</updated><authors><author><keyname>Gusfield</keyname><forenames>Dan</forenames></author></authors><title>G\&quot;odel for Goldilocks: A Rigorous, Streamlined Proof of (a variant of)
  G\&quot;odel's First Incompleteness Theorem</title><categories>math.LO cs.LO</categories><comments>Version 2 corrects typos and one definition in the first version, and
  expands or contracts parts of the exposition, but the main content remains
  the same. Version 3 removes an unnecessary comment in Version 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most discussions of G\&quot;odel's theorems fall into one of two types: either
they emphasize perceived philosophical, cultural &quot;meanings&quot; of the theorems,
and perhaps sketch some of the ideas of the proofs, usually relating G\&quot;odel's
proofs to riddles and paradoxes, but do not attempt to present rigorous,
complete proofs; or they do present rigorous proofs, but in the traditional
style of mathematical logic, with all of its heavy notation and difficult
definitions, and technical issues which reflect G\&quot;odel's original approach and
broader logical issues. Many non-specialists are frustrated by these two
extreme types of expositions and want a complete, rigorous proof that they can
understand. Such an exposition is possible, because many people have realized
that variants of G\&quot;odel's first incompleteness theorem can be rigorously
proved by a simpler middle approach, avoiding philosophical discussions and
hand-waiving at one extreme; and also avoiding the heavy machinery of
traditional mathematical logic, and many of the harder detail's of G\&quot;odel's
original proof, at the other extreme. This is the just-right Goldilocks
approach. In this exposition we give a short, self-contained Goldilocks
exposition of G\&quot;odel's first theorem, aimed at a broad, undergraduate
audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5954</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5954</id><created>2014-09-21</created><authors><author><keyname>Zhang</keyname><forenames>Di</forenames></author><author><keyname>Zhou</keyname><forenames>Zhenyu</forenames></author><author><keyname>Sato</keyname><forenames>Takuro</forenames></author></authors><title>An Energy Efficiency policy for Communications with C-RAN, ICN and
  Transition Smooth</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Towards next generation communications, Energy Efficiency (EE) attracts lots
of attentions nowadays. Some innovative techniques have been proposed in prior
literatures, especially the sleep mechanism of base station (BS). Yet how to
sleep and when to sleep are still vague concepts. Another, most of the studies
focus on the cellular section or core networks separately while integral and
comprehensive version is neglected in prior literatures. In this paper,the
integral optimization structure is studied based on cloud radio network (C-RAN)
and information centric network (ICN) that raised latest combined with the
sleep mode. The original C-RAN and ICN structures are amended in terms of
reality application of sleep techniques. While adopting the sleep techniques
both in core and cellular, apart from previous works, a transition smooth
method that solve the current surge problems which is ignored before is further
proposed. Based on the new method, it will be much more feasible to adopt the
sleep techniques by knowing the appropriate occasion for transition between
sleep and idle mode. Comprehensive computer based simulation results
demonstrate that this integer proposal achieves better EE feature with
negligible impact on quality of service (QoS) of user equipments (UEs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5957</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5957</id><created>2014-09-21</created><updated>2015-02-10</updated><authors><author><keyname>Kovalsky</keyname><forenames>Shahar Z.</forenames></author><author><keyname>Glasner</keyname><forenames>Daniel</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author></authors><title>A Global Approach for Solving Edge-Matching Puzzles</title><categories>cs.CV</categories><journal-ref>SIAM J. Imaging Sciences, Vol. 8, Issue 2, 916--938, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider apictorial edge-matching puzzles, in which the goal is to arrange
a collection of puzzle pieces with colored edges so that the colors match along
the edges of adjacent pieces. We devise an algebraic representation for this
problem and provide conditions under which it exactly characterizes a puzzle.
Using the new representation, we recast the combinatorial, discrete problem of
solving puzzles as a global, polynomial system of equations with continuous
variables. We further propose new algorithms for generating approximate
solutions to the continuous problem by solving a sequence of convex
relaxations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5959</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5959</id><created>2014-09-21</created><updated>2015-05-09</updated><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author></authors><title>Automorphism group of the modified bubble-sort graph</title><categories>math.CO cs.DM math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modified bubble-sort graph of dimension $n$ is the Cayley graph of $S_n$
generated by $n$ cyclically adjacent transpositions. In the present paper, it
is shown that the automorphism group of the modified bubble sort graph of
dimension $n$ is $S_n \times D_{2n}$, for all $n \ge 5$. Thus, a complete
structural description of the automorphism group of the modified bubble-sort
graph is obtained. A similar direct product decomposition is seen to hold for
arbitrary normal Cayley graphs generated by transposition sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5965</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5965</id><created>2014-09-21</created><authors><author><keyname>Ciurana</keyname><forenames>Alex</forenames></author><author><keyname>Martin</keyname><forenames>Vicente</forenames></author><author><keyname>Martinez-Mateo</keyname><forenames>Jesus</forenames></author><author><keyname>Schrenk</keyname><forenames>Bernhard</forenames></author><author><keyname>Peev</keyname><forenames>Momtchil</forenames></author><author><keyname>Poppe</keyname><forenames>Andreas</forenames></author></authors><title>Entanglement Distribution in Optical Networks</title><categories>quant-ph cs.NI physics.optics</categories><comments>26 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to generate entangled photon-pairs over a broad wavelength range
opens the door to the simultaneous distribution of entanglement to multiple
users in a network by using centralized sources and flexible
wavelength-division multiplexing schemes. Here we show the design of a
metropolitan optical network consisting of tree-type access networks whereby
entangled photon-pairs are distributed to any pair of users, independent of
their location. The network is constructed employing commercial off-the-shelf
components and uses the existing infrastructure, which allows for moderate
deployment costs. We further develop a channel plan and a network-architecture
design to provide a direct optical path between any pair of users, thus
allowing classical and one-way quantum communication as well as entanglement
distribution. This allows the simultaneous operation of multiple quantum
information technologies. Finally, we present a more flexible backbone
architecture that pushes away the load limitations of the original network
design by extending its reach, number of users and capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5980</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5980</id><created>2014-09-21</created><authors><author><keyname>Garimella</keyname><forenames>Kiran</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Cin</keyname><forenames>Sonia Dal</forenames></author></authors><title>From &quot;I love you babe&quot; to &quot;leave me alone&quot; - Romantic Relationship
  Breakups on Twitter</title><categories>cs.SI cs.CY</categories><comments>To appear in the 6th International Conference on Social Informatics
  (SocInfo 2014), Barcelona</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use public data from Twitter to study the breakups of the romantic
relationships of 661 couples. Couples are identified through profile references
such as @user1 writing &quot;@user2 is the best boyfriend ever!!&quot;. Using this data
set we find evidence for a number of existing hypotheses describing
psychological processes including (i) pre-relationship closeness being
indicative of post-relationship closeness, (ii) &quot;stonewalling&quot;, i.e., ignoring
messages by a partner, being indicative of a pending breakup, and (iii)
post-breakup depression. We also observe a previously undocumented phenomenon
of &quot;batch un-friending and being un-friended&quot; where users who break up
experience sudden drops of 15-20 followers and friends. Our work shows that
public Twitter data can be used to gain new insights into psychological
processes surrounding relationship dissolutions, something that most people go
through at least once in their lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5987</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5987</id><created>2014-09-21</created><authors><author><keyname>Fang</keyname><forenames>Qizhi</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Jia</forenames></author><author><keyname>Zhang</keyname><forenames>Jialin</forenames></author></authors><title>Computing the Least-core and Nucleolus for Threshold Cardinality
  Matching Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative games provide a framework for fair and stable profit allocation
in multi-agent systems. \emph{Core}, \emph{least-core} and \emph{nucleolus} are
such solution concepts that characterize stability of cooperation. In this
paper, we study the algorithmic issues on the least-core and nucleolus of
threshold cardinality matching games (TCMG). A TCMG is defined on a graph
$G=(V,E)$ and a threshold $T$, in which the player set is $V$ and the profit of
a coalition $S\subseteq V$ is 1 if the size of a maximum matching in $G[S]$
meets or exceeds $T$, and 0 otherwise. We first show that for a TCMG, the
problems of computing least-core value, finding and verifying least-core payoff
are all polynomial time solvable. We also provide a general characterization of
the least core for a large class of TCMG. Next, based on Gallai-Edmonds
Decomposition in matching theory, we give a concise formulation of the
nucleolus for a typical case of TCMG which the threshold $T$ equals $1$. When
the threshold $T$ is relevant to the input size, we prove that the nucleolus
can be obtained in polynomial time in bipartite graphs and graphs with a
perfect matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5993</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5993</id><created>2014-09-21</created><authors><author><keyname>Horowitz</keyname><forenames>Matanya B.</forenames></author><author><keyname>Burdick</keyname><forenames>Joel W.</forenames></author></authors><title>Optimal Navigation Functions for Nonlinear Stochastic Systems</title><categories>cs.RO</categories><comments>Accepted to IROS 2014. 8 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new methodology to craft navigation functions for
nonlinear systems with stochastic uncertainty. The method relies on the
transformation of the Hamilton-Jacobi-Bellman (HJB) equation into a linear
partial differential equation. This approach allows for optimality criteria to
be incorporated into the navigation function, and generalizes several existing
results in navigation functions. It is shown that the HJB and that existing
navigation functions in the literature sit on ends of a spectrum of
optimization problems, upon which tradeoffs may be made in problem complexity.
In particular, it is shown that under certain criteria the optimal navigation
function is related to Laplace's equation, previously used in the literature,
through an exponential transform. Further, analytical solutions to the HJB are
available in simplified domains, yielding guidance towards optimality for
approximation schemes. Examples are used to illustrate the role that noise, and
optimality can potentially play in navigation system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.5995</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.5995</id><created>2014-09-21</created><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>On the strengths of connectivity and robustness in general random
  intersection graphs</title><categories>cs.DM cs.SI math.CO math.PR physics.soc-ph</categories><comments>This paper about random graphs appears in IEEE Conference on Decision
  and Control (CDC) 2014, the premier conference in control theory</comments><msc-class>05C80, 60B20</msc-class><acm-class>G.2.2; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random intersection graphs have received much attention for nearly two
decades, and currently have a wide range of applications ranging from key
predistribution in wireless sensor networks to modeling social networks. In
this paper, we investigate the strengths of connectivity and robustness in a
general random intersection graph model. Specifically, we establish sharp
asymptotic zero-one laws for $k$-connectivity and $k$-robustness, as well as
the asymptotically exact probability of $k$-connectivity, for any positive
integer $k$. The $k$-connectivity property quantifies how resilient is the
connectivity of a graph against node or edge failures. On the other hand,
$k$-robustness measures the effectiveness of local diffusion strategies (that
do not use global graph topology information) in spreading information over the
graph in the presence of misbehaving nodes. In addition to presenting the
results under the general random intersection graph model, we consider two
special cases of the general model, a binomial random intersection graph and a
uniform random intersection graph, which both have numerous applications as
well. For these two specialized graphs, our results on asymptotically exact
probabilities of $k$-connectivity and asymptotic zero-one laws for
$k$-robustness are also novel in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6001</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6001</id><created>2014-09-21</created><authors><author><keyname>El-Mougy</keyname><forenames>Amr</forenames></author><author><keyname>Ibnkahla</keyname><forenames>Mohamed</forenames></author><author><keyname>Hattab</keyname><forenames>Ghaith</forenames></author><author><keyname>Ejaz</keyname><forenames>Waleed</forenames></author></authors><title>Reconfigurable Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>28 pages, 26 figures; Submitted to the Proceedings of the IEEE (a
  special issue on Reconfigurable Systems)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Driven by the advent of sophisticated and ubiquitous applications, and the
ever-growing need for information, wireless networks are without a doubt
steadily evolving into profoundly more complex and dynamic systems. The user
demands are progressively rampant, while application requirements continue to
expand in both range and diversity. Future wireless networks, therefore, must
be equipped with the ability to handle numerous, albeit challenging
requirements. Network reconfiguration, considered as a prominent network
paradigm, is envisioned to play a key role in leveraging future network
performance and considerably advancing current user experiences. This paper
presents a comprehensive overview of reconfigurable wireless networks and an
in-depth analysis of reconfiguration at all layers of the protocol stack. Such
networks characteristically possess the ability to reconfigure and adapt their
hardware and software components and architectures, thus enabling flexible
delivery of broad services, as well as sustaining robust operation under highly
dynamic conditions. The paper offers a unifying framework for research in
reconfigurable wireless networks. This should provide the reader with a
holistic view of concepts, methods, and strategies in reconfigurable wireless
networks. Focus is given to reconfigurable systems in relatively new and
emerging research areas such as cognitive radio networks, cross-layer
reconfiguration and software-defined networks. In addition, modern networks
have to be intelligent and capable of self-organization. Thus, this paper
discusses the concept of network intelligence as a means to enable
reconfiguration in highly complex and dynamic networks. Finally, the paper is
supported with several examples and case studies showing the tremendous impact
of reconfiguration on wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6002</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6002</id><created>2014-09-21</created><authors><author><keyname>Darties</keyname><forenames>Benoit</forenames><affiliation>Le2i</affiliation></author><author><keyname>Gastineau</keyname><forenames>Nicolas</forenames><affiliation>Le2i</affiliation></author><author><keyname>Togni</keyname><forenames>Olivier</forenames><affiliation>Le2i</affiliation></author></authors><title>Completely Independent Spanning Trees in Some Regular Graphs</title><categories>cs.DM math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $k\ge 2$ be an integer and $T_1,\ldots, T_k$ be spanning trees of a graph
$G$. If for any pair of vertices $(u,v)$ of $V(G)$, the paths from $u$ to $v$
in each $T_i$, $1\le i\le k$, do not contain common edges and common vertices,
except the vertices $u$ and $v$, then $T_1,\ldots, T_k$ are completely
independent spanning trees in $G$. For $2k$-regular graphs which are
$2k$-connected, such as the Cartesian product of a complete graph of order
$2k-1$ and a cycle and some Cartesian products of three cycles (for $k=3$), the
maximum number of completely independent spanning trees contained in these
graphs is determined and it turns out that this maximum is not always $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6011</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6011</id><created>2014-09-21</created><updated>2016-02-29</updated><authors><author><keyname>Cousins</keyname><forenames>Ben</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Gaussian Cooling and an O*(n^3) Algorithms for Volume and Gaussian
  Volume</title><categories>cs.DS cs.CC math.FA</categories><comments>This paper is a combination of two previously published conference
  papers: &quot;A Cubic Algorithm for Computing Gaussian Volume&quot; (SODA 2014) and
  &quot;Bypassing KLS: Gaussian Cooling and an $O^*(n^3)$ Volume Algorithm&quot; (STOC
  2015). Additionally, this version has a major simplification to the main
  proof in the latter conference paper. (Lemma 3.2 in this version) 36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an $O^*(n^3)$ randomized algorithm for estimating the volume of a
well-rounded convex body given by a membership oracle, improving on the
previous best complexity of $O^*(n^4)$. The new algorithmic ingredient is an
accelerated cooling schedule where the rate of cooling increases with the
temperature. Previously, the known approach for potentially achieving this
asymptotic complexity relied on a positive resolution of the KLS hyperplane
conjecture, a central open problem in convex geometry.
  We also obtain an $O^*(n^3)$ randomized algorithm for integrating a standard
Gaussian distribution over an arbitrary convex set containing the unit ball.
Both the volume and Gaussian volume algorithms use an improved algorithm for
sampling a Gaussian distribution restricted to a convex body. In this latter
setting, as we show, the KLS conjecture holds and for a spherical Gaussian
distribution with variance $\sigma^2$, the sampling complexity is
$O^*(\max\{n^3, \sigma^2n^2\})$ for the first sample and $O^*(\max\{n^2,
\sigma^2n^2\})$ for every subsequent sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6015</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6015</id><created>2014-09-21</created><updated>2014-10-27</updated><authors><author><keyname>Ramaswami</keyname><forenames>Suneeta</forenames></author><author><keyname>Siqueira</keyname><forenames>Marcelo</forenames></author></authors><title>A fast algorithm for computing irreducible triangulations of closed
  surfaces in $E^d$</title><categories>math.GT cs.CG cs.DS</categories><comments>52 pages, a shorter version of this Technical Report is about to be
  submitted to Elsevier Journal Computational Geometry: Theory and Applications</comments><msc-class>05C10, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a fast algorithm for computing an irreducible triangulation
$T^\prime$ of an oriented, connected, boundaryless, and compact surface $S$ in
$E^d$ from any given triangulation $T$ of $S$. If the genus $g$ of $S$ is
positive, then our algorithm takes $O(g^2+gn)$ time to obtain $T^\prime$, where
$n$ is the number of triangles of $T$. Otherwise, $T^\prime$ is obtained in
linear time in $n$. While the latter upper bound is optimal, the former upper
bound improves upon the currently best known upper bound by a $(\lg n / g)$
factor. In both cases, the memory space required by our algorithm is in
${\Theta}(n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6021</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6021</id><created>2014-09-21</created><updated>2014-11-18</updated><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>On $k$-connectivity and minimum vertex degree in random $s$-intersection
  graphs</title><categories>math.CO cs.DM cs.SI math.PR physics.soc-ph</categories><comments>This paper appears in ACM-SIAM Meeting on Analytic Algorithmics and
  Combinatorics (ANALCO) 2015, a conference co-located with ACM-SIAM Symposium
  on Discrete Algorithms (SODA) 2015</comments><msc-class>05C80, 60B20</msc-class><acm-class>G.2.2; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random $s$-intersection graphs have recently received much interest in a wide
range of application areas. Broadly speaking, a random $s$-intersection graph
is constructed by first assigning each vertex a set of items in some random
manner, and then putting an undirected edge between all pairs of vertices that
share at least $s$ items (the graph is called a random intersection graph when
$s=1$). A special case of particular interest is a uniform random
$s$-intersection graph, where each vertex independently selects the same number
of items uniformly at random from a common item pool. Another important case is
a binomial random $s$-intersection graph, where each item from a pool is
independently assigned to each vertex with the same probability. Both models
have found numerous applications thus far including cryptanalysis, and the
modeling of recommender systems, secure sensor networks, online social
networks, trust networks and small-world networks (uniform random
$s$-intersection graphs), as well as clustering analysis, classification, and
the design of integrated circuits (binomial random $s$-intersection graphs).
  In this paper, for binomial/uniform random $s$-intersection graphs, we
present results related to $k$-connectivity and minimum vertex degree.
Specifically, we derive the asymptotically exact probabilities and zero-one
laws for the following three properties: (i) $k$-vertex-connectivity, (ii)
$k$-edge-connectivity and (iii) the property of minimum vertex degree being at
least $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6022</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6022</id><created>2014-09-21</created><updated>2015-02-06</updated><authors><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>Exact Analysis of k-Connectivity in Secure Sensor Networks with
  Unreliable Links</title><categories>math.CO cs.DM cs.SI math.PR physics.soc-ph</categories><msc-class>05C80, 60B20</msc-class><acm-class>G.2.2; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Eschenauer--Gligor (EG) random key predistribution scheme has been widely
recognized as a typical approach to secure communications in wireless sensor
networks (WSNs). However, there is a lack of precise probability analysis on
the reliable connectivity of WSNs under the EG scheme. To address this, we
rigorously derive the asymptotically exact probability of $k$-connectivity in
WSNs employing the EG scheme with unreliable links represented by independent
on/off channels, where $k$-connectivity ensures that the network remains
connected despite the failure of any $(k-1)$ sensors or links. Our analytical
results are confirmed via numerical experiments, and they provide precise
guidelines for the design of secure WSNs that exhibit a desired level of
reliability against node and link failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6023</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6023</id><created>2014-09-21</created><authors><author><keyname>Van Aken</keyname><forenames>Jerry R.</forenames></author></authors><title>A High-Level Model of Neocortical Feedback Based on an Event Window
  Segmentation Algorithm</title><categories>cs.NE</categories><comments>44 pages, 9 figures</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The author previously presented an event window segmentation (EWS) algorithm
[5] that uses purely statistical methods to learn to recognize recurring
patterns in an input stream of events. In the following discussion, the EWS
algorithm is first extended to make predictions about future events. Next, this
extended algorithm is used to construct a high-level, simplified model of a
neocortical hierarchy. An event stream enters at the bottom of the hierarchy,
and drives processing activity upward in the hierarchy. Successively higher
regions in the hierarchy learn to recognize successively deeper levels of
patterns in these events as they propagate from the bottom of the hierarchy.
The lower levels in the hierarchy use the predictions from the levels above to
strengthen their own predictions. A C++ source code listing of the model
implementation and test program is included as an appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6033</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6033</id><created>2014-09-21</created><authors><author><keyname>Li</keyname><forenames>Cong</forenames></author><author><keyname>Li</keyname><forenames>Qian</forenames></author><author><keyname>Van Mieghem</keyname><forenames>Piet</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Wang</keyname><forenames>Huijuan</forenames></author></authors><title>Correlation between centrality metrics and their application to the
  opinion model</title><categories>physics.soc-ph cs.SI</categories><comments>20 pages</comments><doi>10.1140/epjb/e2015-50671-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent decades, a number of centrality metrics describing network
properties of nodes have been proposed to rank the importance of nodes. In
order to understand the correlations between centrality metrics and to
approximate a high-complexity centrality metric by a strongly correlated
low-complexity metric, we ?first study the correlation between centrality
metrics in terms of their Pearson correlation coefficient and their similarity
in ranking of nodes. In addition to considering the widely used centrality
metrics, we introduce a new centrality measure, the degree mass. The m order
degree mass of a node is the sum of the weighted degree of the node and its
neighbors no further than m hops away. We find that the B_{n}, the closeness,
and the components of x_{1} are strongly correlated with the degree, the
1st-order degree mass and the 2nd-order degree mass, respectively, in both
network models and real-world networks. We then theoretically prove that the
Pearson correlation coefficient between x_{1} and the 2nd-order degree mass is
larger than that between x_{1} and a lower order degree mass. Finally, we
investigate the effect of the inflexible antagonists selected based on
different centrality metrics in helping one opinion to compete with another in
the inflexible antagonists opinion model. Interestingly, we find that selecting
the inflexible antagonists based on the leverage, the B_{n}, or the degree is
more effective in opinion-competition than using other centrality metrics in
all types of networks. This observation is supported by our previous
observations, i.e., that there is a strong linear correlation between the
degree and the B_{n}, as well as a high centrality similarity between the
leverage and the degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6041</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6041</id><created>2014-09-21</created><authors><author><keyname>Ghifary</keyname><forenames>Muhammad</forenames></author><author><keyname>Kleijn</keyname><forenames>W. Bastiaan</forenames></author><author><keyname>Zhang</keyname><forenames>Mengjie</forenames></author></authors><title>Domain Adaptive Neural Networks for Object Recognition</title><categories>cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple neural network model to deal with the domain adaptation
problem in object recognition. Our model incorporates the Maximum Mean
Discrepancy (MMD) measure as a regularization in the supervised learning to
reduce the distribution mismatch between the source and target domains in the
latent space. From experiments, we demonstrate that the MMD regularization is
an effective tool to provide good domain adaptation models on both SURF
features and raw image pixels of a particular image data set. We also show that
our proposed model, preceded by the denoising auto-encoder pretraining,
achieves better performance than recent benchmark models on the same data sets.
This work represents the first study of MMD measure in the context of neural
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6045</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6045</id><created>2014-09-21</created><authors><author><keyname>Honeine</keyname><forenames>Paul</forenames></author></authors><title>Analyzing sparse dictionaries for online learning with kernels</title><categories>stat.ML cs.CV cs.IT cs.LG math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many signal processing and machine learning methods share essentially the
same linear-in-the-parameter model, with as many parameters as available
samples as in kernel-based machines. Sparse approximation is essential in many
disciplines, with new challenges emerging in online learning with kernels. To
this end, several sparsity measures have been proposed in the literature to
quantify sparse dictionaries and constructing relevant ones, the most prolific
ones being the distance, the approximation, the coherence and the Babel
measures. In this paper, we analyze sparse dictionaries based on these
measures. By conducting an eigenvalue analysis, we show that these sparsity
measures share many properties, including the linear independence condition and
inducing a well-posed optimization problem. Furthermore, we prove that there
exists a quasi-isometry between the parameter (i.e., dual) space and the
dictionary's induced feature space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6046</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6046</id><created>2014-09-21</created><authors><author><keyname>Honeine</keyname><forenames>Paul</forenames></author></authors><title>Approximation errors of online sparsification criteria</title><categories>stat.ML cs.CV cs.IT cs.LG cs.NE math.IT</categories><comments>10 pages</comments><doi>10.1109/TSP.2015.2442960</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning frameworks, such as resource-allocating networks,
kernel-based methods, Gaussian processes, and radial-basis-function networks,
require a sparsification scheme in order to address the online learning
paradigm. For this purpose, several online sparsification criteria have been
proposed to restrict the model definition on a subset of samples. The most
known criterion is the (linear) approximation criterion, which discards any
sample that can be well represented by the already contributing samples, an
operation with excessive computational complexity. Several computationally
efficient sparsification criteria have been introduced in the literature, such
as the distance, the coherence and the Babel criteria. In this paper, we
provide a framework that connects these sparsification criteria to the issue of
approximating samples, by deriving theoretical bounds on the approximation
errors. Moreover, we investigate the error of approximating any feature, by
proposing upper-bounds on the approximation error for each of the
aforementioned sparsification criteria. Two classes of features are described
in detail, the empirical mean and the principal axes in the kernel principal
component analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6052</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6052</id><created>2014-09-21</created><authors><author><keyname>Gatterbauer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Oblivious Bounds on the Probability of Boolean Functions</title><categories>cs.AI cs.DB</categories><comments>34 pages, 14 figures, supersedes: http://arxiv.org/abs/1105.2813</comments><journal-ref>Pre-print for ACM Transactions on Database Systems, January 2014,
  Vol 39, No 1, Article 5</journal-ref><doi>10.1145/2532641</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops upper and lower bounds for the probability of Boolean
functions by treating multiple occurrences of variables as independent and
assigning them new individual probabilities. We call this approach dissociation
and give an exact characterization of optimal oblivious bounds, i.e. when the
new probabilities are chosen independent of the probabilities of all other
variables. Our motivation comes from the weighted model counting problem (or,
equivalently, the problem of computing the probability of a Boolean function),
which is #P-hard in general. By performing several dissociations, one can
transform a Boolean formula whose probability is difficult to compute, into one
whose probability is easy to compute, and which is guaranteed to provide an
upper or lower bound on the probability of the original formula by choosing
appropriate probabilities for the dissociated variables. Our new bounds shed
light on the connection between previous relaxation-based and model-based
approximations and unify them as concrete choices in a larger design space. We
also show how our theory allows a standard relational database management
system (DBMS) to both upper and lower bound hard probabilistic queries in
guaranteed polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6059</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6059</id><created>2014-09-21</created><authors><author><keyname>Lu</keyname><forenames>Songtao</forenames></author><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author></authors><title>Joint Optimization of Power Allocation and Training Duration for Uplink
  Multiuser MIMO Communications</title><categories>cs.IT math.IT</categories><comments>Submitted to WCNC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multiuser multiple-input multiple-output
(MU-MIMO) communication system between a base station equipped with multiple
antennas and multiple mobile users each equipped with a single antenna. The
uplink scenario is considered. The uplink channels are acquired by the base
station through a training phase. Two linear processing schemes are considered,
namely maximum-ratio combining (MRC) and zero-forcing (ZF). We optimize the
training period and optimal training energy under the average and peak power
constraint so that an achievable sum rate is maximized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6070</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6070</id><created>2014-09-21</created><authors><author><keyname>Graham</keyname><forenames>Benjamin</forenames></author></authors><title>Spatially-sparse convolutional neural networks</title><categories>cs.CV cs.NE</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) perform well on problems such as
handwriting recognition and image classification. However, the performance of
the networks is often limited by budget and time constraints, particularly when
trying to train deep networks.
  Motivated by the problem of online handwriting recognition, we developed a
CNN for processing spatially-sparse inputs; a character drawn with a one-pixel
wide pen on a high resolution grid looks like a sparse matrix. Taking advantage
of the sparsity allowed us more efficiently to train and test large, deep CNNs.
On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test
error of 3.82%.
  Although pictures are not sparse, they can be thought of as sparse by adding
padding. Applying a deep convolutional network using sparsity has resulted in a
substantial reduction in test error on the CIFAR small picture datasets: 6.28%
on CIFAR-10 and 24.30% for CIFAR-100.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6075</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6075</id><created>2014-09-21</created><updated>2014-11-04</updated><authors><author><keyname>Ward</keyname><forenames>Tyler</forenames></author></authors><title>The Information Theoretically Efficient Model (ITEM): A model for
  computerized analysis of large datasets</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document discusses the Information Theoretically Efficient Model (ITEM),
a computerized system to generate an information theoretically efficient
multinomial logistic regression from a general dataset. More specifically, this
model is designed to succeed even where the logit transform of the dependent
variable is not necessarily linear in the independent variables. This research
shows that for large datasets, the resulting models can be produced on modern
computers in a tractable amount of time. These models are also resistant to
overfitting, and as such they tend to produce interpretable models with only a
limited number of features, all of which are designed to be well behaved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6076</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6076</id><created>2014-09-21</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Mackenzie</keyname><forenames>Simon</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author><author><keyname>Ye</keyname><forenames>Chun</forenames></author></authors><title>Structure and complexity of ex post efficient random assignments</title><categories>cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the random assignment problem, objects are randomly assigned to agents
keeping in view the agents' preferences over objects. A random assignment
specifies the probability of an agent getting an object. We examine the
structural and computational aspects of ex post efficiency of random
assignments. We first show that whereas an ex post efficient assignment can be
computed easily, checking whether a given random assignment is ex post
efficient is NP-complete. Hence implementing a given random assignment via
deterministic Pareto optimal assignments is NP-hard. We then formalize another
concept of efficiency called robust ex post efficiency that is weaker than
stochastic dominance efficiency but stronger than ex post efficiency. We
present a characterization of robust ex post efficiency and show that it can be
tested in polynomial time if there are a constant number of agent types. It is
shown that the well-known random serial dictatorship rule is not robust ex post
efficient. Finally, we show that whereas robust ex post efficiency depends
solely on which entries of the assignment matrix are zero/non-zero, ex post
efficiency of an assignment depends on the actual values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6080</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6080</id><created>2014-09-22</created><updated>2015-02-06</updated><authors><author><keyname>Mitra</keyname><forenames>Adway</forenames></author><author><keyname>Biswas</keyname><forenames>Soma</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Chiranjib</forenames></author></authors><title>Temporally Coherent Bayesian Models for Entity Discovery in Videos by
  Tracklet Clustering</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A video can be represented as a sequence of tracklets, each spanning 10-20
frames, and associated with one entity (eg. a person). The task of \emph{Entity
Discovery} in videos can be naturally posed as tracklet clustering. We approach
this task by leveraging \emph{Temporal Coherence}(TC): the fundamental property
of videos that each tracklet is likely to be associated with the same entity as
its temporal neighbors. Our major contributions are the first Bayesian
nonparametric models for TC at tracklet-level. We extend Chinese Restaurant
Process (CRP) to propose TC-CRP, and further to Temporally Coherent Chinese
Restaurant Franchise (TC-CRF) to jointly model short temporal segments. On the
task of discovering persons in TV serial videos without meta-data like scripts,
these methods show considerable improvement in cluster purity and person
coverage compared to state-of-the-art approaches to tracklet clustering. We
represent entities with mixture components, and tracklets with vectors of very
generic features, which can work for any type of entity (not necessarily
person). The proposed methods can perform online tracklet clustering on
streaming videos with little performance deterioration unlike existing
approaches, and can automatically reject tracklets resulting from false
detections. Finally we discuss entity-driven video summarization- where some
temporal segments of the video are selected automatically based on the
discovered entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6092</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6092</id><created>2014-09-22</created><updated>2014-10-02</updated><authors><author><keyname>Hengjia</keyname><forenames>Wei</forenames></author><author><keyname>Hui</keyname><forenames>Zhang</forenames></author><author><keyname>Mingzhi</keyname><forenames>Zhu</forenames></author><author><keyname>Gennian</keyname><forenames>Ge</forenames></author></authors><title>Optimal Ternary Constant-Composition Codes with Weight Four and Distance
  Six</title><categories>cs.IT math.CO math.IT</categories><comments>44 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sizes of optimal constant-composition codes of weight three have been
determined by Chee, Ge and Ling with four cases in doubt. Group divisible codes
played an important role in their constructions. In this paper, we study the
problem of constructing optimal ternary constant-composition codes with Hamming
weight four and minimum distance six. The problem is solved with a small number
of lengths undetermined. The previously known results are those with code
length no greater than 10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6099</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6099</id><created>2014-09-22</created><authors><author><keyname>Sidiropoulos</keyname><forenames>Antonis</forenames></author><author><keyname>Katsaros</keyname><forenames>Dimitrios</forenames></author><author><keyname>Manolopoulos</keyname><forenames>Yannis</forenames></author></authors><title>Identification of Influential Scientists vs. Mass Producers by the
  Perfectionism Index</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>27 pages, 10 figures, 11 tables. arXiv admin note: substantial text
  overlap with arXiv:1309.0277</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of h-index has been proposed to easily assess a researcher's
performance with a single number. However, by using only this number, we lose
significant information about the distribution of citations per article in an
author's publication list. In this article, we study an author's citation curve
and we define two new areas related to this curve. We call these &quot;penalty
areas&quot;, since the greater they are, the more an author's performance is
penalized. We exploit these areas to establish new indices, namely PI and XPI,
aiming at categorizing researchers in two distinct categories: &quot;influentials&quot;
and &quot;mass producers&quot;; the former category produces articles which are (almost
all) with high impact, and the latter category produces a lot of articles with
moderate or no impact at all. Using data from Microsoft Academic Service, we
evaluate the merits mainly of PI as a useful tool for scientometric studies. We
establish its effectiveness into separating the scientists into influentials
and mass producers; we demonstrate its robustness against self-citations, and
its uncorrelation to traditional indices. Finally, we apply PI to rank
prominent scientists in the areas of databases, networks and multimedia,
exhibiting the strength of the index in fulfilling its design goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6110</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6110</id><created>2014-09-22</created><updated>2014-11-04</updated><authors><author><keyname>Soare</keyname><forenames>Marta</forenames></author><author><keyname>Lazaric</keyname><forenames>Alessandro</forenames></author><author><keyname>Munos</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Best-Arm Identification in Linear Bandits</title><categories>cs.LG</categories><comments>In Advances in Neural Information Processing Systems 27 (NIPS), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the best-arm identification problem in linear bandit, where the
rewards of the arms depend linearly on an unknown parameter $\theta^*$ and the
objective is to return the arm with the largest reward. We characterize the
complexity of the problem and introduce sample allocation strategies that pull
arms to identify the best arm with a fixed confidence, while minimizing the
sample budget. In particular, we show the importance of exploiting the global
linear structure to improve the estimate of the reward of near-optimal arms. We
analyze the proposed strategies and compare their empirical performance.
Finally, as a by-product of our analysis, we point out the connection to the
$G$-optimality criterion used in optimal experimental design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6111</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6111</id><created>2014-09-22</created><authors><author><keyname>Zhao</keyname><forenames>Xiaochuan</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Distributed Clustering and Learning Over Networks</title><categories>math.OC cs.LG cs.MA cs.SY stat.ML</categories><comments>47 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed processing over networks relies on in-network processing and
cooperation among neighboring agents. Cooperation is beneficial when agents
share a common objective. However, in many applications agents may belong to
different clusters that pursue different objectives. Then, indiscriminate
cooperation will lead to undesired results. In this work, we propose an
adaptive clustering and learning scheme that allows agents to learn which
neighbors they should cooperate with and which other neighbors they should
ignore. In doing so, the resulting algorithm enables the agents to identify
their clusters and to attain improved learning and estimation accuracy over
networks. We carry out a detailed mean-square analysis and assess the error
probabilities of Types I and II, i.e., false alarm and mis-detection, for the
clustering mechanism. Among other results, we establish that these
probabilities decay exponentially with the step-sizes so that the probability
of correct clustering can be made arbitrarily close to one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6112</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6112</id><created>2014-09-22</created><updated>2015-11-10</updated><authors><author><keyname>Fratani</keyname><forenames>Severine</forenames></author><author><keyname>Voundy</keyname><forenames>El Makki</forenames></author></authors><title>Dyck-based characterizations of Indexed Languages</title><categories>cs.FL</categories><comments>The general approach can be improved and some statements are
  inaccurate</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indexed languages are a generalization of context-free languages and form a
proper subset of context-sensitive languages. We propose to generalize to
indexed languages several well known characterizations of context-free
languages: namely, the characterization by rational transductions defined by
Nivat, the Chomsky-Sch\&quot;utzenberger theorem, and the logical characterization
proved by Lautemann et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6136</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6136</id><created>2014-09-22</created><updated>2015-11-04</updated><authors><author><keyname>Mahalanobis</keyname><forenames>Ayan</forenames></author><author><keyname>Singh</keyname><forenames>Anupam</forenames></author></authors><title>Gaussian elimination in unitary groups with an application to
  cryptography</title><categories>math.GR cs.CR</categories><msc-class>20H30, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian elimination is used in special linear groups to solve the word
problem. In this paper, we extend Gaussian elimination to unitary groups. These
algorithms have an application in building a public-key cryptosystem, we
demonstrate that.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6142</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6142</id><created>2014-09-22</created><authors><author><keyname>Klimann</keyname><forenames>Ines</forenames></author><author><keyname>Picantin</keyname><forenames>Matthieu</forenames></author><author><keyname>Savchuk</keyname><forenames>Dmytro</forenames></author></authors><title>A connected 3-state reversible Mealy automaton cannot generate an
  infinite Burnside group</title><categories>cs.FL math.GR</categories><comments>12 pages, 4 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The class of automaton groups is a rich source of the simplest examples of
infinite Burnside groups. However, there are some classes of automata that do
not contain such examples. For instance, all infinite Burnside automaton groups
in the literature are generated by non reversible Mealy automata and it was
recently shown that 2-state invertible-reversible Mealy automata cannot
generate infinite Burnside groups. Here we extend this result to connected
3-state invertible-reversible Mealy automata, using new original techniques.
The results provide the first uniform method to construct elements of infinite
order in each infinite group in this class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6144</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6144</id><created>2014-09-22</created><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author><author><keyname>Richard</keyname><forenames>Adrien</forenames></author><author><keyname>Riis</keyname><forenames>S&#xf8;ren</forenames></author></authors><title>Fixed points of Boolean networks, guessing graphs, and coding theory</title><categories>cs.DM cs.IT math.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are interested in the number of fixed points of functions
$f:A^n\to A^n$ over a finite alphabet $A$ defined on a given signed digraph
$D$. We first use techniques from network coding to derive some lower bounds on
the number of fixed points that only depends on $D$. We then discover
relationships between the number of fixed points of $f$ and problems in coding
theory, especially the design of codes for the asymmetric channel. Using these
relationships, we derive upper and lower bounds on the number of fixed points,
which significantly improve those given in the literature. We also unveil some
interesting behaviour of the number of fixed points of functions with a given
signed digraph when the alphabet varies. We finally prove that signed digraphs
with more (disjoint) positive cycles actually do not necessarily have functions
with more fixed points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6150</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6150</id><created>2014-09-22</created><updated>2015-10-02</updated><authors><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author><author><keyname>Oyarzun</keyname><forenames>Diego</forenames></author><author><keyname>Angeli</keyname><forenames>David</forenames></author><author><keyname>Stan</keyname><forenames>Guy-Bart</forenames></author></authors><title>Shaping Pulses to Control Bistable Biological Systems</title><categories>math.OC cs.SY q-bio.QM</categories><comments>14 pages, contains material from the paper in Proc Amer Control Conf
  2015, (pp. 3138-3143) and &quot;Shaping pulses to control bistable systems
  analysis, computation and counterexamples&quot;, which is due to appear in
  Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study how to shape temporal pulses to switch a bistable
system between its stable steady states. Our motivation for pulse-based control
comes from applications in synthetic biology, where it is generally difficult
to implement real-time feedback control systems due to technical limitations in
sensors and actuators. We show that for monotone bistable systems, the
estimation of the set of all pulses that switch the system reduces to the
computation of one non-increasing curve. We provide an efficient algorithm to
compute this curve and illustrate the results with a genetic bistable system
commonly used in synthetic biology. We also extend these results to models with
parametric uncertainty and provide a number of examples and counterexamples
that demonstrate the power and limitations of the current theory. In order to
show the full potential of the framework, we consider the problem of inducing
oscillations in a monotone biochemical system using a combination of temporal
pulses and event-based control. Our results provide an insight into the
dynamics of bistable systems under external inputs and open up numerous
directions for future investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6151</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6151</id><created>2014-09-22</created><authors><author><keyname>Zgraggen</keyname><forenames>Aldo U.</forenames></author><author><keyname>Fagiano</keyname><forenames>Lorenzo</forenames></author><author><keyname>Morari</keyname><forenames>Manfred</forenames></author></authors><title>Automatic Retraction and Full Cycle Operation for a Class of Airborne
  Wind Energy Generators</title><categories>cs.SY</categories><comments>This manuscript is a preprint of a paper submitted for possible
  publication on the IEEE Transactions on Control Systems Technology and is
  subject to IEEE Copyright. If accepted, the copy of record will be available
  at IEEEXplore library: http://ieeexplore.ieee.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Airborne wind energy systems aim to harvest the power of winds blowing at
altitudes higher than what conventional wind turbines reach. They employ a
tethered flying structure, usually a wing, and exploit the aerodynamic lift to
produce electrical power. In the case of ground-based systems, where the
traction force on the tether is used to drive a generator on the ground, a two
phase power cycle is carried out: one phase to produce power, where the tether
is reeled out under high traction force, and a second phase where the tether is
recoiled under minimal load. The problem of controlling a tethered wing in this
second phase, the retraction phase, is addressed here, by proposing two
possible control strategies. Theoretical analyses, numerical simulations, and
experimental results are presented to show the performance of the two
approaches. Finally, the experimental results of complete autonomous power
generation cycles are reported and compared with first-principle models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6155</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6155</id><created>2014-09-22</created><updated>2014-10-05</updated><authors><author><keyname>Lu</keyname><forenames>Cewu</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Chen</keyname><forenames>Qifeng</forenames></author><author><keyname>Law</keyname><forenames>Hei</forenames></author><author><keyname>Xiao</keyname><forenames>Yao</forenames></author><author><keyname>Tang</keyname><forenames>Chi-Keung</forenames></author></authors><title>1-HKUST: Object Detection in ILSVRC 2014</title><categories>cs.CV</categories><comments>3 pages; Author list fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Imagenet Large Scale Visual Recognition Challenge (ILSVRC) is the one of
the most important big data challenges to date. We participated in the object
detection track of ILSVRC 2014 and received the fourth place among the 38
teams. We introduce in our object detection system a number of novel techniques
in localization and recognition. For localization, initial candidate proposals
are generated using selective search, and a novel bounding boxes regression
method is used for better object localization. For recognition, to represent a
candidate proposal, we adopt three features, namely, RCNN feature, IFV feature,
and DPM feature. Given these features, category-specific combination functions
are learned to improve the object recognition rate. In addition, object context
in the form of background priors and object interaction priors are learned and
applied in our system. Our ILSVRC 2014 results are reported alongside with the
results of other participating teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6172</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6172</id><created>2014-09-22</created><updated>2015-07-17</updated><authors><author><keyname>Fourny</keyname><forenames>Ghislain</forenames></author><author><keyname>Reiche</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Dupuy</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Perfect Prediction Equilibrium</title><categories>cs.GT</categories><comments>This is an extended technical report (46 pages) that contains all the
  material related to the PPE including many annexes. It integrates feedback
  received from the 2nd International Conference on Economic Philosophy, as
  well as feedback received from numerous discussions</comments><msc-class>91A18</msc-class><acm-class>H.4.m; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of finite games in extensive form with perfect information
and strict preferences, this paper introduces a new equilibrium concept: the
Perfect Prediction Equilibrium (PPE).
  In the Nash paradigm, rational players consider that the opponent's strategy
is fixed while maximizing their payoff. The PPE, on the other hand, models the
behavior of agents with an alternate form of rationality that involves a
Stackelberg competition with the past.
  Agents with this form of rationality integrate in their reasoning that they
have such accurate logical and predictive skills, that the world is fully
transparent: all players share the same knowledge and know as much as an
omniscient external observer. In particular, there is common knowledge of the
solution of the game including the reached outcome and the thought process
leading to it. The PPE is stable given each player's knowledge of its actual
outcome and uses no assumptions at unreached nodes.
  This paper gives the general definition and construction of the PPE as a
fixpoint problem, proves its existence, uniqueness and Pareto optimality, and
presents two algorithms to compute it. Finally, the PPE is put in perspective
with existing literature (Newcomb's Problem, Superrationality, Nash
Equilibrium, Subgame Perfect Equilibrium, Backward Induction Paradox, Forward
Induction).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6182</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6182</id><created>2014-09-22</created><updated>2014-09-23</updated><authors><author><keyname>Alarte</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Insa</keyname><forenames>David</forenames></author><author><keyname>Silva</keyname><forenames>Josep</forenames></author><author><keyname>Tamarit</keyname><forenames>Salvador</forenames></author></authors><title>A Benchmark Suite for Template Detection and Content Extraction</title><categories>cs.IR</categories><comments>10 pages, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Template detection and content extraction are two of the main areas of
information retrieval applied to the Web. They perform different analyses over
the structure and content of webpages to extract some part of the document.
However, their objective is different. While template detection identifies the
template of a webpage (usually comparing with other webpages of the same
website), content extraction identifies the main content of the webpage
discarding the other part. Therefore, they are somehow complementary, because
the main content is not part of the template. It has been measured that
templates represent between 40% and 50% of data on the Web. Therefore,
identifying templates is essential for indexing tasks because templates usually
contain irrelevant information such as advertisements, menus and banners.
Processing and storing this information is likely to lead to a waste of
resources (storage space, bandwidth, etc.). Similarly, identifying the main
content is essential for many information retrieval tasks. In this paper, we
present a benchmark suite to test different approaches for template detection
and content extraction. The suite is public, and it contains real heterogeneous
webpages that have been labelled so that different techniques can be suitable
(and automatically) compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6193</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6193</id><created>2014-09-22</created><updated>2014-11-18</updated><authors><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Squartini</keyname><forenames>Tiziano</forenames></author><author><keyname>Gabrielli</keyname><forenames>Andrea</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author></authors><title>Estimating topological properties of weighted networks from limited
  information</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-fin.ST</categories><journal-ref>Phys. Rev. E 92, 040802 (2015)</journal-ref><doi>10.1103/PhysRevE.92.040802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in studying and modeling economic and financial systems
is represented by privacy issues, which put severe limitations on the amount of
accessible information. Here we introduce a novel, highly nontrivial method to
reconstruct the structural properties of complex weighted networks of this kind
using only partial information: the total number of nodes and links, and the
values of the strength for all nodes. The latter are used as fitness to
estimate the unknown node degrees through a standard configuration model. Then,
these estimated degrees and the strengths are used to calibrate an enhanced
configuration model in order to generate ensembles of networks intended to
represent the real system. The method, which is tested on real economic and
financial networks, while drastically reducing the amount of information needed
to infer network properties, turns out to be remarkably effective$-$thus
representing a valuable tool for gaining insights on privacy-protected
socioeconomic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6197</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6197</id><created>2014-09-22</created><authors><author><keyname>Sarigol</keyname><forenames>Emre</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Online Privacy as a Collective Phenomenon</title><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of online privacy is often reduced to individual decisions to
hide or reveal personal information in online social networks (OSNs). However,
with the increasing use of OSNs, it becomes more important to understand the
role of the social network in disclosing personal information that a user has
not revealed voluntarily: How much of our private information do our friends
disclose about us, and how much of our privacy is lost simply because of online
social interaction? Without strong technical effort, an OSN may be able to
exploit the assortativity of human private features, this way constructing
shadow profiles with information that users chose not to share. Furthermore,
because many users share their phone and email contact lists, this allows an
OSN to create full shadow profiles for people who do not even have an account
for this OSN.
  We empirically test the feasibility of constructing shadow profiles of sexual
orientation for users and non-users, using data from more than 3 Million
accounts of a single OSN. We quantify a lower bound for the predictive power
derived from the social network of a user, to demonstrate how the
predictability of sexual orientation increases with the size of this network
and the tendency to share personal information. This allows us to define a
privacy leak factor that links individual privacy loss with the decision of
other individuals to disclose information. Our statistical analysis reveals
that some individuals are at a higher risk of privacy loss, as prediction
accuracy increases for users with a larger and more homogeneous first- and
second-order neighborhood of their social network. While we do not provide
evidence that shadow profiles exist at all, our results show that disclosing of
private information is not restricted to an individual choice, but becomes a
collective decision that has implications for policy and privacy regulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6199</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6199</id><created>2014-09-22</created><authors><author><keyname>Dubey</keyname><forenames>Chandan</forenames></author><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author></authors><title>Computing the $p$-adic Canonical Quadratic Form in Polynomial Time</title><categories>cs.DS math.NT math.RA</categories><comments>arXiv admin note: text overlap with arXiv:1404.0281</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $n$-ary integral quadratic form is a formal expression
$Q(x_1,..,x_n)=\sum_{1\leq i,j\leq n}a_{ij}x_ix_j$ in $n$-variables
$x_1,...,x_n$, where $a_{ij}=a_{ji} \in \mathbb{Z}$. We present a randomized
polynomial time algorithm that given a quadratic form $Q(x_1,...,x_n)$, a prime
$p$, and a positive integer $k$ outputs a $\mathtt{U} \in
\text{GL}_n(\mathbb{Z}/p^k\mathbb{Z})$ such that $\mathtt{U}$ transforms $Q$ to
its $p$-adic canonical form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6215</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6215</id><created>2014-09-22</created><updated>2015-06-04</updated><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author><author><keyname>Podolskii</keyname><forenames>Vladimir V.</forenames></author></authors><title>Tropical Effective Primary and Dual Nullstellens\&quot;atze</title><categories>math.AG cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tropical algebra is an emerging field with a number of applications in
various areas of mathematics. In many of these applications appeal to tropical
polynomials allows to study properties of mathematical objects such as
algebraic varieties and algebraic curves from the computational point of view.
This makes it important to study both mathematical and computational aspects of
tropical polynomials.
  In this paper we prove a tropical Nullstellensatz and moreover we show an
effective formulation of this theorem. Nullstellensatz is a natural step in
building algebraic theory of tropical polynomials and its effective version is
relevant for computational aspects of this field.
  On our way we establish a simple formulation of min-plus and tropical linear
dualities. We also observe a close connection between tropical and min-plus
polynomial systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6226</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6226</id><created>2014-09-22</created><authors><author><keyname>Martins</keyname><forenames>Ricardo</forenames></author><author><keyname>Ferreira</keyname><forenames>Jo&#xe3;o Filipe</forenames></author><author><keyname>Dias</keyname><forenames>Jorge</forenames></author></authors><title>Touch attention Bayesian models for robotic active haptic exploration of
  heterogeneous surfaces</title><categories>cs.RO</categories><comments>8 pages, presented in IROS 2014, Chicago</comments><journal-ref>Proceedings of 2014 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2014), Chicago, USA, Sept. 14-18, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work contributes to the development of active haptic exploration
strategies of surfaces using robotic hands in environments with an unknown
structure. The architecture of the proposed approach consists two main Bayesian
models, implementing the touch attention mechanisms of the system. The model
pi_per perceives and discriminates different categories of materials (haptic
stimulus) integrating compliance and texture features extracted from haptic
sensory data. The model pi_tar actively infers the next region of the workspace
that should be explored by the robotic system, integrating the task
information, the permanently updated saliency and uncertainty maps extracted
from the perceived haptic stimulus map, as well as, inhibition-of-return
mechanisms.
  The experimental results demonstrate that the Bayesian model pi_per can be
used to discriminate 10 different classes of materials with an average
recognition rate higher than 90% . The generalization capability of the
proposed models was demonstrated experimentally. The ATLAS robot, in the
simulation, was able to perform the following of a discontinuity between two
regions made of different materials with a divergence smaller than 1cm (30
trials). The tests were performed in scenarios with 3 different configurations
of the discontinuity. The Bayesian models have demonstrated the capability to
manage the uncertainty about the structure of the surfaces and sensory noise to
make correct motor decisions from haptic percepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6231</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6231</id><created>2014-09-22</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>EM NANTES, IRCCyN</affiliation></author><author><keyname>Bondarenko</keyname><forenames>Dmitry</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>EM NANTES, IRCCyN</affiliation></author><author><keyname>Briot</keyname><forenames>S&#xe9;bastien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Compliance error compensation in robotic-based milling</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>Informatics in Control, Automation and Robotics, .-L. Ferrier, A.
  Bernard, O. Gusikhin, K. Madani (Ed.) (2014) 197-216</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with the problem of compliance errors compensation in
robotic-based milling. Contrary to previous works that assume that the
forces/torques generated by the manufacturing process are constant, the
interaction between the milling tool and the workpiece is modeled in details.
It takes into account the tool geometry, the number of teeth, the feed rate,
the spindle rotation speed and the properties of the material to be processed.
Due to high level of the disturbing forces/torques, the developed compensation
technique is based on the non-linear stiffness model that allows us to modify
the target trajectory taking into account nonlinearities and to avoid the
chattering effect. Illustrative example is presented that deals with
robotic-based milling of aluminum alloy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6234</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6234</id><created>2014-09-22</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>EM NANTES, IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>EM NANTES, IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>EM NANTES, IRCCyN</affiliation></author></authors><title>Accuracy Improvement of Robot-Based Milling Using an Enhanced
  Manipulator Model</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>Advances on Theory and Practice of Robots and Manipulators (2014)
  73-81</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to the accuracy improvement of robot-based milling by
using an enhanced manipulator model that takes into account both geometric and
elastostatic factors. Particular attention is paid to the model parameters
identification accuracy. In contrast to other works, the proposed approach
takes into account impact of the gravity compensator and link weights on the
manipulator elastostatic properties. In order to improve the identification
accuracy, the industry oriented performance measure is used to define optimal
measurement configurations and an enhanced partial pose measurement method is
applied for the identification of the model parameters. The advantages of the
developed approach are confirmed by experimental results that deal with the
elastostatic calibration of a heavy industrial robot used for milling. The
achieved accuracy improvement factor is about 2.4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6235</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6235</id><created>2014-09-22</created><authors><author><keyname>Ginosar</keyname><forenames>Shiry</forenames></author><author><keyname>Haas</keyname><forenames>Daniel</forenames></author><author><keyname>Brown</keyname><forenames>Timothy</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Detecting People in Cubist Art</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the human visual system is surprisingly robust to extreme distortion
when recognizing objects, most evaluations of computer object detection methods
focus only on robustness to natural form deformations such as people's pose
changes. To determine whether algorithms truly mirror the flexibility of human
vision, they must be compared against human vision at its limits. For example,
in Cubist abstract art, painted objects are distorted by object fragmentation
and part-reorganization, to the point that human vision often fails to
recognize them. In this paper, we evaluate existing object detection methods on
these abstract renditions of objects, comparing human annotators to four
state-of-the-art object detectors on a corpus of Picasso paintings. Our results
demonstrate that while human perception significantly outperforms current
methods, human perception and part-based models exhibit a similarly graceful
degradation in object detection performance as the objects become increasingly
abstract and fragmented, corroborating the theory of part-based object
representation in the brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6241</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6241</id><created>2014-09-22</created><authors><author><keyname>Bergamini</keyname><forenames>Elisabetta</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Staudt</keyname><forenames>Christian L.</forenames></author></authors><title>Approximating Betweenness Centrality in Large Evolving Networks</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>17 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Betweenness centrality ranks the importance of nodes by their participation
in all shortest paths of the network. Therefore computing exact betweenness
values is impractical in large networks. For static networks, approximation
based on randomly sampled paths has been shown to be significantly faster in
practice. However, for dynamic networks, no approximation algorithm for
betweenness centrality is known that improves on static recomputation. We
address this deficit by proposing two incremental approximation algorithms (for
weighted and unweighted connected graphs) which provide a provable guarantee on
the absolute approximation error. Processing batches of edge insertions, our
algorithms yield significant speedups up to a factor of $10^4$ compared to
restarting the approximation. This is enabled by investing memory to store and
efficiently update shortest paths. As a building block, we also propose an
asymptotically faster algorithm for updating the SSSP problem in unweighted
graphs. Our experimental study shows that our algorithms are the first to make
in-memory computation of a betweenness ranking practical for million-edge
semi-dynamic networks. Moreover, our results show that the accuracy is even
better than the theoretical guarantees in terms of absolutes errors and the
rank of nodes is well preserved, in particular for those with high betweenness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6247</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6247</id><created>2014-09-19</created><updated>2014-11-14</updated><authors><author><keyname>Kuriyama</keyname><forenames>Takayuki</forenames></author></authors><title>Learning Algorithm for Relation-Substitutable Context-Free Languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalized the class of $k,l$-substitutable languages (Yoshinala, 2008).
Each language in the generalized class is closed under a good substitutability.
The substitutability is defined by a recognizable equivalence relation. We show
the convergence of our generalized learning algorithm. The size of the
characteristic sample is smaller than Yoshinaka's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6253</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6253</id><created>2014-09-19</created><authors><author><keyname>Camilli</keyname><forenames>Matteo</forenames></author></authors><title>Constructing Coverability Graphs for Time Basic Petri Nets</title><categories>cs.LO cs.SE</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-Basic Petri nets, is a powerful formalism for modeling real-time systems
where time constraints are expressed through time functions of marking's time
description associated with transition, representing possible firing times. We
introduce a technique for coverability analysis based on the building of a
finite graph. This technique further exploits the time anonymous concept [5,6],
in order to deal with topologically unbounded nets, exploits the concept of a
coverage of TA tokens, i.e., a sort of {\omega} anonymous timestamp. Such a
coverability analysis technique is able to construct coverability trees/graphs
for unbounded Time-Basic Petri net models. The termination of the algorithm is
guaranteed as long as, within the input model, tokens growing without limit,
can be anonymized. This means that we are able to manage models that do not
exhibit Zeno behavior and do not express actions depending on infinite past
events. This is actually a reasonable limitation because, generally, real-world
examples do not exhibit such a behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6277</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6277</id><created>2014-09-22</created><updated>2015-02-19</updated><authors><author><keyname>Georgiadis</keyname><forenames>Loukas</forenames></author><author><keyname>Italiano</keyname><forenames>Giuseppe F.</forenames></author><author><keyname>Laura</keyname><forenames>Luigi</forenames></author><author><keyname>Parotsidis</keyname><forenames>Nikos</forenames></author></authors><title>2-Vertex Connectivity in Directed Graphs</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1407.3041</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We complement our study of 2-connectivity in directed graphs, by considering
the computation of the following 2-vertex-connectivity relations: We say that
two vertices v and w are 2-vertex-connected if there are two internally
vertex-disjoint paths from v to w and two internally vertex-disjoint paths from
w to v. We also say that v and w are vertex-resilient if the removal of any
vertex different from v and w leaves v and w in the same strongly connected
component. We show how to compute the above relations in linear time so that we
can report in constant time if two vertices are 2-vertex-connected or if they
are vertex-resilient. We also show how to compute in linear time a sparse
certificate for these relations, i.e., a subgraph of the input graph that has
O(n) edges and maintains the same 2-vertex-connectivity and vertex-resilience
relations as the input graph, where n is the number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6281</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6281</id><created>2014-09-22</created><authors><author><keyname>Kesidis</keyname><forenames>George</forenames></author><author><keyname>Mercer</keyname><forenames>Douglas</forenames></author><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author><author><keyname>Fdida</keyname><forenames>Serge</forenames></author></authors><title>Roaming charges for customers of cellular-wireless entrant and incumbent
  providers</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a simple two-player game involving a large incumbent and small
entrant into a cellular wireless access provider marketplace. The entrant's
customers must pay roaming charges. We assume that the roaming charges are
regulated, because if they are dictated by the incumbent then they could be set
so high so as to be a barrier to entry in the marketplace. The game is studied
at its Nash equilibrium. A roaming charge is identified that is arguably fair
in the sense that revenues for the access providers are proportionate to their
infrastructure costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6287</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6287</id><created>2014-09-22</created><authors><author><keyname>Vomlel</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Tichavsk&#xfd;</keyname><forenames>Petr</forenames></author></authors><title>On tensor rank of conditional probability tables in Bayesian networks</title><categories>cs.AI</categories><msc-class>68T37</msc-class><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A difficult task in modeling with Bayesian networks is the elicitation of
numerical parameters of Bayesian networks. A large number of parameters is
needed to specify a conditional probability table (CPT) that has a larger
parent set. In this paper we show that, most CPTs from real applications of
Bayesian networks can actually be very well approximated by tables that require
substantially less parameters. This observation has practical consequence not
only for model elicitation but also for efficient probabilistic reasoning with
these networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6288</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6288</id><created>2014-09-22</created><authors><author><keyname>Liu</keyname><forenames>Mengmeng</forenames></author><author><keyname>Ives</keyname><forenames>Zachary G.</forenames></author><author><keyname>Loo</keyname><forenames>Boon Thau</forenames></author></authors><title>Enabling Incremental Query Re-Optimization</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As declarative query processing techniques expand in scope --- to the Web,
data streams, network routers, and cloud platforms --- there is an increasing
need for adaptive query processing techniques that can re-plan in the presence
of failures or unanticipated performance changes. A status update on the data
distributions or the compute nodes may have significant repercussions on the
choice of which query plan should be running. Ideally, new system architectures
would be able to make cost-based decisions about reallocating work, migrating
data, etc., and react quickly as real-time status information becomes
available. Existing cost-based query optimizers are not incremental in nature,
and must be run &quot;from scratch&quot; upon each status or cost update. Hence, they
generally result in adaptive schemes that can only react slowly to updates.
  An open question has been whether it is possible to build a cost-based
re-optimization architecture for adaptive query processing in a streaming or
repeated query execution environment, e.g., by incrementally updating optimizer
state given new cost information. We show that this can be achieved
beneficially, especially for stream processing workloads. Our techniques build
upon the recently proposed approach of formulating query plan enumeration as a
set of recursive datalog queries; we develop a variety of novel optimization
approaches to ensure effective pruning in both static and incremental cases. We
implement our solution within an existing research query processing system, and
show that it effectively supports cost-based initial optimization as well as
frequent adaptivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6336</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6336</id><created>2014-09-22</created><updated>2015-08-18</updated><authors><author><keyname>Inoue</keyname><forenames>Hiroyasu</forenames></author></authors><title>Evidence for a creative dilemma posed by repeated collaborations</title><categories>cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0137418</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focused on how repeat collaborations in projects for inventions affect
performance. Repeat collaborations have two contradictory aspects. A positive
aspect is team development or experience, and a negative aspect is team
degeneration or decline. Since both contradicting phenomena are observed,
inventors have a dilemma as to whether they should keep collaborating in a team
or not. The dilemma has not previously been quantitatively analyzed.
  We provide quantitative and extensive analyses of the dilemma in creative
projects by using patent data from Japan and the United States. We confirm
three predictions to quantitatively validate the existence of the dilemma. The
first prediction is that the greater the patent a team achieves, the longer the
team will work together. The second prediction is that the impact of
consecutive patents decreases after a team makes a remarkable invention, which
is measured by the impact of patents. The third prediction is that the
expectation of impact with new teams is greater than that with the same teams
successful in the past. We find these predictions are validated in patents
published in Japan and the United States. On the basis of these three
predictions, we can quantitatively validate the dilemma in creative projects.
We also propose preventive strategies for degeneration. One is developing
technological diversity, and another is developing inventor diversity in
teams.We find the two strategies are both effective by validating with the
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6346</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6346</id><created>2014-09-22</created><authors><author><keyname>Placzek</keyname><forenames>Bartlomiej</forenames></author></authors><title>Communication-aware algorithms for target tracking in wireless sensor
  networks</title><categories>cs.NI</categories><comments>11 pages, 4 figures</comments><journal-ref>Communications in Computer and Information Science, vol. 431, pp.
  69-78 (2014)</journal-ref><doi>10.1007/978-3-319-07941-7_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces algorithms for target tracking in wireless sensor
networks (WSNs) that enable reduction of data communication cost. The objective
of the considered problem is to control movement of a mobile sink which has to
reach a moving target in the shortest possible time. Consumption of the WSN
energy resources is reduced by transferring only necessary data readings
(target positions) to the mobile sink. Simulations were performed to evaluate
the proposed algorithms against existing methods. The experimental results
confirm that the introduced tracking algorithms allow the data communication
cost to be considerably reduced without significant increase in the amount of
time that the sink needs to catch the target.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6354</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6354</id><created>2014-09-22</created><updated>2015-05-22</updated><authors><author><keyname>Coogan</keyname><forenames>Samuel</forenames></author><author><keyname>Arcak</keyname><forenames>Murat</forenames></author></authors><title>A Compartmental Model for Traffic Networks and its Dynamical Behavior</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a macroscopic traffic network flow model suitable for analysis as
a dynamical system, and we qualitatively analyze equilibrium flows as well as
convergence. Flows at a junction are determined by downstream supply of
capacity as well as upstream demand of traffic wishing to flow through the
junction. This approach is rooted in the celebrated Cell Transmission Model for
freeway traffic flow. Unlike related results which rely on certain system
cooperativity properties, our model generally does not possess these
properties. We show that the lack of cooperativity is in fact a useful feature
that allows traffic control methods, such as ramp metering, to be effective.
Finally, we leverage the results of the paper to develop a linear program for
optimal ramp metering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6359</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6359</id><created>2014-09-22</created><authors><author><keyname>Placzek</keyname><forenames>Bartlomiej</forenames></author></authors><title>Neighborhood Selection and Rules Identification for Cellular Automata: A
  Rough Sets Approach</title><categories>cs.AI nlin.CG</categories><comments>11 pages, 3 figures</comments><journal-ref>Lecture Notes in Computer Science, vol. 8385, pp. 721-730 (2014)</journal-ref><doi>10.1007/978-3-642-55195-6_68</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a method is proposed which uses data mining techniques based on
rough sets theory to select neighborhood and determine update rule for cellular
automata (CA). According to the proposed approach, neighborhood is detected by
reducts calculations and a rule-learning algorithm is applied to induce a set
of decision rules that define the evolution of CA. Experiments were performed
with use of synthetic as well as real-world data sets. The results show that
the introduced method allows identification of both deterministic and
probabilistic CA-based models of real-world phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6365</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6365</id><created>2014-09-22</created><authors><author><keyname>Georgiou</keyname><forenames>Konstantinos</forenames></author><author><keyname>Lee</keyname><forenames>Edward</forenames></author></authors><title>Lift &amp; Project Systems Performing on the Partial-Vertex-Cover Polytope</title><categories>cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study integrality gap (IG) lower bounds on strong LP and SDP relaxations
derived by the Sherali-Adams (SA), Lovasz-Schrijver-SDP (LS+), and
Sherali-Adams-SDP (SA+) lift-and-project (L&amp;P) systems for the
t-Partial-Vertex-Cover (t-PVC) problem, a variation of the classic Vertex-Cover
problem in which only t edges need to be covered. t-PVC admits a
2-approximation using various algorithmic techniques, all relying on a natural
LP relaxation. Starting from this LP relaxation, our main results assert that
for every epsilon &gt; 0, level-Theta(n) LPs or SDPs derived by all known L&amp;P
systems that have been used for positive algorithmic results (but the Lasserre
hierarchy) have IGs at least (1-epsilon)n/t, where n is the number of vertices
of the input graph. Our lower bounds are nearly tight.
  Our results show that restricted yet powerful models of computation derived
by many L&amp;P systems fail to witness c-approximate solutions to t-PVC for any
constant c, and for t = O(n). This is one of the very few known examples of an
intractable combinatorial optimization problem for which LP-based algorithms
induce a constant approximation ratio, still lift-and-project LP and SDP
tightenings of the same LP have unbounded IGs.
  We also show that the SDP that has given the best algorithm known for t-PVC
has integrality gap n/t on instances that can be solved by the level-1 LP
relaxation derived by the LS system. This constitutes another rare phenomenon
where (even in specific instances) a static LP outperforms an SDP that has been
used for the best approximation guarantee for the problem at hand. Finally, one
of our main contributions is that we make explicit of a new and simple
methodology of constructing solutions to LP relaxations that almost trivially
satisfy constraints derived by all SDP L&amp;P systems known to be useful for
algorithmic positive results (except the La system).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6366</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6366</id><created>2014-09-22</created><authors><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author></authors><title>A direct proof for Lovett's bound on the communication complexity of low
  rank matrices</title><categories>cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The log-rank conjecture in communication complexity suggests that the
deterministic communication complexity of any Boolean rank-r function is
bounded by polylog(r). Recently, major progress was made by Lovett who proved
that the communication complexity is bounded by O(r^1/2 * log r). Lovett's
proof is based on known estimates on the discrepancy of low-rank matrices. We
give a simple, direct proof based on a hyperplane rounding argument that in our
opinion sheds more light on the reason why a root factor suffices and what is
necessary to improve on this factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6369</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6369</id><created>2014-09-22</created><authors><author><keyname>AlKindy</keyname><forenames>Bassam</forenames></author><author><keyname>Couchot</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Mouly</keyname><forenames>Arnaud</forenames></author><author><keyname>Salomon</keyname><forenames>Michel</forenames></author><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author></authors><title>Finding the Core-Genes of Chloroplasts</title><categories>cs.CE q-bio.PE</categories><journal-ref>Journal of Bioscience, Biochemistry, and Bioinformatics,
  4(5):357--364, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the recent evolution of sequencing techniques, the number of available
genomes is rising steadily, leading to the possibility to make large scale
genomic comparison between sets of close species. An interesting question to
answer is: what is the common functionality genes of a collection of species,
or conversely, to determine what is specific to a given species when compared
to other ones belonging in the same genus, family, etc. Investigating such
problem means to find both core and pan genomes of a collection of species,
\textit{i.e.}, genes in common to all the species vs. the set of all genes in
all species under consideration. However, obtaining trustworthy core and pan
genomes is not an easy task, leading to a large amount of computation, and
requiring a rigorous methodology. Surprisingly, as far as we know, this
methodology in finding core and pan genomes has not really been deeply
investigated. This research work tries to fill this gap by focusing only on
chloroplastic genomes, whose reasonable sizes allow a deep study. To achieve
this goal, a collection of 99 chloroplasts are considered in this article. Two
methodologies have been investigated, respectively based on sequence
similarities and genes names taken from annotation tools. The obtained results
will finally be evaluated in terms of biological relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6382</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6382</id><created>2014-09-22</created><authors><author><keyname>Macedo</keyname><forenames>Rolando G&#xf3;mez</forenames></author><author><keyname>Zald&#xed;var</keyname><forenames>Felipe</forenames></author></authors><title>On the Category of Group Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the category of group codes, that generalizes the category of linear
codes over a finite field, and with the generalized notions of direct sums and
ndecomposable group codes, we prove that every MDS non trivial code, every
perfect non trivial code, and every constant weight nondegenerate group code
are indecomposable. We prove that every group code is a direct sum of
indecomposable group codes, and using this result we obtain the automorphism
groups of any group code in terms of its decomposition in indecomposable
components. We conclude with the determination of the structure of decomposable
cyclic group codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6392</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6392</id><created>2014-09-22</created><authors><author><keyname>Hattab</keyname><forenames>Ghaith</forenames></author><author><keyname>Ibnkahla</keyname><forenames>Mohamed</forenames></author></authors><title>Enhanced Pilot-Based Spectrum Sensing Algorithm</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures; published in Proc. IEEE Biennial Symps. on
  Commun. (QBSC'14), June 2014</comments><journal-ref>Proc. IEEE Biennial Symps. on Commun. (QBSC'14), pp.57-60, June
  2014</journal-ref><doi>10.1109/QBSC.2014.6841184</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we develop an enhanced pilot-based spectrum sensing algorithm
for cognitive radio. Unlike conventional pilot-based detectors which merely
detect the presence of pilot signals, the proposed detector also utilizes the
presence of the signal that carries the actual information. We analytically
compare the performance of the proposed detector with the conventional one, and
we show that the detection performance is significantly improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6394</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6394</id><created>2014-09-22</created><authors><author><keyname>Hattab</keyname><forenames>Ghaith</forenames></author><author><keyname>Ibnkahla</keyname><forenames>Mohamed</forenames></author></authors><title>Multiband Spectrum Sensing: Challenges and Limitations</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures; published in Proc. WiSense Workshop, August 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Multiband spectrum access presents the next generation of cognitive radio
networks (CRNs), where multiple bands are sensed and accessed to enhance the
network's throughput, improve spectrum's maintenance, and reduce handoff
frequency and data transmission interruptions due to the activities of the
primary users. In this paper, we discuss the challenges and limitations of the
major multiband spectrum sensing techniques. Particularly, we highlight the
edge-detection problem and examine several issues of the state-of-the-art
wavelet-based techniques. We also study the compressive sensing problem.
Finally, we highlight the promises of utilizing the angle-domain for the CRNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6397</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6397</id><created>2014-09-22</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Fagerberg</keyname><forenames>Rolf</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>Optimal local routing on Delaunay triangulations defined by empty
  equilateral triangles</title><categories>cs.CG</categories><comments>26 pages, 18 figures. Journal version of results presented at SODA
  2012 and CCCG 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic local routing algorithm that is guaranteed to find
a path between any pair of vertices in a half-$\theta_6$-graph (the
half-$\theta_6$-graph is equivalent to the Delaunay triangulation where the
empty region is an equilateral triangle). The length of the path is at most
$5/\sqrt{3} \approx 2.887$ times the Euclidean distance between the pair of
vertices. Moreover, we show that no local routing algorithm can achieve a
better routing ratio, thereby proving that our routing algorithm is optimal.
This is somewhat surprising because the spanning ratio of the
half-$\theta_6$-graph is 2, meaning that even though there always exists a path
whose lengths is at most twice the Euclidean distance, we cannot always find
such a path when routing locally.
  Since every triangulation can be embedded in the plane as a
half-$\theta_6$-graph using $O(\log n)$ bits per vertex coordinate via
Schnyder's embedding scheme (SODA 1990), our result provides a competitive
local routing algorithm for every such embedded triangulation. Finally, we show
how our routing algorithm can be adapted to provide a routing ratio of
$15/\sqrt{3} \approx 8.660$ on two bounded degree subgraphs of the
half-$\theta_6$-graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6404</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6404</id><created>2014-09-22</created><authors><author><keyname>Wang</keyname><forenames>Yuh-Shyang</forenames></author><author><keyname>Matni</keyname><forenames>Nikolai</forenames></author><author><keyname>Doyle</keyname><forenames>John C.</forenames></author></authors><title>Localized LQR Optimal Control</title><categories>cs.SY math.OC</categories><comments>Extended version for 2014 CDC submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a receding horizon like control scheme for localizable
distributed systems, in which the effect of each local disturbance is limited
spatially and temporally. We characterize such systems by a set of linear
equality constraints, and show that the resulting feasibility test can be
solved in a localized and distributed way. We also show that the solution of
the local feasibility tests can be used to synthesize a receding horizon like
controller that achieves the desired closed loop response in a localized manner
as well. Finally, we formulate the Localized LQR (LLQR) optimal control problem
and derive an analytic solution for the optimal controller. Through a numerical
example, we show that the LLQR optimal controller, with its constraints on
locality, settling time, and communication delay, can achieve similar
performance as an unconstrained H2 optimal controller, but can be designed and
implemented in a localized and distributed way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6414</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6414</id><created>2014-09-23</created><authors><author><keyname>Gao</keyname><forenames>Sicun</forenames></author><author><keyname>Kong</keyname><forenames>Soonho</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund</forenames></author></authors><title>Proof Generation from Delta-Decisions</title><categories>cs.LO</categories><comments>Appeared in SYNASC'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to generate and validate logical proofs of unsatisfiability from
delta-complete decision procedures that rely on error-prone numerical
algorithms. Solving this problem is important for ensuring correctness of the
decision procedures. At the same time, it is a new approach for automated
theorem proving over real numbers. We design a first-order calculus, and
transform the computational steps of constraint solving into logic proofs,
which are then validated using proof-checking algorithms. As an application, we
demonstrate how proofs generated from our solver can establish many nonlinear
lemmas in the the formal proof of the Kepler Conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6415</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6415</id><created>2014-09-23</created><updated>2014-09-29</updated><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Hume</keyname><forenames>Margee</forenames></author></authors><title>Ensemble Sensing on Smart Werables for a better Telehealth System</title><categories>cs.CY</categories><comments>This paper has been withdrawn due to some conceptual error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Telehealth offers interesting avenues for improving healthcare access in
vulnerable populations through use of electronic devices in the patient's home
that monitor and assess for early complications. However, complication of
operation and poor reliability hinders the wide acceptability of telehealth
services. We propose ensemble sensing on everyday wearable devices, which does
not impose the burden of carrying wearable sensors, yet offers a seamless and
simple platform to deliver telehealth services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6428</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6428</id><created>2014-09-23</created><authors><author><keyname>Waguih</keyname><forenames>Dalia Attia</forenames><affiliation>Qatar Computing Research Institute</affiliation></author><author><keyname>Berti-Equille</keyname><forenames>Laure</forenames><affiliation>Qatar Computing Research Institute</affiliation></author></authors><title>Truth Discovery Algorithms: An Experimental Evaluation</title><categories>cs.DB</categories><comments>13 pages, 17 figures, Qatar Computing Research Institute Technical
  Report, May 2014</comments><report-no>QCRI Technical Report, May 2014</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A fundamental problem in data fusion is to determine the veracity of
multi-source data in order to resolve conflicts. While previous work in truth
discovery has proved to be useful in practice for specific settings, sources'
behavior or data set characteristics, there has been limited systematic
comparison of the competing methods in terms of efficiency, usability, and
repeatability. We remedy this deficit by providing a comprehensive review of 12
state-of-the art algorithms for truth discovery. We provide reference
implementations and an in-depth evaluation of the methods based on extensive
experiments on synthetic and real-world data. We analyze aspects of the problem
that have not been explicitly studied before, such as the impact of
initialization and parameter setting, convergence, and scalability. We provide
an experimental framework for extensively comparing the methods in a wide range
of truth discovery scenarios where source coverage, numbers and distributions
of conflicts, and true positive claims can be controlled and used to evaluate
the quality and performance of the algorithms. Finally, we report comprehensive
findings obtained from the experiments and provide new insights for future
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6431</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6431</id><created>2014-09-23</created><authors><author><keyname>Asplund</keyname><forenames>Mikael</forenames></author><author><keyname>Nadjm-Tehrani</keyname><forenames>Simin</forenames></author></authors><title>Modelling Correlated Mobility</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When nodes in a mobile network cluster together or move according to common
external factors (e.g., cars that follow the road network), the resulting
contact patterns become correlated. In this work we address the question of
modelling such correlated mobility movements for the analysis of intermittently
connected networks. We propose to use the concept of node colouring time to
characterise dynamic node contact patterns. We analyse how this model compares
to existing work, and demonstrate how to extract the relevant data from actual
trace files. Moreover, we show how this information can be used to derive the
latency distribution of DTN routing protocols. Our model achieves a very good
fit to simulated results based on real vehicular mobility traces, whereas
models which assumes independent contacts do not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6440</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6440</id><created>2014-09-23</created><updated>2014-10-31</updated><authors><author><keyname>Khogali</keyname><forenames>Rashid</forenames></author></authors><title>A non-linear learning &amp; classification algorithm that achieves full
  training accuracy with stellar classification accuracy</title><categories>cs.CV cs.LG</categories><comments>43 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fast Non-linear and non-iterative learning and classification algorithm is
synthesized and validated. This algorithm named the &quot;Reverse Ripple
Effect(R.R.E)&quot;, achieves 100% learning accuracy but is computationally
expensive upon classification. The R.R.E is a (deterministic) algorithm that
super imposes Gaussian weighted functions on training points. In this work, the
R.R.E algorithm is compared against known learning and classification
techniques/algorithms such as: the Perceptron Criterion algorithm, Linear
Support Vector machines, the Linear Fisher Discriminant and a simple Neural
Network. The classification accuracy of the R.R.E algorithm is evaluated using
simulations conducted in MATLAB. The R.R.E algorithm's behaviour is analyzed
under linearly and non-linearly separable data sets. For the comparison with
the Neural Network, the classical XOR problem is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6448</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6448</id><created>2014-09-23</created><authors><author><keyname>Han</keyname><forenames>Bo</forenames></author><author><keyname>He</keyname><forenames>Bo</forenames></author><author><keyname>Sun</keyname><forenames>Tingting</forenames></author><author><keyname>Ma</keyname><forenames>Mengmeng</forenames></author><author><keyname>Lendasse</keyname><forenames>Amaury</forenames></author></authors><title>HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition
  using Hierarchical Feature Selection</title><categories>cs.CV cs.LG</categories><comments>Submitted to IEEE Computational Intelligence Magazine in 09/2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel method for fast face recognition called
L1/2 Regularized Sparse Representation using Hierarchical Feature Selection
(HSR). By employing hierarchical feature selection, we can compress the scale
and dimension of global dictionary, which directly contributes to the decrease
of computational cost in sparse representation that our approach is strongly
rooted in. It consists of Gabor wavelets and Extreme Learning Machine
Auto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features
can be extracted at multiple scales and orientations to form Gabor-feature
based image, which in turn improves the recognition rate. Besides, in the
presence of occluded face image, the scale of Gabor-feature based global
dictionary can be compressed accordingly because redundancies exist in
Gabor-feature based occlusion dictionary. For ELM-AE part, the dimension of
Gabor-feature based global dictionary can be compressed because
high-dimensional face images can be rapidly represented by low-dimensional
feature. By introducing L1/2 regularization, our approach can produce sparser
and more robust representation compared to regularized Sparse Representation
based Classification (SRC), which also contributes to the decrease of the
computational cost in sparse representation. In comparison with related work
such as SRC and Gabor-feature based SRC (GSRC), experimental results on a
variety of face databases demonstrate the great advantage of our method for
computational cost. Moreover, we also achieve approximate or even better
recognition rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6466</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6466</id><created>2014-09-23</created><authors><author><keyname>Li</keyname><forenames>Yongming</forenames></author><author><keyname>Ma</keyname><forenames>Zhanyou</forenames></author></authors><title>Quantitative Computation Tree Logic Model Checking Based on Generalized
  Possibility Measures</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study generalized possibilistic computation tree logic model checking in
this paper, which is an extension of possibilistic computation logic model
checking introduced by Y.Li, Y.Li and Z.Ma (2014). The system is modeled by
generalized possibilistic Kripke structures (GPKS, in short), and the verifying
property is specified by a generalized possibilistic computation tree logic
(GPoCTL, in short) formula. Based on generalized possibility measures and
generalized necessity measures, the method of generalized possibilistic
computation tree logic model checking is discussed, and the corresponding
algorithm and its complexity are shown in detail. Furthermore, the comparison
between PoCTL introduced in (2013) and GPoCTL is given. Finally, a thermostat
example is given to illustrate the GPoCTL model-checking method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6470</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6470</id><created>2014-09-23</created><updated>2015-04-28</updated><authors><author><keyname>Agarwal</keyname><forenames>Manas</forenames></author><author><keyname>Singh</keyname><forenames>Rishi Ranjan</forenames></author><author><keyname>Chaudhary</keyname><forenames>Shubham</forenames></author><author><keyname>Iyengar</keyname><forenames>Sudarshan</forenames></author></authors><title>BOLT : Efficient Betweenness Ordering in almost Linear Time</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centrality measures, erstwhile popular amongst the sociologists and
psychologists, have seen wide and increasing applications across several
disciplines of late. In conjunction with the big data problems there came the
need to analyze big networks and in this connection, centrality measures became
of great interest to the community of mathematicians, computer scientists and
physicists. While it is an important question to ask how one can rank vertices
based on their importance in a network, there hasn't been a commonly accepted
definition, mainly due to the subjectivity of the term &quot;importance&quot;. Amongst a
plethora of application specific definitions available in the literature to
rank the vertices, closeness centrality, betweenness centrality and eigenvector
centrality (page-rank) have been the most important and widely applied ones. In
the current paper, we formulate a method to determine the betweenness ordering
of $k$ vertices without exactly computing their betweenness indices - which is
a daunting task for networks of large size. The method results very efficient
ordering even when runs for linear time in the number of edges. We apply our
approach to find the betweenness ordering of $k$ vertices in several synthetic
and real world graphs. We compare our method with the available techniques in
the literature and show that our method fares several times better than the
currently known techniques. We further show that the accuracy of our algorithm
gets better with the increase in size and density of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6489</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6489</id><created>2014-09-23</created><updated>2014-10-16</updated><authors><author><keyname>Roux</keyname><forenames>Stephane Le</forenames></author></authors><title>On terminating improvement in two-player games</title><categories>cs.GT</categories><comments>The proof of Proposition 2 (p 11-12) was incomplete in the first
  version</comments><msc-class>91A</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A real-valued game has the finite improvement property (FIP), if starting
from an arbitrary strategy profile and letting the players change strategies to
increase their individual payoffs in a sequential but non-deterministic order
always reaches a Nash equilibrium. E.g., potential games have the FIP. Many of
them have the FIP by chance nonetheless, since modifying even a single payoff
may ruin the property. This article characterises (in quadratic time) the class
of the finite games where FIP not only holds but is also preserved when
modifying all the occurrences of an arbitrary payoff. The characterisation
relies on a pattern-matching sufficient condition for games (finite or
infinite) to enjoy the FIP, and is followed by an inductive description of this
class.
  A real-valued game is weakly acyclic if the improvement described above can
reach a Nash equilibrium. This article characterises the finite such games
using Markov chains and almost sure convergence to equilibrium. It also gives
an inductive description of the two-player such games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6497</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6497</id><created>2014-09-23</created><authors><author><keyname>Dinis</keyname><forenames>L.</forenames></author></authors><title>Optimal sequence for Parrondo games</title><categories>physics.soc-ph cs.GT</categories><comments>7 pages, 3 figures</comments><journal-ref>Physical Review E 77, 021124 (2008)</journal-ref><doi>10.1103/PhysRevE.77.021124</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm based on backward induction is devised in order to compute the
optimal sequence of games to be played in Parrondo games. The algorithm can be
used to find the optimal sequence for any finite number of turns or in the
steady state, showing that ABABB... is the sequence with the highest steady
state average gain. The algorithm can also be generalised to find the optimal
adaptive strategy in a multi-player version of the games, where a finite number
of players may choose, at every turn, the game the whole ensemble should play.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6498</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6498</id><created>2014-09-23</created><updated>2015-02-17</updated><authors><author><keyname>Chung</keyname><forenames>Moo K.</forenames></author><author><keyname>Qiu</keyname><forenames>Anqi</forenames></author><author><keyname>Seo</keyname><forenames>Seongho</forenames></author><author><keyname>Vorperian</keyname><forenames>Houri K.</forenames></author></authors><title>Unified Heat Kernel Regression for Diffusion, Kernel Smoothing and
  Wavelets on Manifolds and Its Application to Mandible Growth Modeling in CT
  Images</title><categories>cs.CV stat.ME</categories><comments>Accepted in Medical Image Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel kernel regression framework for smoothing scalar surface
data using the Laplace-Beltrami eigenfunctions. Starting with the heat kernel
constructed from the eigenfunctions, we formulate a new bivariate kernel
regression framework as a weighted eigenfunction expansion with the heat kernel
as the weights. The new kernel regression is mathematically equivalent to
isotropic heat diffusion, kernel smoothing and recently popular diffusion
wavelets. Unlike many previous partial differential equation based approaches
involving diffusion, our approach represents the solution of diffusion
analytically, reducing numerical inaccuracy and slow convergence. The numerical
implementation is validated on a unit sphere using spherical harmonics. As an
illustration, we have applied the method in characterizing the localized growth
pattern of mandible surfaces obtained in CT images from subjects between ages 0
and 20 years by regressing the length of displacement vectors with respect to
the template surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6502</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6502</id><created>2014-09-23</created><updated>2015-03-17</updated><authors><author><keyname>Cito</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Leitner</keyname><forenames>Philipp</forenames></author><author><keyname>Fritz</keyname><forenames>Thomas</forenames></author><author><keyname>Gall</keyname><forenames>Harald C.</forenames></author></authors><title>The Making of Cloud Applications An Empirical Study on Software
  Development for the Cloud</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is gaining more and more traction as a deployment and
provisioning model for software. While a large body of research already covers
how to optimally operate a cloud system, we still lack insights into how
professional software engineers actually use clouds, and how the cloud impacts
development practices. This paper reports on the first systematic study on how
software developers build applications in the cloud. We conducted a
mixed-method study, consisting of qualitative interviews of 25 professional
developers and a quantitative survey with 294 responses. Our results show that
adopting the cloud has a profound impact throughout the software development
process, as well as on how developers utilize tools and data in their daily
work. Among other things, we found that (1) developers need better means to
anticipate runtime problems and rigorously define metrics for improved fault
localization and (2) the cloud offers an abundance of operational data,
however, developers still often rely on their experience and intuition rather
than utilizing metrics. From our findings, we extracted a set of guidelines for
cloud development and identified challenges for researchers and tool vendors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6503</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6503</id><created>2014-09-23</created><updated>2014-10-02</updated><authors><author><keyname>He</keyname><forenames>Zhe</forenames></author><author><keyname>Huang</keyname><forenames>Yi-Ming</forenames></author><author><keyname>Xu</keyname><forenames>Rui-Jie</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author><author><keyname>Ou-Yang</keyname><forenames>Zhong-Can</forenames></author></authors><title>Network cluster detecting in associated bi-graph view</title><categories>physics.soc-ph cs.SI</categories><comments>6 pagers,6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We find there is relationship between the associated bigraph and the cluster
(or community) detecting on network. By imbedding the associated bigraph of
some network (suppose it has cluster structures) into some space, we can
identify the clusters on this network, which is a new method for network
cluster detecting. And this method, of which the physical meaning is clear and
the time complexity is acceptable, may provide us a new point to understand the
structure and character of networks. In this paper, We test the methods on
serval computer-generated networks and real networks. A computer-generated
network with 128 vertexes and the Zachary Network, which presents the structure
of a karate club, can be partitioned correctly by these methods. And the
Dolphin Network, which presents the relationship between 62 dolphins on the
coast of New Zealand, is partitioned reasonably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6505</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6505</id><created>2014-09-23</created><updated>2015-05-21</updated><authors><author><keyname>Chevalier</keyname><forenames>Pierre-Yves</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author></authors><title>Efficient Algorithms for the Consensus Decision Problem</title><categories>cs.SY cs.MA math.OC</categories><comments>Small modifications after comments from reviewers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of determining if a discrete time switched consensus
system converges for any switching sequence and that of determining if it
converges for at least one switching sequence. For these two problems, we
provide necessary and sufficient conditions that can be checked in singly
exponential time. As a side result, we prove the existence of a polynomial time
algorithm for the first problem when the system switches between only two
subsystems whose corresponding graphs are undirected, a problem that had been
suggested to be NP-hard by Blondel and Olshevsky.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6510</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6510</id><created>2014-09-23</created><authors><author><keyname>Cela</keyname><forenames>Eranda</forenames></author><author><keyname>Deineko</keyname><forenames>Vladimir G.</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>Linearizable special cases of the QAP</title><categories>math.OC cs.DS</categories><comments>11 pages</comments><msc-class>90C27</msc-class><acm-class>F.2.2; G.1.6; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider special cases of the quadratic assignment problem (QAP) that are
linearizable in the sense of Bookhold. We provide combinatorial
characterizations of the linearizable instances of the weighted feedback arc
set QAP, and of the linearizable instances of the traveling salesman QAP. As a
by-product, this yields a new well-solvable special case of the weighted
feedback arc set problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6512</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6512</id><created>2014-09-23</created><updated>2014-09-28</updated><authors><author><keyname>Moulahi</keyname><forenames>Bilel</forenames></author><author><keyname>Tamine</keyname><forenames>Lynda</forenames></author><author><keyname>Yahia</keyname><forenames>Sadok Ben</forenames></author></authors><title>Learning to Match for Multi-criteria Document Relevance</title><categories>cs.IR</categories><comments>9 pages</comments><acm-class>H.3.3; H.4.0; H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In light of the tremendous amount of data produced by social media, a large
body of research have revisited the relevance estimation of the users'
generated content. Most of the studies have stressed the multidimensional
nature of relevance and proved the effectiveness of combining the different
criteria that it embodies. Traditional relevance estimates combination methods
are often based on linear combination schemes. However, despite being
effective, those aggregation mechanisms are not effective in real-life
applications since they heavily rely on the non-realistic independence property
of the relevance dimensions. In this paper, we propose to tackle this issue
through the design of a novel fuzzy-based document ranking model. We also
propose an automated methodology to capture the importance of relevance
dimensions, as well as information about their interaction. This model, based
on the Choquet Integral, allows to optimize the aggregated documents relevance
scores using any target information retrieval relevance metric. Experiments
within the TREC Microblog task and a social personalized information retrieval
task highlighted that our model significantly outperforms a wide range of
state-of-the-art aggregation operators, as well as a representative learning to
rank methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6526</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6526</id><created>2014-09-23</created><authors><author><keyname>Bafna</keyname><forenames>Siddharth</forenames></author><author><keyname>Pandey</keyname><forenames>Avichal</forenames></author><author><keyname>Verma</keyname><forenames>Kshitiz</forenames></author></authors><title>Anatomy of the Internet Peering Disputes</title><categories>cs.NI</categories><comments>6 Pages, 4 Figures explaining classification of the collected Peering
  Disputes, 1 Table describing collected Peering Disputes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet peering disputes have had an impact on the Internet AS-graph. As a
result, the customers of the ASes often suffer because they cannot reach to the
all of the Internet. There is a lack of study of the disputes that have taken
place so far, even though each dispute is individually well understood. In this
paper, we collect data on 26 disputes from from various resources, categorise
them in a systemic manner to understand them from geographical and temporal
point of views. There are some ASes that are more involved in disputes than
others. In the end, we conclude we need to collect more data as it would be
more interesting to have data on the Internet peering disputes around the
world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6540</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6540</id><created>2014-09-23</created><authors><author><keyname>Tuxanidy</keyname><forenames>Aleksandr</forenames></author><author><keyname>Wang</keyname><forenames>Qiang</forenames></author></authors><title>Compositional inverses, complete mappings, orthogonal Latin squares and
  bent functions</title><categories>math.NT cs.IT math.CO math.IT</categories><msc-class>11T06, 05B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study compositional inverses of permutation polynomials, complete
mappings, mutually orthogonal Latin squares, and bent vectorial functions.
Recently it was obtained in [33] the compositional inverses of linearized
permutation binomials over finite fields. It was also noted in [29] that
computing inverses of bijections of subspaces have applications in determining
the compositional inverses of certain permutation classes related to linearized
polynomials. In this paper we obtain compositional inverses of a class of
linearized binomials permuting the kernel of the trace map. As an application
of this result, we give the compositional inverse of a class of complete
mappings. This complete mapping class improves upon a recent construction given
in [34]. We also construct recursively a class of complete mappings involving
multi-trace functions. Finally we use these complete mappings to derive a set
of mutually orthogonal Latin squares, and to construct a class of $p$-ary bent
vectorial functions from the Maiorana-McFarland class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6548</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6548</id><created>2014-09-22</created><authors><author><keyname>Januzaj</keyname><forenames>Eshref</forenames></author><author><keyname>Kriegel</keyname><forenames>Hans-Peter</forenames></author><author><keyname>Pfeifle</keyname><forenames>Martin</forenames></author></authors><title>Scalable Density-Based Distributed Clustering</title><categories>cs.DB</categories><comments>12 pages, 11 figures</comments><journal-ref>8th European Conference on Principles and Practice of Knowledge
  Discovery in Databases (PKDD) Pisa, Italy, September 20-24, 2004, J.-F.
  Boulicaut et al. (Eds.): LNAI 3202, pp 231-244, Springer-Verlag Berlin
  Heidelberg 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering has become an increasingly important task in analysing huge
amounts of data. Traditional applications require that all data has to be
located at the site where it is scrutinized. Nowadays, large amounts of
heterogeneous, complex data reside on different, independently working
computers which are connected to each other via local or wide area networks. In
this paper, we propose a scalable density-based distributed clustering
algorithm which allows a user-defined trade-off between clustering quality and
the number of transmitted objects from the different local sites to a global
server site. Our approach consists of the following steps: First, we order all
objects located at a local site according to a quality criterion reflecting
their suitability to serve as local representatives. Then we send the best of
these representatives to a server site where they are clustered with a slightly
enhanced density-based clustering algorithm. This approach is very efficient,
because the local detemination of suitable representatives can be carried out
quickly and independently from each other. Furthermore, based on the scalable
number of the most suitable local representatives, the global clustering can be
done very effectively and efficiently. In our experimental evaluation, we will
show that our new scalable density-based distributed clustering approach
results in high quality clusterings with scalable transmission cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6551</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6551</id><created>2014-09-23</created><authors><author><keyname>Chimani</keyname><forenames>Markus</forenames></author><author><keyname>Spoerhase</keyname><forenames>Joachim</forenames></author></authors><title>Network Design Problems with Bounded Distances via Shallow-Light Steiner
  Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a directed graph $G$ with non-correlated edge lengths and costs, the
\emph{network design problem with bounded distances} asks for a cost-minimal
spanning subgraph subject to a length bound for all node pairs. We give a
bi-criteria $(2+\varepsilon,O(n^{0.5+\varepsilon}))$-approximation for this
problem. This improves on the currently best known linear approximation bound,
at the cost of violating the distance bound by a factor of at
most~$2+\varepsilon$.
  In the course of proving this result, the related problem of \emph{directed
shallow-light Steiner trees} arises as a subproblem. In the context of directed
graphs, approximations to this problem have been elusive. We present the first
non-trivial result by proposing a
$(1+\varepsilon,O(|R|^{\varepsilon}))$-ap\-proxi\-ma\-tion, where $R$ are the
terminals.
  Finally, we show how to apply our results to obtain an
$(\alpha+\varepsilon,O(n^{0.5+\varepsilon}))$-approximation for
\emph{light-weight directed $\alpha$-spanners}. For this, no non-trivial
approximation algorithm has been known before. All running times depends on $n$
and $\varepsilon$ and are polynomial in $n$ for any fixed $\varepsilon&gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6554</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6554</id><created>2014-09-23</created><authors><author><keyname>Mirzahasanloo</keyname><forenames>Taher Shahbazi</forenames></author></authors><title>A Single-Processor Approach to Speech Processing Pipeline of Bilateral
  Cochlear Implants</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This dissertation covers a single-processor approach to the speech processing
pipeline of bilateral Cochlear Implants (CIs). The use of only a single
processor to provide binaural stimulation signals overcomes the synchronization
problem, which is an existing challenging problem in the deployment of
bilateral CI devices. The developed single-processor speech processing pipeline
provides CI users with a sense of directionality. Its non-synchronization
feature as well as low computational and memory requirements make it a suitable
solution for actual deployment. A speech enhancement framework is developed
that incorporates different non-Euclidean speech distortion criteria and
different noise environments. This framework not only allows the design of
environment-optimized parameters but also enables a user-specific solution
where the anthropometric measurements of an individual user are incorporated
into the training process to obtain individualized bilateral parameters. The
developed techniques are primarily meant for bilateral CIs, however, they are
general purpose in the sense that they are also applicable to binaural hearing
aids, bimodal devices having hearing aid in one ear and cochlear implant in the
other ear as well as dual-channel speech enhancement applications. Extensive
experiments have shown the effectiveness of the developed solution in six
commonly encountered noise environments compared to a similar one-channel
pipeline when using two separate processors or when using independent
sequential processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6556</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6556</id><created>2014-09-23</created><authors><author><keyname>Isa</keyname><forenames>Mohd Anuar Mat</forenames></author><author><keyname>Hashim</keyname><forenames>Habibah</forenames></author></authors><title>Adversary Model: Adaptive Chosen Ciphertext Attack with Timing Attack</title><categories>cs.CR</categories><comments>Extended paper for 'A Secure TFTP Protocol with Security Proofs', in
  Lecture Notes in Engineering and Computer Science: Proceedings of The World
  Congress on Engineering 2014, (WCE 2014), 2014, vol. 1</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We have introduced a novel adversary model in Chosen-Ciphertext Attack with
Timing Attack (CCA2-TA) and it was a practical model because the model
incorporates the timing attack. This paper is an extended paper for 'A Secure
TFTP Protocol with Security Proofs'.
  Keywords - Timing Attack, Random Oracle Model, Indistinguishabilit, Chosen
Plaintext Attack, CPA, Chosen Ciphertext Attack, IND-CCA1, Adaptive Chosen
Ciphertext Attack, IND-CCA2, Trivial File Transfer Protocol, TFTP, Security,
Trust, Privacy, Trusted Computing, UBOOT, AES, IOT, Lightweight, Asymmetric,
Symmetric, Raspberry Pi, ARM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6559</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6559</id><created>2014-09-22</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>E-Business Experiences with Online Auctions</title><categories>cs.CY cs.GT</categories><comments>13 pages, 5 figures</comments><journal-ref>Managing E-Commerce and Mobile Computing Technologies IRM Press,
  2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online auctions are among the most influential e-business applications. Their
impact on trading for businesses, as well as consumers, is both remarkable and
inevitable. There have been considerable efforts in setting up market places,
but, with respects to market volume, online trading is still in its early
stages. This chapter discusses the benefits of the concept of Internet
marketplaces, with the highest impact on pricing strategies, namely, the
conduction of online business auctions. We discuss their benefits, problems and
possible solutions. In addition, we sketch actions for suppliers to achieve a
better strategic position in the upcoming Internet market places.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6560</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6560</id><created>2014-09-23</created><authors><author><keyname>Sahneh</keyname><forenames>Faryad Darabi</forenames></author><author><keyname>Scoglio</keyname><forenames>Caterina</forenames></author><author><keyname>Van Mieghem</keyname><forenames>Piet</forenames></author></authors><title>Exact Coupling Threshold for Structural Transition in Interconnected
  Networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1103/PhysRevE.92.040801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interconnected networks are mathematical representation of systems where two
or more simple networks are coupled to each other. Depending on the coupling
weight between the two components, the interconnected network can function in
two regimes: one where the two networks are structurally distinguishable, and
one where they are not. The coupling threshold--denoting this structural
transition--is one of the most crucial concepts in interconnected networks.
Yet, current information about the coupling threshold is limited. This letter
presents an analytical expression for the exact value of the coupling threshold
and outlines network interrelation implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6578</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6578</id><created>2014-09-22</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>MontiArc - Architectural Modeling of Interactive Distributed and
  Cyber-Physical Systems</title><categories>cs.SE</categories><comments>102 pages, RWTH Aachen University, Technical Report. AIB-2012-03.
  February 2012</comments><report-no>AIB-2012-03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents MontiArc, a modeling language for the description of
Component &amp; Connector architectures. A component is a unit executing
computations and/or storing data. Information flow between components is
modeled via unidirectional connectors connecting typed, directed ports of the
interfaces of components. Language features of the ADL MontiArc include
hierarchical decomposition of components, subtyping by structural inheritance,
component type definitions and reference declarations for reuse, generic
component types and configurable components, syntactic sugar for connectors,
and controlled implicit creation of connections and subcomponent declarations.
This technical report gives an overview of the MontiArc language and is a
reference for the MontiArc grammar intended to enable reuse and extension of
MontiArc and MontiArc related tools. MontiArc is implemented using the DSL
framework MontiCore. Available tools include an editor with syntax highlighting
and code completion as well as a simulation framework with a Java code
generator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6579</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6579</id><created>2014-09-22</created><authors><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Engineering Autonomous Driving Software</title><categories>cs.SE</categories><comments>29 pages, 8 figures, C. Rouff, M. Hinchey (Eds.). Experience from the
  DARPA Urban Challenge. Springer, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A larger number of people with heterogeneous knowledge and skills running a
project together needs an adaptable, target, and skill-specific engineering
process. This especially holds for a project to develop a highly innovative,
autonomously driving vehicle to participate in the 2007 DARPA Urban Challenge.
In this contribution, we present essential elements of a software and systems
engineering process to develop a so-called artificial intelligence capable of
driving autonomously in complex urban situations. The process itself includes
agile concepts, like a test first approach, continuous integration of all
software modules, and a reliable release and configuration management assisted
by software tools in integrated development environments. However, one of the
most important elements for an efficient and stringent development is the
ability to efficiently test the behavior of the developed system in a flexible
and modular system simulation for urban situations both interactively and
unattendedly. We call this the simulate first approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6580</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6580</id><created>2014-09-22</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Modeling Language Variability</title><categories>cs.SE</categories><comments>16 pages, 14 figures</comments><journal-ref>Workshop on Modeling, Development and Verification of Adaptive
  Systems. 16th Monterey Workshop, Redmond, Microsoft Research, March 31-April
  2, 2010. LNCS vol. 6662, pp. 17-32, Springer, 2011</journal-ref><doi>10.1007/978-3-642-21292-5_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A systematic way of defining variants of a modeling language is useful for
adapting the language to domain or project specific needs. Variants can be
obtained by adapting the syntax or semantics of the language. In this paper, we
take a formal approach to define modeling language variability and show how
this helps to reason about language variants, models, and their semantics
formally. We introduce the notion of semantic language refinement meaning that
one semantics variant is implied by another. Leaving open all variation points
that a modeling language offers yields the notion of the inner semantics of
that language. Properties of the modeling language which do not depend on the
selection of specific variants are called invariant language properties with
respect to a variation point. These properties consequently follow from the
inner semantics of a model or language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6581</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6581</id><created>2014-09-22</created><authors><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Rendel</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Busse</keyname><forenames>Carsten</forenames></author><author><keyname>Jablonski</keyname><forenames>Thorsten</forenames></author><author><keyname>Wolf</keyname><forenames>Fabian</forenames></author></authors><title>Product Line Metrics for Legacy Software in Practice</title><categories>cs.SE</categories><comments>4 pages, 5 figures, Proceedings of the 14th International Software
  Product Line Conference (SPLC 2010) Volume 2, Lancester University, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, customer products like vehicles do not only contain mechanical
parts but also a highly complex software and their manufacturers have to offer
many variants of technically very similar systems with sometimes only small
differences in their behavior. The proper reuse of software artifacts which
realize this behavior using a software product line is discussed in recent
literature and appropriate methods and techniques for their management are
proposed. However, establishing a software product line for integrating already
existing legacy software to reuse valuable resources for future similar
products is very company-specific. In this paper, a method is outlined for
evaluating objectively a legacy software's potential to create a software
product line. This method is applied to several development projects at
Volkswagen AG Business Unit Braunschweig to evaluate the software product line
potential for steering systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6582</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6582</id><created>2014-09-22</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Modeling Language Variability</title><categories>cs.SE</categories><comments>9 pages, 6 figures, Workshop on Modeling, Development and
  Verification of Adaptive Systems. (16th Monterey Workshop). Redmond,
  Microsoft Research, Mar. 31- Apr. 2, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A systematic way of defining variants of a modeling language is useful for
adopting the language to domain or project specific needs. Variants can be
obtained by adopting the syntax or semantics of the language. In this paper, we
take a formal approach to define modeling language variability and show how
this helps to reason about language variants, models, and their semantics
formally. We introduce the notion of semantic language refinement meaning that
one semantics variant is implied by another.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6583</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6583</id><created>2014-09-22</created><authors><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Rendel</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Measuring the Ability to Form a Product Line from Existing Products</title><categories>cs.SE</categories><comments>4 pages, 5 figures, Proceedings of the Fourth International Workshop
  on Variability Modelling of Software-intensive Systems (VaMoS). ICB Research
  Report No. 37, Institute for Computer Science and Business Information
  Systems, University of Duisburg-Essen, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A product line approach can save valuable resources by reusing artifacts.
Especially for software artifacts, the reuse of existing components is highly
desirable. In recent literature, the creation of software product lines is
mainly proposed from a top-down point of view regarding features which are
visible by customers. In practice, however, the design for a product line often
arises from one or few existing products that descend from a very first product
starting with copy-paste and evolving individually. In this contribution, we
propose the theoretical basis to derive a set of metrics for evaluating similar
software products in an objective manner. These metrics are used to evaluate
the set of product's ability to form a product line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6584</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6584</id><created>2014-09-22</created><authors><author><keyname>Rauskolb</keyname><forenames>Fred W.</forenames></author><author><keyname>Berger</keyname><forenames>Kai</forenames></author><author><keyname>Lipski</keyname><forenames>Christian</forenames></author><author><keyname>Magnor</keyname><forenames>Marcus</forenames></author><author><keyname>Cornelsen</keyname><forenames>Karsten</forenames></author><author><keyname>Effertz</keyname><forenames>Jan</forenames></author><author><keyname>Form</keyname><forenames>Thomas</forenames></author><author><keyname>Graefe</keyname><forenames>Fabian</forenames></author><author><keyname>Ohl</keyname><forenames>Sebastian</forenames></author><author><keyname>Schumacher</keyname><forenames>Walter</forenames></author><author><keyname>Wille</keyname><forenames>J&#xf6;rn Marten</forenames></author><author><keyname>Hecker</keyname><forenames>Peter</forenames></author><author><keyname>Nothdurft</keyname><forenames>Tobias</forenames></author><author><keyname>Doering</keyname><forenames>Michael</forenames></author><author><keyname>Homeier</keyname><forenames>Kai</forenames></author><author><keyname>Morgenroth</keyname><forenames>Johannes</forenames></author><author><keyname>Wolf</keyname><forenames>Lars</forenames></author><author><keyname>Basarke</keyname><forenames>Christian</forenames></author><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>G&#xfc;lke</keyname><forenames>Tim</forenames></author><author><keyname>Klose</keyname><forenames>Felix</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Caroline: An Autonomously Driving Vehicle for Urban Environments</title><categories>cs.RO</categories><comments>68 pages, 7 figures</comments><journal-ref>M. Buehler, K. Iagnemma, S. Singh (Eds.). The DARPA Urban
  Challenge - Autonomous Vehicles in City Traffic. Springer Tracts in Advanced
  Robotics, Volume 56, pp. 441-508, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 2007 DARPA Urban Challenge afforded the golden opportunity for the
Technische Universit\&quot;at Braunschweig to demonstrate its abilities to develop
an autonomously driving vehicle to compete with the world's best competitors.
After several stages of qualification, our team CarOLO qualified early for the
DARPA Urban Challenge Final Event and was among only eleven teams from
initially 89 competitors to compete in the final. We had the ability to work
together in a large group of experts, each contributing his expertise in his
discipline, and significant organisational, financial and technical support by
local sponsors who helped us to become the best non-US team. In this report, we
describe the 2007 DARPA Urban Challenge, our contribution &quot;Caroline&quot;, the
technology and algorithms along with her performance in the DARPA Urban
Challenge Final Event on November 3, 2007.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6585</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6585</id><created>2014-09-22</created><authors><author><keyname>Cengarle</keyname><forenames>Maria Victoria</forenames></author><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Variability within Modeling Language Definitions</title><categories>cs.SE</categories><comments>15 pages, 14 figures, 1 table</comments><journal-ref>Model Driven Engineering Languages and Systems. Proceedings of
  MODELS 2009, LNCS 5795. Denver, Colorado, USA, October 2009</journal-ref><doi>10.1007/978-3-642-04425-0_54</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a taxonomy of the variability mechanisms offered by modeling
languages. The definition of a formal language encompasses a syntax and a
semantic domain as well as the mapping that relates them, thus language
variabilities are classified according to which of those three pillars they
address. This work furthermore proposes a framework to explicitly document and
manage the variation points and their corresponding variants of a variable
modeling language. The framework enables the systematic study of various kinds
of variabilities and their interdependencies. Moreover, it allows a methodical
customization of a language, for example, to a given application domain. The
taxonomy of variability is explicitly of interest for the UML to provide a more
precise understanding of its variation points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6586</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6586</id><created>2014-09-22</created><authors><author><keyname>Herrmann</keyname><forenames>Christoph</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schindler</keyname><forenames>Martin</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author></authors><title>Scaling-Up Model-Based-Development for Large Heterogeneous Systems with
  Compositional Modeling</title><categories>cs.SE</categories><comments>5 pages, 3 figures, Proceedings of the 2009 International Conference
  on Software Engineeering in Research and Practice, Vol. 1. Ed.: H. Arabnia,
  H. Reza. July 13-16. Las Vegas, Nevada, USA. 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based development and in particular MDA [1], [2] have promised to be
especially suited for the development of complex, heterogeneous, and large
software systems. However, so far MDA has failed to fulfill this promise to a
larger extent because of tool support being inadequate and clumsy and
methodologies not being appropriate for an effective development. This article
discusses what went wrong in current MDA approaches and what needs to be done
to make MDA suited for ultra-large, distributed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6587</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6587</id><created>2014-09-22</created><authors><author><keyname>Bartelt</keyname><forenames>Christian</forenames></author><author><keyname>Broy</keyname><forenames>Manfred</forenames></author><author><keyname>Herrmann</keyname><forenames>Christoph</forenames></author><author><keyname>Knauss</keyname><forenames>Eric</forenames></author><author><keyname>Kuhrmann</keyname><forenames>Marco</forenames></author><author><keyname>Rausch</keyname><forenames>Andreas</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schneider</keyname><forenames>Kurt</forenames></author></authors><title>Orchestration of Global Software Engineering Projects</title><categories>cs.SE</categories><comments>6 pages, 5 figures</comments><journal-ref>Proceedings of the Third International Workshop on Tool Support
  Development and Management in Distributed Software Projects, collocated with
  the Fourth IEEE International Conference on Global Software Engineering ICGSE
  2009</journal-ref><doi>10.1109/ICGSE.2009.52</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global software engineering has become a fact in many companies due to real
necessity in practice. In contrast to co-located projects global projects face
a number of additional software engineering challenges. Among them quality
management has become much more difficult and schedule and budget overruns can
be observed more often. Compared to co-located projects global software
engineering is even more challenging due to the need for integration of
different cultures, different languages, and different time zones - across
companies, and across countries. The diversity of development locations on
several levels seriously endangers an effective and goal-oriented progress of
projects. In this position paper we discuss reasons for global development,
sketch settings for distribution and views of orchestration of dislocated
companies in a global project that can be seen as a &quot;virtual project
environment&quot;. We also present a collection of questions, which we consider
relevant for global software engineering. The questions motivate further
discussion to derive a research agenda in global software engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6588</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6588</id><created>2014-09-22</created><authors><author><keyname>Fieber</keyname><forenames>Florian</forenames></author><author><keyname>Regnat</keyname><forenames>Nikolaus</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Assessing usability of model driven development in industrial projects</title><categories>cs.SE</categories><comments>9 pages, 4 figures</comments><journal-ref>T Bailey, R. Vogel, J. Mansell (Eds.): 4th European Workshop on
  &quot;From code centric to model centric software engineering: Practices,
  Implications and ROI&quot; (C2M). 24. Juni 2009, University of Twente,
  NL-Enschede. CTIT Workshop Proceedings</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An integral use of the model driven development paradigm influences and
changes an organization's software development division rather heavily. Such a
paradigm reduces some tasks in complexity and costs, but also introduces new
tasks and, if introduced seriously, has severe affects on activities and roles
in the software development process. As the model becomes the most important
development artifact, there are new challenges to the development team, e. g.
assessing the model's quality, model partitioning and configuration management
for distributed teams, setup of build management, tool chaining and tracing of
information through the various artifacts. Organizations coping with model
driven development need to successfully introduce new tools and new ways of
thinking, they are challenged in adopting their processes and training their
staff. This paper presents an ongoing research project on the assessment of the
usability of modeling and model driven development at a global industrial
organization with its headquarters in Germany. The matter of interest is the
analysis of the usability of modeling (especially with the UML) and model
driven development by accomplishing an empirical, quantitative survey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6589</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6589</id><created>2014-09-22</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>System Model-Based Definition of Modeling Language Semantics</title><categories>cs.SE</categories><comments>15 pages, 7 figures</comments><journal-ref>Proc. of FMOODS/FORTE 2009, LNCS 5522. 2009</journal-ref><doi>10.1007/978-3-642-02138-1_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an approach to define the semantics for
object-oriented modeling languages. One important property of this semantics is
to support underspecified and incomplete models. To this end, semantics is
given as predicates over elements of the semantic domain. This domain is called
the system model which is a general declarative characterization of object
systems. The system model is very detailed since it captures various relevant
structural, behavioral, and interaction aspects. This allows us to re-use the
system model as a domain for various kinds of object-oriented modeling
languages. As a major consequence the integration of language semantics is
straight-forward. The whole approach is supported by tools that do not
constrain the semantics definition's expressiveness and flexibility while
making it machinecheckable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6590</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6590</id><created>2014-09-22</created><authors><author><keyname>Beresnev</keyname><forenames>Alexej</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schoven</keyname><forenames>Frank</forenames></author></authors><title>Automated Testing of Graphical Models in Heterogeneous Test Environments</title><categories>cs.SE</categories><comments>6 pages, 7 figures</comments><journal-ref>6th International Workshop on Intelligent Transportation. Hamburg
  University of Technology. 24.-25. M\&quot;arz 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated and therefore repeatable tests are important in product lines to
ensure and maintain quality of software functions as well as efficiency of the
developers. This article shows methods for fully automated testing of SIMULINK
models with the slUnit framework and techniques for the integration of slUnit
tests in a general test framework to allow integrated and automated test
reports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6591</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6591</id><created>2014-09-22</created><authors><author><keyname>B&#xe9;zivin</keyname><forenames>Jean</forenames></author><author><keyname>Paige</keyname><forenames>Richard F.</forenames></author><author><keyname>A&#xdf;mann</keyname><forenames>Uwe</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schmidt</keyname><forenames>Doug</forenames></author></authors><title>Manifesto - Model Engineering for Complex Systems</title><categories>cs.SE</categories><comments>4 pages, 1 figures</comments><journal-ref>Dagstuhl Seminar Proceedings. Perspectives Workshop: Model
  Engineering of Complex Systems (MECS) Schloss Dagstuhl - Leibniz-Zentrum fuer
  Informatik, Germany, ISSN 1862-4405, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex systems are hard to define. Nevertheless they are more and more
frequently encountered. Examples include a worldwide airline traffic management
system, a global telecommunication or energy infrastructure or even the whole
legacy portfolio accumulated for more than thirty years in a large insurance
company. There are currently few engineering methods and tools to deal with
them in practice. The purpose of this Dagstuhl Perspectives Workshop on Model
Engineering for Complex Systems was to study the applicability of Model Driven
Engineering (MDE) to the development and management of complex systems. MDE is
a software engineering field based on few simple and sound principles. Its
power stems from the assumption of considering everything - engineering
artefacts, manipulations of artefacts, etc - as a model. Our intuition was that
MDE may provide the right level of abstraction to move the study of complex
systems from an informal goal to more concrete grounds. In order to provide
first evidence in support of this intuition, the workshop studied different
visions and different approaches to the development and management of different
kinds of complex systems. This note presents the summary of the discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6592</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6592</id><created>2014-09-22</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wimmel</keyname><forenames>Guido</forenames></author></authors><title>A Framework for Realtime Online Auctions</title><categories>cs.SE cs.GT</categories><comments>13 pages, 4 figures</comments><journal-ref>Managing Information Technology in a Global Economy. Proceedings
  of IRMA International Conference, Toronto pp. 908-912, Idea Group Publishing,
  2001</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the existing E-Commerce applications, online auctions are the most
influential ones. Their impact on trading in the B2B (business to business) as
well as in the B2C (business to consumer) and C2C (consumer to consumer) areas
will be inevitable. This article describes the architecture of a web-based
realtime online auction system, together with the functional and technical
requirements that evolved during the development process and heavily influenced
the architecture. From the point of view of this real world case study, ways to
minimize the development time and yet ensure a robust and flexible system are
explained: combining standard software and self- developed components, reusing
code wherever possible, and employing the eXtreme Programming approach and its
test concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6596</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6596</id><created>2014-09-22</created><authors><author><keyname>Fontoura</keyname><forenames>Marcus</forenames></author><author><keyname>Pree</keyname><forenames>Wolfgang</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>The WebShop E-Commerce Framework</title><categories>cs.SE cs.CY</categories><comments>6 pages, 15 figures</comments><journal-ref>International Conference on Internet Computing CSREA press, 2001</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an e-commerce framework called WebShop, which was
developed by the authors for the purpose of demonstrating the use of UML and
the UML-F in the domain of Web applications. Thus, the WebShop is not regarded
as a full-fledged system out of which real Web stores can be derived. For
example, the framework in the presented version does not encounter security
features. However, it presents the most important variation points related to
online catalogs. The UML-F Web site http://www.UML-F.net provides the Java
source files and some sample adaptations of WebShop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6597</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6597</id><created>2014-09-22</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Executable Modeling with UML. A Vision or a Nightmare?</title><categories>cs.SE</categories><comments>12 pages, 0 figures</comments><journal-ref>Issues &amp; Trends of Information Technology Management in
  Contemporary Associations, Seattle. Idea Group Publishing, Hershey, London,
  pp. 697-701. 2002</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extreme Programming is the most prominent new, light-weight (or agile)
methods, defined to contrast the current heavy-weight and partially overloaded
object-oriented methods. It focuses on the core issues of software technology.
One of its principles is not to rely on diagrams to document a system. In this
paper, we examine what properties a modeling language like UML must have in
order to support the Extreme Programming approach effectively. In particular,
we discuss how such a diagrammatic programming language must look like to
replace a textual programming language and what benefits and problems such an
approach may bring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6598</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6598</id><created>2014-09-22</created><authors><author><keyname>Cook</keyname><forenames>Steve</forenames></author><author><keyname>Kleppe</keyname><forenames>Anneke</forenames></author><author><keyname>Mitchell</keyname><forenames>Richard</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Warmer</keyname><forenames>Jos</forenames></author><author><keyname>Wills</keyname><forenames>Alan</forenames></author></authors><title>The Amsterdam Manifesto on OCL</title><categories>cs.SE</categories><comments>35 pages, 4 figures</comments><journal-ref>Object Modeling with the OCL, pp. 115-149 LNCS 2263, Springer
  Verlag, 2002</journal-ref><doi>10.1007/3-540-45669-4_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In November 1998 the authors participated in a two-day workshop on the Object
Constraint Language (OCL) in Amsterdam. The focus was to clarify issues about
the semantics and the use of OCL, and to discuss useful and necessary
extensions of OCL. Various topics have been raised and clarified. This
manifesto contains the results of that workshop and the following work on these
topics. Overview of OCL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6599</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6599</id><created>2014-09-22</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Astrid</forenames></author></authors><title>Quantitative Survey on Extreme Programming Projects</title><categories>cs.SE</categories><comments>6 pages, 0 figures</comments><journal-ref>Third International Conference on Extreme Programming and Flexible
  Processes in Software Engineering, XP2002, May 26-30, Alghero, Italy, pg.
  95-100, 2002.(short version of TUM-I0110)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years the Extreme Programming (XP) community has grown
substantially. Many XP projects have started and a substantial amount are
already finished. As the interest in the XP approach is constantly increasing
worldwide throughout all software intensive application domains, it was time to
start a first survey on XP. This paper presents the results of 45 evaluated
questionnaires that have been received during the Summer 2001 survey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6600</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6600</id><created>2014-09-22</created><authors><author><keyname>Turk</keyname><forenames>Dan</forenames></author><author><keyname>France</keyname><forenames>Robert</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Limitations of Agile Software Processes</title><categories>cs.SE</categories><comments>4 pages, 0 figures</comments><journal-ref>Third International Conference on Extreme Programming and Flexible
  Processes in Software Engineering, XP2002, May 26-30, Alghero, Italy, pg.
  43-46, 2002</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software developers and project managers are struggling to assess the
appropriateness of agile processes to their development environments. This
paper identifies limitations that apply to many of the published agile
processes in terms of the types of projects in which their application may be
problematic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6601</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6601</id><created>2014-09-22</created><authors><author><keyname>Thomas</keyname><forenames>Ulrike</forenames></author><author><keyname>Hirzinger</keyname><forenames>Gerd</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schulze</keyname><forenames>Christoph</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>A New Skill Based Robot Programming Language Using UML/P Statecharts</title><categories>cs.SE cs.RO</categories><comments>6 pages, 10 figures, Proceedings of the 2013 IEEE International
  Conference on Robotics and Automation (ICRA), Karlsruhe, Germany</comments><doi>10.1109/ICRA.2013.6630615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the new robot programming language LightRocks (Light
Weight Robot Coding for Skills), a domain specific language (DSL) for robot
programming. The language offers three different level of abstraction for robot
programming. On lowest level skills are coded by domain experts. On a more
abstract level these skills are supposed to be combined by shop floor workers
or technicians to define tasks. The language is designed to allow as much
flexibility as necessary on the lowest level of abstraction and is kept as
simple as possible with the more abstract layers. A Statechart like model is
used to describe the different levels of detail. For this we apply the UML/P
and the language workbench MontiCore. To this end we are able to generate code
while hiding controller specific implementation details. In addition the
development in LightRocks is supported by a generic graphical editor
implemented as an Eclipse plugin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6602</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6602</id><created>2014-09-22</created><authors><author><keyname>da Cruz</keyname><forenames>David Bettencourt</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wimmel</keyname><forenames>Guido</forenames></author></authors><title>Retrofitting Security into a Web-Based Information System</title><categories>cs.SE</categories><comments>5 pages, 0 figures</comments><journal-ref>Web Engineering. International Conference ICWE 2003. Oviedo,
  Spain, Proceedings LNCS 2722, Springer Verlag, July 2003</journal-ref><doi>10.1007/3-540-45068-8_58</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on an incremental method that allows adding security
mechanisms to an existing, but insecure system, such as a prototype or a legacy
system. The incremental method is presented and as a showcase its application
is demonstrated at the example of a Web-based information system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6603</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6603</id><created>2014-09-22</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Model-Based Testing of Object-Oriented Systems</title><categories>cs.SE</categories><comments>23 pages, 17 figures</comments><journal-ref>Formal Methods for Components and Objects International Symposium,
  FMCO 2002. Leiden, November 2002, Revised Lectures. LNCS 2852, Springer
  Verlag, 2003</journal-ref><doi>10.1007/978-3-540-39656-7_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a model-based approach to testing as a vital part of
software development. It argues that an approach using models as central
development artifact needs to be added to the portfolio of software engineering
techniques, to further increase efficiency and flexibility of the development
as well as quality and reusability of results. Then test case modeling is
examined in depth and related to an evolutionary approach to model
transformation. A number of test patterns is proposed that have proven helpful
to the design of testable object-oriented systems. In contrast to other
approaches, this approach uses explicit models for test cases instead of trying
to derive (many) test cases from a single model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6604</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6604</id><created>2014-09-22</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Scholz</keyname><forenames>Peter</forenames></author></authors><title>Scaling the Management of Extreme Programming Projects</title><categories>cs.SE</categories><comments>7 pages, 4 figures</comments><journal-ref>Projects &amp; Profits. Special Issue on Management of Extreme
  Programming Projects, Vol. III (8), pp. 11-18. ICFAI-Press, Hyderabat, August
  2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  XP is a code-oriented, light-weight software engineering methodology, suited
merely for small-sized teams who develop software that relies on vague or
rapidly changing requirements. Being very code-oriented, the discipline of
systems engineering knows it as approach of incremental system change. In this
contribution, we discuss the enhanced version of a concept on how to extend XP
on large scale projects with hundreds of software engineers and programmers,
respectively. Previous versions were already presented in [1] and [12]. The
basic idea is to apply the &quot;hierarchical approach&quot;, a management principle of
reorganizing companies, as well as well-known moderation principles to XP
project organization. We show similarities between software engineering methods
and company reorganization processes and discuss how the elements of the
hierarchical approach can improve XP. We provide guidelines on how to scale up
XP to very large projects e.g. those common in telecommunication industry and
IT technology consultancy firms by using moderation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6605</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6605</id><created>2014-09-22</created><authors><author><keyname>Botaschanjan</keyname><forenames>Jewgenij</forenames></author><author><keyname>Pister</keyname><forenames>Markus</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Testing Agile Requirements Models</title><categories>cs.SE</categories><comments>7 pages, 5 figures</comments><journal-ref>Journal of Zhejiang University SCIENCE, Volume 5, No. 5, pp
  587-593, May, 2004. ISSN 1009-3095</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a model-based approach to validate software requirements
in agile development processes by simulation and in particular automated
testing. The use of models as central development artifact needs to be added to
the portfolio of software engineering techniques, to further increase
efficiency and flexibility of the development beginning already early in the
requirements definition phase. Testing requirements are some of the most
important techniques to give feedback and to increase the quality of the
result. Therefore testing of artifacts should be introduced as early as
possible, even in the requirements definition phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6606</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6606</id><created>2014-09-22</created><authors><author><keyname>Schmidt</keyname><forenames>Stefan</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Fischer</keyname><forenames>Stefan</forenames></author><author><keyname>W&#xe4;tjen</keyname><forenames>Dietmar</forenames></author></authors><title>A Security Architecture for Mobile Wireless Sensor Networks</title><categories>cs.CR cs.NI</categories><comments>12 pages, 1 figures</comments><journal-ref>Security in Ad-hoc and Sensor Networks: First European Workshop
  (ESAS 2004) Heidelberg, Germany, August 6, 2004, Revised Selected Papers.
  LNCS 3313, pp 166-177, Springer-Verlag Berlin Heidelberg 2005</journal-ref><doi>10.1007/978-3-540-30496-8_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks increasingly become viable solutions to many
challenging problems and will successively be deployed in many areas in the
future. However, deploying new technology without security in mind has often
proved to be unreasonably dangerous. We propose a security architecture for
self-organizing mobile wireless sensor networks that prevents many attacks
these networks are exposed to. Furthermore, it limits the security impact of
some attacks that cannot be prevented. We analyse our security architecure and
show that it provides the desired security aspects while still being a
lightweight solution and thus being applicable for self-organizing mobile
wireless sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6608</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6608</id><created>2014-09-22</created><authors><author><keyname>J&#xfc;rjens</keyname><forenames>Jan</forenames></author><author><keyname>Fernandez</keyname><forenames>Eduardo B.</forenames></author><author><keyname>France</keyname><forenames>Robert B.</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Heitmeyer</keyname><forenames>Constance</forenames></author></authors><title>Critical Systems Development Using Modeling Languages. (CSDUML-04):
  Current Developments and Future Challenges (Report on the Third International
  Workshop)</title><categories>cs.SE</categories><comments>9 pages, 0 figures</comments><journal-ref>UML Modeling Languages and Applications, UML 2004 Satellite
  Activities, Lisbon, Portugal, October 11-15, 2004</journal-ref><doi>10.1007/978-3-540-31797-5_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a short report on the contributions to and some discussions made and
conclusions drawn at the Third International Workshop on Critical Systems
Development Using Modeling Languages (CSDUML'04).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6609</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6609</id><created>2014-09-22</created><authors><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Techniques Enabling Generator Refactoring</title><categories>cs.SE</categories><comments>2 pages, 5 figures, Technical Report, TR-CCTC/DI-36 Centro de
  Ciencias e Tecnologias de Computacao, Departamento de Informatica
  Universidade do Minho, Braga, Portugal, Juli 2005</comments><report-no>TR-CCTC/DI-36</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our approach to use refactoring techniques together with
code generation. Refactoring is particularly useful if not only the generated
classes but also the generator itself can be adapted in an automatic fashion.
We have developed a simple demonstration prototype to illustrate this. The
demonstration is based on a special technique where the template for the code
generation is defined as compilable source code. The directives to ll out this
template prototype to the actual classes are embedded in the source as
comments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6610</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6610</id><created>2014-09-22</created><authors><author><keyname>Turk</keyname><forenames>Dan</forenames></author><author><keyname>France</keyname><forenames>Robert</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Assumptions Underlying Agile Software Development Processes</title><categories>cs.SE</categories><comments>33 pages, 4 figures</comments><journal-ref>Journal of Database Management, Volume 16, No. 4, pp. 62-87,
  October-December 2005 Idea Group Inc., 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agile processes focus on facilitating early and fast production of working
code, and are based on software development process models that support
iterative, incremental development of software. Although agile methods have
existed for a number of years now, answers to questions concerning the
suitability of agile processes to particular software development environments
are still often based on anecdotal accounts of experiences. An appreciation of
the (often unstated) assumptions underlying agile processes can lead to a
better understanding of the applicability of agile processes to particular
situations. Agile processes are less likely to be applicable in situations in
which core assumptions do not hold. This paper examines the principles and
advocated practices of agile processes to identify underlying assumptions. The
paper also identifies limitations that may arise from these assumptions and
outlines how the limitations can be addresses by incorporating other software
development techniques and practices into agile development environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6611</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6611</id><created>2014-09-22</created><authors><author><keyname>B&#xe9;zivin</keyname><forenames>Jean</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Sch&#xfc;rr</keyname><forenames>Andy</forenames></author><author><keyname>Tratt</keyname><forenames>Laurence</forenames></author></authors><title>Model Transformations in Practice Workshop (MTiP)</title><categories>cs.SE</categories><comments>8 pages, 4 figures</comments><journal-ref>Satellite Events at the MoDELS 2005 Conference, MoDELS 2005. J-M
  Bruel (Ed.), LNCS 3844. Springer, January 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model Transformations in Practice (MTiP) 2005 was a workshop which provided a
forum for the model transformation community to discuss practical model
transformation issues. Although many different model transformation approaches
have been proposed and explored in recent years, there has been little work on
comparing and contrasting various approaches. Without such comparisons, it is
hard to assess new model transformation approaches such as the upcoming OMG
MOF/QVT recommendation, or to discern sensible future paths for the area. Our
aims with the workshop were to create a forum that would help lead to an
increased understanding of the relative merits of different model
transformation techniques and approaches. A more advanced understanding of such
merits is of considerable benefit to both the model transformation and wider
modelling communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6612</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6612</id><created>2014-09-22</created><authors><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Towards Enabling Architectural Refactorings through Source Code
  Annotations</title><categories>cs.SE</categories><comments>10 pages, 6 figures</comments><journal-ref>Proceedings der Modellierung 2006.. 22.-24. M\&quot;arz 2006,
  Innsbruck. GI-Edition - Lecture Notes in Informatics, LNI P-82, ISBN
  3-88579-176-5, 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that software needs to change to meet new requirements. The
synchronization of software architecture models and implementation is of high
importance to keep the architecture documents useful and the software evolution
process manageable. In this paper we achieve this synchronization by a two-step
process. First, we augment the source code with architectural information.
Second, this &quot;lightweight architectural model&quot; can be checked more easily
against the full architectural description. Based on this approach refactorings
on either side (code or architecture) are detected automatically and
conformance checks become possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6613</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6613</id><created>2014-09-22</created><authors><author><keyname>Broy</keyname><forenames>Manfred</forenames></author><author><keyname>Cengarle</keyname><forenames>Maria Victoria</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Towards a System Model for UML. The Structural Data Model</title><categories>cs.SE</categories><comments>34 pages, 4 figures, Munich University of Technology, Technical
  Report TUM-I0612. June 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document we introduce a system model as the basis for a semantic
model for UML 2.0. The system model is supposed to form the core and foundation
of the UML semantics definition. For that purpose the basic system is targeted
towards UML. This document is structured as follows: In the rest of Section 1
we will discuss the general approach and highlight the main decisions. This
section is important to understand the rest of this document. Section 2
contains the actual definition of the structural part of the system model. It
is built in layers as described in Section 1. For brevity of the approach, we
defer deeper discussions into the Appendix in Section 4. This document is part
of a project on the formalization of the UML 2.0 in cooperation between the
Queens University Kingston and the Technische Universit\&quot;aten Braunschweig and
M\&quot;unchen. This version 1.0 is the result of a longer effort to define the
structure, behavior and interaction of object-oriented, possibly distributed
systems abstract enough to be of general value, but also in sufficient detail
for a semantic foundation of the UML. We also wish to thank external reviewers,
and especially Gregor von Bochmann, Gregor Engels and S\'ebastien G\'erard for
their help.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6616</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6616</id><created>2014-09-22</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Agile Test-based Modeling</title><categories>cs.SE</categories><comments>6 pages, 3 figures, Proceedings of the 2006 International Conference
  on Software Engineering Research &amp; Practice. SERP'2006. CSREA Press, USA,
  June 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model driven architecture (MDA) concentrates on the use of models during
software development. An approach using models as the central development
artifact is more abstract, more compact and thus more effective and probably
also less error prone. Although the ideas of MDA exist already for years, there
is still much to improve in the development process as well as the underlying
techniques and tools. Therefore, this paper is a follow up on, reexamining und
updating the statements made there. Here two major and strongly related
techniques are identified and discussed: Test case modeling and an evolutionary
approach to model transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6617</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6617</id><created>2014-09-22</created><authors><author><keyname>Gajanovic</keyname><forenames>Borislav</forenames></author><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Model Driven Testing of Time Sensitive Distributed Systems</title><categories>cs.SE</categories><comments>18 pages, J.-P. Babau, J. Champeau, S. G\'erard (Hrsg.): From MDD
  Concepts to Experiments and Illustrations, ISTE Ltd. 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we demonstrate an approach to model structure and behavior of
distributed systems, to map those models to a lightweight execution engine by
using a functional programming language and to systematically define and
execute tests for these models within the same technology. This is a
prerequisite for a smooth integration of model based development into an agile
method. The novelty of this paper is the demonstration, how composition and
state machine models for distributed asynchronously communicating systems can
easily be mapped to a lazy functional language and then using standard testing
techniques to define test on those programs. In particular distributed timing
aspects and underspecification can be treated accordingly within a functional
language, using a certain style of functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6618</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6618</id><created>2014-09-22</created><authors><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author></authors><title>Roles in Software Development using Domain Specific Modeling Languages</title><categories>cs.SE</categories><comments>9 pages, 3 figures</comments><journal-ref>Proceedings of the 6th OOPSLA Workshop on Domain-Specific Modeling
  (DSM' 06), Portland, Oregon USA Technical Report TR-37, Jyv\&quot;askyl\&quot;a
  University, Finland, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain-specific modelling languages (DSMLs) successfully separate the
conceptual and technical design of a software system by modelling requirements
in the DSML and adding technical elements by appropriate generator technology.
In this paper we describe the roles within an agile development process that
allows us to implement a software system by using a combination of domain
specific models and source code. We describe the setup of such a process using
the MontiCore framework and demonstrate the advantages by describing how a
group of developers with diverse individual skills can develop automotive HMI
software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6619</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6619</id><created>2014-09-22</created><authors><author><keyname>Broy</keyname><forenames>Manfred</forenames></author><author><keyname>Crane</keyname><forenames>Michelle L.</forenames></author><author><keyname>Dingel</keyname><forenames>Juergen</forenames></author><author><keyname>Hartman</keyname><forenames>Alan</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhar</forenames></author><author><keyname>Selic</keyname><forenames>Brau</forenames></author></authors><title>UML 2 Semantics Symposium: Formal Semantics for UML</title><categories>cs.SE</categories><comments>6 pages, 1 figures</comments><journal-ref>Models in Software Engineering. Workshops and Symposia at Models
  2006. Genoa. LNCS 4364, Springer, January 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this symposium, held in conjunction with MoDELS 2006, was to
present the current state of research of the UML 2 Semantics Project. Equally
important to receiving feedback from an audience of experts was the opportunity
to invite researchers in the field to discuss their own work related to a
formal semantics for the Unified Modeling Language. This symposium is a
follow-on to our first workshop, held in conjunction with ECMDA 2005.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6620</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6620</id><created>2014-09-22</created><authors><author><keyname>France</keyname><forenames>Robert</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Model-Driven Development of Complex Software: A Research Roadmap</title><categories>cs.SE</categories><comments>18 pages</comments><journal-ref>Future of Software Engineering 2007 at ICSE. Minneapolis, pg.
  37-54, IEEE, May 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The term Model-Driven Engineering (MDE) is typically used to describe
software development approaches in which abstract models of software systems
are created and systematically transformed to concrete implementations. In this
paper we give an overview of current research in MDE and discuss some of the
major challenges that must be tackled in order to realize the MDE vision of
software development. We argue that full realizations of the MDE vision may not
be possible in the near to medium-term primarily because of the wicked problems
involved. On the other hand, attempting to realize the vision will provide
insights that can be used to significantly reduce the gap between evolving
software complexity and the technologies used to manage complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6621</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6621</id><created>2014-09-22</created><authors><author><keyname>Herrmann</keyname><forenames>Christoph</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schindler</keyname><forenames>Martin</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author></authors><title>An Algebraic View on the Semantics of model Composition</title><categories>cs.SE</categories><comments>15 pages, 3 figures, 2 tables</comments><journal-ref>Proceedings of the Third European Conference on Model Driven
  Architecture - Foundations and Applications (ECMDA-FA 2007), Haifa, Israel D.
  H. Akehurst, R. Vogel, R. F. Paige: LNCS 4530, pp. 99-113 Springer-Verlag
  Berlin-Heidelberg, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increased complexity of software development projects more and
more systems are described by models. The sheer size makes it impractical to
describe these systems by a single model. Instead many models are developed
that provide several complementary views on the system to be developed. This
however leads to a need for compositional models. This paper describes a
foundational theory of model composition in form of an algebra to explicitly
clarify different variants and uses of composition, their interplay with the
semantics of the involved models and their composition operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6622</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6622</id><created>2014-09-22</created><authors><author><keyname>Cengarle</keyname><forenames>Maria Victoria</forenames></author><author><keyname>Dingel</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>System-Model-Based Simulation of UML Models</title><categories>cs.SE</categories><comments>15 pages, 8 figures, Proceedings Nordic Workshop on Model Driven
  Engineering (NW-MODE 2007), Aug. 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work has presented our ongoing e orts to define a &quot;reference
semantics&quot; for the UML, that is, a mathematically defined system model that is
envisaged to cover all of the UML eventually, and that also carefully avoids
the introduction of any unwarranted restrictions or biases. Due to the use of
underspecification, the system model is not executable. This paper shows how
the system model can serve as the basis for a highly customizable execution and
simulation environment for the UML. The design and implementation of a
prototype of such an environment is described and its use for the
experimentation with different semantic variation points is illustrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6623</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6623</id><created>2014-09-22</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schindler</keyname><forenames>Martin</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author></authors><title>Textbased Modeling</title><categories>cs.SE</categories><comments>9 pages, 1 figures</comments><journal-ref>Proceedings of the 4th International Workshop on Software Language
  Engineering (ateM 2007), Nashville, TN, USA, October 2007 Informatik-Bericht
  Nr. 4/2007, Johannes-Gutenberg-Universit\&quot;at Mainz, October 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As modeling becomes a crucial activity in software development the question
may be asked whether currently used graphical representations are the best
option to model systems efficiently. This position paper discusses the
advantages of text-based modeling over commonly used graphical representations.
It is inspired through the advent of new extensible development tools like
Eclipse. The discussion is illustrated by showing a textual version of UML
state machines as Eclipse plugins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6624</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6624</id><created>2014-09-22</created><authors><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author></authors><title>Integrated Definition of Abstract and Concrete Syntax for Textual
  Languages</title><categories>cs.SE</categories><comments>15 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:1409.2367</comments><journal-ref>Proceedings of the ACM/IEEE 10th International Conference on Model
  Driven Engineering Languages and Systems (MODELS 2007), Nashville, TN, USA,
  LNCS 4735</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An understandable concrete syntax and a comprehensible abstract syntax are
two central aspects of defining a modeling language. Both representations of a
language significantly overlap in their structure and also information, but may
also differ in parts of the information. To avoid discrepancies and problems
while handling the language, concrete and abstract syntax need to be
consistently defined. This will become an even bigger problem, when domain
specific languages will become used to a larger extent. In this paper we
present an extended grammar format that avoids redundancy between concrete and
abstract syntax by allowing an integrated definition of both for textual
modeling languages. For an amendment of the usability of the abstract syntax it
furthermore integrates meta-modeling concepts like associations and inheritance
into a well-understood grammar-based approach. This forms a sound foundation
for an extensible grammar and therefore language definition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6625</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6625</id><created>2014-09-22</created><authors><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author></authors><title>Efficient Editor Generation for Compositional DSLs in Eclipse</title><categories>cs.SE</categories><comments>11 pages, 9 figures, 1 table</comments><journal-ref>Proceedings of the 7th OOPSLA Workshop on Domain-Specific Modeling
  (DSM' 07), Montreal, Quebec, Canada Technical Report TR-38, Jyv\&quot;askyl\&quot;a
  University, Finland 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a domain specific language (DSL) shall be commonly used, it is important
for the development team to have a comfortable editor well integrated in the
project's development environment. As DSL are rather often subject to changes,
efficient development and evolvement of a comfortable editor is an important
success criterion for the acceptance of domain specific languages. In this
paper we demonstrate how this issue is addressed in the MontiCore DSL
development framework. Basically an extension of the MontiCore DSL definition
language can be used to efficiently generate DSL editors for Eclipse. The
generation tool also supports the compositional language definition features of
MontiCore and allows therefore the reuse of existing language and editor
definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6626</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6626</id><created>2014-09-22</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Hartmann</keyname><forenames>Jochen</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Kriebel</keyname><forenames>Stefan</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>View-Based Modeling of Function Nets</title><categories>cs.SE</categories><comments>6 pages, 4 figures</comments><journal-ref>Proceedings of the Object-oriented Modelling of Embedded Real-Time
  Systems (OMER4) Workshop, Paderborn, October 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach to model features and function nets of
automotive systems comprehensively. In order to bridge the gap between feature
requirements and function nets, we describe an approach to describe both using
a SysML-based notation. If requirements on the automotive system are changed by
several developers responsible for different features, it is important for
developers to have a good overview and understanding of the functions affected.
We show that this can be comprehensively modeled using so called &quot;feature
views&quot;. In order to validate these views against the complete function nets,
consistency checks are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6628</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6628</id><created>2014-09-22</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Hartmann</keyname><forenames>Jochen</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Kriebel</keyname><forenames>Stefan</forenames></author><author><keyname>Rothhart</keyname><forenames>Lutz</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Modelling Automotive Function Nets with Views for Features, Variants,
  and Modes</title><categories>cs.SE</categories><comments>7 pages, 7 figures, 4th European Congress ERTS - Embedded Real Time
  Software, Toulouse 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling the logical architecture of an automotive system as one central
step in the development process leads to an early understanding of the
fundamental functional properties of the system under design. This supports
developers in making design decisions. However, due to the large size and
complexity of the system and hence the logical architecture, a good notation,
method and tooling is necessary. In this paper, we show how logical
architectures can be modelled succinctly as function nets using a SysML-based
notation. The usefulness for developers is increased by comprehensible views on
the complete model that describe automotive features, variants, and modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6629</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6629</id><created>2014-09-22</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Pinkernell</keyname><forenames>Claas</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhar</forenames></author></authors><title>Modeling Variants of Automotive Systems using Views</title><categories>cs.SE</categories><comments>14 pages, 9 figures</comments><journal-ref>Tagungsband Modellierungs-Workshop MBEFF: Modellbasierte
  Entwicklung von eingebetteten Fahrzeugfunktionen. Berlin, M\&quot;arz 2008,
  Informatik-Bericht 2008-01, CFG-Fakult\&quot;at, TU Braunschweig, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach of modeling variability of automotive system
architectures using function nets, views and feature diagrams. A function net
models an architecture hierarchically and views are used to omit parts of such
a model to focus on certain functionalities. In combination with feature
diagrams that describe valid variants, the concepts of feature and variant
views are introduced to model architectural variants. The relationship between
views, variants and the underlying complete architectural model is discussed.
Methodological aspects that come along with this approach are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6630</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6630</id><created>2014-09-22</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Hartmann</keyname><forenames>Jochen</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Kriebel</keyname><forenames>Stefan</forenames></author><author><keyname>Rothhart</keyname><forenames>Lutz</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>View-Centric Modeling of Automotive Logical Architectures</title><categories>cs.SE</categories><comments>10 pages, 4 figures</comments><journal-ref>Tagungsband des Dagstuhl-Workshop MBEES: Modellbasierte
  Entwicklung eingebetteter Systeme IV. Informatik-Bericht 2008-02,
  CFG-Fakult\&quot;at, TU Braunschweig, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling the logical architecture is an often underestimated development step
to gain an early insight into the fundamental functional properties of an
automotive system. An architectural description supports developers in making
design decisions for further development steps like the refinement towards a
software architecture or the partition of logical functions on ECUs and buses.
However, due to the large size and complexity of the system and hence the
logical architecture, a good notation, method, and tooling is necessary. In
this paper, we show how the logical architectures can be modeled succinctly as
function nets using a SysML-based notation. The usefulness for developers is
increased by comprehensible views on the complete model to describe automotive
features in a self contained way including their variants, modes, and related
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6631</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6631</id><created>2014-09-22</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schindler</keyname><forenames>Martin</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author></authors><title>MontiCore: A Framework for the Development of Textual Domain Specific
  Languages</title><categories>cs.SE</categories><comments>2 pages, 1 figures, 30th International Conference on Software
  Engineering (ICSE). Companion Volume. (2008)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we demonstrate a framework for efficient development of textual
domain specific languages and supporting tools. We use a redundance-free and
compact definition of a readable concrete syntax and a comprehensible abstract
syntax as both representations significantly overlap in their structure. To
further improve the usability of the abstract syntax this definition format
integrates additional concepts like associations and inheritance into the
well-understood grammar-based approach. Modularity concepts like language
inheritance and embedding are used to simplify the development of languages
based on already existing ones. In addition, the generation of editors and a
template approach for code generation is explained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6633</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6633</id><created>2014-09-22</created><authors><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Stefan</forenames></author></authors><title>MontiCore: Modular Development of Textual Domain Specific Languages</title><categories>cs.SE</categories><comments>19 pages, 11 figures</comments><journal-ref>Proceedings of the 46th International Conference Objects, Models,
  Components, Patterns (TOOLS Europe). Zurich, Switzerland, 2008 R. F. Paige,
  B. Meyer: LNBIP 11, pp. 297-315 Springer-Verlag Berlin-Heidelberg 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reuse is a key technique for a more efficient development and ensures the
quality of the results. In object technology explicit encapsulation,
interfaces, and inheritance are well known principles for independent
development that enable combination and reuse of developed artifacts. In this
paper we apply modularity concepts for domain specific languages (DSLs) and
discuss how they help to design new languages by extending existing ones and
composing fragments to new DSLs. We use an extended grammar format with
appropriate tool support that avoids redefinition of existing functionalities
by introducing language inheritance and embedding as first class artifacts in a
DSL definition. Language embedding and inheritance is not only assisted by the
parser, but also by the editor, and algorithms based on tree traversal like
context checkers, pretty printers, and code generators. We demonstrate that
compositional engineering of new languages becomes a useful concept when
starting to define project-individual DSLs using appropriate tool support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6634</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6634</id><created>2014-09-22</created><authors><author><keyname>Rei&#xdf;</keyname><forenames>Dirk</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schulze-Quester</keyname><forenames>Marvin</forenames></author><author><keyname>Stein</keyname><forenames>Mark</forenames></author></authors><title>Evolving and Implanting Web-based E-Government Systems in Universities</title><categories>cs.CY cs.SE</categories><comments>14 pages, 3 figures</comments><journal-ref>Proceedings to the 2nd International United Information Systems
  Conference (UNISCON) Klagenfurt, Austria, April 22-25, 2008 R. Kaschek, C.
  Kop, C. Steinberger, G. Fliedl: LNBIP 5, pp. 282-295 Springer-Verlag
  Berlin-Heidelberg 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bologna Process has triggered a major restructuring of the current
university diploma into a bachelor/master system. As one effect, the
administration effort for the new system has increased dramatically. As a
second effect, students need and demand a much better information policy, given
the new possibilities of the internet. Both to increase efficiency of the
university's administration and to provide students as well as lecturers with
modern e government services, it is inevitable to evolve the current
IT-infrastructure of a university into a modern web-based landscape of systems
that support business processes on campus. In this paper, we describe the
approach taken at the Braunschweig University of Technology to evolve the
existing landscape of legacy systems by adding bridges between previously
unrelated parts, adding and customizing unused modules of existing software to
bring information and services online and to develop new software, where old
modules could not serve the necessary purposes. Most of all, both
implementation of the results in university's business processes and the
resulting quick feedback and wishes for feature enhancement are seen as part of
the software development processes and discussed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6635</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6635</id><created>2014-09-22</created><authors><author><keyname>Cengarle</keyname><forenames>Maria Victoria</forenames></author><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>System Model Semantics of Class Diagrams</title><categories>cs.SE</categories><comments>32 pages, Informatik-Bericht 2008-05 Technische Universit\&quot;at
  Braunschweig, Carl-Friedrich-Gauss-Fakult\&quot;at, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defining semantics for UML is a difficult task. Disagreements in the meaning
of UML constructs as well as the size of UML are major obstacles. In this
report, we describe our approach to define the semantics for UML. Semantics is
defined denotationally as a mapping into our semantics domain called the system
model [4, 5, 6]. We demonstrate our approach by defining the semantics for a
comprehensive version of class diagrams. The semantics definition is detailed
for UML/P class diagrams, a variant of class diagrams which restricts the use
of a few methodologically and semantically involved concepts. Class diagrams
are well-known and rather easy to understand and thus perfect to examine the
usability of the system model for precise semantic mappings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6636</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6636</id><created>2014-09-22</created><authors><author><keyname>Cengarle</keyname><forenames>Maria Victoria</forenames></author><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>System Model Semantics of Statecharts</title><categories>cs.SE</categories><comments>51 pages, 5 figures, Informatik-Bericht 2008-04 Technische
  Universit\&quot;at Braunschweig, Carl-Friedrich-Gauss-Fakult\&quot;at, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, semantics for Statecharts is defined based on a mathematical
model of object systems called &quot;system model&quot;. The semantics definition is
detailed for UML/P Statecharts, a variant of Statecharts which restricts the
use of a few methodologically and semantically difficult concepts. After
transforming full UML/P Statecharts to simplified but semantically equivalent
Statecharts, the semantics is defined denotationally as a mapping into the
system model. It is also sketched how already existing Statechart semantics can
be mapped into the system model. This report follows, in which we introduced
our approach in detail and defined semantics for UML class diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6637</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6637</id><created>2014-09-22</created><authors><author><keyname>Broy</keyname><forenames>Manfred</forenames></author><author><keyname>Cengarle</keyname><forenames>Maria Victoria</forenames></author><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Modular Description of a Comprehensive Semantics Model for the UML
  (Version 2.0)</title><categories>cs.SE</categories><comments>113 pages, 16 figures, Informatik-Bericht 2008-06 Technische
  Universit\&quot;at Braunschweig, Carl-Friedrich-Gauss-Fakult\&quot;at, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document, we introduce a system model as a semantic domain for the
Unified Modeling Language (UML) [OMG07a, OMG07b]. The system model is supposed
to form a possible core and foundation of the UML semantics definition. For
that purpose, the definitions in this document are targeted towards UML which
means that central concepts of UML have been formalized as theories of the
system model. This document is structured as follows: In the rest of Chapter 1,
we discuss the general approach and highlight the main decisions. This chapter
is important to understand the rest of this document. Chapter 2 contains the
definition of the structural part of the system model. Chapters 3 and 4 contain
the control and communication related definition definitions which form the
basis to describe the state of a system in Chapter 5. Two variants of state
transitions systems are introduced to define object behavior in Chapters 6
(event-based) and 7 (timed). Chapter 8 concludes the document. This document is
the second version of the system model which is the result of a major effort to
define the structure, behavior and interaction of object oriented, possibly
distributed systems abstract enough to be of general value, but also in
sufficient detail for a semantic foundation of the UML. The first version of
the system model can be found in [BCR06, BCR07a, BCR07b].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6644</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6644</id><created>2014-09-23</created><updated>2015-01-05</updated><authors><author><keyname>Ponce</keyname><forenames>Colin</forenames></author><author><keyname>Bindel</keyname><forenames>David</forenames></author></authors><title>FLiER: Practical Topology Error Correction Using Sparse PMUs</title><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a Fingerprint Linear Estimation Routine (FLiER) to
identify topology errors in power networks using readings from
sparsely-deployed phasor measurement units (PMUs). When a power line is removed
from a network, or when a substation is reconfigured, the event leaves a unique
&quot;voltage fingerprint&quot; of bus voltage changes that we can identify using only
the portion of the network directly observed by the PMUs. The naive brute-force
approach to identify a failed line from such voltage fingerprints, though
simple and accurate, is slow. We derive an approximate algorithm based on a
local linearization that is faster and only slightly less accurate. We present
experimental results using the IEEE 57-bus, IEEE 118-bus, and Polish 1999-2000
winter peak networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6654</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6654</id><created>2014-09-23</created><authors><author><keyname>Prasad</keyname><forenames>Sudhakar</forenames></author></authors><title>Bayesian Error Based Sequences of Mutual Information Bounds</title><categories>cs.IT math.IT</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inverse relation between mutual information (MI) and Bayesian error is
sharpened by deriving finite sequences of upper and lower bounds on MI in terms
of the minimum probability of error (MPE) and related Bayesian quantities. The
well known Fano upper bound and Feder-Merhav lower bound on equivocation are
tightened by including a succession of posterior probabilities starting at the
largest, which directly controls the MPE, and proceeding to successively lower
ones. A number of other interesting results are also derived, including a
sequence of upper bounds on the MPE in terms of a previously introduced
sequence of generalized posterior distributions. The tightness of the various
bounds is illustrated for a simple application of joint spatial localization
and spectral typing of a point source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6673</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6673</id><created>2014-09-20</created><authors><author><keyname>Bayram</keyname><forenames>Islam Safak</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author><author><keyname>Devetsikiotis</keyname><forenames>Michael</forenames></author></authors><title>Unsplittable Load Balancing in a Network of Charging Stations Under QoS
  Guarantees</title><categories>math.OC cs.NI</categories><comments>Accepted for Publication in IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The operation of the power grid is becoming more stressed, due to the
addition of new large loads represented by Electric Vehicles (EVs) and a more
intermittent supply due to the incorporation of renewable sources. As a
consequence, the coordination and control of projected EV demand in a network
of fast charging stations becomes a critical and challenging problem.
  In this paper, we introduce a game theoretic based decentralized control
mechanism to alleviate negative impacts from the EV demand. The proposed
mechanism takes into consideration the non-uniform spatial distribution of EVs
that induces uneven power demand at each charging facility, and aims to: (i)
avoid straining grid resources by offering price incentives so that customers
accept being routed to less busy stations, (ii) maximize total revenue by
serving more customers with the same amount of grid resources, and (iii)
provide charging service to customers with a certain level of
Quality-of-Service (QoS), the latter defined as the long term customer blocking
probability. We examine three scenarios of increased complexity that gradually
approximate real world settings. The obtained results show that the proposed
framework leads to substantial performance improvements in terms of the
aforementioned goals, when compared to current state of affairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6678</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6678</id><created>2014-09-23</created><authors><author><keyname>Bhardwaj</keyname><forenames>Anant</forenames></author><author><keyname>Luciano</keyname><forenames>Dave</forenames></author><author><keyname>Klemmer</keyname><forenames>Scott</forenames></author></authors><title>Redprint: Integrating API specific &quot;instant example&quot; and &quot;instant
  documentation&quot; display interface in IDEs</title><categories>cs.HC</categories><comments>UIST 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software libraries for most of the modern programming languages are numerous,
large and complex. Remembering the syntax and usage of APIs is a difficult task
for not just novices but also expert programmers. IDEs (Integrated Development
Environment) provide capabilities like autocomplete and intellisense to assist
programmers; however, programmers still need to visit search engines like
Google to find API (Application Program Interface) documentation and samples.
This paper evaluates Redprint - a browser based development environment for PHP
that integrates API specific &quot;Instant Example&quot; and &quot;Instant Documentation&quot;
display interfaces. A comparative laboratory study shows that integrating API
specific &quot;Instant Example&quot; and &quot;Instant Documentation&quot; display interfaces into
a development environment significantly reduces the cost of searching and thus
significantly reduces the time to develop software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6679</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6679</id><created>2014-09-23</created><authors><author><keyname>L</keyname><forenames>Aashiha Priyadarshni.</forenames></author></authors><title>Heterogeneous Multi core processors for improving the efficiency of
  Market basket analysis algorithm in data mining</title><categories>cs.DC</categories><comments>4 pages, 2 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V15(1):16-19, Sep 2014. ISSN:2231-2803. www.ijcttjournal.org. Published by
  Seventh Sense Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V15P103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous multi core processors can offer diverse computing capabilities.
The efficiency of Market Basket Analysis Algorithm can be improved with
heterogeneous multi core processors. Market basket analysis algorithm utilises
apriori algorithm and is one of the popular data mining algorithms which can
utilise Map/Reduce framework to perform analysis. The algorithm generates
association rules based on transactional data and Map/Reduce motivates to
redesign and convert the existing sequential algorithms for efficiency. Hadoop
is the parallel programming platform built on Hadoop Distributed File
Systems(HDFS) for Map/Reduce computation that process data as (key, value)
pairs. In Hadoop map/reduce, the sequential jobs are parallelised and the Job
Tracker assigns parallel tasks to the Task Tracker. Based on single threaded or
multithreaded parallel tasks in the task tracker, execution is carried out in
the appropriate cores. For this, a new scheduler called MB Scheduler can be
developed. Switching between the cores can be made static or dynamic. The use
of heterogeneous multi core processors optimizes processing capabilities and
power requirements for a processor and improves the performance of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6680</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6680</id><created>2014-09-23</created><authors><author><keyname>Bhardwaj</keyname><forenames>Anant</forenames></author><author><keyname>Kim</keyname><forenames>Juho</forenames></author><author><keyname>Dow</keyname><forenames>Steven</forenames></author><author><keyname>Karger</keyname><forenames>David</forenames></author><author><keyname>Madden</keyname><forenames>Sam</forenames></author><author><keyname>Miller</keyname><forenames>Rob</forenames></author><author><keyname>Zhang</keyname><forenames>Haoqi</forenames></author></authors><title>Attendee-Sourcing: Exploring The Design Space of Community-Informed
  Conference Scheduling</title><categories>cs.HC cs.SI</categories><comments>HCOMP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructing a good conference schedule for a large multi-track conference
needs to take into account the preferences and constraints of organizers,
authors, and attendees. Creating a schedule which has fewer conflicts for
authors and attendees, and thematically coherent sessions is a challenging
task.
  Cobi introduced an alternative approach to conference scheduling by engaging
the community to play an active role in the planning process. The current Cobi
pipeline consists of committee-sourcing and author-sourcing to plan a
conference schedule. We further explore the design space of community-sourcing
by introducing attendee-sourcing -- a process that collects input from
conference attendees and encodes them as preferences and constraints for
creating sessions and schedule. For CHI 2014, a large multi-track conference in
human-computer interaction with more than 3,000 attendees and 1,000 authors, we
collected attendees' preferences by making available all the accepted papers at
the conference on a paper recommendation tool we built called Confer, for a
period of 45 days before announcing the conference program (sessions and
schedule). We compare the preferences marked on Confer with the preferences
collected from Cobi's author-sourcing approach. We show that attendee-sourcing
can provide insights beyond what can be discovered by author-sourcing. For CHI
2014, the results show value in the method and attendees' participation. It
produces data that provides more alternatives in scheduling and complements
data collected from other methods for creating coherent sessions and reducing
conflicts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6689</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6689</id><created>2014-09-17</created><authors><author><keyname>Hassanat</keyname><forenames>Ahmad Basheer</forenames></author></authors><title>Visual Words for Automatic Lip-Reading</title><categories>cs.CV</categories><journal-ref>PhD thesis, the University of Buckingham, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lip reading is used to understand or interpret speech without hearing it, a
technique especially mastered by people with hearing difficulties. The ability
to lip read enables a person with a hearing impairment to communicate with
others and to engage in social activities, which otherwise would be difficult.
Recent advances in the fields of computer vision, pattern recognition, and
signal processing has led to a growing interest in automating this challenging
task of lip reading. Indeed, automating the human ability to lip read, a
process referred to as visual speech recognition, could open the door for other
novel applications. This thesis investigates various issues faced by an
automated lip-reading system and proposes a novel &quot;visual words&quot; based approach
to automatic lip reading. The proposed approach includes a novel automatic face
localisation scheme and a lip localisation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6690</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6690</id><created>2014-09-23</created><updated>2014-10-01</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author></authors><title>The Value 1 Problem Under Finite-memory Strategies for Concurrent
  Mean-payoff Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider concurrent mean-payoff games, a very well-studied class of
two-player (player 1 vs player 2) zero-sum games on finite-state graphs where
every transition is assigned a reward between 0 and 1, and the payoff function
is the long-run average of the rewards. The value is the maximal expected
payoff that player 1 can guarantee against all strategies of player 2. We
consider the computation of the set of states with value 1 under finite-memory
strategies for player 1, and our main results for the problem are as follows:
(1) we present a polynomial-time algorithm; (2) we show that whenever there is
a finite-memory strategy, there is a stationary strategy that does not need
memory at all; and (3) we present an optimal bound (which is double
exponential) on the patience of stationary strategies (where patience of a
distribution is the inverse of the smallest positive probability and represents
a complexity measure of a stationary strategy).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6736</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6736</id><created>2014-09-23</created><authors><author><keyname>Khmou</keyname><forenames>Youssef</forenames></author><author><keyname>Safi</keyname><forenames>Said</forenames></author></authors><title>Angle of Arrival Detection with Fifth Order Phase Operators</title><categories>cs.IT math.IT physics.space-ph</categories><comments>4 pages, 3 figures</comments><journal-ref>World Academy of Science, Engineering and Technology,
  International Science Index 90, International Journal of Mathematical,
  Computational, Physical and Quantum Engineering, 8(6), 961 - 964. (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a fifth order propagator operators are proposed for estimating
the Angles Of Arrival (AOA) of narrowband electromagnetic waves impinging on
antenna array when its number of sensors is larger than the number of radiating
sources.
  The array response matrix is partitioned into five linearly dependent phases
to construct the noise projector using five different propagators from non
diagonal blocks of the spectral matrice of the received data; hence, five
different estimators are proposed to estimate the angles of the sources. The
simulation results proved the performance of the proposed estimators in the
presence of white noise comparatively to high resolution eigen based spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6739</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6739</id><created>2014-09-23</created><updated>2014-10-17</updated><authors><author><keyname>Li</keyname><forenames>Shi</forenames></author></authors><title>On Uniform Capacitated $k$-Median Beyond the Natural LP Relaxation</title><categories>cs.DS</categories><comments>19 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the uniform capacitated $k$-median problem. Obtaining
a constant approximation algorithm for this problem is a notorious open
problem; most previous works gave constant approximations by either violating
the capacity constraints or the cardinality constraint. Notably, all these
algorithms are based on the natural LP-relaxation for the problem. The
LP-relaxation has unbounded integrality gap, even when we are allowed to
violate the capacity constraints or the cardinality constraint by a factor of
$2-\epsilon$.
  Our result is an $\exp(O(1/\epsilon^2))$-approximation algorithm for the
problem that violates the cardinality constraint by a factor of $1+\epsilon$.
This is already beyond the capability of the natural LP relaxation, as it has
unbounded integrality gap even if we are allowed to open $(2-\epsilon)k$
facilities. Indeed, our result is based on a novel LP for this problem.
  The version as we described is the hard-capacitated version of the problem,
as we can only open one facility at each location. This is as opposed to the
soft-capacitated version, in which we are allowed to open more than one
facilities at each location. We give a simple proof that in the uniform
capacitated case, the soft-capacitated version and the hard-capacitated version
are actually equivalent, up to a small constant loss in the approximation
ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6745</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6745</id><created>2014-09-23</created><authors><author><keyname>Nwogu</keyname><forenames>Ifeoma</forenames></author><author><keyname>Erdogan</keyname><forenames>Goker</forenames></author><author><keyname>Yildirim</keyname><forenames>Ilker</forenames></author><author><keyname>Jacobs</keyname><forenames>Robert</forenames></author></authors><title>A Concept Learning Approach to Multisensory Object Perception</title><categories>cs.CV</categories><comments>6 pages and 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a computational model of concept learning using Bayesian
inference for a grammatically structured hypothesis space, and test the model
on multisensory (visual and haptics) recognition of 3D objects. The study is
performed on a set of artificially generated 3D objects known as fribbles,
which are complex, multipart objects with categorical structures. The goal of
this work is to develop a working multisensory representational model that
integrates major themes on concepts and concepts learning from the cognitive
science literature. The model combines the representational power of a
probabilistic generative grammar with the inferential power of Bayesian
induction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6758</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6758</id><created>2014-09-23</created><updated>2014-11-13</updated><authors><author><keyname>Kekatos</keyname><forenames>Vassilis</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Conejo</keyname><forenames>Antonio J.</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Stochastic Reactive Power Management in Microgrids with Renewables</title><categories>math.OC cs.SY</categories><comments>Accepted in the IEEE Trans. on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distribution microgrids are being challenged by reverse power flows and
voltage fluctuations due to renewable generation, demand response, and electric
vehicles. Advances in photovoltaic (PV) inverters offer new opportunities for
reactive power management provided PV owners have the right investment
incentives. In this context, reactive power compensation is considered here as
an ancillary service. Accounting for the increasing time-variability of
distributed generation and demand, a stochastic reactive power compensation
scheme is developed. Given uncertain active power injections, an online
reactive control scheme is devised. This scheme is distribution-free and relies
solely on power injection data. Reactive injections are updated using the
Lagrange multipliers of a second-order cone program. Numerical tests on an
industrial 47-bus microgrid and the residential IEEE 123-bus feeder corroborate
the reactive power management efficiency of the novel stochastic scheme over
its deterministic alternative, as well as its capability to track variations in
solar generation and household demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6765</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6765</id><created>2014-09-23</created><updated>2015-03-31</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author></authors><title>A Generalization of the AL method for Fair Allocation of Indivisible
  Objects</title><categories>cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the assignment problem in which agents express ordinal
preferences over $m$ objects and the objects are allocated to the agents based
on the preferences. In a recent paper, Brams, Kilgour, and Klamler (2014)
presented the AL method to compute an envy-free assignment for two agents. The
AL method crucially depends on the assumption that agents have strict
preferences over objects. We generalize the AL method to the case where agents
may express indifferences and prove the axiomatic properties satisfied by the
algorithm. As a result of the generalization, we also get a $O(m)$ speedup on
previous algorithms to check whether a complete envy-free assignment exists or
not. Finally, we show that unless P=NP, there can be no polynomial-time
extension of GAL to the case of arbitrary number of agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6767</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6767</id><created>2014-09-22</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Agile Modeling with the UML</title><categories>cs.SE</categories><comments>14 pages, 8 figures, Radical Innovations of Software and Systems
  Engineering in the Future. 9th International Workshop, RISSEF 2002. Venice,
  Italy, October 2002. LNCS 2941. Springer Verlag 2004. arXiv admin note:
  substantial text overlap with arXiv:1409.6616</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a model-based approach to software development. It
argues that an approach using models as central development artifact needs to
be added to the portfolio of software engineering techniques, to further
increase efficiency and flexibility of the development as well as quality and
reusability of the results. Two major and strongly related techniques are
identified and discussed: Test case modeling and an evolutionary approach to
model transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6768</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6768</id><created>2014-09-22</created><authors><author><keyname>Jacobi</keyname><forenames>Carsten</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Hierarchical XP</title><categories>cs.SE</categories><comments>14 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1409.6604</comments><journal-ref>Extreme Programming Examined. Addison-Wesley, 2001</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  XP is a light-weight methodology suited particularly for small-sized teams
that develop software which has only vague or rapidly changing requirements.
The discipline of systems engineering knows it as approach of incremental
system change or also of &quot;muddling through&quot;. In this paper, we introduce three
well known methods of reorganizing companies, namely, the holistic approach,
the incremental approach, and the hierarchical approach. We show similarities
between software engineering methods and company reorganizationprocesses. In
this context, we discuss the extreme programming (XP) approach and how the
elements of the hierarchical approach can improve XP. We provide hints on how
to scale up XP to larger projects e.g. those common in the telecommunication
industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6771</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6771</id><created>2014-09-23</created><authors><author><keyname>Zinoviev</keyname><forenames>Dmitry</forenames></author><author><keyname>Stefanescu</keyname><forenames>Dan</forenames></author><author><keyname>Benbrahim</keyname><forenames>Hamid</forenames></author><author><keyname>Meszoely</keyname><forenames>Greta</forenames></author></authors><title>Mitigation of Delayed Management Costs in Transaction-Oriented Systems</title><categories>cs.DC cs.NI</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abundant examples of complex transaction-oriented networks (TONs) can be
found in a variety of disciplines, including information and communication
technology, finances, commodity trading, and real estate. A transaction in a
TON is executed as a sequence of subtransactions associated with the network
nodes, and is committed if every subtransaction is committed. A subtransaction
incurs a two-fold overhead on the host node: the fixed transient operational
cost and the cost of long-term management (e.g. archiving and support) that
potentially grows exponentially with the transaction length. If the overall
cost exceeds the node capacity, the node fails and all subtransaction incident
to the node, and their parent distributed transactions, are aborted. A TON
resilience can be measured in terms of either external workloads or intrinsic
node fault rates that cause the TON to partially or fully choke. We demonstrate
that under certain conditions, these two measures are equivalent. We further
show that the exponential growth of the long-term management costs can be
mitigated by adjusting the effective operational cost: in other words, that the
future maintenance costs could be absorbed into the transient operational
costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6775</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6775</id><created>2014-09-23</created><updated>2014-09-27</updated><authors><author><keyname>Zhang</keyname><forenames>Rick</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>A Queueing Network Approach to the Analysis and Control of
  Mobility-On-Demand Systems</title><categories>cs.PF cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a queueing network approach to the analysis and control
of mobility-on-demand (MoD) systems for urban personal transportation. A MoD
system consists of a fleet of vehicles providing one-way car sharing service
and a team of drivers to rebalance such vehicles. The drivers then rebalance
themselves by driving select customers similar to a taxi service. We model the
MoD system as two coupled closed Jackson networks with passenger loss. We show
that the system can be approximately balanced by solving two decoupled linear
programs and exactly balanced through nonlinear optimization. The rebalancing
techniques are applied to a system sizing example using taxi data in three
neighborhoods of Manhattan, which suggests that the optimal vehicle-to-driver
ratio in a MoD system is between 3 and 5. Lastly, we formulate a real-time
closed-loop rebalancing policy for drivers and demonstrate its stability (in
terms of customer wait times) for typical system loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6777</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6777</id><created>2014-09-23</created><updated>2015-02-26</updated><authors><author><keyname>Fujii</keyname><forenames>Keisuke</forenames></author><author><keyname>Kobayashi</keyname><forenames>Hirotada</forenames></author><author><keyname>Morimae</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author><author><keyname>Tamate</keyname><forenames>Shuhei</forenames></author><author><keyname>Tani</keyname><forenames>Seiichiro</forenames></author></authors><title>Impossibility of Classically Simulating One-Clean-Qubit Computation</title><categories>quant-ph cs.CC</categories><comments>13 pages, 1 figure. New results and new authors have been added.
  (DQC1_2 is improved to DQC1_1, and collapse of PH is improved from 3rd to 2nd
  level.) Title is also changed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deterministic quantum computation with one quantum bit (DQC1) is a restricted
model of quantum computing where the input state is the completely mixed state
except for a single clean qubit, and only a single output qubit is measured at
the end of the computing. It is proved that the restriction of quantum
computation to the DQC1 model does not change the complexity classes NQP and
SBQP. As a main consequence, it follows that the DQC1 model cannot be
efficiently simulated by classical computers unless the polynomial-time
hierarchy collapses to the second level (more precisely, to AM), which answers
the long-standing open problem posed by Knill and Laflamme under the very
plausible complexity assumption. The argument developed in this paper also
weakens the complexity assumption necessary for the existing impossibility
results on classical simulation of various sub-universal quantum computing
models, such as the IQP model and the Boson sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6780</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6780</id><created>2014-09-23</created><updated>2015-10-01</updated><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Hartikainen</keyname><forenames>Aleksi</forenames></author><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Juha</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Sir&#xe9;n</keyname><forenames>Jouni</forenames></author></authors><title>Document Counting in Practice</title><categories>cs.DS</categories><comments>This is a slightly extended version of the paper that was presented
  at DCC 2015. The implementations are available at
  http://jltsiren.kapsi.fi/rlcsa and https://github.com/ahartik/succinct</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of counting the number of strings in a collection
where a given pattern appears, which has applications in information retrieval
and data mining. Existing solutions are in a theoretical stage. We implement
these solutions and develop some new variants, comparing them experimentally on
various datasets. Our results not only show which are the best options for each
situation and help discard practically unappealing solutions, but also uncover
some unexpected compressibility properties of the best data structures. By
taking advantage of these properties, we can reduce the size of the structures
by a factor of 5--400, depending on the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6792</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6792</id><created>2014-09-23</created><updated>2014-12-17</updated><authors><author><keyname>Takahashi</keyname><forenames>Yasuhiro</forenames></author><author><keyname>Tani</keyname><forenames>Seiichiro</forenames></author><author><keyname>Yamazaki</keyname><forenames>Takeshi</forenames></author><author><keyname>Tanaka</keyname><forenames>Kazuyuki</forenames></author></authors><title>Commuting Quantum Circuits with Few Outputs are Unlikely to be
  Classically Simulatable</title><categories>quant-ph cs.CC</categories><comments>19 pages, 6 figures; v2: Theorems 1 and 3 improved, proofs modified</comments><journal-ref>Quantum Information and Computation, Vol. 16, No. 3&amp;4, (2016)
  0251-0270</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the classical simulatability of commuting quantum circuits with n
input qubits and O(log n) output qubits, where a quantum circuit is classically
simulatable if its output probability distribution can be sampled up to an
exponentially small additive error in classical polynomial time. First, we show
that there exists a commuting quantum circuit that is not classically
simulatable unless the polynomial hierarchy collapses to the third level. This
is the first formal evidence that a commuting quantum circuit is not
classically simulatable even when the number of output qubits is exponentially
small. Then, we consider a generalized version of the circuit and clarify the
condition under which it is classically simulatable. Lastly, we apply the
argument for the above evidence to Clifford circuits in a similar setting and
provide evidence that such a circuit augmented by a depth-1 non-Clifford layer
is not classically simulatable. These results reveal subtle differences between
quantum and classical computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6805</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6805</id><created>2014-09-23</created><authors><author><keyname>Ren</keyname><forenames>Siting</forenames></author><author><keyname>Gao</keyname><forenames>Sheng</forenames></author></authors><title>Improving Cross-domain Recommendation through Probabilistic
  Cluster-level Latent Factor Model--Extended Version</title><categories>cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-domain recommendation has been proposed to transfer user behavior
pattern by pooling together the rating data from multiple domains to alleviate
the sparsity problem appearing in single rating domains. However, previous
models only assume that multiple domains share a latent common rating pattern
based on the user-item co-clustering. To capture diversities among different
domains, we propose a novel Probabilistic Cluster-level Latent Factor (PCLF)
model to improve the cross-domain recommendation performance. Experiments on
several real world datasets demonstrate that our proposed model outperforms the
state-of-the-art methods for the cross-domain recommendation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6806</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6806</id><created>2014-09-23</created><updated>2014-09-26</updated><authors><author><keyname>Fiori</keyname><forenames>Marcelo</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>On spectral properties for graph matching and graph isomorphism problems</title><categories>math.CO cs.DM math.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problems related to graph matching and isomorphisms are very important both
from a theoretical and practical perspective, with applications ranging from
image and video analysis to biological and biomedical problems. The graph
matching problem is challenging from a computational point of view, and
therefore different relaxations are commonly used. Although common relaxations
techniques tend to work well for matching perfectly isomorphic graphs, it is
not yet fully understood under which conditions the relaxed problem is
guaranteed to obtain the correct answer.
  In this paper we prove that the graph matching problem and its most common
convex relaxation, where the matching domain of permutation matrices is
substituted with its convex hull of doubly-stochastic matrices, are equivalent
for a certain class of graphs, such equivalence being based on spectral
properties of the corresponding adjacency matrices. We also derive results
about the automorphism group of a graph, and provide fundamental spectral
properties of the adjacency matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6808</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6808</id><created>2014-09-23</created><authors><author><keyname>Chen</keyname><forenames>Jinyuan</forenames></author><author><keyname>Yang</keyname><forenames>Sheng</forenames></author><author><keyname>Ozgur</keyname><forenames>Ayfer</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Achieving Full DoF in Heterogeneous Parallel Broadcast Channels with
  Outdated CSIT</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, September 2014.
  This work was presented in part at ISIT2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider communication over heterogeneous parallel channels, where a
transmitter is connected to two users via two parallel channels: a MIMO
broadcast channel (BC) and a noiseless rate-limited multicast channel. We
characterize the optimal degrees of freedom (DoF) region of this setting when
the transmitter has delayed channel state information (CSIT) regarding the MIMO
BC. Our results show that jointly coding over the two channels strictly
outperforms simple channel aggregation and can even achieve the instantaneous
CSIT performance with completely outdated CSIT on the MIMO BC in the sum DoF
sense; this happens when the multicast rate of the second channel is larger
than a certain threshold. The main idea is to send information over the MIMO BC
at a rate above its capacity and then use the second channel to send additional
side information to allow for reliable decoding at both receivers. We call this
scheme a two-phase overload-multicast strategy. We show that such a strategy is
also sum DoF optimal for the K-user MIMO BC with a parallel multicast channel
when the rate of the multicast channel is high enough and can again achieve the
instantaneous CSIT performance (optimal sum DoF) with completely outdated CSIT.
For the regime where the capacity of the multicast channel is small, we propose
another joint coding strategy which is sum DoF optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6813</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6813</id><created>2014-09-23</created><updated>2015-09-03</updated><authors><author><keyname>Rahmani</keyname><forenames>Hossein</forenames></author><author><keyname>Mahmood</keyname><forenames>Arif</forenames></author><author><keyname>Huynh</keyname><forenames>Du</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal</forenames></author></authors><title>Histogram of Oriented Principal Components for Cross-View Action
  Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing techniques for 3D action recognition are sensitive to viewpoint
variations because they extract features from depth images which are viewpoint
dependent. In contrast, we directly process pointclouds for cross-view action
recognition from unknown and unseen views. We propose the Histogram of Oriented
Principal Components (HOPC) descriptor that is robust to noise, viewpoint,
scale and action speed variations. At a 3D point, HOPC is computed by
projecting the three scaled eigenvectors of the pointcloud within its local
spatio-temporal support volume onto the vertices of a regular dodecahedron.
HOPC is also used for the detection of Spatio-Temporal Keypoints (STK) in 3D
pointcloud sequences so that view-invariant STK descriptors (or Local HOPC
descriptors) at these key locations only are used for action recognition. We
also propose a global descriptor computed from the normalized spatio-temporal
distribution of STKs in 4-D, which we refer to as STK-D. We have evaluated the
performance of our proposed descriptors against nine existing techniques on two
cross-view and three single-view human action recognition datasets. The
Experimental results show that our techniques provide significant improvement
over state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6825</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6825</id><created>2014-09-24</created><authors><author><keyname>Zaraket</keyname><forenames>Fadi A.</forenames></author><author><keyname>Noureddine</keyname><forenames>Mohamad</forenames></author></authors><title>Model Checking Software Programs with First Order Logic Specifications
  using AIG Solvers</title><categories>cs.SE cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static verification techniques leverage Boolean formula satisfiability
solvers such as SAT and SMT solvers that operate on conjunctive normal form and
first order logic formulae, respectively, to validate programs. They force
bounds on variable ranges and execution time and translate the program and its
specifications into a Boolean formula. They are limited to programs of
relatively low complexity for the following reasons. (1) A small increase in
the bounds can cause a large increase in the size of the translated formula.
(2) Boolean satisfiability solvers are restricted to using optimizations that
apply at the level of the formula. Finally, (3) the Boolean formulae often need
to be regenerated with higher bounds to ensure the correctness of the
translation. We present a method that uses sequential circuits, Boolean
formulae with memory elements and hierarchical structure, and sequential
circuit synthesis and verification frameworks to validate programs. (1)
Sequential circuits are much more succinct than Boolean formulae with no memory
elements and preserve the high-level structure of the program. (2) Encoding the
problem as a sequential circuit enables the use of a number of powerful
automated analysis techniques that have no counterparts for other Boolean
formulae. Our method takes an imperative program with a first order logic
specification consisting of a precondition and a postcondition pair, and a
bound on the program variable ranges, and produces a sequential circuit with a
designated output that is true when the program violates the specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6828</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6828</id><created>2014-09-24</created><authors><author><keyname>Shang</keyname><forenames>Shang</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Hui</keyname><forenames>Pan</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev</forenames></author></authors><title>An Upper Bound on the Convergence Time for Quantized Consensus of
  Arbitrary Static Graphs</title><categories>cs.SY cs.DC</categories><comments>to appear in IEEE Trans. on Automatic Control, January, 2015. arXiv
  admin note: substantial text overlap with arXiv:1208.0788</comments><journal-ref>IEEE Trans. on Automatic Control, 60(4):1127-32, April, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a class of distributed quantized consensus algorithms for
arbitrary static networks. In the initial setting, each node in the network has
an integer value. Nodes exchange their current estimate of the mean value in
the network, and then update their estimation by communicating with their
neighbors in a limited capacity channel in an asynchronous clock setting.
Eventually, all nodes reach consensus with quantized precision. We analyze the
expected convergence time for the general quantized consensus algorithm
proposed by Kashyap et al \cite{Kashyap}. We use the theory of electric
networks, random walks, and couplings of Markov chains to derive an $O(N^3\log
N)$ upper bound for the expected convergence time on an arbitrary graph of size
$N$, improving on the state of art bound of $O(N^5)$ for quantized consensus
algorithms. Our result is not dependent on graph topology. Example of complete
graphs is given to show how to extend the analysis to graphs of given topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6831</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6831</id><created>2014-09-24</created><authors><author><keyname>Shang</keyname><forenames>Shang</forenames></author><author><keyname>Wang</keyname><forenames>Tiance</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev</forenames></author></authors><title>The Application of Differential Privacy for Rank Aggregation: Privacy
  and Accuracy</title><categories>cs.AI cs.CR</categories><comments>Fusion 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential risk of privacy leakage prevents users from sharing their
honest opinions on social platforms. This paper addresses the problem of
privacy preservation if the query returns the histogram of rankings. The
framework of differential privacy is applied to rank aggregation. The error
probability of the aggregated ranking is analyzed as a result of noise added in
order to achieve differential privacy. Upper bounds on the error rates for any
positional ranking rule are derived under the assumption that profiles are
uniformly distributed. Simulation results are provided to validate the
probabilistic analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6838</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6838</id><created>2014-09-24</created><authors><author><keyname>Wang</keyname><forenames>Ruxin</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>Recent Progress in Image Deblurring</title><categories>cs.CV</categories><comments>53 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper comprehensively reviews the recent development of image
deblurring, including non-blind/blind, spatially invariant/variant deblurring
techniques. Indeed, these techniques share the same objective of inferring a
latent sharp image from one or several corresponding blurry images, while the
blind deblurring techniques are also required to derive an accurate blur
kernel. Considering the critical role of image restoration in modern imaging
systems to provide high-quality images under complex environments such as
motion, undesirable lighting conditions, and imperfect system components, image
deblurring has attracted growing attention in recent years. From the viewpoint
of how to handle the ill-posedness which is a crucial issue in deblurring
tasks, existing methods can be grouped into five categories: Bayesian inference
framework, variational methods, sparse representation-based methods,
homography-based modeling, and region-based methods. In spite of achieving a
certain level of development, image deblurring, especially the blind case, is
limited in its success by complex application conditions which make the blur
kernel hard to obtain and be spatially variant. We provide a holistic
understanding and deep insight into image deblurring in this review. An
analysis of the empirical evidence for representative methods, practical
issues, as well as a discussion of promising future directions are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6843</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6843</id><created>2014-09-24</created><updated>2014-10-13</updated><authors><author><keyname>Javarone</keyname><forenames>Marco Alberto</forenames></author></authors><title>Poker as a Skill Game: Rational vs Irrational Behaviors</title><categories>physics.soc-ph cs.GT cs.SI</categories><comments>15 pages, 6 figures</comments><doi>10.1088/1742-5468/2015/03/P03018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In several countries poker games are, probably, the most famous card games.
Although each variant of poker has its own rules, all of them are characterized
by the utilization of money to make sense the challenge. Nowadays, in the
collective consciousness, some variants of poker are referred as skill games
and others as gamble games. The utilization of money plays a fundamental role
as it affects the way people play these games. In particular, different
psychological behaviors can be observed during a challenge. Just to cite a few,
rationality, fear, composure, and even madness, can strongly drive the players'
strategy. Under this perspective, a poker table can be considered as a
psychology lab, where several human behaviors can be observed. In this work, we
develop a preliminary analysis about the role of rationality in poker games,
using the variant Texas Hold'em as reference. In particular, we compare the
performances of two different kinds of players, i.e., rational players vs
irrational players, during a poker tournament. Results show that these
behaviors (i.e., rationality and irrationality) affect both the outcomes of
challenges and the way poker should be classified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6848</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6848</id><created>2014-09-24</created><authors><author><keyname>Chen</keyname><forenames>Xinquan</forenames></author></authors><title>A New Clustering Algorithm Based on Near Neighbor Influence</title><categories>cs.DB</categories><comments>21 pages, 9 figures, and 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Clustering based on Near Neighbor Influence (CNNI), a new
clustering algorithm which is inspired by the idea of near neighbor and the
superposition principle of influence. In order to clearly describe this
algorithm, it introduces some important concepts, such as near neighbor point
set, near neighbor influence, and similarity measure. By simulated experiments
of some artificial data sets and seven real data sets, we observe that this
algorithm can often get good clustering quality when making proper value of
some parameters. At last, it gives some research expectations to popularize
this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6856</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6856</id><created>2014-09-24</created><authors><author><keyname>Padberg</keyname><forenames>Julia</forenames></author></authors><title>Reconfigurable Decorated PT Nets with Inhibitor Arcs and Transition
  Priorities</title><categories>cs.LO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deal with additional control structures for decorated PT
Nets. The main contribution are inhibitor arcs and priorities. The first ensure
that a marking can inhibit the firing of a transition. Inhibitor arcs force
that the transition may only fire when the place is empty. an order of
transitions restrict the firing, so that an transition may fire only if it has
the highest priority of all enabled transitions. This concept is shown to be
compatible with reconfigurable Petri nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6873</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6873</id><created>2014-09-24</created><updated>2015-07-22</updated><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Probabilistic thread algebra</title><categories>cs.LO cs.PL</categories><comments>25 pages (arXiv admin note: text overlap with arXiv:1408.2955,
  arXiv:1402.4950); some simplifications made; substantially revised</comments><acm-class>D.3.3; D.4.1; F.1.1; F.1.2</acm-class><journal-ref>Scientific Annals of Computer Science 25(2):211--243, 2015</journal-ref><doi>10.7561/SACS.2015.2.211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We add probabilistic features to basic thread algebra and its extensions with
thread-service interaction and strategic interleaving. Here, threads represent
the behaviours produced by instruction sequences under execution and services
represent the behaviours exhibited by the components of execution environments
of instruction sequences. In a paper concerned with probabilistic instruction
sequences, we proposed several kinds of probabilistic instructions and gave an
informal explanation for each of them. The probabilistic features added to the
extension of basic thread algebra with thread-service interaction make it
possible to give a formal explanation in terms of non-probabilistic
instructions and probabilistic services. The probabilistic features added to
the extensions of basic thread algebra with strategic interleaving make it
possible to cover strategies corresponding to probabilistic scheduling
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6874</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6874</id><created>2014-09-24</created><updated>2014-11-23</updated><authors><author><keyname>Walk</keyname><forenames>Philipp</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author><author><keyname>Pfander</keyname><forenames>G&#xf6;tz E.</forenames></author></authors><title>On the Stability of Sparse Convolutions</title><categories>cs.IT math.IT</categories><comments>submitted to Applied and Computational Harmonic Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a stability result for sparse convolutions on $\ell^2(G)\times
\ell^1(G)$ for torsion-free discrete Abelian groups $G$ such as $\mathbb{Z}$.
It turns out, that the torsion-free property prevents full cancellation in the
convolution of sparse sequences and hence allows to establish stability in each
entry, that is, for any fixed entry of the convolution the resulting linear map
is injective with an universal lower norm bound, which only depends on the
support cardinalities of the sequences. This can be seen as a reverse statement
of the famous Young inequality for sparse convolutions. Our result hinges on a
compression argument in additive set theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6883</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6883</id><created>2014-09-24</created><authors><author><keyname>Chakkor</keyname><forenames>Saad</forenames></author><author><keyname>Baghouri</keyname><forenames>Mostafa</forenames></author><author><keyname>Hajraoui</keyname><forenames>Abderrahmane</forenames></author></authors><title>Performance Analysis of Faults Detection in Wind Turbine Generator Based
  on High-Resolution Frequency Estimation Methods</title><categories>cs.SY</categories><comments>10 pages, 17 figures, 4 tables, journal paper</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA)2014,Vol. 5, No. 4, pages 139-148</journal-ref><doi>10.14569/IJACSA.2014.050420</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Electrical energy production based on wind power has become the most popular
renewable resources in the recent years because it gets reliable clean energy
with minimum cost. The major challenge for wind turbines is the electrical and
the mechanical failures which can occur at any time causing prospective
breakdowns and damages and therefore it leads to machine downtimes and to
energy production loss. To circumvent this problem, several tools and
techniques have been developed and used to enhance fault detection and
diagnosis to be found in the stator current signature for wind turbines
generators. Among these methods, parametric or super-resolution frequency
estimation methods, which provides typical spectrum estimation, can be useful
for this purpose. Facing on the plurality of these algorithms, a comparative
performance analysis is made to evaluate robustness based on different metrics:
accuracy, dispersion, computation cost, perturbations and faults severity.
Finally, simulation results in MATLAB with most occurring faults indicate that
ESPRIT and R-MUSIC algorithms have high capability of correctly identifying the
frequencies of fault characteristic components, a performance ranking had been
carried out to demonstrate the efficiency of the studied methods in faults
detecting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6884</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6884</id><created>2014-09-24</created><authors><author><keyname>Chakkor</keyname><forenames>Saad</forenames></author><author><keyname>Cheikh</keyname><forenames>El Ahmadi</forenames></author><author><keyname>Baghouri</keyname><forenames>Mostafa</forenames></author><author><keyname>Hajraoui</keyname><forenames>Abderrahmane</forenames></author></authors><title>Comparative Performance Analysis of Wireless Communication Protocols for
  Intelligent Sensors and Their Applications</title><categories>cs.NI</categories><comments>10 pages, 11 figures, 7 tables, journal paper</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 5, No. 4, 2014, pages 76-85</journal-ref><doi>10.14569/IJACSA.2014.050413</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The systems based on intelligent sensors are currently expanding, due to
theirs functions and theirs performances of intelligence: transmitting and
receiving data in real-time, computation and processing algorithms, metrology
remote, diagnostics, automation and storage measurements...The radio frequency
wireless communication with its multitude offers a better solution for data
traffic in this kind of systems. The mains objectives of this paper is to
present a solution of the problem related to the selection criteria of a better
wireless communication technology face up to the constraints imposed by the
intended application and the evaluation of its key features. The comparison
between the different wireless technologies (Wi-Fi, Wi-Max, UWB, Bluetooth,
ZigBee, ZigBeeIP, GSM/GPRS) focuses on their performance which depends on the
areas of utilization. Furthermore, it shows the limits of their
characteristics. Study findings can be used by the developers/ engineers to
deduce the optimal mode to integrate and to operate a system that guarantees
quality of communication, minimizing energy consumption, reducing the
implementation cost and avoiding time constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6902</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6902</id><created>2014-09-24</created><authors><author><keyname>Goseling</keyname><forenames>Jasper</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Sign-Compute-Resolve for Random Access</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at 52nd Annual Allerton Conference on
  Communication, Control, and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to random access that is based on three elements:
physical-layer network coding, signature codes and tree splitting. Upon
occurrence of a collision, physical-layer network coding enables the receiver
to decode the sum of the information that was transmitted by the individual
users. For each user this information consists of the data that the user wants
to communicate as well as the user's signature. As long as no more than $K$
users collide, their identities can be recovered from the sum of their
signatures. A splitting protocol is used to deal with the case that more than
$K$ users collide. We measure the performance of the proposed method in terms
of user resolution rate as well as overall throughput of the system. The
results show that our approach significantly increases the performance of the
system even compared to coded random access, where collisions are not wasted,
but are reused in successive interference cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6911</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6911</id><created>2014-09-24</created><updated>2014-11-18</updated><authors><author><keyname>Shen</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Xue</keyname><forenames>Xiangyang</forenames></author></authors><title>Do More Dropouts in Pool5 Feature Maps for Better Object Detection</title><categories>cs.CV</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Convolutional Neural Networks (CNNs) have gained great success in image
classification and object detection. In these fields, the outputs of all layers
of CNNs are usually considered as a high dimensional feature vector extracted
from an input image and the correspondence between finer level feature vectors
and concepts that the input image contains is all-important. However, fewer
studies focus on this deserving issue. On considering the correspondence, we
propose a novel approach which generates an edited version for each original
CNN feature vector by applying the maximum entropy principle to abandon
particular vectors. These selected vectors correspond to the unfriendly
concepts in each image category. The classifier trained from merged feature
sets can significantly improve model generalization of individual categories
when training data is limited. The experimental results for
classification-based object detection on canonical datasets including VOC 2007
(60.1%), 2010 (56.4%) and 2012 (56.3%) show obvious improvement in mean average
precision (mAP) with simple linear support vector machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6913</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6913</id><created>2014-09-24</created><updated>2015-03-26</updated><authors><author><keyname>Dubey</keyname><forenames>Chandan</forenames></author><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author></authors><title>Generating a Quadratic Forms from a Given Genus</title><categories>cs.DS math.NT math.RA</categories><comments>arXiv admin note: text overlap with arXiv:1409.6199</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a non-empty genus in $n$ dimensions with determinant $d$, we give a
randomized algorithm that outputs a quadratic form from this genus. The time
complexity of the algorithm is poly$(n,\log d)$; assuming Generalized Riemann
Hypothesis (GRH).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6915</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6915</id><created>2014-09-24</created><authors><author><keyname>Fontoura</keyname><forenames>Marcus</forenames></author><author><keyname>Pree</keyname><forenames>Wolfgang</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>UML-F: A Modeling Language for Object-Oriented Frameworks</title><categories>cs.SE</categories><comments>22 pages, 10 figures</comments><journal-ref>Proceedings of ECOOP 2000-Object-Oriented Programming Conference,
  pp.63-83 LNCS 1850, Springer Verlag, 2000</journal-ref><doi>10.1007/3-540-45102-1_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents the essential features of a new member of the UML language
family that supports working with object-oriented frameworks. This UML
extension, called UML-F, allows the explicit representation of framework
variation points. The paper discusses some of the relevant aspects of UML-F,
which is based on standard UML extension mechanisms. A case study shows how it
can be used to assist framework development. A discussion of additional tools
for automating framework implementation and instantiation rounds out the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6916</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6916</id><created>2014-09-24</created><authors><author><keyname>Cook</keyname><forenames>Steve</forenames></author><author><keyname>Kleppe</keyname><forenames>Anneke</forenames></author><author><keyname>Mitchell</keyname><forenames>Richard</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Warmer</keyname><forenames>Jos</forenames></author><author><keyname>Wills</keyname><forenames>Alan</forenames></author></authors><title>Defining UML Family Members Using Prefaces</title><categories>cs.SE</categories><comments>13 pages, 5 figures</comments><journal-ref>Technology of Object-Oriented Languages and Systems, TOOLS'99
  Pacific. IEEE Computer Society, 1999</journal-ref><doi>10.1109/TOOLS.1999.809418</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unified Modeling Language is extensible, and so can be regarded as a
family of languages. Implicitly or explicitly, any particular UML model should
be accompanied by a definition of the particular UML family member used for the
model. The definition should cover syntactic and semantic issues. This paper
proposes a mechanism for associating models with such definitions. Any
particular definition would form what we call a preface. The name is intended
to suggest that the definition of a particular UML family member must
conceptually come before any model built using that family member. A preface
would be large, and should be organised using packages. This would allow large
amounts of sharing between different prefaces. The paper proposes that prefaces
should have an axiomatic style of semantics, through not necessarily fully
formal, and it offers a general approach to semantics that would reduce
problems of inconsistency within a large preface, based on the idea of general
cases and special cases
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6917</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6917</id><created>2014-09-24</created><authors><author><keyname>Evans</keyname><forenames>Andy</forenames></author><author><keyname>Lano</keyname><forenames>Kevin</forenames></author><author><keyname>France</keyname><forenames>Robert</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Meta-Modeling Semantics of UML</title><categories>cs.SE</categories><comments>18 pages, 6 figures</comments><journal-ref>Behavioral Specifications of Businesses and Systems. Kluver
  Academic Publisher, 1999</journal-ref><doi>10.1007/978-1-4615-5229-1_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unified Modelling Language is emerging as a de-facto standard for
modelling object-oriented systems. However, the semantics document that a part
of the standard definition primarily provides a description of the language's
syntax and well-formedness rules. The meaning of the language, which is mainly
described in English, is too informal and unstructured to provide a foundation
for developing formal analysis and development techniques. This paper outlines
a formalisation strategy for making precise the core semantics of UML. This is
achieved by strengthening the denotational semantics of the existing UML
metamodel. To illustrate the approach, the semantics of
generalization/specialization are made precise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6919</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6919</id><created>2014-09-24</created><authors><author><keyname>Evans</keyname><forenames>Andy</forenames></author><author><keyname>France</keyname><forenames>Robert</forenames></author><author><keyname>Lano</keyname><forenames>Kevin</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>The UML as a Formal Modeling Notation</title><categories>cs.SE</categories><comments>13 pages, 0 figures</comments><journal-ref>The Unified Modeling Language - Workshop UML'98: Beyond the
  Notation. Springer Verlag Berlin, LNCS 1618, 1999</journal-ref><doi>10.1007/978-3-540-48480-6_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unified Modeling Language (UML) is rapidly emerging as a de-facto
standard for modelling OO systems. Given this role, it is imperative that the
UML needs a well-defined, fully explored semantics. Such semantics is required
in order to ensure that UML concepts are precisely stated and defined. In this
paper we motivate an approach to formalizing UML in which formal specification
techniques are used to gain insight into the semantics of UML notations and
diagrams and describe a roadmap for this approach. The authors initiated the
Precise UML (PUML) group in order to develop a precise semantic model for UML
diagrams. The semantic model is to be used as the basis for a set of
diagrammatical transformation rules, which enable formal deductions to be made
about UML diagrams. A small example shows how these rules can be used to verify
whether one class diagram is a valid deduction of another. Because these rules
are presented at the diagrammatical level, it will be argued that UML can be
successfully used as a formal modelling tool without the notational
complexities that are commonly found in textual specification techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6921</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6921</id><created>2014-09-24</created><authors><author><keyname>Kozik</keyname><forenames>Jakub</forenames></author><author><keyname>Shabanov</keyname><forenames>Dmitry</forenames></author></authors><title>Improved algorithms for colorings of simple hypergraphs and applications</title><categories>math.CO cs.DM</categories><comments>16 pages</comments><msc-class>05C15, 05C65, 05D40</msc-class><acm-class>G.2.1; G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with extremal problems concerning colorings of hypergraphs.
By using a random recoloring algorithm we show that any $n$-uniform simple
(i.e. every two distinct edges share at most one vertex) hypergraph $H$ with
maximum edge degree at most \[
  \Delta(H)\leq c\cdot nr^{n-1}, \] is $r$-colorable, where $c&gt;0$ is an
absolute constant. %We prove also that similar result holds for $b$-simple
hypergraphs.
  As an application of our proof technique we establish a new lower bound for
Van der Waerden number $W(n,r)$, the minimum $N$ such that in any $r$-coloring
of the set $\{1,...,N\}$ there exists a monochromatic arithmetic progression of
length $n$. We show that \[
  W(n,r)&gt;c\cdot r^{n-1}, \] for some absolute constant $c&gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6923</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6923</id><created>2014-09-24</created><authors><author><keyname>Kilov</keyname><forenames>Haim</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Second ECOOP Workshop on Precise Behavioral Semantics (with an Emphasis
  on OO Business Specifications)</title><categories>cs.SE</categories><comments>21 pages, 0 figures</comments><journal-ref>Object-Oriented Technology - ECOOP'98 Workshop Reader. 1998</journal-ref><doi>10.1007/3-540-49255-0_39</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business specifications are essential to describe and understand businesses
(and, in particular, business rules) independently of any computing systems
used for their possible automation. They have to express this understanding in
a clear, precise, and explicit way, in order to act as a common ground between
business domain experts and software developers. They also provide the basis
for reuse of concepts and constructs (&quot;patterns&quot;) common to all - from finance
to telecommunications -, or a large number of, businesses, and in doing so save
intellectual effort, time and money. Moreover, these patterns substantially
ease the elicitation and validation of business specifications during
walkthroughs with business customers, and support separation of concerns using
viewpoints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6924</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6924</id><created>2014-09-24</created><authors><author><keyname>Huber</keyname><forenames>Franz</forenames></author><author><keyname>Rausch</keyname><forenames>Andreas</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Modeling Dynamic Component Interfaces</title><categories>cs.SE</categories><comments>13 pages, 5 figures</comments><journal-ref>TOOLS 26, Technology of Object-Oriented Languages and Systems.
  IEEE Computer Society. 1998</journal-ref><doi>10.1109/TOOLS.1998.711003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We adopt a component model based on object-oriented systems, introducing the
concepts of components and their structure. A component consists of a
dynamically changing set of connected objects. Only some of these objects are
interface objects, and are thus accessible from the environment. During the
component lifetime not only the number of objects, but also that of interface
objects, and their connections change. To describe this situation, we introduce
component interface diagrams (CIDs)-an adaption of UML diagrams-as a notation
to characterize interfaces of components, their structure, and their
navigability. We show how CIDs can be used to describe the in-house developed
Open Editor Framework (OEF). Finally, we give guidelines that allow to map
components described with CIDs directly to several component technologies, like
ActiveX, CORBA, or Java Beans
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6925</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6925</id><created>2014-09-24</created><updated>2015-10-13</updated><authors><author><keyname>Giannakopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>A Note on Selling Optimally Two Uniformly Distributed Goods</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new, much simplified and straightforward proof to a result of
Pavlov [2011] regarding the revenue maximizing mechanism for selling two goods
with uniformly i.i.d. valuations over intervals $[c,c+1]$, to an additive
buyer. This is done by explicitly defining optimal dual solutions to a relaxed
version of the problem, where the convexity requirement for the bidder's
utility has been dropped. Their optimality comes directly from their structure,
through the use of exact complementarity. For $c=0$ and $c\geq 0.092$ it turns
out that the corresponding optimal primal solution is a feasible selling
mechanism, thus the initial relaxation comes without a loss, and revenue
maximality follows. However, for $0&lt;c&lt;0.092$ that's not the case, providing the
first clear example where relaxing convexity provably does not come for free,
even in a two-item regularly i.i.d. setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6926</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6926</id><created>2014-09-24</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>A Note on Semantics (with an Emphasis on UML)</title><categories>cs.SE</categories><comments>21 pages, 2 figures</comments><report-no>TUM-I9813</report-no><journal-ref>Second ECOOP Workshop on Precise Behavioral Semantics Technische
  Universit\&quot;at M\&quot;unchen, TUM-I9813. 1998</journal-ref><doi>10.1007/3-540-49255-0_39</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note clarifies the concept of syntax and semantics and their
relationships. Today, a lot of confusion arises from the fact that the word
&quot;semantics&quot; is used in different meanings. We discuss a general approach at
defining semantics that is feasible for both textual and diagrammatic notations
and discuss this approach using an example formalization. The formalization of
hierarchical Mealy automata and their semantics definition using input/output
behaviors allows us to define a specification, as well as an implementation
semantics. Finally, a classification of different approaches that fit in this
framework is given. This classification may also serve as guideline when
defining a semantics for a new language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6928</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6928</id><created>2014-09-24</created><authors><author><keyname>Evans</keyname><forenames>Andy</forenames></author><author><keyname>France</keyname><forenames>Robert</forenames></author><author><keyname>Lano</keyname><forenames>Kevin</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Developing the UML as a Formal Modelling Notation</title><categories>cs.SE</categories><comments>12 pages, 0 figures. arXiv admin note: substantial text overlap with
  arXiv:1409.6919</comments><journal-ref>UML'98 Beyond the notation. International Workshop Mulhouse. Ecole
  Superieure Mulhouse, Universite de Haute-Alsace. 1998</journal-ref><doi>10.1007/978-3-540-48480-6_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unified Modeling Language (UML) is rapidly emerging as a de-facto
standard for modelling OO systems. Given this role, it is imperative that the
UML have a well- defined, fully explored semantics. Such semantics is required
in order to ensure that UML concepts are precisely stated and defined. In this
paper we describe and motivate an approach to formalizing UML in which formal
specification techniques are used to gain insight into the semantics of UML
notations and diagrams. We present work carried out by the Precise UML (PUML)
group on the development of a precise semantic model for UML class diagrams.
The semantic model is used as the basis for a set of diagrammatical
transformation rules, which enable formal deductions to be made about UML class
diagrams. It is also shown how these rules can be used to verify whether one
class diagram is a valid refinement (design) of another. Because these rules
are presented at the diagrammatical level, it will be argued that UML can be
successfully used as a formal modelling tool without the notational
complexities that are commonly found in formal specification techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6930</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6930</id><created>2014-09-24</created><authors><author><keyname>Breu</keyname><forenames>Ruth</forenames></author><author><keyname>Grosu</keyname><forenames>Radu</forenames></author><author><keyname>Huber</keyname><forenames>Franz</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schwerin</keyname><forenames>Wolfgang</forenames></author></authors><title>Towards a Precise Semantics for Object-Oriented Modeling Techniques</title><categories>cs.SE</categories><comments>6 pages, 0 figures</comments><journal-ref>Object-Oriented Technology, ECOOP'97 Workshop Reader. Jan Bosch,
  Stuart Mitchell (eds.) Springer Verlag, LNCS 1357, 1997</journal-ref><doi>10.1007/3-540-69687-3_42</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a possible way how a precise semantics of object
oriented modeling techniques can be achieved and what the possible benefits are
.We outline the main modeling techniques used in the SysLab project sketch how
a precise semantics can be given and how this semantics can be used during the
development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6931</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6931</id><created>2014-09-24</created><authors><author><keyname>Fuchs</keyname><forenames>Max</forenames></author><author><keyname>Nazareth</keyname><forenames>Dieter</forenames></author><author><keyname>Daniel</keyname><forenames>Dirk</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>BMW-ROOM An Object-Oriented Method for ASCET</title><categories>cs.SE</categories><comments>11 pages, 12 figures</comments><journal-ref>SAE'98, Cobo Center (Detroit, Michigan, USA), Society of
  Automotive Engineers, 1998</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an object-oriented method customized for a tool-assisted
development of car software components. Tough market conditions motivate smart
software development. ASCET SD is a tool to generate target code from graphic
specifications, avoiding costly programming in C. But ASCET lacks guidelines on
what to do, how to do it, in what order, like a fully equipped kitchen without
a cooking book. Plans to employ the tool for BMW vehicle software sparked off
demand for an adequate, object-oriented real-time methodology. We show how to
scan the methodology market in order to adopt an already existing method for
this purpose. The result of the adaptation of a chosen method to ASCET SD is a
pragmatic version of ROOM, which we call BROOM. We present a modeling guidebook
that includes process recommendations not only for the automotive sector, but
for real-time software development in general. The method suggests to produce
early prototypes that are validated and refined to completion. BROOM offers
phase-independent, harmonic guidelines. Product requirements, in form of
scenarios, are transformed through several activities into operational models.
BROOM takes advantage of ASCET's rich experimentation- and code generation
features. These allow to validate emerging models on button press. The factual
development of a simplified heating/cooling system at BMW serves as a running
example throughout the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6932</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6932</id><created>2014-09-24</created><authors><author><keyname>Philipps</keyname><forenames>Jan</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Refinement of Information Flow Architectures</title><categories>cs.SE</categories><comments>10 pages, 2 figures</comments><journal-ref>ICFEM'97 Proceedings, Hiroshima. Japan., M. Hinchey, IEEE CS
  Press. 1997</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A calculus is presented for the stepwise refinement of abstract information
flow architectures. We give a mathematical model for information flow
components based on relations between input and output communication histories,
and describe system architectures using two views: the glass box view is a
network of basic components, while the black box view regards the network
itself as a component. This allows us to hierarchically compose systems. The
calculus consists of basic rules to add or remove components and channels, and
to replace components by subnetworks and vice versa. The correctness of the
rules is justified by the refinement relation on the black box view of
architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6941</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6941</id><created>2014-09-24</created><authors><author><keyname>Chen</keyname><forenames>Yue</forenames></author><author><keyname>Bu&#x161;i&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author></authors><title>Individual risk in mean-field control models for decentralized control,
  with application to automated demand response</title><categories>math.OC cs.SY</categories><comments>Publication without appendix to appear in the 53rd IEEE Conf. on
  Decision and Control, December, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flexibility of energy consumption can be harnessed for the purposes of
ancillary services in a large power grid. In prior work by the authors a
randomized control architecture is introduced for individual loads for this
purpose. In examples it is shown that the control architecture can be designed
so that control of the loads is easy at the grid level: Tracking of a balancing
authority reference signal is possible, while ensuring that the quality of
service (QoS) for each load is acceptable on average. The analysis was based on
a mean field limit (as the number of loads approaches infinity), combined with
an LTI-system approximation of the aggregate nonlinear model. This paper
examines in depth the issue of individual risk in these systems. The main
contributions of the paper are of two kinds:
  Risk is modeled and quantified:
  (i) The average performance is not an adequate measure of success. It is
found empirically that a histogram of QoS is approximately Gaussian, and
consequently each load will eventually receive poor service.
  (ii) The variance can be estimated from a refinement of the LTI model that
includes a white-noise disturbance; variance is a function of the randomized
policy, as well as the power spectral density of the reference signal.
  Additional local control can eliminate risk:
  (iii) The histogram of QoS is truncated through this local control, so that
strict bounds on service quality are guaranteed.
  (iv) This has insignificant impact on the grid-level performance, beyond a
modest reduction in capacity of ancillary service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6952</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6952</id><created>2014-09-24</created><updated>2015-04-04</updated><authors><author><keyname>Bonnet</keyname><forenames>Edouard</forenames></author><author><keyname>Escoffier</keyname><forenames>Bruno</forenames></author><author><keyname>Paschos</keyname><forenames>Vangelis</forenames></author><author><keyname>Stamoulis</keyname><forenames>Georgios</forenames></author></authors><title>A 0.821-ratio purely combinatorial algorithm for maximum $k$-vertex
  cover in bipartite graphs</title><categories>cs.DS</categories><comments>25 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal in this paper is to propose a \textit{combinatorial algorithm} that
beats the only such algorithm known previously, the greedy one. We study the
polynomial approximation of the Maximum Vertex Cover Problem in bipartite
graphs by a purely combinatorial algorithm and present a computer assisted
analysis of it, that finds the worst case approximation guarantee that is
bounded below by~0.821.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6964</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6964</id><created>2014-09-24</created><authors><author><keyname>Soos</keyname><forenames>Sandor</forenames></author></authors><title>Knowledge discovery via multidimensional science maps: the case of the
  Species Problem</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Science mapping (SM), the study of the organization and development of
science and technology, is a rapidly developing field within information
science. The volume of available data allows this methodology to empirically
address such issues as the historical development of topics, discourses, fields
or the entire science system. Based on the pool of related methods, we are
proposing an integration of various maps to obtain a novel kind of science map
we call multidimensional. The basic idea behind is to combine the most
informative relations available from various maps based on different
bibliometric indicators, in order to produce a rich structrue for the study of
knowledge dynamics, with special emphasis on causal-historical connections. As
a proof of concept, we deploy the proposed framework in an extensive case study
on a historical topic from the life sciences, namely, the debate on the species
concept in biosystematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6967</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6967</id><created>2014-09-24</created><updated>2014-11-20</updated><authors><author><keyname>Dhurandhar</keyname><forenames>Amit</forenames></author><author><keyname>Gurumoorthy</keyname><forenames>Karthik</forenames></author></authors><title>Symmetric Submodular Clustering with Actionable Constraint</title><categories>cs.DS</categories><comments>This research work benefited from the support of the AIRBUS Group
  Corporate Foundation Chair in Mathematics of Complex Systems established in
  ICTS-TIFR. appears in Discrete Optimization in Machine Learning, A Neural
  Information Processing Systems (NIPS) Workshop, 2014</comments><msc-class>68W25, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering with submodular functions has been of interest over the last few
years. Symmetric submodular functions are of particular interest as minimizing
them is significantly more efficient and they include many commonly used
functions in practice viz. graph cuts, mutual information. In this paper, we
propose a novel constraint to make clustering actionable which is motivated by
applications across multiple domains, and pose the problem of performing
symmetric submodular clustering subject to this constraint. We see that
obtaining a $k$ partition with approximation guarantees is a non-trivial task
requiring further work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6977</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6977</id><created>2014-09-24</created><authors><author><keyname>Hoyrup</keyname><forenames>Mathieu</forenames></author><author><keyname>Rojas</keyname><forenames>Cristobal</forenames></author></authors><title>On the information carried by programs about the objects they compute</title><categories>cs.LO math.LO</categories><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computability theory and computable analysis, finite programs can compute
infinite objects. Presenting a computable object via any program for it,
provides at least as much information as presenting the object itself, written
on an infinite tape. What additional information do programs provide? We
characterize this additional information to be any upper bound on the
Kolmogorov complexity of the object. Hence we identify the exact relationship
between Markov-computability and Type-2-computability. We then use this
relationship to obtain several results characterizing the computational and
topological structure of Markov-semidecidable sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6981</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6981</id><created>2014-09-24</created><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author></authors><title>Unsupervised learning of regression mixture models with unknown number
  of components</title><categories>stat.ME cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regression mixture models are widely studied in statistics, machine learning
and data analysis. Fitting regression mixtures is challenging and is usually
performed by maximum likelihood by using the expectation-maximization (EM)
algorithm. However, it is well-known that the initialization is crucial for EM.
If the initialization is inappropriately performed, the EM algorithm may lead
to unsatisfactory results. The EM algorithm also requires the number of
clusters to be given a priori; the problem of selecting the number of mixture
components requires using model selection criteria to choose one from a set of
pre-estimated candidate models. We propose a new fully unsupervised algorithm
to learn regression mixture models with unknown number of components. The
developed unsupervised learning approach consists in a penalized maximum
likelihood estimation carried out by a robust expectation-maximization (EM)
algorithm for fitting polynomial, spline and B-spline regressions mixtures. The
proposed learning approach is fully unsupervised: 1) it simultaneously infers
the model parameters and the optimal number of the regression mixture
components from the data as the learning proceeds, rather than in a two-fold
scheme as in standard model-based clustering using afterward model selection
criteria, and 2) it does not require accurate initialization unlike the
standard EM for regression mixtures. The developed approach is applied to curve
clustering problems. Numerical experiments on simulated data show that the
proposed robust EM algorithm performs well and provides accurate results in
terms of robustness with regard initialization and retrieving the optimal
partition with the actual number of clusters. An application to real data in
the framework of functional data clustering, confirms the benefit of the
proposed approach for practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.6991</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.6991</id><created>2014-09-24</created><authors><author><keyname>Li</keyname><forenames>Yunsheng</forenames></author><author><keyname>Jin</keyname><forenames>Chi</forenames></author></authors><title>An Extended Small-Gain Theorem</title><categories>cs.SY</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the results of the general small-gain theorem proposed by Z.P
Jiang. The significance of this extension is two fold. First, it allows one to
use general vector norm to characterize the input-to-output property of two
interconnected subsystems. Second, the gain from input signals to output of
each sub-system is provided, which offers one more flexibility in control
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7017</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7017</id><created>2014-09-24</created><updated>2015-02-05</updated><authors><author><keyname>G&#xe9;nois</keyname><forenames>Mathieu</forenames></author><author><keyname>Vestergaard</keyname><forenames>Christian L.</forenames></author><author><keyname>Fournet</keyname><forenames>Julie</forenames></author><author><keyname>Panisson</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Bonmarin</keyname><forenames>Isabelle</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author></authors><title>Data on face-to-face contacts in an office building suggests a low-cost
  vaccination strategy based on community linkers</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Network Science 3, 326-347 (2015)</journal-ref><doi>10.1017/nws.2015.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical data on contacts between individuals in social contexts play an
important role in providing information for models describing human behavior
and how epidemics spread in populations. Here, we analyze data on face-to-face
contacts collected in an office building. The statistical properties of
contacts are similar to other social situations, but important differences are
observed in the contact network structure. In particular, the contact network
is strongly shaped by the organization of the offices in departments, which has
consequences in the design of accurate agent-based models of epidemic spread.
We consider the contact network as a potential substrate for infectious disease
spread and show that its sparsity tends to prevent outbreaks of rapidly
spreading epidemics. Moreover, we define three typical behaviors according to
the fraction $f$ of links each individual shares outside its own department:
residents, wanderers and linkers. Linkers ($f\sim 50\%$) act as bridges in the
network and have large betweenness centralities. Thus, a vaccination strategy
targeting linkers efficiently prevents large outbreaks. As such a behavior may
be spotted a priori in the offices' organization or from surveys, without the
full knowledge of the time-resolved contact network, this result may help the
design of efficient, low-cost vaccination or social-distancing strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7033</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7033</id><created>2014-09-24</created><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Zolt&#xe1;n</forenames></author></authors><title>Shortest Paths in Nearly Conservative Digraphs</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the following notion: a digraph $D=(V,A)$ with arc weights $c:
A\rightarrow \R$ is called nearly conservative if every negative cycle consists
of two arcs. Computing shortest paths in nearly conservative digraphs is
NP-hard, and even deciding whether a digraph is nearly conservative is
coNP-complete.
  We show that the &quot;All Pairs Shortest Path&quot; problem is fixed parameter
tractable with various parameters for nearly conservative digraphs. The results
also apply for the special case of conservative mixed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7034</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7034</id><created>2014-09-24</created><authors><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author><author><keyname>Negrete-Pincetic</keyname><forenames>Matias</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author><author><keyname>Varaiya</keyname><forenames>Pravin</forenames></author></authors><title>Rate-constrained Energy Services: Allocation Policies and Market
  Decisions</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of renewable generation poses operational and economic
challenges for the electricity grid. For the core problem of power balance, the
legacy paradigm of tailoring supply to follow random demand may be
inappropriate under deep penetration of uncertain and intermittent renewable
generation. In this situation, there is an emerging consensus that the
alternative approach of controlling demand to follow random supply offers
compelling economic benefits in terms of reduced regulation costs. This
approach exploits the flexibility of demand side resources and requires
sensing, actuation, and communication infrastructure; distributed control
algorithms; and viable schemes to compensate participating loads. This paper
considers rate-constrained energy services which are a specific paradigm for
flexible demand. These services are characterized by a specified delivery
window, the total amount of energy that must be supplied over this window, and
the maximum rate at which this energy may be delivered. We consider a forward
market where rate-constrained energy services are traded. We explore allocation
policies and market decisions of a supplier in this market. The supplier owns a
generation mix that includes some uncertain renewable generation and may also
purchase energy in day-ahead and real-time markets to meet customer demand. The
supplier must optimally select the portfolio of rate-constrained services to
sell, the amount of day-ahead energy to buy, and the policies for making
real-time energy purchases and allocations to customers to maximize its
expected profit. We offer solutions to the supplier's decision and control
problems to economically provide rate constrained energy services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7042</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7042</id><created>2014-09-24</created><authors><author><keyname>Schwabe</keyname><forenames>Arne</forenames></author><author><keyname>Karl</keyname><forenames>Holger</forenames></author></authors><title>Adding Geographical Embedding to AS Topology Generation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To study large-scale effects on the Internet various mod- els have been
introduced to generate Internet-like autono- mous system (AS) topologies. The
models for large-scale AS topologies have been focused on replicating
structural graph properties. One of the most promising model is the Positive
Feedback Model model (PFP). These models how- ever lack the ability to generate
routing path and realistic latency. We present a model to enrich the AS peering
graph with peering points. Our new model allows to calculate path for the
connections between end hosts and to infer the latency from these paths. We
introduce a new notion for the genera- tion of AS topologies: the compactness
of an AS. We introduce an algorithm based on the PFP algorithm which generates
instances for our model. Verifying the gen- erated model instances shows that
the resulting latencies as well as the geographic properties match measured
data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7047</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7047</id><created>2014-09-24</created><authors><author><keyname>Wang</keyname><forenames>Tiance</forenames></author><author><keyname>Hui</keyname><forenames>Pan</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>Cooperative Caching based on File Popularity Ranking in Delay Tolerant
  Networks</title><categories>cs.NI</categories><comments>6 pages, 2 figures, ExtremeCom 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing storage sizes and WiFi/Bluetooth capabilities of mobile devices
have made them a good platform for opportunistic content sharing. In this work
we propose a network model to study this in a setting with two characteristics:
1. delay tolerant; 2. lack of infrastructure. Mobile users generate requests
and opportunistically download from other users they meet, via Bluetooth or
WiFi. The difference in popularity of different web content induces a
non-uniform request distribution, which is usually a Zipf's law distribution.
We evaluate the performance of different caching schemes and derive the optimal
scheme using convex optimization techniques. The optimal solution is found
efficiently using a binary search method. It is shown that as the network
mobility increases, the performance of the optimal scheme far exceeds the
traditional caching scheme. To the best of our knowledge, our work is the first
to consider popularity ranking in performance evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7060</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7060</id><created>2014-09-22</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Scholz</keyname><forenames>Peter</forenames></author></authors><title>A manager's view on large scale XP projects</title><categories>cs.SE</categories><comments>4 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1409.6604</comments><journal-ref>Third International Conference on Extreme Programming and Flexible
  Processes in Software Engineering, XP2002, May 26-30, Alghero, Italy, pg.
  158-159, 2002</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  XP is a code oriented, light weight software engineering methodology, suited
merely for small sized teams who develop software that relies on vague or
rapidly changing requirements. Being very code oriented, the discipline of
systems engineering knows it as approach of incremental system change. In this
contribution, we discuss the enhanced version of a concept on how to extend XP
on large scale projects with hundreds of software engineers and programmers,
respectively. A previous version was already presented in [1]. The basic idea
is to apply the &quot;hierarchical approach&quot;, a management principle of reorganizing
companies, as well as well known moderation principles to XP project
organization. We show similarities between software engineering methods and
company reorganization processes and discuss how the elements of the
hierarchical approach can improve XP. We provide guidelines on how to scale up
XP to very large projects e.g. those common in telecommunication industry and
IT technology consultancy firms by using moderation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7074</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7074</id><created>2014-09-24</created><authors><author><keyname>Fisher</keyname><forenames>Charles K.</forenames></author></authors><title>Variational Pseudolikelihood for Regularized Ising Inference</title><categories>cond-mat.stat-mech cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I propose a variational approach to maximum pseudolikelihood inference of the
Ising model. The variational algorithm is more computationally efficient, and
does a better job predicting out-of-sample correlations than $L_2$ regularized
maximum pseudolikelihood inference as well as mean field and isolated spin pair
approximations with pseudocount regularization. The key to the approach is a
variational energy that regularizes the inference problem by shrinking the
couplings towards zero, while still allowing some large couplings to explain
strong correlations. The utility of the variational pseudolikelihood approach
is illustrated by training an Ising model to represent the letters A-J using
samples of letters from different computer fonts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7085</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7085</id><created>2014-09-24</created><authors><author><keyname>Baker</keyname><forenames>Kathryn</forenames></author><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Callison-Burch</keyname><forenames>Chris</forenames></author><author><keyname>Dorr</keyname><forenames>Bonnie J.</forenames></author><author><keyname>Filardo</keyname><forenames>Nathaniel W.</forenames></author><author><keyname>Levin</keyname><forenames>Lori</forenames></author><author><keyname>Miller</keyname><forenames>Scott</forenames></author><author><keyname>Piatko</keyname><forenames>Christine</forenames></author></authors><title>Semantically-Informed Syntactic Machine Translation: A Tree-Grafting
  Approach</title><categories>cs.CL cs.LG stat.ML</categories><comments>10 pages, 7 figures, 3 tables; appeared in Proceedings of the Ninth
  Conference of the Association for Machine Translation in the Americas (AMTA),
  October 2010</comments><acm-class>I.2.7; I.2.6; I.5.1; I.5.4</acm-class><journal-ref>In Proceedings of the Ninth Conference of the Association for
  Machine Translation in the Americas (AMTA), Denver, Colorado, October 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a unified and coherent syntactic framework for supporting a
semantically-informed syntactic approach to statistical machine translation.
Semantically enriched syntactic tags assigned to the target-language training
texts improved translation quality. The resulting system significantly
outperformed a linguistically naive baseline model (Hiero), and reached the
highest scores yet reported on the NIST 2009 Urdu-English translation task.
This finding supports the hypothesis (posed by many researchers in the MT
community, e.g., in DARPA GALE) that both syntactic and semantic information
are critical for improving translation quality---and further demonstrates that
large gains can be achieved for low-resource languages with different word
order than English.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7092</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7092</id><created>2014-09-24</created><updated>2014-10-10</updated><authors><author><keyname>Dong</keyname><forenames>Mo</forenames></author><author><keyname>Li</keyname><forenames>Qingxi</forenames></author><author><keyname>Zarchy</keyname><forenames>Doron</forenames></author><author><keyname>Godfrey</keyname><forenames>Brighten</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author></authors><title>PCC: Re-architecting Congestion Control for Consistent High Performance</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TCP and its variants have suffered from surprisingly poor performance for
decades. We argue the TCP family has little hope to achieve consistent high
performance due to a fundamental architectural deficiency: hardwiring
packet-level events to control responses without understanding the real
performance result of its actions. We propose Performance-oriented Congestion
Control (PCC), a new congestion control architecture in which each sender
continuously observes the connection between its actions and empirically
experienced performance, enabling it to consistently adopt actions that result
in high performance. We prove that PCC converges to a stable and fair
equilibrium. Across many real-world and challenging environments, PCC shows
consistent and often 10x performance improvement, with better fairness and
stability than TCP. PCC requires no router hardware support or new packet
format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7109</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7109</id><created>2014-09-24</created><authors><author><keyname>Chakkor</keyname><forenames>Saad</forenames></author><author><keyname>Cheikh</keyname><forenames>El Ahmadi</forenames></author><author><keyname>Baghouri</keyname><forenames>Mostafa</forenames></author><author><keyname>Hajraoui</keyname><forenames>Abderrahmane</forenames></author></authors><title>Efficiency Evaluation Metrics for Wireless Intelligent Sensors
  Applications</title><categories>cs.NI</categories><comments>10 pages, 14 figures, 7 tables, journal research paper. arXiv admin
  note: substantial text overlap with arXiv:1409.6884</comments><journal-ref>International Journal of Intelligent Systems and Application,
  Volume 6 No 10, October 2014, Mecs Press Publisher</journal-ref><doi>10.5815/ijisa.2014.10.01</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The metrology field has been progressed with the appearance of the wireless
intelligent sensor systems providing more capabilities such as signal
processing, remote multi-sensing fusion etc. This kind of devices is rapidly
making their way into medical and industrial monitoring, collision avoidance,
traffic control, automotive and others applications. However, numerous design
challenges for wireless intelligent sensors systems are imposed to overcome the
physical limitations in data traffic, such as system noise, real time
communication, signal attenuation, response dynamics, power consumption, and
effective conversion rates etc, especially for applications requiring specific
performances. This paper analyzes the performance metrics of the mentioned
sensing devices systems which stands for superior measurement, more accuracy
and reliability. Study findings prescribe researchers, developers/ engineers
and users to realizing an optimal sensing motes design strategy that offers
operational advantages which can offer cost-effective solutions for an
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7121</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7121</id><created>2014-09-22</created><authors><author><keyname>Basarke</keyname><forenames>Christian</forenames></author><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Software &amp; Systems Engineering Process and Tools for the Development of
  Autonomous Driving Intelligence</title><categories>cs.SE</categories><comments>28 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:1409.6579</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a large number of people with heterogeneous knowledge and skills run a
project together, it is important to use a sensible engineering process. This
especially holds for a project building an intelligent autonomously driving car
to participate in the 2007 DARPA Urban Challenge. In this article, we present
essential elements of a software and system engineering process for the
development of artificial intelligence capable of driving autonomously in
complex urban situations. The process includes agile concepts, like test first
approach, continuous integration of every software module and a reliable
release and configuration management assisted by software tools in integrated
development environments. However, the most important ingredients for an
efficient and stringent development are the ability to efficiently test the
behavior of the developed system in a flexible and modular simulator for urban
situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7122</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7122</id><created>2014-09-24</created><updated>2015-06-18</updated><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Lu</keyname><forenames>Xuanxuan</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Joint Transceiver Design for Wireless Sensor Networks through Block
  Coordinate Descent Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the joint transceiver design in a wireless sensor
network where multiple sensors observe the same physical event and transmit
their contaminated observations to a fusion center, with all nodes equipped
with multiple antennae and linear filters. Under the mean square error (MSE)
criterion, the joint beamforming design problem can be formulated as a
nonconvex optimization problem. To attack this problem, various block
coordinate descent (BCD) algorithms are proposed with convergence being
carefully examined. First we propose a two block coordinate descent (2-BCD)
algorithm that iteratively designs all the beamformers and the linear receiver,
where both subproblems are convex and the convergence of limit points to
stationary points is guaranteed. Besides, the thorough solution to optimizing
one single beamformer is given, which, although discussed several times, is
usually incomplete in existing literature. Based on that, multiple block
coordinate descent algorithms are proposed. Solving the joint beamformers'
design by cyclically updating each separate beamformer under the 2-BCD
framework gives birth to a layered BCD algorithm, which guarantees convergence
to stationary points. Besides that, a wide class of multiple BCD algorithms
using the general essentially cyclic updating rule has been studied. As will be
seen, by appropriately adjusting the update of single beamformer, fast
converging, highly efficient and stationary point achieving algorithms can be
obtained. Extensive numerical results are presented to verify our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7136</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7136</id><created>2014-09-24</created><authors><author><keyname>Das</keyname><forenames>Jayanta Kumar</forenames></author><author><keyname>Rout</keyname><forenames>Ranjeet Kumar</forenames></author><author><keyname>Choudhury</keyname><forenames>Pabitra Pal</forenames></author></authors><title>Analysis of Boolean Functions based on Interaction Graphs and their
  influence in System Biology</title><categories>cs.SY</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interaction graphs provide an important qualitative modeling approach for
System Biology. This paper presents a novel approach for construction of
interaction graph with the help of Boolean function decomposition. Each
decomposition part (Consisting of 2-bits) of the Boolean functions has some
important significance. In the dynamics of a biological system, each variable
or node is nothing but gene or protein. Their regulation has been explored in
terms of interaction graphs which are generated by Boolean functions. In this
paper, different classes of Boolean functions with regards to Interaction Graph
with biologically significant properties have been adumbrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7163</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7163</id><created>2014-09-25</created><authors><author><keyname>Nguyen</keyname><forenames>Khoa D.</forenames></author><author><keyname>Letzepis</keyname><forenames>Nick</forenames></author><author><keyname>Fabregas</keyname><forenames>Albert Guillen i</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author></authors><title>Causal/Predictive Imperfect Channel State Information in Block-Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>Extended version of ISIT 2010 paper. 23 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-input multi-output (MIMO) block-fading channel with a
general model for channel state information at the transmitter (CSIT). The
model covers systems with causal CSIT, where only CSIT of past fading blocks is
available, and predictive CSIT, where CSIT of some future fading blocks is
available. The optimal diversity-multiplexing tradeoff (DMT) and rate-diversity
tradeoff (RDT) of the channel are studied under long-term power constraints.
The impact of imperfect (mismatched) CSIT on the optimal DMT and RDT is also
investigated. Our results show the outage diversity gain obtained by providing
imperfect causal/predictive CSIT, leading to new insights into system design
and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7164</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7164</id><created>2014-09-25</created><authors><author><keyname>Zhu</keyname><forenames>Zhuotun</forenames></author><author><keyname>Wang</keyname><forenames>Xinggang</forenames></author><author><keyname>Bai</keyname><forenames>Song</forenames></author><author><keyname>Yao</keyname><forenames>Cong</forenames></author><author><keyname>Bai</keyname><forenames>Xiang</forenames></author></authors><title>Deep Learning Representation using Autoencoder for 3D Shape Retrieval</title><categories>cs.CV</categories><comments>6 pages, 7 figures, 2014ICSPAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of how to build a deep learning representation for 3D
shape. Deep learning has shown to be very effective in variety of visual
applications, such as image classification and object detection. However, it
has not been successfully applied to 3D shape recognition. This is because 3D
shape has complex structure in 3D space and there are limited number of 3D
shapes for feature learning. To address these problems, we project 3D shapes
into 2D space and use autoencoder for feature learning on the 2D images. High
accuracy 3D shape retrieval performance is obtained by aggregating the features
learned on 2D images. In addition, we show the proposed deep learning feature
is complementary to conventional local image descriptors. By combing the global
deep learning representation and the local descriptor representation, our
method can obtain the state-of-the-art performance on 3D shape retrieval
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7165</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7165</id><created>2014-09-25</created><authors><author><keyname>Wu</keyname><forenames>Liang</forenames></author><author><keyname>Xiong</keyname><forenames>Hui</forenames></author><author><keyname>Du</keyname><forenames>Liang</forenames></author><author><keyname>Liu</keyname><forenames>Bo</forenames></author><author><keyname>Xu</keyname><forenames>Guandong</forenames></author><author><keyname>Ge</keyname><forenames>Yong</forenames></author><author><keyname>Fu</keyname><forenames>Yanjie</forenames></author><author><keyname>Zhou</keyname><forenames>Yuanchun</forenames></author><author><keyname>Li</keyname><forenames>Jianhui</forenames></author></authors><title>Heterogeneous Metric Learning with Content-based Regularization for
  Software Artifact Retrieval</title><categories>cs.LG cs.IR cs.SE</categories><comments>to appear in IEEE International Conference on Data Mining (ICDM),
  Shen Zhen, China, December 2014</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The problem of software artifact retrieval has the goal to effectively locate
software artifacts, such as a piece of source code, in a large code repository.
This problem has been traditionally addressed through the textual query. In
other words, information retrieval techniques will be exploited based on the
textual similarity between queries and textual representation of software
artifacts, which is generated by collecting words from comments, identifiers,
and descriptions of programs. However, in addition to these semantic
information, there are rich information embedded in source codes themselves.
These source codes, if analyzed properly, can be a rich source for enhancing
the efforts of software artifact retrieval. To this end, in this paper, we
develop a feature extraction method on source codes. Specifically, this method
can capture both the inherent information in the source codes and the semantic
information hidden in the comments, descriptions, and identifiers of the source
codes. Moreover, we design a heterogeneous metric learning approach, which
allows to integrate code features and text features into the same latent
semantic space. This, in turn, can help to measure the artifact similarity by
exploiting the joint power of both code and text features. Finally, extensive
experiments on real-world data show that the proposed method can help to
improve the performances of software artifact retrieval with a significant
margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7166</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7166</id><created>2014-09-25</created><updated>2015-07-08</updated><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author><author><keyname>Yang</keyname><forenames>Lan</forenames></author><author><keyname>Wang</keyname><forenames>Jingbin</forenames></author><author><keyname>Azevedo</keyname><forenames>Lorenzo</forenames></author></authors><title>An Efficient Topology-Based Algorithm for Transient Analysis of Power
  Grid</title><categories>cs.OH cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the design flow of integrated circuits, chip-level verification is an
important step that sanity checks the performance is as expected. Power grid
verification is one of the most expensive and time-consuming steps of
chip-level verification, due to its extremely large size. Efficient power grid
analysis technology is highly demanded as it saves computing resources and
enables faster iteration. In this paper, a topology-base power grid transient
analysis algorithm is proposed. Nodal analysis is adopted to analyze the
topology which is mathematically equivalent to iteratively solving a positive
semi-definite linear equation. The convergence of the method is proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7186</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7186</id><created>2014-09-25</created><updated>2015-07-08</updated><authors><author><keyname>Bellio</keyname><forenames>Ruggero</forenames></author><author><keyname>Ceschia</keyname><forenames>Sara</forenames></author><author><keyname>Di Gaspero</keyname><forenames>Luca</forenames></author><author><keyname>Schaerf</keyname><forenames>Andrea</forenames></author><author><keyname>Urli</keyname><forenames>Tommaso</forenames></author></authors><title>Feature-based tuning of simulated annealing applied to the
  curriculum-based course timetabling problem</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the university course timetabling problem, which is one of the
most studied problems in educational timetabling. In particular, we focus our
attention on the formulation known as the curriculum-based course timetabling
problem, which has been tackled by many researchers and for which there are
many available benchmarks.
  The contribution of this paper is twofold. First, we propose an effective and
robust single-stage simulated annealing method for solving the problem.
Secondly, we design and apply an extensive and statistically-principled
methodology for the parameter tuning procedure. The outcome of this analysis is
a methodology for modeling the relationship between search method parameters
and instance features that allows us to set the parameters for unseen instances
on the basis of a simple inspection of the instance itself. Using this
methodology, our algorithm, despite its apparent simplicity, has been able to
achieve high quality results on a set of popular benchmarks.
  A final contribution of the paper is a novel set of real-world instances,
which could be used as a benchmark for future comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7195</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7195</id><created>2014-09-25</created><updated>2016-01-28</updated><authors><author><keyname>Bodas</keyname><forenames>Tejas</forenames></author><author><keyname>Ganesh</keyname><forenames>A.</forenames></author><author><keyname>Manjunath</keyname><forenames>D.</forenames></author></authors><title>Tolls and Welfare Optimization for Multiclass Traffic in Multiqueue
  Systems</title><categories>cs.GT cs.PF</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider a queueing system with multiple heterogeneous servers serving a
multiclass population. The classes are distinguished by the time costs. All
customers have i.i.d. service requirements. Arriving customers do not see the
instantaneous queue occupancy. Arrivals are randomly routed to one of the
servers and the routing probabilities are determined centrally to optimize the
expected waiting cost. This is, in general, a difficult optimization problem
and we obtain the structure of the routing matrix. Next we consider a system in
which each queue charges an admission price. The arrivals are routed randomly
to minimize an individual objective function that includes the expected waiting
cost and the admission price. Once again, we obtain the structure of the
equilibrium routing matrix for this case. Finally, we determine the admission
prices to make the equilibrium routing probability matrix equal to a given
optimal routing probability matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7202</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7202</id><created>2014-09-25</created><updated>2014-11-23</updated><authors><author><keyname>Naghibi</keyname><forenames>Tofigh</forenames></author><author><keyname>Pfister</keyname><forenames>Beat</forenames></author></authors><title>A Boosting Framework on Grounds of Online Learning</title><categories>cs.LG</categories><comments>Accepted in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By exploiting the duality between boosting and online learning, we present a
boosting framework which proves to be extremely powerful thanks to employing
the vast knowledge available in the online learning area. Using this framework,
we develop various algorithms to address multiple practically and theoretically
interesting questions including sparse boosting, smooth-distribution boosting,
agnostic learning and some generalization to double-projection online learning
algorithms, as a by-product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7217</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7217</id><created>2014-09-25</created><updated>2014-10-14</updated><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author></authors><title>A note on the longest common substring with $k$-mismatches problem</title><categories>cs.DS</categories><msc-class>68W32</msc-class><acm-class>F.2.2; H.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently introduced longest common substring with $k$-mismatches
($k$-LCF) problem is to find, given two sequences $S_1$ and $S_2$ of length $n$
each, a longest substring $A_1$ of $S_1$ and $A_2$ of $S_2$ such that the
Hamming distance between $A_1$ and $A_2$ is at most $k$. So far, the only
subquadratic time result for this problem was known for $k =
1$~\cite{FGKU2014}. We first present two output-dependent algorithms solving
the $k$-LCF problem and show that for $k = O(\log^{1-\varepsilon} n)$, where
$\varepsilon &gt; 0$, at least one of them works in subquadratic time, using
$O(n)$ words of space. The choice of one of these two algorithms to be applied
for a given input can be done after linear time and space preprocessing.
Finally we present a tabulation-based algorithm working, in its range of
applicability, in $O(n^2\log\min(k+\ell_0, \sigma)/\log n)$ time, where
$\ell_0$ is the length of the standard longest common substring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7222</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7222</id><created>2014-09-25</created><updated>2015-01-31</updated><authors><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Moss</keyname><forenames>Peter</forenames></author></authors><title>Hybrid Channel Pre-Inversion and Interference Alignment Strategies</title><categories>cs.IT math.IT</categories><comments>Submitted to ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider strategies for MIMO interference channels which
combine the notions of interference alignment and channel pre-inversion. Users
collaborate to form data-sharing groups, enabling them to clear interference
within a group, while interference alignment is employed to clear interference
between groups. To improve the capacity of our schemes at finite SNR, we
propose that the groups of users invert their subchannel using a regularized
Tikhonov inverse. We provide a new sleeker derivation of the optimal Tikhonov
parameter, and use random matrix theory to provide an explicit formula for the
SINR as the size of the system increases, which we believe is a new result. For
every possible grouping of K = 4 users each with N = 5 antennas, we completely
classify the degrees of freedom available to each user when using such hybrid
schemes, and construct explicit interference alignment strategies which
maximize the sum DoF. Lastly, we provide simulation results which compute the
ergodic capacity of such schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7228</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7228</id><created>2014-09-25</created><authors><author><keyname>Falcunit</keyname><forenames>Dixie F.</forenames><suffix>Jr.</suffix></author><author><keyname>Sison</keyname><forenames>Virgilio P.</forenames></author></authors><title>Cyclic Codes over the Matrix Ring $M_2(\FFF_p)$ and Their Isometric
  Images over $\FFF_{p^2}+u\FFF_{p^2}$</title><categories>cs.IT math.IT</categories><comments>14 pages, presented at the International Zurich Seminar on
  Communications, Zurich, Switzerland, February 2014</comments><msc-class>94B05, 94B65, 94B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\FF_p$ be the prime field with $p$ elements. We derive the homogeneous
weight on the Frobenius matrix ring $M_2(\FF_p)$ in terms of the generating
character. We also give a generalization of the Lee weight on the finite chain
ring $\FF_{p^2}+u\FF_{p^2}$ where $u^2=0$. A non-commutative ring, denoted by
$\mathcal{F}_{p^2}+\mathbf{v}_p \mathcal{F}_{p^2}$, $\mathbf{v}_p$ an
involution in $M_2(\FF_p)$, that is isomorphic to $M_2(\FF_p)$ and is a left
$\FF_{p^2}$- vector space, is constructed through a unital embedding $\tau$
from $\FF_{p^2}$ to $M_2(\FF_p)$. The elements of $\mathcal{F}_{p^2}$ come from
$M_2(\FF_p)$ such that $\tau(\FF_{p^2})=\mathcal{F}_{p^2}$. The irreducible
polynomial $f(x)=x^2+x+(p-1) \in \FF_p[x]$ required in $\tau$ restricts our
study of cyclic codes over $M_2(\FF_p)$ endowed with the Bachoc weight to the
case $p\equiv$ $2$ or $3$ mod $5$. The images of these codes via a left
$\FF_p$-module isometry are additive cyclic codes over $\FF_{p^2}+u\FF_{p^2}$
endowed with the Lee weight. New examples of such codes are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7231</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7231</id><created>2014-09-25</created><authors><author><keyname>Breu</keyname><forenames>Ruth</forenames></author><author><keyname>Grosu</keyname><forenames>Radu</forenames></author><author><keyname>Hofmann</keyname><forenames>Christoph</forenames></author><author><keyname>Huber</keyname><forenames>Franz</forenames></author><author><keyname>Kr&#xfc;ger</keyname><forenames>Ingolf</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schmidt</keyname><forenames>Monika</forenames></author><author><keyname>Schwerin</keyname><forenames>Wolfgang</forenames></author></authors><title>Exemplary and Complete Object Interaction Descriptions</title><categories>cs.SE</categories><comments>11 pages, 4 figures</comments><report-no>TUM-I9737</report-no><journal-ref>Proceedings OOPSLA'97 Workshop on Object-oriented Behavioral
  Semantics, Haim Kilov, Bernhard Rumpe, Ian Simmonds (eds.), TUM-I9737. TU
  Munich, 1997</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a variant of message sequence diagrams called EETs
Extended Event Traces We provide the graphical notation discuss the
methodological use of EETs to describe behavior of object oriented business
information systems and sketch their semantics Special emphasis is put on the
different implications of using EETs for exemplary and complete interaction
descriptions. The possibility to describe interactions between single objects
as well as composite objects with EETs makes them particularly suitable to
describe the behavior of large systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7232</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7232</id><created>2014-09-25</created><authors><author><keyname>Klein</keyname><forenames>Cornel</forenames></author><author><keyname>Prehofer</keyname><forenames>Christian</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Feature Specification and Refinement with State Transition Diagrams</title><categories>cs.SE</categories><comments>15 pages, 7 figures</comments><journal-ref>Fourth IEEE Workshop on Feature Interactions in Telecommunications
  Networks and Distributed Systems, P. Dini, IOS-Press. 1997</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a graphic specification technique, called state
transition diagrams (STD), and show the application to the feature interaction
problem. Using a stream-based formal semantics, we provide refinement rules for
STDs. Refinements define an implementation relation on STD specifications. We
view features as particular refinements which add previously unspecified
behavior to a given STD specification. The refinement relation is then used to
add features, and to define the notion of conflicting features. Our techniques
are demonstrated by a systematic development of an example given in [25].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7233</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7233</id><created>2014-09-25</created><authors><author><keyname>Paech</keyname><forenames>Barbara</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>State Based Service Description</title><categories>cs.SE</categories><comments>10 pages, 3 figures</comments><journal-ref>FMOODS'97: Formal Methods for Open Object-based Distributed
  Systems. Year: 1997</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose I/O state transition diagrams for service
description In contrast to other techniques like for example Statecharts we
allow to model non atomic services by sequences of transitions This is
especially important in a distributed system where concurrent service
invocation cannot be prohibited We give a mathematical model of object
behaviour based on concurrent and sequential messages Then we give a precise
semantics of the service descriptions in terms of the mathematical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7234</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7234</id><created>2014-09-25</created><authors><author><keyname>Breu</keyname><forenames>Ruth</forenames></author><author><keyname>Hinkel</keyname><forenames>Ursula</forenames></author><author><keyname>Hofmann</keyname><forenames>Christoph</forenames></author><author><keyname>Klein</keyname><forenames>Cornel</forenames></author><author><keyname>Paech</keyname><forenames>Barbara</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Thurner</keyname><forenames>V.</forenames></author></authors><title>Towards a Formalization of the Unified Modeling Language</title><categories>cs.SE</categories><comments>23 pages, 5 figures</comments><journal-ref>Proceedings of ECOOP'97 - Object-Oriented Programming, 11th
  European Conference, Mehmet Aksit, Satoshi Matsuoka (ed.), Jyv\&quot;askyl\&quot;a,
  Finland, June 1997, Springer Verlag, LNCS 1241</journal-ref><doi>10.1007/BFb0053386</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unified Modeling Language UML is a language for specifying visualizing
and documenting object oriented systems UML combines the concepts of OOA OODOMT
and OOSE and is intended as a standard in the domain of object oriented
analysis and design Due to the missing formal mathematical foundation of UML
the syntax and the semantics of a number of UML constructs are not precisely
defined.This paper outlines a proposal for the formal foundation of UML that is
based on a mathematical system model
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7236</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7236</id><created>2014-09-25</created><authors><author><keyname>Klein</keyname><forenames>Cornel</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Broy</keyname><forenames>Manfred</forenames></author></authors><title>A stream-based mathematical model for distributed information processing
  systems - SysLab system model</title><categories>cs.SE</categories><comments>16 pages, 2 figures</comments><journal-ref>Proceedings of the first International Workshop on Formal Methods
  for Open Object-based Distributed Systems. Chapmann &amp; Hall. 1996</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the SysLab project we develop a software engineering method based on a
mathematical foundation. The SysLab system model serves as an abstract
mathematical model for information systems and their components. It is used to
formalize the semantics of all used description techniques such as object
diagrams state automata sequence charts or data-flow diagrams. Based on the
requirements for such a reference model, we define the system model including
its different views and their relationships.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7239</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7239</id><created>2014-09-25</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Thurner</keyname><forenames>V.</forenames></author></authors><title>Refining Business Processes</title><categories>cs.SE</categories><comments>16 pages, 10 figures</comments><report-no>TUM-I9820</report-no><journal-ref>Seventh OOPSLA Workshop on Precise Behavioral Semantics (with an
  Emphasis on OO Business Specifications) Technical University Munich,
  TUM-I9820. 1998</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a calculus for re nement of business process models
based on a precisede nition of business processes and process nets Business
process models are a vital concept for communicating with experts of the
application domain Depending on the roles and responsibilities of the
application domain experts involved process models are discussed on different
levels of abstraction These may range from detailed regulations for process
execution to the interrelation of basic core processes on a strategic level To
ensure consistency and to allow for a exible integration of process information
on di erent levels of abstraction we introduce re nement rules that allow the
incremental addition to and re nement of the information in a process model
while maintaining the validity of more abstract high level processes In
particular we allow the decomposition of single processes and logical data
channels as well as the extension of the interface and channel structure to
information that is newly gained or increased in relevance during the modeling
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7240</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7240</id><created>2014-09-25</created><authors><author><keyname>&#x141;&#x105;cki</keyname><forenames>Jakub</forenames></author><author><keyname>Sankowski</keyname><forenames>Piotr</forenames></author></authors><title>Optimal decremental connectivity in planar graphs</title><categories>cs.DS</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show an algorithm for dynamic maintenance of connectivity information in
an undirected planar graph subject to edge deletions. Our algorithm may answer
connectivity queries of the form `Are vertices $u$ and $v$ connected with a
path?' in constant time. The queries can be intermixed with any sequence of
edge deletions, and the algorithm handles all updates in $O(n)$ time. This
results improves over previously known $O(n \log n)$ time algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7241</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7241</id><created>2014-09-25</created><authors><author><keyname>Philipps</keyname><forenames>Jan</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Stepwise Refinement of Data Flow Architectures</title><categories>cs.SE</categories><comments>18 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1409.6932</comments><report-no>TUM-I9746</report-no><journal-ref>Software Architectures and Design Patterns in Business
  Applications Technische Universit\&quot;at M\&quot;unchen, TUM-I9746. 1997</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at integrating heterogeneous documents used in pragmatic
software develpoment methods to describe views with a formal refinement based
software development process. Therefore we propose an integrated semantics of
heterogeneous documents based on a common system model and a set of syntactic
development steps with a welldefined semantics for document evolution. The use
of the development steps is demonstrated in a small example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7243</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7243</id><created>2014-09-25</created><authors><author><keyname>Bergner</keyname><forenames>Klaus</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Event Handling in ET++ - A Case Study in the Algebraic Specification of
  Object-Oriented Application Frameworks</title><categories>cs.SE</categories><comments>18 pages, 2 figures, Technical Report TUM-I9503, TU Munich, 1995</comments><report-no>TUM-I9503</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we investigate the suitability of algebraic specication
techniques for the modular speci cation of complex object oriented systems As
an example part of the event handling mechanism of the application framework ET
is speci ed using a variant of the algebraic specication language Spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7244</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7244</id><created>2014-09-25</created><authors><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author></authors><title>Multi-Dimensional and Non-Uniform Constellation Optimization via the
  Special Orthogonal Group</title><categories>cs.IT math.IT</categories><comments>To appear at ITW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the goal of optimizing the CM capacity of a finite constellation over a
Rayleigh fading channel, we construct for all dimensions which are a power of 2
families of rotation matrices which optimize a certain objective function
controlling the CM capacity. Our construction does not depend on any
assumptions about the constellation, dimension, or signal-to-noise ratio. We
confirm the benefits of our construction for uniform and non-uniform
constellations at a large range of SNR values through numerous simulations. We
show that in two and four dimensions one can obtain a further potential
increase in CM capacity by jointly considering non-uniform and rotated
constellations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7246</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7246</id><created>2014-09-25</created><updated>2016-01-07</updated><authors><author><keyname>Hirche</keyname><forenames>Christoph</forenames></author><author><keyname>Morgan</keyname><forenames>Ciara</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Polar codes in network quantum information theory</title><categories>quant-ph cs.IT math.IT</categories><comments>18 pages, 2 figures, v2: 10 pages, double column, version accepted
  for publication</comments><journal-ref>IEEE Transactions on Information Theory, vol. 62, no. 2, pages
  915-924, February 2016</journal-ref><doi>10.1109/TIT.2016.2514319</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar coding is a method for communication over noisy classical channels
which is provably capacity-achieving and has an efficient encoding and
decoding. Recently, this method has been generalized to the realm of quantum
information processing, for tasks such as classical communication, private
classical communication, and quantum communication. In the present work, we
apply the polar coding method to network quantum information theory, by making
use of recent advances for related classical tasks. In particular, we consider
problems such as the compound multiple access channel and the quantum
interference channel. The main result of our work is that it is possible to
achieve the best known inner bounds on the achievable rate regions for these
tasks, without requiring a so-called quantum simultaneous decoder. Thus, our
work paves the way for developing network quantum information theory further
without requiring a quantum simultaneous decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7247</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7247</id><created>2014-09-25</created><authors><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Barreal</keyname><forenames>Amaro</forenames></author></authors><title>Node Repair for Distributed Storage Systems over Fading Channels</title><categories>cs.IT math.IT</categories><comments>To appear in ISITA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage systems and associated storage codes can efficiently
store a large amount of data while ensuring that data is retrievable in case of
node failure. The study of such systems, particularly the design of storage
codes over finite fields, assumes that the physical channel through which the
nodes communicate is error-free. This is not always the case, for example, in a
wireless storage system.
  We study the probability that a subpacket is repaired incorrectly during node
repair in a distributed storage system, in which the nodes communicate over an
AWGN or Rayleigh fading channels. The asymptotic probability (as SNR increases)
that a node is repaired incorrectly is shown to be completely determined by the
repair locality of the DSS and the symbol error rate of the wireless channel.
Lastly, we propose some design criteria for physical layer coding in this
scenario, and use it to compute optimally rotated QAM constellations for use in
wireless distributed storage systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7254</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7254</id><created>2014-09-25</created><authors><author><keyname>An</keyname><forenames>Jisun</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author></authors><title>Partisan Sharing: Facebook Evidence and Societal Consequences</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Published in Proc. of WWW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hypothesis of selective exposure assumes that people seek out information
that supports their views and eschew information that conflicts with their
beliefs, and that has negative consequences on our society. Few researchers
have recently found counter evidence of selective exposure in social media:
users are exposed to politically diverse articles. No work has looked at what
happens after exposure, particularly how individuals react to such exposure,
though. Users might well be exposed to diverse articles but share only the
partisan ones. To test this, we study partisan sharing on Facebook: the
tendency for users to predominantly share like-minded news articles and avoid
conflicting ones. We verified four main hypotheses. That is, whether partisan
sharing: 1) exists at all; 2) changes across individuals (e.g., depending on
their interest in politics); 3) changes over time (e.g., around elections); and
4) changes depending on perceived importance of topics. We indeed find strong
evidence for partisan sharing. To test whether it has any consequence in the
real world, we built a web application for BBC viewers of a popular political
program, resulting in a controlled experiment involving more than 70
individuals. Based on what they share and on survey data, we find that partisan
sharing has negative consequences: distorted perception of reality. However, we
do also find positive aspects of partisan sharing: it is associated with people
who are more knowledgeable about politics and engage more with it as they are
more likely to vote in the general elections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7256</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7256</id><created>2014-09-09</created><authors><author><keyname>Xie</keyname><forenames>Yihui</forenames></author><author><keyname>Hofmann</keyname><forenames>Heike</forenames></author><author><keyname>Cheng</keyname><forenames>Xiaoyue</forenames></author></authors><title>Reactive Programming for Interactive Graphics</title><categories>cs.GR cs.HC</categories><comments>Published in at http://dx.doi.org/10.1214/14-STS477 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS477</report-no><journal-ref>Statistical Science 2014, Vol. 29, No. 2, 201-213</journal-ref><doi>10.1214/14-STS477</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the big challenges of developing interactive statistical applications
is the management of the data pipeline, which controls transformations from
data to plot. The user's interactions needs to be propagated through these
modules and reflected in the output representation at a fast pace. Each
individual module may be easy to develop and manage, but the dependency
structure can be quite challenging. The MVC (Model/View/Controller) pattern is
an attempt to solve the problem by separating the user's interaction from the
representation of the data. In this paper we discuss the paradigm of reactive
programming in the framework of the MVC architecture and show its applicability
to interactive graphics. Under this paradigm, developers benefit from the
separation of user interaction from the graphical representation, which makes
it easier for users and developers to extend interactive applications. We show
the central role of reactive data objects in an interactive graphics system,
implemented as the R package cranvas, which is freely available on GitHub and
the main developers include the authors of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7261</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7261</id><created>2014-09-25</created><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Magnus</forenames></author></authors><title>Polynomial Kernels and User Reductions for the Workflow Satisfiability
  Problem</title><categories>cs.CC cs.DS</categories><comments>An extended abstract appears in the proceedings of IPEC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Workflow Satisfiability Problem (WSP) is a problem of practical interest
that arises whenever tasks need to be performed by authorized users, subject to
constraints defined by business rules. We are required to decide whether there
exists a plan -- an assignment of tasks to authorized users -- such that all
constraints are satisfied.
  The WSP is, in fact, the conservative Constraint Satisfaction Problem (i.e.,
for each variable, here called task, we have a unary authorization constraint)
and is, thus, NP-complete. It was observed by Wang and Li (2010) that the
number k of tasks is often quite small and so can be used as a parameter, and
several subsequent works have studied the parameterized complexity of WSP
regarding parameter k.
  We take a more detailed look at the kernelization complexity of WSP(\Gamma)
when \Gamma\ denotes a finite or infinite set of allowed constraints. Our main
result is a dichotomy for the case that all constraints in \Gamma\ are regular:
(1) We are able to reduce the number n of users to n' &lt;= k. This entails a
kernelization to size poly(k) for finite \Gamma, and, under mild technical
conditions, to size poly(k+m) for infinite \Gamma, where m denotes the number
of constraints. (2) Already WSP(R) for some R \in \Gamma\ allows no polynomial
kernelization in k+m unless the polynomial hierarchy collapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7264</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7264</id><created>2014-09-25</created><authors><author><keyname>Yahya</keyname><forenames>W. A.</forenames></author><author><keyname>Oyewumi</keyname><forenames>K. J.</forenames></author><author><keyname>Sen</keyname><forenames>K. D.</forenames></author></authors><title>Quantum Information Entropies for the $\ell$-state P\&quot;oschl-Teller-type
  potential</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we obtained the position-momentum uncertainties and some
uncertainty relations for the P\&quot;oschl-Teller-type potential for any $\ell$.
The radial expectation values of $r^{-2}$, $r^{2}$ and $p^{2}$ are obtained
from which the Heisenberg Uncertainty principle holds for the potential model
under consideration. The Fisher information is then obtained and it is observed
that the Fisher-information-based uncertainty relation and the Cramer-Rao
inequality hold for this even power potential. Some numerical and graphical
results are displayed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7272</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7272</id><created>2014-09-25</created><authors><author><keyname>Stern</keyname><forenames>Ulrich</forenames></author><author><keyname>Yang</keyname><forenames>Chung-Hui</forenames></author></authors><title>Ctrax extensions for tracking in difficult lighting conditions</title><categories>q-bio.QM cs.CV</categories><journal-ref>Scientific Reports 5 (2015), 10432</journal-ref><doi>10.1038/srep10432</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fly tracking software Ctrax by Branson et al. is popular for positional
tracking of animals both within and beyond the fly community. Ctrax was not
designed to handle tracking in difficult lighting conditions with strong
shadows or recurring &quot;on&quot;/&quot;off&quot; changes in lighting - a condition that will
likely become increasingly common due to the advent of red-shifted
channelrhodopsin. We describe Ctrax extensions we developed that address this
problem. The extensions enabled good tracking accuracy in three types of
difficult lighting conditions in our lab. Our technique handling shadows relies
on &quot;single animal tracking&quot;; the other techniques should be widely applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7275</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7275</id><created>2014-09-25</created><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>The meaning-frequency law in Zipfian optimization models of
  communication</title><categories>cs.CL physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to Zipf's meaning-frequency law, words that are more frequent tend
to have more meanings. Here it is shown that a linear dependency between the
frequency of a form and its number of meanings is found in a family of models
of Zipf's law for word frequencies. This is evidence for a weak version of the
meaning-frequency law. Interestingly, that weak law (a) is not an inevitable of
property of the assumptions of the family and (b) is found at least in the
narrow regime where those models exhibit Zipf's law for word frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7277</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7277</id><created>2014-09-24</created><authors><author><keyname>Dasig</keyname><forenames>Daniel D.</forenames><suffix>Jr</suffix></author></authors><title>A Study on the Sectors of Economy serviced by Pre-industry System
  Developers among companies in Metro Manila: A Tool for Business Reengineering</title><categories>cs.CY cs.SE</categories><comments>15 pages, 5 figures, 1 table, International Journal of Business
  Information Systems Strategies (IJBISS) Volume 3, Number 3, August 2014</comments><doi>10.14810/ijbiss.2014.3301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the emergence of transformative global economy, information system has
became a necessity in businesses to obtain organizations operational
excellence, adaptation to new business models, improved decision making and
providing exceptional customer service, and eventual competitive advantage of
the enterprise setting while keeping business alliances. This paper presents
sectors of economy serviced by the pre-industry developers, explores the
evolution of computer-based information system designed and developed by
pre-industry system developers, and examine the effects of an information
system in business to countervail indentified recurring problems. Nineteen of
forty-six identified sectors of economy falls in the categories of primary,
secondary, tertiary, quarternary and quinary were the recipient of
computer-based system designed and developed. There have been several effects
of computer-based systems to organizations, including the implied relevance to
their business processes, continuum process improvement, business process
reengineering, business driver and facilitator, and customer satisfaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7281</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7281</id><created>2014-09-25</created><authors><author><keyname>Cabalar</keyname><forenames>Pedro</forenames></author><author><keyname>Fandinno</keyname><forenames>Jorge</forenames></author><author><keyname>Fink</keyname><forenames>Michael</forenames></author></authors><title>Causal Graph Justifications of Logic Programs</title><categories>cs.AI cs.LO</categories><journal-ref>Theory and Practice of Logic Programming (2014), volume 14, issue
  4-5, pp. 603-618</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a multi-valued extension of logic programs under the
stable models semantics where each true atom in a model is associated with a
set of justifications. These justifications are expressed in terms of causal
graphs formed by rule labels and edges that represent their application
ordering. For positive programs, we show that the causal justifications
obtained for a given atom have a direct correspon- dence to (relevant)
syntactic proofs of that atom using the program rules involved in the graphs.
The most interesting contribution is that this causal information is obtained
in a purely semantic way, by algebraic op- erations (product, sum and
application) on a lattice of causal values whose ordering relation expresses
when a justification is stronger than another. Finally, for programs with
negation, we define the concept of causal stable model by introducing an
analogous transformation to Gelfond and Lifschitz's program reduct. As a
result, default negation behaves as &quot;absence of proof&quot; and no justification is
derived from negative liter
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7286</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7286</id><created>2014-09-25</created><updated>2015-08-19</updated><authors><author><keyname>Campello</keyname><forenames>Antonio</forenames></author><author><keyname>Vaishampayan</keyname><forenames>Vinay A.</forenames></author></authors><title>Reliability of Erasure Coded Storage Systems: A Geometric Approach</title><categories>cs.DC cs.IT math.IT</categories><comments>28 pages. 8 figures. Presented in part at IEEE International
  Conference on BigData 2013, Santa Clara, CA, Oct. 2013 and to be presented in
  part at 2014 IEEE Information Theory Workshop, Tasmania, Australia, Nov.
  2014. New analysis added May 2015. Further Update Aug. 2015</comments><acm-class>G.3; B.3.2; G.2.1; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the probability of data loss, or equivalently, the reliability
function for an erasure coded distributed data storage system under worst case
conditions. Data loss in an erasure coded system depends on probability
distributions for the disk repair duration and the disk failure duration. In
previous works, the data loss probability of such systems has been studied
under the assumption of exponentially distributed disk failure and disk repair
durations, using well-known analytic methods from the theory of Markov
processes. These methods lead to an estimate of the integral of the reliability
function.
  Here, we address the problem of directly calculating the data loss
probability for general repair and failure duration distributions. A closed
limiting form is developed for the probability of data loss and it is shown
that the probability of the event that a repair duration exceeds a failure
duration is sufficient for characterizing the data loss probability.
  For the case of constant repair duration, we develop an expression for the
conditional data loss probability given the number of failures experienced by a
each node in a given time window. We do so by developing a geometric approach
that relies on the computation of volumes of a family of polytopes that are
related to the code. An exact calculation is provided and an upper bound on the
data loss probability is obtained by posing the problem as a set avoidance
problem. Theoretical calculations are compared to simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7288</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7288</id><created>2014-09-25</created><updated>2014-09-26</updated><authors><author><keyname>Brunetti</keyname><forenames>Ilaria</forenames></author><author><keyname>El-Azouzi</keyname><forenames>Rachid</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Altruism in groups: an evolutionary games approach</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit in this paper the relation between evolution of species and the
mathematical tool of evolutionary games, which has been used to model and
predict it. We indicate known shortcoming of this model that restricts the
capacity of evolutionary games to model groups of individuals that share a
common gene or a common fitness function. In this paper we provide a new
concept to remedy this shortcoming in the standard evolutionary games in order
to cover this kind of behavior. Further, we explore the relationship between
this new concept and Nash equilibrium or ESS. We indicate through the study of
some example in the biology as Hawk and Dove game, Stag Hunt Game and Prisoner
Dilemma, that when taking into account a utility that is common to a group of
individuals, the equilibrium structure may change dramatically. We also study
the multiple access control in slotted Aloha based wireless networks. We
analyze the impact of the altruism behavior on the performance at the
equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7289</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7289</id><created>2014-09-25</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Pham</keyname><forenames>Ducson</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Stream quantiles via maximal entropy histograms</title><categories>cs.DS</categories><comments>appears in International Conference on Neural Information Processing,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of estimating the running quantile of a data stream
when the memory for storing observations is limited. We (i) highlight the
limitations of approaches previously described in the literature which make
them unsuitable for non-stationary streams, (ii) describe a novel principle for
the utilization of the available storage space, and (iii) introduce two novel
algorithms which exploit the proposed principle. Experiments on three large
real-world data sets demonstrate that the proposed methods vastly outperform
the existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7291</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7291</id><created>2014-09-25</created><updated>2015-03-31</updated><authors><author><keyname>Zhang</keyname><forenames>Changwang</forenames></author><author><keyname>Zhou</keyname><forenames>Shi</forenames></author><author><keyname>Miller</keyname><forenames>Joel C.</forenames></author><author><keyname>Cox</keyname><forenames>Ingemar J.</forenames></author><author><keyname>Chain</keyname><forenames>Benjamin M.</forenames></author></authors><title>Optimizing Hybrid Spreading in Metapopulations</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><journal-ref>Scientific Reports. 2015 Apr 29;5:9924</journal-ref><doi>10.1038/srep09924</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemic spreading phenomena are ubiquitous in nature and society. Examples
include the spreading of diseases, information, and computer viruses. Epidemics
can spread by local spreading, where infected nodes can only infect a limited
set of direct target nodes and global spreading, where an infected node can
infect every other node. In reality, many epidemics spread using a hybrid
mixture of both types of spreading. In this study we develop a theoretical
framework for studying hybrid epidemics, and examine the optimum balance
between spreading mechanisms in terms of achieving the maximum outbreak size.
We show the existence of critically hybrid epidemics where neither spreading
mechanism alone can cause a noticeable spread but a combination of the two
spreading mechanisms would produce an enormous outbreak. Our results provide
new strategies for maximising beneficial epidemics and estimating the worst
outcome of damaging hybrid epidemics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7307</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7307</id><created>2014-09-25</created><authors><author><keyname>Gan</keyname><forenames>Yufei</forenames></author><author><keyname>Zhuo</keyname><forenames>Tong</forenames></author><author><keyname>He</keyname><forenames>Chu</forenames></author></authors><title>Image Classification with A Deep Network Model based on Compressive
  Sensing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To simplify the parameter of the deep learning network, a cascaded
compressive sensing model &quot;CSNet&quot; is implemented for image classification.
Firstly, we use cascaded compressive sensing network to learn feature from the
data. Secondly, CSNet generates the feature by binary hashing and block-wise
histograms. Finally, a linear SVM classifier is used to classify these
features. The experiments on the MNIST dataset indicate that higher
classification accuracy can be obtained by this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7311</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7311</id><created>2014-09-25</created><updated>2014-09-30</updated><authors><author><keyname>van Leeuwen</keyname><forenames>Matthijs</forenames></author><author><keyname>Ukkonen</keyname><forenames>Antti</forenames></author></authors><title>Estimating the pattern frequency spectrum inside the browser</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a browser application for estimating the number of frequent
patterns, in particular itemsets, as well as the pattern frequency spectrum.
The pattern frequency spectrum is defined as the function that shows for every
value of the frequency threshold $\sigma$ the number of patterns that are
frequent in a given dataset. Our demo implements a recent algorithm proposed by
the authors for finding the spectrum. The demo is 100% JavaScript, and runs in
all modern browsers. We observe that modern JavaScript engines can deliver
performance that makes it viable to run non-trivial data analysis algorithms in
browser applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7313</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7313</id><created>2014-09-25</created><authors><author><keyname>Gan</keyname><forenames>Yufei</forenames></author><author><keyname>Yang</keyname><forenames>Teng</forenames></author><author><keyname>He</keyname><forenames>Chu</forenames></author></authors><title>A Deep Graph Embedding Network Model for Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new deep learning network &quot;GENet&quot;, it combines
the multi-layer network architec- ture and graph embedding framework. Firstly,
we use simplest unsupervised learning PCA/LDA as first layer to generate the
low- level feature. Secondly, many cascaded dimensionality reduction layers
based on graph embedding framework are applied to GENet. Finally, a linear SVM
classifier is used to classify dimension-reduced features. The experiments
indicate that higher classification accuracy can be obtained by this algorithm
on the CMU-PIE, ORL, Extended Yale B dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7316</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7316</id><created>2014-09-25</created><authors><author><keyname>Baydin</keyname><forenames>Atilim Gunes</forenames></author><author><keyname>Pearlmutter</keyname><forenames>Barak A.</forenames></author></authors><title>An Analysis of Publication Venues for Automatic Differentiation Research</title><categories>cs.DL cs.MS</categories><comments>6 pages, 3 figures</comments><msc-class>00A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the results of our analysis of publication venues for papers on
automatic differentiation (AD), covering academic journals and conference
proceedings. Our data are collected from the AD publications database
maintained by the autodiff.org community website. The database is purpose-built
for the AD field and is expanding via submissions by AD researchers. Therefore,
it provides a relatively noise-free list of publications relating to the field.
However, it does include noise in the form of variant spellings of journal and
conference names. We handle this by manually correcting and merging these
variants under the official names of corresponding venues. We also share the
raw data we get after these corrections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7324</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7324</id><created>2014-09-25</created><authors><author><keyname>Baudry</keyname><forenames>Benoit</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>The Multiple Facets of Software Diversity: Recent Developments in Year
  2000 and Beyond</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early experiments with software diversity in the mid 1970's investigated
N-version programming and recovery blocks to increase the reliability of
embedded systems. Four decades later, the literature about software diversity
has expanded in multiple directions: goals (fault-tolerance, security, software
engineering); means (managed or automated diversity) and analytical studies
(quantification of diversity and its impact). Our paper contributes to the
field of software diversity as the first paper that adopts an inclusive vision
of the area, with an emphasis on the most recent advances in the field. This
survey includes classical work about design and data diversity for fault
tolerance, as well as the cybersecurity literature that investigates
randomization at different system levels. It broadens this standard scope of
diversity, to include the study and exploitation of natural diversity and the
management of diverse software products. Our survey includes the most recent
works, with an emphasis from 2000 to present. The targeted audience is
researchers and practitioners in one of the surveyed fields, who miss the big
picture of software diversity. Assembling the multiple facets of this
fascinating topic sheds a new light on the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7334</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7334</id><created>2014-09-25</created><authors><author><keyname>Ghorbanzadeh</keyname><forenames>Mo</forenames></author><author><keyname>Visotsky</keyname><forenames>Eugene</forenames></author><author><keyname>Yang</keyname><forenames>Weidong</forenames></author><author><keyname>Moorut</keyname><forenames>Prakash</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>Radar in-Band Interference Effects on Macrocell LTE Uplink Deployments
  in the U.S. 3.5 GHz Band</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  National Telecommunications and Information Administration (NTIA) has
proposed vast exclusions zones between radar and Worldwide Interoperability for
Microwave Access (WiMAX) systems which are also being considered as geographic
separations between radars and 3.5 GHz Long Term Evolution (LTE) systems
without investigating any changes induced by the distinct nature of LTE as
opposed to WiMAX. This paper performs a detailed system-level analysis of the
interference effects from shipborne radar systems into LTE systems. Even though
the results reveal impacts of radar interference on LTE systems performance,
they provide clear indications of conspicuously narrower exclusion zones for
LTE vis-\`a-vis those of WiMAX and pave the way toward deploying LTE at 3.5 GHz
within the coastline populous areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7336</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7336</id><created>2014-09-23</created><updated>2015-10-16</updated><authors><author><keyname>C&#xe1;rdenas</keyname><forenames>Juan Pablo</forenames></author><author><keyname>Gonz&#xe1;lez</keyname><forenames>Iv&#xe1;n</forenames></author><author><keyname>Vidal</keyname><forenames>Gerardo</forenames></author><author><keyname>Fuentes</keyname><forenames>Miguel</forenames></author></authors><title>Does network complexity help organize Babel's library?</title><categories>physics.soc-ph cs.CL nlin.AO physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study properties of texts from the perspective of complex
network theory. Words in given texts are linked by co-occurrence and
transformed into networks, and we observe that these display topological
properties common to other complex systems. However, there are some properties
that seem to be exclusive to texts; many of these properties depend on the
frequency of words in the text, while others seem to be strictly determined by
the grammar. Precisely, these properties allow for a categorization of texts as
either with a sense and others encoded or senseless.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7352</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7352</id><created>2014-09-09</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Cao</keyname><forenames>Zhenfu</forenames></author></authors><title>On Shor's Factoring Algorithm with More Registers and the Problem to
  Certify Quantum Computers</title><categories>cs.DS</categories><comments>12 pages. The extended abstract of this paper appeared in Proceeding
  of 2nd International Symposium on Information Science and Engineering, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shor's factoring algorithm uses two quantum registers. By introducing more
registers we show that the measured numbers in these registers which are of the
same pre-measurement state, should be equal if the original Shor's complexity
argument is sound. This contradicts the argument that the second register has
$r$ possible measured values. There is an anonymous comment which argues that
the states in these registers are entangled. If so, the entanglement involving
many quantum registers can not be interpreted by the mechanism of EPR pairs and
the like. In view of this peculiar entanglement has not yet been mentioned and
investigated, we think the claim that the Shor's algorithm runs in polynomial
time needs more physical verifications. We also discuss the problem to certify
quantum computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7367</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7367</id><created>2014-09-02</created><authors><author><keyname>Callegari</keyname><forenames>Sergio</forenames></author></authors><title>Coding of Stereo Signals by a Single Digital {\Delta}{\Sigma} Modulator</title><categories>cs.IT math.IT</categories><comments>4 pages, 5 figures. Pre-print from conference proceedings</comments><journal-ref>IEEE 20th International Conference on Electronics, Circuits, and
  Systems (ICECS 2013), pp. 589-592, Dec. 2013</journal-ref><doi>10.1109/ICECS.2013.6815483</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possibility of using a single digital {\Delta}{\Sigma} modulator to
simultaneously encode the two channels of a stereo signal is illustrated. From
the modulated stream, the two channels can be recovered with minimal processing
and no cross-talk. Notably, demultiplexing does not affect the sample-depth so
that, after it, one still has a data stream suitable for directly driving a
power bridge and convertible into analog by mere low-pass filtering.
Furthermore, the approach is very flexible and if one channel is unused, it
lets the other get improved dynamic range and SNR. The approach can take
advantage of recent techniques for the design of {\Delta}{\Sigma} modulators,
including methods for psychoacoustically optimal distribution of quantization
noise. Code is available to replicate the proposed examples and as a general
computer aided design tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7368</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7368</id><created>2014-09-25</created><updated>2015-09-21</updated><authors><author><keyname>Kulathumani</keyname><forenames>Vinod</forenames></author><author><keyname>Arora</keyname><forenames>Anish</forenames></author><author><keyname>Parker</keyname><forenames>Kenneth</forenames></author><author><keyname>Sridharan</keyname><forenames>Mukundan</forenames></author><author><keyname>Nakagawa</keyname><forenames>Masahiro</forenames></author></authors><title>Census: Fast, scalable and robust data aggregation in MANETs</title><categories>cs.NI</categories><comments>25 pages, technical report, index terms:random walk, MANET,
  statistical aggregation, gossip, local gradients</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes Census, a protocol for data aggregation and statistical
counting in MANETs. Census operates by circulating a set of tokens in the
network using biased random walks such that each node is visited by at least
one token. The protocol is structure-free so as to avoid high messaging
overhead for maintaining structure in the presence of node mobility. It biases
the random walks of tokens so as to achieve fast cover time; the bias involves
short albeit multi-hop gradients that guide the tokens towards hitherto
unvisited nodes. Census thus achieves a cover time of O(N/k) and message
overhead of O(Nlog(N)/k) where N is the number of nodes and k the number of
tokens in the network. Notably, it enjoys scalability and robustness, which we
demonstrate via simulations in networks ranging from 100 to 4000 nodes under
diff?erent network densities and mobility models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7370</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7370</id><created>2014-09-25</created><updated>2015-09-09</updated><authors><author><keyname>Kulathumani</keyname><forenames>Vinod</forenames></author><author><keyname>Sridharan</keyname><forenames>Mukundan</forenames></author><author><keyname>Arora</keyname><forenames>Anish</forenames></author><author><keyname>Lemon</keyname><forenames>Bryan</forenames></author><author><keyname>Parker</keyname><forenames>Kenneth</forenames></author></authors><title>On the repair time scaling wall for MANETs</title><categories>cs.NI</categories><comments>10 pages; Index terms: MANET, path failure, repair time, network
  capacity, link estimation, local routing, neighborhood discovery</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inability of practical MANET deployments to scale beyond about 100 nodes
has traditionally been blamed on insufficient network capacity for supporting
routing related control traffic. However, this paper points out that network
capacity is significantly under-utilized by standard MANET routing algorithms
at observed scaling limits. Therefore, as opposed to identifying the scaling
limit for MANET routing from a capacity stand-point, it is instead
characterized as a function of the interaction between dynamics of path failure
(caused due to mobility) and path repair. This leads to the discovery of the
repair time scaling wall, which is used to explain observed scaling limits in
MANETs. The factors behind the repair time scaling wall are identified and
techniques to extend the scaling limits are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7384</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7384</id><created>2014-09-25</created><updated>2014-11-12</updated><authors><author><keyname>Naghibi</keyname><forenames>Tofigh</forenames></author><author><keyname>Hoffmann</keyname><forenames>Sarah</forenames></author><author><keyname>Pfister</keyname><forenames>Beat</forenames></author></authors><title>A Semidefinite Programming Based Search Strategy for Feature Selection
  with Mutual Information Measure</title><categories>cs.LG</categories><comments>IEEETrans On Pattern Analysis and Machine Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature subset selection, as a special case of the general subset selection
problem, has been the topic of a considerable number of studies due to the
growing importance of data-mining applications. In the feature subset selection
problem there are two main issues that need to be addressed: (i) Finding an
appropriate measure function than can be fairly fast and robustly computed for
high-dimensional data. (ii) A search strategy to optimize the measure over the
subset space in a reasonable amount of time. In this article mutual information
between features and class labels is considered to be the measure function. Two
series expansions for mutual information are proposed, and it is shown that
most heuristic criteria suggested in the literature are truncated
approximations of these expansions. It is well-known that searching the whole
subset space is an NP-hard problem. Here, instead of the conventional
sequential search algorithms, we suggest a parallel search strategy based on
semidefinite programming (SDP) that can search through the subset space in
polynomial time. By exploiting the similarities between the proposed algorithm
and an instance of the maximum-cut problem in graph theory, the approximation
ratio of this algorithm is derived and is compared with the approximation ratio
of the backward elimination method. The experiments show that it can be
misleading to judge the quality of a measure solely based on the classification
accuracy, without taking the effect of the non-optimum search strategy into
account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7386</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7386</id><created>2014-09-25</created><authors><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author></authors><title>Performance of Stanford and Minipar Parser on Biomedical Texts</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the performance of two dependency parsers, namely Stanford and
Minipar, on biomedical texts has been reported. The performance of te parsers
to assignm dependencies between two biomedical concepts that are already proved
to be connected is not satisfying. Both Stanford and Minipar, being statistical
parsers, fail to assign dependency relation between two connected concepts if
they are distant by at least one clause. Minipar's performance, in terms of
precision, recall and the F-score of the attachment score (e.g., correctly
identified head in a dependency), to parse biomedical text is also measured
taking the Stanford's as a gold standard. The results suggest that Minipar is
not suitable yet to parse biomedical texts. In addition, a qualitative
investigation reveals that the difference between working principles of the
parsers also play a vital role for Minipar's degraded performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7403</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7403</id><created>2014-09-25</created><updated>2015-06-03</updated><authors><author><keyname>Wolpert</keyname><forenames>David H.</forenames></author><author><keyname>Grochow</keyname><forenames>Joshua A.</forenames></author><author><keyname>Libby</keyname><forenames>Eric</forenames></author><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Optimal high-level descriptions of dynamical systems</title><categories>cs.IT cond-mat.stat-mech cs.AI cs.CE math.IT q-bio.PE</categories><comments>33 pages. Updated discussion and references</comments><report-no>SFI Working Paper #15-06-017</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To analyze high-dimensional systems, many fields in science and engineering
rely on high-level descriptions, sometimes called &quot;macrostates,&quot;
&quot;coarse-grainings,&quot; or &quot;effective theories&quot;. Examples of such descriptions
include the thermodynamic properties of a large collection of point particles
undergoing reversible dynamics, the variables in a macroeconomic model
describing the individuals that participate in an economy, and the summary
state of a cell composed of a large set of biochemical networks.
  Often these high-level descriptions are constructed without considering the
ultimate reason for needing them in the first place. Here, we formalize and
quantify one such purpose: the need to predict observables of interest
concerning the high-dimensional system with as high accuracy as possible, while
minimizing the computational cost of doing so. The resulting State Space
Compression (SSC) framework provides a guide for how to solve for the {optimal}
high-level description of a given dynamical system, rather than constructing it
based on human intuition alone.
  In this preliminary report, we introduce SSC, and illustrate it with several
information-theoretic quantifications of &quot;accuracy&quot;, all with different
implications for the optimal compression. We also discuss some other possible
applications of SSC beyond the goal of accurate prediction. These include SSC
as a measure of the complexity of a dynamical system, and as a way to quantify
information flow between the scales of a system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7408</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7408</id><created>2014-09-25</created><authors><author><keyname>Liu</keyname><forenames>Xishuo</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author></authors><title>LP-decodable multipermutation codes</title><categories>cs.IT math.IT</categories><comments>This work was supported by NSF and NSERC. To appear at the 2014
  Allerton Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new way of constructing and decoding
multipermutation codes. Multipermutations are permutations of a multiset that
may consist of duplicate entries. We first introduce a new class of matrices
called multipermutation matrices. We characterize the convex hull of
multipermutation matrices. Based on this characterization, we propose a new
class of codes that we term LP-decodable multipermutation codes. Then, we
derive two LP decoding algorithms. We first formulate an LP decoding problem
for memoryless channels. We then derive an LP algorithm that minimizes the
Chebyshev distance. Finally, we show a numerical example of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7410</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7410</id><created>2014-09-25</created><updated>2015-05-03</updated><authors><author><keyname>Ravanbakhsh</keyname><forenames>Siamak</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author></authors><title>Revisiting Algebra and Complexity of Inference in Graphical Models</title><categories>cs.AI cs.CC math.RA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the form and complexity of inference in graphical models
using the abstraction offered by algebraic structures. In particular, we
broadly formalize inference problems in graphical models by viewing them as a
sequence of operations based on commutative semigroups. We then study the
computational complexity of inference by organizing various problems into an
&quot;inference hierarchy&quot;. When the underlying structure of an inference problem is
a commutative semiring -- i.e. a combination of two commutative semigroups with
the distributive law -- a message passing procedure called belief propagation
can leverage this distributive law to perform polynomial-time inference for
certain problems. After establishing the NP-hardness of inference in any
commutative semiring, we investigate the relation between algebraic properties
in this setting and further show that polynomial-time inference using
distributive law does not (trivially) extend to inference problems that are
expressed using more than two commutative semigroups. We then extend the
algebraic treatment of message passing procedures to survey propagation,
providing a novel perspective using a combination of two commutative semirings.
This formulation generalizes the application of survey propagation to new
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7411</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7411</id><created>2014-09-25</created><authors><author><keyname>Hedges</keyname><forenames>Jules</forenames></author><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author><author><keyname>Winschel</keyname><forenames>Evguenia</forenames></author><author><keyname>Winschel</keyname><forenames>Viktor</forenames></author><author><keyname>Zahn</keyname><forenames>Philipp</forenames></author></authors><title>A Higher-order Framework for Decision Problems and Games</title><categories>cs.LO cs.GT math.LO</categories><comments>45 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new unified framework for modelling both decision problems and
finite games based on quantifiers and selection functions. We show that the
canonical utility maximisation is one special case of a quantifier and that our
more abstract framework provides several additional degrees of freedom in
modelling. In particular, incomplete preferences, non-maximising heuristics,
and context-dependent motives can be taken into account when describing an
agent's goal. We introduce a suitable generalisation of Nash equilibrium for
games in terms of quantifiers and selection functions. Moreover, we introduce a
refinement of Nash that captures context-dependency of goals. Modelling in our
framework is compositional as the parts of the game are modular and can be
easily exchanged. We provide an extended example where we illustrate concepts
and highlight the benefits of our alternative modelling approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7418</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7418</id><created>2014-09-25</created><authors><author><keyname>Bardhan</keyname><forenames>Jaydeep P.</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author></authors><title>Modeling Charge-Sign Asymmetric Solvation Free Energies With Nonlinear
  Boundary Conditions</title><categories>physics.chem-ph cs.CE physics.comp-ph</categories><comments>7 pages, 2 figures, accepted to Journal of Chemical Physics</comments><doi>10.1063/1.4897324</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that charge-sign-dependent asymmetric hydration can be modeled
accurately using linear Poisson theory but replacing the standard
electric-displacement boundary condition with a simple nonlinear boundary
condition. Using a single multiplicative scaling factor to determine atomic
radii from molecular dynamics Lennard-Jones parameters, the new model
accurately reproduces MD free-energy calculations of hydration asymmetries for
(i) monatomic ions, (ii) titratable amino acids in both their protonated and
unprotonated states, and (iii) the Mobley &quot;bracelet&quot; and &quot;rod&quot; test problems
[J. Phys. Chem. B, v. 112:2408, 2008]. Remarkably, the model also justifies the
use of linear response expressions for charging free energies. Our
boundary-element method implementation demonstrates the ease with which other
continuum-electrostatic solvers can be extended to include asymmetry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7425</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7425</id><created>2014-09-25</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Raichel</keyname><forenames>Banjamin</forenames></author></authors><title>Net and Prune: A Linear Time Algorithm for Euclidean Distance Problems</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a general framework for getting expected linear time constant
factor approximations (and in many cases FPTAS's) to several well known
problems in Computational Geometry, such as $k$-center clustering and farthest
nearest neighbor. The new approach is robust to variations in the input
problem, and yet it is simple, elegant and practical. In particular, many of
these well studied problems which fit easily into our framework, either
previously had no linear time approximation algorithm, or required rather
involved algorithms and analysis. A short list of the problems we consider
include farthest nearest neighbor, $k$-center clustering, smallest disk
enclosing $k$ points, $k$th largest distance, $k$th smallest $m$-nearest
neighbor distance, $k$th heaviest edge in the MST and other spanning forest
type problems, problems involving upward closed set systems, and more. Finally,
we show how to extend our framework such that the linear running time bound
holds with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7433</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7433</id><created>2014-09-25</created><authors><author><keyname>Tong</keyname><forenames>Zhen</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>Throughput Analysis for Wireless Networks with Full-Duplex Radios</title><categories>cs.IT cs.NI math.IT</categories><comments>4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the throughput for wireless network with full-duplex
radios using stochastic geometry. Full-duplex (FD) radios can exchange data
simultaneously with each other. On the other hand, the downside of FD
transmission is that it will inevitably cause extra interference to the network
compared to half-duplex (HD) transmission. In this paper, we focus on a
wireless network of nodes with both HD and FD capabilities and derive and
optimize the throughput in such a network. Our analytical result shows that if
the network is adapting an ALOHA protocol, the maximal throughput is always
achieved by scheduling all concurrently transmitting nodes to work in FD mode
instead of a mixed FD/HD mode or HD mode regardless of the network
configurations. Moreover, the throughput gain of using FD transmission over HD
transmission is analytically lower and upper bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7442</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7442</id><created>2014-09-25</created><authors><author><keyname>Boulanger</keyname><forenames>Jeremie</forenames></author><author><keyname>Said</keyname><forenames>Salem</forenames></author><author><keyname>Bihan</keyname><forenames>Nicolas Le</forenames></author><author><keyname>Manton</keyname><forenames>Jonathan</forenames></author></authors><title>Filtering from Observations on Stiefel Manifolds</title><categories>cs.OH</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of optimal filtering for partially observed
signals taking values on the rotation group. More precisely, one or more
components are considered not to be available in the measurement of the
attitude of a 3D rigid body. In such cases, the observed signal takes its
values on a Stiefel manifold. It is demonstrated how to filter the observed
signal through the anti-development built from observations. A particle filter
implementation is proposed to perform the estimation of the signal partially
observed and corrupted by noise. The sampling issue is also addressed and
interpolation methods are introduced. Illustration of the proposed technique on
synthetic data demonstrates the ability of the approach to estimate the angular
velocity of a partially observed 3D system partially observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7450</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7450</id><created>2014-09-25</created><authors><author><keyname>Qin</keyname><forenames>Jing</forenames></author><author><keyname>Guo</keyname><forenames>Weihong</forenames></author></authors><title>Two-stage Geometric Information Guided Image Reconstruction</title><categories>math.OC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressive sensing, it is challenging to reconstruct image of high
quality from very few noisy linear projections. Existing methods mostly work
well on piecewise constant images but not so well on piecewise smooth images
such as natural images, medical images that contain a lot of details. We
propose a two-stage method called GeoCS to recover images with rich geometric
information from very limited amount of noisy measurements. The method adopts
the shearlet transform that is mathematically proven to be optimal in sparsely
representing images containing anisotropic features such as edges, corners,
spikes etc. It also uses the weighted total variation (TV) sparsity with
spatially variant weights to preserve sharp edges but to reduce the staircase
effects of TV. Geometric information extracted from the results of stage I
serves as an initial prior for stage II which alternates image reconstruction
and geometric information update in a mutually beneficial way. GeoCS has been
tested on incomplete spectral Fourier samples. It is applicable to other types
of measurements as well. Experimental results on various complicated images
show that GeoCS is efficient and generates high-quality images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7458</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7458</id><created>2014-09-25</created><authors><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Venkat</keyname><forenames>Kartik</forenames></author><author><keyname>Han</keyname><forenames>Yanjun</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Beyond Maximum Likelihood: from Theory to Practice</title><categories>stat.ME cs.DS cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum likelihood is the most widely used statistical estimation technique.
Recent work by the authors introduced a general methodology for the
construction of estimators for functionals in parametric models, and
demonstrated improvements - both in theory and in practice - over the maximum
likelihood estimator (MLE), particularly in high dimensional scenarios
involving parameter dimension comparable to or larger than the number of
samples. This approach to estimation, building on results from approximation
theory, is shown to yield minimax rate-optimal estimators for a wide class of
functionals, implementable with modest computational requirements. In a
nutshell, a message of this recent work is that, for a wide class of
functionals, the performance of these essentially optimal estimators with $n$
samples is comparable to that of the MLE with $n \ln n$ samples.
  In the present paper, we highlight the applicability of the aforementioned
methodology to statistical problems beyond functional estimation, and show that
it can yield substantial gains. For example, we demonstrate that for learning
tree-structured graphical models, our approach achieves a significant reduction
of the required data size compared with the classical Chow--Liu algorithm,
which is an implementation of the MLE, to achieve the same accuracy. The key
step in improving the Chow--Liu algorithm is to replace the empirical mutual
information with the estimator for mutual information proposed by the authors.
Further, applying the same replacement approach to classical Bayesian network
classification, the resulting classifiers uniformly outperform the previous
classifiers on 26 widely used datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7461</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7461</id><created>2014-09-25</created><authors><author><keyname>&#x130;rsoy</keyname><forenames>Ozan</forenames></author><author><keyname>Alpayd&#x131;n</keyname><forenames>Ethem</forenames></author></authors><title>Autoencoder Trees</title><categories>cs.LG stat.ML</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss an autoencoder model in which the encoding and decoding functions
are implemented by decision trees. We use the soft decision tree where internal
nodes realize soft multivariate splits given by a gating function and the
overall output is the average of all leaves weighted by the gating values on
their path. The encoder tree takes the input and generates a lower dimensional
representation in the leaves and the decoder tree takes this and reconstructs
the original input. Exploiting the continuity of the trees, autoencoder trees
are trained with stochastic gradient descent. On handwritten digit and news
data, we see that the autoencoder trees yield good reconstruction error
compared to traditional autoencoder perceptrons. We also see that the
autoencoder tree captures hierarchical representations at different
granularities of the data on its different levels and the leaves capture the
localities in the input space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7465</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7465</id><created>2014-09-25</created><authors><author><keyname>El-Khatib</keyname><forenames>Rafah</forenames></author><author><keyname>Barbier</keyname><forenames>Jean</forenames></author><author><keyname>Sakata</keyname><forenames>Ayaka</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>Error correcting codes and spatial coupling</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT</categories><comments>Chapter of &quot;Statistical Physics, Optimization, Inference, and
  Message-Passing Algorithms&quot;, Eds.: F. Krzakala, F. Ricci-Tersenghi, L.
  Zdeborov\`a, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University
  Press, to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These are notes from the lecture of R\&quot;udiger Urbanke given at the autumn
school &quot;Statistical Physics, Optimization, Inference, and Message-Passing
Algorithms&quot;, that took place in Les Houches, France from Monday September 30th,
2013, till Friday October 11th, 2013. The school was organized by Florent
Krzakala from UPMC and ENS Paris, Federico Ricci-Tersenghi from La Sapienza
Roma, Lenka Zdeborov\`a from CEA Saclay and CNRS, and Riccardo Zecchina from
Politecnico Torino. The first three sections cover the basics of polar codes
and low density parity check codes. In the last three sections, we see how the
spatial coupling helps belief propagation decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7472</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7472</id><created>2014-09-26</created><authors><author><keyname>Wang</keyname><forenames>Jiannan</forenames></author><author><keyname>Li</keyname><forenames>Guoliang</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Feng</keyname><forenames>Jianhua</forenames></author></authors><title>The Expected Optimal Labeling Order Problem for Crowdsourced Joins and
  Entity Resolution</title><categories>cs.DB</categories><comments>This is a note for explaining an incorrect claim in our SIGMOD 2013
  paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the SIGMOD 2013 conference, we published a paper extending our earlier
work on crowdsourced entity resolution to improve crowdsourced join processing
by exploiting transitive relationships [Wang et al. 2013]. The VLDB 2014
conference has a paper that follows up on our previous work [Vesdapunt et al.,
2014], which points out and corrects a mistake we made in our SIGMOD paper.
Specifically, in Section 4.2 of our SIGMOD paper, we defined the &quot;Expected
Optimal Labeling Order&quot; (EOLO) problem, and proposed an algorithm for solving
it. We incorrectly claimed that our algorithm is optimal. In their paper,
Vesdapunt et al. show that the problem is actually NP-Hard, and based on that
observation, propose a new algorithm to solve it. In this note, we would like
to put the Vesdapunt et al. results in context, something we believe that their
paper does not adequately do.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7473</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7473</id><created>2014-09-26</created><authors><author><keyname>Nurdin</keyname><forenames>Hendra I.</forenames></author><author><keyname>Gough</keyname><forenames>John E.</forenames></author></authors><title>Modular Quantum Memories Using Passive Linear Optics and Coherent
  Feedback</title><categories>quant-ph cs.SY</categories><comments>24 pages, 8 figures. Submitted. Comments welcome</comments><journal-ref>Quantum Inf. Comput. 15, pp 1017-1040 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that quantum memory for qudit states encoded in a
single photon pulsed optical field has a conceptually simple modular
realization using only passive linear optics and coherent feedback. We exploit
the idea that two decaying optical cavities can be coupled in a coherent
feedback configuration to create an internal mode of the coupled system which
is isolated and decoherence-free for the purpose of qubit storage. The qubit
memory can then be switched between writing/read-out mode and storage mode
simply by varying the routing of certain freely propagating optical fields in
the network. It is then shown that the qubit memories can be interconnected
with one another to form a qudit quantum memory. We explain each of the phase
of writing, storage, and read-out for this modular quantum memory scheme. The
results point a way towards modular architectures for complex compound quantum
memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7474</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7474</id><created>2014-09-26</created><authors><author><keyname>Li</keyname><forenames>Zhongbin</forenames></author><author><keyname>Shi</keyname><forenames>Wenzhong</forenames></author><author><keyname>Wang</keyname><forenames>Qunming</forenames></author><author><keyname>Miao</keyname><forenames>Zelang</forenames></author></authors><title>Extracting man-made objects from remote sensing images via fast level
  set evolutions</title><categories>cs.CV</categories><comments>This paper includes 31 pages and 12 figures</comments><msc-class>68T10, 68T45</msc-class><acm-class>B.2.4; I.4.6; I.4.8</acm-class><journal-ref>IEEE Transactions on Geoscience and Remote Sensing, Vol.53(2),
  pp.883-899, 2015</journal-ref><doi>10.1109/TGRS.2015.2454251</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object extraction from remote sensing images has long been an intensive
research topic in the field of surveying and mapping. Most existing methods are
devoted to handling just one type of object and little attention has been paid
to improving the computational efficiency. In recent years, level set evolution
(LSE) has been shown to be very promising for object extraction in the
community of image processing and computer vision because it can handle
topological changes automatically while achieving high accuracy. However, the
application of state-of-the-art LSEs is compromised by laborious parameter
tuning and expensive computation. In this paper, we proposed two fast LSEs for
man-made object extraction from high spatial resolution remote sensing images.
The traditional mean curvature-based regularization term is replaced by a
Gaussian kernel and it is mathematically sound to do that. Thus a larger time
step can be used in the numerical scheme to expedite the proposed LSEs. In
contrast to existing methods, the proposed LSEs are significantly faster. Most
importantly, they involve much fewer parameters while achieving better
performance. The advantages of the proposed LSEs over other state-of-the-art
approaches have been verified by a range of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7475</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7475</id><created>2014-09-26</created><updated>2014-11-05</updated><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Requisite Variety, Autopoiesis, and Self-organization</title><categories>nlin.AO cs.OH</categories><comments>Invited keynote at WOSC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ashby's law of requisite variety states that a controller must have at least
as much variety (complexity) as the controlled. Maturana and Varela proposed
autopoiesis (self-production) to define living systems. Living systems also
require to fulfill the law of requisite variety. A measure of autopoiesis has
been proposed as the ratio between the complexity of a system and the
complexity of its environment. Self-organization can be used as a concept to
guide the design of systems towards higher values of autopoiesis, with the
potential of making technology more &quot;living&quot;, i.e. adaptive and robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7476</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7476</id><created>2014-09-26</created><authors><author><keyname>Join</keyname><forenames>C&#xe9;dric</forenames><affiliation>INRIA Lille - Nord Europe, CRAN, AL.I.E.N.</affiliation></author><author><keyname>Voyant</keyname><forenames>Cyril</forenames><affiliation>SPE</affiliation></author><author><keyname>Fliess</keyname><forenames>Michel</forenames><affiliation>AL.I.E.N., LIX</affiliation></author><author><keyname>Muselli</keyname><forenames>Marc</forenames><affiliation>SPE</affiliation></author><author><keyname>Nivet</keyname><forenames>Marie Laure</forenames><affiliation>SPE</affiliation></author><author><keyname>Paoli</keyname><forenames>Christophe</forenames><affiliation>CRAN</affiliation></author><author><keyname>Chaxel</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>CRAN</affiliation></author></authors><title>Short-term solar irradiance and irradiation forecasts via different time
  series techniques: A preliminary study</title><categories>cs.LG physics.ao-ph</categories><proxy>ccsd</proxy><journal-ref>3rd International Symposium on Environment-Friendly Energies and
  Applications (EFEA 2014), Pars : France (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This communication is devoted to solar irradiance and irradiation short-term
forecasts, which are useful for electricity production. Several different time
series approaches are employed. Our results and the corresponding numerical
simulations show that techniques which do not need a large amount of historical
data behave better than those which need them, especially when those data are
quite noisy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7478</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7478</id><created>2014-09-26</created><authors><author><keyname>Aguirre</keyname><forenames>Hernan</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Liefooghe</keyname><forenames>Arnaud</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Verel</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LISIC</affiliation></author><author><keyname>Tanaka</keyname><forenames>Kiyoshi</forenames></author></authors><title>An Analysis on Selection for High-Resolution Approximations in
  Many-Objective Optimization</title><categories>cs.NE</categories><comments>apperas in Parallel Problem Solving from Nature - PPSN XIII,
  Ljubljana : Slovenia (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the behavior of three elitist multi- and many-objective
evolutionary algorithms generating a high-resolution approximation of the
Pareto optimal set. Several search-assessment indicators are defined to trace
the dynamics of survival selection and measure the ability to simultaneously
keep optimal solutions and discover new ones under different population sizes,
set as a fraction of the size of the Pareto optimal set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7480</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7480</id><created>2014-09-26</created><updated>2015-06-01</updated><authors><author><keyname>Elhoseiny</keyname><forenames>Mohamed</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Generalized Twin Gaussian Processes using Sharma-Mittal Divergence</title><categories>cs.LG cs.CV stat.ML</categories><comments>This work got accepted for Publication in the Machine Learning
  Journal 2015. The work is scheduled for presentation at ECML-PKDD 2015
  journal track papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a growing interest in mutual information measures due to their
wide range of applications in Machine Learning and Computer Vision. In this
paper, we present a generalized structured regression framework based on
Shama-Mittal divergence, a relative entropy measure, which is introduced to the
Machine Learning community in this work. Sharma-Mittal (SM) divergence is a
generalized mutual information measure for the widely used R\'enyi, Tsallis,
Bhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, we
study Sharma-Mittal divergence as a cost function in the context of the Twin
Gaussian Processes (TGP)~\citep{Bo:2010}, which generalizes over the
KL-divergence without computational penalty. We show interesting properties of
Sharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missing
insights in the traditional TGP formulation. However, we generalize this theory
based on SM-divergence instead of KL-divergence which is a special case.
Experimentally, we evaluated the proposed SMTGP framework on several datasets.
The results show that SMTGP reaches better predictions than KL-based TGP, since
it offers a bigger class of models through its parameters that we learn from
the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7488</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7488</id><created>2014-09-26</created><updated>2014-11-11</updated><authors><author><keyname>He</keyname><forenames>Yuguo</forenames><affiliation>School of Computer Science and Technology, Beijing Institute of Technology, China</affiliation></author></authors><title>On the strictness of the quantifier structure hierarchy in first-order
  logic</title><categories>cs.LO</categories><comments>38 pages, 8 figures</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (November
  13, 2014) lmcs:965</journal-ref><doi>10.2168/LMCS-10(4:3)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a natural hierarchy in first-order logic, namely the quantifier
structure hierarchy, which gives a systematic classification of first-order
formulas based on structural quantifier resource. We define a variant of
Ehrenfeucht-Fraisse games that characterizes quantifier classes and use it to
prove that this hierarchy is strict over finite structures, using strategy
compositions. Moreover, we prove that this hierarchy is strict even over
ordered finite structures, which is interesting in the context of descriptive
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7489</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7489</id><created>2014-09-26</created><updated>2014-10-12</updated><authors><author><keyname>An</keyname><forenames>Jisun</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Crowcroft</keyname><forenames>Jon</forenames></author></authors><title>Recommending Investors for Crowdfunding Projects</title><categories>cs.SI cs.CY cs.HC physics.soc-ph stat.ML</categories><comments>Published in Proc. of WWW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To bring their innovative ideas to market, those embarking in new ventures
have to raise money, and, to do so, they have often resorted to banks and
venture capitalists. Nowadays, they have an additional option: that of
crowdfunding. The name refers to the idea that funds come from a network of
people on the Internet who are passionate about supporting others' projects.
One of the most popular crowdfunding sites is Kickstarter. In it, creators post
descriptions of their projects and advertise them on social media sites (mainly
Twitter), while investors look for projects to support. The most common reason
for project failure is the inability of founders to connect with a sufficient
number of investors, and that is mainly because hitherto there has not been any
automatic way of matching creators and investors. We thus set out to propose
different ways of recommending investors found on Twitter for specific
Kickstarter projects. We do so by conducting hypothesis-driven analyses of
pledging behavior and translate the corresponding findings into different
recommendation strategies. The best strategy achieves, on average, 84% of
accuracy in predicting a list of potential investors' Twitter accounts for any
given project. Our findings also produced key insights about the whys and
wherefores of investors deciding to support innovative efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7495</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7495</id><created>2014-09-26</created><updated>2015-02-27</updated><authors><author><keyname>Ganin</keyname><forenames>Yaroslav</forenames></author><author><keyname>Lempitsky</keyname><forenames>Victor</forenames></author></authors><title>Unsupervised Domain Adaptation by Backpropagation</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Top-performing deep architectures are trained on massive amounts of labeled
data. In the absence of labeled data for a certain task, domain adaptation
often provides an attractive option given that labeled data of similar nature
but from a different domain (e.g. synthetic images) are available. Here, we
propose a new approach to domain adaptation in deep architectures that can be
trained on large amount of labeled data from the source domain and large amount
of unlabeled data from the target domain (no labeled target-domain data is
necessary).
  As the training progresses, the approach promotes the emergence of &quot;deep&quot;
features that are (i) discriminative for the main learning task on the source
domain and (ii) invariant with respect to the shift between the domains. We
show that this adaptation behaviour can be achieved in almost any feed-forward
model by augmenting it with few standard layers and a simple new gradient
reversal layer. The resulting augmented architecture can be trained using
standard backpropagation.
  Overall, the approach can be implemented with little effort using any of the
deep-learning packages. The method performs very well in a series of image
classification experiments, achieving adaptation effect in the presence of big
domain shifts and outperforming previous state-of-the-art on Office datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7509</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7509</id><created>2014-09-26</created><updated>2014-10-20</updated><authors><author><keyname>Caltais</keyname><forenames>Georgiana</forenames></author></authors><title>Expression-based aliasing for OO-languages</title><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alias analysis has been an interesting research topic in verification and
optimization of programs. The undecidability of determining whether two
expressions in a program may reference to the same object is the main source of
the challenges raised in alias analysis. In this paper we propose an extension
of a previously introduced alias calculus based on program expressions, to the
setting of unbounded program executions s.a. infinite loops and recursive
calls. Moreover, we devise a corresponding executable specification in the
K-framework. An important property of our extension is that, in a
non-concurrent setting, the corresponding alias expressions can be
over-approximated in terms of a notion of regular expressions. This further
enables us to show that the associated K-machinery implements an algorithm that
always stops and provides a sound over-approximation of the &quot;may aliasing&quot;
information, where soundness stands for the lack of false negatives. As a case
study, we analyze the integration and further applications of the alias
calculus in SCOOP. The latter is an object-oriented programming model for
concurrency, recently formalized in Maude; K-definitions can be compiled into
Maude for execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7514</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7514</id><created>2014-09-26</created><updated>2014-10-06</updated><authors><author><keyname>Caltais</keyname><forenames>Georgiana</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Coffman deadlocks in SCOOP</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the deadlock detection problem in the context of
SCOOP - an OO-programming model for concurrency, recently formalized in Maude.
We present the integration of a deadlock detection mechanism on top of the
aforementioned formalization and analyze how an abstract semantics of SCOOP
based on a notion of &quot;may alias expressions&quot; can contribute to improving the
deadlock detection procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7520</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7520</id><created>2014-09-26</created><updated>2015-06-09</updated><authors><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author></authors><title>More is less: Connectivity in fractal regions</title><categories>cs.NI cond-mat.dis-nn cond-mat.stat-mech</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ad-hoc networks are often deployed in regions with complicated boundaries. We
show that if the boundary is modeled as a fractal, a network requiring line of
sight connections has the counterintuitive property that increasing the number
of nodes decreases the full connection probability. We characterise this decay
as a stretched exponential involving the fractal dimension of the boundary, and
discuss mitigation strategies. Applications of this study include the analysis
and design of sensor networks operating in rugged terrain (e.g. railway
cuttings), mm-wave networks in industrial settings and
vehicle-to-vehicle/vehicle-to-infrastructure networks in urban environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7542</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7542</id><created>2014-09-26</created><authors><author><keyname>Castellan</keyname><forenames>Simon</forenames><affiliation>LIP</affiliation></author><author><keyname>Clairambault</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author><author><keyname>Winskel</keyname><forenames>Glynn</forenames></author></authors><title>Concurrent Hyland-Ong games</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical report, we build a cartesian closed category of
non-deterministic concurrent strategies playing on arenas. We show that this
CCC admits as a sub-CCC the standard category of arenas and Hyland-Ong innocent
strategies. Our strategies, have much more possible behaviours than standard
Hyland-Ong innocent strategies - however the purpose of this technical report
is to define our CCC, and we leave for later its use for semantics of
programming languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7551</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7551</id><created>2014-09-26</created><authors><author><keyname>A</keyname><forenames>Krishna Chaitanya</forenames></author><author><keyname>Mukherji</keyname><forenames>Utpal</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Algorithms for Stochastic Games on Interference Channels</title><categories>cs.IT cs.GT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless channel shared by multiple transmitter-receiver pairs.
Their transmissions interfere with each other. Each transmitter-receiver pair
aims to maximize its long-term average transmission rate subject to an average
power constraint. This scenario is modeled as a stochastic game. We provide
sufficient conditions for existence and uniqueness of a Nash equilibrium (NE).
We then formulate the problem of finding NE as a variational inequality (VI)
problem and present an algorithm to solve the VI using regularization. We also
provide distributed algorithms to compute Pareto optimal solutions for the
proposed game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7552</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7552</id><created>2014-09-26</created><updated>2015-09-16</updated><authors><author><keyname>Kulick</keyname><forenames>Johannes</forenames></author><author><keyname>Lieck</keyname><forenames>Robert</forenames></author><author><keyname>Toussaint</keyname><forenames>Marc</forenames></author></authors><title>The Advantage of Cross Entropy over Entropy in Iterative Information
  Gathering</title><categories>stat.ML cs.LG</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gathering the most information by picking the least amount of data is a
common task in experimental design or when exploring an unknown environment in
reinforcement learning and robotics. A widely used measure for quantifying the
information contained in some distribution of interest is its entropy. Greedily
minimizing the expected entropy is therefore a standard method for choosing
samples in order to gain strong beliefs about the underlying random variables.
We show that this approach is prone to temporally getting stuck in local optima
corresponding to wrongly biased beliefs. We suggest instead maximizing the
expected cross entropy between old and new belief, which aims at challenging
refutable beliefs and thereby avoids these local optima. We show that both
criteria are closely related and that their difference can be traced back to
the asymmetry of the Kullback-Leibler divergence. In illustrative examples as
well as simulated and real-world experiments we demonstrate the advantage of
cross entropy over simple entropy for practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7556</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7556</id><created>2014-09-26</created><updated>2015-05-25</updated><authors><author><keyname>Fernando</keyname><forenames>Basura</forenames></author><author><keyname>Tommasi</keyname><forenames>Tatiana</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Location Recognition Over Large Time Lags</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Would it be possible to automatically associate ancient pictures to modern
ones and create fancy cultural heritage city maps? We introduce here the task
of recognizing the location depicted in an old photo given modern annotated
images collected from the Internet. We present an extensive analysis on
different features, looking for the most discriminative and most robust to the
image variability induced by large time lags. Moreover, we show that the
described task benefits from domain adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7570</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7570</id><created>2014-09-26</created><authors><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author></authors><title>Optimized Compressed Sensing Matrix Design for Noisy Communication
  Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE ICC 2015 (EXTENDED VERSION)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a power-constrained sensing matrix design problem for a
compressed sensing framework. We adopt a mean square error (MSE) performance
criterion for sparse source reconstruction in a system where the
source-to-sensor channel and the sensor-to-decoder communication channel are
noisy. Our proposed sensing matrix design procedure relies upon minimizing a
lower-bound on the MSE. Under certain conditions, we derive closed-form
solutions to the optimization problem. Through numerical experiments, by
applying practical sparse reconstruction algorithms, we show the strength of
the proposed scheme by comparing it with other relevant methods. We discuss the
computational complexity of our design method, and develop an equivalent
stochastic optimization method to the problem of interest that can be solved
approximately with a significantly less computational burden. We illustrate
that the low-complexity method still outperforms the popular competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7575</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7575</id><created>2014-09-26</created><authors><author><keyname>Galiotto</keyname><forenames>Carlo</forenames></author><author><keyname>Gomez-Miguelez</keyname><forenames>Ismael</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author><author><keyname>Doyle</keyname><forenames>Linda</forenames></author></authors><title>Effect of LOS/NLOS Propagation on Area Spectral Efficiency and Energy
  Efficiency of Small-Cells</title><categories>cs.NI</categories><comments>Accepted to IEEE GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the effect of Line-of-Sight (LOS) and
Non-Line-of-Sight (NLOS) propagation on the Area Spectral Efficiency (ASE) and
on the energy efficiency of dense small-cell networks. We show that including
both LOS and NLOS propagation in the path-loss model provides a completely
different picture of the behaviours of ASE and energy efficiency than what
would be observed in case of either LOS or NLOS propagation only. In
particular, with combined LOS/NLOS path-loss, the ASE exhibits superlinear and
sublinear behaviour at low and high cell densities, respectively. In addition,
the energy efficiency as a function of the cell density has a global maximum
and is not a monotonically increasing function like in case of LOS or NLOS
propagation only. Based on our findings, we claim that Line-of-Sight (LOS) and
Non-Line-of-Sight (NLOS) propagations play an important role in studying the
performance of extremely dense small-cell networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7579</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7579</id><created>2014-09-26</created><authors><author><keyname>Parravano</keyname><forenames>Antonio</forenames></author><author><keyname>Noguera</keyname><forenames>Jos&#xe9; A.</forenames></author><author><keyname>Tena</keyname><forenames>Jordi</forenames></author><author><keyname>Hermida</keyname><forenames>Paula</forenames></author></authors><title>Field evidence of social influence in the expression of political
  preferences: the case of secessionist flags in Barcelona</title><categories>physics.soc-ph cs.SI</categories><comments>27 pages, 13 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different models of social influence have explored the dynamics of social
contagion, imitation, and diffusion of different types of traits, opinions, and
conducts. However, few behavioral data indicating social influence dynamics
have been obtained from direct observation in `natural' social contexts. The
present research provides that kind of evidence in the case of the public
expression of political preferences in the city of Barcelona, where thousands
of citizens supporting the secession of Catalonia from Spain have placed a
Catalan flag in their balconies. We present two different studies. 1) In July
2013 we registered the number of flags in 26% of the the city. We find that
there is a large dispersion in the density of flags in districts with similar
density of pro-independence voters. However, we find that the density of flags
tends to be fostered in those electoral district where there is a clear
majority of pro-independence vote, while it is inhibited in the opposite cases.
2) During 17 days around Catalonia's 2013 National Holiday we observed the
position at balcony resolution of the flags displayed in the facades of 82
blocks. We compare the clustering of flags on the facades observed each day to
equivalent random distributions and find that successive hangings of flags are
not independent events but that a local influence mechanism is favoring their
clustering. We also find that except for the National Holiday day the density
of flags tends to be fostered in those facades where there is a clear majority
of pro-independence vote.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7580</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7580</id><created>2014-09-26</created><authors><author><keyname>Blum</keyname><forenames>Christian</forenames></author><author><keyname>Hafner</keyname><forenames>Verena V.</forenames></author></authors><title>Gradient-based Taxis Algorithms for Network Robotics</title><categories>cs.RO cs.AI cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the physical location of a specific network node is a prototypical
task for navigation inside a wireless network. In this paper, we consider in
depth the implications of wireless communication as a measurement input of
gradient-based taxis algorithms. We discuss how gradients can be measured and
determine the errors of this estimation. We then introduce a gradient-based
taxis algorithm as an example of a family of gradient-based, convergent
algorithms and discuss its convergence in the context of network robotics. We
also conduct an exemplary experiment to show how to overcome some of the
specific problems related to network robotics. Finally, we show how to adapt
this framework to more complex objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7591</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7591</id><created>2014-09-26</created><authors><author><keyname>Maiya</keyname><forenames>Arun S.</forenames></author><author><keyname>Rolfe</keyname><forenames>Robert M.</forenames></author></authors><title>Topic Similarity Networks: Visual Analytics for Large Document Sets</title><categories>cs.CL cs.HC cs.IR cs.SI stat.ML</categories><comments>9 pages; 2014 IEEE International Conference on Big Data (IEEE BigData
  2014)</comments><acm-class>I.2.6; I.2.7; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate ways in which to improve the interpretability of LDA topic
models by better analyzing and visualizing their outputs. We focus on examining
what we refer to as topic similarity networks: graphs in which nodes represent
latent topics in text collections and links represent similarity among topics.
We describe efficient and effective approaches to both building and labeling
such networks. Visualizations of topic models based on these networks are shown
to be a powerful means of exploring, characterizing, and summarizing large
collections of unstructured text documents. They help to &quot;tease out&quot;
non-obvious connections among different sets of documents and provide insights
into how topics form larger themes. We demonstrate the efficacy and
practicality of these approaches through two case studies: 1) NSF grants for
basic research spanning a 14 year period and 2) the entire English portion of
Wikipedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7595</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7595</id><created>2014-09-26</created><authors><author><keyname>Chan</keyname><forenames>Hau</forenames></author><author><keyname>Chen</keyname><forenames>Jing</forenames></author></authors><title>Truthful Multi-unit Procurements with Budgets</title><categories>cs.GT</categories><comments>To appear at WINE 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study procurement games where each seller supplies multiple units of his
item, with a cost per unit known only to him. The buyer can purchase any number
of units from each seller, values different combinations of the items
differently, and has a budget for his total payment.
  For a special class of procurement games, the {\em bounded knapsack} problem,
we show that no universally truthful budget-feasible mechanism can approximate
the optimal value of the buyer within $\ln n$, where $n$ is the total number of
units of all items available. We then construct a polynomial-time mechanism
that gives a $4(1+\ln n)$-approximation for procurement games with {\em concave
additive valuations}, which include bounded knapsack as a special case. Our
mechanism is thus optimal up to a constant factor. Moreover, for the bounded
knapsack problem, given the well-known FPTAS, our results imply there is a
provable gap between the optimization domain and the mechanism design domain.
  Finally, for procurement games with {\em sub-additive valuations}, we
construct a universally truthful budget-feasible mechanism that gives an
$O(\frac{\log^2 n}{\log \log n})$-approximation in polynomial time with a
demand oracle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7612</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7612</id><created>2014-09-25</created><authors><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author></authors><title>Semi-supervised Classification for Natural Language Processing</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-supervised classification is an interesting idea where classification
models are learned from both labeled and unlabeled data. It has several
advantages over supervised classification in natural language processing
domain. For instance, supervised classification exploits only labeled data that
are expensive, often difficult to get, inadequate in quantity, and require
human experts for annotation. On the other hand, unlabeled data are inexpensive
and abundant. Despite the fact that many factors limit the wide-spread use of
semi-supervised classification, it has become popular since its level of
performance is empirically as good as supervised classification. This study
explores the possibilities and achievements as well as complexity and
limitations of semi-supervised classification for several natural langue
processing tasks like parsing, biomedical information processing, text
classification, and summarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7614</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7614</id><created>2014-09-25</created><authors><author><keyname>Chatterjee</keyname><forenames>Avhishek</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Generalized Opinion Dynamics from Local Optimization Rules</title><categories>math.DS cs.MA cs.SY physics.soc-ph</categories><comments>20 pages, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study generalizations of the Hegselmann-Krause (HK) model for opinion
dynamics, incorporating features and parameters that are natural components of
observed social systems. The first generalization is one where the strength of
influence depends on the distance of the agents' opinions. Under this setup, we
identify conditions under which the opinions converge in finite time, and
provide a qualitative characterization of the equilibrium. We interpret the HK
model opinion update rule as a quadratic cost-minimization rule. This enables a
second generalization: a family of update rules which possess different
equilibrium properties. Subsequently, we investigate models in which a external
force can behave strategically to modulate/influence user updates. We consider
cases where this external force can introduce additional agents and cases where
they can modify the cost structures for other agents. We describe and analyze
some strategies through which such modulation may be possible in an
order-optimal manner. Our simulations demonstrate that generalized dynamics
differ qualitatively and quantitatively from traditional HK dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7615</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7615</id><created>2014-09-26</created><authors><author><keyname>Dreier</keyname><forenames>Jan</forenames></author></authors><title>Overlapping Communities in Complex Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communities are subsets of a network that are densely connected inside and
share only few connections to the rest of the network. The aim of this research
is the development and evaluation of an efficient algorithm for detection of
overlapping, fuzzy communities. The algorithm gets as input some members of
each community that we aim to discover. We call these members seed nodes. The
algorithm then propagates this information by using random walks that start at
non-seed nodes and end as they reach a seed node. The probability that a random
walk starting at a non-seed node $v$ ends at a seed node $s$ is then equated
with the probability that $v$ belongs to the communities of $s$. The algorithm
runs in time $\tilde{O}(l \cdot m \cdot \log n)$, where $l$ is the number of
communities to detect, $m$ is the number of edges, $n$ is the number of nodes.
The $\tilde{O}$-notation hides a factor of at most $(\log \log n)^2$. The LFR
benchmark proposed by Lancichinetti et al.\ is used to evaluate the performance
of the algorithm. We found that, given a good set of seed nodes, it is able to
reconstruct the communities of a network in a meaningful manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7618</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7618</id><created>2014-09-26</created><updated>2015-09-21</updated><authors><author><keyname>Luo</keyname><forenames>Wenhan</forenames></author><author><keyname>Xing</keyname><forenames>Junliang</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaoqin</forenames></author><author><keyname>Zhao</keyname><forenames>Xiaowei</forenames></author><author><keyname>Kim</keyname><forenames>Tae-Kyun</forenames></author></authors><title>Multiple Object Tracking: A Literature Review</title><categories>cs.CV</categories><acm-class>I.4.8</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Multiple Object Tracking is an important computer vision task which has
gained increasing attention due to its academic and commercial potential.
Although different approaches have been proposed to tackle it, there still
exist many issues unsolved. In order to help readers understand this topic, we
contribute a systematic and comprehensive review. In the review, we inspect
recent advances in various aspects and propose some interesting directions for
future research.
  To our best knowledge, there has not been any review about this topic in the
community. We endeavor to provide a thorough review on the development of this
problem in the last decades. The main contributions are fourfold: 1) Key
aspects in a multiple object tracking system, including how to formulate MOT
generally, how to categorize MOT algorithms, what needs to be considered when
developing a MOT system and how to evaluate a MOT system, are discussed from
the perspective of understanding a topic. We believe this could not only
provide researchers, especially new comers to the topic of MOT, a general
understanding of the state of the arts, but also help them to comprehend the
essential components of a MOT system and the inter-component connection. 2)
Instead of enumerating individual works, we discuss existing work according to
the various aspects involved in a MOT system. In each aspect, methods are
divided into different groups and each group is discussed in details for the
principles, advances and drawbacks. 3) We examine experiments of existing
publications and give tables which list results on the popular data sets to
provide convenient comparison. We also provide some interesting discoveries by
analyzing these tables. 4) We offer some potential directions and respective
discussions about MOT, which are still open issues and need more research
efforts. This would be helpful to identify interesting problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7619</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7619</id><created>2014-09-25</created><authors><author><keyname>Ovchinnikova</keyname><forenames>Ekaterina</forenames></author><author><keyname>Zaytsev</keyname><forenames>Vladimir</forenames></author><author><keyname>Wertheim</keyname><forenames>Suzanne</forenames></author><author><keyname>Israel</keyname><forenames>Ross</forenames></author></authors><title>Generating Conceptual Metaphors from Proposition Stores</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contemporary research on computational processing of linguistic metaphors is
divided into two main branches: metaphor recognition and metaphor
interpretation. We take a different line of research and present an automated
method for generating conceptual metaphors from linguistic data. Given the
generated conceptual metaphors, we find corresponding linguistic metaphors in
corpora. In this paper, we describe our approach and its evaluation using
English and Russian data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7623</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7623</id><created>2014-09-26</created><authors><author><keyname>Sharma</keyname><forenames>Rajesh</forenames></author><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author><author><keyname>Montesi</keyname><forenames>Danilo</forenames></author></authors><title>Missing data in multiplex networks: a preliminary study</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic problem in the analysis of social networks is missing data. When a
network model does not accurately capture all the actors or relationships in
the social system under study, measures computed on the network and ultimately
the final outcomes of the analysis can be severely distorted. For this reason,
researchers in social network analysis have characterised the impact of
different types of missing data on existing network measures. Recently a lot of
attention has been devoted to the study of multiple-network systems, e.g.,
multiplex networks. In these systems missing data has an even more significant
impact on the outcomes of the analyses. However, to the best of our knowledge,
no study has focused on this problem yet. This work is a first step in the
direction of understanding the impact of missing data in multiple networks. We
first discuss the main reasons for missingness in these systems, then we
explore the relation between various types of missing information and their
effect on network properties. We provide initial experimental evidence based on
both real and synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7626</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7626</id><created>2014-09-26</created><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames></author><author><keyname>Giovanidis</keyname><forenames>Anastasios</forenames></author></authors><title>Optimal Geographic Caching In Cellular Networks</title><categories>cs.NI</categories><comments>6 pages, 6 figures, conference</comments><journal-ref>2015 IEEE International Conference on Communications (ICC)</journal-ref><doi>10.1109/ICC.2015.7248843</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the problem of an optimal geographic placement of
content in wireless cellular networks modelled by Poisson point processes.
Specifically, for the typical user requesting some particular content and whose
popularity follows a given law (e.g. Zipf), we calculate the probability of
finding the content cached in one of the base stations. Wireless coverage
follows the usual signal-to-interference-and noise ratio (SINR) model, or some
variants of it. We formulate and solve the problem of an optimal randomized
content placement policy, to maximize the user's hit probability. The result
dictates that it is not always optimal to follow the standard policy &quot;cache the
most popular content, everywhere&quot;. In fact, our numerical results regarding
three different coverage scenarios, show that the optimal policy significantly
increases the chances of hit under high-coverage regime, i.e., when the
probabilities of coverage by more than just one station are high enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7628</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7628</id><created>2014-09-26</created><authors><author><keyname>Lozano</keyname><forenames>Roberto Casta&#xf1;eda</forenames></author><author><keyname>Schulte</keyname><forenames>Christian</forenames></author></authors><title>Survey on Combinatorial Register Allocation and Instruction Scheduling</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Register allocation and instruction scheduling are two central compiler
back-end problems that are critical for quality. In the last two decades,
combinatorial optimization has emerged as an alternative approach to
traditional, heuristic algorithms for these problems. Combinatorial approaches
are generally slower but more flexible than their heuristic counterparts and
have the potential to generate optimal code. This paper surveys existing
literature on combinatorial register allocation and instruction scheduling. The
survey covers approaches that solve each problem in isolation as well as
approaches that integrate both problems. The latter have the potential to
generate code that is globally optimal by capturing the trade-off between
conflicting register allocation and instruction scheduling decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7635</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7635</id><created>2014-09-24</created><authors><author><keyname>Goldfarb</keyname><forenames>Daniel</forenames></author></authors><title>An Application of Topological Data Analysis to Hockey Analytics</title><categories>cs.OH</categories><comments>19 pages, 15 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies the major computational tool from Topological Data
Analysis (TDA), persistent homology, to discover patterns in the data related
to professional sports teams. I will use official game data from the
North-American National Hockey League (NHL) 2013-2014 season to discover the
correlation between the composition of NHL teams with the currently preferred
offensive performance markers. Specifically, I develop and use the program
TeamPlex (based on the JavaPlex software library) to generate the persistence
bar-codes. TeamPlex is applied to players as data points in a multidimensional
(up to 12-D) data space where each coordinate corresponds to a selected
performance marker.
  The conclusion is that team's offensive performance (measured by the popular
characteristic used in NHL called the Corsi number) correlates with two
bar-code characteristics: greater \textit{sparsity} reflected in the longer
bars in dimension 0 and lower \textit{tunneling} reflected in the low
number/length of the 1-dimensional classes. The methodology can be used by team
managers in identifying deficiencies in the present composition of the team and
analyzing player trades and acquisitions. We give an example of a proposed
trade which should improve the Corsi number of the team.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7637</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7637</id><created>2014-09-26</created><authors><author><keyname>Segura</keyname><forenames>Marcelo</forenames></author><author><keyname>Niranjayan</keyname><forenames>S.</forenames></author><author><keyname>Hashemi</keyname><forenames>Hossein</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Experimental Demonstration of Nanosecond Accuracy Wireless Network
  Synchronization</title><categories>cs.NI</categories><comments>Submitted to ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate wireless timing synchronization has been an extremely important
topic in wireless sensor networks, required in applications ranging from
distributed beam forming to precision localization and navigation. However, it
is very challenging to realize, in particular when the required accuracy should
be better than the runtime between the nodes. This work presents, to our
knowledge for the first time, an experimental timing synchronization scheme
that achieves a timing accuracy better than 5ns rms in a network with 4 nodes.
The experimental hardware is built from commercially available components and
based on software defined ultra wideband transceivers. The protocol for
establishing the synchronization is based on our recently developed blink
protocol that can scale from the small network demonstrated here to larger
networks of hundreds or thousands of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7685</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7685</id><created>2014-09-26</created><updated>2014-10-20</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Eldan</keyname><forenames>Ronen</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>R&#xe1;cz</keyname><forenames>Mikl&#xf3;s Z.</forenames></author></authors><title>From trees to seeds: on the inference of the seed from large trees in
  the uniform attachment model</title><categories>math.PR cs.DM cs.SI math.ST stat.TH</categories><comments>26 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the influence of the seed in random trees grown according to the
uniform attachment model, also known as uniform random recursive trees. We show
that different seeds lead to different distributions of limiting trees from a
total variation point of view. To do this, we construct statistics that
measure, in a certain well-defined sense, global &quot;balancedness&quot; properties of
such trees. Our paper follows recent results on the same question for the
preferential attachment model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7686</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7686</id><created>2014-09-26</created><authors><author><keyname>K&#xfc;mmerer</keyname><forenames>Matthias</forenames></author><author><keyname>Wallis</keyname><forenames>Thomas</forenames></author><author><keyname>Bethge</keyname><forenames>Matthias</forenames></author></authors><title>How close are we to understanding image-based saliency?</title><categories>cs.CV q-bio.NC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the set of the many complex factors driving gaze placement, the
properities of an image that are associated with fixations under free viewing
conditions have been studied extensively. There is a general impression that
the field is close to understanding this particular association. Here we frame
saliency models probabilistically as point processes, allowing the calculation
of log-likelihoods and bringing saliency evaluation into the domain of
information. We compared the information gain of state-of-the-art models to a
gold standard and find that only one third of the explainable spatial
information is captured. We additionally provide a principled method to show
where and how models fail to capture information in the fixations. Thus,
contrary to previous assertions, purely spatial saliency remains a significant
challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7687</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7687</id><created>2014-09-25</created><authors><author><keyname>Panda</keyname><forenames>Aurojit</forenames></author><author><keyname>Lahav</keyname><forenames>Ori</forenames></author><author><keyname>Argyraki</keyname><forenames>Katerina</forenames></author><author><keyname>Sagiv</keyname><forenames>Mooly</forenames></author><author><keyname>Shenker</keyname><forenames>Scott</forenames></author></authors><title>Verifying Isolation Properties in the Presence of Middleboxes</title><categories>cs.NI cs.LO</categories><comments>Under submission to NSDI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Great progress has been made recently in verifying the correctness of router
forwarding tables. However, these approaches do not work for networks
containing middleboxes such as caches and firewalls whose forwarding behavior
depends on previously observed traffic. We explore how to verify isolation
properties in networks that include such &quot;dynamic datapath&quot; elements using
model checking. Our work leverages recent advances in SMT solvers, and the main
challenge lies in scaling the approach to handle large and complicated
networks. While the straightforward application of model checking to this
problem can only handle very small networks (if at all), our approach can
verify simple realistic invariants on networks containing 30,000 middleboxes in
a few minutes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7688</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7688</id><created>2014-09-26</created><updated>2014-10-01</updated><authors><author><keyname>Canale</keyname><forenames>Eduardo</forenames></author><author><keyname>Romero</keyname><forenames>Pablo</forenames></author><author><keyname>Rubino</keyname><forenames>Gerardo</forenames></author></authors><title>Irrelevant Components and Exact Computation of the Diameter Constrained
  Reliability</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be a simple graph with $|V|=n$ nodes and $|E|=m$ links, a
subset $K \subseteq V$ of \emph{terminals}, a vector $p=(p_1,...,p_m) \in
[0,1]^m$ and a positive integer $d$, called \emph{diameter}. We assume nodes
are perfect but links fail stochastically and independently, with probabilities
$q_i=1-p_i$. The \emph{diameter-constrained reliability} (DCR for short), is
the probability that the terminals of the resulting subgraph remain connected
by paths composed by $d$ links, or less. This number is denoted by
$R_{K,G}^{d}(p)$. The general computation of the parameter $R_{K,G}^{d}(p)$
belongs to the class of $\mathcal{N}\mathcal{P}$-Hard problems, since is
subsumes the complexity that a random graph is connected.
  A discussion of the computational complexity for DCR-subproblems is provided
in terms of the number of terminal nodes $k=|K|$ and diameter $d$. Either when
$d=1$ or when $d=2$ and $k$ is fixed, the DCR is inside the class $\mathcal{P}$
of polynomial-time problems. The DCR turns $\mathcal{N}\mathcal{P}$-Hard even
if $k \geq 2$ and $d\geq 3$ are fixed, or in an all-terminal scenario when
$d=2$. The traditional approach is to design either exponential exact
algorithms or efficient solutions for particular graph classes.
  The contributions of this paper are two-fold. First, a new recursive class of
graphs are shown to have efficient DCR computation. Second, we define a
factorization method in order to develop an exact DCR computation in general.
The approach is inspired in prior works related with the determination of
irrelevant links and deletion-contraction formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7724</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7724</id><created>2014-07-23</created><authors><author><keyname>Weber</keyname><forenames>Zachary</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author></authors><title>Using 3D Printing to Visualize Social Media Big Data</title><categories>cs.GR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data volume continues to grow at unprecedented rates. One of the key
features that makes big data valuable is the promise to find unknown patterns
or correlations that may be able to improve the quality of processes or
systems. Unfortunately, with the exponential growth in data, users often have
difficulty in visualizing the often-unstructured, non-homogeneous data coming
from a variety of sources. The recent growth in popularity of 3D printing has
ushered in a revolutionary way to interact with big data. Using a 3D printed
mockup up a physical or notional environment, one can display data on the
mockup to show real-time data patterns. In this poster and demonstration, we
describe the process of 3D printing and demonstrate an application of
displaying Twitter data on a 3D mockup of the Massachusetts Institute of
Technology (MIT) campus, known as LuminoCity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7729</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7729</id><created>2014-07-30</created><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Context-Based Information Retrieval in Risky Environment</title><categories>cs.IR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1408.2195</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-Based Information Retrieval is recently modelled as an exploration/
exploitation trade-off (exr/exp) problem, where the system has to choose
between maximizing its expected rewards dealing with its current knowledge
(exploitation) and learning more about the unknown user's preferences to
improve its knowledge (exploration). This problem has been addressed by the
reinforcement learning community but they do not consider the risk level of the
current user's situation, where it may be dangerous to explore the
non-top-ranked documents the user may not desire in his/her current situation
if the risk level is high. We introduce in this paper an algorithm named
CBIR-R-greedy that considers the risk level of the user's situation to
adaptively balance between exr and exp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7733</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7733</id><created>2014-09-26</created><authors><author><keyname>Barjasteh</keyname><forenames>Iman</forenames></author><author><keyname>Liu</keyname><forenames>Ying</forenames></author><author><keyname>Radha</keyname><forenames>Hayder</forenames></author></authors><title>Trending Videos: Measurement and Analysis</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike popular videos, which would have already achieved high viewership
numbers by the time they are declared popular, YouTube trending videos
represent content that targets viewers attention over a relatively short time,
and has the potential of becoming popular. Despite their importance and
visibility, YouTube trending videos have not been studied or analyzed
thoroughly. In this paper, we present our findings for measuring, analyzing,
and comparing key aspects of YouTube trending videos. Our study is based on
collecting and monitoring high-resolution time-series of the viewership and
related statistics of more than 8,000 YouTube videos over an aggregate period
of nine months. Since trending videos are declared as such just several hours
after they are uploaded, we are able to analyze trending videos time-series
across critical and sufficiently-long durations of their lifecycle. In
addition, we analyze the profile of users who upload trending videos, to
potentially identify the role that these users profile plays in getting their
uploaded videos trending. Furthermore, we conduct a directional-relationship
analysis among all pairs of trending videos time-series that we have monitored.
We employ Granger Causality (GC) with significance testing to conduct this
analysis. Unlike traditional correlation measures, our directional-relationship
analysis provides a deeper insight onto the viewership pattern of different
categories of trending videos. Trending videos and their channels have clear
distinct statistical attributes when compared to other YouTube content that has
not been labeled as trending. Our results also reveal a highly asymmetric
directional-relationship among different categories of trending videos. Our
directionality analysis also shows a clear pattern of viewership toward popular
categories, whereas some categories tend to be isolated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7755</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7755</id><created>2014-09-26</created><authors><author><keyname>Yan</keyname><forenames>Han</forenames></author><author><keyname>He</keyname><forenames>Yingzi</forenames></author></authors><title>Drag-Tracking Guidance for Entry Vehicles Without Drag Rate Measurement</title><categories>cs.SY</categories><comments>23 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust entry guidance law without drag rate measurement is designed for
drag-tracking in this paper. The bank angle is regarded as the control
variable. First, a state feedback guidance law (bank angle magnitude) that
requires the drag and its rate as feedback information is designed to make the
drag-tracking error be input-to-state stable (ISS) with respect to
uncertainties. Then a high gain observer is utilized to estimate the drag rate
which is difficult for a vehicle to measure accurately in practice. Stability
analysis as well as simulation results show the efficiency of the presented
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7758</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7758</id><created>2014-09-26</created><authors><author><keyname>Yao</keyname><forenames>Zhe</forenames></author><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author></authors><title>Combating Corrupt Messages in Sparse Clustered Associative Memories</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze and extend the neural network based associative
memory proposed by Gripon and Berrou. This associative memory resembles the
celebrated Willshaw model with an added partite cluster structure. In the
literature, two retrieving schemes have been proposed for the network dynamics,
namely sum-of-sum and sum-of-max. They both offer considerably better
performance than Willshaw and Hopfield networks, when comparable retrieval
scenarios are considered. Former discussions and experiments concentrate on the
erasure scenario, where a partial message is used as a probe to the network, in
the hope of retrieving the full message. In this regard, sum-of-max outperforms
sum-of-sum in terms of retrieval rate by a large margin. However, we observe
that when noise and errors are present and the network is queried by a corrupt
probe, sum-of-max faces a severe limitation as its stringent activation rule
prevents a neuron from reviving back into play once deactivated. In this
manuscript, we categorize and analyze different error scenarios so that both
the erasure and the corrupt scenarios can be treated consistently. We make an
amendment to the network structure to improve the retrieval rate, at the cost
of an extra scalar per neuron. Afterwards, five different approaches are
proposed to deal with corrupt probes. As a result, we extend the network
capability, and also increase the robustness of the retrieving procedure. We
then experimentally compare all these proposals and discuss pros and cons of
each approach under different types of errors. Simulation results show that if
carefully designed, the network is able to preserve both a high retrieval rate
and a low running time simultaneously, even when queried by a corrupt probe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7760</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7760</id><created>2014-09-27</created><authors><author><keyname>Payer</keyname><forenames>Mathias</forenames></author><author><keyname>Crane</keyname><forenames>Stephen</forenames></author><author><keyname>Larsen</keyname><forenames>Per</forenames></author><author><keyname>Brunthaler</keyname><forenames>Stefan</forenames></author><author><keyname>Wartell</keyname><forenames>Richard</forenames></author><author><keyname>Franz</keyname><forenames>Michael</forenames></author></authors><title>Similarity-based matching meets Malware Diversity</title><categories>cs.CR cs.PL cs.SE cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similarity metrics, e.g., signatures as used by anti-virus products, are the
dominant technique to detect if a given binary is malware. The underlying
assumption of this approach is that all instances of a malware (or even malware
family) will be similar to each other.
  Software diversification is a probabilistic technique that uses code and data
randomization and expressiveness in the target instruction set to generate
large amounts of functionally equivalent but different binaries. Malware
diversity builds on software diversity and ensures that any two diversified
instances of the same malware have low similarity (according to a set of
similarity metrics). An LLVM-based prototype implementation diversifies both
code and data of binaries and our evaluation shows that signatures based on
similarity only match one or few instances in a pool of diversified binaries
generated from the same source code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7764</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7764</id><created>2014-09-27</created><authors><author><keyname>Quader</keyname><forenames>Saad</forenames></author></authors><title>A (Somewhat Dated) Comparative Study of Betweenness Centrality
  Algorithms on GPU</title><categories>cs.SI cs.DC cs.DS</categories><comments>This study was done as a class project on the HPC course CSE 5304
  (Fall 2012) at the University of Connecticut, and hence it does not cover any
  advances since January 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of computing the Betweenness Centrality (BC) is important in
analyzing graphs in many practical applications like social networks,
biological networks, transportation networks, electrical circuits, etc. Since
this problem is computation intensive, researchers have been developing
algorithms using high performance computing resources like supercomputers,
clusters, and Graphics Processing Units (GPUs). Current GPU algorithms for
computing BC employ Brandes' sequential algorithm with different trade-offs for
thread scheduling, data structures, and atomic operations. In this paper, we
study three GPU algorithms for computing BC of unweighted, directed, scale-free
networks. We discuss and measure the trade-offs of their design choices about
balanced thread scheduling, atomic operations, synchronizations and latency
hiding. Our program is written in NVIDIA CUDA C and was tested on an NVIDIA
Tesla M2050 GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7765</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7765</id><created>2014-09-27</created><updated>2015-12-27</updated><authors><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>Upper bound on list-decoding radius of binary codes</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>IEEE Trans. Inform. Theory, accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of packing Hamming balls of a given relative radius
subject to the constraint that they cover any point of the ambient Hamming
space with multiplicity at most $L$. For odd $L\ge 3$ an asymptotic upper bound
on the rate of any such packing is proven. Resulting bound improves the best
known bound (due to Blinovsky'1986) for rates below a certain threshold. Method
is a superposition of the linear-programming idea of Ashikhmin, Barg and Litsyn
(that was used previously to improve the estimates of Blinovsky for $L=2$) and
a Ramsey-theoretic technique of Blinovsky. As an application it is shown that
for all odd $L$ the slope of the rate-radius tradeoff is zero at zero rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7771</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7771</id><created>2014-09-27</created><authors><author><keyname>Dutta</keyname><forenames>Chinmoy</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Rajaraman</keyname><forenames>Rajmohan</forenames></author><author><keyname>Sun</keyname><forenames>Zhifeng</forenames></author><author><keyname>Viola</keyname><forenames>Emanuele</forenames></author></authors><title>Global Information Sharing under Network Dynamics</title><categories>cs.DC cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1112.0384</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how to spread $k$ tokens of information to every node on an $n$-node
dynamic network, the edges of which are changing at each round. This basic {\em
gossip problem} can be completed in $O(n + k)$ rounds in any static network,
and determining its complexity in dynamic networks is central to understanding
the algorithmic limits and capabilities of various dynamic network models. Our
focus is on token-forwarding algorithms, which do not manipulate tokens in any
way other than storing, copying and forwarding them.
  We first consider the {\em strongly adaptive} adversary model where in each
round, each node first chooses a token to broadcast to all its neighbors
(without knowing who they are), and then an adversary chooses an arbitrary
connected communication network for that round with the knowledge of the tokens
chosen by each node. We show that $\Omega(nk/\log n + n)$ rounds are needed for
any randomized (centralized or distributed) token-forwarding algorithm to
disseminate the $k$ tokens, thus resolving an open problem raised
in~\cite{kuhn+lo:dynamic}. The bound applies to a wide class of initial token
distributions, including those in which each token is held by exactly one node
and {\em well-mixed} ones in which each node has each token independently with
a constant probability.
  We also show several upper bounds in varying models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7777</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7777</id><created>2014-09-27</created><authors><author><keyname>Guyet</keyname><forenames>Thomas</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Moinard</keyname><forenames>Yves</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Quiniou</keyname><forenames>Ren&#xe9;</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Using Answer Set Programming for pattern mining</title><categories>cs.AI cs.DB cs.LO</categories><comments>Intelligence Artificielle Fondamentale (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Serial pattern mining consists in extracting the frequent sequential patterns
from a unique sequence of itemsets. This paper explores the ability of a
declarative language, such as Answer Set Programming (ASP), to solve this issue
efficiently. We propose several ASP implementations of the frequent sequential
pattern mining task: a non-incremental and an incremental resolution. The
results show that the incremental resolution is more efficient than the
non-incremental one, but both ASP programs are less efficient than dedicated
algorithms. Nonetheless, this approach can be seen as a first step toward a
generic framework for sequential pattern mining with constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7779</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7779</id><created>2014-09-27</created><updated>2014-10-02</updated><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Optimized Training Design for Multi-Antenna Wireless Energy Transfer in
  Frequency-Selective Channel</title><categories>cs.IT math.IT</categories><comments>submitted for possible conference publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the optimal training design for a multiple-input
single-output (MISO) wireless energy transfer (WET) system in
frequency-selective channels, where the frequency-diversity and
energy-beamforming gains can be both achieved by properly learning the channel
state information (CSI) at the energy transmitter (ET). By exploiting channel
reciprocity, a two-phase channel training scheme is proposed to achieve the
diversity and beamforming gains, respectively. In the first phase, pilot
signals are sent from the energy receiver (ER) over a selected subset of the
available frequency sub-bands, through which the sub-band that exhibits the
largest sum-power over all the antennas at the ET is determined and its index
is sent back to the ER. In the second phase, the selected sub-band is further
trained for the ET to estimate the multi-antenna channel and implement energy
beamforming. We propose to maximize the net energy harvested at the ER, which
is the total harvested energy offset by that used for the two-phase channel
training. The optimal training design, including the number of sub-bands
trained and the energy allocated for each of the two phases, is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7780</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7780</id><created>2014-09-27</created><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Zhao</keyname><forenames>Shiguang</forenames></author><author><keyname>Gao</keyname><forenames>Xin</forenames></author></authors><title>Maximum mutual information regularized classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel pattern classification approach is proposed by
regularizing the classifier learning to maximize mutual information between the
classification response and the true class label. We argue that, with the
learned classifier, the uncertainty of the true class label of a data sample
should be reduced by knowing its classification response as much as possible.
The reduced uncertainty is measured by the mutual information between the
classification response and the true class label. To this end, when learning a
linear classifier, we propose to maximize the mutual information between
classification responses and true class labels of training samples, besides
minimizing the classification error and reduc- ing the classifier complexity.
An objective function is constructed by modeling mutual information with
entropy estimation, and it is optimized by a gradi- ent descend method in an
iterative algorithm. Experiments on two real world pattern classification
problems show the significant improvements achieved by maximum mutual
information regularization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7787</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7787</id><created>2014-09-27</created><authors><author><keyname>Crocco</keyname><forenames>Marco</forenames></author><author><keyname>Cristani</keyname><forenames>Marco</forenames></author><author><keyname>Trucco</keyname><forenames>Andrea</forenames></author><author><keyname>Murino</keyname><forenames>Vittorio</forenames></author></authors><title>Audio Surveillance: a Systematic Review</title><categories>cs.SD cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite surveillance systems are becoming increasingly ubiquitous in our
living environment, automated surveillance, currently based on video sensory
modality and machine intelligence, lacks most of the time the robustness and
reliability required in several real applications. To tackle this issue, audio
sensory devices have been taken into account, both alone or in combination with
video, giving birth, in the last decade, to a considerable amount of research.
In this paper audio-based automated surveillance methods are organized into a
comprehensive survey: a general taxonomy, inspired by the more widespread video
surveillance field, is proposed in order to systematically describe the methods
covering background subtraction, event classification, object tracking and
situation analysis. For each of these tasks, all the significant works are
reviewed, detailing their pros and cons and the context for which they have
been proposed. Moreover, a specific section is devoted to audio features,
discussing their expressiveness and their employment in the above described
tasks. Differently, from other surveys on audio processing and analysis, the
present one is specifically targeted to automated surveillance, highlighting
the target applications of each described methods and providing the reader
tables and schemes useful to retrieve the most suited algorithms for a specific
requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7788</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7788</id><created>2014-09-27</created><authors><author><keyname>Francis</keyname><forenames>Maria</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>On Ideal Lattices and Gr\&quot;obner Bases</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we draw a connection between ideal lattices and Gr\&quot;{o}bner
bases in the multivariate polynomial rings over integers. We study extension of
ideal lattices in $\mathbb{Z}[x]/\langle f \rangle$ (Lyubashevsky \&amp;
Micciancio, 2006) to ideal lattices in
$\mathbb{Z}[x_1,\ldots,x_n]/\mathfrak{a}$, the multivariate case, where $f$ is
a polynomial in $\mathbb{Z}[X]$ and $\mathfrak{a}$ is an ideal in
$\mathbb{Z}[x_1,\ldots,x_n]$. Ideal lattices in univariate case are interpreted
as generalizations of cyclic lattices. We introduce a notion of multivariate
cyclic lattices and we show that multivariate ideal lattices are indeed a
generalization of them. We show that the fact that existence of ideal lattice
in univariate case if and only if $f$ is monic translates to short reduced
Gr\&quot;obner basis (Francis \&amp; Dukkipati, 2014) of $\mathfrak{a}$ is monic in
multivariate case. We, thereby, give a necessary and sufficient condition for
residue class polynomial rings over $\mathbb{Z}$ to have ideal lattices. We
also characterize ideals in $\mathbb{Z}[x_1,\ldots,x_n]$ that give rise to full
rank lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7790</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7790</id><created>2014-09-27</created><authors><author><keyname>Chauhan</keyname><forenames>Ankit</forenames></author><author><keyname>Rao</keyname><forenames>B. V. Raghavendra</forenames></author></authors><title>Parameterized Analogues of Probabilistic Computation</title><categories>cs.CC</categories><comments>Submitted to a conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study structural aspects of randomized parameterized computation. We
introduce a new class ${\sf W[P]}$-${\sf PFPT}$ as a natural parameterized
analogue of ${\sf PP}$. Our definition uses the machine based characterization
of the parameterized complexity class ${\sf W[P]}$ obtained by Chen et.al [TCS
2005]. We translate most of the structural properties and characterizations of
the class ${\sf PP}$ to the new class ${W[P]}$-${\sf PFPT}$.
  We study a parameterization of the polynomial identity testing problem based
on the degree of the polynomial computed by the arithmetic circuit. We obtain a
parameterized analogue of the well known Schwartz-Zippel lemma [Schwartz, JACM
80 and Zippel, EUROSAM 79].
  Additionally, we introduce a parameterized variant of permanent, and prove
its $\#W[1]$ completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7794</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7794</id><created>2014-09-27</created><updated>2015-11-19</updated><authors><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Hoi</keyname><forenames>Steven C. H.</forenames></author><author><keyname>Mei</keyname><forenames>Tao</forenames></author><author><keyname>Yu</keyname><forenames>Nenghai</forenames></author></authors><title>Large-scale Online Feature Selection for Ultra-high Dimensional Sparse
  Data</title><categories>cs.LG cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection with large-scale high-dimensional data is important yet
very challenging in machine learning and data mining. Online feature selection
is a promising new paradigm that is more efficient and scalable than batch
feature section methods, but the existing online approaches usually fall short
in their inferior efficacy as compared with batch approaches. In this paper, we
present a novel second-order online feature selection scheme that is simple yet
effective, very fast and extremely scalable to deal with large-scale ultra-high
dimensional sparse data streams. The basic idea is to improve the existing
first-order online feature selection methods by exploiting second-order
information for choosing the subset of important features with high confidence
weights. However, unlike many second-order learning methods that often suffer
from extra high computational cost, we devise a novel smart algorithm for
second-order online feature selection using a MaxHeap-based approach, which is
not only more effective than the existing first-order approaches, but also
significantly more efficient and scalable for large-scale feature selection
with ultra-high dimensional sparse data, as validated from our extensive
experiments. Impressively, on a billion-scale synthetic dataset (1-billion
dimensions, 1-billion nonzero features, and 1-million samples), our new
algorithm took only 8 minutes on a single PC, which is orders of magnitudes
faster than traditional batch approaches. \url{http://arxiv.org/abs/1409.7794}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7808</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7808</id><created>2014-09-27</created><authors><author><keyname>Newstadt</keyname><forenames>Gregory E.</forenames></author><author><keyname>Mu</keyname><forenames>Beipeng</forenames></author><author><keyname>Wei</keyname><forenames>Dennis</forenames></author><author><keyname>How</keyname><forenames>Jonathan P.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Resource-Constrained Adaptive Search for Sparse Multi-Class Targets with
  Varying Importance</title><categories>cs.IT math.IT</categories><comments>49 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In sparse target inference problems it has been shown that significant gains
can be achieved by adaptive sensing using convex criteria. We generalize
previous work on adaptive sensing to (a) include multiple classes of targets
with different levels of importance and (b) accommodate multiple sensor models.
New optimization policies are developed to allocate a limited resource budget
to simultaneously locate, classify and estimate a sparse number of targets
embedded in a large space. Upper and lower bounds on the performance of the
proposed policies are derived by analyzing a baseline policy, which allocates
resources uniformly across the scene, and an oracle policy which has a priori
knowledge of the target locations/classes. These bounds quantify analytically
the potential benefit of adaptive sensing as a function of target frequency and
importance. Numerical results indicate that the proposed policies perform close
to the oracle bound (&lt;3dB) when signal quality is sufficiently high
(e.g.~performance within 3 dB for SNR above 15 dB). Moreover, the proposed
policies improve on previous policies in terms of reducing estimation error,
reducing misclassification probability, and increasing expected return. To
account for sensors with different levels of agility, three sensor models are
considered: global adaptive (GA), which can allocate different amounts of
resource to each location in the space; global uniform (GU), which can allocate
resources uniformly across the scene; and local adaptive (LA), which can
allocate fixed units to a subset of locations. Policies that use a mixture of
GU and LA sensors are shown to perform similarly to those that use GA sensors
while being more easily implementable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7818</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7818</id><created>2014-09-27</created><updated>2015-11-25</updated><authors><author><keyname>Minaee</keyname><forenames>Shervin</forenames></author><author><keyname>Abdolrashidi</keyname><forenames>AmirAli</forenames></author></authors><title>On The Power of Joint Wavelet-DCT Features for Multispectral Palmprint
  Recognition</title><categories>cs.CV</categories><comments>Asilomar Conference on Signals, Systems and Computers, IEEE, 2015,
  (to Appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric-based identification has drawn a lot of attention in the recent
years. Among all biometrics, palmprint is known to possess a rich set of
features. In this paper we have proposed to use DCT-based features in parallel
with wavelet-based ones for palmprint identification. PCA is applied to the
features to reduce their dimensionality and the majority voting algorithm is
used to perform classification. The features introduced here result in a
near-perfectly accurate identification. This method is tested on a well-known
multispectral palmprint database and an accuracy rate of 99.97-100\% is
achieved, outperforming all previous methods in similar conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7822</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7822</id><created>2014-09-27</created><authors><author><keyname>Uykan</keyname><forenames>Zekeriya</forenames></author><author><keyname>Jantti</keyname><forenames>Riku</forenames></author></authors><title>Input-Output Clustering Criterion (IOCC) for Optimizing Distributed
  Antenna Locations</title><categories>cs.IT math.IT</categories><comments>totally 13 plots in Fig.1 to Fig.11, 32 pages, submitted to IEEE
  Trans. Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an input-output space clustering criterion (IOCC)
to optimize the locations of the remote antenna units (RAUs) of generalized
Distributed Antenna Systems (DASs) under sum power constraint. In IOCC, the
input space refers to RAU location space and output space refers to location
specific ergodic capacity space for noise-limited environments. Given a
location-specific arbitrary desired ergodic capacity function over a
geographical area, we define the error as the difference between actual and
desired ergodic capacity. Our investigations show that i) the IOCC provides an
upper bound to the cell averaged ergodic capacity error; and ii) the derived
upper bound is equal to a weighted quantization error function in
location-capacity space (input-output space) and iii) the upper bound can be
made arbitrarily small by a clustering process increasing the number of RAUs
for a feasible DAS. IOCC converts the RAU location problem into a codebook
design problem in vector quantization in input-output space, and thus includes
the Squared Distance Criterion (SDC) for DAS in [15] (and other related papers)
as a special case, which takes only the input space into account. Computer
simulations confirm the theoretical findings and show that the IOCC outperforms
the SDC for DAS in terms of the defined cell averaged &quot;effective&quot; ergodic
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7830</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7830</id><created>2014-09-27</created><authors><author><keyname>Adamczewski</keyname><forenames>Kamil</forenames></author><author><keyname>Matejczyk</keyname><forenames>Szymon</forenames></author><author><keyname>Michalak</keyname><forenames>Tomasz P.</forenames></author></authors><title>How good is the Shapley value-based approach to the influence
  maximization problem?</title><categories>cs.AI cs.MA cs.SI</categories><comments>21st European Conference on Artificial Intelligence</comments><acm-class>I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shapley value has been recently advocated as a method to choose the seed
nodes for the process of information diffusion. Intuitively, since the Shapley
value evaluates the average marginal contribution of a player to the
coalitional game, it can be used in the network context to evaluate the
marginal contribution of a node in the process of information diffusion given
various groups of already 'infected' nodes. Although the above direction of
research seems promising, the current liter- ature is missing a throughout
assessment of its performance. The aim of this work is to provide such an
assessment of the existing Shapley value-based approaches to information
diffusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7831</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7831</id><created>2014-09-27</created><authors><author><keyname>Molnar</keyname><forenames>F.</forenames><suffix>Jr.</suffix></author><author><keyname>Derzsy</keyname><forenames>N.</forenames></author><author><keyname>Szymanski</keyname><forenames>B. K.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Building Damage-Resilient Dominating Sets in Complex Networks against
  Random and Targeted Attacks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Scientific Reports 5, 8321 (2015)</journal-ref><doi>10.1038/srep08321</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the vulnerability of dominating sets against random and targeted
node removals in complex networks. While small, cost-efficient dominating sets
play a significant role in controllability and observability of these networks,
a fixed and intact network structure is always implicitly assumed. We find that
cost-efficiency of dominating sets optimized for small size alone comes at a
price of being vulnerable to damage; domination in the remaining network can be
severely disrupted, even if a small fraction of dominator nodes are lost. We
develop two new methods for finding flexible dominating sets, allowing either
adjustable overall resilience, or dominating set size, while maximizing the
dominated fraction of the remaining network after the attack. We analyze the
efficiency of each method on synthetic scale-free networks, as well as real
complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7841</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7841</id><created>2014-09-27</created><authors><author><keyname>Baklanova</keyname><forenames>Nadezhda</forenames></author><author><keyname>Ricciotti</keyname><forenames>Wilmer</forenames></author><author><keyname>Smaus</keyname><forenames>Jan-Georg</forenames></author><author><keyname>Strecker</keyname><forenames>Martin</forenames></author></authors><title>Abstracting an operational semantics to finite automata</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an apparent similarity between the descriptions of small-step
operational semantics of imperative programs and the semantics of finite
automata, so defining an abstraction mapping from semantics to automata and
proving a simulation property seems to be easy. This paper aims at identifying
the reasons why simple proofs break, among them artifacts in the semantics that
lead to stuttering steps in the simulation. We then present a semantics based
on the zipper data structure, with a direct interpretation of evaluation as
navigation in the syntax tree. The abstraction function is then defined by
equivalence class construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7842</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7842</id><created>2014-09-27</created><updated>2015-05-03</updated><authors><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author></authors><title>When Darwin meets Lorenz: Evolving new chaotic attractors through
  genetic programming</title><categories>nlin.CD cs.NE math.DS</categories><comments>81 pages, 128 figures</comments><journal-ref>Chaos, Solitons &amp; Fractals, Volume 76, Pages 141-155, 2015</journal-ref><doi>10.1016/j.chaos.2015.03.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel methodology for automatically finding new
chaotic attractors through a computational intelligence technique known as
multi-gene genetic programming (MGGP). We apply this technique to the case of
the Lorenz attractor and evolve several new chaotic attractors based on the
basic Lorenz template. The MGGP algorithm automatically finds new nonlinear
expressions for the different state variables starting from the original Lorenz
system. The Lyapunov exponents of each of the attractors are calculated
numerically based on the time series of the state variables using time delay
embedding techniques. The MGGP algorithm tries to search the functional space
of the attractors by aiming to maximise the largest Lyapunov exponent (LLE) of
the evolved attractors. To demonstrate the potential of the proposed
methodology, we report over one hundred new chaotic attractor structures along
with their parameters, which are evolved from just the Lorenz system alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7844</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7844</id><created>2014-09-27</created><authors><author><keyname>Chandra</keyname><forenames>Souvik</forenames></author><author><keyname>Mehta</keyname><forenames>Dhagash</forenames></author><author><keyname>Chakrabortty</keyname><forenames>Aranya</forenames></author></authors><title>Exploring the Impact of Wind Penetration on Power System Equilibrium
  Using a Numerical Continuation Approach</title><categories>cs.SY</categories><comments>7 pages, 14 figures. Submitted to a Special Session of American
  Control Conference to be held in Palmer House Hilton, Chicago, IL, USA, in
  July 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate how the equilibrium characteristics of
conventional power systems may change with an increase in wind penetration. We
first derive a differential-algebraic model of a power system network
consisting of synchronous generators, loads and a wind power plant modeled by a
wind turbine and a doubly-fed induction generator (DFIG). The models of these
three components are coupled via nonlinear power flow equations. In contrast to
the traditional approach for solving the power flows via iterative methods that
often lead to only local solutions, we apply a recently developed
parameter-homotopy based numerical continuation algorithm to compute all
possible solutions. The method solves the power flow equations over multiple
values of the wind penetration level with far less computational effort instead
of solving them at each value individually. We observe that depending on the
penetration limit and the setpoint value for the magnitude of the wind bus
voltage, the system may exhibit several undesired or even unstable equilibria.
We illustrate these results through a detailed simulation of a 5-machine power
system model with wind injection, and highlight how the solutions may be
helpful for small-signal stability assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7850</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7850</id><created>2014-09-27</created><authors><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Brown</keyname><forenames>D. Richard</forenames><suffix>III</suffix></author><author><keyname>Boutin</keyname><forenames>Mireille</forenames></author></authors><title>Distributed Reception with Spatial Multiplexing: MIMO Systems for the
  Internet of Things</title><categories>cs.IT math.IT</categories><comments>11 pages, 7 figures, submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of things (IoT) holds much commercial potential and could
facilitate distributed multiple-input multiple-output (MIMO) communication in
future systems. We study a distributed reception scenario in which a
transmitter equipped with multiple antennas sends multiple streams via spatial
multiplexing to a large number of geographically separated single antenna
receive nodes. The receive nodes then quantize their received signals and
forward the quantized received signals to a receive fusion center. With global
channel knowledge and forwarded quantized information from the receive nodes,
the fusion center attempts to decode the transmitted symbols. We assume the
transmit vector consists of phase shift keying (PSK) constellation points, and
each receive node quantizes its received signal with one bit for each of the
real and imaginary parts of the signal to minimize the transmission overhead
between the receive nodes and the fusion center. Fusing this data is a
non-trivial problem because the receive nodes cannot decode the transmitted
symbols before quantization. Instead, each receive node processes a single
quantity, i.e., the received signal, regardless of the number of transmitted
symbols. We develop an optimal maximum likelihood (ML) receiver and a
low-complexity zero-forcing (ZF)-type receiver at the fusion center. Despite
its suboptimality, the ZF-type receiver is simple to implement and shows
comparable performance with the ML receiver in the low signal-to-noise ratio
(SNR) regime but experiences an error rate floor at high SNR. It is shown that
this error floor can be overcome by increasing the number of receive nodes.
Hence, the ZF-type receiver would be a practical solution for distributed
reception with spatial multiplexing in the era of the IoT where we can easily
have a large number of receive nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7852</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7852</id><created>2014-09-27</created><updated>2015-05-01</updated><authors><author><keyname>Ambikasaran</keyname><forenames>Sivaram</forenames></author></authors><title>Generalized Rybicki Press algorithm</title><categories>math.NA cs.DS cs.NA stat.CO</categories><comments>13 pages, 11 figures, 1 table</comments><msc-class>05C50, 05C85, 62M10, 05B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses a more general and numerically stable Rybicki Press
algorithm, which enables inverting and computing determinants of covariance
matrices, whose elements are sums of exponentials. The algorithm is true in
exact arithmetic and relies on introducing new variables and corresponding
equations, thereby converting the matrix into a banded matrix of larger size.
Linear complexity banded algorithms for solving linear systems and computing
determinants on the larger matrix enable linear complexity algorithms for the
initial semi-separable matrix as well. Benchmarks provided illustrate the
linear scaling of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7853</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7853</id><created>2014-09-27</created><authors><author><keyname>Mouzali</keyname><forenames>Aziz</forenames></author><author><keyname>Merazka</keyname><forenames>Fatiha</forenames></author></authors><title>Quantum correction with three codes</title><categories>cs.IT math.IT quant-ph</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provise an implementation of five, seven and nine-qubits
error correcting codes on a classical computer using the quantum simulator
Feynman program. We also compare the three codes by computing the fidelity when
double errors occurs in a depolarizing channel. As triple errors and more are
considered very unlikely, it has negligible effect on the next results
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7862</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7862</id><created>2014-09-27</created><updated>2014-11-04</updated><authors><author><keyname>C&#xe1;rdenas</keyname><forenames>Juan Pablo</forenames></author><author><keyname>Vidal</keyname><forenames>Gerardo</forenames></author><author><keyname>Olivares</keyname><forenames>Gast&#xf3;n</forenames></author></authors><title>Complexity, Selectivity and Asymmetry in the Conformation of the Power
  Phenomenon. Analysis of Chilean Society</title><categories>physics.soc-ph cs.SI</categories><msc-class>Primary: 05C82, 91D30, Secondary: 90B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we analyzed the relationships between powerful politicians and
businessmen of Chile in order to study the phenomenon of social power. We
developed our study according to Complex Network Theory but also using
traditional sociological theories of Power and Elites. Our analyses suggest
that the studied network displays common properties of Complex Networks, such
as scaling in connectivity distribution, properties of small-world networks,
and modular structure, among others. We also observed that social power (a
proposed metric is presented in this work) is also distributed inhomogeneously.
However, the most interesting observation is that this inhomogeneous power and
connectivity distribution, among other observed properties, may be the result
of a dynamic and unregulated process of network growth in which powerful people
tend to link to similar others. The compatibility between people, increasingly
selective as the network grows, could generate the presence of extremely
powerful people, but also a constant inequality of power where the difference
between the most powerful is the same as among the least powerful. Our results
are also in accordance with sociological theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7878</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7878</id><created>2014-09-28</created><authors><author><keyname>Berti</keyname><forenames>Alessandro</forenames></author></authors><title>An Improved Node Ranking for Label Propagation and Modularity based
  Clustering</title><categories>cs.SI physics.soc-ph</categories><comments>2 pages, 2 tables, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper I'll speak about non-spectral clustering techniques and see how
a node ordering based on centrality measures can improve the quality of
communities detected. I'll also discuss an improvement to existing techniques,
which further improves modularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7889</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7889</id><created>2014-09-28</created><updated>2014-12-04</updated><authors><author><keyname>Mryglod</keyname><forenames>Olesya</forenames></author><author><keyname>Kenna</keyname><forenames>Ralph</forenames></author><author><keyname>Holovatch</keyname><forenames>Yurij</forenames></author></authors><title>Is your EPL attractive? Classification of publications through download
  statistics</title><categories>cs.DL physics.soc-ph</categories><comments>6 pages, 8 figures, accepted for publication in EPL</comments><journal-ref>Europhys. Lett. 108 (2014) 50011</journal-ref><doi>10.1209/0295-5075/108/50011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we consider the download statistics of EPL publications. We find that
papers in the journal are characterised by fast accumulations of downloads
during the first couple of months after publication, followed by slower rates
thereafter, behaviour which can be represented by a model with predictive
power. We also find that individual papers can be classified in various ways,
allowing us to compare categories for open-access and non-open-access papers.
For example, for the latter publications, which comprise the bulk of EPL
papers, a small proportion (2%) display intense bursts of download activity,
possibly following an extended period of less remarkable behaviour. About 18%
have an especially high degree of attractiveness over and above what is typical
for the journal. One can also classify the ageing of attractiveness by
examining download half-lives. Approximately 18% have strong interest
initially, waning in time. A further 20% exhibit &quot;delayed recognition&quot; with
relatively late spurs in download activity. Although open-access papers enjoy
more downloads on average, the proportions falling into each category are
similar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7916</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7916</id><created>2014-09-28</created><authors><author><keyname>R.</keyname><forenames>Nallakumar</forenames></author><author><keyname>S</keyname><forenames>Sruthi Priya K.</forenames></author></authors><title>A Survey on Deadline Constrained Workflow Scheduling Algorithms in Cloud
  Environment</title><categories>cs.DC cs.SE</categories><comments>6 pages,2 figures,Published with International Journal of Computer
  Science Trends and Technology(IJCST)</comments><journal-ref>IJCST V2(5): Page(44-50) Sep 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cloud Computing is the latest blooming technology in the era of Computer
Science and Information Technology domain. There is an enormous pool of data
centres, which are termed as Clouds where the services and associated data are
being deployed and users need a constant Internet connection to access them.
One of the highlights in Cloud is the delivering of applications or services in
an on-demand environment. One of the most promising areas in Cloud scheduling
is Scheduling of workflows which is intended to match the request of the user
to the appropriate resources. There are several algorithms to automate the
workflows in a way to satisfy the Quality of service (QoS) of the user among
which deadline is considered as a major criterion, i.e. Satisfying the needs of
the user with minimized cost and within the minimum stipulated time. This paper
surveys various workflow scheduling algorithms that have a deadline as its
criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7922</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7922</id><created>2014-09-28</created><authors><author><keyname>Zetzsche</keyname><forenames>Georg</forenames></author></authors><title>Computing downward closures for stacked counter automata</title><categories>cs.FL cs.LO</categories><comments>34 pages, 1 figure; submitted</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The downward closure of a language $L$ of words is the set of all (not
necessarily contiguous) subwords of members of $L$. It is well known that the
downward closure of any language is regular. Although the downward closure
seems to be a promising abstraction, there are only few language classes for
which an automaton for the downward closure is known to be computable.
  It is shown here that for stacked counter automata, the downward closure is
computable. Stacked counter automata are finite automata with a storage
mechanism obtained by \emph{adding blind counters} and \emph{building stacks}.
Hence, they generalize pushdown and blind counter automata.
  The class of languages accepted by these automata are precisely those in the
hierarchy obtained from the context-free languages by alternating two closure
operators: imposing semilinear constraints and taking the algebraic extension.
The main tool for computing downward closures is the new concept of Parikh
annotations. As a second application of Parikh annotations, it is shown that
the hierarchy above is strict at every level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7926</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7926</id><created>2014-09-28</created><updated>2014-11-16</updated><authors><author><keyname>Ratliff</keyname><forenames>Lillian J.</forenames></author><author><keyname>Barreto</keyname><forenames>Carlos</forenames></author><author><keyname>Dong</keyname><forenames>Roy</forenames></author><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author><author><keyname>Cardenas</keyname><forenames>Alvaro</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Effects of Risk on Privacy Contracts for Demand-Side Management</title><categories>math.OC cs.CY</categories><comments>revised proposition 4; fixed typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As smart meters continue to be deployed around the world collecting
unprecedented levels of fine-grained data about consumers, we need to find
mechanisms that are fair to both, (1) the electric utility who needs the data
to improve their operations, and (2) the consumer who has a valuation of
privacy but at the same time benefits from sharing consumption data. In this
paper we address this problem by proposing privacy contracts between electric
utilities and consumers with the goal of maximizing the social welfare of both.
Our mathematical model designs an optimization problem between a population of
users that have different valuations on privacy and the costs of operation by
the utility. We then show how contracts can change depending on the probability
of a privacy breach. This line of research can help inform not only current but
also future smart meter collection practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7930</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7930</id><created>2014-09-28</created><updated>2015-02-09</updated><authors><author><keyname>Han</keyname><forenames>Weijia</forenames></author><author><keyname>Sang</keyname><forenames>Huiyan</forenames></author><author><keyname>Sheng</keyname><forenames>Min</forenames></author><author><keyname>Li</keyname><forenames>Jiandong</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Cognitive Learning of Statistical Primary Patterns via Bayesian Network</title><categories>cs.LG</categories><comments>This paper has been refreshed with a new version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio (CR) technology, the trend of sensing is no longer to only
detect the presence of active primary users. A large number of applications
demand for more comprehensive knowledge on primary user behaviors in spatial,
temporal, and frequency domains. To satisfy such requirements, we study the
statistical relationship among primary users by introducing a Bayesian network
(BN) based framework. How to learn such a BN structure is a long standing
issue, not fully understood even in the statistical learning community.
Besides, another key problem in this learning scenario is that the CR has to
identify how many variables are in the BN, which is usually considered as prior
knowledge in statistical learning applications. To solve such two issues
simultaneously, this paper proposes a BN structure learning scheme consisting
of an efficient structure learning algorithm and a blind variable
identification scheme. The proposed approach incurs significantly lower
computational complexity compared with previous ones, and is capable of
determining the structure without assuming much prior knowledge about
variables. With this result, cognitive users could efficiently understand the
statistical pattern of primary networks, such that more efficient cognitive
protocols could be designed across different network layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7935</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7935</id><created>2014-09-28</created><authors><author><keyname>Kuminski</keyname><forenames>Evan</forenames></author><author><keyname>George</keyname><forenames>Joe</forenames></author><author><keyname>Wallin</keyname><forenames>John</forenames></author><author><keyname>Shamir</keyname><forenames>Lior</forenames></author></authors><title>Combining human and machine learning for morphological analysis of
  galaxy images</title><categories>astro-ph.IM astro-ph.GA cs.CV cs.LG</categories><comments>PASP, accepted</comments><doi>10.1086/678977</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing importance of digital sky surveys collecting many millions of
galaxy images has reinforced the need for robust methods that can perform
morphological analysis of large galaxy image databases. Citizen science
initiatives such as Galaxy Zoo showed that large datasets of galaxy images can
be analyzed effectively by non-scientist volunteers, but since databases
generated by robotic telescopes grow much faster than the processing power of
any group of citizen scientists, it is clear that computer analysis is
required. Here we propose to use citizen science data for training machine
learning systems, and show experimental results demonstrating that machine
learning systems can be trained with citizen science data. Our findings show
that the performance of machine learning depends on the quality of the data,
which can be improved by using samples that have a high degree of agreement
between the citizen scientists. The source code of the method is publicly
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7936</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7936</id><created>2014-09-28</created><authors><author><keyname>Chacholski</keyname><forenames>Wojciech</forenames></author><author><keyname>Scolamiero</keyname><forenames>Martina</forenames></author><author><keyname>Vaccarino</keyname><forenames>Francesco</forenames></author></authors><title>Combinatorial presentation of multidimensional persistent homology</title><categories>math.AT cs.CG math.AC</categories><comments>21 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multifiltration is a functor indexed by $\mathbb{N}^r$ that maps any
morphism to a monomorphism. The goal of this paper is to describe in an
explicit and combinatorial way the natural $\mathbb{N}^r$-graded $R[x_1,\ldots,
x_r]$-module structure on the homology of a multifiltration of simplicial
complexes. To do that we study multifiltrations of sets and vector spaces. We
prove in particular that the $\mathbb{N}^r$-graded $R[x_1,\ldots, x_r]$-modules
that can occur as $R$-spans of multifiltrations of sets are the direct sums of
monomial ideals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7938</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7938</id><created>2014-09-28</created><updated>2014-11-28</updated><authors><author><keyname>Mirzasoleiman</keyname><forenames>Baharan</forenames></author><author><keyname>Badanidiyuru</keyname><forenames>Ashwinkumar</forenames></author><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Lazier Than Lazy Greedy</title><categories>cs.LG cs.DS cs.IR</categories><comments>In Proc. Conference on Artificial Intelligence (AAAI), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it possible to maximize a monotone submodular function faster than the
widely used lazy greedy algorithm (also known as accelerated greedy), both in
theory and practice? In this paper, we develop the first linear-time algorithm
for maximizing a general monotone submodular function subject to a cardinality
constraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can
achieve a $(1-1/e-\varepsilon)$ approximation guarantee, in expectation, to the
optimum solution in time linear in the size of the data and independent of the
cardinality constraint. We empirically demonstrate the effectiveness of our
algorithm on submodular functions arising in data summarization, including
training large-scale kernel methods, exemplar-based clustering, and sensor
placement. We observe that STOCHASTIC-GREEDY practically achieves the same
utility value as lazy greedy but runs much faster. More surprisingly, we
observe that in many practical scenarios STOCHASTIC-GREEDY does not evaluate
the whole fraction of data points even once and still achieves
indistinguishable results compared to lazy greedy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7948</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7948</id><created>2014-09-28</created><authors><author><keyname>Lao</keyname><forenames>Joe</forenames></author></authors><title>A network-dependent rewarding system: proof-of-mining</title><categories>cs.CY cs.CR</categories><comments>3 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A soft control of the network activity through varying reward in a
proof-of-work (PoW) cryptocurrency is reported. Rewards are the necessity to
incent the contributors activities (i.e., mining) in order to maintain the PoW
network. Contrary to constant rewarding in a certain period implemented in most
of cryptocurrency, such as bitcoin, we propose a network-dependent rewarding
model system, primarily including two phases: 1) activities encouraging phase
in which higher rewards are issued at higher network activities; and 2)
discouraging further increase of activities by reducing rewards. The advantages
of this system include 1) fair distribution of rewards among a variety of
contributors, and 2) enforcing a limit to the network activity and hence the
cost of maintaining the PoW network. This mechanism requires network
contributors to show their participation in order to earn maximum rewards,
i.e., proof-of-mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7963</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7963</id><created>2014-09-28</created><authors><author><keyname>Jain</keyname><forenames>Arjun</forenames></author><author><keyname>Tompson</keyname><forenames>Jonathan</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author><author><keyname>Bregler</keyname><forenames>Christoph</forenames></author></authors><title>MoDeep: A Deep Learning Framework Using Motion Features for Human Pose
  Estimation</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel and efficient method for articulated human
pose estimation in videos using a convolutional network architecture, which
incorporates both color and motion features. We propose a new human body pose
dataset, FLIC-motion, that extends the FLIC dataset with additional motion
features. We apply our architecture to this dataset and report significantly
better performance than current state-of-the-art pose detection systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7966</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7966</id><created>2014-09-28</created><authors><author><keyname>Rodriguez-Aseretto</keyname><forenames>Dario</forenames></author><author><keyname>Schaerer</keyname><forenames>Christian</forenames></author><author><keyname>de Rigo</keyname><forenames>Daniele</forenames></author></authors><title>Architecture of Environmental Risk Modelling: for a faster and more
  robust response to natural disasters</title><categories>cs.CE</categories><comments>12 pages, 1 figure, 1 text box, presented at the 3rd Conference of
  Computational Interdisciplinary Sciences (CCIS 2014), Asuncion, Paraguay</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demands on the disaster response capacity of the European Union are likely to
increase, as the impacts of disasters continue to grow both in size and
frequency. This has resulted in intensive research on issues concerning
spatially-explicit information and modelling and their multiple sources of
uncertainty. Geospatial support is one of the forms of assistance frequently
required by emergency response centres along with hazard forecast and event
management assessment. Robust modelling of natural hazards requires dynamic
simulations under an array of multiple inputs from different sources.
Uncertainty is associated with meteorological forecast and calibration of the
model parameters. Software uncertainty also derives from the data
transformation models (D-TM) needed for predicting hazard behaviour and its
consequences. On the other hand, social contributions have recently been
recognized as valuable in raw-data collection and mapping efforts traditionally
dominated by professional organizations. Here an architecture overview is
proposed for adaptive and robust modelling of natural hazards, following the
Semantic Array Programming paradigm to also include the distributed array of
social contributors called Citizen Sensor in a semantically-enhanced strategy
for D-TM modelling. The modelling architecture proposes a multicriteria
approach for assessing the array of potential impacts with qualitative rapid
assessment methods based on a Partial Open Loop Feedback Control (POLFC) schema
and complementing more traditional and accurate a-posteriori assessment. We
discuss the computational aspect of environmental risk modelling using
array-based parallel paradigms on High Performance Computing (HPC) platforms,
in order for the implications of urgency to be introduced into the systems
(Urgent-HPC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7971</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7971</id><created>2014-09-28</created><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Ketterl</keyname><forenames>Thomas P.</forenames></author><author><keyname>Arrobo</keyname><forenames>Gabriel E.</forenames></author><author><keyname>Gitlin</keyname><forenames>Richard D.</forenames></author></authors><title>Modeling In vivo Wireless Path Loss</title><categories>cs.OH</categories><doi>10.1109/IMWS-BIO.2014.7032404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our long-term research goal is to model the in vivo wireless channel. As a
first step towards this goal, in this paper we performed in vivo path loss
measurements at 2.4GHz and make a comparison with free space path loss. We
calculate the path loss by using the electric field radiated by a
Hertzian-Dipole located inside the abdominal cavity. The simulations quantify
and confirm that the path loss falls more rapidly inside the body than outside
the body. We also observe fluctuations of the path loss caused by the
inhomogeneity of the human body. In comparison with the path loss measured with
monopole antennas, we conclude that the significant variations in Received
Signal Strength is caused by both the angular dependent path loss and the
significantly modified in vivo antenna effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7978</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7978</id><created>2014-09-28</created><authors><author><keyname>Shao</keyname><forenames>Junming</forenames></author><author><keyname>Han</keyname><forenames>Zhichao</forenames></author><author><keyname>Yang</keyname><forenames>Qinli</forenames></author></authors><title>Community Detection via Local Dynamic Interaction</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 11 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we uncover the natural communities in a real network that allows
insight into its underlying structure and also potential functions? In this
paper, we introduce a new community detection algorithm, called Attractor,
which automatically spots the communities or groups in a network over time via
local dynamic interaction. The basic idea is to envision a network as a
dynamical system, and each agent interacts with its local partners. Instead of
investigating the node dynamics, we actually examine the change of &quot;distances&quot;
among linked nodes. As time evolves, these distances will be shrunk or
stretched gradually based on their topological structures. Finally all
distances among linked nodes will converge into a stable pattern, and
communities can be intuitively identified. Thanks to the dynamic viewpoint of
community detection, Attractor has several potential attractive properties: (a)
Attractor provides an intuitive solution to analyze the community structure of
a network, and faithfully captures the natural communities (with high quality).
(b) Owing to its time complexity $O(|E|)$, Attractor allows finding communities
on large networks. (c) The small communities or anomalies, usually existing in
real-world networks, can be well pinpointed. (d) Attractor is easy to
parameterize, since there is no need to specify the number of clusters.
Extensive experiments on synthetic and real-world networks further demonstrate
the effectiveness and efficiency of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7979</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7979</id><created>2014-09-28</created><authors><author><keyname>Berbeglia</keyname><forenames>Gerardo</forenames></author><author><keyname>Sloan</keyname><forenames>Peter</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author></authors><title>Bounds on the Profitability of a Durable Good Monopolist</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A durable good is a long-lasting good that can be consumed repeatedly over
time, and a duropolist is a monopolist in the market of a durable good.
Theoretically, less is known about durable goods than their more well-studied
counterparts, consumable and perishable goods. It was quite startling,
therefore, when Ronald Coase (1972) conjectured that a duropolist has no
monopoly power at all! Specifically, a duropolist who lacks commitment power
cannot sell the good above the competitive price if the time between periods
approaches zero. The Coase conjecture was first proved by Gul et al. (1986)
under an infinite time horizon model with non-atomic consumers. Remarkably, the
situation changes dramatically for atomic consumers and an infinite time
horizon. Bagnoli et al. (1989) showed the existence of a subgame perfect Nash
equilibrium where the duropolist extracts all the consumer surplus, provided
the discount factor is large enough. Thus, for atomic consumers, the duropolist
may have perfect price discriminatory power! Observe that, in these cases,
duropoly profits are either arbitrarily smaller or arbitrarily larger than the
corresponding static monopoly profits - the profit a monopolist for an
equivalent consumable good could generate. Neither situation accords in
practice with the profitability of durable good producers. Indeed we show that
the results of Gul et al. (1986) and Bagnoli et al. (1989) are driven by the
infinite time horizons. For finite time horizons, duropoly profits for any
equilibrium satisfying the standard skimming property closely relate to static
monopoly profits. In particular, for atomic agents, we prove that duropoly
profits are always at least as large as static monopoly profits, but never
exceed double the static monopoly profits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7984</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7984</id><created>2014-09-28</created><authors><author><keyname>Chen</keyname><forenames>Mingming</forenames></author><author><keyname>Zhao</keyname><forenames>Jichang</forenames></author><author><keyname>Liang</keyname><forenames>Xiao</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>Weighted Shortest Path Models: A Revisit to the Simulation of Internet
  Routing</title><categories>cs.NI</categories><comments>12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how packets are routed in Internet is significantly important
to Internet measurement and modeling. The conventional solution for route
simulation is based on the assumption of unweighted shortest path. However, it
has been found and widely accepted that a packet in Internet is usually not
transmitted along the unweighted shortest path between its source and
destination. To better simulate the routing behavior of a packet, we thoroughly
explore the real-world Internet routes and present a novel local information
based simulation model, with a tuning parameter, which assigns weights to links
based on local information and then simulates the Internet route with weighted
shortest path. Comparisons with baseline approaches show its capability in well
replicating the route length distribution and other structural properties of
the Internet topology. Meanwhile, the optimal parameter of this model locates
in the range of (0, 2), which implies that a packet inside the Internet
inclines to move to nodes with small degrees. This behavior actually reflects
the design philosophy of Internet routing, balancing between network efficiency
and traffic congestion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.7985</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.7985</id><created>2014-09-28</created><updated>2014-11-25</updated><authors><author><keyname>Sim</keyname><forenames>Yanchuan</forenames></author><author><keyname>Routledge</keyname><forenames>Bryan</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>The Utility of Text: The Case of Amicus Briefs and the Supreme Court</title><categories>cs.CL cs.AI cs.GT cs.LG</categories><comments>Working draft</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We explore the idea that authoring a piece of text is an act of maximizing
one's expected utility. To make this idea concrete, we consider the societally
important decisions of the Supreme Court of the United States. Extensive past
work in quantitative political science provides a framework for empirically
modeling the decisions of justices and how they relate to text. We incorporate
into such a model texts authored by amici curiae (&quot;friends of the court&quot;
separate from the litigants) who seek to weigh in on the decision, then
explicitly model their goals in a random utility model. We demonstrate the
benefits of this approach in improved vote prediction and the ability to
perform counterfactual analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8008</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8008</id><created>2014-09-29</created><authors><author><keyname>Das</keyname><forenames>Arjun</forenames></author><author><keyname>Garain</keyname><forenames>Utpal</forenames></author></authors><title>CRF-based Named Entity Recognition @ICON 2013</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes performance of CRF based systems for Named Entity
Recognition (NER) in Indian language as a part of ICON 2013 shared task. In
this task we have considered a set of language independent features for all the
languages. Only for English a language specific feature, i.e. capitalization,
has been added. Next the use of gazetteer is explored for Bengali, Hindi and
English. The gazetteers are built from Wikipedia and other sources. Test
results show that the system achieves the highest F measure of 88% for English
and the lowest F measure of 69% for both Tamil and Telugu. Note that for the
least performing two languages no gazetteer was used. NER in Bengali and Hindi
finds accuracy (F measure) of 87% and 79%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8018</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8018</id><created>2014-09-29</created><authors><author><keyname>Deepu</keyname><forenames>C. J.</forenames></author><author><keyname>Zhang</keyname><forenames>X.</forenames></author><author><keyname>Liew</keyname><forenames>W. -S.</forenames></author><author><keyname>Wong</keyname><forenames>D. L. T.</forenames></author><author><keyname>Lian</keyname><forenames>Y.</forenames></author></authors><title>An ECG-on-Chip with 535-nW/Channel Integrated Lossless Data Compressor
  for Wireless Sensors</title><categories>cs.AR</categories><journal-ref>IEEE Journal of Solid-State Circuits, Nov 2014</journal-ref><doi>10.1109/JSSC.2014.2349994</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a low-power ECG recording system-on-chip (SoC) with
on-chip low-complexity lossless ECG compression for data reduction in
wireless/ambulatory ECG sensor devices. The chip uses a linear slope predictor
for data compression, and incorporates a novel low-complexity dynamic
coding-packaging scheme to frame the prediction error into fixed-length 16-bit
format. The proposed technique achieves an average compression ratio of 2.25x
on MIT/BIH ECG database. Implemented in a standard 0.35 um process, the
compressor uses 0.565K gates/channel occupying 0.4 mm2 for four channels, and
consumes 535 nW/channel at 2.4 V for ECG sampled at 512 Hz. Small size and
ultra-low power consumption makes the proposed technique suitable for wearable
ECG sensor applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8020</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8020</id><created>2014-09-29</created><authors><author><keyname>Deepu</keyname><forenames>C. J.</forenames></author><author><keyname>Xu</keyname><forenames>X. Y.</forenames></author><author><keyname>Zou</keyname><forenames>X. D.</forenames></author><author><keyname>Yao</keyname><forenames>L. B.</forenames></author><author><keyname>Lian</keyname><forenames>Y.</forenames></author></authors><title>An ECG-on-Chip for Wearable Cardiac Monitoring Devices</title><categories>cs.AR</categories><journal-ref>5th IEEE International Symposium on Electronic Design Test and
  Applications 2010</journal-ref><doi>10.1109/DELTA.2010.43</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a highly integrated, low power chip solution for ECG
signal processing in wearable devices. The chip contains an instrumentation
amplifier with programmable gain, a band-pass filter, a 12-bit SAR ADC, a novel
QRS detector, 8K on-chip SRAM, and relevant control circuitry and CPU
interfaces. The analog front end circuits accurately senses and digitizes the
raw ECG signal, which is then filtered to extract the QRS. The sampling
frequency used is 256 Hz. ECG samples are buffered locally on an asynchronous
FIFO and is read out using a faster clock, as and when it is required by the
host CPU via an SPI interface. The chip was designed and implemented in 0.35um
standard CMOS process. The analog core operates at 1V while the digital
circuits and SRAM operate at 3.3V. The chip total core area is 5.74 mm^2 and
consumes 9.6uW. Small size and low power consumption make this design suitable
for usage in wearable heart monitoring devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8021</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8021</id><created>2014-09-29</created><authors><author><keyname>Deepu</keyname><forenames>Chacko John</forenames></author><author><keyname>Chen</keyname><forenames>Zhihao</forenames></author><author><keyname>Teo</keyname><forenames>Ju Teng</forenames></author><author><keyname>Ng</keyname><forenames>Soon Huat</forenames></author><author><keyname>Yang</keyname><forenames>Xiefeng</forenames></author><author><keyname>Lian</keyname><forenames>Yong</forenames></author></authors><title>A Smart Cushion for Real-Time Heart Rate Monitoring</title><categories>cs.OH</categories><comments>2012 IEEE Biomedical Circuits and Systems Conference</comments><doi>10.1109/BioCAS.2012.6418512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a smart cushion for real time heart rate monitoring. The
cushion comprises of an integrated micro-bending fiber sensor, which records
the BCG (Ballistocardiogram) signal without direct skin-electrode contact, and
an optical transceiver that does signal amplification, digitization, and
pre-filtering. To remove the artifacts and extract heart rate from BCG signal,
a computationally efficient heart rate detection algorithm is developed. The
system doesn't require any pre-training and is highly responsive with the
outputs updated every 3 sec and initial response within first 10 sec. Tests
conducted on human subjects show the detected heart rate closely matches the
one from a commercial SpO2 device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8027</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8027</id><created>2014-09-29</created><updated>2015-01-23</updated><authors><author><keyname>Wolff</keyname><forenames>J. Gerard</forenames></author></authors><title>Autonomous robots and the SP theory of intelligence</title><categories>cs.AI</categories><journal-ref>IEEE Access, 2, 1629-1651, 2014</journal-ref><doi>10.1109/ACCESS.2014.2382753</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is about how the &quot;SP theory of intelligence&quot; and its realisation
in the &quot;SP machine&quot; (both outlined in the article) may help to solve
computer-related problems in the design of autonomous robots, meaning robots
that do not depend on external intelligence or power supplies, are mobile, and
are designed to exhibit as much human-like intelligence as possible. The
article is about: how to increase the computational and energy efficiency of
computers and reduce their bulk; how to achieve human-like versatility in
intelligence; and likewise for human-like adaptability in intelligence. The SP
system has potential for substantial gains in computational and energy
efficiency and reductions in the bulkiness of computers: by reducing the size
of data to be processed; by exploiting statistical information that the system
gathers; and via an updated version of Donald Hebb's concept of a &quot;cell
assembly&quot;. Towards human-like versatility in intelligence, the SP system has
strengths in unsupervised learning, natural language processing, pattern
recognition, information retrieval, several kinds of reasoning, planning,
problem solving, and more, with seamless integration amongst structures and
functions. The SP system's strengths in unsupervised learning and other aspects
of intelligence may help to achieve human-like adaptability in intelligence
via: the learning of natural language; learning to see; building 3D models of
objects and of a robot's surroundings; learning regularities in the workings of
a robot and in the robot's environment; exploration and play; learning major
skills; and secondary forms of learning. Also discussed are: how the SP system
may process parallel streams of information; generalisation of knowledge,
correction of over-generalisations, and learning from dirty data; how to cut
the cost of learning; and reinforcements, motivations, goals, and
demonstration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8028</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8028</id><created>2014-09-29</created><authors><author><keyname>Raumer</keyname><forenames>Daniel</forenames></author><author><keyname>Fuchs</keyname><forenames>Christoph</forenames></author><author><keyname>Groh</keyname><forenames>Georg</forenames></author></authors><title>Reaching Consensus Among Mobile Agents: A Distributed Protocol for the
  Detection of Social Situations</title><categories>cs.SI cs.MA</categories><comments>16 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical social encounters are governed by a set of socio-psychological
behavioral rules with a high degree of uniform validity. Past research has
shown how these rules or the resulting properties of the encounters (e.g. the
geometry of interaction) can be used for algorithmic detection of social
interaction. In this paper, we present a distributed protocol to gain a common
understanding of the existing social situations among agents.
  Our approach allows a group of agents to combine their subjective assessment
of an ongoing social situation. Based on perceived social cues obtained from
raw data signals, they reach a consensus about the existence, parameters, and
participants of a social situation. We evaluate our protocol using two
real-world datasets with social interaction information and additional
synthetic data generated by our social-aware mobility model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8029</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8029</id><created>2014-09-29</created><updated>2015-12-28</updated><authors><author><keyname>Chatterjee</keyname><forenames>Arnab</forenames></author><author><keyname>Ghosh</keyname><forenames>Asim</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Bikas K</forenames></author></authors><title>Universality of citation distributions for academic institutions and
  journals</title><categories>physics.soc-ph cs.DL</categories><comments>9 pages, 6 figs + Supplementary information (8 pages, 7 fig, 6
  tables). Accepted in PLoS ONE</comments><journal-ref>PLOS ONE 11 (2016) e0146762</journal-ref><doi>10.1371/journal.pone.0146762</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citations measure the importance of a publication, and may serve as a proxy
for its popularity and quality of its contents. Here we study the distributions
of citations to publications from individual academic institutions for a single
year. The average number of citations have large variations between different
institutions across the world, but the probability distributions of citations
for individual institutions can be rescaled to a common form by scaling the
citations by the average number of citations for that institution. We find this
feature seem to be universal for a broad selection of institutions irrespective
of the average number of citations per article. A similar analysis for
citations to publications in a particular journal in a single year reveals
similar results. We find high absolute inequality for both these sets, Gini
coefficients being around $0.66$ and $0.58$ for institutions and journals
respectively. We also find that the top $25$% of the articles hold about $75$%
of the total citations for institutions and the top $29$% of the articles hold
about $71$% of the total citations for journals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8035</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8035</id><created>2014-09-29</created><updated>2014-10-10</updated><authors><author><keyname>Schwenk</keyname><forenames>Guido</forenames></author><author><keyname>Bach</keyname><forenames>Sebastian</forenames></author></authors><title>Detecting Behavioral and Structural Anomalies in MediaCloud Applications</title><categories>cs.CR</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past years technological advances such as the increasing bandwidth in
network infrastructures and new software developments such as message and
agent-based systems gave rise to the field of cloud technologies, which have
evolved from abstract concepts to concrete solutions, ranging from flexible,
platform-independent systems to highly specialized software solutions. In this
paper we introduce and evaluate two anomaly detection methods to achieve a
higher level of security in a specific cloud solution for interactive media,
the Media Cloud from Alcatel-Lucent. The Media Cloud focuses on real-time
processing of interactive media applications, allowing for optimal resource
planning using highly specific functional components. The proposed anomaly
detection methods are designed to work complimentary to each other and are
capable of detecting known and unknown vulnerabilities and security issues,
offering very low false positive rates and very high detection rates, as is
shown by the evaluation on real Media Cloud data and synthetic data. The
proposed methods use behavioral and structural features, and are capable of
locating the detected anomalies as well, giving the executing analyst easy
insight into the running processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8053</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8053</id><created>2014-09-29</created><authors><author><keyname>Wolff</keyname><forenames>J. Gerard</forenames></author></authors><title>Medical diagnosis as pattern recognition in a framework of information
  compression by multiple alignment, unification and search</title><categories>cs.AI</categories><journal-ref>Decision Support Systems 42, 608-625, 2006</journal-ref><doi>10.1016/j.dss.2005.02.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel approach to medical diagnosis based on the SP
theory of computing and cognition. The main attractions of this approach are: a
format for representing diseases that is simple and intuitive; an ability to
cope with errors and uncertainties in diagnostic information; the simplicity of
storing statistical information as frequencies of occurrence of diseases; a
method for evaluating alternative diagnostic hypotheses that yields true
probabilities; and a framework that should facilitate unsupervised learning of
medical knowledge and the integration of medical diagnosis with other AI
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8056</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8056</id><created>2014-09-29</created><updated>2014-11-02</updated><authors><author><keyname>Hirschowitz</keyname><forenames>Tom</forenames><affiliation>CNRS, Universit&#xe9; de Savoie</affiliation></author></authors><title>Full abstraction for fair testing in CCS (expanded version)</title><categories>cs.LO</categories><comments>80 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (October
  31, 2014) lmcs:1090</journal-ref><doi>10.2168/LMCS-10(4:2)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work with Pous, we defined a semantics for CCS which may both be
viewed as an innocent form of presheaf semantics and as a concurrent form of
game semantics. We define in this setting an analogue of fair testing
equivalence, which we prove fully abstract w.r.t. standard fair testing
equivalence. The proof relies on a new algebraic notion called playground,
which represents the `rule of the game'. From any playground, we derive two
languages equipped with labelled transition systems, as well as a strong,
functional bisimulation between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8061</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8061</id><created>2014-09-29</created><updated>2014-10-11</updated><authors><author><keyname>Liu</keyname><forenames>Kangqi</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>A New DoF Upper Bound and Its Achievability for $K$-User MIMO Y Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, submitted to IEEE ICC 2015. arXiv admin note:
  text overlap with arXiv:1405.0718</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is to study the degrees of freedom (DoF) for the $K$-user MIMO Y
channel. Previously, two transmission frameworks have been proposed for the DoF
analysis when $N \geq 2M$, where $M$ and $N$ denote the number of antennas at
each source node and the relay node respectively. The first method is named as
signal group based alignment proposed by Hua et al. in [1]. The second is named
as signal pattern approach introduced by Wang et al. in [2]. But both of them
only studied certain antenna configurations. The maximum achievable DoF in the
general case still remains unknown. In this work, we first derive a new upper
bound of the DoF using the genie-aided approach. Then, we propose a more
general transmission framework, generalized signal alignment (GSA), and show
that the previous two methods are both special cases of GSA. With GSA, we prove
that the new DoF upper bound is achievable when $\frac{N}{M} \in
\left(0,2+\frac{4}{K(K-1)}\right] \cup \left[K-2, +\infty\right)$. The DoF
analysis in this paper provides a major step forward towards the fundamental
capacity limit of the $K$-user MIMO Y channel. It also offers a new approach of
integrating interference alignment with physical layer network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8063</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8063</id><created>2014-09-29</created><updated>2014-12-25</updated><authors><author><keyname>Dadush</keyname><forenames>Daniel</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author><author><keyname>Stephens-Davidowitz</keyname><forenames>Noah</forenames></author></authors><title>On the Closest Vector Problem with a Distance Guarantee</title><categories>cs.DS cs.CC</categories><comments>An early version of the paper was titled &quot;On Bounded Distance
  Decoding and the Closest Vector Problem with Preprocessing&quot;. Conference on
  Computational Complexity (2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a substantially more efficient variant, both in terms of running
time and size of preprocessing advice, of the algorithm by Liu, Lyubashevsky,
and Micciancio for solving CVPP (the preprocessing version of the Closest
Vector Problem, CVP) with a distance guarantee. For instance, for any $\alpha &lt;
1/2$, our algorithm finds the (unique) closest lattice point for any target
point whose distance from the lattice is at most $\alpha$ times the length of
the shortest nonzero lattice vector, requires as preprocessing advice only $N
\approx \widetilde{O}(n \exp(\alpha^2 n /(1-2\alpha)^2))$ vectors, and runs in
time $\widetilde{O}(nN)$.
  As our second main contribution, we present reductions showing that it
suffices to solve CVP, both in its plain and preprocessing versions, when the
input target point is within some bounded distance of the lattice. The
reductions are based on ideas due to Kannan and a recent sparsification
technique due to Dadush and Kun. Combining our reductions with the LLM
algorithm gives an approximation factor of $O(n/\sqrt{\log n})$ for search
CVPP, improving on the previous best of $O(n^{1.5})$ due to Lagarias, Lenstra,
and Schnorr. When combined with our improved algorithm we obtain, somewhat
surprisingly, that only O(n) vectors of preprocessing advice are sufficient to
solve CVPP with (the only slightly worse) approximation factor of O(n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8072</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8072</id><created>2014-09-29</created><authors><author><keyname>Nicola</keyname><forenames>Mastronardi</forenames></author><author><keyname>Paul</keyname><forenames>Van Dooren</forenames></author></authors><title>Revisiting the stability of computing the roots of a quadratic
  polynomial</title><categories>math.NA cs.NA</categories><comments>13 pages</comments><msc-class>65H04</msc-class><acm-class>F.2.1; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show in this paper that the roots $x_1$ and $x_2$ of a scalar quadratic
polynomial $ax^2+bx+c=0$ with real or complex coefficients $a$, $b$ $c$ can be
computed in a element-wise mixed stable manner, measured in a relative sense.
We also show that this is a stronger property than norm-wise backward
stability, but weaker than element-wise backward stability. We finally show
that there does not exist any method that can compute the roots in an
element-wise backward stable sense, which is also illustrated by some numerical
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8083</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8083</id><created>2014-09-29</created><authors><author><keyname>Ermis</keyname><forenames>Beyza</forenames></author><author><keyname>Y&#x131;lmaz</keyname><forenames>Y. Kenan</forenames></author><author><keyname>Cemgil</keyname><forenames>A. Taylan</forenames></author><author><keyname>Acar</keyname><forenames>Evrim</forenames></author></authors><title>Variational Inference For Probabilistic Latent Tensor Factorization with
  KL Divergence</title><categories>stat.CO cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Latent Tensor Factorization (PLTF) is a recently proposed
probabilistic framework for modelling multi-way data. Not only the common
tensor factorization models but also any arbitrary tensor factorization
structure can be realized by the PLTF framework. This paper presents full
Bayesian inference via variational Bayes that facilitates more powerful
modelling and allows more sophisticated inference on the PLTF framework. We
illustrate our approach on model order selection and link prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8098</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8098</id><created>2014-09-29</created><authors><author><keyname>Jaradat</keyname><forenames>Ward</forenames></author><author><keyname>Dearle</keyname><forenames>Alan</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>Workflow Partitioning and Deployment on the Cloud using Orchestra</title><categories>cs.DC</categories><comments>To appear in Proceedings of the IEEE/ACM 7th International Conference
  on Utility and Cloud Computing (UCC 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orchestrating service-oriented workflows is typically based on a design model
that routes both data and control through a single point - the centralised
workflow engine. This causes scalability problems that include the unnecessary
consumption of the network bandwidth, high latency in transmitting data between
the services, and performance bottlenecks. These problems are highly prominent
when orchestrating workflows that are composed from services dispersed across
distant geographical locations. This paper presents a novel workflow
partitioning approach, which attempts to improve the scalability of
orchestrating large-scale workflows. It permits the workflow computation to be
moved towards the services providing the data in order to garner optimal
performance results. This is achieved by decomposing the workflow into smaller
sub workflows for parallel execution, and determining the most appropriate
network locations to which these sub workflows are transmitted and subsequently
executed. This paper demonstrates the efficiency of our approach using a set of
experimental workflows that are orchestrated over Amazon EC2 and across several
geographic network regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8104</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8104</id><created>2014-09-29</created><updated>2015-07-22</updated><authors><author><keyname>Luo</keyname><forenames>Shixin</forenames></author><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Lim</keyname><forenames>Teng Joon</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Capacity Region of MISO Broadcast Channel for Simultaneous Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>32 pages, 5 figures, submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a multiple-input single-output (MISO) broadcast channel
(BC) featuring simultaneous wireless information and power transfer (SWIPT),
where a multi-antenna access point (AP) delivers both information and energy
via radio signals to multiple single-antenna receivers simultaneously, and each
receiver implements either information decoding (ID) or energy harvesting (EH).
In particular, pseudo-random sequences that are {\it a priori} known and
therefore can be cancelled at each ID receiver is used as the energy signals,
and the information-theoretically optimal dirty paper coding (DPC) is employed
for the information transmission. We characterize the capacity region for ID
receivers under given energy requirements for EH receivers, by solving a
sequence of weighted sum-rate (WSR) maximization (WSRMax) problems subject to a
maximum sum-power constraint for the AP, and a set of minimum harvested power
constraints for individual EH receivers. The problem corresponds to a new form
of WSRMax problem in MISO-BC with combined maximum and minimum linear transmit
covariance constraints (MaxLTCCs and MinLTCCs), which differs from the
celebrated capacity region characterization problem for MISO-BC under a set of
MaxLTCCs only and is challenging to solve. By extending the general BC-multiple
access channel (MAC) duality, which is only applicable to WSRMax problems with
MaxLTCCs, and applying the ellipsoid method, we propose an efficient algorithm
to solve this problem globally optimally. Furthermore, we also propose two
suboptimal algorithms with lower complexity by assuming that the information
and energy signals are designed separately. Finally, numerical results are
provided to validate our proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8112</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8112</id><created>2014-09-29</created><authors><author><keyname>Kleinbort</keyname><forenames>Michal</forenames></author><author><keyname>Salzman</keyname><forenames>Oren</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>Efficient high-quality motion planning by fast all-pairs
  r-nearest-neighbors</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling-based motion-planning algorithms typically rely on nearest-neighbor
(NN) queries when constructing a roadmap. Recent results suggest that in
various settings NN queries may be the computational bottleneck of such
algorithms. Moreover, in several asymptotically-optimal algorithms these NN
queries are of a specific form: Given a set of points and a radius r report all
pairs of points whose distance is at most r. This calls for an
application-specific NN data structure tailored to efficiently answering this
type of queries. Randomly transformed grids (RTG) were recently proposed by
Aiger et al. as a tool to answer such queries and have been shown to outperform
common implementations of NN data structures in this context. In this work we
employ RTG for sampling-based motion-planning algorithms and describe an
efficient implementation of the approach. We show that for motion-planning, RTG
allow for faster convergence to high-quality solutions when compared with
existing NN data structures. Additionally, RTG enable significantly shorter
construction times for batched-PRM variants; specifically, we demonstrate a
speedup by a factor of two to three for some scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8125</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8125</id><created>2014-09-29</created><updated>2014-12-29</updated><authors><author><keyname>Utkovski</keyname><forenames>Zoran</forenames></author><author><keyname>Eftimov</keyname><forenames>Tome</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Random Access Protocols with Collision Resolution in a Noncoherent
  Setting</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures; EDIT: A version of this work has been submitted
  for publication in the IEEE Wireless Communication Letters Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless systems are increasingly used for Machine-Type Communication (MTC),
where the users sporadically send very short messages. In such a setting, the
overhead imposed by channel estimation is substantial, thereby demanding
noncoherent communication. In this paper we consider a noncoherent setup in
which users randomly access the medium to send short messages to a common
receiver. We propose a transmission scheme based on Gabor frames, where each
user has a dedicated codebook of M possible codewords, while the codebook
simultaneously serves as an ID for the user. The scheme is used as a basis for
a simple protocol for collision resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8133</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8133</id><created>2014-09-29</created><updated>2014-10-02</updated><authors><author><keyname>Bekos</keyname><forenames>Michael</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Veeramoni</keyname><forenames>Sankar</forenames></author></authors><title>The Maximum k-Differential Coloring Problem</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an $n$-vertex graph $G$ and two positive integers $d,k \in \mathbb{N}$,
the ($d,kn$)-differential coloring problem asks for a coloring of the vertices
of $G$ (if one exists) with distinct numbers from 1 to $kn$ (treated as
\emph{colors}), such that the minimum difference between the two colors of any
adjacent vertices is at least $d$. While it was known that the problem of
determining whether a general graph is ($2,n$)-differential colorable is
NP-complete, our main contribution is a complete characterization of bipartite,
planar and outerplanar graphs that admit ($2,kn$)-differential colorings. For
practical reasons, we consider also color ranges larger than $n$, i.e., $k &gt;
1$. We show that it is NP-complete to determine whether a graph admits a
($3,2n$)-differential coloring. The same negative result holds for the
($\lfloor 2n/3 \rfloor, 2n$-differential coloring problem, even in the case
where the input graph is planar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8135</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8135</id><created>2014-09-29</created><updated>2014-10-01</updated><authors><author><keyname>Stassiy</keyname><forenames>Igor</forenames></author></authors><title>A note on the Minimum Norm Point algorithm</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a provably more efficient implementation of the Minimum Norm Point
Algorithm conceived by Fujishige than the one presented in \cite{FUJI06}. The
algorithm solves the minimization problem for a class of functions known as
submodular. Many important functions, such as minimum cut in the graph, have
the so called submodular property \cite{FUJI82}. It is known that the problem
can also be efficiently solved in strongly polynomial time \cite{IWAT01},
however known theoretical bounds are far from being practical. We present an
improved implementation of the algorithm, for which unfortunately no worst case
bounds are know, but which performs very well in practice. With the
modifications presented, the algorithm performs an order of magnitude faster
for certain submodular functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8146</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8146</id><created>2014-09-29</created><authors><author><keyname>Jaber</keyname><forenames>Mohamad</forenames></author><author><keyname>Noureddine</keyname><forenames>Mohamad</forenames></author><author><keyname>Zaraket</keyname><forenames>Fadi A.</forenames></author></authors><title>From High-Level Modeling Towards Efficient and Trustworthy Circuits</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1109.5505 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behavior-Interaction-Priority (BIP) is a layered embedded system design and
verification framework that provides separation of functionality,
synchronization, and priority concerns to simplify system design and to
establish correctness by construction. The framework comes with a runtime
engine and a suite of verification tools that uses D-Finder and NuSMV as model
checkers. In this paper we provide a method and a supporting tool that takes a
BIP system and a set of invariants and computes a reduced sequential circuit
with a system-specific scheduler and with a designated output that is true when
the invariants hold. Our method uses ABC, a sequential circuit synthesis and
verification framework to (1) generate an efficient FPGA implementation of the
system, and to (2) verify the system and debug it in case a counterexample was
found. Moreover we generate a concurrent C implementation of the circuit that
can be directly used as a simulator. We evaluated our method with two large
systems and our results outperform those possible with existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8152</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8152</id><created>2014-09-29</created><authors><author><keyname>Mejova</keyname><forenames>Yelena</forenames></author><author><keyname>Zhang</keyname><forenames>Amy X.</forenames></author><author><keyname>Diakopoulos</keyname><forenames>Nicholas</forenames></author><author><keyname>Castillo</keyname><forenames>Carlos</forenames></author></authors><title>Controversy and Sentiment in Online News</title><categories>cs.CY cs.CL</categories><comments>Computation+Journalism Symposium 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  How do news sources tackle controversial issues? In this work, we take a
data-driven approach to understand how controversy interplays with emotional
expression and biased language in the news. We begin by introducing a new
dataset of controversial and non-controversial terms collected using
crowdsourcing. Then, focusing on 15 major U.S. news outlets, we compare
millions of articles discussing controversial and non-controversial issues over
a span of 7 months. We find that in general, when it comes to controversial
issues, the use of negative affect and biased language is prevalent, while the
use of strong emotion is tempered. We also observe many differences across news
sources. Using these findings, we show that we can indicate to what extent an
issue is controversial, by comparing it with other issues in terms of how they
are portrayed across different media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8171</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8171</id><created>2014-09-29</created><authors><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author><author><keyname>Shen</keyname><forenames>Huijie</forenames></author></authors><title>An Analysis of BitTorrent Cross-Swarm Peer Participation and
  Geolocational Distribution</title><categories>cs.NI cs.CY</categories><comments>The First International Workshop on Hot Topics in Big Data and
  Networking (HotData I)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-Peer (P2P) file-sharing is becoming increasingly popular in recent
years. In 2012, it was reported that P2P traffic consumed over 5,374 petabytes
per month, which accounted for approximately 20.5% of consumer internet
traffic. TV is the popular content type on The Pirate Bay (the world's largest
BitTorrent indexing website). In this paper, an analysis of the swarms of the
most popular pirated TV shows is conducted. The purpose of this data gathering
exercise is to enumerate the peer distribution at different geolocational
levels, to measure the temporal trend of the swarm and to discover the amount
of cross-swarm peer participation. Snapshots containing peer related
information involved in the unauthorised distribution of this content were
collected at a high frequency resulting in a more accurate landscape of the
total involvement. The volume of data collected throughout the monitoring of
the network exceeded 2 terabytes. The presented analysis and the results
presented can aid in network usage prediction, bandwidth provisioning and
future network design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8174</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8174</id><created>2014-09-29</created><authors><author><keyname>Farina</keyname><forenames>Jason</forenames></author><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author><author><keyname>Kechadi</keyname><forenames>M-Tahar</forenames></author></authors><title>BitTorrent Sync: First Impressions and Digital Forensic Implications</title><categories>cs.CR cs.NI</categories><comments>Proc. of Digtial Forensics Research Workshop (DFRWS EU 2014)</comments><journal-ref>Digital Investigation, Volume 11, Supplement 1, Pages S77-S86,
  (2014)</journal-ref><doi>10.1016/j.diin.2014.03.010</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With professional and home Internet users becoming increasingly concerned
with data protection and privacy, the privacy afforded by popular cloud file
synchronisation services, such as Dropbox, OneDrive and Google Drive, is coming
under scrutiny in the press. A number of these services have recently been
reported as sharing information with governmental security agencies without
warrants. BitTorrent Sync is seen as an alternative by many and has gathered
over two million users by December 2013 (doubling since the previous month).
The service is completely decentralised, offers much of the same
synchronisation functionality of cloud powered services and utilises encryption
for data transmission (and optionally for remote storage). The importance of
understanding BitTorrent Sync and its resulting digital investigative
implications for law enforcement and forensic investigators will be paramount
to future investigations. This paper outlines the client application, its
detected network traffic and identifies artefacts that may be of value as
evidence for future digital investigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8183</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8183</id><created>2014-09-29</created><authors><author><keyname>Lorenzen</keyname><forenames>Matthias</forenames></author><author><keyname>Allg&#xf6;wer</keyname><forenames>Frank</forenames></author><author><keyname>Dabbene</keyname><forenames>Fabrizio</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author></authors><title>An Improved Constraint-Tightening Approach for Stochastic MPC</title><categories>math.OC cs.SY</categories><comments>Paper has been submitted to ACC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of achieving a good trade-off in Stochastic Model Predictive
Control between the competing goals of improving the average performance and
reducing conservativeness, while still guaranteeing recursive feasibility and
low computational complexity, is addressed. We propose a novel, less
restrictive scheme which is based on considering stability and recursive
feasibility separately. Through an explicit first step constraint we guarantee
recursive feasibility. In particular we guarantee the existence of a feasible
input trajectory at each time instant, but we only require that the input
sequence computed at time $k$ remains feasible at time $k+1$ for most
disturbances but not necessarily for all, which suffices for stability. To
overcome the computational complexity of probabilistic constraints, we propose
an offline constraint-tightening procedure, which can be efficiently solved via
a sampling approach to the desired accuracy. The online computational
complexity of the resulting Model Predictive Control (MPC) algorithm is similar
to that of a nominal MPC with terminal region. A numerical example, which
provides a comparison with classical, recursively feasible Stochastic MPC and
Robust MPC, shows the efficacy of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8185</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8185</id><created>2014-09-29</created><updated>2015-09-11</updated><authors><author><keyname>Tsiligkaridis</keyname><forenames>Theodoros</forenames></author><author><keyname>Forsythe</keyname><forenames>Keith W.</forenames></author></authors><title>Adaptive Low-Complexity Sequential Inference for Dirichlet Process
  Mixture Models</title><categories>stat.ML cs.LG stat.ME</categories><comments>25 pages, To appear in Advances in Neural Information Processing
  Systems (NIPS) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a sequential low-complexity inference procedure for Dirichlet
process mixtures of Gaussians for online clustering and parameter estimation
when the number of clusters are unknown a-priori. We present an easily
computable, closed form parametric expression for the conditional likelihood,
in which hyperparameters are recursively updated as a function of the streaming
data assuming conjugate priors. Motivated by large-sample asymptotics, we
propose a novel adaptive low-complexity design for the Dirichlet process
concentration parameter and show that the number of classes grow at most at a
logarithmic rate. We further prove that in the large-sample limit, the
conditional likelihood and data predictive distribution become asymptotically
Gaussian. We demonstrate through experiments on synthetic and real data sets
that our approach is superior to other online state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8186</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8186</id><created>2014-09-29</created><authors><author><keyname>Thierry</keyname><forenames>Bertrand</forenames></author><author><keyname>Antoine</keyname><forenames>Xavier</forenames></author><author><keyname>Chniti</keyname><forenames>Chokri</forenames></author><author><keyname>Alzubaidi</keyname><forenames>Hasan</forenames></author></authors><title>$\mu$-diff: an open-source Matlab toolbox for computing multiple
  scattering problems by disks</title><categories>cs.MS</categories><comments>27 pages, 15 figures, associated code available online at
  http://mu-diff.math.cnrs.fr</comments><msc-class>35J05, 78A45, 78A48, 76Q05, 65M70, 31A10</msc-class><doi>10.1016/j.cpc.2015.03.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to describe a Matlab toolbox, called $\mu$-diff, for
modeling and numerically solving two-dimensional complex multiple scattering by
a large collection of circular cylinders. The approximation methods in
$\mu$-diff are based on the Fourier series expansions of the four basic
integral operators arising in scattering theory. Based on these expressions, an
efficient spectrally accurate finite-dimensional solution of multiple
scattering problems can be simply obtained for complex media even when many
scatterers are considered as well as large frequencies. The solution of the
global linear system to solve can use either direct solvers or preconditioned
iterative Krylov subspace solvers for block Toeplitz matrices. Based on this
approach, this paper explains how the code is built and organized. Some
complete numerical examples of applications (direct and inverse scattering) are
provided to show that $\mu$-diff is a flexible, efficient and robust toolbox
for solving some complex multiple scattering problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8191</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8191</id><created>2014-09-29</created><authors><author><keyname>Allesiardo</keyname><forenames>Robin</forenames></author><author><keyname>Feraud</keyname><forenames>Raphael</forenames></author><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>A Neural Networks Committee for the Contextual Bandit Problem</title><categories>cs.NE cs.LG</categories><comments>21st International Conference on Neural Information Processing</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new contextual bandit algorithm, NeuralBandit, which
does not need hypothesis on stationarity of contexts and rewards. Several
neural networks are trained to modelize the value of rewards knowing the
context. Two variants, based on multi-experts approach, are proposed to choose
online the parameters of multi-layer perceptrons. The proposed algorithms are
successfully tested on a large dataset with and without stationarity of
rewards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8196</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8196</id><created>2014-09-29</created><updated>2015-09-30</updated><authors><author><keyname>Farrell</keyname><forenames>Matthew</forenames></author><author><keyname>Goodrich</keyname><forenames>Timothy</forenames></author><author><keyname>Lemons</keyname><forenames>Nathan</forenames></author><author><keyname>Reidl</keyname><forenames>Felix</forenames></author><author><keyname>Villaamil</keyname><forenames>Fernando S&#xe1;nchez</forenames></author><author><keyname>Sullivan</keyname><forenames>Blair D.</forenames></author></authors><title>Hyperbolicity, degeneracy, and expansion of random intersection graphs</title><categories>cs.SI cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the conditions under which several algorithmically exploitable
structural features hold for random intersection graphs, a natural model for
many real-world networks where edges correspond to shared attributes.
Specifically, we fully characterize the degeneracy of random intersection
graphs, and prove that the model asymptotically almost surely produces graphs
with hyperbolicity at least $\log{n}$. Further, we prove that when degenerate,
the graphs generated by this model belong to a bounded-expansion graph class
with high probability, a property particularly suitable for the design of
linear time algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8202</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8202</id><created>2014-09-29</created><authors><author><keyname>De Felice</keyname><forenames>Matteo</forenames></author><author><keyname>Petitta</keyname><forenames>Marcello</forenames></author><author><keyname>Ruti</keyname><forenames>Paolo M.</forenames></author></authors><title>Short-Term Predictability of Photovoltaic Production over Italy</title><categories>cs.LG stat.AP</categories><comments>Submitted to Renewable Energy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photovoltaic (PV) power production increased drastically in Europe throughout
the last years. About the 6% of electricity in Italy comes from PV and for an
efficient management of the power grid an accurate and reliable forecasting of
production would be needed. Starting from a dataset of electricity production
of 65 Italian solar plants for the years 2011-2012 we investigate the
possibility to forecast daily production from one to ten days of lead time
without using on site measurements. Our study is divided in two parts: an
assessment of the predictability of meteorological variables using weather
forecasts and an analysis on the application of data-driven modelling in
predicting solar power production. We calibrate a SVM model using available
observations and then we force the same model with the predicted variables from
weather forecasts with a lead time from one to ten days. As expected, solar
power production is strongly influenced by cloudiness and clear sky, in fact we
observe that while during summer we obtain a general error under the 10%
(slightly lower in south Italy), during winter the error is abundantly above
the 20%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8211</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8211</id><created>2014-09-29</created><updated>2014-09-30</updated><authors><author><keyname>Kuksa</keyname><forenames>Pavel P.</forenames></author></authors><title>Efficient multivariate sequence classification</title><categories>cs.LG</categories><comments>multivariate sequence classification, string kernels, vector
  quantization, direct feature quantization, music classification, protein
  classification</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel-based approaches for sequence classification have been successfully
applied to a variety of domains, including the text categorization, image
classification, speech analysis, biological sequence analysis, time series and
music classification, where they show some of the most accurate results.
  Typical kernel functions for sequences in these domains (e.g., bag-of-words,
mismatch, or subsequence kernels) are restricted to {\em discrete univariate}
(i.e. one-dimensional) string data, such as sequences of words in the text
analysis, codeword sequences in the image analysis, or nucleotide or amino acid
sequences in the DNA and protein sequence analysis. However, original sequence
data are often of real-valued multivariate nature, i.e. are not univariate and
discrete as required by typical $k$-mer based sequence kernel functions.
  In this work, we consider the problem of the {\em multivariate} sequence
classification such as classification of multivariate music sequences, or
multidimensional protein sequence representations. To this end, we extend {\em
univariate} kernel functions typically used in sequence analysis and propose
efficient {\em multivariate} similarity kernel method (MVDFQ-SK) based on (1) a
direct feature quantization (DFQ) of each sequence dimension in the original
{\em real-valued} multivariate sequences and (2) applying novel multivariate
discrete kernel measures on these multivariate discrete DFQ sequence
representations to more accurately capture similarity relationships among
sequences and improve classification performance.
  Experiments using the proposed MVDFQ-SK kernel method show excellent
classification performance on three challenging music classification tasks as
well as protein sequence classification with significant 25-40% improvements
over univariate kernel methods and existing state-of-the-art sequence
classification methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8212</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8212</id><created>2014-09-29</created><authors><author><keyname>Karabat</keyname><forenames>Cagatay</forenames></author><author><keyname>Kiraz</keyname><forenames>Mehmet Sabir</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author><author><keyname>Savas</keyname><forenames>Erkay</forenames></author></authors><title>THRIVE: Threshold Homomorphic encryption based secure and privacy
  preserving bIometric VErification system</title><categories>cs.CR</categories><doi>10.1186/s13634-015-0255-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new biometric verification and template
protection system which we call the THRIVE system. The system includes novel
enrollment and authentication protocols based on threshold homomorphic
cryptosystem where the private key is shared between a user and the verifier.
In the THRIVE system, only encrypted binary biometric templates are stored in
the database and verification is performed via homomorphically randomized
templates, thus, original templates are never revealed during the
authentication stage. The THRIVE system is designed for the malicious model
where the cheating party may arbitrarily deviate from the protocol
specification. Since threshold homomorphic encryption scheme is used, a
malicious database owner cannot perform decryption on encrypted templates of
the users in the database. Therefore, security of the THRIVE system is enhanced
using a two-factor authentication scheme involving the user's private key and
the biometric data. We prove security and privacy preservation capability of
the proposed system in the simulation-based model with no assumption. The
proposed system is suitable for applications where the user does not want to
reveal her biometrics to the verifier in plain form but she needs to proof her
physical presence by using biometrics. The system can be used with any
biometric modality and biometric feature extraction scheme whose output
templates can be binarized. The overall connection time for the proposed THRIVE
system is estimated to be 336 ms on average for 256-bit biohash vectors on a
desktop PC running with quad-core 3.2 GHz CPUs at 10 Mbit/s up/down link
connection speed. Consequently, the proposed system can be efficiently used in
real life applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8220</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8220</id><created>2014-09-29</created><authors><author><keyname>Couvreur</keyname><forenames>Alain</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>M&#xe1;rquez-Corbella</keyname><forenames>Irene</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Pellikaan</keyname><forenames>Ruud</forenames></author></authors><title>Cryptanalysis of public-key cryptosystems that use subcodes of algebraic
  geometry codes</title><categories>cs.IT cs.CR math.AG math.IT</categories><proxy>ccsd</proxy><journal-ref>CIM-MS Series by Springer-Verlag (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial time attack on the McEliece public key cryptosystem
based on subcodes of algebraic geometry (AG) codes. The proposed attack reposes
on the distinguishability of such codes from random codes using the Schur
product. Wieschebrink treated the genus zero case a few years ago but his
approach cannot be extent straightforwardly to other genera. We address this
problem by introducing and using a new notion, which we call the t-closure of a
code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8225</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8225</id><created>2014-09-29</created><updated>2015-01-31</updated><authors><author><keyname>Le</keyname><forenames>Ngoc Khuyen</forenames><affiliation>LTCI</affiliation></author><author><keyname>Martins</keyname><forenames>Philippe</forenames><affiliation>LTCI</affiliation></author><author><keyname>Decreusefond</keyname><forenames>Laurent</forenames><affiliation>LTCI</affiliation></author><author><keyname>Vergne</keyname><forenames>Anais</forenames><affiliation>LTCI</affiliation></author></authors><title>Construction of the generalized Cech complex</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an algorithm which constructs the generalized
Cech complex. The generalized Cech complex represents the topology of a
wireless network whose cells are different in size. This complex is often used
in many application to locate the boundary holes or to save energy consumption
in wireless networks. The complexity of a construction of the Cech complex to
analyze the coverage structure is found to be a polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8227</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8227</id><created>2014-09-29</created><authors><author><keyname>Batkovich</keyname><forenames>D.</forenames></author><author><keyname>Kirienko</keyname><forenames>Yu.</forenames></author><author><keyname>Kompaniets</keyname><forenames>M.</forenames></author><author><keyname>Novikov</keyname><forenames>S.</forenames></author></authors><title>GraphState - a tool for graph identification and labelling</title><categories>hep-ph cs.OH hep-th</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present python libraries for Feynman graphs manipulation. The key feature
of these libraries is usage of generalization of graph representation offered
by B. G. Nickel et al. In this approach graph is represented in some unique
'canonical' form that depends only on its combinatorial type. The uniqueness of
graph representation gives an efficient way for isomorphism finding, searching
for subgraphs and other graph manipulation tasks. Though offered libraries were
originally designed for Feynman graphs, they might be useful for more general
graph problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8228</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8228</id><created>2014-09-21</created><updated>2015-04-21</updated><authors><author><keyname>Haase</keyname><forenames>Christoph</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author></authors><title>The Odds of Staying on Budget</title><categories>cs.CC cs.DM cs.LO</categories><comments>Technical report for an ICALP'15 paper. 30 pages, 1 figure</comments><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given Markov chains and Markov decision processes (MDPs) whose transitions
are labelled with non-negative integer costs, we study the computational
complexity of deciding whether the probability of paths whose accumulated cost
satisfies a Boolean combination of inequalities exceeds a given threshold. For
acyclic Markov chains, we show that this problem is PP-complete, whereas it is
hard for the PosSLP problem and in PSPACE for general Markov chains. Moreover,
for acyclic and general MDPs, we prove PSPACE- and EXP-completeness,
respectively. Our results have direct implications on the complexity of
computing reward quantiles in succinctly represented stochastic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8230</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8230</id><created>2014-09-29</created><authors><author><keyname>Anaya</keyname><forenames>Josue</forenames></author><author><keyname>Barbu</keyname><forenames>Adrian</forenames></author></authors><title>RENOIR - A Benchmark Dataset for Real Noise Reduction Evaluation</title><categories>cs.CV</categories><comments>8 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a dataset of uncompressed color images taken with
three digital cameras and exhibiting different levels of natural noise due to
low-light conditions. For each scene there are on average two low-noise and two
high noise images that are aligned at the pixel level both spatially and in
intensity. The dataset contains over 100 scenes and more than 400 images,
including both RAW formatted images and 8 bit BMP pixel and intensity aligned
images. We also introduce a method for estimating the true noise level in each
of our images. We use our dataset to analyze three current state of the art
denoising algorithms: Active Random Field, BM3D, and Multi-Layer Perceptron. We
found that BM3D obtains the best denoising results, however it is the slowest
of the three methods with Active Random Field taking only a few seconds and
Multi-Later Perceptron and BM3D taking a few minutes to half an hour to denoise
a 10 to 18 mega-pixel image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8235</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8235</id><created>2014-09-29</created><authors><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Shur</keyname><forenames>Arseny M.</forenames></author></authors><title>On Searching Zimin Patterns</title><categories>cs.DM math.CO</categories><comments>15 pages, 1 figure, 2 tables; submitted to Theoretical Computer
  Science (05.2014)</comments><msc-class>68R15, 68W32</msc-class><journal-ref>Theoretical Computer Science, Vol. 571 (2015), 50-57</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the area of pattern avoidability the central role is played by special
words called Zimin patterns. The symbols of these patterns are treated as
variables and the rank of the pattern is its number of variables. Zimin type of
a word $x$ is introduced here as the maximum rank of a Zimin pattern matching
$x$. We show how to compute Zimin type of a word on-line in linear time.
Consequently we get a quadratic time, linear-space algorithm for searching
Zimin patterns in words. Then we how the Zimin type of the length $n$ prefix of
the infinite Fibonacci word is related to the representation of $n$ in the
Fibonacci numeration system. Using this relation, we prove that Zimin types of
such prefixes and Zimin patterns inside them can be found in logarithmic time.
Finally, we give some bounds on the function $f(n,k)$ such that every $k$-ary
word of length at least $f(n,k)$ has a factor that matches the rank $n$ Zimin
pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8239</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8239</id><created>2014-09-29</created><authors><author><keyname>Agrawal</keyname><forenames>Tanmay</forenames></author><author><keyname>Shirwadkar</keyname><forenames>Ashay</forenames></author><author><keyname>Gaikar</keyname><forenames>Pratik</forenames></author><author><keyname>Verma</keyname><forenames>Kushagra</forenames></author></authors><title>MetaCache: Efficient Metadata Caching in Linux file system</title><categories>cs.SY</categories><comments>4 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, Linux file systems have to manage millions of tiny files for
different applications, and face with higher metadata operations. So how to
provide such high metadata performance with such enormous number of files and
large scale directories is a big challenge for Linux file system. We viewed
that metadata lookup operations dominate metadata workload and incur low
metadata performance. In this paper, we present a metadata cache to accelerate
metadata access for Linux file system. Through this optimization, the Linux
file system (such as EXT2, EXT4, BTRFS, etc.) can gain improvement in read
rates as well as write rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8252</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8252</id><created>2014-09-29</created><updated>2014-09-30</updated><authors><author><keyname>Han</keyname><forenames>Tao</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>Provisioning Green Energy for Base Stations in Heterogeneous Networks</title><categories>cs.NI</categories><report-no>TR-ANL-2014-006</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular networks are among the major energy hoggers of communication
networks, and their contributions to the global energy consumption increase
rapidly due to the surges of data traffic. With the development of green energy
technologies, base stations (BSs) can be powered by green energy in order to
reduce the on-grid energy consumption, and subsequently reduce the carbon
footprints. However, equipping a BS with a green energy system incurs
additional capital expenditure (CAPEX) that is determined by the size of the
green energy generator, the battery capacity, and other installation expenses.
In this paper, we introduce and investigate the green energy provisioning (GEP)
problem which aims to minimize the CAPEX of deploying green energy systems in
BSs while satisfying the QoS requirements of cellular networks. The GEP problem
is challenging because it involves the optimization over multiple time slots
and across multiple BSs. We decompose the GEP problem into the weighted energy
minimization problem and the green energy system sizing problem, and propose a
green energy provisioning solution consisting of the provision cost aware
traffic load balancing algorithm and the binary energy system sizing algorithm
to solve the sub-problems and subsequently solve the GEP problem. We validate
the performance and the viability of the proposed green energy provisioning
solution through extensive simulations, which also conform to our analytically
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8254</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8254</id><created>2014-07-04</created><authors><author><keyname>Valeyev</keyname><forenames>Rustem</forenames></author></authors><title>About accuracy of the solution of NP-complete tasks</title><categories>cs.CC</categories><comments>11 pages, 8 figures</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On example of tasks of class NP the questions concerning accuracy of work of
already existing and possible in the future algorithms for the solution of
tasks on discrete structures are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8267</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8267</id><created>2014-09-29</created><authors><author><keyname>Han</keyname><forenames>Tao</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>Network Utility Aware Traffic Loading Balancing in Backhaul-constrained
  Cache-enabled Small Cell Networks with Hybrid Power Supplies</title><categories>cs.NI</categories><report-no>TR-ANL-2014-007</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explosive data traffic growth leads to a continuous surge in capacity demands
across mobile networks. In order to provision high network capacity, small cell
base stations (SCBSs) are widely deployed. Owing to the close proximity to
mobile users, SCBSs can effectively enhance the network capacity and offloading
traffic load from macro BSs (MBSs). However, the cost-effective backhaul may
not be readily available for SCBSs, thus leading to backhaul constraints in
small cell networks (SCNs). Enabling cache in BSs may mitigate the backhaul
constraints in SCNs. Moreover, the dense deployment of SCBSs may incur
excessive energy consumption. To alleviate brown power consumption, renewable
energy will be explored to power BSs. In such a network, it is challenging to
dynamically balance traffic load among BSs to optimize the network utilities.
In this paper, we investigate the traffic load balancing in
backhaul-constrained cache-enabled small cell networks powered by hybrid energy
sources. We have proposed a network utility aware (NUA) traffic load balancing
scheme that optimizes user association to strike a tradeoff between the green
power utilization and the traffic delivery latency. On balancing the traffic
load, the proposed NUA traffic load balancing scheme considers the green power
utilization, the traffic delivery latency in both BSs and their backhaul, and
the cache hit ratio. The NUA traffic load balancing scheme allows dynamically
adjusting the tradeoff between the green power utilization and the traffic
delivery latency. We have proved the convergence and the optimality of the
proposed NUA traffic load balancing scheme. Through extensive simulations, we
have compared performance of the NUA traffic load balancing scheme with other
schemes and showed its advantages in backhaul-constrained cache-enabled small
cell networks with hybrid power supplies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8268</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8268</id><created>2014-09-29</created><updated>2015-02-11</updated><authors><author><keyname>Banerjee</keyname><forenames>Soumya Jyoti</forenames></author><author><keyname>Sinha</keyname><forenames>Saptarshi</forenames></author><author><keyname>Roy</keyname><forenames>Soumen</forenames></author></authors><title>Slow poisoning and destruction of networks: Edge proximity and its
  implications for biological and infrastructure networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-bio.MN</categories><comments>5 Pages, 5 figures, Revtex-4.1</comments><journal-ref>Phys. Rev. E 91, 022807 (2015)</journal-ref><doi>10.1103/PhysRevE.91.022807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a network metric, edge proximity, ${\cal P}_e$, which demonstrates
the importance of specific edges in a network, hitherto not captured by
existing network metrics. The effects of removing edges with high ${\cal P}_e$
might initially seem inconspicuous but are eventually shown to be very harmful
for networks. Compared to existing strategies, the removal of edges by ${\cal
P}_e$ leads to a remarkable increase in the diameter and average shortest path
length in undirected real and random networks till the first disconnection and
well beyond. ${\cal P}_e$ can be consistently used to rupture the network into
two nearly equal parts, thus presenting a very potent strategy to greatly harm
a network. Targeting by ${\cal P}_e$ causes notable efficiency loss in U.S. and
European power grid networks. ${\cal P}_e$ identifies proteins with essential
cellular functions in protein-protein interaction networks. It pinpoints
regulatory neural connections and important portions of the neural and brain
networks, respectively. Energy flow interactions identified by ${\cal P}_e$
form the backbone of long food web chains. Finally, we scrutinize the potential
of ${\cal P}_e$ in edge controllability dynamics of directed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8276</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8276</id><created>2014-09-29</created><authors><author><keyname>Ermis</keyname><forenames>Beyza</forenames></author><author><keyname>Cemgil</keyname><forenames>A. Taylan</forenames></author></authors><title>A Bayesian Tensor Factorization Model via Variational Inference for Link
  Prediction</title><categories>cs.LG cs.NA stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:1409.8083</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic approaches for tensor factorization aim to extract meaningful
structure from incomplete data by postulating low rank constraints. Recently,
variational Bayesian (VB) inference techniques have successfully been applied
to large scale models. This paper presents full Bayesian inference via VB on
both single and coupled tensor factorization models. Our method can be run even
for very large models and is easily implemented. It exhibits better prediction
performance than existing approaches based on maximum likelihood on several
real-world datasets for missing link prediction problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8277</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8277</id><created>2014-09-29</created><updated>2015-08-31</updated><authors><author><keyname>Vanli</keyname><forenames>N. Denizcan</forenames></author><author><keyname>Sayin</keyname><forenames>Muhammed O.</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>Stochastic Subgradient Algorithms for Strongly Convex Optimization over
  Distributed Networks</title><categories>cs.NA cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study diffusion and consensus based optimization of a sum of unknown
convex objective functions over distributed networks. The only access to these
functions is through stochastic gradient oracles, each of which is only
available at a different node, and a limited number of gradient oracle calls is
allowed at each node. In this framework, we introduce a convex optimization
algorithm based on the stochastic gradient descent (SGD) updates. Particularly,
we use a carefully designed time-dependent weighted averaging of the SGD
iterates, which yields a convergence rate of
$O\left(\frac{N\sqrt{N}}{T}\right)$ after $T$ gradient updates for each node on
a network of $N$ nodes. We then show that after $T$ gradient oracle calls, the
average SGD iterate achieves a mean square deviation (MSD) of
$O\left(\frac{\sqrt{N}}{T}\right)$. This rate of convergence is optimal as it
matches the performance lower bound up to constant terms. Similar to the SGD
algorithm, the computational complexity of the proposed algorithm also scales
linearly with the dimensionality of the data. Furthermore, the communication
load of the proposed method is the same as the communication load of the SGD
algorithm. Thus, the proposed algorithm is highly efficient in terms of
complexity and communication load. We illustrate the merits of the algorithm
with respect to the state-of-art methods over benchmark real life data sets and
widely studied network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8280</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8280</id><created>2014-09-29</created><authors><author><keyname>Fass</keyname><forenames>Didier</forenames><affiliation>LORIA</affiliation></author></authors><title>Reclaiming human machine nature</title><categories>cs.HC</categories><comments>Published in HCI International 2014, Heraklion : Greece (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extending and modifying his domain of life by artifact production is one of
the main characteristics of humankind. From the first hominid, who used a wood
stick or a stone for extending his upper limbs and augmenting his gesture
strength, to current systems engineers who used technologies for augmenting
human cognition, perception and action, extending human body capabilities
remains a big issue. From more than fifty years cybernetics, computer and
cognitive sciences have imposed only one reductionist model of human machine
systems: cognitive systems. Inspired by philosophy, behaviorist psychology and
the information treatment metaphor, the cognitive system paradigm requires a
function view and a functional analysis in human systems design process.
According that design approach, human have been reduced to his metaphysical and
functional properties in a new dualism. Human body requirements have been left
to physical ergonomics or &quot;physiology&quot;. With multidisciplinary convergence, the
issues of &quot;human-machine&quot; systems and &quot;human artifacts&quot; evolve. The loss of
biological and social boundaries between human organisms and interactive and
informational physical artifact questions the current engineering methods and
ergonomic design of cognitive systems. New developpment of human machine
systems for intensive care, human space activities or bio-engineering sytems
requires grounding human systems design on a renewed epistemological framework
for future human systems model and evidence based &quot;bio-engineering&quot;. In that
context, reclaiming human factors, augmented human and human machine nature is
a necessity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8309</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8309</id><created>2014-09-29</created><authors><author><keyname>Hassan</keyname><forenames>Youssef</forenames></author><author><keyname>Aly</keyname><forenames>Mohamed</forenames></author><author><keyname>Atiya</keyname><forenames>Amir</forenames></author></authors><title>Arabic Spelling Correction using Supervised Learning</title><categories>cs.LG cs.CL</categories><comments>System description paper that is submitted in the EMNLP 2014
  conference shared task &quot;Automatic Arabic Error Correction&quot; (Mohit et al.,
  2014) in the Arabic NLP workshop. 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the problem of spelling correction in the Arabic
language utilizing the new corpus provided by QALB (Qatar Arabic Language Bank)
project which is an annotated corpus of sentences with errors and their
corrections. The corpus contains edit, add before, split, merge, add after,
move and other error types. We are concerned with the first four error types as
they contribute more than 90% of the spelling errors in the corpus. The
proposed system has many models to address each error type on its own and then
integrating all the models to provide an efficient and robust system that
achieves an overall recall of 0.59, precision of 0.58 and F1 score of 0.58
including all the error types on the development set. Our system participated
in the QALB 2014 shared task &quot;Automatic Arabic Error Correction&quot; and achieved
an F1 score of 0.6, earning the sixth place out of nine participants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8310</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8310</id><created>2014-09-29</created><authors><author><keyname>Czaja</keyname><forenames>Wojciech</forenames></author><author><keyname>Tanis</keyname><forenames>James</forenames></author></authors><title>Kaczmarz Algorithm and Frames</title><categories>cs.IT math.IT</categories><comments>International Journal of Wavelets, Multiresolution and Information
  Processing, Vol. 11, no. 05, 2013</comments><msc-class>22E46, 53C35, 57S20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequences of unit vectors for which the Kaczmarz algorithm always converges
in Hilbert space can be characterized in frame theory by tight frames with
constant 1. We generalize this result to the context of frames and bases. In
particular, we show that the only effective sequences which are Riesz bases are
orthonormal bases. Moreover, we consider the infinite system of linear
algebraic equations $A x = b$ and characterize the (bounded) matrices $A$ for
which the Kaczmarz algorithm always converges to a solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8318</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8318</id><created>2014-09-29</created><updated>2015-12-09</updated><authors><author><keyname>Beyer</keyname><forenames>Stephan</forenames></author><author><keyname>Chimani</keyname><forenames>Markus</forenames></author></authors><title>Strong Steiner Tree Approximations in Practice</title><categories>cs.DS</categories><comments>33 pages, 7 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this experimental study we consider Steiner tree approximations that
guarantee a constant approximation of ratio smaller than $2$. The considered
greedy algorithms and approaches based on linear programming involve the
incorporation of $k$-restricted full components for some $k \geq 3$. For most
of the algorithms, their strongest theoretical approximation bounds are only
achieved for $k \to \infty$. However, the running time is also exponentially
dependent on $k$, so only small $k$ are tractable in practice.
  We investigate different implementation aspects and parameter choices that
finally allow us to construct algorithms (somewhat) feasible for practical use.
We compare the algorithms against each other, to an exact LP-based algorithm,
and to fast and simple $2$-approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8324</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8324</id><created>2014-09-29</created><updated>2015-04-26</updated><authors><author><keyname>Eyal</keyname><forenames>Ittay</forenames></author><author><keyname>Birman</keyname><forenames>Ken</forenames></author><author><keyname>van Renesse</keyname><forenames>Robbert</forenames></author></authors><title>Cache Serializability: Reducing Inconsistency in Edge Transactions</title><categories>cs.DC</categories><comments>Ittay Eyal, Ken Birman, Robbert van Renesse, &quot;Cache Serializability:
  Reducing Inconsistency in Edge Transactions,&quot; Distributed Computing Systems
  (ICDCS), IEEE 35th International Conference on, June~29 2015--July~2 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Read-only caches are widely used in cloud infrastructures to reduce access
latency and load on backend databases. Operators view coherent caches as
impractical at genuinely large scale and many client-facing caches are updated
in an asynchronous manner with best-effort pipelines. Existing solutions that
support cache consistency are inapplicable to this scenario since they require
a round trip to the database on every cache transaction.
  Existing incoherent cache technologies are oblivious to transactional data
access, even if the backend database supports transactions. We propose T-Cache,
a novel caching policy for read-only transactions in which inconsistency is
tolerable (won't cause safety violations) but undesirable (has a cost). T-Cache
improves cache consistency despite asynchronous and unreliable communication
between the cache and the database. We define cache-serializability, a variant
of serializability that is suitable for incoherent caches, and prove that with
unbounded resources T-Cache implements this new specification. With limited
resources, T-Cache allows the system manager to choose a trade-off between
performance and consistency.
  Our evaluation shows that T-Cache detects many inconsistencies with only
nominal overhead. We use synthetic workloads to demonstrate the efficacy of
T-Cache when data accesses are clustered and its adaptive reaction to workload
changes. With workloads based on the real-world topologies, T-Cache detects
43-70% of the inconsistencies and increases the rate of consistent transactions
by 33-58%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8325</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8325</id><created>2014-09-29</created><authors><author><keyname>Huang</keyname><forenames>Xueqing</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>RF Energy Harvesting Enabled Power Sharing in Relay Networks</title><categories>cs.IT math.IT</categories><comments>An abbreviated version will be presented at IEEE online GreenComm,
  Nov., 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through simultaneous energy and information transfer, radio frequency (RF)
energy harvesting (EH) reduces the energy consumption of the wireless networks.
It also provides a new approach for the wireless devices to share each other's
energy storage, without relying on the power grid or traffic offloading. In
this paper, we study RF energy harvesting enabled power balancing within the
decode-and-forward (DF) relaying-enhanced cooperative wireless system. An
optimal power allocation policy is proposed for the scenario where both source
and relay nodes can draw power from the radio frequency signals transmitted by
each other. To maximize the overall throughput while meeting the energy
constraints imposed by the RF sources, an optimization problem is formulated
and solved. Based on different harvesting efficiency and channel condition,
closed form solutions for optimal joint source and relay power allocation are
derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8327</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8327</id><created>2014-09-29</created><authors><author><keyname>Prando</keyname><forenames>Giulia</forenames></author><author><keyname>Chiuso</keyname><forenames>Alessandro</forenames></author><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author></authors><title>Bayesian and regularization approaches to multivariable linear system
  identification: the role of rank penalties</title><categories>cs.SY cs.LG stat.ML</categories><comments>to appear in IEEE Conference on Decision and Control, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in linear system identification have proposed the use of
non-parameteric methods, relying on regularization strategies, to handle the
so-called bias/variance trade-off. This paper introduces an impulse response
estimator which relies on an $\ell_2$-type regularization including a
rank-penalty derived using the log-det heuristic as a smooth approximation to
the rank function. This allows to account for different properties of the
estimated impulse response (e.g. smoothness and stability) while also
penalizing high-complexity models. This also allows to account and enforce
coupling between different input-output channels in MIMO systems. According to
the Bayesian paradigm, the parameters defining the relative weight of the two
regularization terms as well as the structure of the rank penalty are estimated
optimizing the marginal likelihood. Once these hyperameters have been
estimated, the impulse response estimate is available in closed form.
Experiments show that the proposed method is superior to the estimator relying
on the &quot;classic&quot; $\ell_2$-regularization alone as well as those based in atomic
and nuclear norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8332</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8332</id><created>2014-09-29</created><updated>2015-10-19</updated><authors><author><keyname>Martin</keyname><forenames>Samuel</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author></authors><title>Continuous-Time Consensus under Non-Instantaneous Reciprocity</title><categories>cs.SY</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider continuous-time consensus systems whose interactions satisfy a
form or reciprocity that is not instantaneous, but happens over time. We show
that these systems have certain desirable properties: They always converge
independently of the specific interactions taking place and there exist simple
conditions on the interactions for two agents to converge to the same value.
This was until now only known for systems with instantaneous reciprocity. These
result are of particular relevance when analyzing systems where interactions
are a priori unknown, being for example endogenously determined or random. We
apply our results to an instance of such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8352</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8352</id><created>2014-09-29</created><updated>2014-12-02</updated><authors><author><keyname>Lin</keyname><forenames>Chi-Heng</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Lin</keyname><forenames>Chih-Chung</forenames></author><author><keyname>Liao</keyname><forenames>Wanjiun</forenames></author></authors><title>Multicast Group Management for Multi-View 3D Videos in Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence of 3D mobile devices available in the markets, mobile 3D
video services become increasingly important for video service providers, such
as Youtube and Netflix, while multi-view 3D videos are potential to bring out
varied innovative applications. However, enabling multi-view 3D video services
may overwhelm WiFi networks when we multicast every view of a video. In this
paper, therefore, we propose to incorporate depth-image-based rendering (DIBR),
which allows each mobile client to synthesize the desired view from nearby left
and right views, to effectively reduce the bandwidth consumption. Moreover, due
to varied channel conditions, each client may suffer from different packet loss
probabilities, and retransmissions incur additional bandwidth consumption. To
address this issue, we first analyze the merit of view protection via DIBR for
multi-view video multicast and then design a new protocol, named Multi-View
Group Management Protocol (MVGMP), for the dynamic group management of
multicast users. Simulation results manifest that our protocol effectively
reduces bandwidth consumption and increases the probability for each client to
successfully playback the desired view of a multi-view 3D video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8359</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8359</id><created>2014-09-29</created><updated>2015-10-14</updated><authors><author><keyname>Jain</keyname><forenames>Swayambhoo</forenames></author><author><keyname>Kim</keyname><forenames>Seung-Jun</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Backhaul-Constrained Multi-Cell Cooperation Leveraging Sparsity and
  Spectral Clustering</title><categories>cs.IT cs.NI math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-cell cooperative processing with limited backhaul traffic is studied
for cellular uplinks. Aiming at reduced backhaul overhead, a
sparsity-regularized multi-cell receive-filter design problem is formulated.
Both unstructured distributed cooperation as well as clustered cooperation, in
which base station groups are formed for tight cooperation, are considered.
Dynamic clustered cooperation, where the sparse equalizer and the cooperation
clusters are jointly determined, is solved via alternating minimization based
on spectral clustering and group-sparse regression. Furthermore, decentralized
implementations of both unstructured and clustered cooperation schemes are
developed for scalability, robustness and computational efficiency. Extensive
numerical tests verify the efficacy of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8370</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8370</id><created>2014-09-29</created><updated>2015-02-03</updated><authors><author><keyname>Ozdemir</keyname><forenames>O.</forenames></author><author><keyname>Wimalajeewa</keyname><forenames>T.</forenames></author><author><keyname>Dulek</keyname><forenames>B.</forenames></author><author><keyname>Varshney</keyname><forenames>P. K.</forenames></author><author><keyname>Su</keyname><forenames>W.</forenames></author></authors><title>Asynchronous Linear Modulation Classification with Multiple Sensors via
  Generalized EM Algorithm</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of automatic modulation classification
with multiple sensors in the presence of unknown time offset, phase offset and
received signal amplitude. We develop a novel hybrid maximum likelihood (HML)
classification scheme based on a generalized expectation maximization (GEM)
algorithm. GEM is capable of finding ML estimates numerically that are
extremely hard to obtain otherwise. Assuming a good initialization technique is
available for GEM, we show that the classification performance can be greatly
improved with multiple sensors compared to that with a single sensor,
especially when the signal-to-noise ratio (SNR) is low. We further demonstrate
the superior performance of our approach when simulated annealing (SA) with
uniform as well as nonuniform grids is employed for initialization of GEM in
low SNR regions. The proposed GEM based approach employs only a small number of
samples (in the order of hundreds) at a given sensor node to perform both time
and phase synchronization, signal power estimation, followed by modulation
classification. We provide simulation results to show the computational
efficiency and effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8372</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8372</id><created>2014-09-29</created><authors><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>Indonesian's presidential social media campaigns</title><categories>cs.CY</categories><comments>Best Paper Award, L. A. Abdillah, &quot;Indonesian's presidential social
  media campaigns,&quot; in Seminar Nasional Sistem Informasi Indonesia
  (SESINDO2014), ITS, Surabaya, 22 September, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media has been used for political campaigns of presidential candidates
in a number of democratic countries, including Indonesia.The candidates for
president and vice president have been following the latest trends in the
virtual world by using social media such as: 1) Facebook, and 2) Twitter.The
author's continuing research with a focus on the popularity of candidates for
president and vice president of the Indonesian presidential election in July 9,
2014.The authors found that Facebook remains a social media most widely used in
Indonesia for presidential campaigns. Prabowo secured the most number of fans
on Facebook. While Joko Widodo secured the most number of followers on
Twitter.Facebook users is dominated by the user in the age range 18-24 years,
and most cities are using Facebook is Jakarta.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8389</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8389</id><created>2014-09-30</created><authors><author><keyname>Dragan</keyname><forenames>Feodor F.</forenames></author><author><keyname>K&#xf6;hler</keyname><forenames>Ekkehard</forenames></author><author><keyname>Leitert</keyname><forenames>Arne</forenames></author></authors><title>Line-distortion, Bandwidth and Path-length of a graph</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the minimum line-distortion and the minimum bandwidth problems
on unweighted graphs and their relations with the minimum length of a
Robertson-Seymour's path-decomposition. The length of a path-decomposition of a
graph is the largest diameter of a bag in the decomposition. The path-length of
a graph is the minimum length over all its path-decompositions. In particular,
we show:
  - if a graph $G$ can be embedded into the line with distortion $k$, then $G$
admits a Robertson-Seymour's path-decomposition with bags of diameter at most
$k$ in $G$;
  - for every class of graphs with path-length bounded by a constant, there
exist an efficient constant-factor approximation algorithm for the minimum
line-distortion problem and an efficient constant-factor approximation
algorithm for the minimum bandwidth problem;
  - there is an efficient 2-approximation algorithm for computing the
path-length of an arbitrary graph;
  - AT-free graphs and some intersection families of graphs have path-length at
most 2;
  - for AT-free graphs, there exist a linear time 8-approximation algorithm for
the minimum line-distortion problem and a linear time 4-approximation algorithm
for the minimum bandwidth problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8402</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8402</id><created>2014-09-30</created><updated>2015-08-04</updated><authors><author><keyname>Guo</keyname><forenames>Yinghao</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Optimal Pricing and Load Sharing for Energy Saving in Communications
  Cooperation</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we propose a pricing mechanism for the uplink communication
cooperation to save the energy of mobile terminals (MTs) in wireless cellular
network. Under the uncertainties of the other MTs' channel and battery
conditions, a source MT in low battery level or bad channel condition is
allowed to select and pay another MT in proximity to help forward its data
package to the base station (BS). We formulate the source MT's pricing and load
sharing problem as an optimization problem to minimize its expected energy
cost. When the source MT cannot split its data package for a certain multimedia
application, we motivate the selected relay MT to forward the whole data
package by obtaining the optimal pricing through a dichotomous search
algorithm. When the source MT can split the data package, we jointly optimize
the pricing and load sharing with the relay MT and propose an alternating
optimization algorithm that achieves near-optimal solution. Extensive numerical
results are provided to show that our proposed pricing mechanism can
significantly decrease the source MT's expected energy cost, and load sharing
is more cost-efficient when the size of the data package is large and the
average number of helping MTs is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8403</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8403</id><created>2014-09-30</created><updated>2015-08-28</updated><authors><author><keyname>Akata</keyname><forenames>Zeynep</forenames></author><author><keyname>Reed</keyname><forenames>Scott</forenames></author><author><keyname>Walter</keyname><forenames>Daniel</forenames></author><author><keyname>Lee</keyname><forenames>Honglak</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Evaluation of Output Embeddings for Fine-Grained Image Classification</title><categories>cs.CV</categories><comments>@inproceedings {ARWLS15, title = {Evaluation of Output Embeddings for
  Fine-Grained Image Classification}, booktitle = {IEEE Computer Vision and
  Pattern Recognition}, year = {2015}, author = {Zeynep Akata and Scott Reed
  and Daniel Walter and Honglak Lee and Bernt Schiele} }</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image classification has advanced significantly in recent years with the
availability of large-scale image sets. However, fine-grained classification
remains a major challenge due to the annotation cost of large numbers of
fine-grained categories. This project shows that compelling classification
performance can be achieved on such categories even without labeled training
data. Given image and class embeddings, we learn a compatibility function such
that matching embeddings are assigned a higher score than mismatching ones;
zero-shot classification of an image proceeds by finding the label yielding the
highest joint compatibility score. We use state-of-the-art image features and
focus on different supervised attributes and unsupervised output embeddings
either derived from hierarchies or learned from unlabeled text corpora. We
establish a substantially improved state-of-the-art on the Animals with
Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate
that purely unsupervised output embeddings (learned from Wikipedia and improved
with fine-grained text) achieve compelling results, even outperforming the
previous supervised state-of-the-art. By combining different output embeddings,
we further improve results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8404</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8404</id><created>2014-09-30</created><updated>2014-11-12</updated><authors><author><keyname>Schulz</keyname><forenames>Alexander</forenames></author></authors><title>Converting Reconfigurable Petri Nets to Maude</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking is an important aim of the theoretical computer science. It
enables the verification of a model with a set of properties such as liveness,
deadlock or safety. One of the typical modelling techniques are Petri nets they
are well understood and can be used for a model checking. Reconfigurable Petri
nets are based on a Petri nets with a set of rules. These rules can be used
dynamically to change the net. Missing is the possibility to verify a
reconfigurable net and properties such as deadlocks or liveness. This paper
introduces a conversion from reconfigurable Petri net to Maude, that allows us
to fill the gap. It presents a net transformation approach which is based on
Maude's equation- and rewrite logic as well as the LTLR model checker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8428</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8428</id><created>2014-09-30</created><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Nonstochastic Multi-Armed Bandits with Graph-Structured Feedback</title><categories>cs.LG stat.ML</categories><comments>Preliminary versions of parts of this paper appeared in [1,20], and
  also as arXiv papers arXiv:1106.2436 and arXiv:1307.4564</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and study a partial-information model of online learning, where a
decision maker repeatedly chooses from a finite set of actions, and observes
some subset of the associated losses. This naturally models several situations
where the losses of different actions are related, and knowing the loss of one
action provides information on the loss of other actions. Moreover, it
generalizes and interpolates between the well studied full-information setting
(where all losses are revealed) and the bandit setting (where only the loss of
the action chosen by the player is revealed). We provide several algorithms
addressing different variants of our setting, and provide tight regret bounds
depending on combinatorial properties of the information feedback structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8434</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8434</id><created>2014-09-30</created><authors><author><keyname>Seo</keyname><forenames>Junyeong</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author><author><keyname>Lee</keyname><forenames>Gilwon</forenames></author><author><keyname>Kim</keyname><forenames>Donggun</forenames></author></authors><title>Pilot Beam Sequence Design for Channel Estimation in Millimeter-Wave
  MIMO Systems: A POMDP Framework</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, submitted to IEEE ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, adaptive pilot beam sequence design for channel estimation in
large millimeter-wave (mmWave) MIMO systems is considered. By exploiting the
sparsity of mmWave MIMO channels with the virtual channel representation and
imposing a Markovian random walk assumption on the physical movement of the
line-of-sight (LOS) and reflection clusters, it is shown that the sparse
channel estimation problem in large mmWave MIMO systems reduces to a sequential
detection problem that finds the locations and values of the non-zero-valued
bins in a two-dimensional rectangular grid, and the optimal adaptive pilot
design problem can be cast into the framework of a partially observable Markov
decision process (POMDP). Under the POMDP framework, an optimal adaptive pilot
beam sequence design method is obtained to maximize the accumulated
transmission data rate for a given period of time. Numerical results are
provided to validate our pilot signal design method and they show that the
proposed method yields good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8460</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8460</id><created>2014-09-30</created><updated>2015-05-21</updated><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Yang</keyname><forenames>Hong-Chuan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Delay Reduction in Multi-Hop Device-to-Device Communication using
  Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of reducing the broadcast decoding delay of
wireless networks using instantly decodable network coding (IDNC) based
device-to-device (D2D) communications. In a D2D configuration, devices in the
network can help hasten the recovery of the lost packets of other devices in
their transmission range by sending network coded packets. Unlike previous
works that assumed fully connected network, this paper proposes a partially
connected configuration in which the decision should be made not only on the
packet combinations but also on the set of transmitting devices. First, the
different events occurring at each device are identified so as to derive an
expression for the probability distribution of the decoding delay. The joint
optimization problem over the set of transmitting devices and the packet
combinations of each is, then, formulated. The optimal solution of the joint
optimization problem is derived using a graph theory approach by introducing
the cooperation graph and reformulating the problem as a maximum weight clique
problem in which the weight of each vertex is the contribution of the device
identified by the vertex. Through extensive simulations, the decoding delay
experienced by all devices in the Point to Multi-Point (PMP) configuration, the
fully connected D2D (FC-D2D) configuration and the more practical partially
connected D2D (PC-D2D) configuration are compared. Numerical results suggest
that the PC-D2D outperforms the FC-D2D and provides appreciable gain especially
for poorly connected networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8464</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8464</id><created>2014-09-30</created><authors><author><keyname>Slivovsky</keyname><forenames>Friedrich</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Model Counting for Formulas of Bounded Clique-Width</title><categories>cs.CC cs.DS</categories><comments>Extended version of a paper published at ISAAC 2013</comments><journal-ref>Proceedings of ISAAC 2013. Lecture Notes in Computer Science, vol.
  8283, pp. 677-687, Springer, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that #SAT is polynomial-time tractable for classes of CNF formulas
whose incidence graphs have bounded symmetric clique-width (or bounded
clique-width, or bounded rank-width). This result strictly generalizes
polynomial-time tractability results for classes of formulas with signed
incidence graphs of bounded clique-width and classes of formulas with incidence
graphs of bounded modular treewidth, which were the most general results of
this kind known so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8470</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8470</id><created>2014-09-30</created><authors><author><keyname>Moreira</keyname><forenames>Catarina</forenames></author><author><keyname>Wichert</keyname><forenames>Andreas</forenames></author></authors><title>Interference Effects in Quantum Belief Networks</title><categories>cs.AI</categories><journal-ref>Applied Soft Computing, Volume 25, December 2014, Pages 64 - 85</journal-ref><doi>10.1016/j.asoc.2014.09.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic graphical models such as Bayesian Networks are one of the most
powerful structures known by the Computer Science community for deriving
probabilistic inferences. However, modern cognitive psychology has revealed
that human decisions could not follow the rules of classical probability
theory, because humans cannot process large amounts of data in order to make
judgements. Consequently, the inferences performed are based on limited data
coupled with several heuristics, leading to violations of the law of total
probability. This means that probabilistic graphical models based on classical
probability theory are too limited to fully simulate and explain various
aspects of human decision making.
  Quantum probability theory was developed in order to accommodate the
paradoxical findings that the classical theory could not explain. Recent
findings in cognitive psychology revealed that quantum probability can fully
describe human decisions in an elegant framework. Their findings suggest that,
before taking a decision, human thoughts are seen as superposed waves that can
interfere with each other, influencing the final decision.
  In this work, we propose a new Bayesian Network based on the psychological
findings of cognitive scientists. We made experiments with two very well known
Bayesian Networks from the literature. The results obtained revealed that the
quantum like Bayesian Network can affect drastically the probabilistic
inferences, specially when the levels of uncertainty of the network are very
high (no pieces of evidence observed). When the levels of uncertainty are very
low, then the proposed quantum like network collapses to its classical
counterpart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8481</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8481</id><created>2014-09-30</created><authors><author><keyname>Liao</keyname><forenames>Hao</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author></authors><title>Reconstructing propagation networks with temporal similarity metrics</title><categories>physics.soc-ph cs.SI stat.AP</categories><comments>8 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Node similarity is a significant property driving the growth of real
networks. In this paper, based on the observed spreading results we apply the
node similarity metrics to reconstruct propagation networks. We find that the
reconstruction accuracy of the similarity metrics is strongly influenced by the
infection rate of the spreading process. Moreover, there is a range of
infection rate in which the reconstruction accuracy of some similarity metrics
drops to nearly zero. In order to improve the similarity-based reconstruction
method, we finally propose a temporal similarity metric to take into account
the time information of the spreading. The reconstruction results are
remarkably improved with the new method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8484</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8484</id><created>2014-09-30</created><authors><author><keyname>Napoli</keyname><forenames>Christian</forenames></author><author><keyname>Pappalardo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Tramontana</keyname><forenames>Emiliano</forenames></author></authors><title>An agent-driven semantical identifier using radial basis neural networks
  and reinforcement learning</title><categories>cs.NE cs.AI cs.CL cs.LG cs.MA</categories><comments>Published on: Proceedings of the XV Workshop &quot;Dagli Oggetti agli
  Agenti&quot; (WOA 2014), Catania, Italy, Sepember. 25-26, 2014</comments><msc-class>68T01, 68T05, 68T10, 68T50, 68U15</msc-class><acm-class>C.2.1; I.2.6; I.2.7</acm-class><journal-ref>Proceedings of the XV Workshop &quot;Dagli Oggetti agli Agenti&quot; (WOA
  2014), on CEUR-WS, volume 1260, ISSN: 1613-073, Catania, Italy, Sepember.
  25-26, 2014. http://ceur-ws.org/Vol-1260/</journal-ref><doi>10.13140/2.1.1446.7843</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the huge availability of documents in digital form, and the deception
possibility raise bound to the essence of digital documents and the way they
are spread, the authorship attribution problem has constantly increased its
relevance. Nowadays, authorship attribution,for both information retrieval and
analysis, has gained great importance in the context of security, trust and
copyright preservation. This work proposes an innovative multi-agent driven
machine learning technique that has been developed for authorship attribution.
By means of a preprocessing for word-grouping and time-period related analysis
of the common lexicon, we determine a bias reference level for the recurrence
frequency of the words within analysed texts, and then train a Radial Basis
Neural Networks (RBPNN)-based classifier to identify the correct author. The
main advantage of the proposed approach lies in the generality of the semantic
analysis, which can be applied to different contexts and lexical domains,
without requiring any modification. Moreover, the proposed system is able to
incorporate an external input, meant to tune the classifier, and then
self-adjust by means of continuous learning reinforcement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8485</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8485</id><created>2014-09-30</created><authors><author><keyname>Liao</keyname><forenames>Hao</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Predicting missing links via correlation between nodes</title><categories>physics.soc-ph cs.SI physics.data-an stat.ML</categories><comments>7 pages, 3 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:1010.0725 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a fundamental problem in many different fields, link prediction aims to
estimate the likelihood of an existing link between two nodes based on the
observed information. Since this problem is related to many applications
ranging from uncovering missing data to predicting the evolution of networks,
link prediction has been intensively investigated recently and many methods
have been proposed so far. The essential challenge of link prediction is to
estimate the similarity between nodes. Most of the existing methods are based
on the common neighbor index and its variants. In this paper, we propose to
calculate the similarity between nodes by the correlation coefficient. This
method is found to be very effective when applied to calculate similarity based
on high order paths. We finally fuse the correlation-based method with the
resource allocation method, and find that the combined method can substantially
outperform the existing methods, especially in sparse networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8486</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8486</id><created>2014-09-30</created><authors><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author><author><keyname>Farina</keyname><forenames>Jason</forenames></author><author><keyname>Khac</keyname><forenames>Nhien An Le</forenames></author><author><keyname>Kechadi</keyname><forenames>Tahar</forenames></author></authors><title>Leveraging Decentralization to Extend the Digital Evidence Acquisition
  Window: Case Study on BitTorrent Sync</title><categories>cs.CR cs.CY</categories><comments>Special Issue on Proc. of Sixth International Conference on Digital
  Forensics &amp; Cyber Crime, ICDF2C'14, Journal of Digital Forensics, Security
  and Law, Vol 9, No 2, September 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  File synchronization services such as Dropbox, Google Drive, Microsoft
OneDrive, Apple iCloud, etc., are becoming increasingly popular in today's
always-connected world. A popular alternative to the aforementioned services is
BitTorrent Sync. This is a decentralized/cloudless file synchronization service
and is gaining significant popularity among Internet users with privacy
concerns over where their data is stored and who has the ability to access it.
The focus of this paper is the remote recovery of digital evidence pertaining
to files identified as being accessed or stored on a suspect's computer or
mobile device. A methodology for the identification, investigation, recovery
and verification of such remote digital evidence is outlined. Finally, a
proof-of-concept remote evidence recovery from BitTorrent Sync shared folder
highlighting a number of potential scenarios for the recovery and verification
of such evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8488</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8488</id><created>2014-09-30</created><authors><author><keyname>Kerenidis</keyname><forenames>Iordanis</forenames></author><author><keyname>Lauri&#xe8;re</keyname><forenames>Mathieu</forenames></author><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author><author><keyname>Rennela</keyname><forenames>Mathys</forenames></author></authors><title>Privacy in Quantum Communication Complexity</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In two-party quantum communication complexity, Alice and Bob receive some
classical inputs and wish to compute some function that depends on both these
inputs, while minimizing the communication. This model has found numerous
applications in many areas of computer science. One question that has received
a lot of attention recently is whether it is possible to perform such protocols
in a private way. We show that defining privacy for quantum protocols is not so
straightforward and it depends on whether we assume that the registers where
Alice and Bob receive their classical inputs are in fact classical registers
(and hence unentangled with the rest of the protocol) or quantum registers (and
hence can be entangled with the rest of the protocol or the environment). We
provide new quantum protocols for the Inner Product function and for Private
Information Retrieval, and show that the privacy assuming classical input
registers can be exponentially smaller than the privacy assuming quantum input
registers. We also argue that the right notion of privacy of a communication
protocol is the one assuming classical input registers, since otherwise the
players can deviate considerably from the protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8489</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8489</id><created>2014-09-30</created><authors><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author><author><keyname>Farina</keyname><forenames>Jason</forenames></author><author><keyname>Kechadi</keyname><forenames>M-Tahar</forenames></author></authors><title>BitTorrent Sync: Network Investigation Methodology</title><categories>cs.CR cs.NI</categories><comments>9th International Conference on Availability, Reliability and
  Security (ARES 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The volume of personal information and data most Internet users find
themselves amassing is ever increasing and the fast pace of the modern world
results in most requiring instant access to their files. Millions of these
users turn to cloud based file synchronisation services, such as Dropbox,
Microsoft Skydrive, Apple iCloud and Google Drive, to enable &quot;always-on&quot; access
to their most up-to-date data from any computer or mobile device with an
Internet connection. The prevalence of recent articles covering various
invasion of privacy issues and data protection breaches in the media has caused
many to review their online security practices with their personal information.
To provide an alternative to cloud based file backup and synchronisation,
BitTorrent Inc. released an alternative cloudless file backup and
synchronisation service, named BitTorrent Sync to alpha testers in April 2013.
BitTorrent Sync's popularity rose dramatically throughout 2013, reaching over
two million active users by the end of the year. This paper outlines a number
of scenarios where the network investigation of the service may prove
invaluable as part of a digital forensic investigation. An investigation
methodology is proposed outlining the required steps involved in retrieving
digital evidence from the network and the results from a proof of concept
investigation are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8490</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8490</id><created>2014-09-30</created><authors><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author><author><keyname>Kechadi</keyname><forenames>M-Tahar</forenames></author></authors><title>The Case for a Collaborative Universal Peer-to-Peer Botnet Investigation
  Framework</title><categories>cs.CR cs.NI</categories><comments>9th International Conference on Cyber Warfare and Security
  (ICCWS-2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-Peer (P2P) botnets are becoming widely used as a low-overhead,
efficient, self-maintaining, distributed alternative to the traditional
client/server model across a broad range of cyberattacks. These cyberattacks
can take the form of distributed denial of service attacks, authentication
cracking, spamming, cyberwarfare or malware distribution targeting on financial
systems. These attacks can also cross over into the physical world attacking
critical infrastructure causing its disruption or destruction (power,
communications, water, etc.). P2P technology lends itself well to being
exploited for such malicious purposes due to the minimal setup, running and
maintenance costs involved in executing a globally orchestrated attack,
alongside the perceived additional layer of anonymity. In the ever-evolving
space of botnet technology, reducing the time lag between discovering a newly
developed or updated botnet system and gaining the ability to mitigate against
it is paramount. Often, numerous investigative bodies duplicate their efforts
in creating bespoke tools to combat particular threats. This paper outlines a
framework capable of fast tracking the investigative process through
collaboration between key stakeholders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8493</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8493</id><created>2014-09-30</created><authors><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author><author><keyname>Kechadi</keyname><forenames>M-Tahar</forenames></author></authors><title>Digital Evidence Bag Selection for P2P Network Investigation</title><categories>cs.NI cs.CR</categories><comments>The 7th International Symposium on Digital Forensics and Information
  Security (DFIS-2013); Future Information Technology, Application, and Service</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The collection and handling of court admissible evidence is a fundamental
component of any digital forensic investigation. While the procedures for
handling digital evidence take much of their influence from the established
policies for the collection of physical evidence, due to the obvious
differences in dealing with non-physical evidence, a number of extra policies
and procedures are required. This paper compares and contrasts some of the
existing digital evidence formats or &quot;bags&quot; and analyses them for their
compatibility with evidence gathered from a network source. A new digital
extended evidence bag is proposed to specifically deal with evidence gathered
from P2P networks, incorporating the network byte stream and on-the-fly
metadata generation to aid in expedited identification and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8498</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8498</id><created>2014-09-30</created><authors><author><keyname>Crandall</keyname><forenames>Jacob W.</forenames></author></authors><title>Non-Myopic Learning in Repeated Stochastic Games</title><categories>cs.GT cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses learning in repeated stochastic games (RSGs) played
against unknown associates. Learning in RSGs is extremely challenging due to
their inherently large strategy spaces. Furthermore, these games typically have
multiple (often infinite) equilibria, making attempts to solve them via
equilibrium analysis and rationality assumptions wholly insufficient. As such,
previous learning algorithms for RSGs either learn very slowly or make
extremely limiting assumptions about the game structure or associates'
behaviors. In this paper, we propose and evaluate the notion of game
abstraction by experts (Gabe) for two-player general-sum RSGs. Gabe reduces an
RSG to a multi-armed bandit problem, which can then be solved using an expert
algorithm. Gabe maintains many aspects of the original game, including security
and Pareto optimal Nash equilibria. We demonstrate that Gabe substantially
outperforms existing algorithms in many scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8500</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8500</id><created>2014-09-30</created><updated>2015-03-27</updated><authors><author><keyname>Deleforge</keyname><forenames>Antoine</forenames></author><author><keyname>Forbes</keyname><forenames>Florence</forenames></author><author><keyname>Ba</keyname><forenames>Sileye</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Hyper-Spectral Image Analysis with Partially-Latent Regression and
  Spatial Markov Dependencies</title><categories>stat.AP cs.CV</categories><comments>12 pages, 4 figures, 3 tables</comments><journal-ref>IEEE Journal on Selected Topics in Signal Processing, volume 9,
  number 6, 1037-1048, 2015</journal-ref><doi>10.1109/JSTSP.2015.2416677</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyper-spectral data can be analyzed to recover physical properties at large
planetary scales. This involves resolving inverse problems which can be
addressed within machine learning, with the advantage that, once a relationship
between physical parameters and spectra has been established in a data-driven
fashion, the learned relationship can be used to estimate physical parameters
for new hyper-spectral observations. Within this framework, we propose a
spatially-constrained and partially-latent regression method which maps
high-dimensional inputs (hyper-spectral images) onto low-dimensional responses
(physical parameters such as the local chemical composition of the soil). The
proposed regression model comprises two key features. Firstly, it combines a
Gaussian mixture of locally-linear mappings (GLLiM) with a partially-latent
response model. While the former makes high-dimensional regression tractable,
the latter enables to deal with physical parameters that cannot be observed or,
more generally, with data contaminated by experimental artifacts that cannot be
explained with noise models. Secondly, spatial constraints are introduced in
the model through a Markov random field (MRF) prior which provides a spatial
structure to the Gaussian-mixture hidden variables. Experiments conducted on a
database composed of remotely sensed observations collected from the Mars
planet by the Mars Express orbiter demonstrate the effectiveness of the
proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8518</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8518</id><created>2014-09-30</created><authors><author><keyname>Lillis</keyname><forenames>David</forenames></author><author><keyname>Toolan</keyname><forenames>Fergus</forenames></author><author><keyname>Collier</keyname><forenames>Rem</forenames></author><author><keyname>Dunnion</keyname><forenames>John</forenames></author></authors><title>ProbFuse: A Probabilistic Approach to Data Fusion</title><categories>cs.IR</categories><comments>Proceedings of the 29th Annual International ACM SIGIR Conference on
  Research and Development in Information Retrieval (SIGIR '06), 2006</comments><acm-class>H.3.3</acm-class><doi>10.1145/1148170.1148197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data fusion is the combination of the results of independent searches on a
document collection into one single output result set. It has been shown in the
past that this can greatly improve retrieval effectiveness over that of the
individual results.
  This paper presents probFuse, a probabilistic approach to data fusion.
ProbFuse assumes that the performance of the individual input systems on a
number of training queries is indicative of their future performance. The fused
result set is based on probabilities of relevance calculated during this
training process. Retrieval experiments using data from the TREC ad hoc
collection demonstrate that probFuse achieves results superior to that of the
popular CombMNZ fusion algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8524</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8524</id><created>2014-09-30</created><authors><author><keyname>Disser</keyname><forenames>Yann</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>The Minimum Feasible Tileset problem</title><categories>cs.CC cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Minimum Feasible Tileset problem: Given a set of symbols and
subsets of these symbols (scenarios), find a smallest possible number of pairs
of symbols (tiles) such that each scenario can be formed by selecting at most
one symbol from each tile. We show that this problem is NP-complete even if
each scenario contains at most three symbols. Our main result is a
4/3-approximation algorithm for the general case. In addition, we show that the
Minimum Feasible Tileset problem is fixed-parameter tractable both when
parameterized with the number of scenarios and with the number of symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8536</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8536</id><created>2014-09-30</created><updated>2014-10-08</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>Aslam</keyname><forenames>Javed</forenames></author><author><keyname>Karaman</keyname><forenames>Sertac</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>Optimal Tourist Problem and Anytime Planning of Trip Itineraries</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study the problem in which a mobile sensing robot (our
tourist) is tasked to travel among and gather intelligence at a set of
spatially distributed point-of-interests (POIs). The quality of the information
collected at each POI is characterized by some non-decreasing reward function
over the time spent at the POI. With limited time budget, the robot must
balance between spending time traveling to POIs and spending time at POIs for
information collection (sensing) so as to maximize the total reward.
Alternatively, the robot may be required to acquire a minimum mount of reward
and hopes to do so with the least amount of time. We propose a mixed integer
programming (MIP) based anytime algorithm for solving these two NP-hard
optimization problems to arbitrary precision. The effectiveness of our
algorithm is demonstrated using an extensive set of computational experiments
including the planning of a realistic itinerary for a first-time tourist in
Istanbul.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8558</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8558</id><created>2014-09-30</created><authors><author><keyname>Muthukumar</keyname><forenames>Prasanna Kumar</forenames></author><author><keyname>Black</keyname><forenames>Alan W.</forenames></author></authors><title>A Deep Learning Approach to Data-driven Parameterizations for
  Statistical Parametric Speech Synthesis</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral
coefficients as the vocal tract parameterization of the speech signal. Mel
Cepstral coefficients were never intended to work in a parametric speech
synthesis framework, but as yet, there has been little success in creating a
better parameterization that is more suited to synthesis. In this paper, we use
deep learning algorithms to investigate a data-driven parameterization
technique that is designed for the specific requirements of synthesis. We
create an invertible, low-dimensional, noise-robust encoding of the Mel Log
Spectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is
then unwrapped and used as the initialization for a Multi-Layer Perceptron
(MLP). The MLP is fine-tuned by training it to reconstruct the input at the
output layer. This MLP is then split down the middle to form encoding and
decoding networks. These networks produce a parameterization of the Mel Log
Spectrum that is intended to better fulfill the requirements of synthesis.
Results are reported for experiments conducted using this resulting
parameterization with the ClusterGen speech synthesizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8563</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8563</id><created>2014-09-30</created><updated>2014-12-03</updated><authors><author><keyname>Arteaga</keyname><forenames>Andrea</forenames></author><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Krause</keyname><forenames>Rolf</forenames></author></authors><title>A stencil-based implementation of Parareal in the C++ domain specific
  embedded language STELLA</title><categories>cs.DC math.NA</categories><journal-ref>Applied Mathematics and Computation 267, pp. 727-741, 2015</journal-ref><doi>10.1016/j.amc.2014.12.055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In view of the rapid rise of the number of cores in modern supercomputers,
time-parallel methods that introduce concurrency along the temporal axis are
becoming increasingly popular. For the solution of time-dependent partial
differential equations, these methods can add another direction for concurrency
on top of spatial parallelization. The paper presents an implementation of the
time-parallel Parareal method in a C++ domain specific language for stencil
computations (STELLA). STELLA provides both an OpenMP and a CUDA backend for a
shared memory parallelization, using the CPU or GPU inside a node for the
spatial stencils. Here, we intertwine this node-wise spatial parallelism with
the time-parallel Parareal. This is done by adding an MPI-based implementation
of Parareal, which allows us to parallelize in time across nodes. The
performance of Parareal with both backends is analyzed in terms of speedup,
parallel efficiency and energy-to-solution for an advection-diffusion problem
with a time-dependent diffusion coefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8572</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8572</id><created>2014-09-29</created><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Freshness-Aware Thompson Sampling</title><categories>cs.IR cs.LG</categories><comments>21st International Conference on Neural Information Processing. arXiv
  admin note: text overlap with arXiv:1409.7729</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To follow the dynamicity of the user's content, researchers have recently
started to model interactions between users and the Context-Aware Recommender
Systems (CARS) as a bandit problem where the system needs to deal with
exploration and exploitation dilemma. In this sense, we propose to study the
freshness of the user's content in CARS through the bandit problem. We
introduce in this paper an algorithm named Freshness-Aware Thompson Sampling
(FA-TS) that manages the recommendation of fresh document according to the
user's risk of the situation. The intensive evaluation and the detailed
analysis of the experimental results reveals several important discoveries in
the exploration/exploitation (exr/exp) behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8576</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8576</id><created>2014-09-30</created><authors><author><keyname>Ozkan</keyname><forenames>Huseyin</forenames></author><author><keyname>Pelvan</keyname><forenames>Ozgun S.</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>Data Imputation through the Identification of Local Anomalies</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a comprehensive and statistical framework in a model free
setting for a complete treatment of localized data corruptions due to severe
noise sources, e.g., an occluder in the case of a visual recording. Within this
framework, we propose i) a novel algorithm to efficiently separate, i.e.,
detect and localize, possible corruptions from a given suspicious data instance
and ii) a Maximum A Posteriori (MAP) estimator to impute the corrupted data. As
a generalization to Euclidean distance, we also propose a novel distance
measure, which is based on the ranked deviations among the data attributes and
empirically shown to be superior in separating the corruptions. Our algorithm
first splits the suspicious instance into parts through a binary partitioning
tree in the space of data attributes and iteratively tests those parts to
detect local anomalies using the nominal statistics extracted from an
uncorrupted (clean) reference data set. Once each part is labeled as anomalous
vs normal, the corresponding binary patterns over this tree that characterize
corruptions are identified and the affected attributes are imputed. Under a
certain conditional independency structure assumed for the binary patterns, we
analytically show that the false alarm rate of the introduced algorithm in
detecting the corruptions is independent of the data and can be directly set
without any parameter tuning. The proposed framework is tested over several
well-known machine learning data sets with synthetically generated corruptions;
and experimentally shown to produce remarkable improvements in terms of
classification purposes with strong corruption separation capabilities. Our
experiments also indicate that the proposed algorithms outperform the typical
approaches and are robust to varying training phase conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8578</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8578</id><created>2014-09-30</created><authors><author><keyname>Chen</keyname><forenames>Lu</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Okulicz-Kozaryn</keyname><forenames>Adam</forenames></author></authors><title>U.S. Religious Landscape on Twitter</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages</comments><journal-ref>the 6th International Conference on Social Informatics (SocInfo
  2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Religiosity is a powerful force shaping human societies, affecting domains as
diverse as economic growth or the ability to cope with illness. As more
religious leaders and organizations as well as believers start using social
networking sites (e.g., Twitter, Facebook), online activities become important
extensions to traditional religious rituals and practices. However, there has
been lack of research on religiosity in online social networks. This paper
takes a step toward the understanding of several important aspects of
religiosity on Twitter, based on the analysis of more than 250k U.S. users who
self-declared their religions/belief, including Atheism, Buddhism,
Christianity, Hinduism, Islam, and Judaism. Specifically, (i) we examine the
correlation of geographic distribution of religious people between Twitter and
offline surveys. (ii) We analyze users' tweets and networks to identify
discriminative features of each religious group, and explore supervised methods
to identify believers of different religions. (iii) We study the linkage
preference of different religious groups, and observe a strong preference of
Twitter users connecting to others sharing the same religion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8580</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8580</id><created>2014-09-30</created><updated>2015-06-08</updated><authors><author><keyname>Schilcher</keyname><forenames>Udo</forenames></author><author><keyname>Toumpis</keyname><forenames>Stavros</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author><author><keyname>Crismani</keyname><forenames>Alessandro</forenames></author><author><keyname>Brandner</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Bettstetter</keyname><forenames>Christian</forenames></author></authors><title>Interference Functionals in Poisson Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and prove a theorem that allows the calculation of a class of
functionals on Poisson point processes that have the form of expected values of
sum-products of functions. In proving the theorem, we present a variant of the
Campbell-Mecke theorem from stochastic geometry. We proceed to apply our result
in the calculation of expected values involving interference in wireless
Poisson networks. Based on this, we derive outage probabilities for
transmissions in a Poisson network with Nakagami fading. Our results extend the
stochastic geometry toolbox used for the mathematical analysis of
interference-limited wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8581</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8581</id><created>2014-09-29</created><authors><author><keyname>Kumar</keyname><forenames>M. Anand</forenames></author><author><keyname>Dhanalakshmi</keyname><forenames>V.</forenames></author><author><keyname>Soman</keyname><forenames>K. P.</forenames></author><author><keyname>Sharmiladevi</keyname><forenames>V.</forenames></author></authors><title>Improving the Performance of English-Tamil Statistical Machine
  Translation System using Source-Side Pre-Processing</title><categories>cs.CL</categories><comments>Proc. of Int. Conf. on Advances in Computer Science, AETACS - 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Translation is one of the major oldest and the most active research
area in Natural Language Processing. Currently, Statistical Machine Translation
(SMT) dominates the Machine Translation research. Statistical Machine
Translation is an approach to Machine Translation which uses models to learn
translation patterns directly from data, and generalize them to translate a new
unseen text. The SMT approach is largely language independent, i.e. the models
can be applied to any language pair. Statistical Machine Translation (SMT)
attempts to generate translations using statistical methods based on bilingual
text corpora. Where such corpora are available, excellent results can be
attained translating similar texts, but such corpora are still not available
for many language pairs. Statistical Machine Translation systems, in general,
have difficulty in handling the morphology on the source or the target side
especially for morphologically rich languages. Errors in morphology or syntax
in the target language can have severe consequences on meaning of the sentence.
They change the grammatical function of words or the understanding of the
sentence through the incorrect tense information in verb. Baseline SMT also
known as Phrase Based Statistical Machine Translation (PBSMT) system does not
use any linguistic information and it only operates on surface word form.
Recent researches shown that adding linguistic information helps to improve the
accuracy of the translation with less amount of bilingual corpora. Adding
linguistic information can be done using the Factored Statistical Machine
Translation system through pre-processing steps. This paper investigates about
how English side pre-processing is used to improve the accuracy of
English-Tamil SMT system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8585</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8585</id><created>2014-09-29</created><authors><author><keyname>Zambianchi</keyname><forenames>Vincenzo</forenames></author><author><keyname>Kieffer</keyname><forenames>Michel</forenames></author><author><keyname>Pasolini</keyname><forenames>Gianni</forenames></author><author><keyname>Bassi</keyname><forenames>Francesca</forenames></author><author><keyname>Dardari</keyname><forenames>Davide</forenames></author></authors><title>Efficient Distributed Non-Asymptotic Confidence Regions Computation over
  Wireless Sensor Networks</title><categories>cs.SY</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the distributed computation of confidence regions
tethered to multidimensional parameter estimation under linear measurement
models. In particular, the considered confidence regions are non-asymptotic,
this meaning that the number of required measurements is finite. Distributed
solutions for the computation of non-asymptotic confidence regions are
proposed, suited to wireless sensor networks scenarios. Their performances are
compared in terms of required traffic load, both analytically and numerically.
The evidence emerging from the conducted investigations is that the best
solution for information exchange depends on whether the network topology is
structured or unstructured. The effect on the computation of confidence regions
of information diffusion truncation is also examined. In particular, it is
proven that consistent confidence regions can be computed even when an
incomplete set of measurements is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8588</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8588</id><created>2014-09-30</created><authors><author><keyname>Kelman</keyname><forenames>Guy</forenames></author><author><keyname>Br&#xe9;e</keyname><forenames>David S.</forenames></author><author><keyname>Manes</keyname><forenames>Eran</forenames></author><author><keyname>Lamieri</keyname><forenames>Marco</forenames></author><author><keyname>Golo</keyname><forenames>Natasa</forenames></author><author><keyname>Solomon</keyname><forenames>Sorin</forenames></author></authors><title>Dissortative From the Outside, Assortative From the Inside: Social
  Structure and Behavior in the Industrial Trade Network</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>10 pages, 10 figures, To appear in conference proceedings of the
  IEEE: HICSS-48</comments><journal-ref>In Proceedings of the 48th HICSS, 10, IEEE Computer Society,
  Computer Society Press, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is generally accepted that neighboring nodes in financial networks are
negatively assorted with respect to the correlation between their degrees. This
feature would play an important 'damping' role in the market during downturns
(periods of distress) since this connectivity pattern between firms lowers the
chances of auto-amplifying (the propagation of) distress. In this paper we
explore a trade-network of industrial firms where the nodes are suppliers or
buyers, and the links are those invoices that the suppliers send out to their
buyers and then go on to present to their bank for discounting. The network was
collected by a large Italian bank in 2007, from their intermediation of the
sales on credit made by their clients. The network also shows dissortative
behavior as seen in other studies on financial networks. However, when looking
at the credit rating of the firms, an important attribute internal to each
node, we find that firms that trade with one another share overwhelming
similarity. We know that much data is missing from our data set. However, we
can quantify the amount of missing data using information exposure, a variable
that connects social structure and behavior. This variable is a ratio of the
sales invoices that a supplier presents to their bank over their total sales.
Results reveal a non-trivial and robust relationship between the information
exposure and credit rating of a firm, indicating the influence of the neighbors
on a firm's rating. This methodology provides a new insight into how to
reconstruct a network suffering from incomplete information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8593</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8593</id><created>2014-09-30</created><updated>2016-02-09</updated><authors><author><keyname>M&#xed;nguez</keyname><forenames>Roberto</forenames></author><author><keyname>Casero-Alonso</keyname><forenames>V&#xed;ctor</forenames></author></authors><title>Robust solutions of uncertain mixed-integer linear programs using
  decomposition techniques</title><categories>cs.NA cs.CE math.NA math.OC</categories><comments>31 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust optimization is a framework for modeling optimization problems
involving data uncertainty and during the last decades has been an area of
active research. If we focus on linear programming (LP) problems with i)
uncertain data, ii) binary decisions and iii) hard constraints within an
ellipsoidal uncertainty set, this paper provides a different interpretation of
their robust counterpart (RC) inspired from decomposition techniques. This new
interpretation allows the proposal of an ad-hoc decomposition technique to
solve the RC problem with the following advantages: i) it improves
tractability, specially for large-scale problems, and ii) it provides exact
bounds for the probability of constraint violation in case the probability
distribution of uncertain parameters are completely defined by using first and
second-order probability moments. An attractive aspect of our method is that it
decomposes the second-order cone programming problem, associated with the
robust counterpart, into a linear master problem and different quadratically
constrained problems (QCP) of considerable lower size. The optimal solution is
achieved through the solution of these master and subproblems within an
iterative scheme based on outer approximations of the second-order cone
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8602</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8602</id><created>2014-09-30</created><authors><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>Cache-aware Performance Modeling and Prediction for Dense Linear Algebra</title><categories>cs.PF</categories><comments>Submitted to PMBS14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Countless applications cast their computational core in terms of dense linear
algebra operations. These operations can usually be implemented by combining
the routines offered by standard linear algebra libraries such as BLAS and
LAPACK, and typically each operation can be obtained in many alternative ways.
Interestingly, identifying the fastest implementation -- without executing it
-- is a challenging task even for experts. An equally challenging task is that
of tuning each routine to performance-optimal configurations. Indeed, the
problem is so difficult that even the default values provided by the libraries
are often considerably suboptimal; as a solution, normally one has to resort to
executing and timing the routines, driven by some form of parameter search. In
this paper, we discuss a methodology to solve both problems: identifying the
best performing algorithm within a family of alternatives, and tuning
algorithmic parameters for maximum performance; in both cases, we do not
execute the algorithms themselves. Instead, our methodology relies on timing
and modeling the computational kernels underlying the algorithms, and on a
technique for tracking the contents of the CPU cache. In general, our
performance predictions allow us to tune dense linear algebra algorithms within
few percents from the best attainable results, thus allowing computational
scientists and code developers alike to efficiently optimize their linear
algebra routines and codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8606</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8606</id><created>2014-09-30</created><authors><author><keyname>Shahrampour</keyname><forenames>Shahin</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Distributed Detection : Finite-time Analysis and Impact of Network
  Topology</title><categories>math.OC cs.LG cs.SI stat.ML</categories><comments>29 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of distributed detection in multi-agent
networks. Agents receive private signals about an unknown state of the world.
The underlying state is globally identifiable, yet informative signals may be
dispersed throughout the network. Using an optimization-based framework, we
develop an iterative local strategy for updating individual beliefs. In
contrast to the existing literature which focuses on asymptotic learning, we
provide a finite-time analysis. Furthermore, we introduce a Kullback-Leibler
cost to compare the efficiency of the algorithm to its centralized counterpart.
Our bounds on the cost are expressed in terms of network size, spectral gap,
centrality of each agent and relative entropy of agents' signal structures. A
key observation is that distributing more informative signals to central agents
results in a faster learning rate. Furthermore, optimizing the weights, we can
speed up learning by improving the spectral gap. We also quantify the effect of
link failures on learning speed in symmetric networks. We finally provide
numerical simulations which verify our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8608</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8608</id><created>2014-09-30</created><authors><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>On the Performance Prediction of BLAS-based Tensor Contractions</title><categories>cs.MS cs.PF</categories><comments>Submitted to PMBS14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor operations are surging as the computational building blocks for a
variety of scientific simulations and the development of high-performance
kernels for such operations is known to be a challenging task. While for
operations on one- and two-dimensional tensors there exist standardized
interfaces and highly-optimized libraries (BLAS), for higher dimensional
tensors neither standards nor highly-tuned implementations exist yet. In this
paper, we consider contractions between two tensors of arbitrary dimensionality
and take on the challenge of generating high-performance implementations by
resorting to sequences of BLAS kernels. The approach consists in breaking the
contraction down into operations that only involve matrices or vectors. Since
in general there are many alternative ways of decomposing a contraction, we are
able to methodically derive a large family of algorithms. The main contribution
of this paper is a systematic methodology to accurately identify the fastest
algorithms in the bunch, without executing them. The goal is instead
accomplished with the help of a set of cache-aware micro-benchmarks for the
underlying BLAS kernels. The predictions we construct from such benchmarks
allow us to reliably single out the best-performing algorithms in a tiny
fraction of the time taken by the direct execution of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8613</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8613</id><created>2014-09-30</created><updated>2015-06-21</updated><authors><author><keyname>Costa</keyname><forenames>Jo&#xe3;o Pita</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael Vejdemo</forenames></author><author><keyname>&#x160;kraba</keyname><forenames>Primo&#x17e;</forenames></author></authors><title>Variable sets over an algebra of lifetimes: a contribution of lattice
  theory to the study of computational topology</title><categories>math.RA cs.CG math.CT</categories><comments>20 pages, 12 figures, AAA88 Conference proceedings at Demonstratio
  Mathematica. The new version has restructured arguments, clearer intuition is
  provided, and several typos corrected</comments><msc-class>03G30, 06D22, 18B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A topos theoretic generalisation of the category of sets allows for modelling
spaces which vary according to time intervals. Persistent homology, or more
generally, persistence is a central tool in topological data analysis, which
examines the structure of data through topology. The basic techniques have been
extended in several different directions, permuting the encoding of topological
features by so called barcodes or equivalently persistence diagrams. The set of
points of all such diagrams determines a complete Heyting algebra that can
explain aspects of the relations between persistent bars through the algebraic
properties of its underlying lattice structure. In this paper, we investigate
the topos of sheaves over such algebra, as well as discuss its construction and
potential for a generalised simplicial homology over it. In particular we are
interested in establishing a topos theoretic unifying theory for the various
flavours of persistent homology that have emerged so far, providing a global
perspective over the algebraic foundations of applied and computational
topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8624</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8624</id><created>2014-09-30</created><authors><author><keyname>Gerdes</keyname><forenames>Lennart</forenames></author><author><keyname>Hellings</keyname><forenames>Christoph</forenames></author><author><keyname>Weiland</keyname><forenames>Lorenz</forenames></author><author><keyname>Utschick</keyname><forenames>Wolfgang</forenames></author></authors><title>The Optimal Input Distribution for Partial Decode-and-Forward in the
  MIMO Relay Channel</title><categories>cs.IT math.IT</categories><comments>23 pages, 2 figures, submitted to IEEE Transactions on Information
  Theory</comments><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the partial decode-and-forward (PDF) strategy for the
Gaussian multiple-input multiple-output (MIMO) relay channel. Unlike for the
decode-and-forward (DF) strategy or point-to-point (P2P) transmission, for
which Gaussian channel inputs are known to be optimal, the input distribution
that maximizes the achievable PDF rate for the Gaussian MIMO relay channel has
remained unknown so far. For some special cases, e.g., for relay channels where
the optimal PDF strategy reduces to DF or P2P transmission, it could be deduced
that Gaussian inputs maximize the PDF rate. For the general case, however, the
problem has remained open until now. In this work, we solve this problem by
proving that the maximum achievable PDF rate for the Gaussian MIMO relay
channel is always attained by Gaussian channel inputs. Our proof relies on the
channel enhancement technique, which was originally introduced by Weingarten et
al. to derive the (private message) capacity region of the Gaussian MIMO
broadcast channel. By combining this technique with a primal decomposition
approach, we first establish that jointly Gaussian source and relay inputs
maximize the achievable PDF rate for the aligned Gaussian MIMO relay channel.
Subsequently, we use a limiting argument to extend this result from the aligned
to the general Gaussian MIMO relay channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8633</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8633</id><created>2014-09-30</created><authors><author><keyname>Carpin</keyname><forenames>Mattia</forenames></author><author><keyname>Zanella</keyname><forenames>Andrea</forenames></author><author><keyname>Rasool</keyname><forenames>Jawad</forenames></author><author><keyname>Mahmood</keyname><forenames>Kashif</forenames></author><author><keyname>Gr&#xf8;ndalen</keyname><forenames>Ole</forenames></author><author><keyname>&#xd8;sterb&#xf8;</keyname><forenames>Olav N.</forenames></author></authors><title>Scheduling Policies for the LTE Downlink Channel: A Performance
  Comparison</title><categories>cs.NI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key feature of the packet scheduler in LTE system is that it can allocate
resources both in the time and frequency domain. Furthermore, the scheduler is
acquainted with channel state information periodically reported by user
equipments either in an aggregate form for the whole downlink channel, or
distinguished for each available subchannel. This mechanism allows for wide
discretion in resource allocation, thus promoting the flourishing of several
scheduling algorithms, with different purposes. It is therefore of great
interest to compare the performance of such algorithms in different scenarios.
A very common simulation tool that can be used for this purpose is ns-3, which
already supports a set of well known scheduling algorithms for LTE downlink,
though it still lacks schedulers that provide throughput guarantees. In this
work we contribute to fill this gap by implementing a scheduling algorithm that
provides long-term throughput guarantees to the different users, while
opportunistically exploiting the instantaneous channel fluctuations to increase
the cell capacity. We then perform a thorough performance analysis of the
different scheduling algorithms by means of extensive ns-3 simulations, both
for saturated UDP and TCP traffic sources. The analysis makes it possible to
appreciate the difference among the scheduling algorithms, and to assess the
performance gain, both in terms of cell capacity and packet service time,
obtained by allowing the schedulers to work on the frequency domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8642</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8642</id><created>2014-09-28</created><authors><author><keyname>He</keyname><forenames>Chao</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Ketterl</keyname><forenames>Thomas P.</forenames></author><author><keyname>Arrobo</keyname><forenames>Gabriel E.</forenames></author><author><keyname>Gitlin</keyname><forenames>Richard D.</forenames></author></authors><title>Performance Evaluation for MIMO In Vivo WBAN Systems</title><categories>cs.IT cs.NI math.IT</categories><doi>10.1109/IMWS-BIO.2014.7032380</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the performance evaluation for a MIMO in vivo WBAN
system, using ANSYS HFSS and the associated complete Human Body Model. We
analyzed MIMO system capacity statistically and FER performance based upon an
IEEE 802.11n system model, with receiver antennas placed at various angular
positions around the human body. We also analyzed MIMO system capacity with
receiver antennas at the front of the body at various distances from
transmitter antennas. The results were compared to SISO arrangements and we
demonstrate that by using 2x2 MIMO in vivo, better performance can be achieved,
and significantly higher system capacity can be achieved when receiver antennas
are located at the back of the body and in front of the body.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8650</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8650</id><created>2014-09-30</created><authors><author><keyname>Thomos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Kurdoglu</keyname><forenames>Eymen</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author><author><keyname>Van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Adaptive Prioritized Random Linear Coding and Scheduling for Layered
  Data Delivery from Multiple Servers</title><categories>cs.IT cs.MM cs.NI math.IT</categories><comments>submitted to IEEE Transactions on Multimedia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we deal with the problem of jointly determining the optimal
coding strategy and the scheduling decisions when receivers obtain layered data
from multiple servers. The layered data is encoded by means of Prioritized
Random Linear Coding (PRLC) in order to be resilient to channel loss while
respecting the unequal levels of importance in the data, and data blocks are
transmitted simultaneously in order to reduce decoding delays and improve the
delivery performance. We formulate the optimal coding and scheduling decisions
problem in our novel framework with the help of Markov Decision Processes
(MDP), which are effective tools for modeling adapting streaming systems.
Reinforcement learning approaches are then proposed to derive reduced
computational complexity solutions to the adaptive coding and scheduling
problems. The novel reinforcement learning approaches and the MDP solution are
examined in an illustrative example for scalable video transmission. Our
methods offer large performance gains over competing methods that deliver the
data blocks sequentially. The experimental evaluation also shows that our novel
algorithms offer continuous playback and guarantee small quality variations
which is not the case for baseline solutions. Finally, our work highlights the
advantages of reinforcement learning algorithms to forecast the temporal
evolution of data demands and to decide the optimal coding and scheduling
decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8653</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8653</id><created>2014-09-30</created><authors><author><keyname>Kealy</keyname><forenames>Tom</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author><author><keyname>Piechocki</keyname><forenames>Robert</forenames></author></authors><title>The capacity of non-identical adaptive group testing</title><categories>cs.IT math.IT math.PR</categories><comments>To be presented at Allerton 2014</comments><journal-ref>2014 52nd Annual Allerton Conference on Communication, Control,
  and Computing (Allerton), p101-108</journal-ref><doi>10.1109/ALLERTON.2014.7028442</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the group testing problem, in the case where the items are
defective independently but with non-constant probability. We introduce and
analyse an algorithm to solve this problem by grouping items together
appropriately. We give conditions under which the algorithm performs
essentially optimally in the sense of information-theoretic capacity. We use
concentration of measure results to bound the probability that this algorithm
requires many more tests than the expected number. This has applications to the
allocation of spectrum to cognitive radios, in the case where a database gives
prior information that a particular band will be occupied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.8670</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.8670</id><created>2014-09-30</created><updated>2015-04-29</updated><authors><author><keyname>Joseph</keyname><affiliation>Seffi</affiliation></author><author><keyname>Naor</keyname></author><author><keyname>Wajc</keyname><forenames>David</forenames></author></authors><title>Near-Optimum Online Ad Allocation for Targeted Advertising</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><doi>10.1145/2764468.2764482</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by Internet targeted advertising, we address several ad allocation
problems. Prior work has established these problems admit no randomized online
algorithm better than $(1-\frac{1}{e})$-competitive
(\cite{karp1990optimal,mehta2007adwords}), yet simple heuristics have been
observed to perform much better in practice. We explain this phenomenon by
studying a generalization of the bounded-degree inputs considered by Buchbinder
et al.~\cite{buchbinder2007online}, graphs which we call $(k,d)-bounded$. In
such graphs the maximal degree on the online side is at most $d$ and the
minimal degree on the offline side is at least $k$. We prove that for such
graphs, these problems' natural greedy algorithms attain competitive ratio
$1-\frac{d-1}{k+d-1}$, tending to \emph{one} as $d/k$ tends to zero. We prove
this bound is tight for these algorithms.
  Next, we develop deterministic primal-dual algorithms for the above problems
achieving competitive ratio $1-(1-\frac{1}{d})^k&gt;1-\frac{1}{e^{k/d}}$, or
\emph{exponentially} better loss as a function of $k/d$, and strictly better
than $1-\frac{1}{e}$ whenever $k\geq d$. We complement our lower bounds with
matching upper bounds for the vertex-weighted problem. Finally, we use our
deterministic algorithms to prove by dual-fitting that simple randomized
algorithms achieve the same bounds in expectation. Our algorithms and analysis
differ from previous ad allocation algorithms, which largely scale bids based
on the spent fraction of their bidder's budget, whereas we scale bids according
to the number of times the bidder could have spent as much as her current bid.
Our algorithms differ from previous online primal-dual algorithms, as they do
not maintain dual feasibility, but only primal-to-dual ratio, and only attain
dual feasibility upon termination. We believe our techniques could find
applications to other well-behaved online packing problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0001</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0001</id><created>2014-09-30</created><authors><author><keyname>Gouyon</keyname><forenames>Fabien</forenames></author><author><keyname>Sturm</keyname><forenames>Bob L.</forenames></author><author><keyname>Oliveira</keyname><forenames>Joao Lobato</forenames></author><author><keyname>Hespanhol</keyname><forenames>Nuno</forenames></author><author><keyname>Langlois</keyname><forenames>Thibault</forenames></author></authors><title>On Evaluation Validity in Music Autotagging</title><categories>cs.IR cs.SD</categories><comments>Submitted for journal publication in September 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Music autotagging, an established problem in Music Information Retrieval,
aims to alleviate the human cost required to manually annotate collections of
recorded music with textual labels by automating the process. Many autotagging
systems have been proposed and evaluated by procedures and datasets that are
now standard (used in MIREX, for instance). Very little work, however, has been
dedicated to determine what these evaluations really mean about an autotagging
system, or the comparison of two systems, for the problem of annotating music
in the real world. In this article, we are concerned with explaining the figure
of merit of an autotagging system evaluated with a standard approach.
Specifically, does the figure of merit, or a comparison of figures of merit,
warrant a conclusion about how well autotagging systems have learned to
describe music with a specific vocabulary? The main contributions of this paper
are a formalization of the notion of validity in autotagging evaluation, and a
method to test it in general. We demonstrate the practical use of our method in
experiments with three specific state-of-the-art autotagging systems --all of
which are reproducible using the linked code and data. Our experiments show for
these specific systems in a simple and objective two-class task that the
standard evaluation approach does not provide valid indicators of their
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0030</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0030</id><created>2014-09-30</created><authors><author><keyname>Antignac</keyname><forenames>Thibaud</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / CITI Insa de Lyon</affiliation></author><author><keyname>M&#xe9;tayer</keyname><forenames>Daniel Le</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / CITI Insa de Lyon, CITI</affiliation></author></authors><title>Privacy by Design: From Technologies to Architectures (Position Paper)</title><categories>cs.CR</categories><proxy>ccsd</proxy><journal-ref>APF - Annual Privacy Forum 2014 8450 (2014) 1-17</journal-ref><doi>10.1007/978-3-319-06749-0_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing work on privacy by design mostly focus on technologies rather than
methodologies and on components rather than architectures. In this paper, we
advocate the idea that privacy by design should also be addressed at the
architectural level and be associated with suitable methodologies. Among other
benefits, architectural descriptions enable a more systematic exploration of
the design space. In addition, because privacy is intrinsically a complex
notion that can be in tension with other requirements, we believe that formal
methods should play a key role in this area. After presenting our position, we
provide some hints on how our approach can turn into practice based on ongoing
work on a privacy by design environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0033</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0033</id><created>2014-09-30</created><updated>2015-03-19</updated><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Karray</keyname><forenames>Mohamed Kadhem</forenames><affiliation>FT R\&amp;D</affiliation></author></authors><title>What frequency bandwidth to run cellular network in a given country? - a
  downlink dimensioning problem</title><categories>cs.NI math.PR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an analytic approach to the frequency bandwidth dimensioning
problem, faced by cellular network operators who deploy/upgrade their networks
in various geographical regions (countries) with an inhomogeneous urbanization.
We present a model allowing one to capture fundamental relations between users'
quality of service parameters (mean downlink throughput), traffic demand, the
density of base station deployment, and the available frequency bandwidth.
These relations depend on the applied cellular technology (3G or 4G impacting
user peak bit-rate) and on the path-loss characteristics observed in different
(urban, sub-urban and rural) areas. We observe that if the distance between
base stations is kept inversely proportional to the distance coefficient of the
path-loss function, then the performance of the typical cells of these
different areas is similar when serving the same (per-cell) traffic demand. In
this case, the frequency bandwidth dimensioning problem can be solved uniformly
across the country applying the mean cell approach proposed in [Blaszczyszyn et
al. WiOpt2014] http://dx.doi.org/10.1109/WIOPT.2014.6850355 . We validate our
approach by comparing the analytical results to measurements in operational
networks in various geographical zones of different countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0040</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0040</id><created>2014-09-30</created><updated>2014-10-07</updated><authors><author><keyname>Bonomo</keyname><forenames>Flavia</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author><author><keyname>Stein</keyname><forenames>Maya</forenames></author></authors><title>3-Colouring graphs without triangles or induced paths on seven vertices</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  \noindent We present an algorithm to $3$-colour a graph $G$ without triangles
or induced paths on seven vertices in $O(|V(G)|^7)$ time. In fact, our
algorithm solves the list $3$-colouring problem, where each vertex is assigned
a subset of $\{1,2,3\}$ as its admissible colours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0046</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0046</id><created>2014-08-09</created><authors><author><keyname>Adeogun</keyname><forenames>Ramoni</forenames></author></authors><title>A Robust MUSIC Based Scheme for Interference Location in Satellite
  Systems with Multibeam Antennas</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate methods for interference location in satellite
communication system using satellite multi-beam antenna with subspace based
schemes. A novel MUSIC based approach is proposed for estimating the direction
of arrival of the interfering sources. The proposed method provides super
resolution and asymptotic maximum likelihood estimates of the direction of
arrivals even at low SNR values. Simulations were performed using typical
satellite multi-beam antenna configurations and results show that the proposed
scheme can effectively estimates the direction of arrival in the azimuth and
elevation spectra. Compared to the support vector regression method, the
proposed approach offer improved estimation accuracy at low SNR values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0083</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0083</id><created>2014-09-30</created><authors><author><keyname>Fu</keyname><forenames>Jie</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author></authors><title>Integrating active sensing into reactive synthesis with temporal logic
  constraints under partial observations</title><categories>cs.SY cs.AI cs.RO</categories><comments>7 pages, 2 figures, submitted to American Control Conference 2015</comments><msc-class>93B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of online reactive planning with sensing actions for
systems with temporal logic constraints in partially observable and dynamic
environments. With incomplete information on the dynamic environment, reactive
controller synthesis amounts to solving a two-player game with partial
observations, which has impractically computational complexity. To alleviate
the high computational burden, online replanning via sensing actions avoids
solving the strategy in the reactive system under partial observations.
Instead, we only solve for a strategy that ensures a given temporal logic
specification can be satisfied had the system have complete observations of its
environment. Such a strategy is then transformed into one which makes control
decisions based on the observed sequence of states (of the interacting system
and its environment). When the system encounters a belief---a set including all
possible hypotheses the system has for the current state---for which the
observation-based strategy is undefined, a sequence of sensing actions are
triggered, chosen by an active sensing strategy, to reduce the uncertainty in
the system's belief. We show that by alternating between the observation-based
strategy and the active sensing strategy, under a mild technical assumption of
the set of sensors in the system, the given temporal logic specification can be
satisfied with probability 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0095</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0095</id><created>2014-09-30</created><authors><author><keyname>Wang</keyname><forenames>Xu</forenames></author><author><keyname>Slavakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Riemannian Multi-Manifold Modeling</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper advocates a novel framework for segmenting a dataset in a
Riemannian manifold $M$ into clusters lying around low-dimensional submanifolds
of $M$. Important examples of $M$, for which the proposed clustering algorithm
is computationally efficient, are the sphere, the set of positive definite
matrices, and the Grassmannian. The clustering problem with these examples of
$M$ is already useful for numerous application domains such as action
identification in video sequences, dynamic texture clustering, brain fiber
segmentation in medical imaging, and clustering of deformed images. The
proposed clustering algorithm constructs a data-affinity matrix by thoroughly
exploiting the intrinsic geometry and then applies spectral clustering. The
intrinsic local geometry is encoded by local sparse coding and more importantly
by directional information of local tangent spaces and geodesics. Theoretical
guarantees are established for a simplified variant of the algorithm even when
the clusters intersect. To avoid complication, these guarantees assume that the
underlying submanifolds are geodesic. Extensive validation on synthetic and
real data demonstrates the resiliency of the proposed method against deviations
from the theoretical model as well as its superior performance over
state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0105</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0105</id><created>2014-10-01</created><authors><author><keyname>Sun</keyname><forenames>Yao</forenames></author><author><keyname>Wang</keyname><forenames>Dingkang</forenames></author><author><keyname>Huang</keyname><forenames>Zhenyu</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author></authors><title>A Monomial-Oriented GVW for Computing Gr\&quot;obner Bases</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GVW algorithm, presented by Gao et al., is a signature-based algorithm
for computing Gr\&quot;obner bases. In this paper, a variant of GVW is presented.
This new algorithm is called a monomial-oriented GVW algorithm or mo-GVW
algorithm for short. The mo-GVW algorithm presents a new frame of GVW and
regards {\em labeled monomials} instead of {\em labeled polynomials} as basic
elements of the algorithm. Being different from the original GVW algorithm, for
each labeled monomial, the mo-GVW makes efforts to find the smallest signature
that can generate this monomial. The mo-GVW algorithm also avoids generating
J-pairs, and uses efficient methods of searching reducers and checking
criteria. Thus, the mo-GVW algorithm has a better performance during practical
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0113</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0113</id><created>2014-10-01</created><authors><author><keyname>Liu</keyname><forenames>Jingchu</forenames></author><author><keyname>Zhao</keyname><forenames>Tao</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>CONCERT: A Cloud-Based Architecture for Next-generation Cellular Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>17 pages, 4 figures, Accepted by IEEE Wireless Communications</comments><doi>10.1109/MWC.2014.7000967</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular networks are one of the corner stones of our information-driven
society. However, existing cellular systems have been seriously challenged by
the explosion of mobile data traffic, the emergence of machine-type
communications and the flourish of mobile Internet services. In this article,
we propose CONCERT (CONvergence of Cloud and cEllulaR sysTems), a converged
edge infrastructure for future cellular communications and mobile computing
services. The proposed architecture is constructed based on the concept of
control/data (C/D) plane decoupling. The data plane includes heterogeneous
physical resources such as radio interfacing equipment, computational
resources, and software-defined switches. The control plane jointly coordinates
physical resources to present them as virtual resources, over which
software-defined services including communications, computing, and management
can be deployed in a flexible manner. Moreover, we introduce new designs for
physical resources placement and task scheduling, so that CONCERT can overcome
the drawbacks of the existing baseband-up centralization approach and better
facilitate innovations in next-generation cellular networks. These advantages
are demonstrated with application examples on the radio access networks (RANs)
with C/D decoupled air interface, delay sensitive machine-type communications,
and real-time mobile cloud gaming. We also discuss some fundamental research
issues arising with the proposed architecture to illuminate future research
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0117</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0117</id><created>2014-10-01</created><updated>2014-10-06</updated><authors><author><keyname>Kanaujia</keyname><forenames>Atul</forenames></author></authors><title>Coupling Top-down and Bottom-up Methods for 3D Human Pose and Shape
  Estimation from Monocular Image Sequences</title><categories>cs.CV</categories><comments>13 Pages, 12 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until recently Intelligence, Surveillance, and Reconnaissance (ISR) focused
on acquiring behavioral information of the targets and their activities.
Continuous evolution of intelligence being gathered of the human centric
activities has put increased focus on the humans, especially inferring their
innate characteristics - size, shapes and physiology. These bio-signatures
extracted from the surveillance sensors can be used to deduce age, ethnicity,
gender and actions, and further characterize human actions in unseen scenarios.
However, recovery of pose and shape of humans in such monocular videos is
inherently an ill-posed problem, marked by frequent depth and view based
ambiguities due to self-occlusion, foreshortening and misalignment. The
likelihood function often yields a highly multimodal posterior that is
difficult to propagate even using the most advanced particle filtering(PF)
algorithms. Motivated by the recent success of the discriminative approaches to
efficiently predict 3D poses directly from the 2D images, we present several
principled approaches to integrate predictive cues using learned regression
models to sustain multimodality of the posterior during tracking. Additionally,
these learned priors can be actively adapted to the test data using a
likelihood based feedback mechanism. Estimated 3D poses are then used to fit 3D
human shape model to each frame independently for inferring anthropometric
bio-signatures. The proposed system is fully automated, robust to noisy test
data and has ability to swiftly recover from tracking failures even after
confronting with significant errors. We evaluate the system on a large number
of monocular human motion sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0123</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0123</id><created>2014-10-01</created><authors><author><keyname>Desjardins</keyname><forenames>Guillaume</forenames></author><author><keyname>Luo</keyname><forenames>Heng</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Deep Tempering</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann Machines (RBMs) are one of the fundamental building
blocks of deep learning. Approximate maximum likelihood training of RBMs
typically necessitates sampling from these models. In many training scenarios,
computationally efficient Gibbs sampling procedures are crippled by poor
mixing. In this work we propose a novel method of sampling from Boltzmann
machines that demonstrates a computationally efficient way to promote mixing.
Our approach leverages an under-appreciated property of deep generative models
such as the Deep Belief Network (DBN), where Gibbs sampling from deeper levels
of the latent variable hierarchy results in dramatically increased ergodicity.
Our approach is thus to train an auxiliary latent hierarchical model, based on
the DBN. When used in conjunction with parallel-tempering, the method is
asymptotically guaranteed to simulate samples from the target RBM. Experimental
results confirm the effectiveness of this sampling strategy in the context of
RBM training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0128</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0128</id><created>2014-10-01</created><authors><author><keyname>Chang</keyname><forenames>Zheng</forenames></author><author><keyname>Gong</keyname><forenames>Jie</forenames></author><author><keyname>Ristaniemi</keyname><forenames>Tapani</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Energy Efficient Resource Allocation and User Scheduling for
  Collaborative Mobile Clouds with Hybrid Receivers</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1303.4006 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the resource allocation and user scheduling algorithm
for minimizing the energy cost of data transmission in the context of OFDMA
collaborative mobile cloud (CMC) with simultaneous wireless information and
power transfer (SWIPT) receivers. The CMC, which consists of several
collaborating MTs offers one potential solution for downlink content
distribution and for the energy consumption reduction at the terminal side.
Meanwhile, as RF signal can carry both information and energy simultaneously,
the induced SWIPT has gained much attention for energy efficiency design of
mobile nodes. Previous work on the design of CMC system mainly focused on the
cloud formulation or energy efficiency investigation, while how to allocate the
radio resource and schedule user transmission lacks attention. With the
objective to minimize the system energy consumption, an optimization problem
which jointly considers subchannel assignment, power allocation and user
scheduling for a group of SWIPT receivers has been presented. The formulated
problem is addressed through the convex optimization technique. Simulation
results demonstrate that the proposed user scheduling and resource allocation
algorithms can achieve significant energy saving performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0151</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0151</id><created>2014-10-01</created><authors><author><keyname>Sinai</keyname><forenames>Meital Ben</forenames></author><author><keyname>Partush</keyname><forenames>Nimrod</forenames></author><author><keyname>Yadid</keyname><forenames>Shir</forenames></author><author><keyname>Yahav</keyname><forenames>Eran</forenames></author></authors><title>Exploiting Social Navigation</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an effective Sybil attack against social location based services.
Our attack is based on creating a large number of reputed &quot;bot drivers&quot;, and
controlling their reported locations using fake GPS reports. We show how this
attack can be used to influence social navigation systems by applying it to
Waze - a prominent social navigation application used by over 50 million
drivers. We show that our attack can fake traffic jams and dramatically
influence routing decisions. We present several techniques for preventing the
attack, and show that effective mitigation likely requires the use of
additional carrier information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0162</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0162</id><created>2014-10-01</created><authors><author><keyname>Yilmaz</keyname><forenames>Ozgur</forenames></author></authors><title>Reservoir Computing using Cellular Automata</title><categories>cs.NE</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel framework of reservoir computing. Cellular automaton is
used as the reservoir of dynamical systems. Input is randomly projected onto
the initial conditions of automaton cells and nonlinear computation is
performed on the input via application of a rule in the automaton for a period
of time. The evolution of the automaton creates a space-time volume of the
automaton state space, and it is used as the reservoir. The proposed framework
is capable of long short-term memory and it requires orders of magnitude less
computation compared to Echo State Networks. Also, for additive cellular
automaton rules, reservoir features can be combined using Boolean operations,
which provides a direct way for concept building and symbolic processing, and
it is much more efficient compared to state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0176</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0176</id><created>2014-10-01</created><authors><author><keyname>Lillis</keyname><forenames>David</forenames></author><author><keyname>Collier</keyname><forenames>Rem</forenames></author><author><keyname>Dragone</keyname><forenames>Mauro</forenames></author><author><keyname>O'Hare</keyname><forenames>G. M. P.</forenames></author></authors><title>An Agent-Based Approach to Component Management</title><categories>cs.MA cs.SE</categories><comments>In Proceedings of the 8th International Conference on Autonomous
  Agents and Multi-Agent Systems (AAMAS '09), Budapest, Hungary, 2009</comments><doi>10.1145/1558013.1558086</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper details the implementation of a software framework that aids the
development of distributed and self-configurable software systems. This
framework is an instance of a novel integration strategy called SoSAA (SOcially
Situated Agent Architecture), which combines Component-Based Software
Engineering and Agent-Oriented Software Engineering, drawing its inspiration
from hybrid agent control architectures. The framework defines a complete
construction process by enhancing a simple component-based framework with
reasoning and self-awareness capabilities through a standardized interface.
  The capabilities of the resulting framework are demonstrated through its
application to a non-trivial Multi Agent System (MAS). The system in question
is a pre-existing Information Retrieval (IR) system that has not previously
taken advantage of CBSE principles. In this paper we contrast these two systems
so as to highlight the benefits of using this new hybrid approach. We also
outline how component-based elements may be integrated into the Agent Factory
agent-oriented application framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0190</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0190</id><created>2014-10-01</created><authors><author><keyname>Hussain</keyname><forenames>Iqbal</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author></authors><title>Rateless Codes for the Multi-Way Relay Channel</title><categories>cs.IT math.IT</categories><comments>4 pages, 03 figures, Accepted in IEEE Wireless Communication Letters</comments><journal-ref>IEEE Wireless Communication Letters 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed Luby transform (DLT) codes for efficient packet
transmission in a multi-way relay network, where the links are modeled as
erasure channels. Density evolution is applied for asymptotic performance
analysis, and subsequently used in a linear-programming design framework for
optimizing the degree distribution at the relay in terms of overhead. Moreover
a buffer is introduced at the relay to enable efficient downlink transmission
even if packets are lost during uplink transmission. Performance losses in
terms of delay and/or erasure rates caused by link erasures during uplink
transmission are thus alleviated. The proposed DLT codes provide significant
improvements in overhead and decoded erasure rates. Numerical results for
finite-length codes follow closely the asymptotic analysis. Our results
demonstrate that the proposed buffer-based DLT codes outperform its
counterparts for lossy uplink transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0198</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0198</id><created>2014-10-01</created><updated>2015-05-07</updated><authors><author><keyname>Darulova</keyname><forenames>Eva</forenames></author><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author></authors><title>On Numerical Error Propagation with Sensitivity</title><categories>cs.PL</categories><report-no>EPFL-REPORT-200132</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An emerging area of research is to automatically compute reasonably accurate
upper bounds on numerical errors, including roundoffs due to the use of a
finite-precision representation for real numbers such as floating point or
fixed-point arithmetic. Previous approaches for this task are limited in their
accuracy and scalability, especially in the presence of nonlinear arithmetic.
Our main idea is to decouple the computation of newly introduced roundoff
errors from the amplification of existing errors. To characterize the
amplification of existing errors, we use the derivatives of functions
corresponding to program fragments. We implemented this technique in an
analysis for programs containing nonlinear computation, conditionals, and a
certain class of loops. We evaluate our system on a number of benchmarks from
embedded systems and scientific computation, showing substantial improvements
in accuracy and scalability over the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0205</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0205</id><created>2014-10-01</created><authors><author><keyname>Shekelyan</keyname><forenames>Michael</forenames></author><author><keyname>Joss&#xe9;</keyname><forenames>Gregor</forenames></author><author><keyname>Schubert</keyname><forenames>Matthias</forenames></author></authors><title>ParetoPrep: Fast computation of Path Skylines Queries</title><categories>cs.DB cs.SI</categories><comments>12 pages, 9 figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing cost optimal paths in network data is a very important task in many
application areas like transportation networks, computer networks or social
graphs. In many cases, the cost of an edge can be described by various cost
criteria. For example, in a road network possible cost criteria are distance,
time, ascent, energy consumption or toll fees. In such a multicriteria network,
a route or path skyline query computes the set of all paths having pareto
optimal costs, i.e. each result path is optimal for different user preferences.
In this paper, we propose a new method for computing route skylines which
significantly decreases processing time and memory consumption. Furthermore,
our method does not rely on any precomputation or indexing method and thus, it
is suitable for dynamically changing edge costs. Our experiments demonstrate
that our method outperforms state of the art approaches and allows highly
efficient path skyline computation without any preprocessing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0210</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0210</id><created>2014-10-01</created><updated>2015-05-05</updated><authors><author><keyname>Malinowski</keyname><forenames>Mateusz</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>A Multi-World Approach to Question Answering about Real-World Scenes
  based on Uncertain Input</title><categories>cs.AI cs.CL cs.CV cs.LG</categories><comments>Published in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for automatically answering questions about images by
bringing together recent advances from natural language processing and computer
vision. We combine discrete reasoning with uncertain predictions by a
multi-world approach that represents uncertainty about the perceived world in a
bayesian framework. Our approach can handle human questions of high complexity
about realistic scenes and replies with range of answer like counts, object
classes, instances and lists of them. The system is directly trained from
question-answer pairs. We establish a first benchmark for this task that can be
seen as a modern attempt at a visual turing test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0213</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0213</id><created>2014-10-01</created><authors><author><keyname>Hussain</keyname><forenames>Iqbal</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author></authors><title>Buffer-Based Distributed LT Codes</title><categories>cs.IT math.IT</categories><comments>14 pages, 17 figures, submitted</comments><journal-ref>IEEE Transactions on Communications 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the design of distributed Luby transform (DLT) codes for erasure
networks with multiple sources and multiple relays, communicating to a single
destination. The erasure-floor performance of DLT codes improves with the
maximum degree of the relay-degree distribution. However, for conventional DLT
codes, the maximum degree is upper-bounded by the number of sources. An
additional constraint is that the sources are required to have the same
information block length. We introduce a $D$-bit buffer for each source-relay
link, which allows the relay to select multiple encoded bits from the same
source for the relay-encoding process; thus, the number of sources no longer
limits the maximum degree at the relay. Furthermore, the introduction of
buffers facilitates the use of different information block sizes across
sources. Based on density evolution we develop an asymptotic analytical
framework for optimization of the relay-degree distribution. We further
integrate techniques for unequal erasure protection into the optimization
framework. The proposed codes are considered for both lossless and lossy
source-relay links. Numerical examples show that there is no loss in erasure
performance for transmission over lossy source-relay links as compared to
lossless links. Additional delays, however, may occur. The design framework and
our contributions are demonstrated by a number of illustrative examples,
showing the improvements obtained by the proposed buffer-based DLT codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0226</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0226</id><created>2014-07-28</created><authors><author><keyname>Lee</keyname><forenames>Juheon</forenames></author><author><keyname>Cai</keyname><forenames>Xiaohao</forenames></author><author><keyname>Schonlieb</keyname><forenames>Carola-Bibiane</forenames></author><author><keyname>Coomes</keyname><forenames>David</forenames></author></authors><title>Non-parametric Image Registration of Airborne LiDAR, Hyperspectral and
  Photographic Imagery of Forests</title><categories>cs.CV</categories><comments>11 pages, 5 figures</comments><acm-class>I.4.8</acm-class><doi>10.1109/TGRS.2015.2431692</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is much current interest in using multi-sensor airborne remote sensing
to monitor the structure and biodiversity of forests. This paper addresses the
application of non-parametric image registration techniques to precisely align
images obtained from multimodal imaging, which is critical for the successful
identification of individual trees using object recognition approaches.
Non-parametric image registration, in particular the technique of optimizing
one objective function containing data fidelity and regularization terms,
provides flexible algorithms for image registration. Using a survey of
woodlands in southern Spain as an example, we show that non-parametric image
registration can be successful at fusing datasets when there is little prior
knowledge about how the datasets are interrelated (i.e. in the absence of
ground control points). The validity of non-parametric registration methods in
airborne remote sensing is demonstrated by a series of experiments. Precise
data fusion is a prerequisite to accurate recognition of objects within
airborne imagery, so non-parametric image registration could make a valuable
contribution to the analysis pipeline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0228</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0228</id><created>2014-09-30</created><authors><author><keyname>Diongue</keyname><forenames>Dame</forenames></author><author><keyname>Thiare</keyname><forenames>Ousmane</forenames></author></authors><title>An Energy Efficient Self-healing Mechanism for Long Life Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>6 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1309.6000</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide an energy efficient self- healing mechanism for
Wireless Sensor Networks. The proposed solution is based on our probabilistic
sentinel scheme. To reduce energy consumption while maintaining good
connectivity between sentinel nodes, we compose our solution on two main
concepts, node adaptation and link adaptation. The first algorithm uses node
adaptation technique and permits to distributively schedule nodes activities
and select a minimum subset of active nodes (sentry) to monitor the interest
region. And secondly, we in- troduce a link control algorithm to ensure better
connectiv- ity between sentinel nodes while avoiding outliers appearance.
Without increasing control messages overhead, performances evaluations show
that our solution is scalable with a steady energy consumption. Simulations
carried out also show that the proposed mechanism ensures good connectivity
between sentry nodes while considerably reducing the total energy spent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0241</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0241</id><created>2014-10-01</created><authors><author><keyname>Parrondo</keyname><forenames>J. M. R.</forenames></author><author><keyname>Dinis</keyname><forenames>L.</forenames></author><author><keyname>Garc&#xed;a-Tora&#xf1;o</keyname><forenames>E.</forenames></author><author><keyname>Sotillo</keyname><forenames>B.</forenames></author></authors><title>Collective decision making and paradoxical games</title><categories>physics.soc-ph cs.GT</categories><comments>9 pages, 6 figures</comments><journal-ref>Eur. Phys. J. Special Topics 143, 39 (2007)</journal-ref><doi>10.1140/epjst/e2007-00068-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an ensemble of individuals playing the two games of the so-called
Parrondo paradox. In our study, players are allowed to choose the game to be
played by the whole ensemble in each turn. The choice cannot conform to the
preferences of all the players and, consequently, they face a simple
frustration phenomenon that requires some strategy to make a collective
decision. We consider several such strategies and analyze how fluctuations can
be used to improve the performance of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0243</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0243</id><created>2014-10-01</created><authors><author><keyname>Pizurica</keyname><forenames>Aleksandra</forenames></author></authors><title>Pattern Encoding on the Poincare Sphere</title><categories>cs.CV</categories><comments>26 pages, 23 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a convenient graphical tool for encoding visual patterns
(such as image patches and image atoms) as point constellations in a space
spanned by perceptual features and with a clear geometrical interpretation.
General theory and a practical pattern encoding scheme are presented, inspired
by encoding polarization states of a light wave on the Poincare sphere. This
new pattern encoding scheme can be useful for many applications in image
processing and computer vision. Here, three possible applications are
illustrated, in clustering perceptually similar patterns, visualizing
properties of learned dictionaries of image atoms and generating new
dictionaries of image atoms from spherical codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0245</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0245</id><created>2014-10-01</created><updated>2015-10-06</updated><authors><author><keyname>Fish</keyname><forenames>Benjamin</forenames></author><author><keyname>Kun</keyname><forenames>Jeremy</forenames></author><author><keyname>Lelkes</keyname><forenames>&#xc1;d&#xe1;m D&#xe1;niel</forenames></author><author><keyname>Reyzin</keyname><forenames>Lev</forenames></author><author><keyname>Tur&#xe1;n</keyname><forenames>Gy&#xf6;rgy</forenames></author></authors><title>On the Computational Complexity of MapReduce</title><categories>cs.CC cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study MapReduce computations from a complexity-theoretic
perspective. First, we formulate a uniform version of the MRC model of Karloff
et al. (2010). We then show that the class of regular languages, and moreover
all of sublogarithmic space, lies in constant round MRC. This result also
applies to the MPC model of Andoni et al. (2014). In addition, we prove that,
conditioned on a variant of the Exponential Time Hypothesis, there are strict
hierarchies within MRC so that increasing the number of rounds or the amount of
time per processor increases the power of MRC. To the best of our knowledge we
are the first to approach the MapReduce model with complexity-theoretic
techniques, and our work lays the foundation for further analysis relating
MapReduce to established complexity classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0260</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0260</id><created>2014-10-01</created><updated>2015-03-13</updated><authors><author><keyname>March</keyname><forenames>William B.</forenames></author><author><keyname>Xiao</keyname><forenames>Bo</forenames></author><author><keyname>Biros</keyname><forenames>George</forenames></author></authors><title>ASKIT: Approximate Skeletonization Kernel-Independent Treecode in High
  Dimensions</title><categories>cs.DS cs.LG</categories><comments>22 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fast algorithm for kernel summation problems in high-dimensions.
These problems appear in computational physics, numerical approximation,
non-parametric statistics, and machine learning. In our context, the sums
depend on a kernel function that is a pair potential defined on a dataset of
points in a high-dimensional Euclidean space. A direct evaluation of the sum
scales quadratically with the number of points. Fast kernel summation methods
can reduce this cost to linear complexity, but the constants involved do not
scale well with the dimensionality of the dataset.
  The main algorithmic components of fast kernel summation algorithms are the
separation of the kernel sum between near and far field (which is the basis for
pruning) and the efficient and accurate approximation of the far field.
  We introduce novel methods for pruning and approximating the far field. Our
far field approximation requires only kernel evaluations and does not use
analytic expansions. Pruning is not done using bounding boxes but rather
combinatorially using a sparsified nearest-neighbor graph of the input. The
time complexity of our algorithm depends linearly on the ambient dimension. The
error in the algorithm depends on the low-rank approximability of the far
field, which in turn depends on the kernel function and on the intrinsic
dimensionality of the distribution of the points. The error of the far field
approximation does not depend on the ambient dimension.
  We present the new algorithm along with experimental results that demonstrate
its performance. We report results for Gaussian kernel sums for 100 million
points in 64 dimensions, for one million points in 1000 dimensions, and for
problems in which the Gaussian kernel has a variable bandwidth. To the best of
our knowledge, all of these experiments are impossible or prohibitively
expensive with existing fast kernel summation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0265</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0265</id><created>2014-10-01</created><authors><author><keyname>Li</keyname><forenames>Chao</forenames></author><author><keyname>Hay</keyname><forenames>Michael</forenames></author><author><keyname>Miklau</keyname><forenames>Gerome</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author></authors><title>A Data- and Workload-Aware Algorithm for Range Queries Under
  Differential Privacy</title><categories>cs.DB</categories><comments>VLDB 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new algorithm for answering a given set of range queries under
$\epsilon$-differential privacy which often achieves substantially lower error
than competing methods. Our algorithm satisfies differential privacy by adding
noise that is adapted to the input data and to the given query set. We first
privately learn a partitioning of the domain into buckets that suit the input
data well. Then we privately estimate counts for each bucket, doing so in a
manner well-suited for the given query set. Since the performance of the
algorithm depends on the input database, we evaluate it on a wide range of real
datasets, showing that we can achieve the benefits of data-dependence on both
&quot;easy&quot; and &quot;hard&quot; databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0276</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0276</id><created>2014-10-01</created><authors><author><keyname>Chuzhoy</keyname><forenames>Julia</forenames></author></authors><title>Improved Bounds for the Flat Wall Theorem</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Flat Wall Theorem of Robertson and Seymour states that there is some
function $f$, such that for all integers $w,t&gt;1$, every graph $G$ containing a
wall of size $f(w,t)$, must contain either (i) a $K_t$-minor; or (ii) a small
subset $A\subset V(G)$ of vertices, and a flat wall of size $w$ in $G\setminus
A$. Kawarabayashi, Thomas and Wollan recently showed a self-contained proof of
this theorem with the following two sets of parameters: (1)
$f(w,t)=\Theta(t^{24}(t^2+w))$ with $|A|=O(t^{24})$, and (2)
$f(w,t)=w^{2^{\Theta(t^{24})}}$ with $|A|\leq t-5$. The latter result gives the
best possible bound on $|A|$. In this paper we improve their bounds to
$f(w,t)=\Theta(t(t+w))$ with $|A|\leq t-5$. For the special case where the
maximum vertex degree in $G$ is bounded by $D$, we show that, if $G$ contains a
wall of size $\Omega(Dt(t+w))$, then either $G$ contains a $K_t$-minor, or
there is a flat wall of size $w$ in $G$. This setting naturally arises in
algorithms for the Edge-Disjoint Paths problem, with $D\leq 4$. Like the proof
of Kawarabayashi et al., our proof is self-contained, except for using a
well-known theorem on routing pairs of disjoint paths. We also provide
efficient algorithms that return either a model of the $K_t$-minor, or a vertex
set $A$ and a flat wall of size $w$ in $G\setminus A$.
  We complement our result for the low-degree scenario by proving an almost
matching lower bound: namely, for all integers $w,t&gt;1$, there is a graph $G$,
containing a wall of size $\Omega(wt)$, such that the maximum vertex degree in
$G$ is 5, and $G$ contains no flat wall of size $w$, and no $K_t$-minor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0277</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0277</id><created>2014-10-01</created><updated>2015-01-08</updated><authors><author><keyname>H&#xe4;ger</keyname><forenames>Christian</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>Terminated and Tailbiting Spatially-Coupled Codes with Optimized Bit
  Mappings for Spectrally Efficient Fiber-Optical Systems</title><categories>cs.IT math.IT physics.optics</categories><comments>This paper has been accepted for publication in the IEEE/OSA Journal
  of Lightwave Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of spectrally efficient fiber-optical communication
systems based on different spatially coupled (SC) forward error correction
(FEC) schemes. In particular, we optimize the allocation of the coded bits from
the FEC encoder to the modulation bits of the signal constellation. Two SC code
classes are considered. The codes in the first class are protograph-based
low-density parity-check (LDPC) codes which are decoded using iterative
soft-decision decoding. The codes in the second class are generalized LDPC
codes which are decoded using iterative hard-decision decoding. For both code
classes, the bit allocation is optimized for the terminated and tailbiting SC
cases based on a density evolution analysis. An optimized bit allocation can
significantly improve the performance of tailbiting SC codes codes over the
baseline sequential allocation, up to the point where they have a comparable
gap to capacity as their terminated counterparts, at a lower FEC overhead. For
the considered terminated SC codes, the optimization only results in marginal
performance improvements, suggesting that in this case a sequential allocation
is close to optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0281</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0281</id><created>2014-08-10</created><authors><author><keyname>De Smedt</keyname><forenames>Tom</forenames></author></authors><title>Modeling Creativity: Case Studies in Python</title><categories>cs.AI</categories><comments>p. 165, University Press Antwerp, ISBN 978-90-5718-260-0</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling Creativity (doctoral dissertation, 2013) explores how creativity can
be represented using computational approaches. Our aim is to construct computer
models that exhibit creativity in an artistic context, that is, that are
capable of generating or evaluating an artwork (visual or linguistic), an
interesting new idea, a subjective opinion. The research was conducted in
2008-2012 at the Computational Linguistics Research Group (CLiPS, University of
Antwerp) under the supervision of Prof. Walter Daelemans. Prior research was
also conducted at the Experimental Media Research Group (EMRG, St. Lucas
University College of Art &amp; Design Antwerp) under the supervision of Lucas
Nijs.
  Modeling Creativity examines creativity in a number of different
perspectives: from its origins in nature, which is essentially blind, to humans
and machines, and from generating creative ideas to evaluating and learning
their novelty and usefulness. We will use a hands-on approach with case studies
and examples in the Python programming language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0286</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0286</id><created>2014-10-01</created><authors><author><keyname>Roorda</keyname><forenames>Dirk</forenames></author><author><keyname>Kalkman</keyname><forenames>Gino</forenames></author><author><keyname>Naaijer</keyname><forenames>Martijn</forenames></author><author><keyname>van Cranenburgh</keyname><forenames>Andreas</forenames></author></authors><title>LAF-Fabric: a data analysis tool for Linguistic Annotation Framework
  with an application to the Hebrew Bible</title><categories>cs.CL</categories><journal-ref>Computational Linguistics in the Netherlands Journal, Volume 4,
  December 2014, pp. 105-109</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Linguistic Annotation Framework (LAF) provides a general, extensible
stand-off markup system for corpora. This paper discusses LAF-Fabric, a new
tool to analyse LAF resources in general with an extension to process the
Hebrew Bible in particular. We first walk through the history of the Hebrew
Bible as text database in decennium-wide steps. Then we describe how LAF-Fabric
may serve as an analysis tool for this corpus. Finally, we describe three
analytic projects/workflows that benefit from the new LAF representation:
  1) the study of linguistic variation: extract cooccurrence data of common
nouns between the books of the Bible (Martijn Naaijer); 2) the study of the
grammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman);
3) construction of a parser of classical Hebrew by Data Oriented Parsing:
generate tree structures from the database (Andreas van Cranenburgh).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0289</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0289</id><created>2014-10-01</created><authors><author><keyname>Sison</keyname><forenames>Virgilio</forenames></author></authors><title>Bases of the Galois Ring $GR(p^r,m)$ over the Integer Ring $Z_{p^r}$</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><msc-class>94B05, 94B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Galois ring $GR(p^r,m)$ of characteristic $p^r$ and cardinality $p^{rm}$,
where $p$ is a prime and $r,m \ge 1$ are integers, is a Galois extension of the
residue class ring $Z_{p^r}$ by a root $\omega$ of a monic basic irreducible
polynomial of degree $m$ over $Z_{p^r}$. Every element of $GR(p^r,m)$ can be
expressed uniquely as a polynomial in $\omega$ with coefficients in $Z_{p^r}$
and degree less than or equal to $m-1$, thus $GR(p^r,m)$ is a free module of
rank $m$ over $Z_{p^r}$ with basis $\{1,\omega, \omega^2,..., \omega^{m-1} \}$.
The ring $Z_{p^r}$ satisfies the invariant dimension property, hence any other
basis of $GR(p^r,m)$, if it exists, will have cardinality $m$.
  This paper was motivated by the code-theoretic problem of finding the
homogeneous bound on the $p^r$-image of a linear block code over $GR(p^r,m)$
with respect to any basis. It would be interesting to consider the dual and
normal bases of $GR(p^r,m)$.
  By using a Vandermonde matrix over $GR(p^r,m)$ in terms of the generalized
Frobenius automorphism, a constructive proof that every basis of $GR(p^r,m)$
has a unique dual basis is given. The notion of normal bases was also
generalized from the classic case for Galois fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0291</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0291</id><created>2014-10-01</created><updated>2014-12-15</updated><authors><author><keyname>Sim</keyname><forenames>Yanchuan</forenames></author></authors><title>A Morphological Analyzer for Japanese Nouns, Verbs and Adjectives</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an open source morphological analyzer for Japanese nouns, verbs
and adjectives. The system builds upon the morphological analyzing capabilities
of MeCab to incorporate finer details of classification such as politeness,
tense, mood and voice attributes. We implemented our analyzer in the form of a
finite state transducer using the open source finite state compiler FOMA
toolkit. The source code and tool is available at
https://bitbucket.org/skylander/yc-nlplab/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0306</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0306</id><created>2014-10-01</created><authors><author><keyname>Sergey</keyname><forenames>Ilya</forenames></author><author><keyname>Nanevski</keyname><forenames>Aleksandar</forenames></author><author><keyname>Banerjee</keyname><forenames>Anindya</forenames></author></authors><title>Specifying and Verifying Concurrent Algorithms with Histories and
  Subjectivity</title><categories>cs.LO</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a lightweight approach to Hoare-style specifications for
fine-grained concurrency, based on a notion of time-stamped histories that
abstractly capture atomic changes in the program state. Our key observation is
that histories form a partial commutative monoid, a structure fundamental for
representation of concurrent resources. This insight provides us with a
unifying mechanism that allows us to treat histories just like heaps in
separation logic. For example, both are subject to the same assertion logic and
inference rules (e.g., the frame rule). Moreover, the notion of ownership
transfer, which usually applies to heaps, has an equivalent in histories. It
can be used to formally represent helping---an important design pattern for
concurrent algorithms whereby one thread can execute code on behalf of another.
Specifications in terms of histories naturally abstract granularity, in the
sense that sophisticated fine-grained algorithms can be given the same
specifications as their simplified coarse-grained counterparts, making them
equally convenient for client-side reasoning. We illustrate our approach on a
number of examples and validate all of them in Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0309</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0309</id><created>2014-10-01</created><updated>2015-06-06</updated><authors><author><keyname>Kaiser</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Saumell</keyname><forenames>Maria</forenames></author><author><keyname>Van Cleemput</keyname><forenames>Nico</forenames></author></authors><title>10-Gabriel graphs are Hamiltonian</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $S$ of points in the plane, the $k$-Gabriel graph of $S$ is the
geometric graph with vertex set $S$, where $p_i,p_j\in S$ are connected by an
edge if and only if the closed disk having segment $\bar{p_ip_j}$ as diameter
contains at most $k$ points of $S \setminus \{p_i,p_j\}$. We consider the
following question: What is the minimum value of $k$ such that the $k$-Gabriel
graph of every point set $S$ contains a Hamiltonian cycle? For this value, we
give an upper bound of 10 and a lower bound of 2. The best previously known
values were 15 and 1, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0310</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0310</id><created>2014-09-12</created><updated>2015-11-27</updated><authors><author><keyname>Milovanov</keyname><forenames>Alexey</forenames></author></authors><title>Some properties of antistochastic strings</title><categories>cs.IT math.IT</categories><comments>18 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Antistochastic strings are those strings that lack any reasonable statistical
explanations. We establish the follow property of such strings: every
absolutely non-stochastic string $x$ is &quot;holographic&quot; in the sense that it can
be restored by a short program from any its part whose length equals the
Kolmogorov complexity of $x$. Further we will show how it can be used for list
decoding from erasing and for prove that symmetry of information fails for
total conditional complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0311</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0311</id><created>2014-08-26</created><updated>2015-03-02</updated><authors><author><keyname>Mukherjee</keyname><forenames>Subhadip</forenames></author><author><keyname>Basu</keyname><forenames>Rupam</forenames></author><author><keyname>Seelamantula</keyname><forenames>Chandra Sekhar</forenames></author></authors><title>$\ell_1$-K-SVD: A Robust Dictionary Learning Algorithm With Simultaneous
  Update</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a dictionary learning algorithm by minimizing the $\ell_1$
distortion metric on the data term, which is known to be robust for
non-Gaussian noise contamination. The proposed algorithm exploits the idea of
iterative minimization of weighted $\ell_2$ error. We refer to this algorithm
as $\ell_1$-K-SVD, where the dictionary atoms and the corresponding sparse
coefficients are simultaneously updated to minimize the $\ell_1$ objective,
resulting in noise-robustness. We demonstrate through experiments that the
$\ell_1$-K-SVD algorithm results in higher atom recovery rate compared with the
K-SVD and the robust dictionary learning (RDL) algorithm proposed by Lu et al.,
both in Gaussian and non-Gaussian noise conditions. We also show that, for
fixed values of sparsity, number of dictionary atoms, and data-dimension, the
$\ell_1$-K-SVD algorithm outperforms the K-SVD and RDL algorithms when the
training set available is small. We apply the proposed algorithm for denoising
natural images corrupted by additive Gaussian and Laplacian noise. The images
denoised using $\ell_1$-K-SVD are observed to have slightly higher peak
signal-to-noise ratio (PSNR) over K-SVD for Laplacian noise, but the
improvement in structural similarity index (SSIM) is significant (approximately
$0.1$) for lower values of input PSNR, indicating the efficacy of the $\ell_1$
metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0316</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0316</id><created>2014-10-01</created><authors><author><keyname>Rowe</keyname><forenames>Brian Lee Yung</forenames></author></authors><title>Using social network graph analysis for interest detection</title><categories>cs.SI cs.CL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A person's interests exist as an internal state and are difficult to define.
Since only external actions are observable, a proxy must be used that
represents someone's interests. Techniques like collaborative filtering,
behavioral targeting, and hashtag analysis implicitly model an individual's
interests. I argue that these models are limited to shallow, temporary
interests, which do not reflect people's deeper interests or passions. I
propose an alternative model of interests that takes advantage of a user's
social graph. The basic principle is that people only follow those that
interest them, so the social graph is an effective and robust proxy for
people's interests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0328</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0328</id><created>2014-10-01</created><updated>2015-04-30</updated><authors><author><keyname>Wang</keyname><forenames>Qing</forenames></author><author><keyname>Giustiniano</keyname><forenames>Domenico</forenames></author><author><keyname>Puccinelli</keyname><forenames>Daniele</forenames></author></authors><title>An Open-Source Research Platform for Embedded Visible Light Networking</title><categories>cs.NI</categories><comments>7 pages, 5 figures; Published by the IEEE Wireless Communications
  Special Issue on Visible Light Communications (2015)</comments><doi>10.1109/MWC.2015.7096291</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the growing interest in Visible Light Communication (VLC), a
reference networking platform based on commercial off-the-shelf components is
not available yet. An open-source platform would lower the barriers to entry to
VLC network research and help the VLC community gain momentum. We introduce
OpenVLC, an open-source VLC research platform based on software-defined
implementation. Built around a credit-card-sized embedded Linux platform with a
simple opto-electronic transceiver front-end, OpenVLC offers a basic physical
layer, a set of essential medium access primitives, as well as interoperability
with Internet protocols. We investigate the performance of OpenVLC and show
examples of how it can be used along with standard network diagnostics tools.
Our software-defined implementation can currently reach throughput in the order
of the basic rate of IEEE 802.15.7 standard. We discuss several techniques that
researchers and engineers could introduce to improve the performance of OpenVLC
and envision several directions that can benefit from OpenVLC by adopting it as
a reference platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0329</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0329</id><created>2014-10-01</created><authors><author><keyname>Eyraud-Dubois</keyname><forenames>Lionel</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Marchal</keyname><forenames>Loris</forenames><affiliation>ENS Lyon / CNRS / Inria Grenoble Rh&#xf4;ne-Alpes, LIP</affiliation></author><author><keyname>Sinnen</keyname><forenames>Oliver</forenames><affiliation>ECE</affiliation></author><author><keyname>Vivien</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>ENS Lyon / CNRS / Inria Grenoble Rh&#xf4;ne-Alpes, LIP</affiliation></author></authors><title>Parallel scheduling of task trees with limited memory</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1210.2580</comments><proxy>ccsd</proxy><report-no>RR-8606</report-no><journal-ref>N&amp;deg; RR-8606 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the execution of tree-shaped task graphs using
multiple processors. Each edge of such a tree represents some large data. A
task can only be executed if all input and output data fit into memory, and a
data can only be removed from memory after the completion of the task that uses
it as an input data. Such trees arise, for instance, in the multifrontal method
of sparse matrix factorization. The peak memory needed for the processing of
the entire tree depends on the execution order of the tasks. With one processor
the objective of the tree traversal is to minimize the required memory. This
problem was well studied and optimal polynomial algorithms were proposed. Here,
we extend the problem by considering multiple processors, which is of obvious
interest in the application area of matrix factorization. With multiple
processors comes the additional objective to minimize the time needed to
traverse the tree, i.e., to minimize the makespan. Not surprisingly, this
problem proves to be much harder than the sequential one. We study the
computational complexity of this problem and provide inapproximability results
even for unit weight trees. We design a series of practical heuristics
achieving different trade-offs between the minimization of peak memory usage
and makespan. Some of these heuristics are able to process a tree while keeping
the memory usage under a given memory limit. The different heuristics are
evaluated in an extensive experimental evaluation using realistic trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0334</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0334</id><created>2014-10-01</created><authors><author><keyname>Morvant</keyname><forenames>Emilie</forenames><affiliation>LHC</affiliation></author></authors><title>Domain adaptation of weighted majority votes via perturbed
  variation-based self-labeling</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><journal-ref>Pattern Recognition Letters (2014) To be published</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In machine learning, the domain adaptation problem arrives when the test
(target) and the train (source) data are generated from different
distributions. A key applied issue is thus the design of algorithms able to
generalize on a new distribution, for which we have no label information. We
focus on learning classification models defined as a weighted majority vote
over a set of real-val ued functions. In this context, Germain et al. (2013)
have shown that a measure of disagreement between these functions is crucial to
control. The core of this measure is a theoretical bound--the C-bound (Lacasse
et al., 2007)--which involves the disagreement and leads to a well performing
majority vote learning algorithm in usual non-adaptative supervised setting:
MinCq. In this work, we propose a framework to extend MinCq to a domain
adaptation scenario. This procedure takes advantage of the recent perturbed
variation divergence between distributions proposed by Harel and Mannor (2012).
Justified by a theoretical bound on the target risk of the vote, we provide to
MinCq a target sample labeled thanks to a perturbed variation-based
self-labeling focused on the regions where the source and target marginals
appear similar. We also study the influence of our self-labeling, from which we
deduce an original process for tuning the hyperparameters. Finally, our
framework called PV-MinCq shows very promising results on a rotation and
translation synthetic problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0336</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0336</id><created>2014-10-01</created><authors><author><keyname>Tiado</keyname><forenames>Mahmadou Issoufou</forenames><affiliation>IRIT</affiliation></author><author><keyname>Dhaou</keyname><forenames>Riadh</forenames><affiliation>IRIT</affiliation></author><author><keyname>Beylot</keyname><forenames>Andr&#xe9;-Luc</forenames><affiliation>IRIT</affiliation></author></authors><title>Cross-Layer Extended Persistent Timeout Policy for SCTP and DSDV</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>ICEER, Marrakech : Morocco (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross layer techniques applied to various protocols stacks provide fair
information sharing between OSI model layers. The performance gains have been
demonstrated for many studied systems within protocols interactions. The
example is illustrative of the reliable transport protocols that use
retransmissions to achieve that reliability function. The performance gains of
the persistent timeout policy for the management of the retransmission timeout
have been produce in some recent works when applying that persistent timeout
policy only to reliable transport protocol. The goal was to give an appropriate
behavior in response to a bad state of the wireless channel that occurs and
temporally blocks the transmission of data. The channel state is given by the
802.11 link layer through cross-layer mechanism. In this paper, the persistent
policy is extended to the network layer and is applied to a stack that uses a
reactive routing protocol, namely the Destination Sequenced Distance-Vector
(DSDV) protocol that also generates additional periodic traffic regardless to
the channel state. We are measuring the influence in terms of performance gains
of the extended persistent policy because of the additional periodic
signalization messages deriving from the used routing protocol. After the
introduction in section I; Section II of this paper presents an overview of the
Stream Control Transmission Protocol (SCTP). Section III describes the behavior
of the DSDV protocol. Section IV presents the extended persistent timeout
policy principle and Section V presents the simulation results used to compare
the using of the traditional and the extended persistent timeout policies
applied to the same protocol stack using SCTP and DSDV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0337</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0337</id><created>2014-10-01</created><authors><author><keyname>Tiado</keyname><forenames>Mahmadou Issoufou</forenames><affiliation>IRIT</affiliation></author><author><keyname>Dhaou</keyname><forenames>Riadh</forenames><affiliation>IRIT</affiliation></author><author><keyname>Beylot</keyname><forenames>Andr&#xe9;-Luc</forenames><affiliation>IRIT</affiliation></author></authors><title>Cross layer Interaction Models for SCTP and OLSR</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>ICEER, Marrakech : Morocco (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution from wired system to the wireless environment opens a set of
challenge for the improvement of the wireless system performances because of
many of their weakness compared to wired networks. To achieve this goal, cross
layer techniques are used to facilitate the sharing of information between the
layers of the OSI model. In some precedent works, the Reverse Cross Layer (RCL)
method has been proposed to facilitate the design of cross layer conceptual
models. The method has the advantage to highlight the impact of each cross
layer interaction on each protocol in order to update its source code and to
describe the intuitive gains that can be achieve. The method may be applied to
a given protocol stack or to an existent cross layer model to integrate new
interactions. In this paper, we are applying the RCL method on the stack that
uses the Stream Control Transport Protocol (SCTP) at the transport layer and
the Optimized Link State Routing (OLSR) at the network layer. Cross layer
conceptual models are produced based on new cross layer interactions that are
proposed to populate the environment subsystem built with the application of
the RCL method. The improvement of the environment subsystem is specified
through the performance gains provide by the new interactions. The
implementation of the interactions that impact the SCTP protocol is described
in the Interaction Description Array. After the introduction, Section II of
this paper presents an overview of the SCTP protocol. Section III is related to
the overview of the OLSR protocol. Section IV is used for the application of
the RCL method and the different interaction arrays it generates. Section V
presents the improvement of the environment subsystem and the definition of the
performance gain of each Cross Layer Atomic Action (CLAA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0342</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0342</id><created>2014-10-01</created><updated>2015-05-05</updated><authors><author><keyname>Udell</keyname><forenames>Madeleine</forenames></author><author><keyname>Horn</keyname><forenames>Corinne</forenames></author><author><keyname>Zadeh</keyname><forenames>Reza</forenames></author><author><keyname>Boyd</keyname><forenames>Stephen</forenames></author></authors><title>Generalized Low Rank Models</title><categories>stat.ML cs.LG math.OC</categories><comments>84 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal components analysis (PCA) is a well-known technique for
approximating a tabular data set by a low rank matrix. Here, we extend the idea
of PCA to handle arbitrary data sets consisting of numerical, Boolean,
categorical, ordinal, and other data types. This framework encompasses many
well known techniques in data analysis, such as nonnegative matrix
factorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD,
and maximum margin matrix factorization. The method handles heterogeneous data
sets, and leads to coherent schemes for compressing, denoising, and imputing
missing entries across all data types simultaneously. It also admits a number
of interesting interpretations of the low rank factors, which allow clustering
of examples or of features. We propose several parallel algorithms for fitting
generalized low rank models, and describe implementations and numerical
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0343</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0343</id><created>2014-10-01</created><authors><author><keyname>Golosovsky</keyname><forenames>Michael</forenames></author><author><keyname>Solomon</keyname><forenames>Sorin</forenames></author></authors><title>Uncovering the dynamics of citations of scientific papers</title><categories>physics.soc-ph cs.DL</categories><comments>18 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate a comprehensive framework that accounts for citation dynamics
of scientific papers and for the age distribution of references. We show that
citation dynamics of scientific papers is nonlinear and this nonlinearity has
far-reaching consequences, such as diverging citation distributions and runaway
papers. We propose a nonlinear stochastic dynamic model of citation dynamics
based on link copying/redirection mechanism. The model is fully calibrated by
empirical data and does not contain free parameters. This model can be a basis
for quantitative probabilistic prediction of citation dynamics of individual
papers and of the journal impact factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0369</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0369</id><created>2014-10-01</created><authors><author><keyname>Yampolskiy</keyname><forenames>Roman V.</forenames></author></authors><title>The Universe of Minds</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper attempts to describe the space of possible mind designs by first
equating all minds to software. Next it proves some interesting properties of
the mind design space such as infinitude of minds, size and representation
complexity of minds. A survey of mind design taxonomies is followed by a
proposal for a new field of investigation devoted to study of minds,
intellectology, a list of open problems for this new field is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0371</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0371</id><created>2014-09-07</created><authors><author><keyname>Raheja</keyname><forenames>J. L.</forenames></author><author><keyname>Ajay</keyname><forenames>B.</forenames></author><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author></authors><title>Real Time Fabric Defect Detection System on an Embedded DSP Platform</title><categories>cs.CV</categories><comments>Optik Elsevier 2013</comments><doi>10.1016/j.ijleo.2013.03.038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In industrial fabric productions, automated real time systems are needed to
find out the minor defects. It will save the cost by not transporting defected
products and also would help in making compmay image of quality fabrics by
sending out only undefected products. A real time fabric defect detection
system (FDDS), implementd on an embedded DSP platform is presented here.
Textural features of fabric image are extracted based on gray level
co-occurrence matrix (GLCM). A sliding window technique is used for defect
detection where window moves over the whole image computing a textural energy
from the GLCM of the fabric image. The energy values are compared to a
reference and the deviations beyond a threshold are reported as defects and
also visually represented by a window. The implementation is carried out on a
TI TMS320DM642 platform and programmed using code composer studio software. The
real time output of this implementation was shown on a monitor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0373</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0373</id><created>2014-08-27</created><authors><author><keyname>Shafi</keyname><forenames>Aamir</forenames></author><author><keyname>Akhtar</keyname><forenames>Aleem</forenames></author><author><keyname>Javed</keyname><forenames>Ansar</forenames></author><author><keyname>Carpenter</keyname><forenames>Bryan</forenames></author></authors><title>Teaching Parallel Programming Using Java</title><categories>cs.CY cs.DC</categories><comments>8 Pages, 6 figures, MPJ Express, MPI Java, Teaching Parallel
  Programming</comments><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an overview of the &quot;Applied Parallel Computing&quot; course
taught to final year Software Engineering undergraduate students in Spring 2014
at NUST, Pakistan. The main objective of the course was to introduce practical
parallel programming tools and techniques for shared and distributed memory
concurrent systems. A unique aspect of the course was that Java was used as the
principle programming language. The course was divided into three sections. The
first section covered parallel programming techniques for shared memory systems
that include multicore and Symmetric Multi-Processor (SMP) systems. In this
section, Java threads was taught as a viable programming API for such systems.
The second section was dedicated to parallel programming tools meant for
distributed memory systems including clusters and network of computers. We used
MPJ Express-a Java MPI library-for conducting programming assignments and lab
work for this section. The third and the final section covered advanced topics
including the MapReduce programming model using Hadoop and the General Purpose
Computing on Graphics Processing Units (GPGPU).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0375</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0375</id><created>2014-10-01</created><authors><author><keyname>Frongillo</keyname><forenames>Rafael M.</forenames></author><author><keyname>Chen</keyname><forenames>Yiling</forenames></author><author><keyname>Kash</keyname><forenames>Ian A.</forenames></author></authors><title>Elicitation for Aggregation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of eliciting and aggregating probabilistic information
from multiple agents. In order to successfully aggregate the predictions of
agents, the principal needs to elicit some notion of confidence from agents,
capturing how much experience or knowledge led to their predictions. To
formalize this, we consider a principal who wishes to elicit predictions about
a random variable from a group of Bayesian agents, each of whom have privately
observed some independent samples of the random variable, and hopes to
aggregate the predictions as if she had directly observed the samples of all
agents. Leveraging techniques from Bayesian statistics, we represent confidence
as the number of samples an agent has observed, which is quantified by a
hyperparameter from a conjugate family of prior distributions. This then allows
us to show that if the principal has access to a few samples, she can achieve
her aggregation goal by eliciting predictions from agents using proper scoring
rules. In particular, if she has access to one sample, she can successfully
aggregate the agents' predictions if and only if every posterior predictive
distribution corresponds to a unique value of the hyperparameter. Furthermore,
this uniqueness holds for many common distributions of interest. When this
uniqueness property does not hold, we construct a novel and intuitive mechanism
where a principal with two samples can elicit and optimally aggregate the
agents' predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0380</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0380</id><created>2014-10-01</created><updated>2014-11-08</updated><authors><author><keyname>Heindlmaier</keyname><forenames>Michael</forenames></author><author><keyname>Reyhanian</keyname><forenames>Navid</forenames></author><author><keyname>Bidokhti</keyname><forenames>Shirin Saeedi</forenames></author></authors><title>On Capacity Regions of Two-Receiver Broadcast Packet Erasure Channels
  with Feedback and Memory</title><categories>cs.IT math.IT</categories><comments>Corrected version, extended version of work presented at Allerton '14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two-receiver broadcast packet erasure channel with feedback and memory is
studied. Memory is modeled using a finite-state Markov chain representing a
channel state. Outer and inner bounds on the capacity region are derived when
the channel state is strictly causally known at the transmitter. The bounds are
both formulated in terms of feasibility problems and they are matching in all
but one of the constraints. The results are extended to feedback with larger
delay. Numerical results show that the bounds are close in many examples and
the gains offered through feedback can be quite large. The presented outer
bound meets the inner bound recently derived in \cite{Kuo_Wang2014} and hence
describes the capacity region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0382</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0382</id><created>2014-09-05</created><updated>2015-01-19</updated><authors><author><keyname>Andrecut</keyname><forenames>M.</forenames></author></authors><title>A String-Based Public Key Cryptosystem</title><categories>cs.CR physics.data-an</categories><comments>In this revised version of the paper we show that the eavesdropper's
  problem of the proposed cryptosystem has a solution, and we give the details
  of the solution</comments><acm-class>E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional methods in public key cryptography are based on number theory,
and suffer from problems such as dealing with very large numbers, making key
creation cumbersome. Here, we propose a new public key cryptosystem based on
strings only, which avoids the difficulties of the traditional number theory
approach. The security mechanism for public and secret keys generation is
ensured by a recursive encoding mechanism embedded in a
quasi-commutative-random function, resulted from the composition of a
quasi-commutative function with a pseudo-random function. In this revised
version of the paper we show that the eavesdropper's problem of the proposed
cryptosystem has a solution, and we give the details of the solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0389</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0389</id><created>2014-10-01</created><authors><author><keyname>Sharmanska</keyname><forenames>Viktoriia</forenames></author><author><keyname>Quadrianto</keyname><forenames>Novi</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>Learning to Transfer Privileged Information</title><categories>cs.CV stat.ML</categories><comments>20 pages with figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a learning framework called learning using privileged
information (LUPI) to the computer vision field. We focus on the prototypical
computer vision problem of teaching computers to recognize objects in images.
We want the computers to be able to learn faster at the expense of providing
extra information during training time. As additional information about the
image data, we look at several scenarios that have been studied in computer
vision before: attributes, bounding boxes and image tags. The information is
privileged as it is available at training time but not at test time. We explore
two maximum-margin techniques that are able to make use of this additional
source of information, for binary and multiclass object classification. We
interpret these methods as learning easiness and hardness of the objects in the
privileged space and then transferring this knowledge to train a better
classifier in the original space. We provide a thorough analysis and comparison
of information transfer from privileged to the original data spaces for both
LUPI methods. Our experiments show that incorporating privileged information
can improve the classification accuracy. Finally, we conduct user studies to
understand which samples are easy and which are hard for human learning, and
explore how this information is related to easy and hard samples when learning
a classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0390</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0390</id><created>2014-10-01</created><updated>2014-11-13</updated><authors><author><keyname>Kone&#x10d;n&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Simple Complexity Analysis of Simplified Direct Search</title><categories>math.OC cs.CC</categories><comments>21 pages, 5 algorithms, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of unconstrained minimization of a smooth function in
the derivative-free setting using. In particular, we propose and study a
simplified variant of the direct search method (of direction type), which we
call simplified direct search (SDS). Unlike standard direct search methods,
which depend on a large number of parameters that need to be tuned, SDS depends
on a single scalar parameter only.
  Despite relevant research activity in direct search methods spanning several
decades, complexity guarantees---bounds on the number of function evaluations
needed to find an approximate solution---were not established until very
recently. In this paper we give a surprisingly brief and unified analysis of
SDS for nonconvex, convex and strongly convex functions. We match the existing
complexity results for direct search in their dependence on the problem
dimension ($n$) and error tolerance ($\epsilon$), but the overall bounds are
simpler, easier to interpret, and have better dependence on other problem
parameters. In particular, we show that for the set of directions formed by the
standard coordinate vectors and their negatives, the number of function
evaluations needed to find an $\epsilon$-solution is $O(n^2 /\epsilon)$ (resp.
$O(n^2 \log(1/\epsilon))$) for the problem of minimizing a convex (resp.
strongly convex) smooth function. In the nonconvex smooth case, the bound is
$O(n^2/\epsilon^2)$, with the goal being the reduction of the norm of the
gradient below $\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0412</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0412</id><created>2014-10-01</created><updated>2015-12-23</updated><authors><author><keyname>Wittmann</keyname><forenames>M.</forenames></author><author><keyname>Zeiser</keyname><forenames>T.</forenames></author><author><keyname>Hager</keyname><forenames>G.</forenames></author><author><keyname>Wellein</keyname><forenames>G.</forenames></author></authors><title>Modeling and analyzing performance for highly optimized propagation
  steps of the lattice Boltzmann method on sparse lattices</title><categories>cs.DC</categories><comments>Updated and extended version. Submitted to ISC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational fluid dynamics (CFD) requires a vast amount of compute cycles
on contemporary large-scale parallel computers. Hence, performance optimization
is a pivotal activity in this field of computational science. Not only does it
reduce the time to solution, but it also allows to minimize the energy
consumption. In this work we study performance optimizations for an
MPI-parallel lattice Boltzmann-based flow solver that uses a sparse lattice
representation with indirect addressing. First we describe how this indirect
addressing can be minimized in order to increase the single-core and chip-level
performance. Second, the communication overhead is reduced via appropriate
partitioning, but maintaining the single core performance improvements. Both
optimizations allow to run the solver at an operating point with minimal energy
consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0413</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0413</id><created>2014-10-01</created><updated>2014-10-09</updated><authors><author><keyname>Frongillo</keyname><forenames>Rafael M.</forenames></author><author><keyname>Reid</keyname><forenames>Mark D.</forenames></author></authors><title>Risk Dynamics in Trade Networks</title><categories>cs.GT cs.AI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new framework to model interactions among agents which seek to
trade to minimize their risk with respect to some future outcome. We quantify
this risk using the concept of risk measures from finance, and introduce a
class of trade dynamics which allow agents to trade contracts contingent upon
the future outcome. We then show that these trade dynamics exactly correspond
to a variant of randomized coordinate descent. By extending the analysis of
these coordinate descent methods to account for our more organic setting, we
are able to show convergence rates for very general trade dynamics, showing
that the market or network converges to a unique steady state. Applying these
results to prediction markets, we expand on recent results by adding
convergence rates and general aggregation properties. Finally, we illustrate
the generality of our framework by applying it to agent interactions on a
scale-free network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0431</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0431</id><created>2014-10-01</created><updated>2014-12-23</updated><authors><author><keyname>Michelusi</keyname><forenames>Nicolo</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Cross-layer design of distributed sensing-estimation with quality
  feedback, Part I: Optimal schemes</title><categories>cs.SY</categories><comments>To appear on IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2014.2388438</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This two-part paper presents a feedback-based cross-layer framework for
distributed sensing and estimation of a dynamic process by a wireless sensor
network (WSN). Sensor nodes wirelessly communicate measurements to the fusion
center (FC). Cross-layer factors such as packet collisions and the
sensing-transmission costs are considered. Each SN adapts its
sensing-transmission action based on its own local observation quality and the
estimation quality feedback from the FC under cost constraints for each SN. In
this first part, the optimization complexity is reduced by exploiting the
statistical symmetry and large network approximation of the WSN. Structural
properties of the optimal policy are derived for a coordinated and a
decentralized scheme. It is proved that a dense WSN provides sensing diversity,
so that only a few SNs with the best local observation quality need to be
activated, despite the fluctuations of the WSN. The optimal policy dictates
that, when the estimation quality is poor, only the best SNs activate,
otherwise all SNs remain idle to preserve energy. The costs of coordination and
feedback are evaluated, revealing the scalability of the decentralized scheme
to large WSNs, at the cost of performance degradation. Simulation results
demonstrate cost savings from 30% to 70% over a non-adaptive scheme, and
significant gains over a previously proposed estimator which does not consider
these cross-layer factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0434</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0434</id><created>2014-10-01</created><updated>2014-12-23</updated><authors><author><keyname>Michelusi</keyname><forenames>Nicolo</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Cross-layer design of distributed sensing-estimation with quality
  feedback, Part II: Myopic schemes</title><categories>cs.SY</categories><comments>To appear on IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2014.2388440</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This two-part paper presents a feedback-based cross-layer framework for
distributed sensing and estimation of a dynamic process by a wireless sensor
network (WSN). Sensor nodes wirelessly communicate measurements to the fusion
center (FC). Cross-layer factors such as packet collisions and the
sensing-transmission costs are considered. Each SN adapts its
sensing-transmission action based on its own local observation quality and the
estimation quality feedback from the FC under cost constraints for each SN. In
this second part, low-complexity myopic sensing-transmission policies (MPs) are
designed to optimize a trade-off between performance and the cost incurred by
each SN. The MP is computed in closed form for a coordinated scheme, whereas an
iterative algorithm is presented for a decentralized one, which converges to a
local optimum. The MP dictates that, when the estimation quality is poor, only
the best SNs activate, otherwise all SNs remain idle to preserve energy. For
both schemes, the threshold on the estimation quality below which the SNs
remain idle is derived in closed form, and is shown to be independent of the
number of channels. It is also proved that a single channel suffices for
severely energy constrained WSNs. The proposed MPs are shown to yield
near-optimal performance with respect to the optimal policy of Part I, at a
fraction of the complexity, thus being more suitable for practical WSN
deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0440</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0440</id><created>2014-10-01</created><authors><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Beygelzimer</keyname><forenames>Alina</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author></authors><title>Scalable Nonlinear Learning with Adaptive Polynomial Expansions</title><categories>cs.LG stat.ML</categories><comments>To appear in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can we effectively learn a nonlinear representation in time comparable to
linear learning? We describe a new algorithm that explicitly and adaptively
expands higher-order interaction features over base linear representations. The
algorithm is designed for extreme computational efficiency, and an extensive
experimental study shows that its computation/prediction tradeoff ability
compares very favorably against strong baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0443</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0443</id><created>2014-10-01</created><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author><author><keyname>Tyagi</keyname><forenames>Himanshu</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Strong Converse for a Degraded Wiretap Channel via Active Hypothesis
  Testing</title><categories>cs.IT math.IT</categories><comments>This paper was presented at Allerton 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish an upper bound on the rate of codes for a wiretap channel with
public feedback for a fixed probability of error and secrecy parameter. As a
corollary, we obtain a strong converse for the capacity of a degraded wiretap
channel with public feedback. Our converse proof is based on a reduction of
active hypothesis testing for discriminating between two channels to coding for
wiretap channel with feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0444</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0444</id><created>2014-10-01</created><authors><author><keyname>Gu</keyname><forenames>J.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Joint SIC and Relay Selection for Cooperative DS-CDMA Systems</title><categories>cs.IT math.IT</categories><comments>5 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a cross-layer design strategy based on a joint
successive interference cancellation (SIC) detection technique and a
multi-relay selection algorithm for the uplink of cooperative direct-sequence
code-division multiple access (DS-CDMA) systems. We devise a low-cost greedy
list-based SIC (GL-SIC) strategy with RAKE receivers as the front-end that can
approach the maximum likelihood detector performance. %Unlike prior art, the
proposed GL-SIC algorithm %exploits the Euclidean distance between users of
interest, multiple %ordering and their constellation points to build an
effective list %of detection candidates. We also present a low-complexity
multi-relay selection algorithm based on greedy techniques that can approach
the performance of an exhaustive search. %A cross-layer %design strategy that
brings together the proposed GL-SIC algorithm %and the greedy relay selection
is then developed. Simulations show an excellent bit error rate performance of
the proposed detection and relay selection algorithms as compared to existing
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0446</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0446</id><created>2014-10-01</created><authors><author><keyname>Mahyari</keyname><forenames>Arash Golibagh</forenames></author><author><keyname>Aviyente</keyname><forenames>Selin</forenames></author></authors><title>Identification of Dynamic functional brain network states Through Tensor
  Decomposition</title><categories>cs.NE q-bio.NC</categories><comments>2014 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advances in high resolution neuroimaging, there has been a growing
interest in the detection of functional brain connectivity. Complex network
theory has been proposed as an attractive mathematical representation of
functional brain networks. However, most of the current studies of functional
brain networks have focused on the computation of graph theoretic indices for
static networks, i.e. long-time averages of connectivity networks. It is
well-known that functional connectivity is a dynamic process and the
construction and reorganization of the networks is key to understanding human
cognition. Therefore, there is a growing need to track dynamic functional brain
networks and identify time intervals over which the network is
quasi-stationary. In this paper, we present a tensor decomposition based method
to identify temporally invariant 'network states' and find a common topographic
representation for each state. The proposed methods are applied to
electroencephalogram (EEG) data during the study of error-related negativity
(ERN).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0461</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0461</id><created>2014-10-02</created><authors><author><keyname>Wilson</keyname><forenames>Graeme N.</forenames></author><author><keyname>Ramirez-Serrano</keyname><forenames>Alejandro</forenames></author><author><keyname>Sun</keyname><forenames>Qiao</forenames></author></authors><title>Vehicle Parameter Independent Gain Matrix Selection for a Quadrotor
  using State-Space Controller Design Methods</title><categories>cs.SY</categories><comments>8 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With quadrotor use seeing extensive growth in recent years, the autonomous
control of these Unmanned Aerial Vehicles (UAVs) is an increasing relevant and
intersting field. In this paper a linear state-space approach at designing a
stable hover controller in the presence of disturbances is presented along with
simulation of control system performance. Additionally the design of a tracking
system, for linear inertial position and yaw, is presented with simulation
results. The gain matrix developed for this control system is independent of
the specific quadrotor parameters, meaning that this same gain matrix can be
used on a wide variety of quadrotors without modification. The hover and
tracking controllers designed in this paper proved to perform well in
simulation under perturbation disturbances and normally distributed
disturbances on the UAVs linear speeds and angular speeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0462</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0462</id><created>2014-10-02</created><authors><author><keyname>Tr&#xe4;ff</keyname><forenames>Jesper Larsson</forenames></author><author><keyname>Wimmer</keyname><forenames>Martin</forenames></author></authors><title>An improved, easily computable combinatorial lower bound for weighted
  graph bipartitioning</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has recently been much progress on exact algorithms for the
(un)weighted graph (bi)partitioning problem using branch-and-bound and related
methods. In this note we present and improve an easily computable, purely
combinatorial lower bound for the weighted bipartitioning problem. The bound is
computable in $O(n\log n+m)$ time steps for weighted graphs with $n$ vertices
and $m$ edges. In the branch-and-bound setting, the bound for each new
subproblem can be updated in $O(n+(m/n)\log n)$ time steps amortized over a
series of $n$ branching steps; a rarely triggered tightening of the bound
requires search on the graph of unassigned vertices and can take from $O(n+m)$
to $O(nm+n^2\log n)$ steps depending on implementation and possible bound
quality. Representing a subproblem uses $O(n)$ space. Although the bound is
weak, we believe that it can be advantageous in a parallel setting to be able
to generate many subproblems fast, possibly out-weighting the advantages of
tighter, but much more expensive (algebraic, spectral, flow) lower bounds.
  We use a recent priority task-scheduling framework for giving a parallel
implementation, and show the relative improvements in bound quality and
solution speed by the different contributions of the lower bound. A detailed
comparison with standardized input graphs to other lower bounds and frameworks
is pending. Detailed investigations of branching and subproblem selection rules
are likewise not the focus here, but various options are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0471</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0471</id><created>2014-10-02</created><authors><author><keyname>Hussain</keyname><forenames>Zakria</forenames></author><author><keyname>Klami</keyname><forenames>Arto</forenames></author><author><keyname>Kujala</keyname><forenames>Jussi</forenames></author><author><keyname>Leung</keyname><forenames>Alex P.</forenames></author><author><keyname>Pasupa</keyname><forenames>Kitsuchart</forenames></author><author><keyname>Auer</keyname><forenames>Peter</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author><author><keyname>Laaksonen</keyname><forenames>Jorma</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author></authors><title>PinView: Implicit Feedback in Content-Based Image Retrieval</title><categories>cs.IR cs.AI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes PinView, a content-based image retrieval system that
exploits implicit relevance feedback collected during a search session. PinView
contains several novel methods to infer the intent of the user. From relevance
feedback, such as eye movements or pointer clicks, and visual features of
images, PinView learns a similarity metric between images which depends on the
current interests of the user. It then retrieves images with a specialized
online learning algorithm that balances the tradeoff between exploring new
images and exploiting the already inferred interests of the user. We have
integrated PinView to the content-based image retrieval system PicSOM, which
enables applying PinView to real-world image databases. With the new algorithms
PinView outperforms the original PicSOM, and in online experiments with real
users the combination of implicit and explicit feedback gives the best results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0474</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0474</id><created>2014-10-02</created><updated>2015-10-27</updated><authors><author><keyname>Martinec</keyname><forenames>Dan</forenames></author><author><keyname>Herman</keyname><forenames>Ivo</forenames></author><author><keyname>&#x160;ebek</keyname><forenames>Michael</forenames></author></authors><title>A travelling wave approach to a multi-agent system with a path-graph
  topology</title><categories>cs.SY</categories><comments>Submitted to the Automatica journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a novel approach for the analysis and control of a
multi-agent system with non-identical agents and a path-graph topology. With
the help of irrational wave transfer functions, the approach describes the
interaction among the agents from the `local' perspective and identify the
travelling waves in the multi-agent system. The local treatment of the
multi-agent system is complementary to the traditional `overall' approach. It
is shown that the different dynamics of the agents creates a virtual boundary
that causes a partial reflection of the travelling waves. Undesired effects due
to the reflection of the waves, such as amplification/attenuation, long
transient or string instability, can be compensated by the feedback controllers
introduced in this paper. A set of functions in MATLAB, which allows numerical
simulation of the proposed approach, have been made available on MATLAB
Central.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0478</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0478</id><created>2014-10-02</created><authors><author><keyname>Das</keyname><forenames>Nibaran</forenames></author><author><keyname>Pramanik</keyname><forenames>Sandip</forenames></author><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Saha</keyname><forenames>Punam Kumar</forenames></author><author><keyname>Sarkar</keyname><forenames>Ram</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author></authors><title>Recognition of Handwritten Bangla Basic Characters and Digits using
  Convex Hull based Feature Set</title><categories>cs.CV</categories><journal-ref>2009 International Conference on Artificial Intelligence and
  Pattern Recognition, At Orlando, Florida pp. 380-386</journal-ref><doi>10.13140/2.1.3689.4089</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dealing with the problem of recognition of handwritten character patterns
of varying shapes and sizes, selection of a proper feature set is important to
achieve high recognition performance. The current research aims to evaluate the
performance of the convex hull based feature set, i.e. 125 features in all
computed over different bays attributes of the convex hull of a pattern, for
effective recognition of isolated handwritten Bangla basic characters and
digits. On experimentation with a database of 10000 samples, the maximum
recognition rate of 76.86% is observed for handwritten Bangla characters. For
Bangla numerals the maximum success rate of 99.45%. is achieved on a database
of 12000 sample. The current work validates the usefulness of a new kind of
feature set for recognition of handwritten Bangla basic characters and
numerals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0485</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0485</id><created>2014-10-02</created><authors><author><keyname>Parrondo</keyname><forenames>J. M. R.</forenames></author><author><keyname>Dinis</keyname><forenames>L.</forenames></author></authors><title>Brownian motion and gambling: from ratchets to paradoxical games</title><categories>physics.soc-ph cond-mat.stat-mech cs.GT</categories><comments>25 pages, 11 figures</comments><journal-ref>Contemporary Physics 45 (2), 147 (2004)</journal-ref><doi>10.1080/00107510310001644836</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two losing gambling games, when alternated in a periodic or random fashion,
can produce a winning game. This paradox has been inspired by certain physical
systems capable of rectifying fluctuations: the so-called Brownian ratchets. In
this paper we review this paradox, from Brownian ratchets to the most recent
studies on collective games, providing some intuitive explanations of the
unexpected phenomena that we will find along the way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0488</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0488</id><created>2014-10-02</created><authors><author><keyname>Dinis</keyname><forenames>L.</forenames></author><author><keyname>Parrondo</keyname><forenames>J. M. R.</forenames></author></authors><title>Inefficiency of voting in Parrondo games</title><categories>physics.soc-ph cond-mat.stat-mech cs.GT</categories><comments>11 pages, 6 figures</comments><journal-ref>Physica A: Statistical Mechanics and its Applications 343, 701
  (2004)</journal-ref><doi>10.1016/j.physa.2004.06.076</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a modification of the so-called Parrondo's paradox where a large
number of individuals choose the game they want to play by voting. We show that
it can be better for the players to vote randomly than to vote according to
their own benefit in one turn. The former yields a winning tendency while the
latter results in steady losses. An explanation of this behaviour is given by
noting that selfish voting prevents the switching between games that is
essential for the total capital to grow. Results for both finite and infinite
number of players are presented. It is shown that the extension of the model to
the history-dependent Parrondo's paradox also displays the same effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0501</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0501</id><created>2014-10-02</created><authors><author><keyname>Contucci</keyname><forenames>Pierluigi</forenames></author><author><keyname>Vernia</keyname><forenames>Cecilia</forenames></author></authors><title>Alienation in Italian cities. Social network fragmentation from
  collective data</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure of a social network of strong ties (trust network)
investigating its property of connectedness versus fragmentation. To this
purpose we analyse an extensive set of census data, about marrying or having
children with immigrants, collected by Italian national statistical institute
for all Italian municipalities from 2001 to 2011. Not using neither obtaining
personal local information but only average ones, our method fully complies
with privacy and confidentiality. Our findings show that large cities display
the behaviour of highly fragmented trust networks where individuals face
possible phenomena of alienation. Smaller cities and villages instead behave
like fully connected social systems with a rich tie structure, where isolation
is rare or completely absent. While confirming classical sociological theories
on alienation in large urban areas our approach provides a quantitative method
to test them and a predictive tool for policy makers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0507</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0507</id><created>2014-10-02</created><authors><author><keyname>Echeveste</keyname><forenames>Rodrigo</forenames></author><author><keyname>Gros</keyname><forenames>Claudius</forenames></author></authors><title>Generating functionals for computational intelligence: the Fisher
  information as an objective function for self-limiting Hebbian learning rules</title><categories>q-bio.NC cond-mat.dis-nn cs.NE</categories><journal-ref>Frontiers in Robotics and AI 1, 1 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating functionals may guide the evolution of a dynamical system and
constitute a possible route for handling the complexity of neural networks as
relevant for computational intelligence. We propose and explore a new objective
function, which allows to obtain plasticity rules for the afferent synaptic
weights. The adaption rules are Hebbian, self-limiting, and result from the
minimization of the Fisher information with respect to the synaptic flux. We
perform a series of simulations examining the behavior of the new learning
rules in various circumstances. The vector of synaptic weights aligns with the
principal direction of input activities, whenever one is present. A linear
discrimination is performed when there are two or more principal directions;
directions having bimodal firing-rate distributions, being characterized by a
negative excess kurtosis, are preferred. We find robust performance and full
homeostatic adaption of the synaptic weights results as a by-product of the
synaptic flux minimization. This self-limiting behavior allows for stable
online learning for arbitrary durations. The neuron acquires new information
when the statistics of input activities is changed at a certain point of the
simulation, showing however, a distinct resilience to unlearn previously
acquired knowledge. Learning is fast when starting with randomly drawn synaptic
weights and substantially slower when the synaptic weights are already fully
adapted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0510</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0510</id><created>2014-10-02</created><authors><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Deep Sequential Neural Network</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural Networks sequentially build high-level features through their
successive layers. We propose here a new neural network model where each layer
is associated with a set of candidate mappings. When an input is processed, at
each layer, one mapping among these candidates is selected according to a
sequential decision process. The resulting model is structured according to a
DAG like architecture, so that a path from the root to a leaf node defines a
sequence of transformations. Instead of considering global transformations,
like in classical multilayer networks, this model allows us for learning a set
of local transformations. It is thus able to process data with different
characteristics through specific sequences of such local transformations,
increasing the expression power of this model w.r.t a classical multilayered
network. The learning algorithm is inspired from policy gradient techniques
coming from the reinforcement learning domain and is used here instead of the
classical back-propagation based gradient descent techniques. Experiments on
different datasets show the relevance of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0519</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0519</id><created>2014-10-02</created><authors><author><keyname>Sahba</keyname><forenames>Farshid</forenames></author><author><keyname>Nazaridoust</keyname><forenames>Maryam</forenames></author></authors><title>Museum Automation with RFID</title><categories>cs.CY</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By increase of culture and knowledge of the people, request for visiting
museums has increased and made the management of these places more complex.
Valuable things in a museum or ancient place must be maintained well and also
it need to managing visitors. To maintain things we should prevent them from
theft, as well as environmental factors such as temperature, humidity, PH,
chemical factors and mechanical events should be monitored. And if the
conditions are damaging, appropriate alerts or reports to managers and experts
should be announced. Visitors should also be monitored, as well as visitors
need to be guided and getting information in the environment. By utilizing RFID
technology and short-distance network tools, technical solutions for more
efficient management and more effective retention in museums can be
implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0531</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0531</id><created>2014-10-02</created><authors><author><keyname>Ishengoma</keyname><forenames>Fredrick Romanus</forenames></author></authors><title>Online Social Networks and Terrorism 2.0 in Developing Countries</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>11 pages</comments><journal-ref>International Journal of Computer Science and Network Solutions,
  Volume 1, Issue 4, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advancement in technology has brought a new era in terrorism where Online
Social Networks have become a major platform of communication with wide range
of usage from message channeling to propaganda and recruitment of new followers
in terrorist groups. Meanwhile, during the terrorist attacks people use social
networks for information exchange, mobilizing and uniting and raising money for
the victims. This paper critically analyses the specific usage of social
networks in the times of terrorism attacks in developing countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0533</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0533</id><created>2014-10-02</created><authors><author><keyname>Mtaho</keyname><forenames>Adam B.</forenames></author><author><keyname>Ishengoma</keyname><forenames>Fredrick Romanus</forenames></author></authors><title>Factors Affecting QoS in Tanzania Cellular Networks</title><categories>cs.CY</categories><comments>7 Pages</comments><journal-ref>International Journal of Computer Science and Network SolutionsI,
  Volume 2, Issue 4, April, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality of service in cellular communication system is a topic that recently
has raised much interest for many researchers. This paper presents the findings
obtained from the study on factors affecting QoS in Tanzania cellular networks.
The study was carried out in Dodoma Municipal, Tanzania. The study employed
cross sectional research design. Information was gathered from structured
questionnaire of 240 subscribers during the study of quality of service for the
four leading cellular networks in Tanzania. Both qualitative and quantitative
data from field survey were collected and analyzed using Statistical Package
for Social Sciences and Excel software. The study findings show that the major
factors that degrade QoS in Tanzania cellular networks are inadequate network
infrastructure, lack of fairness from service providers and little efforts
taken by the government in enforcing the national agreed standards. Other
factors are lack of reliable end to end systems, geographical terrain, low
quality handsets, poor government monitoring on standards and lack of
subscriber skills and training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0534</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0534</id><created>2014-10-02</created><authors><author><keyname>Ishengoma</keyname><forenames>Fredrick Romanus</forenames></author></authors><title>Authentication System for Smart Homes Based on ARM7TDMI-S and
  IRIS-Fingerprint Recognition Technologies</title><categories>cs.CR</categories><journal-ref>CiiT International Journal of Programmable Device Circuits and
  Systems, Volume 6, Issue 6, August 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid advancement in technology, smart homes have become applicable
and so the need arise to solve the security challenges that are accompanied
with its operation. Passwords and identity cards have been used as traditional
authentication mechanisms in home environments, however, the rise of misuse of
these mechanisms are proving them to be less reliable. For instance, ID cards
can be misplaced, copied or counterfeited and being misused. Conversely,
studies have shown that biometrics authentication systems particularly Iris
Recognition Technology (IRT) and Fingerprint Recognition Technology (FRT) have
the most reliable mechanisms to date providing tremendous accuracy and speed.
As the technology becomes less expensive, application of IRT&amp; FRT in
smart-homes becomes more reliable and appropriate solution for security
challenges. In this paper, we present our approach to design an authentication
system for smart homes based on IRT, FRT and ARM7TDMI.The system employs two
biometrics mechanisms for high reliability whereby initially, system users must
enroll their fingerprints and eyes into the camera. Iris and fingerprint
biometrics are scanned and the images are stored in the database. In the stage
of authentication, FRT and IRT fingerprint scan and analyze points of the
user's current input iris and fingerprint and match with the database contents.
If one or more captured images do not match with the one in the database, then
the system will not give authorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0540</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0540</id><created>2014-10-02</created><authors><author><keyname>Biniaz</keyname><forenames>Ahmad</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Matching in Gabriel Graphs</title><categories>cs.CG</categories><comments>arXiv admin note: text overlap with arXiv:1409.5466</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of $n$ points in the plane, the order-$k$ Gabriel graph on
$P$, denoted by $k$-$GG$, has an edge between two points $p$ and $q$ if and
only if the closed disk with diameter $pq$ contains at most $k$ points of $P$,
excluding $p$ and $q$. We study matching problems in $k$-$GG$ graphs. We show
that a Euclidean bottleneck perfect matching of $P$ is contained in $10$-$GG$,
but $8$-$GG$ may not have any Euclidean bottleneck perfect matching. In
addition we show that $0$-$GG$ has a matching of size at least $\frac{n-1}{4}$
and this bound is tight. We also prove that $1$-$GG$ has a matching of size at
least $\frac{2(n-1)}{5}$ and $2$-$GG$ has a perfect matching. Finally we
consider the problem of blocking the edges of $k$-$GG$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0547</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0547</id><created>2014-10-02</created><updated>2015-01-15</updated><authors><author><keyname>Preen</keyname><forenames>Richard J.</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Design Mining Interacting Wind Turbines</title><categories>cs.NE cs.AI cs.CE</categories><doi>10.1162/EVCO_a_00144</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An initial study of surrogate-assisted evolutionary algorithms used to design
vertical-axis wind turbines wherein candidate prototypes are evaluated under
fan generated wind conditions after being physically instantiated by a 3D
printer has recently been presented. Unlike other approaches, such as
computational fluid dynamics simulations, no mathematical formulations were
used and no model assumptions were made. This paper extends that work by
exploring alternative surrogate modelling and evolutionary techniques. The
accuracy of various modelling algorithms used to estimate the fitness of
evaluated individuals from the initial experiments is compared. The effect of
temporally windowing surrogate model training samples is explored. A
surrogate-assisted approach based on an enhanced local search is introduced;
and alternative coevolution collaboration schemes are examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0553</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0553</id><created>2014-10-02</created><updated>2015-09-09</updated><authors><author><keyname>Cohen-Addad</keyname><forenames>Vincent</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author></authors><title>The Unreasonable Success of Local Search: Geometric Optimization</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is the effectiveness of local search algorithms for geometric problems
in the plane? We prove that local search with neighborhoods of magnitude
$1/\epsilon^c$ is an approximation scheme for the following problems in the
Euclidian plane: TSP with random inputs, Steiner tree with random inputs,
facility location (with worst case inputs), and bicriteria $k$-median (also
with worst case inputs). The randomness assumption is necessary for TSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0562</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0562</id><created>2014-10-02</created><authors><author><keyname>Pantaleoni</keyname><forenames>Jacopo</forenames></author></authors><title>A massively parallel algorithm for constructing the BWT of large string
  sets</title><categories>cs.DS cs.DC</categories><report-no>NVR-2014-002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new scalable, lightweight algorithm to incrementally construct
the BWT and FM-index of large string sets such as those produced by Next
Generation Sequencing. The algorithm is designed for massive parallelism and
can effectively exploit the combination of low capacity high bandwidth memory
and slower external system memory typical of GPU accelerated systems.
Particularly, for a string set of n characters from an alphabet with \sigma
symbols, it uses a constant amount of high-bandwidth memory and at most 3n
log(\sigma) bits of system memory. Given that deep memory hierarchies are
becoming a pervasive trait of high performance computing architectures, we
believe this to be a relevant feature. The implementation can handle reads of
arbitrary length and is up to 2 and respectively 6.5 times faster than
state-of-the-art for short and long genomic reads
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0564</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0564</id><created>2014-10-02</created><authors><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames></author></authors><title>Automatic Generation of Loop-Invariants for Matrix Operations</title><categories>cs.MS cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years it has been shown that for many linear algebra operations it
is possible to create families of algorithms following a very systematic
procedure. We do not refer to the fine tuning of a known algorithm, but to a
methodology for the actual generation of both algorithms and routines to solve
a given target matrix equation. Although systematic, the methodology relies on
complex algebraic manipulations and non-obvious pattern matching, making the
procedure challenging to be performed by hand, our goal is the development of a
fully automated system that from the sole description of a target equation
creates multiple algorithms and routines. We present CL1ck, a symbolic system
written in Mathematica, that starts with an equation, decomposes it into
multiple equations, and returns a set of loop-invariants for the algorithms --
yet to be generated -- that will solve the equation. In a successive step each
loop-invariant is then mapped to its corresponding algorithm and routine. For a
large class of equations, the methodology generates known algorithms as well as
many previously unknown ones. Most interestingly, the methodology unifies
algorithms traditionally developed in isolation. As an example, the five well
known algorithms for the LU factorization are for the first time unified under
a common root.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0567</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0567</id><created>2014-10-02</created><authors><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames></author></authors><title>Knowledge-Based Automatic Generation of Partitioned Matrix Expressions</title><categories>cs.MS cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a series of papers it has been shown that for many linear algebra
operations it is possible to generate families of algorithms by following a
systematic procedure. Although powerful, such a methodology involves complex
algebraic manipulation, symbolic computations and pattern matching, making the
generation a process challenging to be performed by hand. We aim for a fully
automated system that from the sole description of a target operation creates
multiple algorithms without any human intervention. Our approach consists of
three main stages. The first stage yields the core object for the entire
process, the Partitioned Matrix Expression (PME), which establishes how the
target problem may be decomposed in terms of simpler sub-problems. In the
second stage the PME is inspected to identify predicates, the Loop-Invariants,
to be used to set up the skeleton of a family of proofs of correctness. In the
third and last stage the actual algorithms are constructed so that each of them
satisfies its corresponding proof of correctness. In this paper we focus on the
first stage of the process, the automatic generation of Partitioned Matrix
Expressions. In particular, we discuss the steps leading to a PME and the
knowledge necessary for a symbolic system to perform such steps. We also
introduce Cl1ck, a prototype system written in Mathematica that generates PMEs
automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0569</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0569</id><created>2014-10-02</created><authors><author><keyname>Haustein</keyname><forenames>Stefanie</forenames></author><author><keyname>Larivi&#xe8;re</keyname><forenames>Vincent</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author><author><keyname>Amyot</keyname><forenames>Didier</forenames></author><author><keyname>Peters</keyname><forenames>Isabella</forenames></author></authors><title>Tweets vs. Mendeley readers: How do these two social media metrics
  differ?</title><categories>cs.DL</categories><journal-ref>Information Technology 56(5) (2014) 207-215</journal-ref><doi>10.1515/itit-2014-1048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of 1.4 million biomedical papers was analyzed with regards to how often
articles are mentioned on Twitter or saved by users on Mendeley. While Twitter
is a microblogging platform used by a general audience to distribute
information, Mendeley is a reference manager targeted at an academic user group
to organize scholarly literature. Both platforms are used as sources for
so-called altmetrics to measure a new kind of research impact. This analysis
shows in how far they differ and compare to traditional citation impact metrics
based on a large set of PubMed papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0571</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0571</id><created>2014-10-02</created><updated>2014-10-23</updated><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames></author><author><keyname>Litvak</keyname><forenames>Nelly</forenames></author><author><keyname>Prokhorenkova</keyname><forenames>Liudmila Ostroumova</forenames></author><author><keyname>Suyargulova</keyname><forenames>Eugenia</forenames></author></authors><title>Quick Detection of High-degree Entities in Large Directed Networks</title><categories>cs.SI cs.DS physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of quick detection of high-degree
entities in large online social networks. Practical importance of this problem
is attested by a large number of companies that continuously collect and update
statistics about popular entities, usually using the degree of an entity as an
approximation of its popularity. We suggest a simple, efficient, and easy to
implement two-stage randomized algorithm that provides highly accurate
solutions for this problem. For instance, our algorithm needs only one thousand
API requests in order to find the top-100 most followed users in Twitter, a
network with approximately a billion of registered users, with more than 90%
precision. Our algorithm significantly outperforms existing methods and serves
many different purposes, such as finding the most popular users or the most
popular interest groups in social networks. An important contribution of this
work is the analysis of the proposed algorithm using Extreme Value Theory -- a
branch of probability that studies extreme events and properties of largest
order statistics in random samples. Using this theory, we derive an accurate
prediction for the algorithm's performance and show that the number of API
requests for finding the top-k most popular entities is sublinear in the number
of entities. Moreover, we formally show that the high variability among the
entities, expressed through heavy-tailed distributions, is the reason for the
algorithm's efficiency. We quantify this phenomenon in a rigorous mathematical
way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0572</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0572</id><created>2014-10-02</created><updated>2014-10-06</updated><authors><author><keyname>Mani</keyname><forenames>A.</forenames></author></authors><title>Algebraic Semantics of Proto-Transitive Rough Sets</title><categories>cs.AI math.LO math.RA</categories><comments>90 pages, 2 figures, 1 table Pre-Publication Monograph, 1st edition</comments><msc-class>03B52, 08A02, 08A99, 08A55, 68T99, 03G99, 03E99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rough sets over generalized transitive relations like proto-transitive ones
had been initiated by the present author in the year 2012. Subsequently,
approximation of proto-transitive relations by other relations was investigated
and the relation with rough approximations was developed towards constructing
semantics that can handle fragments of structure. It was also proved that
difference of approximations induced by some approximate relations need not
induce rough structures. In this research we develop different semantics of
proto transitive rough sets (PRAX) after characterizing the structure of rough
objects and also develop a theory of dependence for general rough sets and use
it to internalize the Nelson-algebra based approximate semantics developed
earlier. The theory of rough dependence initiated later by the present author
is extended in the process. This monograph is reasonably self-contained and
includes proofs and extensions of representation of objects that were not part
of earlier papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0573</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0573</id><created>2014-10-02</created><authors><author><keyname>Nickson</keyname><forenames>Thomas</forenames></author><author><keyname>Potapov</keyname><forenames>Igor</forenames></author></authors><title>Broadcasting Automata and Patterns on Z^2</title><categories>cs.FL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Broadcasting Automata model draws inspiration from a variety of sources
such as Ad-Hoc radio networks, cellular automata, neighbourhood se- quences and
nature, employing many of the same pattern forming methods that can be seen in
the superposition of waves and resonance. Algorithms for broad- casting
automata model are in the same vain as those encountered in distributed
algorithms using a simple notion of waves, messages passed from automata to au-
tomata throughout the topology, to construct computations. The waves generated
by activating processes in a digital environment can be used for designing a
vari- ety of wave algorithms. In this chapter we aim to study the geometrical
shapes of informational waves on integer grid generated in broadcasting
automata model as well as their potential use for metric approximation in a
discrete space. An explo- ration of the ability to vary the broadcasting radius
of each node leads to results of categorisations of digital discs, their form,
composition, encodings and gener- ation. Results pertaining to the nodal
patterns generated by arbitrary transmission radii on the plane are explored
with a connection to broadcasting sequences and ap- proximation of discrete
metrics of which results are given for the approximation of astroids, a
previously unachievable concave metric, through a novel application of the
aggregation of waves via a number of explored functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0576</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0576</id><created>2014-10-02</created><authors><author><keyname>Pavlovskaia</keyname><forenames>Maria</forenames></author><author><keyname>Tu</keyname><forenames>Kewei</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author></authors><title>Mapping Energy Landscapes of Non-Convex Learning Problems</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many statistical learning problems, the target functions to be optimized
are highly non-convex in various model spaces and thus are difficult to
analyze. In this paper, we compute \emph{Energy Landscape Maps} (ELMs) which
characterize and visualize an energy function with a tree structure, in which
each leaf node represents a local minimum and each non-leaf node represents the
barrier between adjacent energy basins. The ELM also associates each node with
the estimated probability mass and volume for the corresponding energy basin.
We construct ELMs by adopting the generalized Wang-Landau algorithm and
multi-domain sampler that simulates a Markov chain traversing the model space
by dynamically reweighting the energy function. We construct ELMs in the model
space for two classic statistical learning problems: i) clustering with
Gaussian mixture models or Bernoulli templates; and ii) bi-clustering. We
propose a way to measure the difficulties (or complexity) of these learning
problems and study how various conditions affect the landscape complexity, such
as separability of the clusters, the number of examples, and the level of
supervision; and we also visualize the behaviors of different algorithms, such
as K-mean, EM, two-step EM and Swendsen-Wang cuts, in the energy landscapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0582</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0582</id><created>2014-10-02</created><updated>2015-04-01</updated><authors><author><keyname>Kennedy</keyname><forenames>Hugh L.</forenames></author></authors><title>Multidimensional Digital Smoothing Filters for Target Detection</title><categories>cs.CV</categories><comments>With galley proof fixes</comments><journal-ref>Signal Processing, Volume 114, September 2015, Pages 251-264</journal-ref><doi>10.1016/j.sigpro.2015.03.005.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive, causal and non-causal, multidimensional digital filters, with
infinite impulse responses and maximally flat magnitude and delay responses in
the low-frequency region, are designed to negate correlated clutter and
interference in the background and to accumulate power due to dim targets in
the foreground of a surveillance sensor. Expressions relating mean
impulse-response duration, frequency selectivity and group delay, to low-order
linear-difference-equation coefficients are derived using discrete Laguerre
polynomials and discounted least-squares regression, then verified through
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0586</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0586</id><created>2014-10-02</created><authors><author><keyname>Fotouhi</keyname><forenames>Babak</forenames></author><author><keyname>Momeni</keyname><forenames>Naghmeh</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael G.</forenames></author></authors><title>Generalized Friendship Paradox: An Analytical Approach</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The friendship paradox refers to the sociological observation that, while the
people's assessment of their own popularity is typically self-aggrandizing, in
reality they are less popular than their friends. The generalized friendship
paradox is the average alter superiority observed empirically in social
settings, scientific collaboration networks, as well as online social media. We
posit a quality-based network growth model in which the chance for a node to
receive new links depends both on its degree and a quality parameter. Nodes are
assigned qualities the first time they join the network, and these do not
change over time. We analyse the model theoretically, finding expressions for
the joint degree-quality distribution and nearest-neighbor distribution. We
then demonstrate that this model exhibits both the friendship paradox and the
generalized friendship paradox at the network level, regardless of the
distribution of qualities. We also show that, in the proposed model, the degree
and quality of each node are positively correlated regardless of how node
qualities are distributed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0589</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0589</id><created>2014-10-02</created><updated>2015-10-08</updated><authors><author><keyname>Oliveira</keyname><forenames>Mateus de Oliveira</forenames></author></authors><title>An Algorithmic Metatheorem for Directed Treewidth</title><categories>cs.DS cs.CC cs.FL cs.LO</categories><comments>41 pages, 6 figures, Accepted to Discrete Applied Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of directed treewidth was introduced by Johnson, Robertson,
Seymour and Thomas [Journal of Combinatorial Theory, Series B, Vol 82, 2001] as
a first step towards an algorithmic metatheory for digraphs. They showed that
some NP-complete properties such as Hamiltonicity can be decided in polynomial
time on digraphs of constant directed treewidth. Nevertheless, despite more
than one decade of intensive research, the list of hard combinatorial problems
that are known to be solvable in polynomial time when restricted to digraphs of
constant directed treewidth has remained scarce. In this work we enrich this
list by providing for the first time an algorithmic metatheorem connecting the
monadic second order logic of graphs to directed treewidth. We show that most
of the known positive algorithmic results for digraphs of constant directed
treewidth can be reformulated in terms of our metatheorem. Additionally, we
show how to use our metatheorem to provide polynomial time algorithms for two
classes of combinatorial problems that have not yet been studied in the context
of directed width measures. More precisely, for each fixed $k,w \in
\mathbb{N}$, we show how to count in polynomial time on digraphs of directed
treewidth $w$, the number of minimum spanning strong subgraphs that are the
union of $k$ directed paths, and the number of maximal subgraphs that are the
union of $k$ directed paths and satisfy a given minor closed property. To prove
our metatheorem we devise two technical tools which we believe to be of
independent interest. First, we introduce the notion of tree-zig-zag number of
a digraph, a new directed width measure that is at most a constant times
directed treewidth. Second, we introduce the notion of $z$-saturated tree slice
language, a new formalism for the specification and manipulation of infinite
sets of digraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0600</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0600</id><created>2014-10-02</created><updated>2014-10-07</updated><authors><author><keyname>Fourny</keyname><forenames>Ghislain</forenames></author></authors><title>Cell Stores</title><categories>cs.DB</categories><comments>Technical report - 10 pages</comments><msc-class>68P15</msc-class><acm-class>C.2.4; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell stores provide a relational-like, tabular level of abstraction to
business users while leveraging recent database technologies, such as key-value
stores and document stores. This allows to scale up and out the efficient
storage and retrieval of highly dimensional data. Cells are the primary
citizens and exist in different forms, which can be explained with an analogy
to the state of matter: as a gas for efficient storage, as a solid for
efficient retrieval, and as a liquid for efficient interaction with the
business users. Cell stores were abstracted from, and are compatible with the
XBRL standard for importing and exporting data. The first cell store repository
contains roughly 200GB of SEC filings data, and proves that retrieving data
cubes can be performed in real time (the threshold acceptable by a human user
being at most a few seconds).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0602</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0602</id><created>2014-10-02</created><authors><author><keyname>Santana</keyname><forenames>Roberto</forenames></author><author><keyname>McDonald</keyname><forenames>Ross B.</forenames></author><author><keyname>Katzgraber</keyname><forenames>Helmut G.</forenames></author></authors><title>A probabilistic evolutionary optimization approach to compute
  quasiparticle braids</title><categories>quant-ph cs.NE</categories><comments>9 pages,7 figures. Accepted at SEAL 2014</comments><journal-ref>Simulated Evolution and Learning, Lecture Notes in Computer
  Science 8886, 13 (2014)</journal-ref><doi>10.1007/978-3-319-13563-2_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topological quantum computing is an alternative framework for avoiding the
quantum decoherence problem in quantum computation. The problem of executing a
gate in this framework can be posed as the problem of braiding quasiparticles.
Because these are not Abelian, the problem can be reduced to finding an optimal
product of braid generators where the optimality is defined in terms of the
gate approximation and the braid's length. In this paper we propose the use of
different variants of estimation of distribution algorithms to deal with the
problem. Furthermore, we investigate how the regularities of the braid
optimization problem can be translated into statistical regularities by means
of the Boltzmann distribution. We show that our best algorithm is able to
produce many solutions that approximates the target gate with an accuracy in
the order of $10^{-6}$, and have lengths up to 9 times shorter than those
expected from braids of the same accuracy obtained with other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0610</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0610</id><created>2014-10-02</created><authors><author><keyname>Liu</keyname><forenames>Zhe</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>Is Twitter a Public Sphere for Online Conflicts? A Cross-Ideological and
  Cross-Hierarchical Look</title><categories>cs.SI physics.soc-ph</categories><comments>To appear in the 6th International Conference on Social Informatics
  (SocInfo 2014), Barcelona</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise in popularity of Twitter has led to a debate on its impact on public
opinions. The optimists foresee an increase in online participation and
democratization due to social media's personal and interactive nature.
Cyber-pessimists, on the other hand, explain how social media can lead to
selective exposure and can be used as a disguise for those in power to
disseminate biased information. To investigate this debate empirically, we
evaluate Twitter as a public sphere using four metrics: equality, diversity,
reciprocity and quality. Using these measurements, we analyze the communication
patterns between individuals of different hierarchical levels and ideologies.
We do this within the context of three diverse conflicts: Israel-Palestine, US
Democrats-Republicans, and FC Barcelona-Real Madrid. In all cases, we collect
data around a central pair of Twitter accounts representing the two main
parties. Our results show in a quantitative manner that Twitter is not an ideal
public sphere for democratic conversations and that hierarchical effects are
part of the reason why it is not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0617</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0617</id><created>2014-10-02</created><authors><author><keyname>Kanna</keyname><forenames>Sithan</forenames></author><author><keyname>Dini</keyname><forenames>Dahir H.</forenames></author><author><keyname>Xia</keyname><forenames>Yili</forenames></author><author><keyname>Hui</keyname><forenames>Ron</forenames></author><author><keyname>Mandic</keyname><forenames>Danilo P.</forenames></author></authors><title>Distributed Widely Linear Frequency Estimation in Unbalanced Three Phase
  Power Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel method for distributed estimation of the frequency of power systems
is introduced based on the cooperation between multiple measurement nodes. The
proposed distributed widely linear complex Kalman filter (D-ACKF) and the
distributed widely linear extended complex Kalman filter (D-AECKF) employ a
widely linear state space and augmented complex statistics to deal with
unbalanced system conditions and the generality complex signals, both second
order circular (proper) and second order noncircular (improper). It is shown
that the current, strictly linear, estimators are inadequate for unbalanced
systems, a typical case in smart grids, as they do not account for either the
noncircularity of Clarke's \alpha \beta-voltage in unbalanced conditions or the
correlated nature of nodal disturbances. We illuminate the relationship between
the degree of circularity of Clarke's voltage and system imbalance, and prove
that the proposed widely linear estimators are optimal for such conditions,
while also accounting for the correlated and noncircular nature of real-world
nodal disturbances. {Synthetic and real world} case studies over a range of
power system conditions illustrate the theoretical and practical advantages of
the proposed methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0628</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0628</id><created>2014-10-01</created><updated>2015-12-16</updated><authors><author><keyname>De Vogeleer</keyname><forenames>Karel</forenames></author><author><keyname>Memmi</keyname><forenames>Gerard</forenames></author><author><keyname>Jouvelot</keyname><forenames>Pierre</forenames></author><author><keyname>Coelho</keyname><forenames>Fabien</forenames></author></authors><title>Theoretical Analysis of Radiative Cooling for Mobile and Embedded
  Systems</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new global analytical model of the heat dissipation process that occurs in
passively-cooled embedded systems is introduced, and we explicit under what
circumstances the traditional assumption that exponential cooling laws apply in
such context is valid. Since the power consumption and reliability of
microprocessors are highly dependent on temperature, management units need
accurate thermal models. Exponential cooling models are justified for
actively-cooled systems. Here, we analyze the tractability of the cooling law
for a passively cooled body, subject to radiative and convective cooling,
including internal heat generation. Focusing then on embedded system-like
objects, we compare the performance difference between our new passive cooling
law and the conventionally-used exponential one. We show that, for quasi
isothermal cooling surfaces of the order of 1\,dm$^2$ or greater, the radiative
cooling effect may become comparable to the convective cooling one. In other
words, radiation becomes non-negligible for systems with a cooling surface
larger than about 1\,dm$^2$. Otherwise for surfaces below 1\,dm$^2$, we show
that the differences between the exact solution and the exponential cooling law
becomes negligible. In the absence of accurate temperature measurements, an
exponential cooling model is shown to be accurate enough for systems, such as
small-sized SoCs, that require low processing overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0630</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0630</id><created>2014-10-02</created><authors><author><keyname>Ozair</keyname><forenames>Sherjil</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Deep Directed Generative Autoencoders</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For discrete data, the likelihood $P(x)$ can be rewritten exactly and
parametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$
has enough capacity to put no probability mass on any $x'$ for which $f(x')\neq
f(x)$, where $f(\cdot)$ is a deterministic discrete function. The log of the
first factor gives rise to the log-likelihood reconstruction error of an
autoencoder with $f(\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic)
decoder. The log of the second term can be seen as a regularizer on the encoded
activations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder
can be represented by a deep neural network and trained to maximize the average
of the optimal log-likelihood $\log p(x)$. The objective is to learn an encoder
$f(\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than
$X$ itself, estimated by $P(H)$. This &quot;flattens the manifold&quot; or concentrates
probability mass in a smaller number of (relevant) dimensions over which the
distribution factorizes. Generating samples from the model is straightforward
using ancestral sampling. One challenge is that regular back-propagation cannot
be used to obtain the gradient on the parameters of the encoder, but we find
that using the straight-through estimator works well here. We also find that
although optimizing a single level of such architecture may be difficult, much
better results can be obtained by pre-training and stacking them, gradually
transforming the data distribution into one that is more easily captured by a
simple parametric model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0633</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0633</id><created>2014-10-02</created><updated>2015-05-24</updated><authors><author><keyname>Pimentel-Alarc&#xf3;n</keyname><forenames>Daniel L.</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author><author><keyname>Boston</keyname><forenames>Nigel</forenames></author></authors><title>Deterministic Conditions for Subspace Identifiability from Incomplete
  Sampling</title><categories>stat.ML cs.LG math.CO</categories><comments>To appear in Proc. of IEEE ISIT, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a generic $r$-dimensional subspace of $\mathbb{R}^d$, $r&lt;d$, and
suppose that we are only given projections of this subspace onto small subsets
of the canonical coordinates. The paper establishes necessary and sufficient
deterministic conditions on the subsets for subspace identifiability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0640</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0640</id><created>2014-10-02</created><updated>2014-10-06</updated><authors><author><keyname>Escalante</keyname><forenames>Hugo Jair</forenames></author><author><keyname>Garc&#xed;a-Lim&#xf3;n</keyname><forenames>Mauricio A.</forenames></author><author><keyname>Morales-Reyes</keyname><forenames>Alicia</forenames></author><author><keyname>Graff</keyname><forenames>Mario</forenames></author><author><keyname>Montes-y-G&#xf3;mez</keyname><forenames>Manuel</forenames></author><author><keyname>Morales</keyname><forenames>Eduardo F.</forenames></author></authors><title>Term-Weighting Learning via Genetic Programming for Text Classification</title><categories>cs.NE cs.LG</categories><msc-class>68T50, 68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel approach to learning term-weighting schemes
(TWSs) in the context of text classification. In text mining a TWS determines
the way in which documents will be represented in a vector space model, before
applying a classifier. Whereas acceptable performance has been obtained with
standard TWSs (e.g., Boolean and term-frequency schemes), the definition of
TWSs has been traditionally an art. Further, it is still a difficult task to
determine what is the best TWS for a particular problem and it is not clear
yet, whether better schemes, than those currently available, can be generated
by combining known TWS. We propose in this article a genetic program that aims
at learning effective TWSs that can improve the performance of current schemes
in text classification. The genetic program learns how to combine a set of
basic units to give rise to discriminative TWSs. We report an extensive
experimental study comprising data sets from thematic and non-thematic text
classification as well as from image classification. Our study shows the
validity of the proposed method; in fact, we show that TWSs learned with the
genetic program outperform traditional schemes and other TWSs proposed in
recent works. Further, we show that TWSs learned from a specific domain can be
effectively used for other tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0642</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0642</id><created>2014-09-27</created><authors><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author></authors><title>A Note on Archetypal Analysis and the Approximation of Convex Hulls</title><categories>cs.NA cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We briefly review the basic ideas behind archetypal analysis for matrix
factorization and discuss its behavior in approximating the convex hull of a
data sample. We then ask how good such approximations can be and consider
different cases. Understanding archetypal analysis as the problem of computing
a convexity constrained low-rank approximation of the identity matrix provides
estimates for archetypal analysis and the SiVM heuristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0650</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0650</id><created>2014-10-02</created><updated>2014-10-08</updated><authors><author><keyname>Lamperski</keyname><forenames>Andrew</forenames></author></authors><title>Stability of Asynchronous Networked Control Systems with Probabilistic
  Clocks</title><categories>cs.SY math.OC</categories><comments>This paper has been withdrawn due to mathematical errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the stability of sampled and networked control systems
with sampling and communication times governed by probabilistic clocks. The
clock models have few restrictions, and can be used to model numerous phenomena
such as deterministic sampling, jitter, and transmission times of packet
dropping networks. Moreover, the stability theory can be applied to an
arbitrary number of clocks with different distributions, operating
asynchronously. The paper gives Lyapunov-type sufficient conditions for
stochastic stability of nonlinear networked systems. For linear systems, the
paper gives necessary and sufficient conditions for exponential mean square
stability, based on linear matrix inequalities. In both the linear and
nonlinear cases, the Lyapunov inequalities are constructed from a simple linear
combination of the classical inequalities from continuous and discrete time.
Crucially, the stability theorems only depend on the mean sampling intervals.
Thus, they can be applied with only limited statistical information about the
clocks. The Lyapunov theorems are then applied to systems with multirate
sampling, asynchronous communication, delays, and packet losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0705</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0705</id><created>2014-10-03</created><authors><author><keyname>Prisheltsev</keyname><forenames>Mikhail</forenames></author></authors><title>Adaptive two-dimensional wavelet transformation based on the Haar system</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The purpose is to study qualitative and quantitative rates of image
compression by using different Haar wavelet banks. The experimental results of
adaptive compression are provided. The paper deals with specific examples of
orthogonal Haar bases generated by multiresolution analysis. Bases consist of
three piecewise constant wavelet functions with a support $[0,1] \times [0,1]
$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0706</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0706</id><created>2014-10-02</created><authors><author><keyname>Li</keyname><forenames>Ying</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author></authors><title>BRVST: Efficient and Content-Expressive Information Matching Overlay in
  Wireless Networks</title><categories>cs.NI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient and flexible information matching over wireless networks has become
increasingly important and challenging with the popularity of smart devices and
the growth of social-network-based applications. Some existing approaches
designed for wired networks are not applicable to wireless networks, due to
their overwhelming control overheads. In this paper, we propose a reliable and
scalable binary range vector summary tree (BRVST) infrastructure for flexible
information expression support, effective content matching and timely
information dissemination over the dynamic wireless network. A novel attribute
range vector structure has been introduced for efficient and accurate content
representation and a summary tree structure to facilitate information
aggregation. For robust and scalable operations over dynamic wireless network,
the proposed overlay system exploits a virtual hierarchical geographic
management framework. Extensive simulations demonstrate that BRVST has a
significantly faster event matching speed, while incurs very low storage and
traffic overhead, as compared with peer schemes tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0707</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0707</id><created>2014-10-01</created><authors><author><keyname>Canale</keyname><forenames>Eduardo</forenames></author><author><keyname>Romero</keyname><forenames>Pablo</forenames></author><author><keyname>Rubino</keyname><forenames>Gerardo</forenames></author></authors><title>A Full Characterization of Irrelevant Components in Diameter Constrained
  Reliability</title><categories>cs.DC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In classical network reliability analysis, the system under study is a
network with perfect nodes but imperfect link, that fail stochastically and
independently. There, the goal is to find the probability that the resulting
random graph is connected, called \emph{reliability}. Although the exact
reliability computation belongs to the class of $\mathcal{NP}$-Hard problems,
the literature offers three exact methods for exact reliability computation, to
know, Sum of Disjoint Products (SDPs), Inclusion-Exclusion and Factorization.
  Inspired in delay-sensitive applications in telecommunications, H\'ector
Cancela and Louis Petingi defined in 2001 the diameter-constrained reliability,
where terminals are required to be connected by $d$ hops or less, being $d$ a
positive integer, called diameter.
  Factorization theory in classical network reliability is a mature area.
However, an extension to the diameter-constrained context requires at least the
recognition of irrelevant links, and an extension of deletion-contraction
formula. In this paper, we fully characterize the determination of irrelevant
links. Diameter-constrained reliability invariants are presented, which,
together with the recognition of irrelevant links, represent the
building-blocks for a new factorization theory. The paper is closed with a
discussion of trends for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0709</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0709</id><created>2014-10-02</created><authors><author><keyname>Kondor</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Dobos</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Csabai</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Bodor</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Vattay</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Budav&#xe1;ri</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author></authors><title>Efficient classification of billions of points into complex geographic
  regions using hierarchical triangular mesh</title><categories>cs.DB</categories><comments>appears in Proceedings of the 26th International Conference on
  Scientific and Statistical Database Management (2014)</comments><doi>10.1145/2618243.2618245</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a case study about the spatial indexing and regional
classification of billions of geographic coordinates from geo-tagged social
network data using Hierarchical Triangular Mesh (HTM) implemented for Microsoft
SQL Server. Due to the lack of certain features of the HTM library, we use it
in conjunction with the GIS functions of SQL Server to significantly increase
the efficiency of pre-filtering of spatial filter and join queries. For
example, we implemented a new algorithm to compute the HTM tessellation of
complex geographic regions and precomputed the intersections of HTM triangles
and geographic regions for faster false-positive filtering. With full control
over the index structure, HTM-based pre-filtering of simple containment
searches outperforms SQL Server spatial indices by a factor of ten and
HTM-based spatial joins run about a hundred times faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0717</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0717</id><created>2014-10-02</created><authors><author><keyname>Oseledets</keyname><forenames>I. V.</forenames></author><author><keyname>Ovchinnikov</keyname><forenames>G. V.</forenames></author></authors><title>Fast, memory efficient low-rank approximation of SimRank</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SimRank is a well-known similarity measure between graph vertices.
  In this paper novel low-rank approximation of SimRank is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0718</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0718</id><created>2014-10-02</created><updated>2014-11-13</updated><authors><author><keyname>Hill</keyname><forenames>Felix</forenames></author><author><keyname>Cho</keyname><forenames>KyungHyun</forenames></author><author><keyname>Jean</keyname><forenames>Sebastien</forenames></author><author><keyname>Devin</keyname><forenames>Coline</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Not All Neural Embeddings are Born Equal</title><categories>cs.CL</categories><comments>4 pages plus 1 page of references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural language models learn word representations that capture rich
linguistic and conceptual information. Here we investigate the embeddings
learned by neural machine translation models. We show that translation-based
embeddings outperform those learned by cutting-edge monolingual models at
single-language tasks requiring knowledge of conceptual similarity and/or
syntactic role. The findings suggest that, while monolingual models learn
information about how concepts are related, neural-translation models better
capture their true ontological status.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0719</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0719</id><created>2014-10-02</created><updated>2014-10-09</updated><authors><author><keyname>Jacques</keyname><forenames>L.</forenames></author><author><keyname>De Vleeschouwer</keyname><forenames>C.</forenames></author><author><keyname>Boursier</keyname><forenames>Y.</forenames></author><author><keyname>Sudhakar</keyname><forenames>P.</forenames></author><author><keyname>De Mol</keyname><forenames>C.</forenames></author><author><keyname>Pizurica</keyname><forenames>A.</forenames></author><author><keyname>Anthoine</keyname><forenames>S.</forenames></author><author><keyname>Vandergheynst</keyname><forenames>P.</forenames></author><author><keyname>Frossard</keyname><forenames>P.</forenames></author><author><keyname>Bilen</keyname><forenames>C.</forenames></author><author><keyname>Kitic</keyname><forenames>S.</forenames></author><author><keyname>Bertin</keyname><forenames>N.</forenames></author><author><keyname>Gribonval</keyname><forenames>R.</forenames></author><author><keyname>Boumal</keyname><forenames>N.</forenames></author><author><keyname>Mishra</keyname><forenames>B.</forenames></author><author><keyname>Absil</keyname><forenames>P. -A.</forenames></author><author><keyname>Sepulchre</keyname><forenames>R.</forenames></author><author><keyname>Bundervoet</keyname><forenames>S.</forenames></author><author><keyname>Schretter</keyname><forenames>C.</forenames></author><author><keyname>Dooms</keyname><forenames>A.</forenames></author><author><keyname>Schelkens</keyname><forenames>P.</forenames></author><author><keyname>Chabiron</keyname><forenames>O.</forenames></author><author><keyname>Malgouyres</keyname><forenames>F.</forenames></author><author><keyname>Tourneret</keyname><forenames>J. -Y.</forenames></author><author><keyname>Dobigeon</keyname><forenames>N.</forenames></author><author><keyname>Chainais</keyname><forenames>P.</forenames></author><author><keyname>Richard</keyname><forenames>C.</forenames></author><author><keyname>Cornelis</keyname><forenames>B.</forenames></author><author><keyname>Daubechies</keyname><forenames>I.</forenames></author><author><keyname>Dunson</keyname><forenames>D.</forenames></author><author><keyname>Dankova</keyname><forenames>M.</forenames></author><author><keyname>Rajmic</keyname><forenames>P.</forenames></author><author><keyname>Degraux</keyname><forenames>K.</forenames></author><author><keyname>Cambareri</keyname><forenames>V.</forenames></author><author><keyname>Geelen</keyname><forenames>B.</forenames></author><author><keyname>Lafruit</keyname><forenames>G.</forenames></author><author><keyname>Setti</keyname><forenames>G.</forenames></author><author><keyname>Determe</keyname><forenames>J. -F.</forenames></author><author><keyname>Louveaux</keyname><forenames>J.</forenames></author><author><keyname>Horlin</keyname><forenames>F.</forenames></author><author><keyname>Dr&#xe9;meau</keyname><forenames>A.</forenames></author><author><keyname>Heas</keyname><forenames>P.</forenames></author><author><keyname>Herzet</keyname><forenames>C.</forenames></author><author><keyname>Duval</keyname><forenames>V.</forenames></author><author><keyname>Peyr&#xe9;</keyname><forenames>G.</forenames></author><author><keyname>Fawzi</keyname><forenames>A.</forenames></author><author><keyname>Davies</keyname><forenames>M.</forenames></author><author><keyname>Gillis</keyname><forenames>N.</forenames></author><author><keyname>Vavasis</keyname><forenames>S. A.</forenames></author><author><keyname>Soussen</keyname><forenames>C.</forenames></author><author><keyname>Magoarou</keyname><forenames>L. Le</forenames></author><author><keyname>Liang</keyname><forenames>J.</forenames></author><author><keyname>Fadili</keyname><forenames>J.</forenames></author><author><keyname>Liutkus</keyname><forenames>A.</forenames></author><author><keyname>Martina</keyname><forenames>D.</forenames></author><author><keyname>Gigan</keyname><forenames>S.</forenames></author><author><keyname>Daudet</keyname><forenames>L.</forenames></author><author><keyname>Maggioni</keyname><forenames>M.</forenames></author><author><keyname>Minsker</keyname><forenames>S.</forenames></author><author><keyname>Strawn</keyname><forenames>N.</forenames></author><author><keyname>Mory</keyname><forenames>C.</forenames></author><author><keyname>Ngole</keyname><forenames>F.</forenames></author><author><keyname>Starck</keyname><forenames>J. -L.</forenames></author><author><keyname>Loris</keyname><forenames>I.</forenames></author><author><keyname>Vaiter</keyname><forenames>S.</forenames></author><author><keyname>Golbabaee</keyname><forenames>M.</forenames></author><author><keyname>Vukobratovic</keyname><forenames>D.</forenames></author></authors><title>Proceedings of the second &quot;international Traveling Workshop on
  Interactions between Sparse models and Technology&quot; (iTWIST'14)</title><categories>cs.NA cs.CV cs.IT cs.LG math.IT math.OC math.ST stat.TH</categories><comments>69 pages, 24 extended abstracts, iTWIST'14 website:
  http://sites.google.com/site/itwist14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implicit objective of the biennial &quot;international - Traveling Workshop on
Interactions between Sparse models and Technology&quot; (iTWIST) is to foster
collaboration between international scientific teams by disseminating ideas
through both specific oral/poster presentations and free discussions. For its
second edition, the iTWIST workshop took place in the medieval and picturesque
town of Namur in Belgium, from Wednesday August 27th till Friday August 29th,
2014. The workshop was conveniently located in &quot;The Arsenal&quot; building within
walking distance of both hotels and town center. iTWIST'14 has gathered about
70 international participants and has featured 9 invited talks, 10 oral
presentations, and 14 posters on the following themes, all related to the
theory, application and generalization of the &quot;sparsity paradigm&quot;:
Sparsity-driven data sensing and processing; Union of low dimensional
subspaces; Beyond linear and convex inverse problem; Matrix/manifold/graph
sensing/processing; Blind inverse problems and dictionary learning; Sparsity
and computational neuroscience; Information theory, geometry and randomness;
Complexity/accuracy tradeoffs in numerical methods; Sparsity? What's next?;
Sparse machine learning and inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0735</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0735</id><created>2014-10-02</created><authors><author><keyname>Ensafi</keyname><forenames>Roya</forenames></author><author><keyname>Winter</keyname><forenames>Philipp</forenames></author><author><keyname>Mueen</keyname><forenames>Abdullah</forenames></author><author><keyname>Crandall</keyname><forenames>Jedidiah R.</forenames></author></authors><title>Large-scale Spatiotemporal Characterization of Inconsistencies in the
  World's Largest Firewall</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nation-scale firewall, colloquially referred to as the &quot;Great Firewall of
China,&quot; implements many different types of censorship and content filtering to
control China's Internet traffic. Past work has shown that the firewall
occasionally fails. In other words, sometimes clients in China are able to
reach blacklisted servers outside of China. This phenomenon has not yet been
characterized because it is infeasible to find a large and geographically
diverse set of clients in China from which to test connectivity.
  In this paper, we overcome this challenge by using hybrid idle scan
techniques that are able to measure connectivity between a remote client and an
arbitrary server, neither of which are under the control of the researcher
performing measurements. In addition to hybrid idle scans, we present and
employ a novel side channel in the Linux kernel's SYN backlog. We demonstrate
both techniques by measuring the reachability of the Tor network which is known
to be blocked in China. Our measurements reveal that 1) failures in the
firewall occur throughout the entire country without any conspicuous
geographical patterns, 2) a network block in China appears to have unfiltered
access to parts of the Tor network, and 3) the filtering seems to be mostly
centralized at the level of Internet exchange points. Our work also answers
many other open questions about the Great Firewall's architecture and
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0736</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0736</id><created>2014-10-02</created><updated>2015-05-15</updated><authors><author><keyname>Yan</keyname><forenames>Zhicheng</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Piramuthu</keyname><forenames>Robinson</forenames></author><author><keyname>Jagadeesh</keyname><forenames>Vignesh</forenames></author><author><keyname>DeCoste</keyname><forenames>Dennis</forenames></author><author><keyname>Di</keyname><forenames>Wei</forenames></author><author><keyname>Yu</keyname><forenames>Yizhou</forenames></author></authors><title>HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale
  Visual Recognition</title><categories>cs.CV cs.LG cs.NE</categories><comments>Add new results on ImageNet using VGG-16-layer building block net</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In image classification, visual separability between different object
categories is highly uneven, and some categories are more difficult to
distinguish than others. Such difficult categories demand more dedicated
classifiers. However, existing deep convolutional neural networks (CNN) are
trained as flat N-way classifiers, and few efforts have been made to leverage
the hierarchical structure of categories. In this paper, we introduce
hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category
hierarchy. An HD-CNN separates easy classes using a coarse category classifier
while distinguishing difficult classes using fine category classifiers. During
HD-CNN training, component-wise pretraining is followed by global finetuning
with a multinomial logistic loss regularized by a coarse category consistency
term. In addition, conditional executions of fine category classifiers and
layer parameter compression make HD-CNNs scalable for large-scale visual
recognition. We achieve state-of-the-art results on both CIFAR100 and
large-scale ImageNet 1000-class benchmark datasets. In our experiments, we
build up three different HD-CNNs and they lower the top-1 error of the standard
CNNs by 2.65%, 3.1% and 1.1%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0741</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0741</id><created>2014-10-02</created><authors><author><keyname>Israelsen</keyname><forenames>Brett W.</forenames></author><author><keyname>Smith</keyname><forenames>Dale A.</forenames></author></authors><title>Generalized Laguerre Reduction of the Volterra Kernel for Practical
  Identification of Nonlinear Dynamic Systems</title><categories>cs.LG</categories><comments>16 pages</comments><journal-ref>AIChE Spring Meeting 2014, Paper 349438</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Volterra series can be used to model a large subset of nonlinear, dynamic
systems. A major drawback is the number of coefficients required model such
systems. In order to reduce the number of required coefficients, Laguerre
polynomials are used to estimate the Volterra kernels. Existing literature
proposes algorithms for a fixed number of Volterra kernels, and Laguerre
series. This paper presents a novel algorithm for generalized calculation of
the finite order Volterra-Laguerre (VL) series for a MIMO system. An example
addresses the utility of the algorithm in practical application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0745</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0745</id><created>2014-10-02</created><updated>2014-11-19</updated><authors><author><keyname>Wang</keyname><forenames>Qiaosong</forenames></author><author><keyname>Jagadeesh</keyname><forenames>Vignesh</forenames></author><author><keyname>Ressler</keyname><forenames>Bryan</forenames></author><author><keyname>Piramuthu</keyname><forenames>Robinson</forenames></author></authors><title>Im2Fit: Fast 3D Model Fitting and Anthropometrics using Single Consumer
  Depth Camera and Synthetic Data</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in consumer depth sensors have created many opportunities for
human body measurement and modeling. Estimation of 3D body shape is
particularly useful for fashion e-commerce applications such as virtual try-on
or fit personalization. In this paper, we propose a method for capturing
accurate human body shape and anthropometrics from a single consumer grade
depth sensor. We first generate a large dataset of synthetic 3D human body
models using real-world body size distributions. Next, we estimate key body
measurements from a single monocular depth image. We combine body measurement
estimates with local geometry features around key joint positions to form a
robust multi-dimensional feature vector. This allows us to conduct a fast
nearest-neighbor search to every sample in the dataset and return the closest
one. Compared to existing methods, our approach is able to predict accurate
full body parameters from a partial view using measurement parameters learned
from the synthetic dataset. Furthermore, our system is capable of generating 3D
human mesh models in real-time, which is significantly faster than methods
which attempt to model shape and pose deformations. To validate the efficiency
and applicability of our system, we collected a dataset that contains frontal
and back scans of 83 clothed people with ground truth height and weight.
Experiments on real-world dataset show that the proposed method can achieve
real-time performance with competing results achieving an average error of 1.9
cm in estimated measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0759</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0759</id><created>2014-10-03</created><updated>2014-12-17</updated><authors><author><keyname>Chetlur</keyname><forenames>Sharan</forenames></author><author><keyname>Woolley</keyname><forenames>Cliff</forenames></author><author><keyname>Vandermersch</keyname><forenames>Philippe</forenames></author><author><keyname>Cohen</keyname><forenames>Jonathan</forenames></author><author><keyname>Tran</keyname><forenames>John</forenames></author><author><keyname>Catanzaro</keyname><forenames>Bryan</forenames></author><author><keyname>Shelhamer</keyname><forenames>Evan</forenames></author></authors><title>cuDNN: Efficient Primitives for Deep Learning</title><categories>cs.NE cs.LG cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a library of efficient implementations of deep learning
primitives. Deep learning workloads are computationally intensive, and
optimizing their kernels is difficult and time-consuming. As parallel
architectures evolve, kernels must be reoptimized, which makes maintaining
codebases difficult over time. Similar issues have long been addressed in the
HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS).
However, there is no analogous library for deep learning. Without such a
library, researchers implementing deep learning workloads on parallel
processors must create and optimize their own implementations of the main
computational kernels, and this work must be repeated as new parallel
processors emerge. To address this problem, we have created a library similar
in intent to BLAS, with optimized routines for deep learning workloads. Our
implementation contains routines for GPUs, although similarly to the BLAS
library, these routines could be implemented for other platforms. The library
is easy to integrate into existing frameworks, and provides optimized
performance and memory usage. For example, integrating cuDNN into Caffe, a
popular framework for convolutional networks, improves performance by 36% on a
standard model while also reducing memory consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0760</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0760</id><created>2014-10-03</created><updated>2015-03-28</updated><authors><author><keyname>Wang</keyname><forenames>Chih-Hang</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Chen</keyname><forenames>Wen-Tsuen</forenames></author></authors><title>Scheduling for Multi-Camera Surveillance in LTE Networks</title><categories>cs.NI</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless surveillance in cellular networks has become increasingly important,
while commercial LTE surveillance cameras are also available nowadays.
Nevertheless, most scheduling algorithms in the literature are throughput,
fairness, or profit-based approaches, which are not suitable for wireless
surveillance. In this paper, therefore, we explore the resource allocation
problem for a multi-camera surveillance system in 3GPP Long Term Evolution
(LTE) uplink (UL) networks. We minimize the number of allocated resource blocks
(RBs) while guaranteeing the coverage requirement for surveillance systems in
LTE UL networks. Specifically, we formulate the Camera Set Resource Allocation
Problem (CSRAP) and prove that the problem is NP-Hard. We then propose an
Integer Linear Programming formulation for general cases to find the optimal
solution. Moreover, we present a baseline algorithm and devise an approximation
algorithm to solve the problem. Simulation results based on a real surveillance
map and synthetic datasets manifest that the number of allocated RBs can be
effectively reduced compared to the existing approach for LTE networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0768</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0768</id><created>2014-10-03</created><authors><author><keyname>Elkin</keyname><forenames>Michael</forenames></author><author><keyname>Neiman</keyname><forenames>Ofer</forenames></author><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Space-Efficient Path-Reporting Approximate Distance Oracles</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider approximate {\em path-reporting} distance oracles, distance
labeling and labeled routing with extremely low space requirement, for general
undirected graphs. For distance oracles, we show how to break the n\log n space
bound of Thorup and Zwick if approximate {\em paths} rather than distances need
to be reported. For approximate distance labeling and labeled routing, we break
the previously best known space bound of O(log n) words per vertex. The cost
for such space efficiency is an increased stretch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0770</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0770</id><created>2014-10-03</created><updated>2014-11-25</updated><authors><author><keyname>Brandst&#xe4;dt</keyname><forenames>Andreas</forenames></author><author><keyname>Fi&#x10d;ur</keyname><forenames>Pavel</forenames></author><author><keyname>Leitert</keyname><forenames>Arne</forenames></author><author><keyname>Milani&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Polynomial-time Algorithms for Weighted Efficient Domination Problems in
  AT-free Graphs and Dually Chordal Graphs</title><categories>cs.DM</categories><journal-ref>Information Processing Letters 115 (2015) 256-262</journal-ref><doi>10.1016/j.ipl.2014.09.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient dominating set (or perfect code) in a graph is a set of vertices
the closed neighborhoods of which partition the vertex set of the graph. The
minimum weight efficient domination problem is the problem of finding an
efficient dominating set of minimum weight in a given vertex-weighted graph;
the maximum weight efficient domination problem is defined similarly. We
develop a framework for solving the weighted efficient domination problems
based on a reduction to the maximum weight independent set problem in the
square of the input graph. Using this approach, we improve on several previous
results from the literature by deriving polynomial-time algorithms for the
weighted efficient domination problems in the classes of dually chordal and
AT-free graphs. In particular, this answers a question by Lu and Tang regarding
the complexity of the minimum weight efficient domination problem in strongly
chordal graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0773</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0773</id><created>2014-10-03</created><authors><author><keyname>Buchbinder</keyname><forenames>Niv</forenames></author><author><keyname>Feldman</keyname><forenames>Moran</forenames></author><author><keyname>Schwartz</keyname><forenames>Roy</forenames></author></authors><title>Comparing Apples and Oranges: Query Tradeoff in Submodular Maximization</title><categories>cs.DS</categories><comments>29 pages, accepted to SODA 2015</comments><msc-class>68R05, 68W25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast algorithms for submodular maximization problems have a vast potential
use in applicative settings, such as machine learning, social networks, and
economics. Though fast algorithms were known for some special cases, only
recently Badanidiyuru and Vondr\'{a}k (2014) were the first to explicitly look
for such algorithms in the general case of maximizing a monotone submodular
function subject to a matroid independence constraint. The algorithm of
Badanidiyuru and Vondr\'{a}k matches the best possible approximation guarantee,
while trying to reduce the number of value oracle queries the algorithm
performs.
  Our main result is a new algorithm for this general case which establishes a
surprising tradeoff between two seemingly unrelated quantities: the number of
value oracle queries and the number of matroid independence queries performed
by the algorithm. Specifically, one can decrease the former by increasing the
latter and vice versa, while maintaining the best possible approximation
guarantee. Such a tradeoff is very useful since various applications might
incur significantly different costs in querying the value and matroid
independence oracles. Furthermore, in case the rank of the matroid is $O(n^c)$,
where $n$ is the size of the ground set and $c$ is an absolute constant smaller
than $1$, the total number of oracle queries our algorithm uses can be made to
have a smaller magnitude compared to that needed by Badanidiyuru and
Vondr\'{a}k. We also provide even faster algorithms for the well studied
special cases of a cardinality constraint and a partition matroid independence
constraint, both of which capture many real-world applications and have been
widely studied both theorically and in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0781</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0781</id><created>2014-10-03</created><updated>2014-12-07</updated><authors><author><keyname>Cohen</keyname><forenames>Nadav</forenames></author><author><keyname>Shashua</keyname><forenames>Amnon</forenames></author></authors><title>SimNets: A Generalization of Convolutional Networks</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deep layered architecture that generalizes classical
convolutional neural networks (ConvNets). The architecture, called SimNets, is
driven by two operators, one being a similarity function whose family contains
the convolution operator used in ConvNets, and the other is a new soft
max-min-mean operator called MEX that realizes classical operators like ReLU
and max pooling, but has additional capabilities that make SimNets a powerful
generalization of ConvNets. Three interesting properties emerge from the
architecture: (i) the basic input to hidden layer to output machinery contains
as special cases kernel machines with the Exponential and Generalized Gaussian
kernels, the output units being &quot;neurons in feature space&quot; (ii) in its general
form, the basic machinery has a higher abstraction level than kernel machines,
and (iii) initializing networks using unsupervised learning is natural.
Experiments demonstrate the capability of achieving state of the art accuracy
with networks that are an order of magnitude smaller than comparable ConvNets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0782</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0782</id><created>2014-10-03</created><authors><author><keyname>Bonald</keyname><forenames>Thomas</forenames></author><author><keyname>Roberts</keyname><forenames>James</forenames></author></authors><title>Multi-resource fairness: Objectives, algorithms and performance</title><categories>cs.NI cs.GT</categories><acm-class>B.8.2; C.2.1; C.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing efficient and fair algorithms for sharing multiple resources
between heterogeneous demands is becoming increasingly important. Applications
include compute clusters shared by multi-task jobs and routers equipped with
middleboxes shared by flows of different types. We show that the currently
preferred objective of Dominant Resource Fairness has a significantly less
favorable efficiency-fairness tradeoff than alternatives like Proportional
Fairness and our proposal, Bottleneck Max Fairness. In addition to other
desirable properties, these objectives are equally strategyproof in any
realistic scenario with dynamic demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0804</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0804</id><created>2014-10-03</created><updated>2014-10-11</updated><authors><author><keyname>Burak</keyname><forenames>Maciej</forenames></author></authors><title>Multi-step Uniformization with Steady-State Detection in Nonstationary
  M/M/s Queuing Systems</title><categories>cs.PF cs.NA</categories><comments>added DOI information in References</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach to the steady state detection in the uniformization method of
solving continuous time Markov chains is introduced. The method is particularly
useful in solving inhomogenous CTMC's in multiple steps, where the desired
error bound of the whole solution can be distributed not proportionally to the
lengths of the respective intervals, but rather in a way, that maximizes the
chances of detecting a steady state. Additionally, the convergence properties
of the underlying DTMC are used to further enhance the computational savings
due to the steady state detection. The method is applied to the problem of
modeling a Call Center using inhomogenous CTMC model of a M(t)/M(t)/s(t)
queuing system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0818</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0818</id><created>2014-10-03</created><authors><author><keyname>Li</keyname><forenames>Junhua</forenames></author><author><keyname>Struzik</keyname><forenames>Zbigniew</forenames></author><author><keyname>Zhang</keyname><forenames>Liqing</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Feature Learning from Incomplete EEG with Denoising Autoencoder</title><categories>cs.CV q-bio.NC</categories><comments>The paper was accepted for publication by Neurocomputing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An alternative pathway for the human brain to communicate with the outside
world is by means of a brain computer interface (BCI). A BCI can decode
electroencephalogram (EEG) signals of brain activities, and then send a command
or an intent to an external interactive device, such as a wheelchair. The
effectiveness of the BCI depends on the performance in decoding the EEG.
Usually, the EEG is contaminated by different kinds of artefacts (e.g.,
electromyogram (EMG), background activity), which leads to a low decoding
performance. A number of filtering methods can be utilized to remove or weaken
the effects of artefacts, but they generally fail when the EEG contains extreme
artefacts. In such cases, the most common approach is to discard the whole data
segment containing extreme artefacts. This causes the fatal drawback that the
BCI cannot output decoding results during that time. In order to solve this
problem, we employ the Lomb-Scargle periodogram to estimate the spectral power
from incomplete EEG (after removing only parts contaminated by artefacts), and
Denoising Autoencoder (DAE) for learning. The proposed method is evaluated with
motor imagery EEG data. The results show that our method can successfully
decode incomplete EEG to good effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0826</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0826</id><created>2014-10-03</created><authors><author><keyname>Rahimzadeh</keyname><forenames>Parisa</forenames></author><author><keyname>Ashtiani</keyname><forenames>Farid</forenames></author></authors><title>Analytical Evaluation of Saturation Throughput of a Cognitive
  802.11-based WLAN Overlaid on a WiMAX-TDD Network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the saturation throughput of a cognitive single hop WLAN
overlaid on a primary IEEE 802.16e TDD WiMAX network. After the contention
among the secondary nodes, the winner node transmits its data packet in the
empty slots of downlink subframes of WiMAX. Regarding the OFDMA structure as
well as time-scheduled resources in WiMAX, the time duration of opportunities
for the secondary network does not follow simple exponential on-off pattern. To
model the dynamic behavior of opportunities for secondary nodes as well as
contentions to exploit the opportunities, we propose an analytical model
comprised of a discrete-time Markov chain and two inter-related open
multi-class queueing networks. The effects of random number of empty slots at
different frames as the result of random amount of download data, random packet
transmission time at WLAN due to random opportunities in different frames, the
dependency of the number of empty slots at consecutive WiMAX frames, and the
details of 802.11 MAC protocol are included in our analytical approach. We
compare the effect of two resource allocations, i.e., horizontal and vertical
striping on the saturation throughput of the cognitive WLAN. Simulation results
confirm the accuracy of our analytical approach in different conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0828</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0828</id><created>2014-10-03</created><authors><author><keyname>Koziol</keyname><forenames>Zbigniew</forenames></author></authors><title>Dynamics of Users Activity on Web-Blogs</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Activity of users on Internet discussion forums is analyzed. The rank of
users is shown to be approximated better by stretched-exponential function than
by Zipfs law. Cumulative distribution function is found as an excellent tool in
analysis of the dynamics of the collective social phenomena. We are able to
approximate the number of blog comments with time by simple functions that
resemble Fermi-Dirac distribution function: probability of posting a comment is
given by $P(t)=P_0(t) / (1+P_0(t)$, where $P_0(t)=exp(a \cdot log(t/t_0))$. It
is argued that dynamics of blog entries by a specific user ought to be related
to personality of each user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0833</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0833</id><created>2014-10-03</created><updated>2014-10-07</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Loitzenbauer</keyname><forenames>Veronika</forenames></author></authors><title>Improved Algorithms for One-Pair and k-Pair Streett Objectives</title><categories>cs.DS cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computation of the winning set for one-pair Streett objectives and for
k-pair Streett objectives in (standard) graphs as well as in game graphs are
central problems in computer-aided verification, with application to the
verification of open systems, checking interface compatibility, well-formedness
of specifications, the synthesis of systems from specifications, and the
synthesis of reactive systems. We give faster algorithms for the computation of
the winning set for (1) one-pair Streett objectives (aka parity-3 problem) in
game graphs and (2) for k-pair Streett objectives in graphs. For both problems
this represents the first improvement in asymptotic running time in 15 years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0846</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0846</id><created>2014-10-02</created><authors><author><keyname>Boettiger</keyname><forenames>Carl</forenames></author></authors><title>An introduction to Docker for reproducible research, with examples from
  the R environment</title><categories>cs.SE</categories><journal-ref>(2015) ACM SIGOPS Operating Systems Review, Special Issue on
  Repeatability and Sharing of Experimental Artifacts. 49(1), 71-79</journal-ref><doi>10.1145/2723872.2723882</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As computational work becomes more and more integral to many aspects of
scientific research, computational reproducibility has become an issue of
increasing importance to computer systems researchers and domain scientists
alike. Though computational reproducibility seems more straight forward than
replicating physical experiments, the complex and rapidly changing nature of
computer environments makes being able to reproduce and extend such work a
serious challenge. In this paper, I explore common reasons that code developed
for one research project cannot be successfully executed or extended by
subsequent researchers. I review current approaches to these issues, including
virtual machines and workflow systems, and their limitations. I then examine
how the popular emerging technology Docker combines several areas from systems
research - such as operating system virtualization, cross-platform portability,
modular re-usable elements, versioning, and a `DevOps' philosophy, to address
these challenges. I illustrate this with several examples of Docker use with a
focus on the R statistical environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0855</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0855</id><created>2014-10-03</created><authors><author><keyname>Jansen</keyname><forenames>Bart M. P.</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Characterizing the easy-to-find subgraphs from the viewpoint of
  polynomial-time algorithms, kernels, and Turing kernels</title><categories>cs.DS cs.CC</categories><comments>69 pages, extended abstract to appear in the proceedings of SODA 2015</comments><msc-class>68Q17, 68R10</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two fundamental problems related to finding subgraphs: (1) given
graphs G and H, Subgraph Test asks if H is isomorphic to a subgraph of G, (2)
given graphs G, H, and an integer t, Packing asks if G contains t
vertex-disjoint subgraphs isomorphic to H. For every graph class F, let
F-Subgraph Test and F-Packing be the special cases of the two problems where H
is restricted to be in F. Our goal is to study which classes F make the two
problems tractable in one of the following senses:
  * (randomized) polynomial-time solvable,
  * admits a polynomial (many-one) kernel, or
  * admits a polynomial Turing kernel (that is, has an adaptive polynomial-time
procedure that reduces the problem to a polynomial number of instances, each of
which has size bounded polynomially by the size of the solution).
  We identify a simple combinatorial property such that if a hereditary class F
has this property, then F-Packing admits a polynomial kernel, and has no
polynomial (many-one) kernel otherwise, unless the polynomial hierarchy
collapses. Furthermore, if F does not have this property, then F-Packing is
either WK[1]-hard, W[1]-hard, or Long Path-hard, giving evidence that it does
not admit polynomial Turing kernels either.
  For F-Subgraph Test, we show that if every graph of a hereditary class F
satisfies the property that it is possible to delete a bounded number of
vertices such that every remaining component has size at most two, then
F-Subgraph Test is solvable in randomized polynomial time and it is NP-hard
otherwise. We introduce a combinatorial property called (a,b,c,d)-splittability
and show that if every graph in a hereditary class F has this property, then
F-Subgraph Test admits a polynomial Turing kernel and it is WK[1]-hard,
W[1]-hard, or Long Path-hard, otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0868</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0868</id><created>2014-10-03</created><authors><author><keyname>Zhou</keyname><forenames>Shuchang</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author><author><keyname>Feng</keyname><forenames>Xiaobing</forenames></author></authors><title>Group Orbit Optimization: A Unified Approach to Data Normalization</title><categories>cs.NA cs.CV math.NA</categories><msc-class>15-02</msc-class><acm-class>G.1.3; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose and study an optimization problem over a matrix
group orbit that we call \emph{Group Orbit Optimization} (GOO). We prove that
GOO can be used to induce matrix decomposition techniques such as singular
value decomposition (SVD), LU decomposition, QR decomposition, Schur
decomposition and Cholesky decomposition, etc. This gives rise to a unified
framework for matrix decomposition and allows us to bridge these matrix
decomposition methods. Moreover, we generalize GOO for tensor decomposition. As
a concrete application of GOO, we devise a new data decomposition method over a
special linear group to normalize point cloud data. Experiment results show
that our normalization method is able to obtain recovery well from distortions
like shearing, rotation and squeezing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0871</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0871</id><created>2014-10-03</created><updated>2015-12-03</updated><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Esperet</keyname><forenames>Louis</forenames></author><author><keyname>Lemoine</keyname><forenames>Laetitia</forenames></author><author><keyname>Maceli</keyname><forenames>Peter</forenames></author><author><keyname>Maffray</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Penev</keyname><forenames>Irena</forenames></author></authors><title>Graphs with no induced five-vertex path or antipath</title><categories>math.CO cs.DM</categories><comments>13 pages, the paper results from the merging of the two (unpublished)
  manuscripts 'Excluding four-edge paths and their complements', by M.
  Chudnovsky, P. Maceli and I. Penev arXiv:1302.0405 , and 'On $(P_5,
  \overline{P_5})$-free graphs', by L. Esperet, L. Lemoine, and F. Maffray
  (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a graph $G$ contains no induced $5$-vertex path and no induced
complement of a $5$-vertex path if and only if $G$ is obtained from $5$-cycles
and split graphs by repeatedly applying the following operations: substitution,
split unification, and split unification in the complement, where split
unification is a new class-preserving operation introduced here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0879</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0879</id><created>2014-10-03</created><authors><author><keyname>Gregoire</keyname><forenames>Jean</forenames></author></authors><title>Priority-based coordination of mobile robots</title><categories>cs.RO cs.SY</categories><comments>PhD Thesis, 182 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the end of the 1980's, the development of self-driven autonomous
vehicles is an intensive research area in most major industrial countries.
Positive socio-economic potential impacts include a decrease of crashes, a
reduction of travel times, energy efficiency improvements, and a reduced need
of costly physical infrastructure. Some form of vehicle-to-vehicle and/or
vehicle-to-infrastructure cooperation is required to ensure a safe and
efficient global transportation system. This thesis deals with a particular
form of cooperation by studying the problem of coordinating multiple mobile
robots at an intersection area. Most of coordination systems proposed in
previous work consist in planning a trajectory and to control the robots along
the planned trajectory: that is the plan-as-program paradigm where planning is
considered as a generative mechanism of action. The approach of the thesis is
to plan priorities -- the relative order of robots to go through the
intersection -- which is much weaker as many trajectories respect the same
priorities. More precisely, priorities encode the homotopy classes of solutions
to the coordination problem. Priority assignment is equivalent to the choice of
some homotopy class to solve the coordination problem instead of a particular
trajectory. Once priorities are assigned, robots are controlled through a
control law preserving the assigned priorities, i.e., ensuring the described
trajectory belongs to the chosen homotopy class. It results in a more robust
coordination system -- able to handle a large class of unexpected events in a
reactive manner -- particularly well adapted for an application to the
coordination of autonomous vehicles at intersections where cars, public
transport and pedestrians share the road.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0885</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0885</id><created>2014-10-03</created><updated>2014-10-17</updated><authors><author><keyname>Traversaro</keyname><forenames>Silvio</forenames></author><author><keyname>Pucci</keyname><forenames>Daniele</forenames></author><author><keyname>Nori</keyname><forenames>Francesco</forenames></author></authors><title>In Situ Calibration of Six-Axes Force Torque Sensors using Accelerometer
  Measurements</title><categories>cs.RO</categories><comments>8 pages, submitted to ICRA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes techniques to calibrate six-axis force-torque sensors
that can be performed in situ, i.e., without removing the sensor from the
hosting system. We assume that the force-torque sensor is attached to a rigid
body equipped with an accelerometer. Then, the proposed calibration technique
uses the measurements of the accelerometer, but requires neither the knowledge
of the inertial parameters nor the orientation of the rigid body. The proposed
method exploits the geometry induced by the model between the raw measurements
of the sensor and the corresponding force-torque. The validation of the
approach is performed by calibrating two six-axis force-torque sensors of the
iCub humanoid robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0893</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0893</id><created>2014-10-03</created><updated>2015-06-30</updated><authors><author><keyname>Miculan</keyname><forenames>Marino</forenames></author><author><keyname>Peressotti</keyname><forenames>Marco</forenames></author></authors><title>Structural operational semantics for non-deterministic processes with
  quantitative aspects</title><categories>cs.LO</categories><comments>Extended version of arXiv:1406.2066</comments><acm-class>F.1.2; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  General frameworks have been recently proposed as unifying theories for
processes combining non-determinism with quantitative aspects (such as
probabilistic or stochastically timed executions), aiming to provide general
results and tools. This paper provides two contributions in this respect.
First, we present a general GSOS specification format and a corresponding
notion of bisimulation for non-deterministic processes with quantitative
aspects. These specifications define labelled transition systems according to
the ULTraS model, an extension of the usual LTSs where the transition relation
associates any source state and transition label with state reachability weight
functions (like, e.g., probability distributions). This format, hence called
Weight Function GSOS (WF-GSOS), covers many known systems and their
bisimulations (e.g. PEPA, TIPP, PCSP) and GSOS formats (e.g. GSOS, Weighted
GSOS, Segala-GSOS, among others).
  The second contribution is a characterization of these systems as coalgebras
of a class of functors, parametric on the weight structure. This result allows
us to prove soundness and completeness of the WF-GSOS specification format, and
that bisimilarities induced by these specifications are always congruences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0908</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0908</id><created>2014-10-03</created><authors><author><keyname>Yu</keyname><forenames>Xingchen</forenames></author><author><keyname>Fokoue</keyname><forenames>Ernest</forenames></author></authors><title>Probit Normal Correlated Topic Models</title><categories>stat.ML cs.IR cs.LG</categories><comments>11 pages, 2 figures and 2 tables</comments><msc-class>62H25, 62H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logistic normal distribution has recently been adapted via the
transformation of multivariate Gaus- sian variables to model the topical
distribution of documents in the presence of correlations among topics. In this
paper, we propose a probit normal alternative approach to modelling correlated
topical structures. Our use of the probit model in the context of topic
discovery is novel, as many authors have so far con- centrated solely of the
logistic model partly due to the formidable inefficiency of the multinomial
probit model even in the case of very small topical spaces. We herein
circumvent the inefficiency of multinomial probit estimation by using an
adaptation of the diagonal orthant multinomial probit in the topic models
context, resulting in the ability of our topic modelling scheme to handle
corpuses with a large number of latent topics. An additional and very important
benefit of our method lies in the fact that unlike with the logistic normal
model whose non-conjugacy leads to the need for sophisticated sampling schemes,
our ap- proach exploits the natural conjugacy inherent in the auxiliary
formulation of the probit model to achieve greater simplicity. The application
of our proposed scheme to a well known Associated Press corpus not only helps
discover a large number of meaningful topics but also reveals the capturing of
compellingly intuitive correlations among certain topics. Besides, our proposed
approach lends itself to even further scalability thanks to various existing
high performance algorithms and architectures capable of handling millions of
documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0917</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0917</id><created>2014-10-03</created><updated>2015-03-30</updated><authors><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author><author><keyname>Chen</keyname><forenames>Jiayi</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Communication Using a Large-Scale Array of Ubiquitous Antennas: A
  Geometry Approach</title><categories>cs.IT math.IT</categories><comments>single column, double spacing, 30 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent trends of densification and centralized signal processing in radio
access networks suggest that future networks may comprise ubiquitous antennas
coordinated to form a network-wide gigantic array, referred to as the
ubiquitous array (UA). In this paper, the UA communication techniques are
designed and analyzed based on a geometric model. Specifically, the UA is
modeled as a continuous circular/spherical array enclosing target users and
free-space propagation is assumed. First, consider the estimation of multiuser
UA channels induced by user locations. Given single pilot symbols, a novel
channel estimation scheme is proposed that decomposes training signals into
Fourier/Laplace series and thereby translates multiuser channel estimation into
peak detection of a derive function of location. The process is shown to
suppress noise. Moreover, it is proved that estimation error due to
interference diminishes with the increasing minimum user-separation distance
following the power law, where the exponent is 1/3 and 1 for the circular and
spherical UA, respectively. If orthogonal pilot sequences are used, channel
estimation is found to be perfect. Next, consider channel-conjugate data
transmission that maximizes received signal power. The power of interference
between two users is shown to decay with the increasing user-separation
distance sub-linearly and super-linearly for the circular and spherical UA,
respectively. Furthermore, a novel multiuser precoding design is proposed by
exciting different phase modes of the UA and controlling the mode weight
factors to null interference. The number of available degrees of freedom for
interference nulling using the UA is proved to be proportional to the minimum
user-separation distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0925</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0925</id><created>2014-10-03</created><updated>2014-10-23</updated><authors><author><keyname>Prisacariu</keyname><forenames>Victor Adrian</forenames></author><author><keyname>K&#xe4;hler</keyname><forenames>Olaf</forenames></author><author><keyname>Cheng</keyname><forenames>Ming Ming</forenames></author><author><keyname>Ren</keyname><forenames>Carl Yuheng</forenames></author><author><keyname>Valentin</keyname><forenames>Julien</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author><author><keyname>Reid</keyname><forenames>Ian D.</forenames></author><author><keyname>Murray</keyname><forenames>David W.</forenames></author></authors><title>A Framework for the Volumetric Integration of Depth Images</title><categories>cs.CV</categories><comments>17 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Volumetric models have become a popular representation for 3D scenes in
recent years. One of the breakthroughs leading to their popularity was
KinectFusion, where the focus is on 3D reconstruction using RGB-D sensors.
However, monocular SLAM has since also been tackled with very similar
approaches. Representing the reconstruction volumetrically as a truncated
signed distance function leads to most of the simplicity and efficiency that
can be achieved with GPU implementations of these systems. However, this
representation is also memory-intensive and limits the applicability to small
scale reconstructions. Several avenues have been explored for overcoming this
limitation. With the aim of summarizing them and providing for a fast and
flexible 3D reconstruction pipeline, we propose a new, unifying framework
called InfiniTAM. The core idea is that individual steps like camera tracking,
scene representation and integration of new data can easily be replaced and
adapted to the needs of the user. Along with the framework we also provide a
set of components for scalable reconstruction: two implementations of camera
trackers, based on RGB data and on depth data, two representations of the 3D
volumetric data, a dense volume and one based on hashes of subblocks, and an
optional module for swapping subblocks in and out of the typically limited GPU
memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0932</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0932</id><created>2014-10-03</created><updated>2014-11-26</updated><authors><author><keyname>Lin</keyname><forenames>Cedric Yen-Yu</forenames></author><author><keyname>Lin</keyname><forenames>Han-Hsuan</forenames></author></authors><title>Upper bounds on quantum query complexity inspired by the Elitzur-Vaidman
  bomb tester</title><categories>quant-ph cs.CC</categories><comments>32 pages. Minor revisions and corrections. Regev and Schiff's proof
  that P(OR) = \Omega(N) removed</comments><report-no>MIT-CTP/4592</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the Elitzur-Vaidman bomb testing problem [arXiv:hep-th/9305002],
we introduce a new query complexity model, which we call bomb query complexity
$B(f)$. We investigate its relationship with the usual quantum query complexity
$Q(f)$, and show that $B(f)=\Theta(Q(f)^2)$.
  This result gives a new method to upper bound the quantum query complexity:
we give a method of finding bomb query algorithms from classical algorithms,
which then provide nonconstructive upper bounds on $Q(f)=\Theta(\sqrt{B(f)})$.
We subsequently were able to give explicit quantum algorithms matching our
upper bound method. We apply this method on the single-source shortest paths
problem on unweighted graphs, obtaining an algorithm with $O(n^{1.5})$ quantum
query complexity, improving the best known algorithm of $O(n^{1.5}\sqrt{\log
n})$ [arXiv:quant-ph/0606127]. Applying this method to the maximum bipartite
matching problem gives an $O(n^{1.75})$ algorithm, improving the best known
trivial $O(n^2)$ upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0940</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0940</id><created>2014-10-03</created><updated>2015-02-04</updated><authors><author><keyname>Khan</keyname><forenames>Faisal Shah</forenames></author></authors><title>Dominant Strategies in Two Qubit Quantum Computations</title><categories>quant-ph cs.GT math-ph math.MP math.OC</categories><comments>The abstract has been re-written and technical details added to
  section 5 in version 1</comments><doi>10.1007/s11128-015-0945-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nash equilibrium is a solution concept in non-strictly competitive,
non-cooperative game theory that finds applications in various scientific and
engineering disciplines. A non-strictly competitive, non-cooperative game model
is presented here for two qubit quantum computations that allows for the
characterization of Nash equilibrium in these computations via the inner
product of their state space. Nash equilibrium outcomes are optimal under given
constraints and therefore offer a game-theoretic measure of constrained
optimization of two qubit quantum computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0948</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0948</id><created>2014-10-03</created><authors><author><keyname>Rodrigues</keyname><forenames>Eug&#xe9;nio</forenames></author><author><keyname>Gaspar</keyname><forenames>Ad&#xe9;lio R.</forenames></author><author><keyname>Gomes</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>da Silva</keyname><forenames>Manuel Gameiro</forenames></author></authors><title>Contributions of natural ventilation on thermal performance of
  alternative floor plan designs</title><categories>cs.NE cs.SE</categories><comments>8 pages, 2 figures, Proceeding of RoomVent 2014</comments><acm-class>D.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the earliest phase of architectural design process, practitioners
after analyzing the client's design program, legal requirements, topographic
constraints, and preferences synthesize these requirements into architectural
floor plan drawings. Design decisions taken in this phase may significantly
contribute to the building performance. On account of this reason, it is
important to estimate and compare alternative solutions, when it is still
manageable to change the building design.
  The authors have been developing a prototype tool to assist architects during
this initial design phase. It is made up of two algorithms. The first algorithm
generates alternative floor plans according to the architect's preferences and
requirements, and the client's design program. It consists in one evolutionary
strategy approach enhanced with local search technique to allocate rooms on
several levels in the two-dimensional space. The second algorithm evaluates,
ranks, and optimizes those floor plans according to thermal performance
criteria. The prototype tool is coupled with dynamic simulation program, which
estimates the thermal behavior of each solution. A sequential variable
optimization is used to change several geometric values of different
architectural elements in the floor plans to explore the improvement potential.
  In the present communication, the two algorithms are used in an iterative
process to generate and optimize the thermal performance of alternative floor
plans. In the building simulation specifications of EnergyPlus program, the
airflow network model has been used in order to adequately model the air
infiltration and the airflows through indoor spaces. A case study of a
single-family house with three rooms in a single level is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0949</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0949</id><created>2014-10-03</created><updated>2015-01-27</updated><authors><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>Ashkan</keyname><forenames>Azin</forenames></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author></authors><title>Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits</title><categories>cs.LG cs.AI stat.ML</categories><comments>Proceedings of the 18th International Conference on Artificial
  Intelligence and Statistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stochastic combinatorial semi-bandit is an online learning problem where at
each step a learning agent chooses a subset of ground items subject to
constraints, and then observes stochastic weights of these items and receives
their sum as a payoff. In this paper, we close the problem of computationally
and sample efficient learning in stochastic combinatorial semi-bandits. In
particular, we analyze a UCB-like algorithm for solving the problem, which is
known to be computationally efficient; and prove $O(K L (1 / \Delta) \log n)$
and $O(\sqrt{K L n \log n})$ upper bounds on its $n$-step regret, where $L$ is
the number of ground items, $K$ is the maximum number of chosen items, and
$\Delta$ is the gap between the expected returns of the optimal and best
suboptimal solutions. The gap-dependent bound is tight up to a constant factor
and the gap-free bound is tight up to a polylogarithmic factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0952</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0952</id><created>2014-10-03</created><updated>2014-10-27</updated><authors><author><keyname>Wei</keyname><forenames>Dennis</forenames></author><author><keyname>Varshney</keyname><forenames>Kush R.</forenames></author></authors><title>Robust Binary Hypothesis Testing Under Contaminated Likelihoods</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In hypothesis testing, the phenomenon of label noise, in which hypothesis
labels are switched at random, contaminates the likelihood functions. In this
paper, we develop a new method to determine the decision rule when we do not
have knowledge of the uncontaminated likelihoods and contamination
probabilities, but only have knowledge of the contaminated likelihoods. In
particular we pose a minimax optimization problem that finds a decision rule
robust against this lack of knowledge. The method simplifies by application of
linear programming theory. Motivation for this investigation is provided by
problems encountered in workforce analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0956</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0956</id><created>2014-10-03</created><authors><author><keyname>Rossi</keyname><forenames>Federico</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Distributed consensus with mixed time/communication bandwidth
  performance metrics</title><categories>cs.SY cs.DC</categories><comments>Draft, submitted to Allerton 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the inherent trade-off between time and communication
complexity for the distributed consensus problem. In our model, communication
complexity is measured as the maximum data throughput (in bits per second) sent
through the network at a given instant. Such a notion of communication
complexity, referred to as bandwidth complexity, is related to the frequency
bandwidth a designer should collectively allocate to the agents if they were to
communicate via a wireless channel, which represents an important constraint
for dense robotic networks. We prove a lower bound on the bandwidth complexity
of the consensus problem and provide a consensus algorithm that is
bandwidth-optimal for a wide class of consensus functions. We then propose a
distributed algorithm that can trade communication complexity versus time
complexity as a function of a tunable parameter, which can be adjusted by a
system designer as a function of the properties of the wireless communication
channel. We rigorously characterize the tunable algorithm's worst-case
bandwidth complexity and show that it compares favorably with the bandwidth
complexity of well-known consensus algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0969</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0969</id><created>2014-08-26</created><authors><author><keyname>Kadir</keyname><forenames>Abdul</forenames></author></authors><title>A Model of Plant Identification System Using GLCM, Lacunarity And Shen
  Features</title><categories>cs.CV</categories><comments>10 pages</comments><journal-ref>Research Journal of Pharmaceutical, Biological and Chemical
  Sciences, Vol 5(2), 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, many approaches have been introduced by several researchers to
identify plants. Now, applications of texture, shape, color and vein features
are common practices. However, there are many possibilities of methods can be
developed to improve the performance of such identification systems. Therefore,
several experiments had been conducted in this research. As a result, a new
novel approach by using combination of Gray-Level Co-occurrence Matrix,
lacunarity and Shen features and a Bayesian classifier gives a better result
compared to other plant identification systems. For comparison, this research
used two kinds of several datasets that were usually used for testing the
performance of each plant identification system. The results show that the
system gives an accuracy rate of 97.19% when using the Flavia dataset and
95.00% when using the Foliage dataset and outperforms other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0973</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0973</id><created>2014-10-03</created><authors><author><keyname>Smirnova</keyname><forenames>Inna</forenames></author></authors><title>A Systematic Approach to Setting Up Distributed Global Collaborations
  for Software-based Products in the Automotive Domain</title><categories>cs.SE cs.CY</categories><comments>Master's thesis, 72 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an increasing need for organizations to collaborate with internal
and external partners on a global scale for creating software-based products
and services. Many aspects and risks need to be addressed when setting up such
global collaborations. Different types of collaborations such as engineering
collaborations or innovation-focused collaborations need to be considered.
Further aspects such as cultural and social aspects, coordination,
infrastructure, organizational change process, and communication issues need to
be examined. Although there are already experiences available with respect to
setting up global collaborations, they are mainly focusing on certain specific
areas. An overall holistic approach that guides companies in systematically
setting up global collaborations for software-based products is widely missing.
The goal of this thesis is to analyze existing literature and related
information and to extract topics that need be taken into account while
establishing global software development collaborations - to identify
solutions, risks, success factors, strategies, good experiences as well as good
examples. This information is structured in a way so that it can be used by
companies as a well-grounded holistic approach to guide companies effectively
in setting up long-term global collaborations in the domain software
development. The presented approach is based on scientific findings reported in
literature, driven by industry needs, and confirmed by industry experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0983</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0983</id><created>2014-10-03</created><updated>2014-10-07</updated><authors><author><keyname>Portnoi</keyname><forenames>Marcos</forenames></author><author><keyname>Shen</keyname><forenames>Chien-Chung</forenames></author></authors><title>Loc-Auth: Location-Enabled Authentication Through Attribute-Based
  Encryption</title><categories>cs.CR</categories><comments>Accepted at International Conference on Computing, Networking and
  Communications (ICNC 2015)</comments><doi>10.1109/ICCNC.2015.7069321</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional user authentication involves entering a username and password
into a system. Strong authentication security demands, among other
requirements, long, frequently hard-to-remember passwords. Two-factor
authentication aids in the security, even though, as a side effect, might
worsen user experience. We depict a mobile sign-on scheme that benefits from
the dynamic relationship between a user's attributes, the service the user
wishes to utilize, and location (where the user is, and what services are
available there) as an authentication factor. We demonstrate our scheme
employing Bluetooth Low Energy beacons for location awareness and the
expressiveness of Attribute-Based Encryption to capture and leverage the
described relationship. Bluetooth Low Energy beacons broadcast encrypted
messages with encoded access policies. Within range of the beacons, a user with
appropriate attributes is able to decrypt the broadcast message and obtain
parameters that allow the user to perform a short or simplified login.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0989</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0989</id><created>2014-10-03</created><updated>2015-07-27</updated><authors><author><keyname>Giryes</keyname><forenames>Raja</forenames></author><author><keyname>Plan</keyname><forenames>Yaniv</forenames></author><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>On the Effective Measure of Dimension in the Analysis Cosparse Model</title><categories>cs.IT math.IT math.NA math.OC stat.ME</categories><comments>19 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications have benefited remarkably from low-dimensional models in
the recent decade. The fact that many signals, though high dimensional, are
intrinsically low dimensional has given the possibility to recover them stably
from a relatively small number of their measurements. For example, in
compressed sensing with the standard (synthesis) sparsity prior and in matrix
completion, the number of measurements needed is proportional (up to a
logarithmic factor) to the signal's manifold dimension.
  Recently, a new natural low-dimensional signal model has been proposed: the
cosparse analysis prior. In the noiseless case, it is possible to recover
signals from this model, using a combinatorial search, from a number of
measurements proportional to the signal's manifold dimension. However, if we
ask for stability to noise or an efficient (polynomial complexity) solver, all
the existing results demand a number of measurements which is far removed from
the manifold dimension, sometimes far greater. Thus, it is natural to ask
whether this gap is a deficiency of the theory and the solvers, or if there
exists a real barrier in recovering the cosparse signals by relying only on
their manifold dimension. Is there an algorithm which, in the presence of
noise, can accurately recover a cosparse signal from a number of measurements
proportional to the manifold dimension? In this work, we prove that there is no
such algorithm. Further, we show through numerical simulations that even in the
noiseless case convex relaxations fail when the number of measurements is
comparable to the manifold dimension. This gives a practical counter-example to
the growing literature on compressed acquisition of signals based on manifold
dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0993</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0993</id><created>2014-10-03</created><authors><author><keyname>Li</keyname><forenames>Le</forenames></author><author><keyname>Yang</keyname><forenames>Jianjun</forenames></author><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Qin</keyname><forenames>Zhen</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author></authors><title>Document Clustering Based On Max-Correntropy Non-Negative Matrix
  Factorization</title><categories>cs.IR</categories><comments>International Conference of Machine Learning and Cybernetics (ICMLC)
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) has been successfully applied to many
areas for classification and clustering. Commonly-used NMF algorithms mainly
target on minimizing the $l_2$ distance or Kullback-Leibler (KL) divergence,
which may not be suitable for nonlinear case. In this paper, we propose a new
decomposition method by maximizing the correntropy between the original and the
product of two low-rank matrices for document clustering. This method also
allows us to learn the new basis vectors of the semantic feature space from the
data. To our knowledge, we haven't seen any work has been done by maximizing
correntropy in NMF to cluster high dimensional document data. Our experiment
results show the supremacy of our proposed method over other variants of NMF
algorithm on Reuters21578 and TDT2 databasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.0996</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.0996</id><created>2014-10-03</created><authors><author><keyname>Hanneke</keyname><forenames>Steve</forenames></author><author><keyname>Yang</keyname><forenames>Liu</forenames></author></authors><title>Minimax Analysis of Active Learning</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work establishes distribution-free upper and lower bounds on the minimax
label complexity of active learning with general hypothesis classes, under
various noise models. The results reveal a number of surprising facts. In
particular, under the noise model of Tsybakov (2004), the minimax label
complexity of active learning with a VC class is always asymptotically smaller
than that of passive learning, and is typically significantly smaller than the
best previously-published upper bounds in the active learning literature. In
high-noise regimes, it turns out that all active learning problems of a given
VC dimension have roughly the same minimax label complexity, which contrasts
with well-known results for bounded noise. In low-noise regimes, we find that
the label complexity is well-characterized by a simple combinatorial complexity
measure we call the star number. Interestingly, we find that almost all of the
complexity measures previously explored in the active learning literature have
worst-case values exactly equal to the star number. We also propose new active
learning strategies that nearly achieve these minimax label complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1002</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1002</id><created>2014-10-03</created><authors><author><keyname>Song</keyname><forenames>Eva C.</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A Rate-Distortion Based Secrecy System with Side Information at the
  Decoders</title><categories>cs.IT math.IT</categories><comments>8 pages. Allerton 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A secrecy system with side information at the decoders is studied in the
context of lossy source compression over a noiseless broadcast channel. The
decoders have access to different side information sequences that are
correlated with the source. The fidelity of the communication to the legitimate
receiver is measured by a distortion metric, as is traditionally done in the
Wyner-Ziv problem. The secrecy performance of the system is also evaluated
under a distortion metric. An achievable rate-distortion region is derived for
the general case of arbitrarily correlated side information. Exact bounds are
obtained for several special cases in which the side information satisfies
certain constraints. An example is considered in which the side information
sequences come from a binary erasure channel and a binary symmetric channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1006</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1006</id><created>2014-10-03</created><authors><author><keyname>Di Battista</keyname><forenames>Giuseppe</forenames></author><author><keyname>Frati</keyname><forenames>Fabrizio</forenames></author></authors><title>A Survey on Small-Area Planar Graph Drawing</title><categories>cs.CG cs.DM cs.DS</categories><comments>Preliminary version appeared in &quot;Thirty Essays on Geometric Graph
  Theory&quot;, J. Pach (ed.), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey algorithms and bounds for constructing planar drawings of graphs in
small area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1009</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1009</id><created>2014-10-03</created><updated>2014-12-31</updated><authors><author><keyname>Liao</keyname><forenames>Yen-Kai</forenames></author><author><keyname>Wang</keyname><forenames>Chih-Hang</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Chen</keyname><forenames>Wen-Tsuen</forenames></author></authors><title>Uplink Scheduling for LTE Video Surveillance Systems</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the proliferation of applications for the Internet of Things, an
increasing number of machine to machine (M2M) devices are being deployed. In
particular, one of the M2M applications, video surveillance, has been widely
discussed. Long Term Evolution (LTE), which can provide a high rate of data
transmission and wide range of coverage, is a promising standard to serve as an
M2M video surveillance system. In this paper, we studied a performance
maximization problem in an LTE video surveillance system. Given a set of
objects and a set of cameras, each camera has its own performance grade and its
own coverage. The goal is to maximize the performance of the system by
allocating limited resources to cameras while all objects should be monitored
by the selected cameras. We propose a heuristic method to select the cameras
and allocate resources to them to solve the problem. Moreover, to reduce the
load of the LTE system, a dynamic adjustment method is also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1016</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1016</id><created>2014-10-03</created><authors><author><keyname>Chekuri</keyname><forenames>Chandra</forenames></author><author><keyname>Chuzhoy</keyname><forenames>Julia</forenames></author></authors><title>Degree-3 Treewidth Sparsifiers</title><categories>cs.DS</categories><comments>Extended abstract to appear in Proceedings of ACM-SIAM SODA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study treewidth sparsifiers. Informally, given a graph $G$ of treewidth
$k$, a treewidth sparsifier $H$ is a minor of $G$, whose treewidth is close to
$k$, $|V(H)|$ is small, and the maximum vertex degree in $H$ is bounded.
Treewidth sparsifiers of degree $3$ are of particular interest, as routing on
node-disjoint paths, and computing minors seems easier in sub-cubic graphs than
in general graphs.
  In this paper we describe an algorithm that, given a graph $G$ of treewidth
$k$, computes a topological minor $H$ of $G$ such that (i) the treewidth of $H$
is $\Omega(k/\text{polylog}(k))$; (ii) $|V(H)| = O(k^4)$; and (iii) the maximum
vertex degree in $H$ is $3$. The running time of the algorithm is polynomial in
$|V(G)|$ and $k$. Our result is in contrast to the known fact that unless $NP
\subseteq coNP/{\sf poly}$, treewidth does not admit polynomial-size kernels.
One of our key technical tools, which is of independent interest, is a
construction of a small minor that preserves node-disjoint routability between
two pairs of vertex subsets. This is closely related to the open question of
computing small good-quality vertex-cut sparsifiers that are also minors of the
original graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1031</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1031</id><created>2014-10-04</created><authors><author><keyname>Hu</keyname><forenames>Su</forenames></author><author><keyname>Liu</keyname><forenames>Zilong</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Xiong</keyname><forenames>Wenhui</forenames></author><author><keyname>Bi</keyname><forenames>Guoan</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>Sequence Design for Cognitive CDMA Communications under Arbitrary
  Spectrum Hole Constraint</title><categories>cs.IT math.IT</categories><comments>13 pages,10 figures,Accepted by IEEE Journal on Selected Areas in
  Communications (JSAC)--Special Issue:Cognitive Radio Nov, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To support interference-free quasi-synchronous code-division multiple-access
(QS-CDMA) communication with low spectral density profile in a cognitive radio
(CR) network, it is desirable to design a set of CDMA spreading sequences with
zero-correlation zone (ZCZ) property. However, traditional ZCZ sequences (which
assume the availability of the entire spectral band) cannot be used because
their orthogonality will be destroyed by the spectrum hole constraint in a CR
channel. To date, analytical construction of ZCZ CR sequences remains open.
Taking advantage of the Kronecker sequence property, a novel family of
sequences (called &quot;quasi-ZCZ&quot; CR sequences) which displays zero
cross-correlation and near-zero auto-correlation zone property under arbitrary
spectrum hole constraint is presented in this paper. Furthermore, a novel
algorithm is proposed to jointly optimize the peak-to-average power ratio
(PAPR) and the periodic auto-correlations of the proposed quasi-ZCZ CR
sequences. Simulations show that they give rise to single-user bit-error-rate
performance in CR-CDMA systems which outperform traditional non-contiguous
multicarrier CDMA and transform domain communication systems; they also lead to
CR-CDMA systems which are more resilient than non-contiguous OFDM systems to
spectrum sensing mismatch, due to the wideband spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1035</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1035</id><created>2014-10-04</created><updated>2014-10-09</updated><authors><author><keyname>Varior</keyname><forenames>Rahul Rama</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Lu</keyname><forenames>Jiwen</forenames></author></authors><title>Learning Invariant Color Features for Person Re-Identification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matching people across multiple camera views known as person
re-identification, is a challenging problem due to the change in visual
appearance caused by varying lighting conditions. The perceived color of the
subject appears to be different with respect to illumination. Previous works
use color as it is or address these challenges by designing color spaces
focusing on a specific cue. In this paper, we propose a data driven approach
for learning color patterns from pixels sampled from images across two camera
views. The intuition behind this work is that, even though pixel values of same
color would be different across views, they should be encoded with the same
values. We model color feature generation as a learning problem by jointly
learning a linear transformation and a dictionary to encode pixel values. We
also analyze different photometric invariant color spaces. Using color as the
only cue, we compare our approach with all the photometric invariant color
spaces and show superior performance over all of them. Combining with other
learned low-level and high-level features, we obtain promising results in
ViPER, Person Re-ID 2011 and CAVIAR4REID datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1036</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1036</id><created>2014-10-04</created><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Nock</keyname><forenames>Richard</forenames></author></authors><title>Further results on the hyperbolic Voronoi diagrams</title><categories>cs.CG</categories><comments>6 pages, 2 figures (ISVD 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Euclidean geometry, it is well-known that the $k$-order Voronoi diagram in
$\mathbb{R}^d$ can be computed from the vertical projection of the $k$-level of
an arrangement of hyperplanes tangent to a convex potential function in
$\mathbb{R}^{d+1}$: the paraboloid. Similarly, we report for the Klein ball
model of hyperbolic geometry such a {\em concave} potential function: the
northern hemisphere. Furthermore, we also show how to build the hyperbolic
$k$-order diagrams as equivalent clipped power diagrams in $\mathbb{R}^d$. We
investigate the hyperbolic Voronoi diagram in the hyperboloid model and show
how it reduces to a Klein-type model using central projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1037</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1037</id><created>2014-10-04</created><authors><author><keyname>Wang</keyname><forenames>Nannan</forenames></author><author><keyname>Gao</keyname><forenames>Xinbo</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>Facial Feature Point Detection: A Comprehensive Survey</title><categories>cs.CV</categories><comments>32 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comprehensive survey of facial feature point detection
with the assistance of abundant manually labeled images. Facial feature point
detection favors many applications such as face recognition, animation,
tracking, hallucination, expression analysis and 3D face modeling. Existing
methods can be categorized into the following four groups: constrained local
model (CLM)-based, active appearance model (AAM)-based, regression-based, and
other methods. CLM-based methods consist of a shape model and a number of local
experts, each of which is utilized to detect a facial feature point. AAM-based
methods fit a shape model to an image by minimizing texture synthesis errors.
Regression-based methods directly learn a mapping function from facial image
appearance to facial feature points. Besides the above three major categories
of methods, there are also minor categories of methods which we classify into
other methods: graphical model-based methods, joint face alignment methods,
independent facial feature point detectors, and deep learning-based methods.
Though significant progress has been made, facial feature point detection is
limited in its success by wild and real-world conditions: variations across
poses, expressions, illuminations, and occlusions. A comparative illustration
and analysis of representative methods provide us a holistic understanding and
deep insight into facial feature point detection, which also motivates us to
explore promising future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1040</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1040</id><created>2014-10-04</created><authors><author><keyname>Weber</keyname><forenames>Stefan G.</forenames></author></authors><title>Alltagstaugliche Biometrie: Entwicklungen, Herausforderungen und Chancen</title><categories>cs.CR cs.CY</categories><comments>in German</comments><report-no>iit perspektive 21</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article summarizes recent trends in mobile biometrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1042</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1042</id><created>2014-10-04</created><updated>2015-11-19</updated><authors><author><keyname>Czerwi&#x144;ski</keyname><forenames>Wojciech</forenames></author><author><keyname>Martens</keyname><forenames>Wim</forenames></author><author><keyname>van Rooijen</keyname><forenames>Lorijn</forenames></author><author><keyname>Zeitoun</keyname><forenames>Marc</forenames></author><author><keyname>Zetzsche</keyname><forenames>Georg</forenames></author></authors><title>A Characterization for Decidable Separability by Piecewise Testable
  Languages</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The separability problem for word languages of a class $\mathcal{C}$ by
languages of a class $\mathcal{S}$ asks, for two given languages $I$ and $E$
from $\mathcal{C}$, whether there exists a language $S$ from $\mathcal{S}$ that
includes $I$ and excludes $E$, that is, $I \subseteq S$ and $S\cap E =
\emptyset$. In this work, we assume some mild closure properties for
$\mathcal{C}$ and study for which such classes $\mathcal{C}$ separability by
piecewise testable languages (PTL) is decidable. We characterize these classes
in terms of decidability of (two variants of) an unboundedness problem. From
this we deduce that separability by PTL is decidable for a number of language
classes, such as the context-free languages and languages of labeled vector
addition systems. Furthermore, it follows that separability by PTL is decidable
if and only if one can compute for any language of the class its downward
closure with respect to the scattered substring ordering (i.e., if the set of
scattered substrings of any language of the class is effectively regular).
  The obtained decidability results contrast known undecidability results. In
fact, for all the (non-regular) language classes we present as examples with
decidable separability, it is undecidable whether a given language is a PTL
itself.
  Our characterization involves a result of independent interest, which states
that for any kind of languages $I$ and $E$, non-separability is equivalent to
the existence of common patterns in $I$ and $E$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1059</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1059</id><created>2014-10-04</created><authors><author><keyname>Huang</keyname><forenames>Jiangbo</forenames></author></authors><title>Programing implementation of the Quine-McCluskey method for minimization
  of Boolean expression</title><categories>cs.OH</categories><comments>22 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Boolean function is a function that produces a Boolean value output by
logical calculation of Boolean inputs. It plays key roles in programing
algorithms and design of circuits. Minimization of Boolean function is able to
optimize the algorithms and circuits. Quine-McCluskey (QM) method is one of the
most powerful techniques to simplify Boolean expressions. Compared to other
techniques, QM method is more executable and can handle more variables. In
addition, QM method is easier to be implemented in computer programs, which
makes it an efficient technique. There are several versions of QM simulation
codes online, whereas some of them appear to have limitations of variables
numbers or lack the consideration of Dont-Care conditions. Here a QM simulation
code based on C programing is introduced. Theoretically it is able to handle
any number of variables and has taken the Dont-Care conditions into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1068</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1068</id><created>2014-10-04</created><authors><author><keyname>Roychowdhury</keyname><forenames>Anirban</forenames></author><author><keyname>Kulis</keyname><forenames>Brian</forenames></author></authors><title>Gamma Processes, Stick-Breaking, and Variational Inference</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While most Bayesian nonparametric models in machine learning have focused on
the Dirichlet process, the beta process, or their variants, the gamma process
has recently emerged as a useful nonparametric prior in its own right. Current
inference schemes for models involving the gamma process are restricted to
MCMC-based methods, which limits their scalability. In this paper, we present a
variational inference framework for models involving gamma process priors. Our
approach is based on a novel stick-breaking constructive definition of the
gamma process. We prove correctness of this stick-breaking process by using the
characterization of the gamma process as a completely random measure (CRM), and
we explicitly derive the rate measure of our construction using Poisson process
machinery. We also derive error bounds on the truncation of the infinite
process required for variational inference, similar to the truncation analyses
for other nonparametric models based on the Dirichlet and beta processes. Our
representation is then used to derive a variational inference algorithm for a
particular Bayesian nonparametric latent structure formulation known as the
infinite Gamma-Poisson model, where the latent variables are drawn from a gamma
process prior with Poisson likelihoods. Finally, we present results for our
algorithms on nonnegative matrix factorization tasks on document corpora, and
show that we compare favorably to both sampling-based techniques and
variational approaches based on beta-Bernoulli priors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1074</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1074</id><created>2014-10-04</created><authors><author><keyname>Wang</keyname><forenames>Kezhi</forenames></author><author><keyname>Chen</keyname><forenames>Yunfei</forenames></author><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author></authors><title>Outage Probability of Dual-Hop Selective AF With Randomly Distributed
  and Fixed Interferers</title><categories>cs.NI cs.IT math.IT</categories><comments>35 pages, 11 figures, accepted with minor revisions for publication
  as a regular paper in the IEEE Transactions on Vehicular Technology on
  21/09/2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The outage probability performance of a dual-hop amplify-and-forward
selective relaying system with global relay selection is analyzed for
Nakagami-$m$ fading channels in the presence of multiple interferers at both
the relays and the destination. Two different cases are considered. In the
first case, the interferers are assumed to have random number and locations.
Outage probability using the generalized Gamma approximation (GGA) in the form
of one-dimensional integral is derived. In the second case, the interferers are
assumed to have fixed number and locations. Exact outage probability in the
form of one-dimensional integral is derived. For both cases, closed-form
expressions of lower bounds and asymptotic expressions for high
signal-to-interference-plus-noise ratio are also provided. Simplified
closed-form expressions of outage probability for special cases (e.g., dominant
interferences, i.i.d. interferers, Rayleigh distributed signals) are studied.
Numerical results are presented to show the accuracy of our analysis by
examining the effects of the number and locations of interferers on the outage
performances of both AF systems with random and fixed interferers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1076</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1076</id><created>2014-10-04</created><authors><author><keyname>Giroire</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>Inria Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Mazauric</keyname><forenames>Dorian</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S, INRIA Sophia Antipolis</affiliation></author><author><keyname>Moulierac</keyname><forenames>Joanna</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author></authors><title>Energy Efficient Routing by Switching-Off Network Interfaces</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>Energy-Aware Systems and Networking for Sustainable Initiatives
  IGI Global (Ed.) (2012) 207-236</journal-ref><doi>10.4018/978-1-4666-1842-8.ch010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several studies exhibit that the traffic load of the routers only has a small
influence on their energy consumption. Hence, the power consumption in networks
is strongly related to the number of active network elements, such as
interfaces, line cards, base chassis,... The goal thus is to find a routing
that minimizes the (weighted) number of active network elements used when
routing. In this paper, we consider a simplified architecture where a
connection between two routers is represented as a link joining two network
interfaces. When a connection is not used, both network interfaces can be
turned off. Therefore, in order to reduce power consumption, the goal is to
find the routing that minimizes the number of used links while satisfying all
the demands. We first define formally the problem and we model it as an integer
linear program. Then, we prove that this problem is not in APX, that is there
is no polynomial-time constant-factor approximation algorithm. We propose a
heuristic algorithm for this problem and we also prove some negative results
about basic greedy and probabilistic algorithms. Thus we present a study on
specific topologies, such as trees, grids and complete graphs, that provide
bounds and results useful for real topologies. We then exhibit the gain in
terms of number of network interfaces (leading to a global reduction of
approximately 33 MWh for a medium-sized backbone network) for a set of existing
network topologies: we see that for almost all topologies more than one third
of the network interfaces can be spared for usual ranges of operation. Finally,
we discuss the impact of energy efficient routing on the stretch factor and on
fault tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1077</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1077</id><created>2014-10-04</created><updated>2015-06-25</updated><authors><author><keyname>Lopez-Ortiz</keyname><forenames>Alejandro</forenames></author><author><keyname>Maftuleac</keyname><forenames>Daniela</forenames></author></authors><title>Optimal Distributed Searching in the Plane with and without Uncertainty</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of multiple agents or robots searching for a target
in the plane. This is motivated by Search and Rescue operations (SAR) in the
high seas which in the past were often performed with several vessels, and more
recently by swarms of aerial drones and/or unmanned surface vessels.
Coordinating such a search in an effective manner is a non trivial task. In
this paper, we develop first an optimal strategy for searching with k robots
starting from a common origin and moving at unit speed. We then apply the
results from this model to more realistic scenarios such as differential search
speeds, late arrival times to the search effort and low probability of
detection under poor visibility conditions. We show that, surprisingly, the
theoretical idealized model still governs the search with certain suitable
minor adaptations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1080</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1080</id><created>2014-10-04</created><authors><author><keyname>Solovyev</keyname><forenames>Valery D.</forenames></author><author><keyname>Bochkarev</keyname><forenames>Vladimir V.</forenames></author></authors><title>Generating abbreviations using Google Books library</title><categories>cs.CL stat.AP</categories><comments>5 pages, 3 figures</comments><msc-class>91F20, 62P25</msc-class><acm-class>I.2.7; J.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article describes the original method of creating a dictionary of
abbreviations based on the Google Books Ngram Corpus. The dictionary of
abbreviations is designed for Russian, yet as its methodology is universal it
can be applied to any language. The dictionary can be used to define the
function of the period during text segmentation in various applied systems of
text processing. The article describes difficulties encountered in the process
of its construction as well as the ways to overcome them. A model of evaluating
a probability of first and second type errors (extraction accuracy and
fullness) is constructed. Certain statistical data for the use of abbreviations
are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1085</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1085</id><created>2014-10-04</created><authors><author><keyname>Einolghozati</keyname><forenames>Arash</forenames></author><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Design and Analysis of Wireless Communication Systems Using
  Diffusion-Based Molecular Communication Among Bacteria</title><categories>cs.IT cs.ET math.IT q-bio.MN</categories><comments>IEEE Transactions on Wireless communication Vol. 12, 2013. arXiv
  admin note: text overlap with arXiv:1209.2688</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of biologically-inspired wireless communication systems using
bacteria as the basic element of the system is initially motivated by a
phenomenon called \emph{Quorum Sensing}. Due to high randomness in the
individual behavior of a bacterium, reliable communication between two bacteria
is almost impossible. Therefore, we have recently proposed that a population of
bacteria in a cluster is considered as a bio node in the network capable of
molecular transmission and reception. This proposition enables us to form a
reliable bio node out of many unreliable bacteria.
  In this paper, we study the communication between two nodes in such a network
where information is encoded in the concentration of molecules by the
transmitter. The molecules produced by the bacteria in the transmitter node
propagate through the diffusion channel. Then, the concentration of molecules
is sensed by the bacteria population in the receiver node which would decode
the information and output light or fluorescent as a result. The uncertainty in
the communication is caused by all three components of communication, i.e.,
transmission, propagation and reception. We study the theoretical limits of the
information transfer rate in the presence of such uncertainties. Finally, we
consider M-ary signaling schemes and study their achievable rates and
corresponding error probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1086</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1086</id><created>2014-10-04</created><authors><author><keyname>Einolghozati</keyname><forenames>Arash</forenames></author><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Relaying in Diffusion-Based Molecular Communication</title><categories>cs.ET cs.IT math.IT q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular communication between biological entities is a new paradigm in
communications. Recently, we studied molecular communication between two nodes
formed from synthetic bacteria. Due to high randomness in behavior of bacteria,
we used a population of them in each node. The reliability of such
communication systems depends on both the maximum concentration of molecules
that a transmitter node is able to produce at the receiver node as well as the
number of bacteria in each nodes. This maximum concentration of molecules falls
with distance which makes the communication to the far nodes nearly impossible.
In order to alleviate this problem, in this paper, we propose to use a
molecular relaying node. The relay node can resend the message either by the
different or the same type of molecules as the original signal from the
transmitter. We study two scenarios of relaying. In the first scenario, the
relay node simply senses the received concentration and forwards it to the
receiver. We show that this sense and forward scenario, depending on the type
of molecules used for relaying, results in either increasing the range of
concentration of molecules at the receiver or increasing the effective number
of bacteria in the receiver node. For both cases of sense and forward relaying,
we obtain the resulting improvement in channel capacity. We conclude that
multi-type molecular relaying outperforms the single-type relaying. In the
second scenario, we study the decode and forward relaying for the M-ary
signaling scheme. We show that this relaying strategy increases the reliability
of M-ary communication significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1087</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1087</id><created>2014-10-04</created><authors><author><keyname>van der Aa</keyname><forenames>Christine P. D. M.</forenames></author><author><keyname>Pollmann</keyname><forenames>Monique M. H.</forenames></author><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>van der Gaag</keyname><forenames>Rutger Jan</forenames></author></authors><title>Computer-mediated communication in adults with high-functioning Autism
  Spectrum Conditions</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been suggested that people with Autism Spectrum Conditions (ASC) are
attracted to computer-mediated communication (CMC). In this study, several open
questions regarding CMC use in people with ASC which are investigated. We
compare CMC use in adults with high-functioning ASC (N = 113) and a control
group (N = 72). We find that people with ASC (1) spend more time on CMC than
controls, (2) are more positive about CMC, (3) report relatively high levels of
online social life satisfaction, and that (4) CMC use is negatively related to
satisfaction with life for people with ASC. Our results indicate that the ASC
subjects in this study use CMC at least as enthusiastically as controls, and
are proficient and successful in its use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1090</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1090</id><created>2014-10-04</created><authors><author><keyname>Mao</keyname><forenames>Junhua</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Jiang</forenames></author><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author></authors><title>Explain Images with Multimodal Recurrent Neural Networks</title><categories>cs.CV cs.CL cs.LG</categories><acm-class>I.2.6; I.2.7; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model
for generating novel sentence descriptions to explain the content of images. It
directly models the probability distribution of generating a word given
previous words and the image. Image descriptions are generated by sampling from
this distribution. The model consists of two sub-networks: a deep recurrent
neural network for sentences and a deep convolutional network for images. These
two sub-networks interact with each other in a multimodal layer to form the
whole m-RNN model. The effectiveness of our model is validated on three
benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model
outperforms the state-of-the-art generative method. In addition, the m-RNN
model can be applied to retrieval tasks for retrieving images or sentences, and
achieves significant performance improvement over the state-of-the-art methods
which directly optimize the ranking objective function for retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1103</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1103</id><created>2014-10-04</created><updated>2016-03-06</updated><authors><author><keyname>Chaudhuri</keyname><forenames>Sougata</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Online Ranking with Top-1 Feedback</title><categories>cs.LG</categories><comments>Previous version being replaced by conference version. Appeared in
  AISTATS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting where a system learns to rank a fixed set of $m$ items.
The goal is produce good item rankings for users with diverse interests who
interact online with the system for $T$ rounds. We consider a novel top-$1$
feedback model: at the end of each round, the relevance score for only the top
ranked object is revealed. However, the performance of the system is judged on
the entire ranked list. We provide a comprehensive set of results regarding
learnability under this challenging setting. For PairwiseLoss and DCG, two
popular ranking measures, we prove that the minimax regret is
$\Theta(T^{2/3})$. Moreover, the minimax regret is achievable using an
efficient strategy that only spends $O(m \log m)$ time per round. The same
efficient strategy achieves $O(T^{2/3})$ regret for Precision@$k$.
Surprisingly, we show that for normalized versions of these ranking measures,
i.e., AUC, NDCG \&amp; MAP, no online ranking algorithm can have sublinear regret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1113</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1113</id><created>2014-10-05</created><updated>2015-10-05</updated><authors><author><keyname>Anshelevich</keyname><forenames>Elliot</forenames></author><author><keyname>Sekar</keyname><forenames>Shreyas</forenames></author></authors><title>Price Competition in Networked Markets: How do monopolies impact social
  welfare?</title><categories>cs.GT</categories><comments>To appear in Proceedings of WINE 2015: 11th Conference on Web and
  Internet Economics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the efficiency of allocations in large markets with a network
structure where every seller owns an edge in a graph and every buyer desires a
path connecting some nodes. While it is known that stable allocations in such
settings can be very inefficient, the exact properties of equilibria in markets
with multiple sellers are not fully understood even in single-source
single-sink networks. In this work, we show that for a large class of natural
buyer demand functions, we are guaranteed the existence of an equilibrium with
several desirable properties. The crucial insight that we gain into the
equilibrium structure allows us to obtain tight bounds on efficiency in terms
of the various parameters governing the market, especially the number of
monopolies M. All of our efficiency results extend to markets without the
network structure.
  While it is known that monopolies can cause large inefficiencies in general,
our main results for single-source single-sink networks indicate that for
several natural demand functions the efficiency only drops linearly with M. For
example, for concave demand we prove that the efficiency loss is at most a
factor 1+M/2 from the optimum, for demand with monotone hazard rate it is at
most 1+M, and for polynomial demand the efficiency decreases logarithmically
with M. In contrast to previous work that showed that monopolies may adversely
affect welfare, our main contribution is showing that monopolies may not be as
`evil' as they are made out to be; the loss in efficiency is bounded in many
natural markets. Finally, we consider more general, multiple-source networks
and show that in the absence of monopolies, mild assumptions on the network
topology guarantee an equilibrium that maximizes social welfare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1120</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1120</id><created>2014-10-05</created><authors><author><keyname>Iwamoto</keyname><forenames>Mitsugu</forenames></author><author><keyname>Ohta</keyname><forenames>Kazuo</forenames></author><author><keyname>Shikata</keyname><forenames>Junji</forenames></author></authors><title>Security Formalizations and Their Relationships for Encryption and Key
  Agreement in Information-Theoretic Cryptography</title><categories>cs.CR cs.IT math.IT</categories><comments>25 pages. Submitted to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper revisits formalizations of information-theoretic security for
symmetric-key encryption and key agreement protocols which are very fundamental
primitives in cryptography. In general, we can formalize information-theoretic
security in various ways: some of them can be formalized as stand-alone
security by extending (or relaxing) Shannon's perfect secrecy or by other ways
such as semantic security; some of them can be done based on composable
security. Then, a natural question about this is: what is the gap between the
formalizations? To answer the question, we investigate relationships between
several formalizations of information-theoretic security for symmetric-key
encryption and key agreement protocols. Specifically, for symmetric-key
encryption protocols in a general setting including the case where there exist
decryption-errors, we deal with the following formalizations of security:
formalizations extended (or relaxed) from Shannon's perfect secrecy by using
mutual information and statistical distance; information-theoretic analogues of
indistinguishability and semantic security by Goldwasser and Micali; and
composable security by Maurer et al. and Canetti. Then, we explicitly show the
equivalence and non-equivalence between those formalizations. Under the model,
we also derive lower bounds on the adversary's (or distinguisher's) advantage
and the size of secret-keys required under all of the above formalizations.
Although some of them may be already known, we can explicitly derive them all
at once through our relationships between the formalizations. In addition, we
briefly observe impossibility results which easily follow from the lower
bounds. The similar results are also shown for key agreement protocols in a
general setting including the case where there exist agreement-errors in the
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1127</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1127</id><created>2014-10-05</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Xu</keyname><forenames>Shenmeng</forenames></author><author><keyname>Wang</keyname><forenames>Zhi</forenames></author><author><keyname>Peng</keyname><forenames>Lian</forenames></author><author><keyname>Wang</keyname><forenames>Chuanli</forenames></author></authors><title>International Scientific Collaboration of China: Collaborating
  Countries, Institutions and Individuals</title><categories>cs.DL physics.soc-ph</categories><comments>11 pages,3 figures</comments><journal-ref>Scientometrics 95.3 (2013): 885-894</journal-ref><doi>10.1007/s11192-012-0877-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using bibliometric methods, we investigate China's international scientific
collaboration from 3 levels of collaborating countries, institutions and
individuals. We design a database in SQL Server, and make analysis of Chinese
SCI papers based on the corresponding author field. We find that China's
international scientific collaboration is focused on a handful of countries.
Nearly 95% international co-authored papers are collaborated with only 20
countries, among which the USA account for more than 40% of all. Results also
show that Chinese lineage in the international co-authorship is obvious, which
means Chinese immigrant scientists are playing an important role in China's
international scientific collaboration, especially in English-speaking
countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1129</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1129</id><created>2014-10-05</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Xu</keyname><forenames>Shenmeng</forenames></author><author><keyname>Liu</keyname><forenames>Di</forenames></author><author><keyname>Liang</keyname><forenames>Yongxia</forenames></author></authors><title>The Role of Chinese-American Scientists in China-US Scientific
  Collaboration: A Study in Nanotechnology</title><categories>cs.DL physics.data-an</categories><comments>13 pages, 7 figures</comments><journal-ref>Scientometrics 91.3 (2012): 737-749</journal-ref><doi>10.1007/s11192-012-0693-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use bibliometric methods and social network analysis to
analyze the pattern of China-US scientific collaboration on individual level in
nanotechnology. Results show that Chinese-American scientists have been playing
an important role in China-US scientific collaboration. We find that China-US
collaboration in nanotechnology mainly occurs between Chinese and
Chinese-American scientists. In the co-authorship network, Chinese-American
scientists tend to have higher betweenness centrality. Moreover, the series of
polices implemented by the Chinese government to recruit oversea experts seems
to contribute a lot to China-US scientific collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1130</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1130</id><created>2014-10-05</created><authors><author><keyname>Samyn</keyname><forenames>Koen</forenames></author><author><keyname>Van Hoecke</keyname><forenames>Sofie</forenames></author><author><keyname>Pieters</keyname><forenames>Bart</forenames></author><author><keyname>Hollemeersch</keyname><forenames>Charles</forenames></author><author><keyname>Demeulemeester</keyname><forenames>Aljosha</forenames></author><author><keyname>van de Walle</keyname><forenames>Rik</forenames></author></authors><title>Real-time animation of human characters with fuzzy controllers</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The production of animation is a resource intensive process in game
companies. Therefore, techniques to synthesize animations have been developed.
However, these procedural techniques offer limited adaptability by animation
artists. In order to solve this, a fuzzy neural network model of the animation
is proposed, where the parameters can be tuned either by machine learning
techniques that use motion capture data as training data or by the animation
artist himself. This paper illustrates how this real time procedural animation
system can be developed, taking the human gait on flat terrain and inclined
surfaces as example. Currently, the parametric model is capable of synthesizing
animations for various limb sizes and step sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1135</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1135</id><created>2014-10-05</created><authors><author><keyname>Medhat</keyname><forenames>Walaa</forenames></author><author><keyname>Yousef</keyname><forenames>Ahmed H.</forenames></author><author><keyname>Korashy</keyname><forenames>Hoda</forenames></author></authors><title>Corpora Preparation and Stopword List Generation for Arabic data in
  Social Network</title><categories>cs.CL</categories><comments>Language Engineering Conference 2014, Cairo, Egypt, 1-3 December 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a methodology to prepare corpora in Arabic language from
online social network (OSN) and review site for Sentiment Analysis (SA) task.
The paper also proposes a methodology for generating a stopword list from the
prepared corpora. The aim of the paper is to investigate the effect of removing
stopwords on the SA task. The problem is that the stopwords lists generated
before were on Modern Standard Arabic (MSA) which is not the common language
used in OSN. We have generated a stopword list of Egyptian dialect and a
corpus-based list to be used with the OSN corpora. We compare the efficiency of
text classification when using the generated lists along with previously
generated lists of MSA and combining the Egyptian dialect list with the MSA
list. The text classification was performed using Na\&quot;ive Bayes and Decision
Tree classifiers and two feature selection approaches, unigrams and bigram. The
experiments show that the general lists containing the Egyptian dialects words
give better performance than using lists of MSA stopwords only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1141</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1141</id><created>2014-10-05</created><updated>2014-10-28</updated><authors><author><keyname>Livni</keyname><forenames>Roi</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>On the Computational Efficiency of Training Neural Networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>Section 2 is revised due to a mistake</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that neural networks are computationally hard to train. On
the other hand, in practice, modern day neural networks are trained efficiently
using SGD and a variety of tricks that include different activation functions
(e.g. ReLU), over-specification (i.e., train networks which are larger than
needed), and regularization. In this paper we revisit the computational
complexity of training neural networks from a modern perspective. We provide
both positive and negative results, some of them yield new provably efficient
and practical algorithms for training certain types of neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1150</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1150</id><created>2014-10-05</created><updated>2014-12-30</updated><authors><author><keyname>Kolliopoulos</keyname><forenames>Stavros G.</forenames></author><author><keyname>Moysoglou</keyname><forenames>Yannis</forenames></author></authors><title>Extended Formulation Lower Bounds via Hypergraph Coloring?</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploring the power of linear programming for combinatorial optimization
problems has been recently receiving renewed attention after a series of
breakthrough impossibility results. From an algorithmic perspective, the
related questions concern whether there are compact formulations even for
problems that are known to admit polynomial-time algorithms.
  We propose a framework for proving lower bounds on the size of extended
formulations. We do so by introducing a specific type of extended relaxations
that we call product relaxations and is motivated by the study of the
Sherali-Adams (SA) hierarchy. Then we show that for every approximate
relaxation of a polytope P, there is a product relaxation that has the same
size and is at least as strong. We provide a methodology for proving lower
bounds on the size of approximate product relaxations by lower bounding the
chromatic number of an underlying hypergraph, whose vertices correspond to
gap-inducing vectors.
  We extend the definition of product relaxations and our methodology to mixed
integer sets. However in this case we are able to show that mixed product
relaxations are at least as powerful as a special family of extended
formulations. As an application of our method we show an exponential lower
bound on the size of approximate mixed product formulations for the metric
capacitated facility location problem, a problem which seems to be intractable
for linear programming as far as constant-gap compact formulations are
concerned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1151</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1151</id><created>2014-10-05</created><authors><author><keyname>Maslennikova</keyname><forenames>Yulia S.</forenames></author><author><keyname>Bochkarev</keyname><forenames>Vladimir V.</forenames></author></authors><title>Training Algorithm for Neuro-Fuzzy Network Based on Singular Spectrum
  Analysis</title><categories>cs.NE stat.ME</categories><comments>5 pages, 3 figures</comments><msc-class>62M45, 62M10, 68T05</msc-class><acm-class>I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose a combination of an noise-reduction algorithm
based on Singular Spectrum Analysis (SSA) and a standard feedforward neural
prediction model. Basically, the proposed algorithm consists of two different
steps: data preprocessing based on the SSA filtering method and step-by-step
training procedure in which we use a simple feedforward multilayer neural
network with backpropagation learning. The proposed noise-reduction procedure
successfully removes most of the noise. That increases long-term predictability
of the processed dataset comparison with the raw dataset. The method was
applied to predict the International sunspot number RZ time series. The results
show that our combined technique has better performances than those offered by
the same network directly applied to raw dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1155</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1155</id><created>2014-10-05</created><authors><author><keyname>Tahir</keyname><forenames>Amjed</forenames></author><author><keyname>MacDonell</keyname><forenames>Stephen G.</forenames></author><author><keyname>Buchan</keyname><forenames>Jim</forenames></author></authors><title>Understanding Class-level Testability Through Dynamic Analysis</title><categories>cs.SE</categories><comments>10 pages, conference paper, 9th International Conference on
  Evaluation of Novel Approaches to Software Engineering (ENASE), 2014</comments><doi>10.5220/0004883400380047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is generally acknowledged that software testing is both challenging and
time-consuming. Understanding the factors that may positively or negatively
affect testing effort will point to possibilities for reducing this effort.
Consequently there is a significant body of research that has investigated
relationships between static code properties and testability. The work reported
in this paper complements this body of research by providing an empirical
evaluation of the degree of association between runtime properties and
class-level testability in object-oriented (OO) systems. The motivation for the
use of dynamic code properties comes from the success of such metrics in
providing a more complete insight into the multiple dimensions of software
quality. In particular, we investigate the potential relationships between the
runtime characteristics of production code, represented by Dynamic Coupling and
Key Classes, and internal class-level testability. Testability of a class is
considered here at the level of unit tests and two different measures are used
to characterise those unit tests. The selected measures relate to test scope
and structure: one is intended to measure the unit test size, represented by
test lines of code, and the other is designed to reflect the intended design,
represented by the number of test cases. In this research we found that Dynamic
Coupling and Key Classes have significant correlations with class-level
testability measures. We therefore suggest that these properties could be used
as indicators of class-level testability. These results enhance our current
knowledge and should help researchers in the area to build on previous results
regarding factors believed to be related to testability and testing. Our
results should also benefit practitioners in future class testability planning
and maintenance activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1158</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1158</id><created>2014-10-05</created><authors><author><keyname>Milenkoski</keyname><forenames>Aleksandar</forenames></author><author><keyname>Vieira</keyname><forenames>Marco</forenames></author><author><keyname>Payne</keyname><forenames>Bryan D.</forenames></author><author><keyname>Antunes</keyname><forenames>Nuno</forenames></author><author><keyname>Kounev</keyname><forenames>Samuel</forenames></author></authors><title>Technical Information on Vulnerabilities of Hypercall Handlers</title><categories>cs.CR</categories><comments>SPEC (Standard Performance Evaluation Corporation) Research Group ---
  IDS Benchmarking Working Group</comments><report-no>SPEC-RG-2014-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern virtualized service infrastructures expose attack vectors that enable
attacks of high severity, such as attacks targeting hypervisors. A malicious
user of a guest VM (virtual machine) may execute an attack against the
underlying hypervisor via hypercalls, which are software traps from a kernel of
a fully or partially paravirtualized guest VM to the hypervisor. The
exploitation of a vulnerability of a hypercall handler may have severe
consequences such as altering hypervisor's memory, which may result in the
execution of malicious code with hypervisor privilege. Despite the importance
of vulnerabilities of hypercall handlers, there is not much publicly available
information on them. This significantly hinders advances towards securing
hypercall interfaces. In this work, we provide in-depth technical information
on publicly disclosed vulnerabilities of hypercall handlers. Our vulnerability
analysis is based on reverse engineering the released patches fixing the
considered vulnerabilities. For each analyzed vulnerability, we provide
background information essential for understanding the vulnerability, and
information on the vulnerable hypercall handler and the error causing the
vulnerability. We also show how the vulnerability can be triggered and discuss
the state of the targeted hypervisor after the vulnerability has been
triggered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1159</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1159</id><created>2014-10-05</created><authors><author><keyname>Milenkoski</keyname><forenames>Aleksandar</forenames></author><author><keyname>Iosup</keyname><forenames>Alexandru</forenames></author><author><keyname>Kounev</keyname><forenames>Samuel</forenames></author><author><keyname>Sachs</keyname><forenames>Kai</forenames></author><author><keyname>Rygielski</keyname><forenames>Piotr</forenames></author><author><keyname>Ding</keyname><forenames>Jason</forenames></author><author><keyname>Cirne</keyname><forenames>Walfredo</forenames></author><author><keyname>Rosenberg</keyname><forenames>Florian</forenames></author></authors><title>Cloud Usage Patterns: A Formalism for Description of Cloud Usage
  Scenarios</title><categories>cs.DC</categories><comments>SPEC (Standard Performance Evaluation Corporation) Research Group ---
  Cloud Working Group</comments><report-no>SPEC-RG-2013-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is becoming an increasingly lucrative branch of the existing
information and communication technologies (ICT). Enabling a debate about cloud
usage scenarios can help with attracting new customers, sharing best-practices,
and designing new cloud services. In contrast to previous approaches, which
have attempted mainly to formalize the common service delivery models (i.e.,
Infrastructure-as-a-Service, Platform-as-a-Service, and Software-as-a-Service),
in this work, we propose a formalism for describing common cloud usage
scenarios referred to as cloud usage patterns. Our formalism takes a
structuralist approach allowing decomposition of a cloud usage scenario into
elements corresponding to the common cloud service delivery models.
Furthermore, our formalism considers several cloud usage patterns that have
recently emerged, such as hybrid services and value chains in which mediators
are involved, also referred to as value chains with mediators. We propose a
simple yet expressive textual and visual language for our formalism, and we
show how it can be used in practice for describing a variety of real-world
cloud usage scenarios. The scenarios for which we demonstrate our formalism
include resource provisioning of global providers of infrastructure and/or
platform resources, online social networking services, user-data processing
services, online customer and ticketing services, online asset management and
banking applications, CRM (Customer Relationship Management) applications, and
online social gaming applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1160</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1160</id><created>2014-10-05</created><authors><author><keyname>Milenkoski</keyname><forenames>Aleksandar</forenames></author><author><keyname>Kounev</keyname><forenames>Samuel</forenames></author><author><keyname>Avritzer</keyname><forenames>Alberto</forenames></author><author><keyname>Antunes</keyname><forenames>Nuno</forenames></author><author><keyname>Vieira</keyname><forenames>Marco</forenames></author></authors><title>On Benchmarking Intrusion Detection Systems in Virtualized Environments</title><categories>cs.CR cs.SY</categories><comments>SPEC (Standard Performance Evaluation Corporation) Research Group ---
  IDS Benchmarking Working Group</comments><report-no>SPEC-RG-2013-002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern intrusion detection systems (IDSes) for virtualized environments are
deployed in the virtualization layer with components inside the virtual machine
monitor (VMM) and the trusted host virtual machine (VM). Such IDSes can monitor
at the same time the network and host activities of all guest VMs running on
top of a VMM being isolated from malicious users of these VMs. We refer to
IDSes for virtualized environments as VMM-based IDSes. In this work, we analyze
state-of-the-art intrusion detection techniques applied in virtualized
environments and architectures of VMM-based IDSes. Further, we identify
challenges that apply specifically to benchmarking VMM-based IDSes focussing on
workloads and metrics. For example, we discuss the challenge of defining
representative baseline benign workload profiles as well as the challenge of
defining malicious workloads containing attacks targeted at the VMM. We also
discuss the impact of on-demand resource provisioning features of virtualized
environments (e.g., CPU and memory hotplugging, memory ballooning) on IDS
benchmarking measures such as capacity and attack detection accuracy. Finally,
we outline future research directions in the area of benchmarking VMM-based
IDSes and of intrusion detection in virtualized environments in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1165</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1165</id><created>2014-10-05</created><updated>2015-04-08</updated><authors><author><keyname>Srivastava</keyname><forenames>Rupesh Kumar</forenames></author><author><keyname>Masci</keyname><forenames>Jonathan</forenames></author><author><keyname>Gomez</keyname><forenames>Faustino</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Understanding Locally Competitive Networks</title><categories>cs.NE cs.LG</categories><comments>9 pages + 2 supplementary, Accepted to ICLR 2015 Conference track</comments><msc-class>68T30, 68T10</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently proposed neural network activation functions such as rectified
linear, maxout, and local winner-take-all have allowed for faster and more
effective training of deep neural architectures on large and complex datasets.
The common trait among these functions is that they implement local competition
between small groups of computational units within a layer, so that only part
of the network is activated for any given input pattern. In this paper, we
attempt to visualize and understand this self-modularization, and suggest a
unified explanation for the beneficial properties of such networks. We also
show how our insights can be directly useful for efficiently performing
retrieval over large datasets using neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1204</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1204</id><created>2014-10-05</created><authors><author><keyname>Bidoni</keyname><forenames>Zeynab Bahrami</forenames></author><author><keyname>George</keyname><forenames>Roy</forenames></author></authors><title>Network Performance Rank: An Approach for Comparison of Complex Networks</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 4 figures, 4 tables, 47 references</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Researchers have typically concentrated on analyzing what happens internally
in a complex network and using this to distinguish between nodes. However,
there has been less effort towards comparing between different networks. In
this paper, we proposed a novel approach to rank alternative complex networks
based on their performances. We consider this as a ranking problem in decision
analysis based on occurring positive/negative frequent events as criteria, and
using the TOPSIS method to rank alternatives. In order to assign a score to the
networks for each criterion, a statistical method that estimates the expected
value of positive/negative frequent events on a random node is presented. The
proposed technique is efficient in terms of algorithm complexity and is capable
of discriminating events occurring between important nodes over those between
less significant nodes. The experiments, conducted on several synthetic
networks, demonstrate the feasibility and applicability of the ranking
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1209</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1209</id><created>2014-10-05</created><authors><author><keyname>Chauhan</keyname><forenames>Himanshu</forenames></author><author><keyname>Garg</keyname><forenames>Vijay K.</forenames></author></authors><title>Necessary and Sufficient Conditions on Partial Orders for Modeling
  Concurrent Computations</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial orders are used extensively for modeling and analyzing concurrent
computations. In this paper, we define two properties of partially ordered
sets: width-extensibility and interleaving-consistency, and show that a partial
order can be a valid state based model: (1) of some synchronous concurrent
computation iff it is width-extensible, and (2) of some asynchronous concurrent
computation iff it is width-extensible and interleaving-consistent. We also
show a duality between the event based and state based models of concurrent
computations, and give algorithms to convert models between the two domains.
When applied to the problem of checkpointing, our theory leads to a better
understanding of some existing results and algorithms in the field. It also
leads to efficient detection algorithms for predicates whose evaluation
requires knowledge of states from all the processes in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1211</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1211</id><created>2014-10-05</created><updated>2015-07-19</updated><authors><author><keyname>Burnett</keyname><forenames>Sam</forenames></author><author><keyname>Feamster</keyname><forenames>Nick</forenames></author></authors><title>Encore: Lightweight Measurement of Web Censorship with Cross-Origin
  Requests</title><categories>cs.NI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the pervasiveness of Internet censorship, we have scant data on its
extent, mechanisms, and evolution. Measuring censorship is challenging: it
requires continual measurement of reachability to many target sites from
diverse vantage points. Amassing suitable vantage points for longitudinal
measurement is difficult; existing systems have achieved only small,
short-lived deployments. We observe, however, that most Internet users access
content via Web browsers, and the very nature of Web site design allows
browsers to make requests to domains with different origins than the main Web
page. We present Encore, a system that harnesses cross-origin requests to
measure Web filtering from a diverse set of vantage points without requiring
users to install custom software, enabling longitudinal measurements from many
vantage points. We explain how Encore induces Web clients to perform
cross-origin requests that measure Web filtering, design a distributed platform
for scheduling and collecting these measurements, show the feasibility of a
global-scale deployment with a pilot study and an analysis of potentially
censored Web content, identify several cases of filtering in six months of
measurements, and discuss ethical concerns that would arise with widespread
deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1217</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1217</id><created>2014-10-05</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>Curvature from Graph Colorings</title><categories>math.CO cs.DM</categories><comments>14 pages, 6 figures</comments><msc-class>05C15, 05C10, 53C65, 58J20, 05C31, 05C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite simple graph G=(V,E) with chromatic number c and chromatic
polynomial C(x). Every vertex graph coloring f of G defines an index i_f(x)
satisfying the Poincare-Hopf theorem sum_x i_f(x)=chi(G). As a variant to the
index expectation result we prove that E[i_f(x)] is equal to curvature K(x)
satisfying Gauss-Bonnet sum_x K(x) = \chi(G), where the expectation is the
average over the finite probability space containing the C(c) possible
colorings with c colors, for which each coloring has the same probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1228</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1228</id><created>2014-10-05</created><updated>2015-02-20</updated><authors><author><keyname>Steinke</keyname><forenames>Thomas</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>Interactive Fingerprinting Codes and the Hardness of Preventing False
  Discovery</title><categories>cs.CR cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show an essentially tight bound on the number of adaptively chosen
statistical queries that a computationally efficient algorithm can answer
accurately given $n$ samples from an unknown distribution. A statistical query
asks for the expectation of a predicate over the underlying distribution, and
an answer to a statistical query is accurate if it is &quot;close&quot; to the correct
expectation over the distribution. This question was recently studied by Dwork
et al., who showed how to answer $\tilde{\Omega}(n^2)$ queries efficiently, and
also by Hardt and Ullman, who showed that answering $\tilde{O}(n^3)$ queries is
hard. We close the gap between the two bounds and show that, under a standard
hardness assumption, there is no computationally efficient algorithm that,
given $n$ samples from an unknown distribution, can give valid answers to
$O(n^2)$ adaptively chosen statistical queries. An implication of our results
is that computationally efficient algorithms for answering arbitrary,
adaptively chosen statistical queries may as well be differentially private.
  We obtain our results using a new connection between the problem of answering
adaptively chosen statistical queries and a combinatorial object called an
interactive fingerprinting code. In order to optimize our hardness result, we
give a new Fourier-analytic approach to analyzing fingerprinting codes that is
simpler, more flexible, and yields better parameters than previous
constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1231</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1231</id><created>2014-10-05</created><authors><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author><author><keyname>Zhang</keyname><forenames>Kang</forenames></author></authors><title>Bayesian regression and Bitcoin</title><categories>cs.AI math.ST stat.TH</categories><comments>Preliminary version appeared in the Proceedings of 2014 Allerton
  Conference on Communication, Control, and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss the method of Bayesian regression and its efficacy
for predicting price variation of Bitcoin, a recently popularized virtual,
cryptographic currency. Bayesian regression refers to utilizing empirical data
as proxy to perform Bayesian inference. We utilize Bayesian regression for the
so-called &quot;latent source model&quot;. The Bayesian regression for &quot;latent source
model&quot; was introduced and discussed by Chen, Nikolov and Shah (2013) and
Bresler, Chen and Shah (2014) for the purpose of binary classification. They
established theoretical as well as empirical efficacy of the method for the
setting of binary classification.
  In this paper, instead we utilize it for predicting real-valued quantity, the
price of Bitcoin. Based on this price prediction method, we devise a simple
strategy for trading Bitcoin. The strategy is able to nearly double the
investment in less than 60 day period when run against real data trace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1233</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1233</id><created>2014-10-05</created><updated>2016-02-22</updated><authors><author><keyname>Sakov</keyname><forenames>Pavel</forenames></author></authors><title>EnKF-C user guide</title><categories>cs.CE</categories><msc-class>90-04</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EnKF-C provides a light-weight generic framework for off-line data
assimilation into large-scale layered geophysical models with the ensemble
Kalman filter (EnKF). It is coded in C for GNU/Linux platform and can work
either in EnKF or ensemble optimal interpolation (EnOI) mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1237</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1237</id><created>2014-10-05</created><updated>2014-10-06</updated><authors><author><keyname>Lu</keyname><forenames>Hao</forenames></author><author><keyname>Halappanavar</keyname><forenames>Mahantesh</forenames></author><author><keyname>Kalyanaraman</keyname><forenames>Ananth</forenames></author></authors><title>Parallel Heuristics for Scalable Community Detection</title><categories>cs.SI cs.DC physics.soc-ph</categories><comments>Submitted to a journal</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Community detection has become a fundamental operation in numerous
graph-theoretic applications. It is used to reveal natural divisions that exist
within real world networks without imposing prior size or cardinality
constraints on the set of communities. Despite its potential for application,
there is only limited support for community detection on large-scale parallel
computers, largely owing to the irregular and inherently sequential nature of
the underlying heuristics. In this paper, we present parallelization heuristics
for fast community detection using the Louvain method as the serial template.
The Louvain method is an iterative heuristic for modularity optimization.
Originally developed by Blondel et al. in 2008, the method has become
increasingly popular owing to its ability to detect high modularity community
partitions in a fast and memory-efficient manner. However, the method is also
inherently sequential, thereby limiting its scalability. Here, we observe
certain key properties of this method that present challenges for its
parallelization, and consequently propose heuristics that are designed to break
the sequential barrier. For evaluation purposes, we implemented our heuristics
using OpenMP multithreading, and tested them over real world graphs derived
from multiple application domains (e.g., internet, citation, biological).
Compared to the serial Louvain implementation, our parallel implementation is
able to produce community outputs with a higher modularity for most of the
inputs tested, in comparable number or fewer iterations, while providing
absolute speedups of up to 16x using 32 threads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1255</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1255</id><created>2014-10-06</created><updated>2015-02-03</updated><authors><author><keyname>Li</keyname><forenames>Weidong</forenames></author><author><keyname>Liu</keyname><forenames>Xi</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaolu</forenames></author><author><keyname>Zhang</keyname><forenames>Xuejie</forenames></author></authors><title>Multi-resource Fair Allocation with Bounded Number of Tasks in Cloud
  Computing Systems</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dominant resource fairness (DRF) is a popular mechanism for multi-resource
allocation in cloud computing systems. In this paper, we consider a problem of
multi-resource fair allocation with bounded number of tasks. Firstly, we
propose the lexicographically max-min normalized share (LMMNS) fair allocation
mechanism, which is a natural generalization of DRF, and design a non-trivial
optimal algorithm to find a LMMNS fair allocation, whose running time is linear
in the number of users. Secondly, we prove that LMMNS satisfies envy-freeness
(EF) and group strategy-proofness (GSP), and analysis the approximation ratios
of LMMNS, by exploiting the properties of the optimal solution. Thirdly, we
propose a modified version of LMMNS, which is the second mechanism satisfying
sharing incentive, EF, and GSP. Finally, we have implemented LMMNS, and show
that it has a good average-case performance, especially when the number of
resources is 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1257</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1257</id><created>2014-10-06</created><authors><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Choday</keyname><forenames>Sri Harsha</forenames></author><author><keyname>Kim</keyname><forenames>Yusung</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Spin Orbit Torque Based Electronic Neuron</title><categories>cs.ET</categories><doi>10.1063/1.4917011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A device based on current-induced spin-orbit torque (SOT) that functions as
an electronic neuron is proposed in this work. The SOT device implements an
artificial neuron's thresholding (transfer) function. In the first step of a
two-step switching scheme, a charge current places the magnetization of a
nano-magnet along the hard-axis i.e. an unstable point for the magnet. In the
second step, the SOT device (neuron) receives a current (from the synapses)
which moves the magnetization from the unstable point to one of the two stable
states. The polarity of the synaptic current encodes the excitatory and
inhibitory nature of the neuron input, and determines the final orientation of
the magnetization. A resistive crossbar array, functioning as synapses,
generates a bipolar current that is a weighted sum of the inputs. The
simulation of a two layer feed-forward Artificial Neural Network (ANN) based on
the SOT electronic neuron shows that it consumes ~3X lower power than a 45nm
digital CMOS implementation, while reaching ~80% accuracy in the classification
of one hundred images of handwritten digits from the MNIST dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1258</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1258</id><created>2014-10-06</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Peake</keyname><forenames>Ian</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinz</forenames></author></authors><title>Cyber-Virtual Systems: Simulation, Validation &amp; Visualization</title><categories>cs.SE</categories><comments>Preprint, 9th International Conference on Evaluation of Novel
  Approaches to Software Engineering (ENASE 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe our ongoing work and view on simulation, validation and
visualization of cyber-physical systems in industrial automation during
development, operation and maintenance. System models may represent an existing
physical part - for example an existing robot installation - and a software
simulated part - for example a possible future extension. We call such systems
cyber-virtual systems.
  In this paper, we present the existing VITELab infrastructure for
visualization tasks in industrial automation. The new methodology for
simulation and validation motivated in this paper integrates this
infrastructure. We are targeting scenarios, where industrial sites which may be
in remote locations are modeled and visualized from different sites anywhere in
the world.
  Complementing the visualization work, here, we are also concentrating on
software modeling challenges related to cyber-virtual systems and simulation,
testing, validation and verification techniques for them. Software models of
industrial sites require behavioural models of the components of the industrial
sites such as models for tools, robots, workpieces and other machinery as well
as communication and sensor facilities. Furthermore, collaboration between
sites is an important goal of our work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1266</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1266</id><created>2014-10-06</created><updated>2016-01-18</updated><authors><author><keyname>Zhou</keyname><forenames>Xun</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Wireless Power Meets Energy Harvesting: A Joint Energy Allocation
  Approach in OFDM-based System</title><categories>cs.IT math.IT</categories><comments>This is a longer version of a paper to be appear in IEEE Transactions
  on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates an orthogonal frequency division multiplexing
(OFDM)-based wireless powered communication system, where one user harvests
energy from an energy access point (EAP) to power its information transmission
to a data access point (DAP). The channels from the EAP to the user, i.e., the
wireless energy transfer (WET) link, and from the user to the DAP, i.e., the
wireless information transfer (WIT) link, vary over both time slots and
sub-channels (SCs) in general. To avoid interference at DAP, WET and WIT are
scheduled over orthogonal SCs at any slot. Our objective is to maximize the
achievable rate at the DAP by jointly optimizing the SC allocation over time
and the power allocation over time and SCs for both WET and WIT links. Assuming
availability of full channel state information (CSI), the structural results
for the optimal SC/power allocation are obtained and an offline algorithm is
proposed to solve the problem. Furthermore, we propose a low-complexity online
algorithm when causal CSI is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1267</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1267</id><created>2014-10-06</created><authors><author><keyname>Maan</keyname><forenames>Akshay Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>Dinesh Sasi</forenames></author><author><keyname>Sugathan</keyname><forenames>Sherin</forenames></author><author><keyname>James</keyname><forenames>Alex Pappachen</forenames></author></authors><title>Memristive Threshold Logic Circuit Design of Fast Moving Object
  Detection</title><categories>cs.CV cs.AR cs.ET</categories><comments>To be published in IEEE Transactions on Very Large Scale Integration
  (VLSI) Systems</comments><doi>10.1109/TVLSI.2014.2359801</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Real-time detection of moving objects involves memorisation of features in
the template image and their comparison with those in the test image. At high
sampling rates, such techniques face the problems of high algorithmic
complexity and component delays. We present a new resistive switching based
threshold logic cell which encodes the pixels of a template image. The cell
comprises a voltage divider circuit that programs the resistances of the
memristors arranged in a single node threshold logic network and the output is
encoded as a binary value using a CMOS inverter gate. When a test image is
applied to the template-programmed cell, a mismatch in the respective pixels is
seen as a change in the output voltage of the cell. The proposed cell when
compared with CMOS equivalent implementation shows improved performance in
area, leakage power, power dissipation and delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1274</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1274</id><created>2014-10-06</created><updated>2015-10-15</updated><authors><author><keyname>Mamat</keyname><forenames>Kritsada</forenames></author><author><keyname>Santipach</keyname><forenames>Wiroonsak</forenames></author></authors><title>On Transmit Beamforming for MISO-OFDM Channels With Finite-Rate Feedback</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Communications</comments><journal-ref>IEEE Transactions on Communications, vol. 63, no. 11, pp. 4202 -
  4213, Nov. 2015</journal-ref><doi>10.1109/TCOMM.2015.2475420</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With finite-rate feedback, we propose two feedback methods for transmit
beamforming in a point-to-point MISO-OFDM channel. For the first method, a
receiver with perfect channel information, quantizes and feeds back the optimal
transmit beamforming vectors of a few selected subcarriers, which are equally
spaced. Based on those quantized vectors, the transmitter applies either
constant, linear, or higher-order interpolation with the remaining beamforming
vectors. With constant interpolation, we derive the approximate sum achievable
rate and the optimal cluster size that maximizes the approximate rate. For
linear interpolation, we derive a closed-form expression for the phase rotation
by utilizing the correlation between OFDM subcarriers. We also propose a
higher-order interpolation that requires more than two quantized vectors to
interpolate transmit beamformers, and is based on existing channel estimation
methods. Numerical results show that interpolation with the optimized cluster
size can perform significantly better than that with an arbitrary cluster size.
For the second proposed method, a channel impulse response is quantized with a
uniform scalar quantizer. With channel quantization, we also derive the
approximate sum achievable rate. We show that switching between the two methods
for different feedback-rate requirements can perform better than the existing
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1282</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1282</id><created>2014-10-06</created><updated>2015-05-21</updated><authors><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Leung</keyname><forenames>Ka-Cheong</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Capacity Estimation for Vehicle-to-Grid Frequency Regulation Services
  with Smart Charging Mechanism</title><categories>cs.SY</categories><comments>11 pages, Accepted for publication in IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to various green initiatives, renewable energy will be massively
incorporated into the future smart grid. However, the intermittency of the
renewables may result in power imbalance, thus adversely affecting the
stability of a power system. Frequency regulation may be used to maintain the
power balance at all times. As electric vehicles (EVs) become popular, they may
be connected to the grid to form a vehicle-to-grid (V2G) system. An aggregation
of EVs can be coordinated to provide frequency regulation services. However,
V2G is a dynamic system where the participating EVs come and go independently.
Thus it is not easy to estimate the regulation capacities for V2G. In a
preliminary study, we modeled an aggregation of EVs with a queueing network,
whose structure allows us to estimate the capacities for regulation-up and
regulation-down, separately. The estimated capacities from the V2G system can
be used for establishing a regulation contract between an aggregator and the
grid operator, and facilitating a new business model for V2G. In this paper, we
extend our previous development by designing a smart charging mechanism which
can adapt to given characteristics of the EVs and make the performance of the
actual system follow the analytical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1289</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1289</id><created>2014-10-06</created><authors><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author><author><keyname>Doshi</keyname><forenames>Jainam</forenames></author><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author></authors><title>Receiver Antenna Partitioning for Simultaneous Wireless Information and
  Power Transfer</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Powering mobiles using microwave \emph{power transfer} (PT) avoids the
inconvenience of battery recharging by cables and ensures uninterrupted mobile
operation. The integration of PT and \emph{information transfer} (IT) allows
wireless PT to be realized by building on the existing infrastructure for IT
and also leads to compact mobile designs. As a result, \emph{simultaneous
wireless information and power transfer} (SWIPT) has emerged to be an active
research topic that is also the theme of this paper. In this paper, a practical
SWIPT system is considered where two multi-antenna stations perform separate PT
and IT to a multi-antenna mobile to accommodate their difference in ranges. The
mobile dynamically assigns each antenna for either PT or IT. The antenna
partitioning results in a tradeoff between the MIMO IT channel capacity and the
PT efficiency. The optimal partitioning for maximizing the IT rate under a PT
constraint is a NP-hard integer program, and the paper proposes solving it via
efficient greedy algorithms with guaranteed performance. To this end, the
antenna-partitioning problem is proved to be one that optimizes a sub-modular
function over a matroid constraint. This structure allows the application of
two well-known greedy algorithms that yield solutions no smaller than the
optimal one scaled by factors $(1-1/e)$ and 1/2, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1292</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1292</id><created>2014-10-06</created><authors><author><keyname>Nagda</keyname><forenames>Rushil</forenames></author><author><keyname>Satpathi</keyname><forenames>Siddharth</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Optimal Offline and Competitive Online Strategies for
  Transmitter-Receiver Energy Harvesting</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmitter-receiver energy harvesting model is assumed, where both the
transmitter and receiver are powered by random energy source. Given a fixed
number of bits, the problem is to find the optimal transmission power profile
at the transmitter and ON-OFF profile at the receiver to minimize the
transmission time. Structure of the optimal offline strategy is derived
together with an optimal offline policy. An online policy with competitive
ratio of strictly less than two is also derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1309</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1309</id><created>2014-10-06</created><authors><author><keyname>Balliu</keyname><forenames>Alkida</forenames></author><author><keyname>Olivetti</keyname><forenames>Dennis</forenames></author><author><keyname>Babaoglu</keyname><forenames>Ozalp</forenames></author><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author><author><keyname>S&#xee;rbu</keyname><forenames>Alina</forenames></author></authors><title>BiDAl: Big Data Analyzer for Cluster Traces</title><categories>cs.DC</categories><comments>published in E. Pl\&quot;odereder, L. Grunske, E. Schneider, D. Ull
  (editors), proc. INFORMATIK 2014 Workshop on System Software Support for Big
  Data (BigSys 2014), September 25--26 2014, Stuttgart, Germany, Lecture Notes
  in Informatics (LNI) Proceedings, Series of the Gesellschaft f\&quot;ur Informatik
  (GI), Volume P-232, pp. 1781--1795, ISBN 978-3-88579-626-8, ISSN 1617-5468</comments><journal-ref>proc. INFORMATIK 2014 Workshop on System Software Support for Big
  Data (BigSys 2014), Lecture Notes in Informatics (LNI), Volume P-232, pp.
  1781-1795, ISBN 78-3-88579-626-8, ISSN 1617-5468</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern data centers that provide Internet-scale services are stadium-size
structures housing tens of thousands of heterogeneous devices (server clusters,
networking equipment, power and cooling infrastructures) that must operate
continuously and reliably. As part of their operation, these devices produce
large amounts of data in the form of event and error logs that are essential
not only for identifying problems but also for improving data center efficiency
and management. These activities employ data analytics and often exploit hidden
statistical patterns and correlations among different factors present in the
data. Uncovering these patterns and correlations is challenging due to the
sheer volume of data to be analyzed. This paper presents BiDAl, a prototype
&quot;log-data analysis framework&quot; that incorporates various Big Data technologies
to simplify the analysis of data traces from large clusters. BiDAl is written
in Java with a modular and extensible architecture so that different storage
backends (currently, HDFS and SQLite are supported), as well as different
analysis languages (current implementation supports SQL, R and Hadoop
MapReduce) can be easily selected as appropriate. We present the design of
BiDAl and describe our experience using it to analyze several public traces of
Google data clusters for building a simulation model capable of reproducing
observed behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1314</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1314</id><created>2014-10-06</created><updated>2014-12-22</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author></authors><title>The emergence of hyper-altruistic behaviour in conflictual situations</title><categories>q-bio.PE cs.GT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Situations where people have to decide between hurting themselves or another
person are at the core of many individual and global conflicts. Yet little is
known about how people behave when facing these situations in the lab. Here we
report a large experiment in which participants could either take $x$ dollars
from another anonymous participant or give $y$ dollars to the same participant.
Depending on the treatments, participants could also exit the game without
making any decision, but paying a cost. Across different protocols and
parameter specifications, we provide evidence of three regularities: (i) when
exiting is allowed and costless, subjects tend to exit the game; (ii) females
are more likely than males to exit the game, but only when the cost is small;
(iii) when exiting is not allowed, altruistic actions are more common than
predicted by the dominant economic models. In particular, against the
predictions of every dominant economic model, about one sixth of the subjects
show hyper-altruistic tendencies, that is, they prefer giving $y$ rather than
taking $x&gt;y$. In doing so, our findings shed light on human decision-making in
conflictual situations and suggest that economic models should be revised in
order to take into account hyper-altruistic behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1318</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1318</id><created>2014-10-06</created><updated>2015-09-21</updated><authors><author><keyname>Boyar</keyname><forenames>Joan</forenames></author><author><keyname>Find</keyname><forenames>Magnus Gausdal</forenames></author></authors><title>Constructive Relationships Between Algebraic Thickness and Normality</title><categories>cs.CC</categories><comments>Final version published in FCT'2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relationship between two measures of Boolean functions;
\emph{algebraic thickness} and \emph{normality}. For a function $f$, the
algebraic thickness is a variant of the \emph{sparsity}, the number of nonzero
coefficients in the unique GF(2) polynomial representing $f$, and the normality
is the largest dimension of an affine subspace on which $f$ is constant. We
show that for $0 &lt; \epsilon&lt;2$, any function with algebraic thickness
$n^{3-\epsilon}$ is constant on some affine subspace of dimension
$\Omega\left(n^{\frac{\epsilon}{2}}\right)$. Furthermore, we give an algorithm
for finding such a subspace. We show that this is at most a factor of
$\Theta(\sqrt{n})$ from the best guaranteed, and when restricted to the
technique used, is at most a factor of $\Theta(\sqrt{\log n})$ from the best
guaranteed. We also show that a concrete function, majority, has algebraic
thickness $\Omega\left(2^{n^{1/6}}\right)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1333</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1333</id><created>2014-10-06</created><updated>2015-04-02</updated><authors><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>Rank-metric codes and their duality theory</title><categories>cs.IT math.CO math.IT</categories><comments>new version</comments><msc-class>15A03, 15A99, 15B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare the two duality theories of rank-metric codes proposed by Delsarte
and Gabidulin, proving that the former generalizes the latter. We also give an
elementary proof of MacWilliams identities for the general case of Delsarte
rank-metric codes. The identities which we derive are very easy to handle, and
allow us to re-establish in a very concise way the main results of the theory
of rank-metric codes first proved by Delsarte employing the theory of
association schemes and regular semilattices. We also show that our identities
imply as a corollary the original MacWilliams identities established by
Delsarte. We describe how the minimum and maximum rank of a rank-metric code
relate to the minimum and maximum rank of the dual code, giving some bounds and
characterizing the codes attaining them. Then we study optimal anticodes in the
rank metric, describing them in terms of optimal codes (namely, MRD codes). In
particular, we prove that the dual of an optimal anticode is an optimal
anticode. Finally, as an application of our results to a classical problem in
enumerative combinatorics, we derive both a recursive and an explicit formula
for the number of $k \times m$ matrices over a finite field with given rank and
$h$-trace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1342</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1342</id><created>2014-10-06</created><authors><author><keyname>Abdalla</keyname><forenames>O. M.</forenames></author><author><keyname>Hammad</keyname><forenames>Sherif A.</forenames></author><author><keyname>Yousef</keyname><forenames>H. Ahmed</forenames></author></authors><title>A Framework for Real Time Hardware in the loop Simulation for Control
  Design</title><categories>cs.SY</categories><journal-ref>GCC IEEE Conference, Bahrain, November 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a simple framework of low cost Kit which can be used in
control education and training courses to support hardware in the loop
simulation. The kit shows the student or control engineer the effect of delays,
noise, and saturation on the control system. The framework is generic and
flexible to give the user the ability to test and simulate any controller on
any process. The framework uses Matlab environment which gives the user many
tools to build is/her system in a fast and accurate way. Some test cases are
presented for using the framework on different controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1343</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1343</id><created>2014-10-06</created><authors><author><keyname>Medhat</keyname><forenames>Walaa</forenames></author><author><keyname>Yousef</keyname><forenames>Ahmed Hassan</forenames></author><author><keyname>Mohamed</keyname><forenames>Hoda Korashy</forenames></author></authors><title>Combined Algorithm for Data Mining using Association rules</title><categories>cs.DB</categories><comments>Ain Shams Journal of Electrical Engineering, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Association Rule mining is one of the most important fields in data mining
and knowledge discovery. This paper proposes an algorithm that combines the
simple association rules derived from basic Apriori Algorithm with the multiple
minimum support using maximum constraints. The algorithm is implemented, and is
compared to its predecessor algorithms using a novel proposed comparison
algorithm. Results of applying the proposed algorithm show faster performance
than other algorithms without scarifying the accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1348</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1348</id><created>2014-10-06</created><authors><author><keyname>Tantawy</keyname><forenames>Rasha Y.</forenames></author><author><keyname>Farouk</keyname><forenames>Ziad</forenames></author><author><keyname>Mohamed</keyname><forenames>Shereen</forenames></author><author><keyname>Yousef</keyname><forenames>Ahmed H.</forenames></author></authors><title>Using Professional Social Networking as an Innovative Method for Data
  Extraction: The ICT Alumni Index Case Study</title><categories>cs.CY</categories><comments>1st International Conference on Innovation &amp; Entrepreneurship, Cairo,
  April 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lack of data regarding Information and Communications Technology sector
alumni data is a known problem in several countries including Egypt. It is not
clear what entry and senior jobs are occupied by alumni and which countries
attract them. This affects the planning, design and execution of both the ICT
sector and the Education sector. In this research, a joint team is formulated
from the Technology Innovation and Entrepreneurship Center TIEC and the
Ministry of Higher Education. This team is undertaking extensive analysis of
the structure, distribution and development of the ICT skills and employment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1368</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1368</id><created>2014-10-06</created><updated>2015-02-09</updated><authors><author><keyname>Dr&#xe9;meau</keyname><forenames>Ang&#xe9;lique</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author></authors><title>Phase recovery from a Bayesian point of view: the variational approach</title><categories>cs.IT math.IT math.ST stat.AP stat.TH</categories><comments>To appear in the proceedings of IEEE Int'l Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</comments><journal-ref>Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE
  International Conference on Year: 2015 Pages: 3661- 3665</journal-ref><doi>10.1109/ICASSP.2015.7178654</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the phase recovery problem, where a complex signal
vector has to be estimated from the knowledge of the modulus of its linear
projections, from a naive variational Bayesian point of view. In particular, we
derive an iterative algorithm following the minimization of the
Kullback-Leibler divergence under the mean-field assumption, and show on
synthetic data with random projections that this approach leads to an efficient
and robust procedure, with a good computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1371</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1371</id><created>2014-10-06</created><authors><author><keyname>Ebrahimi</keyname><forenames>Javad B.</forenames></author><author><keyname>Siavoshani</keyname><forenames>Mahdi Jafari</forenames></author></authors><title>Linear Index Coding via Graph Homomorphism</title><categories>cs.IT cs.DM math.IT</categories><comments>10 pages, to appear in the 2nd International Conference on Control,
  Decision and Information Technologies (CoDIT'14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the minimum broadcast rate of a linear index code over
$\mathbb{F}_q$ is equal to the $minrank_q$ of the underlying digraph. In [3] it
is proved that for $\mathbb{F}_2$ and any positive integer $k$,
$minrank_q(G)\leq k$ iff there exists a homomorphism from the complement of the
graph $G$ to the complement of a particular undirected graph family called
&quot;graph family $\{G_k\}$&quot;. As observed in [2], by combining these two results
one can relate the linear index coding problem of undirected graphs to the
graph homomorphism problem. In [4], a direct connection between linear index
coding problem and graph homomorphism problem is introduced. In contrast to the
former approach, the direct connection holds for digraphs as well and applies
to any field size. More precisely, in [4], a graph family $\{H_k^q\}$ is
introduced and shown that whether or not the scalar linear index of a digraph
$G$ is less than or equal to $k$ is equivalent to the existence of a graph
homomorphism from the complement of $G$ to the complement of $H_k^q$.
  Here, we first study the structure of the digraphs $H_k^q$. Analogous to the
result of [2] about undirected graphs, we prove that $H_k^q$'s are vertex
transitive digraphs. Using this, and by applying a lemma of Hell and Nesetril
[5], we derive a class of necessary conditions for digraphs $G$ to satisfy
$lind_q(G)\leq k$. Particularly, we obtain new lower bounds on $lind_q(G)$.
  Our next result is about the computational complexity of scalar linear index
of a digraph. It is known that deciding whether the scalar linear index of an
undirected graph is equal to $k$ or not is NP-complete for $k\ge 3$ and is
polynomially decidable for $k=1,2$ [3]. For digraphs, it is shown in [6] that
for the binary alphabet, the decision problem for $k=2$ is NP-complete. We use
graph homomorphism framework to extend this result to arbitrary alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1382</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1382</id><created>2014-10-06</created><authors><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Ting</keyname><forenames>Pangan</forenames></author></authors><title>Performance Limits of Massive MIMO Systems Based on Bayes-Optimal
  Inference</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE ICC 2015 (slightly extended version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a replica analysis for the minimum mean square error (MSE)
of a massive multiple-input multiple-output (MIMO) system by using Bayesian
inference. The Bayes-optimal estimator is adopted to estimate the data symbols
and the channels from a block of received signals in the spatial-temporal
domain. We show that using the Bayes-optimal estimator, the interfering signals
from adjacent cells can be separated from the received signals without pilot
information. In addition, the MSEs with respect to the data symbols and the
channels of the desired users decrease with the number of receive antennas and
the number of data symbols, respectively. There are no residual interference
terms that remain bounded away from zero as the numbers of receive antennas and
data symbols approach infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1387</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1387</id><created>2014-10-02</created><authors><author><keyname>Medina</keyname><forenames>David S.</forenames></author><author><keyname>St-Cyr</keyname><forenames>Amik</forenames></author><author><keyname>Warburton</keyname><forenames>Timothy</forenames></author></authors><title>High-Order Finite-differences on multi-threaded architectures using OCCA</title><categories>math.NA cs.MS cs.NA</categories><comments>ICOSAHOM 2014 conference paper, 9 pages, 2 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-order finite-difference methods are commonly used in wave propagators
for industrial subsurface imaging algorithms. Computational aspects of the
reduced linear elastic vertical transversely isotropic propagator are
considered. Thread parallel algorithms suitable for implementing this
propagator on multi-core and many-core processing devices are introduced.
Portability is addressed through the use of the \OCCA runtime programming
interface. Finally, performance results are shown for various architectures on
a representative synthetic test case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1389</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1389</id><created>2014-10-06</created><authors><author><keyname>Premnath</keyname><forenames>Sriram N.</forenames></author><author><keyname>Haas</keyname><forenames>Zygmunt J.</forenames></author></authors><title>A Practical, Secure, and Verifiable Cloud Computing for Mobile Systems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing systems, in which clients rent and share computing resources
of third party platforms, have gained widespread use in recent years.
Furthermore, cloud computing for mobile systems (i.e., systems in which the
clients are mobile devices) have too been receiving considerable attention in
technical literature. We propose a new method of delegating computations of
resource-constrained mobile clients, in which multiple servers interact to
construct an encrypted program known as garbled circuit. Next, using garbled
inputs from a mobile client, another server executes this garbled circuit and
returns the resulting garbled outputs. Our system assures privacy of the mobile
client's data, even if the executing server chooses to collude with all but one
of the other servers. We adapt the garbled circuit design of Beaver et al. and
the secure multiparty computation protocol of Goldreich et al. for the purpose
of building a secure cloud computing for mobile systems. Our method
incorporates the novel use of the cryptographically secure pseudo random number
generator of Blum et al. that enables the mobile client to efficiently retrieve
the result of the computation, as well as to verify that the evaluator actually
performed the computation. We analyze the server-side and client-side
complexity of our system. Using real-world data, we evaluate our system for a
privacy preserving search application that locates the nearest bank/ATM from
the mobile client. We also measure the time taken to construct and evaluate the
garbled circuit for varying number of servers, demonstrating the feasibility of
our secure and verifiable cloud computing for mobile systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1393</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1393</id><created>2014-09-28</created><updated>2015-05-11</updated><authors><author><keyname>Frank</keyname><forenames>Morgan R.</forenames></author><author><keyname>Williams</keyname><forenames>Jake Ryland</forenames></author><author><keyname>Mitchell</keyname><forenames>Lewis</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author></authors><title>Constructing a taxonomy of fine-grained human movement and activity
  motifs through social media</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Profiting from the emergence of web-scale social data sets, numerous recent
studies have systematically explored human mobility patterns over large
populations and large time scales. Relatively little attention, however, has
been paid to mobility and activity over smaller time-scales, such as a day.
Here, we use Twitter to identify people's frequently visited locations along
with their likely activities as a function of time of day and day of week,
capitalizing on both the content and geolocation of messages. We subsequently
characterize people's transition pattern motifs and demonstrate that spatial
information is encoded in word choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1409</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1409</id><created>2014-09-25</created><authors><author><keyname>Aardal</keyname><forenames>Karen</forenames></author><author><keyname>Bodic</keyname><forenames>Pierre Le</forenames></author></authors><title>Approximation algorithms for the Transportation Problem with Market
  Choice and related models</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given facilities with capacities and clients with penalties and demands, the
transportation problem with market choice consists in finding the minimum-cost
way to partition the clients into unserved clients, paying the penalties, and
into served clients, paying the transportation cost to serve them. We give
polynomial-time reductions from this problem and variants to the
(un)capacitated facility location problem, directly yielding approximation
algorithms, two with constant factors in the metric case, one with a
logarithmic factor in the general case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1434</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1434</id><created>2014-10-06</created><updated>2015-04-26</updated><authors><author><keyname>Kaplan</keyname><forenames>Marc</forenames></author></authors><title>Quantum attacks against iterated block ciphers</title><categories>quant-ph cs.CR</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the amplification of security against quantum attacks provided by
iteration of block ciphers. In the classical case, the Meet-in-the-middle
attack is a generic attack against those constructions. This attack reduces the
time required to break double iterations to only twice the time it takes to
attack a single block cipher, given that the attacker has access to a large
amount of memory. More abstractly, it shows that security by composition does
not achieve exact multiplicative amplification. We present a quantized version
of this attack based on an optimal quantum algorithm for the Element
Distinctness problem. We then use the generalized adversary method to prove the
optimality of the attack. An interesting corollary is that the time-space
tradeoff for quantum attacks is very different from what classical attacks
allow. This first result seems to indicate that composition resists better to
quantum attacks than to classical ones because it prevents the quadratic
speedup achieved by quantizing an exhaustive search.
  We investigate security amplification by composition further by examining the
case of four iterations. We quantize a recent technique called the dissection
attack using the framework of quantum walks. Surprisingly, this leads to better
gains over classical attacks than for double iterations, which seems to
indicate that when the number of iterations grows, the resistance against
quantum attacks decreases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1441</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1441</id><created>2014-10-06</created><updated>2015-10-15</updated><authors><author><keyname>Seshadreesan</keyname><forenames>Kaushik P.</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Fidelity of recovery, geometric squashed entanglement, and measurement
  recoverability</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>v8: 45 pages, 2 figures, final version to appear in Physical Review A</comments><journal-ref>Physical Review A vol. 92, page 042321, October 2015</journal-ref><doi>10.1103/PhysRevA.92.042321</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines the fidelity of recovery of a quantum state on systems
$A$, $B$, and $C$ as a measure of how well one can recover the full state on
all three systems if system $A$ is lost and a recovery operation is performed
on system $C$ alone. The surprisal of the fidelity of recovery (its negative
logarithm) is an information quantity which obeys nearly all of the properties
of the conditional quantum mutual information $I(A;B|C)$, including
non-negativity, monotonicity with respect to local operations, duality,
invariance with respect to local isometries, a dimension bound, and continuity.
We then define a (pseudo) entanglement measure based on this quantity, which we
call the geometric squashed entanglement. We prove that the geometric squashed
entanglement is a 1-LOCC monotone, that it vanishes if and only if the state on
which it is evaluated is unentangled, and that it reduces to the geometric
measure of entanglement if the state is pure. We also show that it is invariant
with respect to local isometries, subadditive, continuous, and normalized on
maximally entangled states. We next define the surprisal of measurement
recoverability, which is an information quantity in the spirit of quantum
discord, characterizing how well one can recover a share of a bipartite state
if it is measured. We prove that this discord-like quantity satisfies several
properties, including non-negativity, faithfulness on classical-quantum states,
invariance with respect to local isometries, a dimension bound, and
normalization on maximally entangled states. This quantity combined with a
recent breakthrough of Fawzi and Renner allows to characterize states with
discord nearly equal to zero as being approximate fixed points of entanglement
breaking channels. Finally, we discuss a multipartite fidelity of recovery and
several of its properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1443</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1443</id><created>2014-10-06</created><updated>2015-09-18</updated><authors><author><keyname>Seshadreesan</keyname><forenames>Kaushik P.</forenames></author><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>R\'enyi squashed entanglement, discord, and relative entropy differences</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>v3: 41 pages, 2 tables, final version</comments><journal-ref>Journal of Physics A: Mathematical and Theoretical vol. 48,
  article no. 395303, September 2015</journal-ref><doi>10.1088/1751-8113/48/39/395303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [Berta et al., J. Math. Phys. 56, 022205 (2015)], we recently proposed
Renyi generalizations of the conditional quantum mutual information of a
tripartite state on $ABC$ (with $C$ being the conditioning system), which were
shown to satisfy some properties that hold for the original quantity, such as
non-negativity, duality, and monotonicity with respect to local operations on
the system $B$ (with it being left open to show that the Renyi quantity is
monotone with respect to local operations on system $A$). Here we define a
Renyi squashed entanglement and a Renyi quantum discord based on a Renyi
conditional quantum mutual information and investigate these quantities in
detail. Taking as a conjecture that the Renyi conditional quantum mutual
information is monotone with respect to local operations on both systems $A$
and $B$, we prove that the Renyi squashed entanglement and the Renyi quantum
discord satisfy many of the properties of the respective original von Neumann
entropy based quantities. In our prior work [Berta et al., Phys. Rev. A 91,
022333 (2015)], we also detailed a procedure to obtain Renyi generalizations of
any quantum information measure that is equal to a linear combination of von
Neumann entropies with coefficients chosen from the set $\{-1,0,1\}$. Here, we
extend this procedure to include differences of relative entropies. Using the
extended procedure and a conjectured monotonicity of the Renyi generalizations
in the Renyi parameter, we discuss potential remainder terms for well known
inequalities such as monotonicity of the relative entropy, joint convexity of
the relative entropy, and the Holevo bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1462</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1462</id><created>2014-10-06</created><authors><author><keyname>Li</keyname><forenames>Nan</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Top Rank Optimization in Linear Time</title><categories>cs.LG cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bipartite ranking aims to learn a real-valued ranking function that orders
positive instances before negative instances. Recent efforts of bipartite
ranking are focused on optimizing ranking accuracy at the top of the ranked
list. Most existing approaches are either to optimize task specific metrics or
to extend the ranking loss by emphasizing more on the error associated with the
top ranked instances, leading to a high computational cost that is super-linear
in the number of training instances. We propose a highly efficient approach,
titled TopPush, for optimizing accuracy at the top that has computational
complexity linear in the number of training instances. We present a novel
analysis that bounds the generalization error for the top ranked instances for
the proposed approach. Empirical study shows that the proposed approach is
highly competitive to the state-of-the-art approaches and is 10-100 times
faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1465</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1465</id><created>2014-10-06</created><updated>2015-10-19</updated><authors><author><keyname>Barrau</keyname><forenames>Axel</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author></authors><title>The invariant extended Kalman filter as a stable observer</title><categories>cs.SY</categories><comments>This paper is going to be submitted for publication in IEEE
  Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the convergence aspects of the invariant extended Kalman filter
(IEKF), when the latter is used as a deterministic non-linear observer on Lie
groups, for continuous-time systems with discrete observations. One of the main
features of invariant observers for left-invariant systems on Lie groups is
that the estimation error is autonomous. In this paper we first generalize this
result by characterizing the (much broader) class of systems for which this
property holds. Then, we leverage the result to prove for those systems the
local stability of the IEKF around any trajectory, under the standard
conditions of the linear case. One mobile robotics example and one inertial
navigation example illustrate the interest of the approach. Simulations
evidence the fact that the EKF is capable of diverging in some challenging
situations, where the IEKF with identical tuning keeps converging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1474</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1474</id><created>2014-10-06</created><authors><author><keyname>Afrin</keyname><forenames>Farzana</forenames></author><author><keyname>Rahaman</keyname><forenames>Mohammad Saiedur</forenames></author></authors><title>An adaptive quasi harmonic broadcasting scheme with optimal bandwidth
  requirement</title><categories>cs.MM cs.NI</categories><comments>IEEE International Conference on Informatics, Electronics &amp; Vision
  (ICIEV), 2013, 6pages, 8 figures</comments><doi>10.1109/ICIEV.2013.6572684</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of Harmonic Broadcasting protocol is to reduce the bandwidth usage in
video-on-demand service where a video is divided into some equal sized segments
and every segment is repeatedly transmitted over a number of channels that
follows harmonic series for channel bandwidth assignment. As the bandwidth of
channels differs from each other and users can join at any time to these
multicast channels, they may experience a synchronization problem between
download and playback. To deal with this issue, some schemes have been
proposed, however, at the cost of additional or wastage of bandwidth or sudden
extreme bandwidth requirement. In this paper we present an adaptive quasi
harmonic broadcasting scheme (AQHB) which delivers all data segment on time
that is the download and playback synchronization problem is eliminated while
keeping the bandwidth consumption as same as traditional harmonic broadcasting
scheme without cost of any additional or wastage of bandwidth. It also ensures
the video server not to increase the channel bandwidth suddenly that is, also
eliminates the sudden buffer requirement at the client side. We present several
analytical results to exhibit the efficiency of our proposed broadcasting
scheme over the existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1478</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1478</id><created>2014-10-06</created><authors><author><keyname>Syropoulos</keyname><forenames>Apostolos</forenames></author></authors><title>Fuzzy Categories</title><categories>cs.LO</categories><journal-ref>Published in Critical Review, a Publication of Society for
  Mathematics of Uncertainty, Volume VII, pp 24-29, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since categories are graphs with additional &quot;structure&quot;, one should start
from fuzzy graphs in order to define a theory of fuzzy categories. Thus is
makes sense to introduce categories whose morphisms are associated with a
plausibility degree that determines to what extend it is possible to &quot;go&quot; from
one object to another one. These categories are called {\em fuzzy categories}.
Of course, the basic properties of these categories are similar but not
identical to their ordinary counterparts. Thus, it is necessary to introduce
notion like fuzzy commutative diagrams, fuzzy initial and fuzzy terminal
objects, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1490</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1490</id><created>2014-10-06</created><updated>2015-01-03</updated><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Komanduri</keyname><forenames>Saranga</forenames></author><author><keyname>Cranor</keyname><forenames>Lorrie</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author></authors><title>Spaced Repetition and Mnemonics Enable Recall of Multiple Strong
  Passwords</title><categories>cs.CR cs.HC</categories><doi>10.14722/ndss.2015.23094</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on a user study that provides evidence that spaced repetition and a
specific mnemonic technique enable users to successfully recall multiple strong
passwords over time. Remote research participants were asked to memorize 4
Person-Action-Object (PAO) stories where they chose a famous person from a
drop-down list and were given machine-generated random action-object pairs.
Users were also shown a photo of a scene and asked to imagine the PAO story
taking place in the scene (e.g., Bill Gates---swallowing---bike on a beach).
Subsequently, they were asked to recall the action-object pairs when prompted
with the associated scene-person pairs following a spaced repetition schedule
over a period of 127+ days. While we evaluated several spaced repetition
schedules, the best results were obtained when users initially returned after
12 hours and then in $1.5\times$ increasing intervals: 77% of the participants
successfully recalled all 4 stories in 10 tests over a period of 158 days. Much
of the forgetting happened in the first test period (12 hours): 89% of
participants who remembered their stories during the first test period
successfully remembered them in every subsequent round. These findings, coupled
with recent results on naturally rehearsing password schemes, suggest that 4
PAO stories could be used to create usable and strong passwords for 14
sensitive accounts following this spaced repetition schedule, possibly with a
few extra upfront rehearsals. In addition, we find that there is an
interference effect across multiple PAO stories: the recall rate of 100% (resp.
90%) for participants who were asked to memorize 1 PAO story (resp. 2 PAO
stories) is significantly better than the recall rate for participants who were
asked to memorize 4 PAO stories. These findings yield concrete advice for
improving constructions of password management schemes and future user studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1579</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1579</id><created>2014-10-06</created><updated>2015-09-20</updated><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Schulz</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>Counting Carambolas</title><categories>math.CO cs.DM</categories><comments>update reflects journal version, to appear in Graphs and
  Combinatorics; 18 pages, 13 figures</comments><doi>10.1007/s00373-015-1621-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give upper and lower bounds on the maximum and minimum number of geometric
configurations of various kinds present (as subgraphs) in a triangulation of
$n$ points in the plane. Configurations of interest include \emph{convex
polygons}, \emph{star-shaped polygons} and \emph{monotone paths}. We also
consider related problems for \emph{directed} planar straight-line graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1593</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1593</id><created>2014-10-06</created><updated>2014-10-21</updated><authors><author><keyname>Davaslioglu</keyname><forenames>Kemal</forenames></author><author><keyname>Coskun</keyname><forenames>Cemil Can</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Energy-Efficient Resource Allocation for Fractional Frequency Reuse in
  Heterogeneous Networks</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the authors. Due to the crucial
  errors in simulation results, we would like to withdraw our manuscript. There
  is no replacement available</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next generation wireless networks face the challenge of increasing energy
consumption while satisfying the unprecedented increase in the data rate
demand. To address this problem, we propose a utility-based energy-efficient
resource allocation algorithm for the downlink transmissions in heterogeneous
networks (HetNets). We consider the fractional frequency reuse (FFR) method in
order to mitigate the intra- and inter-cell interference. The proposed
algorithm divides the resource allocation problem into frequency and power
assignment problems and sequentially solves them. The proposed power control
algorithm uses the gradient ascent method to control the transmit power of
macrocell base stations as most of the power in the network is consumed there.
We also present the optimality conditions of the resource allocation problem
and the convergence of the proposed algorithm. We study the performance of the
proposed algorithm in a Long Term Evolution (LTE) system. Our simulation
results demonstrate that the proposed algorithm provides substantial
improvements in the energy efficiency and throughput of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1594</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1594</id><created>2014-10-06</created><updated>2015-01-15</updated><authors><author><keyname>Winkler</keyname><forenames>Marco</forenames></author><author><keyname>Reichardt</keyname><forenames>Joerg</forenames></author></authors><title>Node-Specific Triad Pattern Mining for Complex-Network Analysis</title><categories>cs.SI cs.DS physics.data-an physics.soc-ph</categories><comments>Published in IEEE ICDMW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mining of graphs in terms of their local substructure is a
well-established methodology to analyze networks. It was hypothesized that
motifs - subgraph patterns which appear significantly more often than expected
at random - play a key role for the ability of a system to perform its task.
Yet the framework commonly used for motif-detection averages over the local
environments of all nodes. Therefore, it remains unclear whether motifs are
overrepresented in the whole system or only in certain regions.
  In this contribution, we overcome this limitation by mining node-specific
triad patterns. For every vertex, the abundance of each triad pattern is
considered only in triads it participates in. We investigate systems of various
fields and find that motifs are distributed highly heterogeneously. In
particular we focus on the feed-forward loop motif which has been alleged to
play a key role in biological networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1599</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1599</id><created>2014-10-06</created><authors><author><keyname>Kouya</keyname><forenames>Tomonori</forenames></author></authors><title>Accelerated Multiple Precision Matrix Multiplication using Strassen's
  Algorithm and Winograd's Variant</title><categories>math.NA cs.NA</categories><comments>JSIAM Letters (Accepted)</comments><msc-class>15A09</msc-class><acm-class>G.1.3; G.1.0</acm-class><journal-ref>JSIAM Letters, Vol. 6 (2014) p. 81-84</journal-ref><doi>10.14495/jsiaml.6.81</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Strassen algorithm and Winograd's variant accelerate matrix
multiplication by using fewer arithmetic operations than standard matrix
multiplication. Although many papers have been published to accelerate single-
as well as double-precision matrix multiplication by using these algorithms, no
research to date has been undertaken to accelerate multiple precision matrix
multiplication. In this paper, we propose a multiple precision matrix
multiplication program for matrices of any size and test its performance. We
also reveal special properties of our program through its application to LU
decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1606</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1606</id><created>2014-10-06</created><updated>2015-04-01</updated><authors><author><keyname>Xiang</keyname><forenames>Xiang</forenames></author><author><keyname>Dao</keyname><forenames>Minh</forenames></author><author><keyname>Hager</keyname><forenames>Gregory D.</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Hierarchical Sparse and Collaborative Low-Rank Representation for
  Emotion Recognition</title><categories>cs.CV</categories><comments>5 pages, 5 figures; accepted to IEEE ICASSP 2015; programs available
  at https://github.com/eglxiang/icassp15_emotion/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design a Collaborative-Hierarchical Sparse and Low-Rank
(C-HiSLR) model that is natural for recognizing human emotion in visual data.
Previous attempts require explicit expression components, which are often
unavailable and difficult to recover. Instead, our model exploits the lowrank
property over expressive facial frames and rescue inexact sparse
representations by incorporating group sparsity. For the CK+ dataset, C-HiSLR
on raw expressive faces performs as competitive as the Sparse Representation
based Classification (SRC) applied on manually prepared emotions. C-HiSLR
performs even better than SRC in terms of true positive rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1613</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1613</id><created>2014-10-07</created><authors><author><keyname>Shila</keyname><forenames>Devu Manikantan</forenames></author><author><keyname>Cao</keyname><forenames>Xianghui</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Yang</keyname><forenames>Zequ</forenames></author><author><keyname>Zhou</keyname><forenames>Yang</forenames></author><author><keyname>Chen</keyname><forenames>Jiming</forenames></author></authors><title>Ghost-in-the-Wireless: Energy Depletion Attack on ZigBee</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ZigBee has been recently drawing a lot of attention as a promising solution
for ubiquitous computing. The ZigBee devices are normally resource-limited,
making the network susceptible to a variety of security threats. This paper
presents a severe attack on ZigBee networks termed as ghost, which leverages
the underlying vulnerabilities of the IEEE 802.15.4 security suites to deplete
the energy of the devices. We manifest that the impact of ghost is severe as it
can reduce the lifetime of devices from years to days and facilitate a variety
of threats including denial of service and replay attacks. We highlight that
merely deploying a standard suite of advanced security techniques does not
necessarily guarantee improved security, but instead might be leveraged by
adversaries to cause severe disruption in the network. We propose several
recommendations on how to localize and withstand the ghost and other related
attacks in ZigBee networks. Extensive simulations are provided to show the
impact of the ghost and the performance of the proposed recommendations.
Moreover, physical experiments also have been conducted and the observations
confirm the severity of the impact by the ghost attack. We believe that the
presented work will aid the researchers to improve the security of ZigBee
further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1625</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1625</id><created>2014-10-07</created><authors><author><keyname>Elango</keyname><forenames>Bakthavachalam</forenames></author><author><keyname>Rajendran</keyname><forenames>Periyaswamy</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>A macro level scientometric analysis of world tribology research output
  (1998 - 2012)</title><categories>cs.DL</categories><comments>Submitted to the Malaysian Journal of Library and Information Science</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Bibliographic records related to tribology research were extracted from
SCOPUS and Web of Science databases for the period of 15 years from 1998 to
2012. Macro-level scientometric indicators such as growth rate, share of
international collaborative papers, citations per paper, and share of non-cited
papers were employed. Further, the Gini coefficient and Simpson Index of
Diversity were used. Two new relative indicators : Relative International
Collaboration Rate (RICR) and Relative Growth Index (RGI) are proposed in this
study. The performance of top countries contributing more than 1000 papers
across the study period was discussed. Contributions and share of continents
and countries by income groups were examined. Further research contributions
and citation impact of selected country groups such as the Developing Eight
Countries (D8), the Association of Southeast Asian Nations (ASEAN), the Union
of South American Nations (UNASUR) and the Emerging and Growth-Leading
Economies (EAGLEs) countries were analyzed. High levels of interdisciplinarity
exist in tribology research. Inequality of distribution between countries is
highest for number of publications and citations. Asia outperforms the other
world regions and China contributes most of the papers (25%), while the United
States receives most of the citations (22%). 84% of total output was
contributed by the Asiatic region, Western Europe and North America together.
Publications from these three world regions received 88% of total citations.
Around 50% of global research output was contributed by China, the United
States and Japan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1627</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1627</id><created>2014-10-07</created><authors><author><keyname>Liu</keyname><forenames>Zihui</forenames></author><author><keyname>Zumbr&#xe4;gel</keyname><forenames>Jens</forenames></author><author><keyname>Greferath</keyname><forenames>Marcus</forenames></author><author><keyname>Wu</keyname><forenames>Xin-Wen</forenames></author></authors><title>New Results on the Pseudoredundancy</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concepts of pseudocodeword and pseudoweight play a fundamental role in
the finite-length analysis of LDPC codes. The pseudoredundancy of a binary
linear code is defined as the minimum number of rows in a parity-check matrix
such that the corresponding minimum pseudoweight equals its minimum Hamming
distance. By using the value assignment of Chen and Kl{\o}ve we present new
results on the pseudocodeword redundancy of binary linear codes. In particular,
we give several upper bounds on the pseudoredundancies of certain codes with
repeated and added coordinates and of certain shortened subcodes. We also
investigate several kinds of k-dimensional binary codes and compute their exact
pseudocodeword redundancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1639</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1639</id><created>2014-10-07</created><authors><author><keyname>Jiang</keyname><forenames>Yichen</forenames></author><author><keyname>Ji</keyname><forenames>Yi</forenames></author><author><keyname>Liu</keyname><forenames>Tianhua</forenames></author></authors><title>An Anonymous Communication Scheme based on Ring Signature in VANETs</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular ad hoc networks allow vehicles to connect themselves as networks so
that cars could communicate with each other. This paper introduces an anonymous
communication scheme providing integrity protection, multi-level privacy and
auditability. The scheme is based on a certificateless ring signature proposed
in this paper, which is contributed to reduce the length of the signature and
simplify the key management. In our scheme, vehicles can compose the anonymous
group without the help of road-side infrastructure or central authority. The
computation overhead is close to a normal signature scheme, so it is efficient
in most application scenarios. We also present a small-scale implementation to
show the availability of the prototype system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1648</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1648</id><created>2014-10-07</created><updated>2015-07-29</updated><authors><author><keyname>Zhang</keyname><forenames>Shengkai</forenames></author><author><keyname>Hui</keyname><forenames>Pan</forenames></author></authors><title>A Survey on Mobile Affective Computing</title><categories>cs.HC</categories><comments>This paper has been withdrawn by the author: Shengkai Zhang! This
  paper was forced to be submitted by the second author: Pan Hui. The language
  and logic are unacceptable. The methodology is completely wrong! There are
  plenty of errors and mistakes. Completely no novelty, no contribution!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey presents recent progress on Affective Computing (AC) using mobile
devices. AC has been one of the most active research topics for decades. The
primary limitation of traditional AC research refers to as impermeable
emotions. This criticism is prominent when emotions are investigated outside
social contexts. It is problematic because some emotions are directed at other
people and arise from interactions with them. The development of smart mobile
wearable devices (e.g., Apple Watch, Google Glass, iPhone, Fitbit) enables the
wild and natural study for AC in the aspect of computer science. This survey
emphasizes the AC study and system using smart wearable devices. Various
models, methodologies and systems are discussed in order to examine the state
of the art. Finally, we discuss remaining challenges and future works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1653</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1653</id><created>2014-10-07</created><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Kisfaludi-Bak</keyname><forenames>S&#xe1;ndor</forenames></author></authors><title>Notes on dual-critical graphs</title><categories>cs.DS cs.CC math.CO</categories><comments>10 pages, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define dual-critical graphs as graphs having an acyclic orientation, where
the indegrees are odd except for the unique source. We have very limited
knowledge about the complexity of dual-criticality testing. By the definition
the problem is in NP, and a result of Bal\'azs and Christian Szegedy provides a
randomized polynomial algorithm, which relies on formal matrix rank computing.
It is unknown whether dual-criticality test can be done in deterministic
polynomial time. Moreover, the question of being in co-NP is also open.
  We give equivalent descriptions for dual-critical graphs in the general case,
and further equivalent descriptions in the special cases of planar graphs and
3-regular graphs. These descriptions provide polynomial algorithms for these
special classes. We also give an FPT algorithm for a relaxed version of
dual-criticality called $k$-dual-criticality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1668</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1668</id><created>2014-10-07</created><updated>2014-10-14</updated><authors><author><keyname>Feng</keyname><forenames>Ling</forenames></author><author><keyname>Hu</keyname><forenames>Yanqing</forenames></author><author><keyname>Li</keyname><forenames>Baowen</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Braunstein</keyname><forenames>Lidia A.</forenames></author></authors><title>Competing for Attention in Social Media under Information Overload
  Conditions</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 5 figures</comments><journal-ref>PLoS ONE 10(7): e0126090 (2015)</journal-ref><doi>10.1371/journal.pone.0126090</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the many forms of modern social media have become major channels for
the dissemination of information, they are becoming overloaded because of the
rapidly-expanding number of information feeds. We analyze the expanding
user-generated content in Sina Weibo, the largest micro-blog site in China, and
find evidence that popular messages often follow a mechanism that differs from
that found in the spread of disease, in contrast to common believe. In this
mechanism, an individual with more friends needs more repeated exposures to
spread further the information. Moreover, our data suggest that in contrast to
epidemics, for certain messages the chance of an individual to share the
message is proportional to the fraction of its neighbours who shared it with
him/her. Thus the greater the number of friends an individual has the greater
the number of repeated contacts needed to spread the message, which is a result
of competition for attention. We model this process using a fractional
susceptible infected recovered (FSIR) model, where the infection probability of
a node is proportional to its fraction of infected neighbors. Our findings have
dramatic implications for information contagion. For example, using the FSIR
model we find that real-world social networks have a finite epidemic threshold.
This is in contrast to the zero threshold that conventional wisdom derives from
disease epidemic models. This means that when individuals are overloaded with
excess information feeds, the information either reaches out the population if
it is above the critical epidemic threshold, or it would never be well
received, leading to only a handful of information contents that can be widely
spread throughout the population.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1681</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1681</id><created>2014-10-07</created><authors><author><keyname>Rakotoarivelo</keyname><forenames>Thierry</forenames></author><author><keyname>Jourjon</keyname><forenames>Guillaume</forenames></author><author><keyname>Mehani</keyname><forenames>Olivier</forenames></author><author><keyname>Ott</keyname><forenames>Maximilian</forenames></author><author><keyname>Zink</keyname><forenames>Mike</forenames></author></authors><title>Repeatable Experiments with LabWiki</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to repeat the experiments from a research study and obtain
similar results is a corner stone in experiment-based scientific discovery.
This essential feature has been often ignored by the distributed computing and
networking community. There are many reasons for that, such as the complexity
of provisioning, configuring, and orchestrating the resources used by
experiments, their multiple external dependencies, and the difficulty to
seamlessly record these dependencies. This paper describes a methodology based
on well-established principles to plan, prepare and execute experiments. We
propose and describe a family of tools, the LabWiki workspace, to support an
experimenter's workflow based on that methodology. This proposed workspace
provides services and mechanisms for each step of an experiment-based study,
while automatically capturing the necessary information to allow others to
repeat, inspect, validate and modify prior experiments. Our LabWiki workspace
builds on existing contributions, and de-facto protocol and model standards,
which emerged from recent experimental facility initiatives. We use a real
experiment as a thread to guide and illustrate the discussion throughout this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1699</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1699</id><created>2014-10-07</created><authors><author><keyname>Weinmann</keyname><forenames>Andreas</forenames></author><author><keyname>Demaret</keyname><forenames>Laurent</forenames></author><author><keyname>Storath</keyname><forenames>Martin</forenames></author></authors><title>Mumford-Shah and Potts Regularization for Manifold-Valued Data with
  Applications to DTI and Q-Ball Imaging</title><categories>math.NA cs.CV math.OC physics.med-ph</categories><doi>10.1007/s10851-015-0628-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mumford-Shah and Potts functionals are powerful variational models for
regularization which are widely used in signal and image processing; typical
applications are edge-preserving denoising and segmentation. Being both
non-smooth and non-convex, they are computationally challenging even for scalar
data. For manifold-valued data, the problem becomes even more involved since
typical features of vector spaces are not available. In this paper, we propose
algorithms for Mumford-Shah and for Potts regularization of manifold-valued
signals and images. For the univariate problems, we derive solvers based on
dynamic programming combined with (convex) optimization techniques for
manifold-valued data. For the class of Cartan-Hadamard manifolds (which
includes the data space in diffusion tensor imaging), we show that our
algorithms compute global minimizers for any starting point. For the
multivariate Mumford-Shah and Potts problems (for image regularization) we
propose a splitting into suitable subproblems which we can solve exactly using
the techniques developed for the corresponding univariate problems. Our method
does not require any a priori restrictions on the edge set and we do not have
to discretize the data space. We apply our method to diffusion tensor imaging
(DTI) as well as Q-ball imaging. Using the DTI model, we obtain a segmentation
of the corpus callosum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1703</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1703</id><created>2014-10-07</created><authors><author><keyname>Fadaei</keyname><forenames>Salman</forenames></author><author><keyname>Bichler</keyname><forenames>Martin</forenames></author></authors><title>A Truthful-in-expectation Mechanism for the Generalized Assignment
  Problem</title><categories>cs.GT cs.DS</categories><comments>16 pages, Accepted at WINE 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a truthful-in-expectation, $(1-\frac{1}{e})$-approximation
mechanism for the generalized assignment auction. In such an auction, each
bidder has a knapsack valuation function and bidders' values for items are
private. We present a novel convex optimization program for the auction which
results in a maximal-in-distributional-range (MIDR) allocation rule. The
presented program contains at least a $(1-\frac{1}{e})$ ratio of the optimal
social welfare. We show how to implement the convex program in polynomial time
using a fractional local search algorithm which approximates the optimal
solution within an arbitrarily small error. This leads to an approximately MIDR
allocation rule which in turn can be transformed to an approximately
truthful-in-expectation mechanism. Our contribution has algorithmic importance,
as well; it simplifies the existing optimization algorithms for the GAP while
the approximation ratio is comparable to the best given approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1710</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1710</id><created>2014-10-07</created><updated>2015-04-27</updated><authors><author><keyname>Gopalkrishnan</keyname><forenames>Manoj</forenames></author></authors><title>A Cost / Speed / Reliability Trade-off in Erasing a Bit</title><categories>cs.SY cond-mat.stat-mech cs.IT math.IT math.OC</categories><comments>10 pages, 2 figures, to appear in proceedings of the 14th
  International Conference on Unconventional and Natural Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a KL-control treatment of the fundamental problem of erasing a
bit. We introduce notions of \textbf{reliability} of information storage via a
reliability timescale $\tau_r$, and speed of erasing via an erasing timescale
$\tau_e$. Our problem formulation captures the tradeoff between speed,
reliability, and the Kullback-Leibler (KL) cost required to erase a bit. We
show that erasing a reliable bit fast costs at least $\log 2 - \log\left(1 -
\operatorname{e}^{-\frac{\tau_e}{\tau_r}}\right) &gt; \log 2$, which goes to
$\frac{1}{2} \log\frac{2\tau_r}{\tau_e}$ when $\tau_r&gt;&gt;\tau_e$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1726</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1726</id><created>2014-10-07</created><authors><author><keyname>Abdelfattah</keyname><forenames>Ahmad</forenames></author><author><keyname>Keyes</keyname><forenames>David</forenames></author><author><keyname>Ltaief</keyname><forenames>Hatem</forenames></author></authors><title>KBLAS: An Optimized Library for Dense Matrix-Vector Multiplication on
  GPU Accelerators</title><categories>cs.MS</categories><comments>Submitted to the ACM Transactions on Mathematical Software</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  KBLAS is a new open source high performance library that provides optimized
kernels for a subset of Level 2 BLAS functionalities on CUDA-enabled GPUs.
Since performance of dense matrix-vector multiplication is hindered by the
overhead of memory accesses, a double-buffering optimization technique is
employed to overlap data motion with computation. After identifying a proper
set of tuning parameters, KBLAS is able to efficiently run on various GPU
architectures across different generations, avoiding the time-consuming step of
code rewriting, while still being compliant with the standard BLAS API. Another
advanced optimization technique allows to ensure coalesced memory access when
dealing with submatrices, especially in the context of high level dense linear
algebra algorithms. All four precisions KBLAS kernels have been leveraged to
multi-GPUs environment, which requires the introduction of new APIs to ease
users' experiences on these challenging systems. The KBLAS performance
outperforms existing state-of-the-art implementations on all matrix sizes,
achieves asymptotically up to 50% and 60% speedup on single GPU and multi-GPUs
systems, respectively, and validates our performance model. A subset of KBLAS
high performance kernels has been integrated into NVIDIA's standard BLAS
implementation (cuBLAS) for larger dissemination, starting version 6.0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1729</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1729</id><created>2014-10-06</created><authors><author><keyname>Shchurov</keyname><forenames>Andrey A.</forenames></author></authors><title>A Formal Model of Distributed Systems For Test Generation Missions</title><categories>cs.SE cs.DC</categories><comments>6 pages, 9 figures</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V15(2):128-133, September 2014</journal-ref><doi>10.14445/22312803/IJCTT-V15P128</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, deployment of distributed systems sets high requirements for
procedures and tools for the complex testing of these systems - virtualization
and cloud technologies make another level of system complexity. As a possible
solution, it is necessary to determine a formal list of control objectives -
checklists. The automated generation of checklists involves analyzing system
models (with the analysis covering paths in a model). But complex distributed
systems are usually a set of coexisting topologies which interact and depend on
each other and it is necessary to use several models in order to cover
different aspects. This work introduces a formal four layered model for test
generation missions on the basis of the component-based approach and the
concept of layered networks. The interlayer mapping determines how the
topological properties on different layers affect each other and, as a
consequence, represents technologies (virtualization, clustering, etc.) used to
build distributed systems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1735</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1735</id><created>2014-10-07</created><updated>2014-10-08</updated><authors><author><keyname>Davis</keyname><forenames>Joshua</forenames></author></authors><title>A Covert Channel Based on Web Read-time Modulation</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A network covert channel is created that operates by modulating the time
between web resource accesses, with an 'average web user' read-time used as a
reference. While the covert channel may be classified as timing based, it does
not operate by changing deterministic protocol attributes such as inter-packet
delay, as do most timing based network covert channels. Instead, our channel
communicates by modulating transaction level read-time, which in the web
browsing case has significant non-deterministic components. The channel is thus
immune to methods typically used to detect timing based network covert
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1740</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1740</id><created>2014-10-07</created><authors><author><keyname>Haustein</keyname><forenames>Stefanie</forenames></author><author><keyname>Bowman</keyname><forenames>Timothy D.</forenames></author><author><keyname>Holmberg</keyname><forenames>Kim</forenames></author><author><keyname>Peters</keyname><forenames>Isabella</forenames></author><author><keyname>Larivi&#xe8;re</keyname><forenames>Vincent</forenames></author></authors><title>Astrophysicists on Twitter: An in-depth analysis of tweeting and
  scientific publication behavior</title><categories>cs.DL</categories><comments>14 pages, 5 figures, 7 tables</comments><journal-ref>Aslib Journal of Information Management 66(3) (2014) 279-296</journal-ref><doi>10.1108/AJIM-09-2013-0081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the tweeting behavior of 37 astrophysicists on Twitter
and compares their tweeting behavior with their publication behavior and
citation impact to show whether they tweet research-related topics or not.
Astrophysicists on Twitter are selected to compare their tweets with their
publications from Web of Science. Different user groups are identified based on
tweeting and publication frequency. A moderate negative correlation (p=-0.390*)
is found between the number of publications and tweets per day, while retweet
and citation rates do not correlate. The similarity between tweets and
abstracts is very low (cos=0.081). User groups show different tweeting behavior
such as retweeting and including hashtags, usernames and URLs. The study is
limited in terms of the small set of astrophysicists. Results are not
necessarily representative of the entire astrophysicist community on Twitter
and they most certainly do not apply to scientists in general. Future research
should apply the methods to a larger set of researchers and other scientific
disciplines. To a certain extent, this study helps to understand how
researchers use Twitter. The results hint at the fact that impact on Twitter
can neither be equated with nor replace traditional research impact metrics.
However, tweets and other so-called altmetrics might be able to reflect other
impact of scientists such as public outreach and science communication. To the
best of our knowledge, this is the first in-depth study comparing researchers'
tweeting activity and behavior with scientific publication output in terms of
quantity, content and impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1747</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1747</id><created>2014-10-06</created><authors><author><keyname>Shchurov</keyname><forenames>Andrey A.</forenames></author><author><keyname>Marik</keyname><forenames>Radek</forenames></author></authors><title>A Formal Approach to Distributed System Tests Design</title><categories>cs.SE cs.DC</categories><comments>10 pages, 6 figures</comments><journal-ref>International Journal of Computer and Information Technology
  (IJCIT) V03(04):696-705, July 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deployment of distributed systems sets high requirements for procedures and
tools for the complex testing of these systems. This work introduces a formal
four-layered model for test generation mission on the basis of the
component-based approach and the concept of layered networks. Based on this
model, we describe the test generation strategy that covers every interaction
from the end-user requirements on all coexisting architectural layers, and
checks the internal consistency of the system technical specifications with
respect to the end-user requirements. The next step introduces the Prolog-based
approach to representing this model and the requirements-coverage strategy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1764</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1764</id><created>2014-10-03</created><authors><author><keyname>Schnetter</keyname><forenames>Erik</forenames></author><author><keyname>Blazewicz</keyname><forenames>Marek</forenames></author><author><keyname>Brandt</keyname><forenames>Steven R.</forenames></author><author><keyname>Koppelman</keyname><forenames>David M.</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Frank</forenames></author></authors><title>Chemora: A PDE Solving Framework for Modern HPC Architectures</title><categories>cs.MS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern HPC architectures consist of heterogeneous multi-core, many-node
systems with deep memory hierarchies. Modern applications employ ever more
advanced discretisation methods to study multi-physics problems. Developing
such applications that explore cutting-edge physics on cutting-edge HPC systems
has become a complex task that requires significant HPC knowledge and
experience. Unfortunately, this combined knowledge is currently out of reach
for all but a few groups of application developers.
  Chemora is a framework for solving systems of Partial Differential Equations
(PDEs) that targets modern HPC architectures. Chemora is based on Cactus, which
sees prominent usage in the computational relativistic astrophysics community.
In Chemora, PDEs are expressed either in a high-level \LaTeX-like language or
in Mathematica. Discretisation stencils are defined separately from equations,
and can include Finite Differences, Discontinuous Galerkin Finite Elements
(DGFE), Adaptive Mesh Refinement (AMR), and multi-block systems.
  We use Chemora in the Einstein Toolkit to implement the Einstein Equations on
CPUs and on accelerators, and study astrophysical systems such as black hole
binaries, neutron stars, and core-collapse supernovae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1773</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1773</id><created>2014-10-02</created><authors><author><keyname>Ishengoma</keyname><forenames>Fredrick Romanus</forenames></author></authors><title>A Novel Design of IEEE 802.15.4 and Solar Based Autonomous Water Quality
  Monitoring Prototype using ECHERP</title><categories>cs.OH</categories><comments>12 pages, International Journal of Computer Science and Network
  Solutions, Volume 2, Issue 1, January 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently advancement in Wireless Sensor Network (WSN) technology has
brought new distributed sensing applications such as water quality monitoring.
With sensing capabilities and using parameters like pH, conductivity and
temperature, the quality of water can be known. This paper proposes a novel
design based on IEEE 802.15.4 (Zig-Bee protocol) and solar energy called
Autonomous Water Quality Monitoring Prototype (AWQMP). The prototype is
designed to use ECHERP routing protocol and Adruino Mega 2560, an open-source
electronic prototyping platform for data acquisition. AWQMP is expected to give
real time data acquirement and to reduce the cost of manual water quality
monitoring due to its autonomous characteristic. Moreover, the proposed
prototype will help to study the behavior of aquatic animals in deployed water
bodies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1775</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1775</id><created>2014-10-07</created><authors><author><keyname>Kim</keyname><forenames>Yongjune</forenames></author><author><keyname>Kumar</keyname><forenames>B. V. K. Vijaya</forenames></author></authors><title>Writing on dirty flash memory</title><categories>cs.IT math.IT</categories><comments>8 pages, accepted to 52nd Annual Allerton Conference on
  Communication, Control, and Computing, Oct. 2014</comments><doi>10.1109/ALLERTON.2014.7028498</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most important challenge in the scaling down of flash memory is its
increased inter-cell interference (ICI). If side information about ICI is known
to the encoder, the flash memory channel can be viewed as similar to Costa's
&quot;writing on dirty paper (dirty paper coding).&quot; We first explain why flash
memories are dirty due to ICI. We then show that &quot;dirty flash memory&quot; can be
changed into &quot;memory with defective cells&quot; model by using only one pre-read
operation. The asymmetry between write and erase operations in flash memory
plays an important role in this change. Based on the &quot;memory with defective
cells&quot; model, we show that additive encoding can significantly improve the
probability of decoding failure by using the side information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1776</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1776</id><created>2014-10-07</created><authors><author><keyname>Smith</keyname><forenames>Fabrizio</forenames></author><author><keyname>Proietti</keyname><forenames>Maurizio</forenames></author></authors><title>Ontology-based Representation and Reasoning on Process Models: A Logic
  Programming Approach</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework grounded in Logic Programming for representing and
reasoning about business processes from both the procedural and ontological
point of views. In particular, our goal is threefold: (1) define a logical
language and a formal semantics for process models enriched with ontology-based
annotations; (2) provide an effective inference mechanism that supports the
combination of reasoning services dealing with the structural definition of a
process model, its behavior, and the domain knowledge related to the
participating business entities; (3) implement such a theoretical framework
into a process modeling and reasoning platform. To this end we define a process
ontology coping with a relevant fragment of the popular BPMN modeling notation.
The behavioral semantics of a process is defined as a state transition system
by following an approach similar to the Fluent Calculus, and allows us to
specify state change in terms of preconditions and effects of the enactment of
activities. Then we show how the procedural process knowledge can be seamlessly
integrated with the domain knowledge specified by using the OWL 2 RL rule-based
ontology language. Our framework provides a wide range of reasoning services,
including CTL model checking, which can be performed by using standard Logic
Programming inference engines through a goal-oriented, efficient, sound and
complete evaluation procedure. We also present a software environment
implementing the proposed framework, and we report on an experimental
evaluation of the system, whose results are encouraging and show the viability
of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1783</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1783</id><created>2014-10-07</created><updated>2015-09-17</updated><authors><author><keyname>Narasimhan</keyname><forenames>Jeyanthi</forenames></author><author><keyname>Holder</keyname><forenames>Lawrence</forenames></author></authors><title>Feature Engineering for Supervised Link Prediction on Dynamic Social
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 12 figures, the 10th international conference on Data
  Mining, DMIN'14. The paper is withdrawn by the author owing to change in
  results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction is an important network science problem in many domains such
as social networks, chem/bio-informatics, etc. Most of these networks are
dynamic in nature with patterns evolving over time. In such cases, it is
necessary to incorporate time in the mining process in a seamless manner to aid
in better prediction performance. We propose a two-step solution strategy to
the link prediction problem in dynamic networks in this work. The first step
involves a novel yet simple feature construction approach using a combination
of domain and topological attributes of the graph. In the second phase, we
perform unconstrained edge selection to identify potential candidates for
prediction by any generic two-class learner. We design various experiments on a
real world collaboration network and show the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1784</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1784</id><created>2014-10-02</created><authors><author><keyname>Masegosa</keyname><forenames>Andres R.</forenames></author></authors><title>Stochastic Discriminative EM</title><categories>cs.LG</categories><comments>UAI 2014 paper + Supplementary Material. In Proceedings of the
  Thirtieth Conference on Uncertainty in Artificial Intelligence (UAI 2014),
  edited by Nevin L. Zhang and Jian Tian. AUAI Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic discriminative EM (sdEM) is an online-EM-type algorithm for
discriminative training of probabilistic generative models belonging to the
exponential family. In this work, we introduce and justify this algorithm as a
stochastic natural gradient descent method, i.e. a method which accounts for
the information geometry in the parameter space of the statistical model. We
show how this learning algorithm can be used to train probabilistic generative
models by minimizing different discriminative loss functions, such as the
negative conditional log-likelihood and the Hinge loss. The resulting models
trained by sdEM are always generative (i.e. they define a joint probability
distribution) and, in consequence, allows to deal with missing data and latent
variables in a principled way either when being learned or when making
predictions. The performance of this method is illustrated by several text
classification problems for which a multinomial naive Bayes and a latent
Dirichlet allocation based classifier are learned using different
discriminative loss functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1805</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1805</id><created>2014-10-07</created><updated>2014-10-08</updated><authors><author><keyname>Morsi</keyname><forenames>Rania</forenames></author><author><keyname>Michalopoulos</keyname><forenames>Diomidis S.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Performance Analysis of Wireless Powered Communication with
  Finite/Infinite Energy Storage</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures, 1 table, submitted for possible conference
  publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider an energy harvesting (EH) node which harvests
energy from a radio frequency (RF) signal broadcasted by an access point (AP)
in the downlink (DL). The node stores the harvested energy in an energy buffer
and uses the stored energy to transmit data to the AP in the uplink (UL). We
consider a simple transmission policy, which accounts for the fact that in
practice the EH node may not have knowledge of the EH profile nor of the UL
channel state information. In particular, in each time slot, the EH node
transmits with either a constant desired power or a lower power if not enough
energy is available in its energy buffer. For this simple policy, we use the
theory of discrete-time continuous-state Markov chains to analyze the limiting
distribution of the stored energy for finite- and infinite-size energy buffers.
Moreover, we take into account imperfections of the energy buffer and the
circuit power consumption of the EH node. For a Rayleigh fading DL channel, we
provide the limiting distribution of the energy buffer content in closed form.
In addition, we analyze the average error rate and the outage probability of a
Rayleigh faded UL channel and show that the diversity order is not affected by
the finite capacity of the energy buffer. Our results reveal that the optimal
desired transmit power by the EH node is always less than the average harvested
power and increases with the capacity of the energy buffer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1823</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1823</id><created>2014-10-07</created><updated>2015-03-04</updated><authors><author><keyname>Juen</keyname><forenames>Joshua</forenames></author><author><keyname>Johnson</keyname><forenames>Aaron</forenames></author><author><keyname>Das</keyname><forenames>Anupam</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author></authors><title>Defending Tor from Network Adversaries: A Case Study of Network Path
  Prediction</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Tor anonymity network has been shown vulnerable to traffic analysis
attacks by autonomous systems and Internet exchanges, which can observe
different overlay hops belonging to the same circuit. We aim to determine
whether network path prediction techniques provide an accurate picture of the
threat from such adversaries, and whether they can be used to avoid this
threat. We perform a measurement study by running traceroutes from Tor relays
to destinations around the Internet. We use the data to evaluate the accuracy
of the autonomous systems and Internet exchanges that are predicted to appear
on the path using state-of-the-art path inference techniques; we also consider
the impact that prediction errors have on Tor security, and whether it is
possible to produce a useful overestimate that does not miss important threats.
Finally, we evaluate the possibility of using these predictions to actively
avoid AS and IX adversaries and the challenges this creates for the design of
Tor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1826</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1826</id><created>2014-10-07</created><authors><author><keyname>Aldridge</keyname><forenames>Matthew</forenames></author><author><keyname>Baldassini</keyname><forenames>Leonardo</forenames></author><author><keyname>Gunderson</keyname><forenames>Karen</forenames></author></authors><title>Almost Separable Matrices</title><categories>math.CO cs.IT math.IT</categories><doi>10.1007/s10878-015-9951-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $m \times n$ matrix $\mathsf{A}$ with column supports $\{S_i\}$ is
$k$-separable if the disjunctions $\bigcup_{i \in \mathcal{K}} S_i$ are all
distinct over all sets $\mathcal{K}$ of cardinality $k$. While a simple
counting bound shows that $m &gt; k \log_2 n/k$ rows are required for a separable
matrix to exist, in fact it is necessary for $m$ to be about a factor of $k$
more than this. In this paper, we consider a weaker definition of `almost
$k$-separability', which requires that the disjunctions are `mostly distinct'.
We show using a random construction that these matrices exist with $m = O(k
\log n)$ rows, which is optimal for $k = O(n^{1-\beta})$. Further, by
calculating explicit constants, we show how almost separable matrices give new
bounds on the rate of nonadaptive group testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1828</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1828</id><created>2014-10-07</created><authors><author><keyname>Cheng</keyname><forenames>Cheng</forenames></author><author><keyname>Jiang</keyname><forenames>Yingchun</forenames></author><author><keyname>Sun</keyname><forenames>Qiyu</forenames></author></authors><title>Sampling and Galerkin reconstruction in reproducing kernel spaces</title><categories>cs.IT math.IT math.NA</categories><msc-class>94A20, 46E22, 65J22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider sampling in a reproducing kernel subspace of
$L^p$. We introduce a pre-reconstruction operator associated with a sampling
scheme and propose a Galerkin reconstruction in general Banach space setting.
We show that the proposed Galerkin method provides a quasi-optimal
approximation, and the corresponding Galerkin equations could be solved by an
iterative approximation-projection algorithm. We also present detailed analysis
and numerical simulations of the Galerkin method for reconstructing signals
with finite rate of innovation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1830</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1830</id><created>2014-10-06</created><authors><author><keyname>Enyioha</keyname><forenames>Chinwendu</forenames></author><author><keyname>Rahimian</keyname><forenames>Mohammad Amin</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Controllability and Fraction of Leaders in Infinite Network</title><categories>cs.SY math.OC physics.soc-ph</categories><comments>6 pages, 3 figures, to appear in 2014 IEEE CDC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study controllability of a network of linear
single-integrator agents when the network size goes to infinity. We first
investigate the effect of increasing size by injecting an input at every node
and requiring that network controllability Gramian remain well-conditioned with
the increasing dimension. We provide theoretical justification to the intuition
that high degree nodes pose a challenge to network controllability. In
particular, the controllability Gramian for the networks with bounded maximum
degrees is shown to remain well-conditioned even as the network size goes to
infinity. In the canonical cases of star, chain and ring networks, we also
provide closed-form expressions which bound the condition number of the
controllability Gramian in terms of the network size. We next consider the
effect of the choice and number of leader nodes by actuating only a subset of
nodes and considering the least eigenvalue of the Gramian as the network size
increases. Accordingly, while a directed star topology can never be made
controllable for all sizes by injecting an input just at a fraction $f&lt;1$ of
nodes; for path or cycle networks, the designer can actuate a non-zero fraction
of nodes and spread them throughout the network in such way that the least
eigenvalue of the Gramians remain bounded away from zero with the increasing
size. The results offer interesting insights on the challenges of control in
large networks and with high-degree nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1838</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1838</id><created>2014-10-07</created><updated>2015-02-20</updated><authors><author><keyname>Michelusi</keyname><forenames>Nicolo</forenames></author><author><keyname>Pirbadian</keyname><forenames>Sahand</forenames></author><author><keyname>El-Naggar</keyname><forenames>Mohamed Y.</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>A Stochastic Model for Electron Transfer in Bacterial Cables</title><categories>cs.ET</categories><comments>IEEE Journal on Selected Areas in Communications</comments><doi>10.1109/JSAC.2014.2367666</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biological systems are known to communicate by diffusing chemical signals in
the surrounding medium. However, most of the recent literature has neglected
the electron transfer mechanism occurring amongst living cells, and its role in
cell-cell communication. Each cell relies on a continuous flow of electrons
from its electron donor to its electron acceptor through the electron transport
chain to produce energy in the form of the molecule adenosine triphosphate, and
to sustain the cell's vital operations and functions. While the importance of
biological electron transfer is well-known for individual cells, the past
decade has also brought about remarkable discoveries of multi-cellular
microbial communities that transfer electrons between cells and across
centimeter length scales, e.g., biofilms and multi-cellular bacterial cables.
These experimental observations open up new frontiers in the design of
electron-based communications networks in microbial communities, which may
coexist with the more well-known communication strategies based on molecular
diffusion, while benefiting from a much shorter communication delay. This paper
develops a stochastic model that links the electron transfer mechanism to the
energetic state of the cell. The model is also extensible to larger
communities, by allowing for electron exchange between neighboring cells.
Moreover, the parameters of the stochastic model are fit to experimental data
available in the literature, and are shown to provide a good fit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1839</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1839</id><created>2014-10-07</created><authors><author><keyname>Choudhury</keyname><forenames>Anamitra Roy</forenames></author><author><keyname>Das</keyname><forenames>Syamantak</forenames></author><author><keyname>Garg</keyname><forenames>Naveen</forenames></author><author><keyname>Kumar</keyname><forenames>Amit</forenames></author></authors><title>Rejecting Jobs to Minimize Load and Maximum Flow-time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online algorithms are usually analyzed using the notion of competitive ratio
which compares the solution obtained by the algorithm to that obtained by an
online adversary for the worst possible input sequence. Often this measure
turns out to be too pessimistic, and one popular approach especially for
scheduling problems has been that of &quot;resource augmentation&quot; which was first
proposed by Kalyanasundaram and Pruhs. Although resource augmentation has been
very successful in dealing with a variety of objective functions, there are
problems for which even a (arbitrary) constant speedup cannot lead to a
constant competitive algorithm. In this paper we propose a &quot;rejection model&quot;
which requires no resource augmentation but which permits the online algorithm
to not serve an epsilon-fraction of the requests.
  The problems considered in this paper are in the restricted assignment
setting where each job can be assigned only to a subset of machines. For the
load balancing problem where the objective is to minimize the maximum load on
any machine, we give $O(\log^2 1/\eps)$-competitive algorithm which rejects at
most an $\eps$-fraction of the jobs. For the problem of minimizing the maximum
weighted flow-time, we give an $O(1/\eps^4)$-competitive algorithm which can
reject at most an $\eps$-fraction of the jobs by weight. We also extend this
result to a more general setting where the weights of a job for measuring its
weighted flow-time and its contribution towards total allowed rejection weight
are different. This is useful, for instance, when we consider the objective of
minimizing the maximum stretch. We obtain an $O(1/\eps^6)$-competitive
algorithm in this case.
  Our algorithms are immediate dispatch, though they may not be immediate
reject. All these problems have very strong lower bounds in the speed
augmentation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1851</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1851</id><created>2014-10-07</created><authors><author><keyname>Kuo</keyname><forenames>Wei-Cheng</forenames></author><author><keyname>Wang</keyname><forenames>Chih-Chun</forenames></author></authors><title>Robust And Optimal Opportunistic Scheduling For Downlink 2-Flow Network
  Coding With Varying Channel Quality and Rate Adaptation</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the downlink traffic from a base station to two
different clients. When assuming infinite backlog, it is known that
inter-session network coding (INC) can significantly increase the throughput of
each flow. However, the corresponding scheduling solution (when assuming
dynamic arrivals instead and requiring bounded delay) is still nascent.
  For the 2-flow downlink scenario, we propose the first opportunistic INC +
scheduling solution that is provably optimal for time-varying channels, i.e.,
the corresponding stability region matches the optimal Shannon capacity.
Specifically, we first introduce a new binary INC operation, which is
distinctly different from the traditional wisdom of XORing two overheard
packets. We then develop a queue-length-based scheduling scheme, which, with
the help of the new INC operation, can robustly and optimally adapt to
time-varying channel quality. We then show that the proposed algorithm can be
easily extended for rate adaptation and it again robustly achieves the optimal
throughput. A byproduct of our results is a scheduling scheme for stochastic
processing networks (SPNs) with random departure, which relaxes the assumption
of deterministic departure in the existing results. The new SPN scheduler could
thus further broaden the applications of SPN scheduling to other real-world
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1864</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1864</id><created>2014-10-07</created><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>A weakly universal cellular automaton in the heptagrid with three states</title><categories>cs.DM nlin.CG</categories><comments>27 pages, 21 figures. arXiv admin note: substantial text overlap with
  arXiv:1403.2373</comments><msc-class>68R05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct a cellular automaton on the heptagrid which is
planar, weakly universal and which have three states only. This result improves
the best result which was with four states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1870</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1870</id><created>2014-10-07</created><updated>2015-12-02</updated><authors><author><keyname>Shekatkar</keyname><forenames>Snehal M.</forenames></author><author><keyname>Ambika</keyname><forenames>G.</forenames></author></authors><title>Complex networks with scale-free nature and hierarchical modularity</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>7 pages, 9 figures</comments><journal-ref>Eur. Phys. J. B (2015) 88: 227</journal-ref><doi>10.1140/epjb/e2015-60501-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative mechanisms which lead to empirically observed structure of
networked systems from diverse fields like biology, technology and social
sciences form a very important part of study of complex networks. The structure
of many networked systems like biological cell, human society and World Wide
Web markedly deviate from that of completely random networks indicating the
presence of underlying processes. Often the main process involved in their
evolution is the addition of links between existing nodes having a common
neighbor. In this context we introduce an important property of the nodes,
which we call mediating capacity, that is generic to many networks. This
capacity decreases rapidly with increase in degree, making hubs weak mediators
of the process. We show that this property of nodes provides an explanation for
the simultaneous occurrence of the observed scale-free structure and
hierarchical modularity in many networked systems. This also explains the high
clustering and small-path length seen in real networks as well as non-zero
degree-correlations. Our study also provides insight into the local process
which ultimately leads to emergence of preferential attachment and hence is
also important in understanding robustness and control of real networks as well
as processes happening on real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1895</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1895</id><created>2014-10-06</created><updated>2016-02-17</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Measuring impact in research evaluations: A thorough discussion of
  methods for, effects of, and problems with impact measurements</title><categories>cs.DL physics.soc-ph</categories><comments>Accepted for publication in Higher Education</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Impact of science is one of the most important topics in scientometrics.
Recent developments show a fundamental change in impact measurements from
impact on science to impact on society. Since impact measurement is currently
in a state of far reaching changes, this paper describes recent developments
and facing problems in this area. For that the results of key publications
(dealing with impact measurement) are discussed. The paper discusses how impact
is generally measured within science and beyond (section 2), which effects
impact measurements have on the science system (section 3), and which problems
are associated with impact measurement (section 4). The problems associated
with impact measurement constitute the focus of this paper: Science is marked
by inequality, random chance, anomalies, the right to make mistakes,
unpredictability, and a high significance of extreme events, which might
distort impact measurements. Scientometricians as the producer of impact scores
and decision makers as their consumers should be aware of these problems and
should consider them in the generation and interpretation of bibliometric
results, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1901</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1901</id><created>2014-10-07</created><authors><author><keyname>Liu</keyname><forenames>Lu</forenames></author><author><keyname>Cao</keyname><forenames>Xianghui</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Li</forenames></author></authors><title>On Optimizing Energy Efficiency in Multi-Radio Multi-Channel Wireless
  Networks</title><categories>cs.NI</categories><comments>6 pages, 5 figures, Accepted to Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-radio multi-channel (MR-MC) networks contribute significant enhancement
in the network throughput by exploiting multiple radio interfaces and
non-overlapping channels. While throughput optimization is one of the main
targets in allocating resource in MR-MC networks, recently, the network energy
efficiency is becoming a more and more important concern. Although turning on
more radios and exploiting more channels for communication is always beneficial
to network capacity, they may not be necessarily desirable from an energy
efficiency perspective. The relationship between these two often conflicting
objectives has not been well-studied in many existing works. In this paper, we
investigate the problem of optimizing energy efficiency under full capacity
operation in MR-MC networks and analyze the optimal choices of numbers of
radios and channels. We provide detailed problem formulation and solution
procedures. In particular, for homogeneous commodity networks, we derive a
theoretical upper bound of the optimal energy efficiency and analyze the
conditions under which such optimality can be achieved. Numerical results
demonstrate that the achieved optimal energy efficiency is close to the
theoretical upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1903</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1903</id><created>2014-10-07</created><authors><author><keyname>Vaquero</keyname><forenames>Luis M.</forenames></author><author><keyname>Cuadrado</keyname><forenames>Felix</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author></authors><title>Systems for Near Real-Time Analysis of Large-Scale Dynamic Graphs</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Graphs are widespread data structures used to model a wide variety of
problems. The sheer amount of data to be processed has prompted the creation of
a myriad of systems that help us cope with massive scale graphs. The pressure
to deliver fast responses to queries on the graph is higher than ever before,
as it is demanded by many applications (e.g. online recommendations, auctions,
terrorism protection, etc.). In addition, graphs change continuously (so do the
real world entities that typically represent). Systems must be ready for both:
near real-time and dynamic massive graphs. We survey systems taking their
scalability, real-time potential and capability to support dynamic changes to
the graph as driving guidelines. The main techniques and limitations are
distilled and categorised. The algorithms run on top of graph systems are not
ready for prime time dynamism either. Therefore,a short overview on dynamic
graph algorithms has also been included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1905</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1905</id><created>2014-10-07</created><authors><author><keyname>Huang</keyname><forenames>Wentao</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author></authors><title>Single-Source/Sink Network Error Correction Is as Hard as
  Multiple-Unicast</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of communicating over a single-source single-terminal
network in the presence of an adversary that may jam a single link of the
network. If any one of the edges can be jammed, the capacity of such networks
is well understood and follows directly from the connection between the minimum
cut and maximum flow in single-source single- terminal networks. In this work
we consider networks in which some edges cannot be jammed, and show that
determining the network communication capacity is at least as hard as solving
the multiple-unicast network coding problem for the error-free case. The latter
problem is a long standing open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1920</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1920</id><created>2014-10-07</created><authors><author><keyname>Chen</keyname><forenames>Yiling</forenames></author><author><keyname>Sheffet</keyname><forenames>Or</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author></authors><title>Privacy Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of analyzing the effect of privacy concerns on the behavior of
selfish utility-maximizing agents has received much attention lately. Privacy
concerns are often modeled by altering the utility functions of agents to
consider also their privacy loss. Such privacy aware agents prefer to take a
randomized strategy even in very simple games in which non-privacy aware agents
play pure strategies. In some cases, the behavior of privacy aware agents
follows the framework of Randomized Response, a well-known mechanism that
preserves differential privacy.
  Our work is aimed at better understanding the behavior of agents in settings
where their privacy concerns are explicitly given. We consider a toy setting
where agent A, in an attempt to discover the secret type of agent B, offers B a
gift that one type of B agent likes and the other type dislikes. As opposed to
previous works, B's incentive to keep her type a secret isn't the result of
&quot;hardwiring&quot; B's utility function to consider privacy, but rather takes the
form of a payment between B and A. We investigate three different types of
payment functions and analyze B's behavior in each of the resulting games. As
we show, under some payments, B's behavior is very different than the behavior
of agents with hardwired privacy concerns and might even be deterministic.
Under a different payment we show that B's BNE strategy does fall into the
framework of Randomized Response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1924</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1924</id><created>2014-10-07</created><authors><author><keyname>Yousefi</keyname><forenames>Mansoor I.</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>On the Per-Sample Capacity of Nondispersive Optical Fibers</title><categories>cs.IT math.IT</categories><comments>Published in 2011</comments><journal-ref>IEEE Transactions on Information Theory, vol. 57, no. 11, pp.
  7522--7541, Nov. 2011</journal-ref><doi>10.1109/TIT.2011.2165793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of the channel defined by the stochastic nonlinear Schr\&quot;odinger
equation, which includes the effects of the Kerr nonlinearity and amplified
spontaneous emission noise, is considered in the case of zero dispersion. In
the absence of dispersion, this channel behaves as a collection of parallel
per-sample channels. The conditional probability density function of the
nonlinear per-sample channels is derived using both a sum-product and a
Fokker-Planck differential equation approach. It is shown that, for a fixed
noise power, the per-sample capacity grows unboundedly with input signal. The
channel can be partitioned into amplitude and phase subchannels, and it is
shown that the contribution to the total capacity of the phase channel declines
for large input powers. It is found that a two-dimensional distribution with a
half-Gaussian profile on the amplitude and uniform phase provides a lower bound
for the zero-dispersion optical fiber channel, which is simple and
asymptotically capacity-achieving at high signal-to-noise ratios (SNRs). A
lower bound on the capacity is also derived in the medium-SNR region. The exact
capacity subject to peak and average power constraints is numerically
quantified using dense multiple ring modulation formats. The differential model
underlying the zero-dispersion channel is reduced to an algebraic model, which
is more tractable for digital communication studies, and in particular it
provides a relation between the zero-dispersion optical channel and a $2 \times
2$ multiple-input multiple-output Rician fading channel. It appears that the
structure of the capacity-achieving input distribution resembles that of the
Rician fading channel, i.e., it is discrete in amplitude with a finite number
of mass points, while continuous and uniform in phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1934</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1934</id><created>2014-10-07</created><authors><author><keyname>Moosavi</keyname><forenames>Azam S. Zavar</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Approximate Exponential Algorithms to Solve the Chemical Master Equation</title><categories>cs.NA</categories><comments>14 pages- 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses new simulation algorithms for stochastic chemical
kinetics that exploit the linearity of the chemical master equation and its
matrix exponential exact solution. These algorithms make use of various
approximations of the matrix exponential to evolve probability densities in
time. A sampling of the approximate solutions of the chemical master equation
is used to derive accelerated stochastic simulation algorithms. Numerical
experiments compare the new methods with the established stochastic simulation
algorithm and the tau-leaping method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1939</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1939</id><created>2014-10-07</created><authors><author><keyname>Freemon</keyname><forenames>D. Michael</forenames></author></authors><title>Maximizing Utilization and Performance of Guaranteed-Bandwidth Long Fat
  Networks and Virtual Circuits</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Like many big science projects, the Large Synoptic Survey Telescope (LSST)
has multiple geographic locations among which large amounts of data must be
transferred. One particular type of data, crosstalk-corrected images, must be
moved from South America to North America under stringent deadline
requirements. LSST is provisioning an international network with bandwidth
guarantees to handle this traffic. In prior work, we re-examined TCP congestion
control for this use case and found that TCP throughput can approach wire
speeds. This work shows that the Hierarchical Token Bucket (HTB) provides an
excellent mechanism by which bandwidth can be managed for a wide range of
traffic types. Using HTB without TCP congestion control over
guaranteed-bandwidth virtual circuits is a compelling solution to the
historical problem of poor TCP performance over long fat networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1940</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1940</id><created>2014-10-07</created><authors><author><keyname>Qi</keyname><affiliation>Rose</affiliation></author><author><keyname>Yu</keyname></author><author><keyname>He</keyname><forenames>Xinran</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author></authors><title>GLAD: Group Anomaly Detection in Social Media Analysis- Extended
  Abstract</title><categories>cs.LG cs.SI</categories><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional anomaly detection on social media mostly focuses on individual
point anomalies while anomalous phenomena usually occur in groups. Therefore it
is valuable to study the collective behavior of individuals and detect group
anomalies. Existing group anomaly detection approaches rely on the assumption
that the groups are known, which can hardly be true in real world social media
applications. In this paper, we take a generative approach by proposing a
hierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLAD
takes both pair-wise and point-wise data as input, automatically infers the
groups and detects group anomalies simultaneously. To account for the dynamic
properties of the social media data, we further generalize GLAD to its dynamic
extension d-GLAD. We conduct extensive experiments to evaluate our models on
both synthetic and real world datasets. The empirical results demonstrate that
our approach is effective and robust in discovering latent groups and detecting
group anomalies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1941</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1941</id><created>2014-10-07</created><authors><author><keyname>Jiang</keyname><forenames>Bomin</forenames></author><author><keyname>Sun</keyname><forenames>Zhiyong</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author></authors><title>Higher order Voronoi based mobile coverage control</title><categories>cs.SY</categories><comments>Submitted to ACC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most current results on coverage control using mobile sensors require that
one partitioned cell is the sole responsibility of one sensor. In this paper,
we consider a class of generalized Voronoi coverage control problems by using
higher order Voronoi partitions, motivated by applications that more than one
senor is required to monitor and cover onecell. We introduce a framework
depending on a coverage performance function incorporating higher order Voronoi
cells and then design a gradient-based controller which allows the multi-sensor
system to achieve a local equilibrium in a distributed manner. In addition, we
provide a number of real world scenarios where our framework can be applied.
Simulation results are also provided to show the controller performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1944</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1944</id><created>2014-10-07</created><authors><author><keyname>Gibson</keyname><forenames>Travis E.</forenames></author><author><keyname>Qu</keyname><forenames>Zheng</forenames></author><author><keyname>Annaswamy</keyname><forenames>Anuradha M.</forenames></author><author><keyname>Lavretsky</keyname><forenames>Eugene</forenames></author></authors><title>Adaptive Output Feedback based on Closed-loop Reference Models</title><categories>cs.SY math.OC nlin.AO</categories><comments>8 Pages, submitted to IEEE Transactions on Automatic Control</comments><journal-ref>Automatic Control, IEEE Transactions on , vol.60, no.10,
  pp.2728-2733, Oct. 2015</journal-ref><doi>10.1109/TAC.2015.2405295</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note presents the design and analysis of an adaptive controller for a
class of linear plants in the presence of output feedback. This controller
makes use of a closed-loop reference model as an observer, and guarantees
global stability and asymptotic output tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1950</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1950</id><created>2014-10-07</created><authors><author><keyname>Coleman</keyname><forenames>David</forenames></author><author><keyname>Sucan</keyname><forenames>Ioan A.</forenames></author><author><keyname>Moll</keyname><forenames>Mark</forenames></author><author><keyname>Okada</keyname><forenames>Kei</forenames></author><author><keyname>Correll</keyname><forenames>Nikolaus</forenames></author></authors><title>Experience-Based Planning with Sparse Roadmap Spanners</title><categories>cs.RO</categories><comments>Submitted to ICRA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an experienced-based planning framework called Thunder that learns
to reduce computation time required to solve high-dimensional planning problems
in varying environments. The approach is especially suited for large
configuration spaces that include many invariant constraints, such as those
found with whole body humanoid motion planning. Experiences are generated using
probabilistic sampling and stored in a sparse roadmap spanner (SPARS), which
provides asymptotically near-optimal coverage of the configuration space,
making storing, retrieving, and repairing past experiences very efficient with
respect to memory and time. The Thunder framework improves upon past
experience-based planners by storing experiences in a graph rather than in
individual paths, eliminating redundant information, providing more
opportunities for path reuse, and providing a theoretical limit to the size of
the experience graph. These properties also lead to improved handling of
dynamically changing environments, reasoning about optimal paths, and reducing
query resolution time. The approach is demonstrated on a 30 degrees of freedom
humanoid robot and compared with the Lightning framework, an experience-based
planner that uses individual paths to store past experiences. In environments
with variable obstacles and stability constraints, experiments show that
Thunder is on average an order of magnitude faster than Lightning and planning
from scratch. Thunder also uses 98.8% less memory to store its experiences
after 10,000 trials when compared to Lightning. Our framework is implemented
and freely available in the Open Motion Planning Library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1956</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1956</id><created>2014-10-07</created><updated>2015-02-18</updated><authors><author><keyname>James</keyname><forenames>Oliver</forenames></author><author><keyname>Lee</keyname><forenames>Heung-No</forenames></author></authors><title>Restricted Isometry Random Variables: Probability Distributions, RIC
  Prediction and Phase Transition Analysis for Gaussian Encoders</title><categories>cs.IT math.IT</categories><comments>15 pages; In revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we aim to generalize the notion of restricted isometry
constant (RIC) in compressive sensing (CS) to restricted isometry random
variable (RIV). Associated with a deterministic encoder there are two RICs,
namely, the left and the right RIC. We show that these RICs can be generalized
to a left RIV and a right RIV for an ensemble of random encoders. We derive the
probability and the cumulative distribution functions of these RIVs for the
most widely used i.i.d. Gaussian encoders. We also derive the asymptotic
distributions of the RIVs and show that the distribution of the left RIV
converges (in distribution) to the Weibull distribution, whereas that of the
right RIV converges to the Gumbel distribution. By adopting the RIV framework,
we bring to forefront that the current practice of using eigenvalues for RIC
prediction can be improved. We show on the one hand that the eigenvalue-based
approaches tend to overestimate the RICs. On the other hand, the RIV-based
analysis yields precise estimates of the RICs. We also demonstrate that this
precise estimation aids to improve the previous RIC-based phase transition
analysis in CS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1966</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1966</id><created>2014-10-07</created><authors><author><keyname>Xu</keyname><forenames>Weiqiang</forenames></author><author><keyname>Yuan</keyname><forenames>Wenchu</forenames></author><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Zhang</keyname><forenames>Yake</forenames></author></authors><title>Distributed Energy Efficient Cross-layer Optimization for Multihop MIMO
  Cognitive Radio Networks with Primary User Rate Protection</title><categories>cs.NI</categories><comments>Submitted to IEEE Transactions on Vehicle Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the unique physical-layer characteristics associated with MIMO and
cognitive radio (CR), the network performance is tightly coupled with
mechanisms at the physical, link, network, and transport layers. In this paper,
we consider an energy-efficient cross-layer optimization problem in multihop
MIMO CR networks. The objective is to balance the weighted network utility and
weighted power consumption of SU sessions, with a minimum PU transmission rate
constraint and SU power consumption constraints. However, this problem is
highly challenging due to the nonconvex PU rate constraint. We propose a
solution that features linearization-based alternative optimization method and
a heuristic primal recovery method. We further develop a distributed algorithm
to jointly optimize covariance matrix at each transmitting SU node, bandwidth
allocation at each SU link, rate control at each session source and
multihop/multi-path routing. Extensive simulation results demonstrate that the
performance of the proposed distributed algorithm is close to that of the
centralized algorithm, and the proposed framework provides an efficient way to
significantly save power consumption, while achieving the network utility very
close to that achieved with full power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1969</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1969</id><created>2014-10-07</created><authors><author><keyname>Cao</keyname><forenames>Xianghui</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangwei</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author></authors><title>Energy Efficient Spectrum Sensing for State Estimation over A Wireless
  Channel</title><categories>cs.NI math.OC</categories><comments>4 pages, 6 figures, accepted to IEEE GlobalSIP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of remote estimation over wireless channel is strongly
affected by sensor data losses due to interference. Although the impact of
interference can be alleviated by performing spectrum sensing and then
transmitting only when the channel is clear, the introduction of spectrum
sensing also incurs extra energy expenditure. In this paper, we investigate the
problem of energy efficient spectrum sensing for state estimation of a general
linear dynamic system, and formulate an optimization problem which minimizes
the total sensor energy consumption while guaranteeing a desired level of
estimation performance. The optimal solution is evaluated through both
analytical and simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1972</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1972</id><created>2014-10-07</created><authors><author><keyname>Zhao</keyname><forenames>Xiang-Yu</forenames></author><author><keyname>Huang</keyname><forenames>Bin</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Zhang</keyname><forenames>Hai-Feng</forenames></author><author><keyname>Chen</keyname><forenames>Duan-Bing</forenames></author></authors><title>Identifying effective multiple spreaders by coloring complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 6 figures</comments><doi>10.1209/0295-5075/108/68005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to identify influential nodes in social networks is of theoretical
significance, which relates to how to prevent epidemic spreading or cascading
failure, how to accelerate information diffusion, and so on. In this Letter, we
make an attempt to find \emph{effective multiple spreaders} in complex networks
by generalizing the idea of the coloring problem in graph theory to complex
networks. In our method, each node in a network is colored by one kind of color
and nodes with the same color are sorted into an independent set. Then, for a
given centrality index, the nodes with the highest centrality in an independent
set are chosen as multiple spreaders. Comparing this approach with the
traditional method, in which nodes with the highest centrality from the
\emph{entire} network perspective are chosen, we find that our method is more
effective in accelerating the spreading process and maximizing the spreading
coverage than the traditional method, no matter in network models or in real
social networks. Meanwhile, the low computational complexity of the coloring
algorithm guarantees the potential applications of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1973</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1973</id><created>2014-10-07</created><updated>2014-11-29</updated><authors><author><keyname>Xu</keyname><forenames>Weiqiang</forenames></author><author><keyname>Zhang</keyname><forenames>Yushu</forenames></author><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Energy Management and Cross Layer Optimization for Wireless Sensor
  Network Powered by Heterogeneous Energy Sources</title><categories>cs.NI</categories><comments>submitted to IEEE Transactions on Wireless Communications, Under
  Second Round Review after Major Revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, utilizing renewable energy for wireless system has attracted
extensive attention. However, due to the instable energy supply and the limited
battery capacity, renewable energy cannot guarantee to provide the perpetual
operation for wireless sensor networks (WSN). The coexistence of renewable
energy and electricity grid is expected as a promising energy supply manner to
remain function for a potentially infinite lifetime. In this paper, we propose
a new system model suitable for WSN, taking into account multiple energy
consumptions due to sensing, transmission and reception, heterogeneous energy
supplies from renewable energy, electricity grid and mixed energy, and
multidimension stochastic natures due to energy harvesting profile, electricity
price and channel condition. A discrete-time stochastic cross-layer
optimization problem is formulated to achieve the optimal trade-off between the
time-average rate utility and electricity cost subject to the data and energy
queuing stability constraints. The Lyapunov drift-plus-penalty with
perturbation technique and block coordinate descent method is applied to obtain
a fully distributed and low-complexity cross-layer algorithm only requiring
knowledge of the instantaneous system state. The explicit trade-off between the
optimization objective and queue backlog is theoretically proven. Finally, the
extensive simulations verify the theoretic claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1980</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1980</id><created>2014-10-08</created><updated>2015-01-29</updated><authors><author><keyname>Menotti</keyname><forenames>David</forenames></author><author><keyname>Chiachia</keyname><forenames>Giovani</forenames></author><author><keyname>Pinto</keyname><forenames>Allan</forenames></author><author><keyname>Schwartz</keyname><forenames>William Robson</forenames></author><author><keyname>Pedrini</keyname><forenames>Helio</forenames></author><author><keyname>Falcao</keyname><forenames>Alexandre Xavier</forenames></author><author><keyname>Rocha</keyname><forenames>Anderson</forenames></author></authors><title>Deep Representations for Iris, Face, and Fingerprint Spoofing Detection</title><categories>cs.CV</categories><comments>Pre-print of article that will appear in the IEEE Transactions on
  Information Forenseics and Security (T.IFS), Special Issue on Biometric
  Spoofing and Countermeasures, vol 10, n. 4, April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometrics systems have significantly improved person identification and
authentication, playing an important role in personal, national, and global
security. However, these systems might be deceived (or &quot;spoofed&quot;) and, despite
the recent advances in spoofing detection, current solutions often rely on
domain knowledge, specific biometric reading systems, and attack types. We
assume a very limited knowledge about biometric spoofing at the sensor to
derive outstanding spoofing detection systems for iris, face, and fingerprint
modalities based on two deep learning approaches. The first approach consists
of learning suitable convolutional network architectures for each domain, while
the second approach focuses on learning the weights of the network via
back-propagation. We consider nine biometric spoofing benchmarks --- each one
containing real and fake samples of a given biometric modality and attack type
--- and learn deep representations for each benchmark by combining and
contrasting the two learning approaches. This strategy not only provides better
comprehension of how these approaches interplay, but also creates systems that
exceed the best known results in eight out of the nine benchmarks. The results
strongly indicate that spoofing detection systems based on convolutional
networks can be robust to attacks already known and possibly adapted, with
little effort, to image-based attacks that are yet to come.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.1995</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.1995</id><created>2014-10-08</created><authors><author><keyname>Kumar</keyname><forenames>V. Kiran</forenames><affiliation>Dravidian University, Kuppam</affiliation></author></authors><title>Semantic Web Approach towards Interoperability and Privacy issues in
  Social Networks</title><categories>cs.SE cs.SI</categories><comments>5 Pages</comments><doi>10.5121/ijwsc.2014.5302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Social Web is a set of social relations that link people through World
Wide Web. This Social Web encompasses how the websites and software are
designed and developed to support social relations. The new paradigms, tools
and web services introduced by Social Web are widely accepted by internet
users. The main drawbacks of these tools are it acts as independent data silos;
hence interoperability among applications is a complex issue. This paper
focuses on this issue and how best we can use semantic web technologies to
achieve interoperability among applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2006</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2006</id><created>2014-10-08</created><updated>2015-08-26</updated><authors><author><keyname>Meissen</keyname><forenames>Chris</forenames></author><author><keyname>Lessard</keyname><forenames>Laurent</forenames></author><author><keyname>Arcak</keyname><forenames>Murat</forenames></author><author><keyname>Packard</keyname><forenames>Andrew</forenames></author></authors><title>Compositional Performance Certification of Interconnected Systems using
  ADMM</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A compositional performance certification method is presented for
interconnected systems using subsystem dissipativity properties and the
interconnection structure. A large-scale optimization problem is formulated to
search for the most relevant dissipativity properties. The alternating
direction method of multipliers (ADMM) is employed to decompose and solve this
problem, and is demonstrated on several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2011</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2011</id><created>2014-10-08</created><updated>2015-09-08</updated><authors><author><keyname>Francis</keyname><forenames>Maria</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>On Ideal Lattices, Gr\&quot;obner Bases and Generalized Hash Functions</title><categories>cs.SC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we draw connections between ideal lattices and multivariate
polynomial rings over integers using Gr\&quot;obner bases. Ideal lattices are ideals
in the residue class ring, $\mathbb{Z}[x]/\langle f \rangle$ (here $f$ is a
monic polynomial), and cryptographic primitives have been built based on these
objects. As ideal lattices in the univariate case are generalizations of cyclic
lattices, we introduce the notion of multivariate cyclic lattices and show that
multivariate ideal lattices are indeed a generalization of them. Based on
multivariate ideal lattices, we establish the existence of collision resistant
hash functions using Gr\&quot;obner basis techniques. For the construction of hash
functions, we define a worst case problem, shortest substitution problem w.r.t.
an ideal in $\mathbb{Z}[x_1,\ldots, x_n]$, and establish hardness results using
functional fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2013</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2013</id><created>2014-10-08</created><authors><author><keyname>Albkerat</keyname><forenames>A.</forenames></author><author><keyname>Issac</keyname><forenames>B.</forenames></author></authors><title>Analysis of IPv6 Transition Technologies</title><categories>cs.NI</categories><comments>pages 19-38, Online link:
  http://airccse.org/journal/cnc/6514cnc02.pdf</comments><journal-ref>International Journal of Computer Networks and Communications
  (IJCNC), 6(5), 19-38.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently IPv6 is extremely popular with companies, organizations and
Internet service providers (ISP) due to the limitations of IPv4. In order to
prevent an abrupt change from IPv4 to IPv6, three mechanisms will be used to
provide a smooth transition from IPv4 to IPv6 with minimum effect on the
network. These mechanisms are Dual-Stack, Tunnel and Translation. This research
will shed the light on IPv4 and IPv6 and assess the automatic and manual
transition strategies of the IPv6 by comparing their performances in order to
show how the transition strategy affects network behaviour. The experiment will
be executed using OPNET Modeler that simulates a network containing a Wide Area
Network (WAN), a Local Area Network (LAN), hosts and servers. The results will
be presented in graphs and tables, with further explanation. The experiment
will use different measurements such as throughput, latency (delay), queuing
delay, and TCP delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2022</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2022</id><created>2014-10-08</created><updated>2015-09-16</updated><authors><author><keyname>Puppis</keyname><forenames>Gabriele</forenames><affiliation>LaBRI - CNRS</affiliation></author><author><keyname>Colcombet</keyname><forenames>Thomas</forenames><affiliation>LIAFA - CNRS</affiliation></author><author><keyname>Ley</keyname><forenames>Clemens</forenames><affiliation>Independent researcher</affiliation></author></authors><title>Logics with rigidly guarded data tests</title><categories>cs.FL cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:10) 2015</journal-ref><doi>10.2168/LMCS-11(3:10)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of orbit finite data monoid was recently introduced by Bojanczyk
as an algebraic object for defining recognizable languages of data words.
Following Buchi's approach, we introduce a variant of monadic second-order
logic with data equality tests that captures precisely the data languages
recognizable by orbit finite data monoids. We also establish, following this
time the approach of Schutzenberger, McNaughton and Papert, that the
first-order fragment of this logic defines exactly the data languages
recognizable by aperiodic orbit finite data monoids. Finally, we consider
another variant of the logic that can be interpreted over generic structures
with data. The data languages defined in this variant are also recognized by
unambiguous finite memory automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2023</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2023</id><created>2014-10-08</created><authors><author><keyname>Edemacu</keyname><forenames>Kennedy</forenames></author><author><keyname>Euku</keyname><forenames>Martin</forenames></author><author><keyname>Ssekibuule</keyname><forenames>Richard</forenames></author></authors><title>Packet Drop Attack Detection Techniques in Wireless Ad hoc Networks: A
  Review</title><categories>cs.CR</categories><comments>12 pages</comments><msc-class>68-XX</msc-class><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.6, No.5, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless ad hoc networks have gained lots of attention due to their ease and
low cost of deployment. This has made ad hoc networks of great importance in
numerous military and civilian applications. But, the lack of centralized
management of these networks makes them vulnerable to a number of security
attacks. One of the attacks is packet drop attack, where a compromised node
drops packets maliciously. Several techniques have been proposed to detect the
packet drop attack in wireless ad hoc networks. Therefore, in this paper we
review some of the packet drop attack detection techniques and comparatively
analyze them basing on; their ability to detect the attack under different
attack strategies (partial and or cooperate attacks), environments and the
computational and communication overheads caused in the process of detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2031</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2031</id><created>2014-10-08</created><updated>2015-02-27</updated><authors><author><keyname>Siemon</keyname><forenames>A.</forenames></author><author><keyname>Menzel</keyname><forenames>S.</forenames></author><author><keyname>Waser</keyname><forenames>R.</forenames></author><author><keyname>Linn</keyname><forenames>E.</forenames></author></authors><title>A Complementary Resistive Switch-based Crossbar Array Adder</title><categories>cs.ET</categories><comments>12 pages, accepted for IEEE Journal on Emerging and Selected Topics
  in Circuits and Systems (JETCAS), issue on Computing in Emerging Technologies</comments><doi>10.1109/JETCAS.2015.2398217</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Redox-based resistive switching devices (ReRAM) are an emerging class of
non-volatile storage elements suited for nanoscale memory applications. In
terms of logic operations, ReRAM devices were suggested to be used as
programmable interconnects, large-scale look-up tables or for sequential logic
operations. However, without additional selector devices these approaches are
not suited for use in large scale nanocrossbar memory arrays, which is the
preferred architecture for ReRAM devices due to the minimum area consumption.
To overcome this issue for the sequential logic approach, we recently
introduced a novel concept, which is suited for passive crossbar arrays using
complementary resistive switches (CRSs). CRS cells offer two high resistive
storage states, and thus, parasitic sneak currents are efficiently avoided.
However, until now the CRS-based logic-in-memory approach was only shown to be
able to perform basic Boolean logic operations using a single CRS cell. In this
paper, we introduce two multi-bit adder schemes using the CRS-based
logic-in-memory approach. We proof the concepts by means of SPICE simulations
using a dynamical memristive device model of a ReRAM cell. Finally, we show the
advantages of our novel adder concept in terms of step count and number of
devices in comparison to a recently published adder approach, which applies the
conventional ReRAM-based sequential logic concept introduced by Borghetti et
al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2045</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2045</id><created>2014-10-08</created><authors><author><keyname>Mandal</keyname><forenames>Ashis Kumar</forenames></author><author><keyname>Sen</keyname><forenames>Rikta</forenames></author></authors><title>Supervised learning Methods for Bangla Web Document Categorization</title><categories>cs.CL cs.LG</categories><comments>13 pages, International Journal of Artificial Intelligence &amp;
  Applications (IJAIA), Vol. 5, No. 5, September 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the use of machine learning approaches, or more
specifically, four supervised learning Methods, namely Decision Tree(C 4.5),
K-Nearest Neighbour (KNN), Na\&quot;ive Bays (NB), and Support Vector Machine (SVM)
for categorization of Bangla web documents. This is a task of automatically
sorting a set of documents into categories from a predefined set. Whereas a
wide range of methods have been applied to English text categorization,
relatively few studies have been conducted on Bangla language text
categorization. Hence, we attempt to analyze the efficiency of those four
methods for categorization of Bangla documents. In order to validate, Bangla
corpus from various websites has been developed and used as examples for the
experiment. For Bangla, empirical results support that all four methods produce
satisfactory performance with SVM attaining good result in terms of high
dimensional and relatively noisy document feature vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2056</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2056</id><created>2014-10-08</created><authors><author><keyname>Rahkar-Farshi</keyname><forenames>Taymaz</forenames></author><author><keyname>Behjat-Jamal</keyname><forenames>Sara</forenames></author><author><keyname>Feizi-Derakhshi</keyname><forenames>Mohammad-Reza</forenames></author></authors><title>An improved multimodal PSO method based on electrostatic interaction
  using n- nearest-neighbor local search</title><categories>cs.AI</categories><comments>10 pages, 8 figures, International Journal of Artificial Intelligence
  &amp; Applications (IJAIA), Vol. 5, No. 5, September 2014</comments><doi>10.5121/ijaia.2014.5506</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper, an improved multimodal optimization (MMO) algorithm,called
LSEPSO,has been proposed. LSEPSO combined Electrostatic Particle Swarm
Optimization (EPSO) algorithm and a local search method and then made some
modification on them. It has been shown to improve global and local optima
finding ability of the algorithm. This algorithm useda modified local search to
improve particle's personal best, which used n-nearest-neighbour instead of
nearest-neighbour. Then, by creating n new points among each particle and n
nearest particles, it tried to find a point which could be the alternative of
particle's personal best. This method prevented particle's attenuation and
following a specific particle by its neighbours. The performed tests on a
number of benchmark functions clearly demonstrated that the improved algorithm
is able to solve MMO problems and outperform other tested algorithms in this
article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2058</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2058</id><created>2014-10-08</created><authors><author><keyname>Aldlawie</keyname><forenames>Adil H. M.</forenames></author><author><keyname>QasMarrogy</keyname><forenames>Ghassan A.</forenames></author></authors><title>Performance evaluation of the effect ofnoise power jammer on the mobile
  bluetooth network</title><categories>cs.NI</categories><comments>8 pages, 7 figures, http://airccse.org/journal/ijc2014.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network jammers can uncover the reliability of the wireless networks, where
the position of the jammer node permit the network to cope with jamming
leveraging varieties of defence strategies. Wireless Bluetooth network is a
short range network, where the Bluetooth transceiver is operating in the 2.4
GHz ISM (industrial, scientific and medical) radio band, and a special type of
frequency hopping pattern was included with the specification of the Bluetooth
technology to offer the necessity requirement for the matching Radio Frequency
RF channels. This paper calculates and evaluates the effect of noise power
jammer and follower jammer on the Personal Area Network (PAN) Bluetooth
network. The results prove that the effect on the Bluetooth network depends on
many factors such as power density, distance and immunity technique, which is
used in many Bluetooth networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2063</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2063</id><created>2014-10-08</created><authors><author><keyname>Costantini</keyname><forenames>Stefania</forenames></author></authors><title>Committment-Based Data-Aware Multi-Agent-Contexts Systems</title><categories>cs.AI</categories><comments>Draft of a paper submitted to an International Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication and interaction among agents have been the subject of extensive
investigation since many years. Commitment-based communication, where
communicating agents are seen as a debtor agent who is committed to a creditor
agent to bring about something (possibly under some conditions) is now very
well-established. The approach of DACMAS (Data-Aware Commitment-based MAS)
lifts commitment-related approaches proposed in the literature from a
propositional to a first-order setting via the adoption the DRL-Lite
Description Logic. Notably, DACMASs provide, beyond commitments, simple forms
of inter-agent event-based communication. Yet, the aspect is missing of making
a MAS able to acquire knowledge from contexts which are not agents and which
are external to the MAS. This topic is coped with in Managed MCSs (Managed
Multi-Context Systems), where however exchanges are among knowledge bases and
not agents. In this paper, we propose the new approach of DACmMCMASs
(Data-Aware Commitment-based managed Multi- Context MAS), so as to obtain a
commitment-based first-order agent system which is able to interact with
heterogeneous external information sources. We show that DACmMCMASs retain the
nice formal properties of the original approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2065</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2065</id><created>2014-10-08</created><authors><author><keyname>Dorta-Gonzalez</keyname><forenames>Pablo</forenames></author><author><keyname>Dorta-Gonzalez</keyname><forenames>Maria Isabel</forenames></author><author><keyname>Suarez-Vega</keyname><forenames>Rafael</forenames></author></authors><title>An approach to the author citation potential: Measures of scientific
  performance which are invariant across scientific fields</title><categories>cs.DL</categories><comments>31 pages, 4 figures and 7 tables. arXiv admin note: text overlap with
  arXiv:1402.6317, arXiv:1304.5101; and with arXiv:1003.2167 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The citation potential is a measure of the probability of being cited.
Obviously, it is different among fields of science, social science, and
humanities because of systematic differences in publication and citation
behaviour across disciplines. In the past, the citation potential was studied
at journal level considering the average number of references in established
groups of journals (for example, the crown indicator is based on the journal
subject categories in the Web of Science database). In this paper, some
characterizations of the author?s scientific research through three different
research dimensions are proposed: production (journal papers), impact (journal
citations), and reference (bibliographical sources). Then, we propose different
measures of the citation potential for authors based on a proportion of these
dimensions. An empirical application, in a set of 120 randomly selected highly
productive authors from the CSIC Research Centre (Spain) in four subject areas,
shows that the ratio between production and impact dimensions is a normalized
measure of the citation potential at the level of individual authors. Moreover,
this ratio reduces the between-group variance in relation to the within-group
variance in a higher proportion than the rest of the indicators analysed.
Furthermore, it is consistent with the type of journal impact indicator used. A
possible application of this result is in the selection and promotion process
within interdisciplinary institutions, since it allows comparisons of authors
based on their particular scientific research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2075</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2075</id><created>2014-10-08</created><authors><author><keyname>Le</keyname><forenames>Van Bang</forenames></author><author><keyname>Oversberg</keyname><forenames>Andrea</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author></authors><title>Squares of $3$-sun-free split graphs</title><categories>cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The square of a graph $G$, denoted by $G^2$, is obtained from $G$ by putting
an edge between two distinct vertices whenever their distance is two. Then $G$
is called a square root of $G^2$. Deciding whether a given graph has a square
root is known to be NP-complete, even if the root is required to be a split
graph, that is, a graph in which the vertex set can be partitioned into a
stable set and a clique.
  We give a wide range of polynomial time solvable cases for the problem of
recognizing if a given graph is the square of some special kind of split graph.
To the best of our knowledge, our result properly contains all previously known
such cases. Our polynomial time algorithms are build on a structural
investigation of graphs that admit a split square root that is 3-sun-free, and
may pave the way toward a dichotomy theorem for recognizing squares of
(3-sun-free) split graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2082</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2082</id><created>2014-10-08</created><updated>2014-10-09</updated><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Sun</keyname><forenames>Maosong</forenames></author></authors><title>Contrastive Unsupervised Word Alignment with Non-Local Features</title><categories>cs.CL</categories><comments>Added the missing Table 1; corrected a typo in Related Work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word alignment is an important natural language processing task that
indicates the correspondence between natural languages. Recently, unsupervised
learning of log-linear models for word alignment has received considerable
attention as it combines the merits of generative and discriminative
approaches. However, a major challenge still remains: it is intractable to
calculate the expectations of non-local features that are critical for
capturing the divergence between natural languages. We propose a contrastive
approach that aims to differentiate observed training examples from noises. It
not only introduces prior knowledge to guide unsupervised learning but also
cancels out partition functions. Based on the observation that the probability
mass of log-linear models for word alignment is usually highly concentrated, we
propose to use top-n alignments to approximate the expectations with respect to
posterior distributions. This allows for efficient and accurate calculation of
expectations of non-local features. Experiments show that our approach achieves
significant improvements over state-of-the-art unsupervised word alignment
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2085</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2085</id><created>2014-10-08</created><authors><author><keyname>Chandra</keyname><forenames>Ashish</forenames></author><author><keyname>Suaib</keyname><forenames>Mohammad</forenames></author><author><keyname>Beg</keyname><forenames>Dr. Rizwan</forenames></author></authors><title>Low cost page quality factors to detect web spam</title><categories>cs.IR</categories><journal-ref>Informatics Engineering, an International Journal (IEIJ) ,Vol.2,
  No.3, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web spam is a big challenge for quality of search engine results. It is very
important for search engines to detect web spam accurately. In this paper we
present 32 low cost quality factors to classify spam and ham pages on real time
basis. These features can be divided in to three categories: (i) URL features,
(ii) Content features, and (iii) Link features. We developed a classifier using
Resilient Back-propagation learning algorithm of neural network and obtained
good accuracy. This classifier can be applied to search engine results on real
time because calculation of these features require very little CPU resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2087</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2087</id><created>2014-10-08</created><updated>2015-09-03</updated><authors><author><keyname>Feghhi</keyname><forenames>Saman</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>A Web Traffic Analysis Attack Using Only Timing Information</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an attack against encrypted web traffic that makes use only of
packet timing information on the uplink. This attack is therefore impervious to
existing packet padding defences. In addition, unlike existing approaches this
timing-only attack does not require knowledge of the start/end of web fetches
and so is effective against traffic streams. We demonstrate the effectiveness
of the attack against both wired and wireless traffic, achieving mean success
rates in excess of 90%. In addition to being of interest in its own right, this
timing-only attack serves to highlight deficiencies in existing defences and so
to areas where it would be beneficial for VPN designers to focus further
attention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2090</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2090</id><created>2014-10-08</created><updated>2015-07-27</updated><authors><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author></authors><title>Power-Constrained Sparse Gaussian Linear Dimensionality Reduction over
  Noisy Channels</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Signal Processing
  (16 pages)</comments><doi>10.1109/TSP.2015.2455521</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate power-constrained sensing matrix design in a
sparse Gaussian linear dimensionality reduction framework. Our study is carried
out in a single--terminal setup as well as in a multi--terminal setup
consisting of orthogonal or coherent multiple access channels (MAC). We adopt
the mean square error (MSE) performance criterion for sparse source
reconstruction in a system where source-to-sensor channel(s) and
sensor-to-decoder communication channel(s) are noisy. Our proposed sensing
matrix design procedure relies upon minimizing a lower-bound on the MSE in
single-- and multiple--terminal setups. We propose a three-stage sensing matrix
optimization scheme that combines semi-definite relaxation (SDR) programming, a
low-rank approximation problem and power-rescaling. Under certain conditions,
we derive closed-form solutions to the proposed optimization procedure. Through
numerical experiments, by applying practical sparse reconstruction algorithms,
we show the superiority of the proposed scheme by comparing it with other
relevant methods. This performance improvement is achieved at the price of
higher computational complexity. Hence, in order to address the complexity
burden, we present an equivalent stochastic optimization method to the problem
of interest that can be solved approximately, while still providing a superior
performance over the popular methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2100</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2100</id><created>2014-10-08</created><authors><author><keyname>Xianyan</keyname><forenames>Wu</forenames></author><author><keyname>Qi</keyname><forenames>Han</forenames></author><author><keyname>Dan</keyname><forenames>Le</forenames></author><author><keyname>Xiamu</keyname><forenames>Niu</forenames></author></authors><title>A New Method for Estimating the Widths of JPEG Images</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image width is important for image understanding. We propose a novel method
to estimate widths for JPEG images when their widths are not available. The key
idea is that the distance between two decoded MCUs (Minimum Coded Unit)
adjacent in the vertical direction is usually small, which is measured by the
average Euclidean distance between the pixels from the bottom row of the top
MCU and the top row of the bottom MCU. On PASCAL VOC 2010 challenge dataset and
USC-SIPI image database, experimental results show the high performance of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2105</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2105</id><created>2014-10-08</created><updated>2015-08-31</updated><authors><author><keyname>Creusefond</keyname><forenames>J.</forenames></author><author><keyname>Largillier</keyname><forenames>T.</forenames></author><author><keyname>Peyronnet</keyname><forenames>S.</forenames></author></authors><title>Finding compact communities in large graphs</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 8 figures</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents an efficient hierarchical clustering algorithm that
solves the problem of core community detection. It is a variant of the standard
community detection problem in which we are particularly interested in the
connected core of communities. To provide a solution to this problem, we
question standard definitions on communities and provide alternatives. We also
propose a function called compactness, designed to assess the quality of a
solution to this problem. Our algorithm is based on a graph traversal
algorithm, the LexDFS. The time complexity of our method is in $O(n\times
log(n))$. Experiments show that our algorithm creates highly compact clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2121</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2121</id><created>2014-10-08</created><authors><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Squartini</keyname><forenames>Tiziano</forenames></author><author><keyname>Musmeci</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Puliga</keyname><forenames>Michelangelo</forenames></author><author><keyname>Gabrielli</keyname><forenames>Andrea</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author><author><keyname>Battiston</keyname><forenames>Stefano</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author></authors><title>Reconstructing topological properties of complex networks using the
  fitness model</title><categories>cs.SI physics.soc-ph q-fin.GN</categories><journal-ref>Social Informatics (series: Lec. Notes Comp. Science 8852/2015),
  pp. 323-333, Springer (edited by L. M. Aiello and D. McFarland) (2015)</journal-ref><doi>10.1007/978-3-319-15168-7_41</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major problem in the study of complex socioeconomic systems is represented
by privacy issues$-$that can put severe limitations on the amount of accessible
information, forcing to build models on the basis of incomplete knowledge. In
this paper we investigate a novel method to reconstruct global topological
properties of a complex network starting from limited information. This method
uses the knowledge of an intrinsic property of the nodes (indicated as
fitness), and the number of connections of only a limited subset of nodes, in
order to generate an ensemble of exponential random graphs that are
representative of the real systems and that can be used to estimate its
topological properties. Here we focus in particular on reconstructing the most
basic properties that are commonly used to describe a network: density of
links, assortativity, clustering. We test the method on both benchmark
synthetic networks and real economic and financial systems, finding a
remarkable robustness with respect to the number of nodes used for calibration.
The method thus represents a valuable tool for gaining insights on
privacy-protected systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2123</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2123</id><created>2014-10-08</created><authors><author><keyname>Grispos</keyname><forenames>George</forenames></author><author><keyname>Storer</keyname><forenames>Tim</forenames></author><author><keyname>Glisson</keyname><forenames>William Bradley</forenames></author></authors><title>Calm Before the Storm: The Challenges of Cloud Computing in Digital
  Forensics</title><categories>cs.CR</categories><journal-ref>G. Grispos, T. Storer, and W.B. Glisson (2012). Calm Before the
  Storm: The Challenges of Cloud Computing in Digital Forensics. International
  Journal of Digital Crime and Forensics, Volume 4, Issue 2, Pages 28-48</journal-ref><doi>10.4018/jdcf.2012040103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a rapidly evolving information technology (IT) phenomenon.
Rather than procure, deploy and manage a physical IT infrastructure to host
their software applications, organizations are increasingly deploying their
infrastructure into remote, virtualized environments, often hosted and managed
by third parties. This development has significant implications for digital
forensic investigators, equipment vendors, law enforcement, as well as
corporate compliance and audit departments (among others). Much of digital
forensic practice assumes careful control and management of IT assets
(particularly data storage) during the conduct of an investigation. This paper
summarises the key aspects of cloud computing and analyses how established
digital forensic procedures will be invalidated in this new environment.
Several new research challenges addressing this changing context are also
identified and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2128</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2128</id><created>2014-10-08</created><updated>2014-12-07</updated><authors><author><keyname>Bertrand</keyname><forenames>Nathalie</forenames><affiliation>INRIA Rennes - Bretagne Atlantique</affiliation></author><author><keyname>Bouyer</keyname><forenames>Patricia</forenames><affiliation>LSV &amp; ENS Cachan</affiliation></author><author><keyname>Brihaye</keyname><forenames>Thomas</forenames><affiliation>Universit&#xe9; de Mons</affiliation></author><author><keyname>Menet</keyname><forenames>Quentin</forenames><affiliation>Universit&#xe9; de Mons</affiliation></author><author><keyname>Baier</keyname><forenames>Christel</forenames><affiliation>Technische Universit&#xe4;t Dresden</affiliation></author><author><keyname>Groesser</keyname><forenames>Marcus</forenames><affiliation>Technische Universit&#xe4;t Dresden</affiliation></author><author><keyname>Jurdzinski</keyname><forenames>Marcin</forenames><affiliation>University of Warwick</affiliation></author></authors><title>Stochastic Timed Automata</title><categories>cs.LO</categories><comments>40 pages + appendix</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  9, 2014) lmcs:1092</journal-ref><doi>10.2168/LMCS-10(4:6)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stochastic timed automaton is a purely stochastic process defined on a
timed automaton, in which both delays and discrete choices are made randomly.
We study the almost-sure model-checking problem for this model, that is, given
a stochastic timed automaton A and a property $\Phi$, we want to decide whether
A satisfies $\Phi$ with probability 1. In this paper, we identify several
classes of automata and of properties for which this can be decided. The proof
relies on the construction of a finite abstraction, called the thick graph,
that we interpret as a finite Markov chain, and for which we can decide the
almost-sure model-checking problem. Correctness of the abstraction holds when
automata are almost-surely fair, which we show, is the case for two large
classes of systems, single- clock automata and so-called weak-reactive
automata. Techniques employed in this article gather tools from real-time
verification and probabilistic verification, as well as topological games
played on timed automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2131</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2131</id><created>2014-10-08</created><authors><author><keyname>Aman</keyname><forenames>Waqas</forenames></author></authors><title>A Framework for Analysis and Comparison of Dynamic Malware Analysis
  Tools</title><categories>cs.CR</categories><comments>12 pages</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  09/2014; 6(5):63-74</journal-ref><doi>10.5121/ijnsa.2014.6505</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Malware writers have employed various obfuscation and polymorphism techniques
to thwart static analysis approaches and bypassing antivirus tools. Dynamic
analysis techniques, however, have essentially overcome these deceits by
observing the actual behaviour of the code execution. In this regard, various
methods, techniques and tools have been proposed. However, because of the
diverse concepts and strategies used in the implementation of these methods and
tools, security researchers and malware analysts find it difficult to select
the required optimum tool to investigate the behaviour of a malware and to
contain the associated risk for their study. Focusing on two dynamic analysis
techniques: Function Call monitoring and Information Flow Tracking, this paper
presents a comparison framework for dynamic malware analysis tools. The
framework will assist the researchers and analysts to recognize the tools
implementation strategy, analysis approach, system wide analysis support and
its overall handling of binaries, helping them to select a suitable and
effective one for their study and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2135</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2135</id><created>2014-10-08</created><authors><author><keyname>Rodrigues</keyname><forenames>Carlo Kleber da Silva</forenames></author></authors><title>On the Optimization of BitTorrent-Like Protocols for Interactive
  On-Demand Streaming Systems</title><categories>cs.NI</categories><comments>20 pages</comments><msc-class>90B18</msc-class><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC), v. 6 (5), September, p. 39 - 58, 2014</journal-ref><doi>10.5121/ijcnc.2014.6503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes two novel optimized BitTorrent-like protocols for
interactive multimedia streaming: the Simple Interactive Streaming Protocol
(SISP) and the Exclusive Interactive Streaming Protocol (EISP). The former
chiefly seeks a trade-off between playback continuity and data diversity, while
the latter is mostly focused on playback continuity. To assure a thorough and
up-to-date approach, related work is carefully examined and important open
issues, concerning the design of BitTorrent-like algorithms, are analyzed as
well. Through simulations, in a variety of near-real file replication
scenarios, the novel protocols are evaluated using distinct performance
metrics. Among the major findings, the final results show that the two novel
proposals are efficient and, besides, focusing on playback continuity ends up
being the best design concept to achieve high quality of service. Lastly,
avenues for further research are included at the end of this paper as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2144</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2144</id><created>2014-10-08</created><updated>2015-08-03</updated><authors><author><keyname>Chang</keyname><forenames>Chih-Hung</forenames></author></authors><title>k-Mixing Properties of Multidimensional Cellular Automata</title><categories>cs.IT math.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the $k$-mixing property of a multidimensional
cellular automaton. Suppose $F$ is a cellular automaton with the local rule $f$
defined on a $d$-dimensional convex hull $\mathcal{C}$ which is generated by an
apex set $C$. Then $F$ is $k$-mixing with respect to the uniform Bernoulli
measure for all positive integer $k$ if $f$ is a permutation at some apex in
$C$. An algorithm called the \emph{Mixing Algorithm} is proposed to verify if a
local rule $f$ is permutive at some apex in $C$. Moreover, the proposed
conditions are optimal. An application of this investigation is to construct a
multidimensional ergodic linear cellular automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2146</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2146</id><created>2014-10-05</created><authors><author><keyname>Khaleghi</keyname><forenames>Ehsan Ebrahimi</forenames><affiliation>LTCI</affiliation></author><author><keyname>Belfiore</keyname><forenames>Jean-Claude</forenames><affiliation>LTCI</affiliation></author></authors><title>Compute-and-Forward for the Interference Channel: Diversity Precoding</title><categories>cs.IT math.IT</categories><comments>Iran Workshop on Communication and Information Theory (IWCIT), 2014,
  Teheran : Iran, Islamic Republic Of (2014)</comments><proxy>ccsd</proxy><doi>10.1109/IWCIT.2014.6842498</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference Alignment is a new solution to over- come the problem of
interference in multiuser wireless com- munication systems. Recently, the
Compute-and-Forward (CF) transform has been proposed to approximate the
capacity of K- user Gaussian Symmetric Interference Channel and practically
perform Interference Alignment in wireless networks. However, this technique
shows a random behavior in the achievable sum- rate, especially at high SNR. In
this work, the origin of this random behavior is analyzed and a novel precoding
technique based on the Golden Ratio is proposed to scale down the fadings
experiences by the achievable sum-rate at high SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2149</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2149</id><created>2014-10-04</created><authors><author><keyname>Bilisoly</keyname><forenames>Roger</forenames></author></authors><title>Language-based Examples in the Statistics Classroom</title><categories>cs.CL</categories><comments>Proceedings based on a Joint Statistical Meetings talk in 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistics pedagogy values using a variety of examples. Thanks to text
resources on the Web, and since statistical packages have the ability to
analyze string data, it is now easy to use language-based examples in a
statistics class. Three such examples are discussed here. First, many types of
wordplay (e.g., crosswords and hangman) involve finding words with letters that
satisfy a certain pattern. Second, linguistics has shown that idiomatic pairs
of words often appear together more frequently than chance. For example, in the
Brown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.)
Third, a pangram contains all the letters of the alphabet at least once. These
are searched for in Charles Dickens' A Christmas Carol, and their lengths are
compared to the expected value given by the unequal probability coupon
collector's problem as well as simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2167</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2167</id><created>2014-10-08</created><updated>2015-02-26</updated><authors><author><keyname>Nardi</keyname><forenames>Luigi</forenames></author><author><keyname>Bodin</keyname><forenames>Bruno</forenames></author><author><keyname>Zia</keyname><forenames>M. Zeeshan</forenames></author><author><keyname>Mawer</keyname><forenames>John</forenames></author><author><keyname>Nisbet</keyname><forenames>Andy</forenames></author><author><keyname>Kelly</keyname><forenames>Paul H. J.</forenames></author><author><keyname>Davison</keyname><forenames>Andrew J.</forenames></author><author><keyname>Luj&#xe1;n</keyname><forenames>Mikel</forenames></author><author><keyname>O'Boyle</keyname><forenames>Michael F. P.</forenames></author><author><keyname>Riley</keyname><forenames>Graham</forenames></author><author><keyname>Topham</keyname><forenames>Nigel</forenames></author><author><keyname>Furber</keyname><forenames>Steve</forenames></author></authors><title>Introducing SLAMBench, a performance and accuracy benchmarking
  methodology for SLAM</title><categories>cs.RO cs.CV cs.DC cs.PF</categories><comments>8 pages, ICRA 2015 conference paper</comments><journal-ref>http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7140009
  IEEE Xplore 2015</journal-ref><doi>10.1109/ICRA.2015.7140009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time dense computer vision and SLAM offer great potential for a new
level of scene modelling, tracking and real environmental interaction for many
types of robot, but their high computational requirements mean that use on mass
market embedded platforms is challenging. Meanwhile, trends in low-cost,
low-power processing are towards massive parallelism and heterogeneity, making
it difficult for robotics and vision researchers to implement their algorithms
in a performance-portable way. In this paper we introduce SLAMBench, a
publicly-available software framework which represents a starting point for
quantitative, comparable and validatable experimental research to investigate
trade-offs in performance, accuracy and energy consumption of a dense RGB-D
SLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP,
OpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-D
sequences with trajectory and scene ground truth for reliable accuracy
comparison of different implementation and algorithms. We present an analysis
and breakdown of the constituent algorithmic elements of KinectFusion, and
experimentally investigate their execution time on a variety of multicore and
GPUaccelerated platforms. For a popular embedded platform, we also present an
analysis of energy efficiency for different configuration alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2168</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2168</id><created>2014-10-07</created><authors><author><keyname>Gajduk</keyname><forenames>Andrej</forenames></author><author><keyname>Todorovski</keyname><forenames>Mirko</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>Improved steady-state stability of power grids with a communication
  infrastructure</title><categories>cs.SY math.OC nlin.AO</categories><comments>Submitted and currently under review in IEEE Transactions on Circuits
  and Systems - 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient control of power systems is becoming increasingly difficult as they
gain in complexity and size. We propose an automatic control strategy that
regulates the mechanical power output of the generators in a power grid based
on information obtained via a communication infrastructure.
  An algorithm that optimizes steady-state stability of a power grid by
iteratively adding communication links is presented. The proposed control
scheme is successfully applied to the IEEE New England and IEEE RTS 96 power
systems, leading to a significant increase in the steady-state stability of the
systems and an improvement in their overall robustness. The resulting
communication network topology differs significantly from the transmission grid
topology. This shows how complex the steady- state control for power systems
is, influenced by the generators configuration, the transmission network
topology, and the manner by which control is executed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2173</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2173</id><created>2014-08-07</created><authors><author><keyname>Aziz</keyname><forenames>K. A. A.</forenames></author><author><keyname>Abdullah</keyname><forenames>S. S.</forenames></author></authors><title>Face Detection Using Radial Basis Functions Neural Networks With Fixed
  Spread</title><categories>cs.CV</categories><comments>6 pages, The Second International Conference on Control,
  Instrumentation and Mechatronic Engineering (CIM09) Malacca, Malaysia, June
  2-3, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presented a face detection system using Radial Basis Function
Neural Networks With Fixed Spread Value. Face detection is the first step in
face recognition system. The purpose is to localize and extract the face region
from the background that will be fed into the face recognition system for
identification. General preprocessing approach was used for normalizing the
image and Radial Basis Function (RBF) Neural Network was used to distinguish
between face and non-face. RBF Neural Networks offer several advantages
compared to other neural network architecture such as they can be trained using
fast two stages training algorithm and the network possesses the property of
best approximation. The output of the network can be optimized by setting
suitable value of center and spread of the RBF. In this paper, fixed spread
value will be used. The Radial Basis Function Neural Network (RBFNN) used to
distinguish faces and non-faces and the evaluation of the system will be the
performance of detection, False Acceptance Rate (FAR), False Rejection Rate
(FRR) and the discriminative properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2175</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2175</id><created>2014-09-10</created><authors><author><keyname>Shrestha</keyname><forenames>Suman</forenames></author></authors><title>Image Denoising using New Adaptive Based Median Filters</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noise is a major issue while transferring images through all kinds of
electronic communication. One of the most common noise in electronic
communication is an impulse noise which is caused by unstable voltage. In this
paper, the comparison of known image denoising techniques is discussed and a
new technique using the decision based approach has been used for the removal
of impulse noise. All these methods can primarily preserve image details while
suppressing impulsive noise. The principle of these techniques is at first
introduced and then analysed with various simulation results using MATLAB. Most
of the previously known techniques are applicable for the denoising of images
corrupted with less noise density. Here a new decision based technique has been
presented which shows better performances than those already being used. The
comparisons are made based on visual appreciation and further quantitatively by
Mean Square error (MSE) and Peak Signal to Noise Ratio (PSNR) of different
filtered images..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.2176</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.2176</id><created>2014-10-07</created><authors><author><keyname>Gajduk</keyname><forenames>Andrej</forenames></author><author><keyname>Todorovski</keyname><forenames>Mirko</forenames></author><author><keyname>Kurths</keyname><forenames>Juergen</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>Improving power grid transient stability by plug-in electric vehicles</title><categories>cs.SY math.OC nlin.AO</categories><comments>15 pages, 4 figures, submitted to New Journal of Physics</comments><doi>10.1088/1367-2630/16/11/115011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plug-in electric vehicles (PEVs) can serve in discharge mode as distributed
energy and power resources operating as vehicle-to-grid (V2G) devices and in
charge mode as loads or grid-to-vehicle (G2V) devices. It has been documented
that PEVs serving as V2G systems can offer possible backup for renewable power
sources, can provide reactive power support, active power regulation, load
balancing, peak load shaving,% and current harmonic filtering, can provide
ancillary services as frequency control and spinning reserves, can improve grid
efficiency, stability, reliability, and generation dispatch, can reduce utility
operating costs and can generate revenue. Here we show that PEVs can even
improve power grid transient stability, that is, stability when the power grid
is subjected to large disturbances, including bus faults, generator and branch
tripping, and sudden large load changes. A control strategy that regulates the
power output of a fleet of PEVs based on the speed of generator turbines is
proposed and tested on the New England 10-unit 39-bus power system. By
regulating the power output of the PEVs we show that (1) speed and voltage
fluctuations resulting from large disturbances can be significantly reduced up
to 5 times, and (2) the critical clearing time can be extended by 20-40%.
Overall, the PEVs control strategy makes the power grid more robust.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="66000" completeListSize="102538">1122234|67001</resumptionToken>
</ListRecords>
</OAI-PMH>
