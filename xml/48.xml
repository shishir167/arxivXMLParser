<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:14:07Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|47001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1306.3976</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.3976</id><created>2013-06-17</created><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>Lifting $\ell_q$-optimization thresholds</title><categories>cs.IT math.IT math.OC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.3774,
  arXiv:1306.3770</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we look at a connection between the $\ell_q,0\leq q\leq 1$,
optimization and under-determined linear systems of equations with sparse
solutions. The case $q=1$, or in other words $\ell_1$ optimization and its a
connection with linear systems has been thoroughly studied in last several
decades; in fact, especially so during the last decade after the seminal works
\cite{CRT,DOnoho06CS} appeared. While current understanding of $\ell_1$
optimization-linear systems connection is fairly known, much less so is the
case with a general $\ell_q,0&lt;q&lt;1$, optimization. In our recent work
\cite{StojnicLqThrBnds10} we provided a study in this direction. As a result we
were able to obtain a collection of lower bounds on various $\ell_q,0\leq q\leq
1$, optimization thresholds. In this paper, we provide a substantial conceptual
improvement of the methodology presented in \cite{StojnicLqThrBnds10}.
Moreover, the practical results in terms of achievable thresholds are also
encouraging. As is usually the case with these and similar problems, the
methodology we developed emphasizes their a combinatorial nature and attempts
to somehow handle it. Although our results' main contributions should be on a
conceptual level, they already give a very strong suggestion that $\ell_q$
optimization can in fact provide a better performance than $\ell_1$, a fact
long believed to be true due to a tighter optimization relaxation it provides
to the original $\ell_0$ sparsity finding oriented original problem
formulation. As such, they in a way give a solid boost to further exploration
of the design of the algorithms that would be able to handle $\ell_q,0&lt;q&lt;1$,
optimization in a reasonable (if not polynomial) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.3977</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.3977</id><created>2013-06-17</created><updated>2015-07-16</updated><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>Compressed sensing of block-sparse positive vectors</title><categories>cs.IT math.IT math.OC</categories><comments>acknowledgement footnote added arXiv admin note: substantial text
  overlap with arXiv:1304.0001, arXiv:0907.3679, arXiv:1306.3801</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we revisit one of the classical problems of compressed sensing.
Namely, we consider linear under-determined systems with sparse solutions. A
substantial success in mathematical characterization of an $\ell_1$
optimization technique typically used for solving such systems has been
achieved during the last decade. Seminal works \cite{CRT,DOnoho06CS} showed
that the $\ell_1$ can recover a so-called linear sparsity (i.e. solve systems
even when the solution has a sparsity linearly proportional to the length of
the unknown vector). Later considerations \cite{DonohoPol,DonohoUnsigned} (as
well as our own ones \cite{StojnicCSetam09,StojnicUpper10}) provided the
precise characterization of this linearity. In this paper we consider the
so-called structured version of the above sparsity driven problem. Namely, we
view a special case of sparse solutions, the so-called block-sparse solutions.
Typically one employs $\ell_2/\ell_1$-optimization as a variant of the standard
$\ell_1$ to handle block-sparse case of sparse solution systems. We considered
systems with block-sparse solutions in a series of work
\cite{StojnicCSetamBlock09,StojnicUpperBlock10,StojnicICASSP09block,StojnicJSTSP09}
where we were able to provide precise performance characterizations if the
$\ell_2/\ell_1$-optimization similar to those obtained for the standard
$\ell_1$ optimization in \cite{StojnicCSetam09,StojnicUpper10}. Here we look at
a similar class of systems where on top of being block-sparse the unknown
vectors are also known to have components of the same sign. In this paper we
slightly adjust $\ell_2/\ell_1$-optimization to account for the known signs and
provide a precise performance characterization of such an adjustment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.3995</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.3995</id><created>2013-06-17</created><updated>2013-09-16</updated><authors><author><keyname>Gogolin</keyname><forenames>C.</forenames></author><author><keyname>Kliesch</keyname><forenames>M.</forenames></author><author><keyname>Aolita</keyname><forenames>L.</forenames></author><author><keyname>Eisert</keyname><forenames>J.</forenames></author></authors><title>Boson-Sampling in the light of sample complexity</title><categories>quant-ph cs.CC</categories><comments>22 pages, 1 figure, typos corrected and minor improvements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boson-Sampling is a classically computationally hard problem that can - in
principle - be efficiently solved with quantum linear optical networks. Very
recently, a rush of experimental activity has ignited with the aim of
developing such devices as feasible instances of quantum simulators. Even
approximate Boson-Sampling is believed to be hard with high probability if the
unitary describing the optical network is drawn from the Haar measure. In this
work we show that in this setup, with probability exponentially close to one in
the number of bosons, no symmetric algorithm can distinguish the Boson-Sampling
distribution from the uniform one from fewer than exponentially many samples.
This means that the two distributions are operationally indistinguishable
without detailed a priori knowledge. We carefully discuss the prospects of
efficiently using knowledge about the implemented unitary for devising
non-symmetric algorithms that could potentially improve upon this. We conclude
that due to the very fact that Boson-Sampling is believed to be hard, efficient
classical certification of Boson-Sampling devices seems to be out of reach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4009</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4009</id><created>2013-06-17</created><authors><author><keyname>Ivanov</keyname><forenames>Mikhail</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Brannstrom</keyname><forenames>Fredrik</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>On the Asymptotic Performance of Bit-Wise Decoders for Coded Modulation</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. Inf. Theory, vol. 60, no. 6, pp. 2796-2804, May. 2014</journal-ref><doi>10.1109/TIT.2014.2312726</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two decoder structures for coded modulation over the Gaussian and flat fading
channels are studied: the maximum likelihood symbol-wise decoder, and the
(suboptimal) bit-wise decoder based on the bit-interleaved coded modulation
paradigm. We consider a 16-ary quadrature amplitude constellation labeled by a
Gray labeling. It is shown that the asymptotic loss in terms of pairwise error
probability, for any two codewords caused by the bit-wise decoder, is bounded
by 1.25 dB. The analysis also shows that for the Gaussian channel the
asymptotic loss is zero for a wide range of linear codes, including all
rate-1/2 convolutional codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4022</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4022</id><created>2013-06-17</created><updated>2013-10-04</updated><authors><author><keyname>Sivan</keyname><forenames>Balasubramanian</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Vickrey Auctions for Irregular Distributions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic result of Bulow and Klemperer \cite{BK96} says that in a
single-item auction recruiting one more bidder and running the Vickrey auction
achieves a higher revenue than the optimal auction's revenue on the original
set of bidders, when values are drawn i.i.d. from a regular distribution. We
give a version of Bulow and Klemperer's result in settings where bidders'
values are drawn from non-i.i.d. irregular distributions. We do this by
modeling irregular distributions as some convex combination of regular
distributions. The regular distributions that constitute the irregular
distribution correspond to different population groups in the bidder
population. Drawing a bidder from this collection of population groups is
equivalent to drawing from some convex combination of these regular
distributions. We show that recruiting one extra bidder from each underlying
population group and running the Vickrey auction gives at least half of the
optimal auction's revenue on the original set of bidders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4029</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4029</id><created>2013-06-17</created><authors><author><keyname>Schneider</keyname><forenames>Stefan</forenames></author></authors><title>Satisfiability Algorithms for Restricted Circuit Classes</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, finding new satisfiability algorithms for various circuit
classes has been a very active line of research. Despite considerable progress,
we are still far away from a definite answer on which circuit classes allow
fast satisfiability algorithms. This survey takes a (far from exhaustive) look
at some recent satisfiability algorithms for a range of circuit classes and
high- lights common themes. A special focus is given to connections between
satisfiability algorithms and circuit lower bounds. A second focus is on
reductions from satisfiability algorithms to a range of polynomial time
problems, such as matrix multiplication and the Vector Domination Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4036</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4036</id><created>2013-06-17</created><updated>2014-01-14</updated><authors><author><keyname>Siddhardh</keyname><forenames>V. Sriram</forenames><affiliation>Sid</affiliation></author><author><keyname>Nadendla</keyname></author><author><keyname>Han</keyname><forenames>Yunghsiang S.</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Distributed Inference with M-ary Quantized Data in the Presence of
  Byzantine Attacks</title><categories>cs.IT cs.CR math.IT stat.AP</categories><comments>15 pages, 8 figures, 1 table, Revision submitted to IEEE Transactions
  on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of distributed inference with M-ary quantized data at the sensors
is investigated in the presence of Byzantine attacks. We assume that the
attacker does not have knowledge about either the true state of the phenomenon
of interest, or the quantization thresholds used at the sensors. Therefore, the
Byzantine nodes attack the inference network by modifying modifying the symbol
corresponding to the quantized data to one of the other M symbols in the
quantization alphabet-set and transmitting the false symbol to the fusion
center (FC). In this paper, we find the optimal Byzantine attack that blinds
any distributed inference network. As the quantization alphabet size increases,
a tremendous improvement in the security performance of the distributed
inference network is observed.
  We also investigate the problem of distributed inference in the presence of
resource-constrained Byzantine attacks. In particular, we focus our attention
on two problems: distributed detection and distributed estimation, when the
Byzantine attacker employs a highly-symmetric attack. For both the problems, we
find the optimal attack strategies employed by the attacker to maximally
degrade the performance of the inference network. A reputation-based scheme for
identifying malicious nodes is also presented as the network's strategy to
mitigate the impact of Byzantine threats on the inference performance of the
distributed sensor network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4037</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4037</id><created>2013-06-17</created><authors><author><keyname>Ferrada</keyname><forenames>H.</forenames></author><author><keyname>Gagie</keyname><forenames>T.</forenames></author><author><keyname>Hirvola</keyname><forenames>T.</forenames></author><author><keyname>Puglisi</keyname><forenames>S. J.</forenames></author></authors><title>Hybrid Indexes for Repetitive Datasets</title><categories>cs.DS</categories><doi>10.1098/rsta.2013.0137</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in DNA sequencing mean databases of thousands of human genomes will
soon be commonplace. In this paper we introduce a simple technique for reducing
the size of conventional indexes on such highly repetitive texts. Given upper
bounds on pattern lengths and edit distances, we preprocess the text with LZ77
to obtain a filtered text, for which we store a conventional index. Later,
given a query, we find all matches in the filtered text, then use their
positions and the structure of the LZ77 parse to find all matches in the
original text. Our experiments show this also significantly reduces query
times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4040</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4040</id><created>2013-06-17</created><authors><author><keyname>Sarraute</keyname><forenames>Carlos</forenames><affiliation>Core Security Technologies</affiliation><affiliation>ITBA</affiliation></author><author><keyname>Richarte</keyname><forenames>Gerardo</forenames><affiliation>Core Security Technologies</affiliation></author><author><keyname>Obes</keyname><forenames>Jorge Lucangeli</forenames><affiliation>UBA</affiliation></author></authors><title>An Algorithm to Find Optimal Attack Paths in Nondeterministic Scenarios</title><categories>cs.CR cs.AI</categories><comments>ACM Workshop on Artificial Intelligence and Security (AISec 2011), at
  ACM CCS Conference 2011</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  As penetration testing frameworks have evolved and have become more complex,
the problem of controlling automatically the pentesting tool has become an
important question. This can be naturally addressed as an attack planning
problem. Previous approaches to this problem were based on modeling the actions
and assets in the PDDL language, and using off-the-shelf AI tools to generate
attack plans. These approaches however are limited. In particular, the planning
is classical (the actions are deterministic) and thus not able to handle the
uncertainty involved in this form of attack planning.
  We herein contribute a planning model that does capture the uncertainty about
the results of the actions, which is modeled as a probability of success of
each action. We present efficient planning algorithms, specifically designed
for this problem, that achieve industrial-scale runtime performance (able to
solve scenarios with several hundred hosts and exploits). These algorithms take
into account the probability of success of the actions and their expected cost
(for example in terms of execution time, or network traffic generated).
  We thus show that probabilistic attack planning can be solved efficiently for
the scenarios that arise when assessing the security of large networks. Two
&quot;primitives&quot; are presented, which are used as building blocks in a framework
separating the overall problem into two levels of abstraction. We also present
the experimental results obtained with our implementation, and conclude with
some ideas for further work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4044</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4044</id><created>2013-06-17</created><updated>2013-06-19</updated><authors><author><keyname>Obes</keyname><forenames>Jorge Lucangeli</forenames><affiliation>Core Security Technologies</affiliation></author><author><keyname>Sarraute</keyname><forenames>Carlos</forenames><affiliation>Core Security Technologies</affiliation><affiliation>ITBA</affiliation></author><author><keyname>Richarte</keyname><forenames>Gerardo</forenames><affiliation>Core Security Technologies</affiliation></author></authors><title>Attack Planning in the Real World</title><categories>cs.CR cs.AI</categories><comments>SecArt'2010 at AAAI 2010, Atlanta, USA. July 12, 2010</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Assessing network security is a complex and difficult task. Attack graphs
have been proposed as a tool to help network administrators understand the
potential weaknesses of their network. However, a problem has not yet been
addressed by previous work on this subject; namely, how to actually execute and
validate the attack paths resulting from the analysis of the attack graph. In
this paper we present a complete PDDL representation of an attack model, and an
implementation that integrates a planner into a penetration testing tool. This
allows to automatically generate attack paths for penetration testing
scenarios, and to validate these attacks by executing the corresponding actions
-including exploits- against the real target network. We present an algorithm
for transforming the information present in the penetration testing tool to the
planning domain, and show how the scalability issues of attack graphs can be
solved using current planners. We include an analysis of the performance of our
solution, showing how our model scales to medium-sized networks and the number
of actions available in current penetration testing tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4048</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4048</id><created>2013-06-17</created><authors><author><keyname>Bereg</keyname><forenames>Sergey</forenames></author><author><keyname>Holroyd</keyname><forenames>Alexander E.</forenames></author><author><keyname>Nachmanson</keyname><forenames>Lev</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author></authors><title>Drawing Permutations with Few Corners</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A permutation may be represented by a collection of paths in the plane. We
consider a natural class of such representations, which we call tangles, in
which the paths consist of straight segments at 45 degree angles, and the
permutation is decomposed into nearest-neighbour transpositions. We address the
problem of minimizing the number of crossings together with the number of
corners of the paths, focusing on classes of permutations in which both can be
minimized simultaneously. We give algorithms for computing such tangles for
several classes of permutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4059</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4059</id><created>2013-06-17</created><authors><author><keyname>Yang</keyname><forenames>Lu</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Deciding Nonnegativity of Polynomials by MAPLE</title><categories>cs.SC</categories><comments>A user guide on using RealRootClassfication to prove the
  nonnegativity of polynomials with 10 examples</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been some effective tools for solving (constant/parametric)
semi-algebraic systems in Maple's library RegularChains since Maple 13. By
using the functions of the library, e.g., RealRootClassfication, one can prove
and discover polynomial inequalities. This paper is more or less a user guide
on using RealRootClassfication to prove the nonnegativity of polynomials. We
show by examples how to use this powerful tool to prove a polynomial is
nonnegative under some polynomial inequality and/or equation constraints. Some
tricks for using the tool are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4063</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4063</id><created>2013-06-17</created><authors><author><keyname>Hussain</keyname><forenames>Shariq</forenames></author><author><keyname>Wang</keyname><forenames>Zhaoshun</forenames></author><author><keyname>Toure</keyname><forenames>Ibrahima Kalil</forenames></author><author><keyname>Diop</keyname><forenames>Abdoulaye</forenames></author></authors><title>Web Service Testing Tools: A Comparative Study</title><categories>cs.SE</categories><comments>7 pages, 4 tables, 3 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, vol.10,
  no.1-3, 2013, pp.641-647</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality of Service (QoS) has gained more importance with the increase in
usage and adoption of web services. In recent years, various tools and
techniques developed for measurement and evaluation of QoS of web services.
There are commercial as well as open-source tools available today which are
being used for monitoring and testing QoS for web services. These tools
facilitate in QoS measurement and analysis and are helpful in evaluation of
service performance in real-time network. In this paper, we describe three
popular open-source tools and compare them in terms of features, usability,
performance, and software requirements. Results of the comparison will help in
adoption and usage of these tools, and also promote development and usage of
open-source web service testing tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4064</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4064</id><created>2013-06-17</created><authors><author><keyname>Small</keyname><forenames>Michael</forenames></author><author><keyname>Judd</keyname><forenames>Kevin</forenames></author><author><keyname>Stemler</keyname><forenames>Thomas</forenames></author></authors><title>A surrogate for networks -- How scale-free is my scale-free network?</title><categories>physics.soc-ph cs.SI nlin.AO physics.data-an</categories><comments>4 pages, 2 figures. Preliminary paper - presented at NOLTA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks are now being studied in a wide range of disciplines across
science and technology. In this paper we propose a method by which one can
probe the properties of experimentally obtained network data. Rather than just
measuring properties of a network inferred from data, we aim to ask how typical
is that network? What properties of the observed network are typical of all
such scale free networks, and which are peculiar? To do this we propose a
series of methods that can be used to generate statistically likely complex
networks which are both similar to the observed data and also consistent with
an underlying null-hypothesis -- for example a particular degree distribution.
There is a direct analogy between the approach we propose here and the
surrogate data methods applied to nonlinear time series data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4066</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4066</id><created>2013-06-18</created><updated>2014-07-22</updated><authors><author><keyname>Fu</keyname><forenames>Tom Z. J.</forenames></author><author><keyname>Ying</keyname><forenames>Qiufang</forenames></author><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author></authors><title>MYE: Missing Year Estimation in Academic Social Networks</title><categories>cs.DL cs.SI</categories><comments>Some typos are corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In bibliometrics studies, a common challenge is how to deal with incorrect or
incomplete data. However, given a large volume of data, there often exists
certain relationships between the data items that can allow us to recover
missing data items and correct erroneous data. In this paper, we study a
particular problem of this sort - estimating the missing year information
associated with publications (and hence authors' years of active publication).
We first propose a simple algorithm that only makes use of the &quot;direct&quot;
information, such as paper citation/reference relationships or paper-author
relationships. The result of this simple algorithm is used as a benchmark for
comparison. Our goal is to develop algorithms that increase both the coverage
(the percentage of missing year papers recovered) and accuracy (mean absolute
error of the estimated year to the real year). We propose some advanced
algorithms that extend inference by information propagation. For each
algorithm, we propose three versions according to the given academic social
network type: a) Homogeneous (only contains paper citation links), b) Bipartite
(only contains paper-author relations), and, c) Heterogeneous (both paper
citation and paper-author relations). We carry out experiments on the three
public data sets (MSR Libra, DBLP and APS), and evaluated by applying the
K-fold cross validation method. We show that the advanced algorithms can
improve both coverage and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4069</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4069</id><created>2013-06-18</created><authors><author><keyname>Mahmood</keyname><forenames>Azhar</forenames></author><author><keyname>Ke</keyname><forenames>Shi</forenames></author><author><keyname>Khatoon</keyname><forenames>Shaheen</forenames></author></authors><title>An Efficient Distributed Data Extraction Method for Mining Sensor
  Networks Data</title><categories>cs.DB cs.NI</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 1, No 2, January 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of Sensor Networks (SNs) are deployed in real world applications
which generate large amount of raw sensory data. Data mining technique to
extract useful knowledge from these applications is an emerging research area
due to its crucial importance but still its a challenge to discover knowledge
efficiently from the sensor network data. In this paper we proposed a
Distributed Data Extraction (DDE) method to extract data from sensor networks
by applying rules based clustering and association rule mining techniques. A
significant amount of sensor readings sent from the sensors to the data
processing point(s) may be lost or corrupted. DDE is also estimating these
missing values from available sensor reading instead of requesting the sensor
node to resend lost reading. DDE also apply data reduction which is able to
reduce the data size while transmitting to sink. Results show our proposed
approach exhibits the maximum data accuracy and efficient data extraction in
term of the entire networks energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4071</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4071</id><created>2013-06-18</created><authors><author><keyname>Jagadish</keyname><forenames>Nitin</forenames></author><author><keyname>H.</keyname><forenames>Manoj</forenames></author><author><keyname>Prasad</keyname><forenames>Nishanth K.</forenames></author><author><keyname>M</keyname><forenames>Sunil Kumar K.</forenames></author></authors><title>A Microcontroller Based Device to Reduce Phanthom Power</title><categories>cs.SY</categories><comments>3 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we concern ourselves with the problem of minimizing the standby
power consumption in some of the house hold appliances. Here we propose a
remote controlled device through which we could reduce the amount of standby
power consumed by the electrical appliances connected to it. This device
provides an option of controlling each of the appliances connected to it
individually or as a whole when required. The device has got number of plug
points each of which could be controlled through the remote and also has a
provision of switching off all the points at once.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4079</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4079</id><created>2013-06-18</created><authors><author><keyname>Jie</keyname><forenames>Zeng</forenames></author></authors><title>A Novel Block-DCT and PCA Based Image Perceptual Hashing Algorithm</title><categories>cs.CV</categories><comments>7 pages, 5 figrues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image perceptual hashing finds applications in content indexing, large-scale
image database management, certification and authentication and digital
watermarking. We propose a Block-DCT and PCA based image perceptual hash in
this article and explore the algorithm in the application of tamper detection.
The main idea of the algorithm is to integrate color histogram and DCT
coefficients of image blocks as perceptual feature, then to compress perceptual
features as inter-feature with PCA, and to threshold to create a robust hash.
The robustness and discrimination properties of the proposed algorithm are
evaluated in detail. Our algorithms first construct a secondary image, derived
from input image by pseudo-randomly extracting features that approximately
capture semi-global geometric characteristics. From the secondary image (which
does not perceptually resemble the input), we further extract the final
features which can be used as a hash value (and can be further suitably
quantized). In this paper, we use spectral matrix invariants as embodied by
Singular Value Decomposition. Surprisingly, formation of the secondary image
turns out be quite important since it not only introduces further robustness,
but also enhances the security properties. Indeed, our experiments reveal that
our hashing algorithms extract most of the geometric information from the
images and hence are robust to severe perturbations (e.g. up to %50 cropping by
area with 20 degree rotations) on images while avoiding misclassification.
Experimental results show that the proposed image perceptual hash algorithm can
effectively address the tamper detection problem with advantageous robustness
and discrimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4080</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4080</id><created>2013-06-18</created><updated>2014-03-18</updated><authors><author><keyname>Bian</keyname><forenames>Yatao</forenames></author><author><keyname>Li</keyname><forenames>Xiong</forenames></author><author><keyname>Liu</keyname><forenames>Yuncai</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Hsuan</forenames></author></authors><title>Parallel Coordinate Descent Newton Method for Efficient
  $\ell_1$-Regularized Minimization</title><categories>cs.LG cs.NA</categories><comments>30 pages, 36 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent years have witnessed advances in parallel algorithms for large
scale optimization problems. Notwithstanding demonstrated success, existing
algorithms that parallelize over features are usually limited by divergence
issues under high parallelism or require data preprocessing to alleviate these
problems. In this work, we propose a Parallel Coordinate Descent Newton
algorithm using multidimensional approximate Newton steps (PCDN), where the
off-diagonal elements of the Hessian are set to zero to enable parallelization.
It randomly partitions the feature set into $b$ bundles/subsets with size of
$P$, and sequentially processes each bundle by first computing the descent
directions for each feature in parallel and then conducting $P$-dimensional
line search to obtain the step size. We show that: (1) PCDN is guaranteed to
converge globally despite increasing parallelism; (2) PCDN converges to the
specified accuracy $\epsilon$ within the limited iteration number of
$T_\epsilon$, and $T_\epsilon$ decreases with increasing parallelism (bundle
size $P$). Using the implementation technique of maintaining intermediate
quantities, we minimize the data transfer and synchronization cost of the
$P$-dimensional line search. For concreteness, the proposed PCDN algorithm is
applied to $\ell_1$-regularized logistic regression and $\ell_2$-loss SVM.
Experimental evaluations on six benchmark datasets show that the proposed PCDN
algorithm exploits parallelism well and outperforms the state-of-the-art
methods in speed without losing accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4082</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4082</id><created>2013-06-18</created><authors><author><keyname>Abid</keyname><forenames>Sohail</forenames></author><author><keyname>Shafi</keyname><forenames>Imran</forenames></author><author><keyname>Abid</keyname><forenames>Shahid</forenames></author></authors><title>Energy efficient routing in mobile ad-hoc networks for Healthcare
  Environments</title><categories>cs.NI</categories><comments>9 pages, 12 figures, 3 tables, IJCSI Journal</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 1, No 1, January 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern and innovative medical applications based on wireless network are
being developed in the commercial sectors as well as in research. The emerging
wireless networks are rapidly becoming a fundamental part of medical solutions
due to increasing accessibility for healthcare professionals/patients reducing
healthcare costs. Discovering the routes among hosts that are energy efficient
without compromise on smooth communication is desirable. This work investigates
energy efficiency of some selected proactive and reactive routing protocols in
wireless network for healthcare environments. After simulation and analysis we
found that DSR is best energy efficient routing protocol among DSR, DSDV and
AODV, because DSR has maximum remaining energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4083</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4083</id><created>2013-06-18</created><authors><author><keyname>Femminella</keyname><forenames>Mauro</forenames></author><author><keyname>Reali</keyname><forenames>Gianluca</forenames></author><author><keyname>Francescangeli</keyname><forenames>Roberto</forenames></author></authors><title>Modeling the guaranteed delivery of bulk data</title><categories>cs.NI</categories><comments>Accepted for publication to IFIP Networking 2013, New York, USA, May
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The delivery of bulk data is an increasingly pressing problem in modern
networks. While in some cases these transfers happen in background without
specific constraints in terms of delivery times, there are a number of
scenarios in which the transfer of tens of GB of data must occur in specific,
limited time windows. In order to face this task, a suitable solution is the
deployment of virtual links with guaranteed bandwidth between endpoints
provided by a Service Overlay Network (SON) provider. We model this scenario as
an optimization problem, in which the target consists of minimizing the costs
of the virtual links provided by the SON and the unknowns are the provisioned
bandwidths of these links. Since the resulting objective function is neither
continuous nor convex, the solution of this problem is really challenging for
standard optimization tools in terms of both convergence time and solution
optimality. We propose a solution based on an heuristic approach which uses the
min-plus algebra. Numerical results show that the proposed heuristic
outperforms the considered optimization tools, whilst maintaining an affordable
computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4085</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4085</id><created>2013-06-18</created><authors><author><keyname>Tayebinik</keyname><forenames>Maryam</forenames></author><author><keyname>Puteh</keyname><forenames>Marlia</forenames></author></authors><title>Blended Learning or E-learning?</title><categories>cs.CY</categories><comments>11 pages, 1 figure, Proceeding of the conference</comments><journal-ref>Tayebinik, M., &amp; Puteh, M. (2012). Blended Learning or E-learning?
  International Magazine on Advances in Computer Science and Telecommunications
  (IMACST), 3(1), 103-110</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  ICT or Information and Communication Technology has pervaded the fields of
education.In recent years the term e-learning has emerged as a result of the
integration of ICT in the education fields. Following the application this
technology into teaching, some pitfalls have been identified and this have led
to the Blended learning phenomenon.However the preference on this new method
has been debated quite extensively.The aim of this paper is to investigate the
advantages of blended learning over face to face instruction through reviews of
related literature.The present survey revealed that blended learning is more
favorable than pure e-learning and offers many advantages for learners like
producing a sense of community or belonging.This study concludes that blended
learning can be considered as an efficient approach of distance learning in
terms of students' learning experience student-student interaction as well as
studentinstructor interaction and is likely to emerge as the predominant
education model in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4090</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4090</id><created>2013-06-18</created><authors><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Song</keyname><forenames>Fei</forenames></author><author><keyname>Yan</keyname><forenames>Huan</forenames></author><author><keyname>Zhang</keyname><forenames>Sidong</forenames></author></authors><title>Modeling of Multipath Transport</title><categories>cs.NI</categories><comments>10 pages, 11 figures</comments><journal-ref>International Journal of Computer Science Issues, Volume 10, Issue
  1, January 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a model for evaluating the transmission performance
of multipath transport. Previous researches focused exclusively on single pair
users in simple scenarios. The distinct perspective in this paper is to build
models for analyzing the performance when multipath transport is used in the
entire network scope. We illustrate the influences on the transmission
performance caused by the variation of network topologies, the services'
arrival rate, the services' size and other parameters. We demonstrate through
simulation that multipath transport could conditionally increase the throughput
than single-path transport. And it has the capability to support higher
services' arrival rate in various network topologies. And higher multi-parent
probability will be beneficial for multipath transport to take its advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4092</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4092</id><created>2013-06-18</created><authors><author><keyname>Wang</keyname><forenames>Xiaolin</forenames></author><author><keyname>Qiu</keyname><forenames>Xun</forenames></author></authors><title>Application of particle swarm optimization for enhanced cyclic steam
  stimulation in a offshore heavy oil reservoir</title><categories>cs.CE</categories><comments>11 pages,6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three different variations of PSO algorithms, i.e. Canonical, Gaussian
Bare-bone and L\'evy Bare-bone PSO, are tested to optimize the ultimate oil
recovery of a large heavy oil reservoir. The performance of these algorithms
was compared in terms of convergence behaviour and the final optimization
results. It is found that, in general, all three types of PSO methods are able
to improve the objective function. The best objective function is found by
using the Canonical PSO, while the other two methods give similar results. The
Gaussian Bare-bone PSO may picks positions that are far away from the optimal
solution. The L\'evy Bare-bone PSO has similar convergence behaviour as the
Canonical PSO. For the specific optimization problem investigated in this
study, it is found that the temperature of the injection steam, CO2 composition
in the injection gas, and the gas injection rates have bigger impact on the
objective function, while steam injection rate and the liquid production rate
have less impact on the objective function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4094</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4094</id><created>2013-06-18</created><authors><author><keyname>Khanum</keyname><forenames>Mohammadi Akheela</forenames></author><author><keyname>Trivedi</keyname><forenames>Munesh C.</forenames></author></authors><title>Exploring Verbalization and Collaboration during Usability Evaluation
  with Children in Context</title><categories>cs.HC</categories><comments>7 pages. arXiv admin note: text overlap with arXiv:1204.2138</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the effect of context on usability evaluation.
The focus is on how children behave and perform when they are tested in
different settings. Two most commonly applied usability evaluation methods: the
think-aloud and constructive interactions are applied to the children in
different physical contexts. We present an experimental design involving 54
children participating in two different configurations of constructive
interaction and a traditional think-aloud. The behavior and performance of the
children in two different physical contexts is measured by evaluating the
results of application of think-aloud and constructive interaction. Finally, we
outline lessons on the impact of context on involving children in usability
testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4111</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4111</id><created>2013-06-18</created><updated>2015-08-14</updated><authors><author><keyname>Bj&#xf6;rklund</keyname><forenames>Andreas</forenames></author><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Kowalik</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Counting thin subgraphs via packings faster than meet-in-the-middle time</title><categories>cs.DS cs.DM</categories><comments>Journal version, 26 pages. Compared to the SODA'14 version, it
  contains some new results: a) improved algorithms for counting t-tuples of
  disjoint s-sets for the special cases of s = 2, 3, 4 and b) new hardness
  arguments</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vassilevska and Williams (STOC 2009) showed how to count simple paths on $k$
vertices and matchings on $k/2$ edges in an $n$-vertex graph in time
$n^{k/2+O(1)}$. In the same year, two different algorithms with the same
runtime were given by Koutis and Williams~(ICALP 2009), and Bj\&quot;orklund
\emph{et al.} (ESA 2009), via $n^{st/2+O(1)}$-time algorithms for counting
$t$-tuples of pairwise disjoint sets drawn from a given family of $s$-sized
subsets of an $n$-element universe. Shortly afterwards, Alon and Gutner (TALG
2010) showed that these problems have $\Omega(n^{\lfloor st/2\rfloor})$ and
$\Omega(n^{\lfloor k/2\rfloor})$ lower bounds when counting by color coding.
  Here we show that one can do better, namely, we show that the
&quot;meet-in-the-middle&quot; exponent $st/2$ can be beaten and give an algorithm that
counts in time $n^{0.45470382 st + O(1)}$ for $t$ a multiple of three. This
implies algorithms for counting occurrences of a fixed subgraph on $k$ vertices
and pathwidth $p\ll k$ in an $n$-vertex graph in $n^{0.45470382k+2p+O(1)}$
time, improving on the three mentioned algorithms for paths and matchings, and
circumventing the color-coding lower bound. We also give improved bounds for
counting $t$-tuples of disjoint $s$-sets for $s=2,3,4$.
  Our algorithms use fast matrix multiplication. We show an argument that this
is necessary to go below the meet-in-the-middle barrier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4121</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4121</id><created>2013-06-18</created><updated>2013-09-06</updated><authors><author><keyname>Barbier</keyname><forenames>Jean</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author><author><keyname>Zhang</keyname><forenames>Pan</forenames></author></authors><title>The hard-core model on random graphs revisited</title><categories>cond-mat.dis-nn cs.DM</categories><comments>9 pages, 2 figures, International Meeting on &quot;Inference, Computation,
  and Spin Glasses&quot; (ICSG2013), Sapporo, Japan</comments><journal-ref>J. Phys.: Conf. Ser. 473 012021 (2013)</journal-ref><doi>10.1088/1742-6596/473/1/012021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the classical hard-core model, also known as independent set and
dual to vertex cover problem, where one puts particles with a first-neighbor
hard-core repulsion on the vertices of a random graph. Although the case of
random graphs with small and very large average degrees respectively are quite
well understood, they yield qualitatively different results and our aim here is
to reconciliate these two cases. We revisit results that can be obtained using
the (heuristic) cavity method and show that it provides a closed-form
conjecture for the exact density of the densest packing on random regular
graphs with degree K&gt;=20, and that for K&gt;16 the nature of the phase transition
is the same as for large K. This also shows that the hard-code model is the
simplest mean-field lattice model for structural glasses and jamming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4133</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4133</id><created>2013-06-18</created><authors><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author><author><keyname>Maximenko</keyname><forenames>Anatoly</forenames></author><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author></authors><title>On M2M communications standards for smart metering</title><categories>cs.NI</categories><comments>The INTHITEN (INternet of THings and ITs ENablers) conference
  http://www.iot.sut.ru/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper discusses M2M communications standards for smart metering. One of
the our goals is to show the failures of ETSI standartization process for M2M
communications. Our paper proposes some extesions to ETSI standards. At the
first hand, it is M-Bus protocol and Open Metering System based on M-Bus. The
paper shows how to estimate wireless M-bus throughput and how to avoid
collisions. After analysis of Open API for M2M, submitted to ETSI, we propose a
new approach in the client-side web development - Web Intents. The main goal
for our suggestions is to simplify the development phase for new applications
by support asynchronous calls and JSON versus XML for data exchange.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4134</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4134</id><created>2013-06-18</created><authors><author><keyname>Arora</keyname><forenames>Suket</forenames></author><author><keyname>Batra</keyname><forenames>Kamaljeet</forenames></author><author><keyname>Singh</keyname><forenames>Sarabjit</forenames></author></authors><title>Dialogue System: A Brief Review</title><categories>cs.CL</categories><comments>4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Dialogue System is a system which interacts with human in natural language.
At present many universities are developing the dialogue system in their
regional language. This paper will discuss about dialogue system, its
components, challenges and its evaluation. This paper helps the researchers for
getting info regarding dialogues system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4136</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4136</id><created>2013-06-18</created><updated>2013-09-19</updated><authors><author><keyname>Granell</keyname><forenames>Clara</forenames></author><author><keyname>Gomez</keyname><forenames>Sergio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Dynamical interplay between awareness and epidemic spreading in
  multiplex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>5 pages + supplemental material</comments><journal-ref>Phys. Rev. Lett. 111 (2013) 128701</journal-ref><doi>10.1103/PhysRevLett.111.128701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the analysis of the interrelation between two processes accounting
for the spreading of an epidemics, and the information awareness to prevent its
infection, on top of multiplex networks. This scenario is representative of an
epidemic process spreading on a network of persistent real contacts, and a
cyclic information awareness process diffusing in the network of virtual social
contacts between the same individuals. The topology corresponds to a multiplex
network where two diffusive processes are interacting affecting each other. The
analysis using a Microscopic Markov Chain Approach (MMCA) reveals the phase
diagram of the incidence of the epidemics and allows to capture the evolution
of the epidemic threshold depending on the topological structure of the
multiplex and the interrelation with the awareness process. Interestingly, the
critical point for the onset of the epidemics has a critical value
(meta-critical point) defined by the awareness dynamics and the topology of the
virtual network, from which the onset increases and the epidemics incidence
decreases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4139</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4139</id><created>2013-06-18</created><authors><author><keyname>Verma</keyname><forenames>Preeti</forenames></author><author><keyname>Arora</keyname><forenames>Suket</forenames></author><author><keyname>Batra</keyname><forenames>Kamaljit</forenames></author></authors><title>Punjabi Language Interface to Database: a brief review</title><categories>cs.CL cs.HC</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike most user-computer interfaces, a natural language interface allows
users to communicate fluently with a computer system with very little
preparation. Databases are often hard to use in cooperating with the users
because of their rigid interface. A good NLIDB allows a user to enter commands
and ask questions in native language and then after interpreting respond to the
user in native language. For a large number of applications requiring
interaction between humans and the computer systems, it would be convenient to
provide the end-user friendly interface. Punjabi language interface to database
would proof fruitful to native people of Punjab, as it provides ease to them to
use various e-governance applications like Punjab Sewa, Suwidha, Online Public
Utility Forms, Online Grievance Cell, Land Records Management System,legacy
matters, e-District, agriculture, etc. Punjabi is the mother tongue of more
than 110 million people all around the world. According to available
information, Punjabi ranks 10th from top out of a total of 6,900 languages
recognized internationally by the United Nations. This paper covers a brief
overview of the Natural language interface to database, its different
components, its advantages, disadvantages, approaches and techniques used. The
paper ends with the work done on Punjabi language interface to database and
future enhancements that can be done.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4144</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4144</id><created>2013-06-18</created><updated>2013-07-16</updated><authors><author><keyname>Kelif</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author><author><keyname>Sigelle</keyname><forenames>Marc</forenames></author></authors><title>Optimal Relay Placement for Capacity and Performance Improvement using a
  Fluid Model for Heterogeneous Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>12 pages, 10 figures. subsection V-C added : validation of the model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of optimal relay placement in a
cellular network assuming network densification, with the aim of maximizing
cell capacity. In our model, a fraction of radio resources is dedicated to the
base-station (BS)/relay nodes (RN) communication. In the remaining resources,
BS and RN transmit simultaneously to users. During this phase, the network is
densified in the sense that the transmitters density and so network capacity
are increased. Intra- and inter-cell interference is taken into account in
Signal to Interference plus Noise Ratio (SINR) simple formulas derived from a
fluid model for heterogeneous network. Optimization can then be quickly
performed using Simulated Annealing. Performance results show that cell
capacity is boosted thanks to densification despite a degradation of the signal
quality. Bounds are also provided on the fraction of resources dedicated to the
BS-RN link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4149</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4149</id><created>2013-06-18</created><authors><author><keyname>Aldecoa</keyname><forenames>Rodrigo</forenames></author><author><keyname>Mar&#xed;n</keyname><forenames>Ignacio</forenames></author></authors><title>Exploring the limits of community detection strategies in complex
  networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>13 pages, 8 figures, 1 table. Scientific Reports (in press)</comments><acm-class>E.1; G.2.2; C.2.1; I.5.3</acm-class><journal-ref>Scientific Reports 3, 2216 (2013)</journal-ref><doi>10.1038/srep02216</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The characterization of network community structure has profound implications
in several scientific areas. Therefore, testing the algorithms developed to
establish the optimal division of a network into communities is a fundamental
problem in the field. We performed here a highly detailed evaluation of
community detection algorithms, which has two main novelties: 1) using complex
closed benchmarks, which provide precise ways to assess whether the solutions
generated by the algorithms are optimal; and, 2) A novel type of analysis,
based on hierarchically clustering the solutions suggested by multiple
community detection algorithms, which allows to easily visualize how different
are those solutions. Surprise, a global parameter that evaluates the quality of
a partition, confirms the power of these analyses. We show that none of the
community detection algorithms tested provide consistently optimal results in
all networks and that Surprise maximization, obtained by combining multiple
algorithms, obtains quasi-optimal performances in these difficult benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4151</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4151</id><created>2013-06-18</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Prakash</keyname><forenames>Anupam</forenames></author><author><keyname>Valiant</keyname><forenames>Gregory</forenames></author></authors><title>Computation in anonymous networks</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify and investigate a computational model arising in molecular
computing, social computing and sensor network. The model is made of of
multiple agents who are computationally limited and posses no global
information. The agents may represent nodes in a social network, sensors, or
molecules in a molecular computer. Assuming that each agent is in one of $k$
states, we say that {\em the system computes} $f:[k]^{n} \to [k]$ if all agents
eventually converge to the correct value of $f$. We present number of general
results characterizing the computational power of the mode. We further present
protocols for computing the plurality function with $O(\log k)$ memory and for
approximately counting the number of nodes of a given color with $O(\log \log
n)$ memory, where $n$ is the number of agents in the networks. These results
are tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4152</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4152</id><created>2013-06-18</created><authors><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author></authors><title>Bioclimating Modelling: A Machine Learning Perspective</title><categories>cs.LG stat.ML</categories><comments>8 pages, In the Proceedings of the 2012 International Joint
  Conferences on Computer, Information, and Systems Sciences, and Engineering
  (CISSE 2012)</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning (ML) approaches are widely used to generate bioclimatic
models for prediction of geographic range of organism as a function of climate.
Applications such as prediction of range shift in organism, range of invasive
species influenced by climate change are important parameters in understanding
the impact of climate change. However, success of machine learning-based
approaches depends on a number of factors. While it can be safely said that no
particular ML technique can be effective in all applications and success of a
technique is predominantly dependent on the application or the type of the
problem, it is useful to understand their behaviour to ensure informed choice
of techniques. This paper presents a comprehensive review of machine
learning-based bioclimatic model generation and analyses the factors
influencing success of such models. Considering the wide use of statistical
techniques, in our discussion we also include conventional statistical
techniques used in bioclimatic modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4161</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4161</id><created>2013-06-18</created><authors><author><keyname>Quintin</keyname><forenames>Jean-Noel</forenames></author><author><keyname>Hasanov</keyname><forenames>Khalid</forenames></author><author><keyname>Lastovetsky</keyname><forenames>Alexey</forenames></author></authors><title>Hierarchical Parallel Matrix Multiplication on Large-Scale Distributed
  Memory Platforms</title><categories>cs.DC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix multiplication is a very important computation kernel both in its own
right as a building block of many scientific applications and as a popular
representative for other scientific applications. Cannon algorithm which dates
back to 1969 was the first efficient algorithm for parallel matrix
multiplication providing theoretically optimal communication cost. However this
algorithm requires a square number of processors. In the mid 1990s, the SUMMA
algorithm was introduced. SUMMA overcomes the shortcomings of Cannon algorithm
as it can be used on a non-square number of processors as well. Since then the
number of processors in HPC platforms has increased by two orders of magnitude
making the contribution of communication in the overall execution time more
significant. Therefore, the state of the art parallel matrix multiplication
algorithms should be revisited to reduce the communication cost further. This
paper introduces a new parallel matrix multiplication algorithm, Hierarchical
SUMMA (HSUMMA), which is a redesign of SUMMA. Our algorithm reduces the
communication cost of SUMMA by introducing a two-level virtual hierarchy into
the two-dimensional arrangement of processors. Experiments on an IBM BlueGene-P
demonstrate the reduction of communication cost up to 2.08 times on 2048 cores
and up to 5.89 times on 16384 cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4166</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4166</id><created>2013-06-18</created><updated>2014-05-26</updated><authors><author><keyname>Kumagai</keyname><forenames>Wataru</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>A New Family of Probability Distributions and Asymptotics of Classical
  and LOCC Conversions</title><categories>quant-ph cs.IT math.IT</categories><comments>44 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the optimal approximate conversion between multiple copies of
pure entangled states in quantum systems when only local operations and
classical communications (LOCC) are allowed. This problem contains a kind of
cloning problem with LOCC restriction as a special case. To derive the
asymptotic LOCC conversion rate, we consider two kinds of approximate
conversions, deterministic conversion and majorization conversion, for
probability distributions, and solve their asymptotic conversion rates up to
the second order. Then, the asymptotic LOCC conversion rate is obtained via the
natural relation between the LOCC conversion and the majorization conversion.
To derive these asymptotic rates, we introduce new probability distributions
named Rayleigh-normal distributions. The family of Rayleigh-normal
distributions includes a Rayleigh distribution and coincides with the standard
normal distribution in the limit case, and the optimal conversion rate is
represented by Rayleigh-normal distribution in a unified manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4174</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4174</id><created>2013-06-18</created><authors><author><keyname>Gunn</keyname><forenames>Lachlan J.</forenames></author><author><keyname>Chappell</keyname><forenames>James M.</forenames></author><author><keyname>Allison</keyname><forenames>Andrew</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>Physical-layer encryption on the public internet: a stochastic approach
  to the Kish-Sethuraman cipher</title><categories>cs.CR</categories><comments>7 pages, 3 figures, to be presented at HotPI-2013</comments><doi>10.1142/S2010194514603615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While information-theoretic security is often associated with the one-time
pad and quantum key distribution, noisy transport media leave room for
classical techniques and even covert operation. Transit times across the public
internet exhibit a degree of randomness, and cannot be determined noiselessly
by an eavesdropper. We demonstrate the use of these measurements for
information-theoretically secure communication over the public internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4193</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4193</id><created>2013-06-18</created><updated>2014-02-10</updated><authors><author><keyname>Liu</keyname><forenames>Jin-Hu</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Yang</keyname><forenames>Chengcheng</forenames></author><author><keyname>Chen</keyname><forenames>Lingjiao</forenames></author><author><keyname>Liu</keyname><forenames>Chuang</forenames></author><author><keyname>Wang</keyname><forenames>Xueqi</forenames></author></authors><title>Gravity Effects on Information Filtering and Network Evolving</title><categories>physics.soc-ph cs.IR cs.SI</categories><doi>10.1371/journal.pone.0091070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, based on the gravity principle of classical physics, we
propose a tunable gravity-based model, which considers tag usage pattern to
weigh both the mass and distance of network nodes. We then apply this model in
solving the problems of information filtering and network evolving.
Experimental results on two real-world data sets, \emph{Del.icio.us} and
\emph{MovieLens}, show that it can not only enhance the algorithmic
performance, but can also better characterize the properties of real networks.
This work may shed some light on the in-depth understanding of the effect of
gravity model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4207</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4207</id><created>2013-06-18</created><authors><author><keyname>Jaiswal</keyname><forenames>Ragesh</forenames></author><author><keyname>Jain</keyname><forenames>Prachi</forenames></author><author><keyname>Yadav</keyname><forenames>Saumya</forenames></author></authors><title>A bad 2-dimensional instance for k-means++</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-means++ seeding algorithm is one of the most popular algorithms that is
used for finding the initial $k$ centers when using the k-means heuristic. The
algorithm is a simple sampling procedure and can be described as follows:
{quote} Pick the first center randomly from among the given points. For $i &gt;
1$, pick a point to be the $i^{th}$ center with probability proportional to the
square of the Euclidean distance of this point to the previously $(i-1)$ chosen
centers. {quote} The k-means++ seeding algorithm is not only simple and fast
but gives an $O(\log{k})$ approximation in expectation as shown by Arthur and
Vassilvitskii \cite{av07}. There are datasets \cite{av07,adk09} on which this
seeding algorithm gives an approximation factor $\Omega(\log{k})$ in
expectation. However, it is not clear from these results if the algorithm
achieves good approximation factor with reasonably large probability (say
$1/poly(k)$). Brunsch and R\&quot;{o}glin \cite{br11} gave a dataset where the
k-means++ seeding algorithm achieves an approximation ratio of $(2/3 -
\epsilon)\cdot \log{k}$ only with probability that is exponentially small in
$k$. However, this and all other known {\em lower-bound examples}
\cite{av07,adk09} are high dimensional. So, an open problem is to understand
the behavior of the algorithm on low dimensional datasets. In this work, we
give a simple two dimensional dataset on which the seeding algorithm achieves
an approximation ratio $c$ (for some universal constant $c$) only with
probability exponentially small in $k$. This is the first step towards solving
open problems posed by Mahajan et al \cite{mnv12} and by Brunsch and R\&quot;{o}glin
\cite{br11}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4219</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4219</id><created>2013-06-18</created><authors><author><keyname>Kolokoltsov</keyname><forenames>Vassili</forenames></author><author><keyname>Passi</keyname><forenames>Hemant</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author></authors><title>Inspection and crime prevention: an evolutionary perspective</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyse inspection games with an evolutionary perspective.
In our evolutionary inspection game with a large population, each individual is
not a rational payoff maximiser, but periodically updates his strategy if he
perceives that other individuals' strategies are more successful than his own,
namely strategies are subject to the evolutionary pressure. We develop this
game into a few directions. Firstly, social norms are incorporated into the
game and we analyse how social norms may influence individuals' propensity to
engage in criminal behaviour. Secondly, a forward-looking inspector is
considered, namely, the inspector chooses the level of law enforcement whilst
taking into account the effect that this choice will have on future crime
rates. Finally, the game is extended to the one with continuous strategy
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4223</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4223</id><created>2013-06-18</created><updated>2013-09-01</updated><authors><author><keyname>Ucar</keyname><forenames>I&#xf1;aki</forenames></author><author><keyname>L&#xf3;pez-Fernandino</keyname><forenames>Felipe</forenames></author><author><keyname>Rodriguez-Ulibarri</keyname><forenames>Pablo</forenames></author><author><keyname>Sesma-Sanchez</keyname><forenames>Laura</forenames></author><author><keyname>Urrea-Mic&#xf3;</keyname><forenames>Veronica</forenames></author><author><keyname>Sevilla</keyname><forenames>Joaqu&#xed;n</forenames></author></authors><title>Growth in the number of references in engineering journal papers during
  the 1972-2013 period</title><categories>cs.DL</categories><comments>11 pages, 4 figures. Published online on August 31, 2013</comments><doi>10.1007/s11192-013-1113-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of references per paper, perhaps the best single index of a
journal's scholarliness, has been studied in different disciplines and periods.
In this paper we present a four decade study of eight engineering journals. A
data set of over 70000 references was generated after automatic data gathering
and manual inspection for errors. Results show a significant increase in the
number of references per paper, the average rises from 8 in 1972 to 25 in 2013.
This growth presents an acceleration around the year 2000, consistent with a
much easier access to search engines and documents produced by the
generalization of the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4230</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4230</id><created>2013-06-18</created><authors><author><keyname>Tukmanov</keyname><forenames>Anvar</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Boussakta</keyname><forenames>Said</forenames></author><author><keyname>Jamalipour</keyname><forenames>Abbas</forenames></author></authors><title>On the Broadcast Latency in Finite Cooperative Wireless Networks</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures</comments><journal-ref>IEEE Transactions on Wireless Communications, vol.11, no.4,
  pp.1307,1313, April 2012</journal-ref><doi>10.1109/TWC.2012.020812.111545</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to study the effect of cooperation on system delay,
quantified as the number of retransmissions required to deliver a broadcast
message to all intended receivers. Unlike existing works on broadcast
scenarios, where distance between nodes is not explicitly considered, we
examine the joint effect of small scale fading and propagation path loss. Also,
we study cooperation in application to finite networks, i.e. when the number of
cooperating nodes is small. Stochastic geometry and order statistics are used
to develop analytical models that tightly match the simulation results for
non-cooperative scenario and provide a lower bound for delay in a cooperative
setting. We demonstrate that even for a simple flooding scenario, cooperative
broadcast achieves significantly lower system delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4242</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4242</id><created>2013-06-18</created><authors><author><keyname>Auger</keyname><forenames>C&#xe9;dric</forenames><affiliation>LIP6</affiliation></author><author><keyname>Bouzid</keyname><forenames>Zohir</forenames><affiliation>LIP6</affiliation></author><author><keyname>Courtieu</keyname><forenames>Pierre</forenames><affiliation>CEDRIC</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, LINCS, IUF</affiliation></author><author><keyname>Urbain</keyname><forenames>Xavier</forenames><affiliation>CEDRIC, LRI</affiliation></author></authors><title>Certified Impossibility Results for Byzantine-Tolerant Mobile Robots</title><categories>cs.LO cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework to build formal developments for robot networks using
the COQ proof assistant, to state and to prove formally various properties. We
focus in this paper on impossibility proofs, as it is natural to take advantage
of the COQ higher order calculus to reason about algorithms as abstract
objects. We present in particular formal proofs of two impossibility results
forconvergence of oblivious mobile robots if respectively more than one half
and more than one third of the robots exhibit Byzantine failures, starting from
the original theorems by Bouzid et al.. Thanks to our formalization, the
corresponding COQ developments are quite compact. To our knowledge, these are
the first certified (in the sense of formally proved) impossibility results for
robot networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4244</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4244</id><created>2013-06-18</created><updated>2013-11-26</updated><authors><author><keyname>Barbulescu</keyname><forenames>Razvan</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Gaudry</keyname><forenames>Pierrick</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Joux</keyname><forenames>Antoine</forenames><affiliation>PRISM</affiliation></author><author><keyname>Thom&#xe9;</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>A quasi-polynomial algorithm for discrete logarithm in finite fields of
  small characteristic</title><categories>cs.CR math.NT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present work, we present a new discrete logarithm algorithm, in the
same vein as in recent works by Joux, using an asymptotically more efficient
descent approach. The main result gives a quasi-polynomial heuristic complexity
for the discrete logarithm problem in finite field of small characteristic. By
quasi-polynomial, we mean a complexity of type $n^{O(\log n)}$ where $n$ is the
bit-size of the cardinality of the finite field. Such a complexity is smaller
than any $L(\varepsilon)$ for $\epsilon&gt;0$. It remains super-polynomial in the
size of the input, but offers a major asymptotic improvement compared to
$L(1/4+o(1))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4245</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4245</id><created>2013-06-18</created><authors><author><keyname>G&#xf3;mez</keyname><forenames>Omar S.</forenames></author><author><keyname>Bat&#xfa;n</keyname><forenames>Jos&#xe9; L.</forenames></author><author><keyname>Aguilar</keyname><forenames>Ra&#xfa;l A.</forenames></author></authors><title>Pair versus Solo Programming -- An Experience Report from a Course on
  Design of Experiments in Software Engineering</title><categories>cs.SE</categories><comments>9 pages; International Journal of Computer Science Issues, 2013</comments><msc-class>68N30</msc-class><acm-class>D.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents an experience report about an experiment that evaluates
duration and effort of pair and solo programming. The experiment was performed
as part of a course on Design of Experiments (DOE) in Software Engineering (SE)
at Autonomous University of Yucatan (UADY). A total of 21 junior student
subjects enrolled in the bachelor's degree program in SE participated in the
experiment. During the experiment, subjects (7 pairs and 7 solos) wrote two
small programs in two sessions. Results show a significant difference (at
alpha=0.1) in favor of pair programming regarding duration (28% decrease), and
a significant difference (at alpha=0.1) in favor of solo programming with
respect to effort (30% decrease). With only a difference of 1%, our results
regarding duration and effort are practically the same as those reported by
Nosek in 1998.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4253</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4253</id><created>2013-06-18</created><authors><author><keyname>Ning</keyname><forenames>Kang</forenames></author><author><keyname>Choi</keyname><forenames>Kwok Pui</forenames></author></authors><title>Systematic assessment of the expected length, variance and distribution
  of Longest Common Subsequences</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Longest Common Subsequence (LCS) problem is a very important problem in
math- ematics, which has a broad application in scheduling problems, physics
and bioinformatics. It is known that the given two random sequences of infinite
lengths, the expected length of LCS will be a constant. however, the value of
this constant is not yet known. Moreover, the variance distribution of LCS
length is also not fully understood. The problem becomes more difficult when
there are (a) multiple sequences, (b) sequences with non-even distribution of
alphabets and (c) large alphabets. This work focus on these more complicated
issues. We have systematically analyze the expected length, variance and
distribution of LCS based on extensive Monte Carlo simulation. The results on
expected length are consistent with currently proved theoretical results, and
the analysis on variance and distribution provide further insights into the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4263</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4263</id><created>2013-06-18</created><authors><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Jaroschek</keyname><forenames>Maximilian</forenames></author><author><keyname>Johansson</keyname><forenames>Fredrik</forenames></author></authors><title>Ore Polynomials in Sage</title><categories>cs.SC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Sage implementation of Ore algebras. The main features for the
most common instances include basic arithmetic and actions; gcrd and lclm;
D-finite closure properties; natural transformations between related algebras;
guessing; desingularization; solvers for polynomials, rational functions and
(generalized) power series. This paper is a tutorial on how to use the package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4265</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4265</id><created>2013-06-18</created><updated>2014-02-12</updated><authors><author><keyname>Maleki</keyname><forenames>Sasan</forenames></author><author><keyname>Tran-Thanh</keyname><forenames>Long</forenames></author><author><keyname>Hines</keyname><forenames>Greg</forenames></author><author><keyname>Rahwan</keyname><forenames>Talal</forenames></author><author><keyname>Rogers</keyname><forenames>Alex</forenames></author></authors><title>Bounding the Estimation Error of Sampling-based Shapley Value
  Approximation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shapley value is arguably the most central normative solution concept in
cooperative game theory. It specifies a unique way in which the reward from
cooperation can be &quot;fairly&quot; divided among players. While it has a wide range of
real world applications, its use is in many cases hampered by the hardness of
its computation. A number of researchers have tackled this problem by (i)
focusing on classes of games where the Shapley value can be computed
efficiently, or (ii) proposing representation formalisms that facilitate such
efficient computation, or (iii) approximating the Shapley value in certain
classes of games. For the classical \textit{characteristic function}
representation, the only attempt to approximate the Shapley value for the
general class of games is due to Castro \textit{et al.} \cite{castro}. While
this algorithm provides a bound on the approximation error, this bound is
\textit{asymptotic}, meaning that it only holds when the number of samples
increases to infinity. On the other hand, when a finite number of samples is
drawn, an unquantifiable error is introduced, meaning that the bound no longer
holds. With this in mind, we provide non-asymptotic bounds on the estimation
error for two cases: where (i) the \textit{variance}, and (ii) the
\textit{range}, of the players' marginal contributions is known. Furthermore,
for the second case, we show that when the range is significantly large
relative to the Shapley value, the bound can be improved (from $O(\frac{r}{m})$
to $O(\sqrt{\frac{r}{m}})$). Finally, we propose, and demonstrate the
effectiveness of using stratified sampling for improving the bounds further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4280</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4280</id><created>2013-06-18</created><authors><author><keyname>Mahmoud</keyname><forenames>Chaker Ben</forenames></author><author><keyname>Bettahar</keyname><forenames>Fathia</forenames></author><author><keyname>Abderrahim</keyname><forenames>Hajer</forenames></author><author><keyname>Saidi</keyname><forenames>Houda</forenames></author></authors><title>Towards a Graph-Based Approach for Web Services Composition</title><categories>cs.SE</categories><journal-ref>IJCSI Volume 10, Issue 1, January 2013 , Page 351</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, Web services (WS) remain a main actor in the implementation of
distributed applications. They represent a new promising paradigm for the
development, deployment and integration of Internet applications. The aim of
Web services composition is to use the skills of several departments to resolve
any problem that cannot be solved individually. The result of this composition
is a compound of Web services that define how they will be used. In this paper,
we propose an approach for automatic web services composition based on the
concepts of directed graphs for the representation and description of Web
services, and the ordering of web services compound execution. In this context,
the user query, defined by a set of inputs and outputs, can be viewed as a
directed graph composed of Web services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4287</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4287</id><created>2013-06-18</created><authors><author><keyname>Lewenstein</keyname><forenames>Moshe</forenames></author><author><keyname>Munro</keyname><forenames>J. Ian</forenames></author><author><keyname>Raman</keyname><forenames>Venkatesh</forenames></author></authors><title>Succinct data structures for representing equivalence classes</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a partition of an n element set into equivalence classes, we consider
time-space tradeoffs for representing it to support the query that asks whether
two given elements are in the same equivalence class. This has various
applications including for testing whether two vertices are in the same
component in an undirected graph or in the same strongly connected component in
a directed graph.
  We consider the problem in several models.
  -- Concerning labeling schemes where we assign labels to elements and the
query is to be answered just by examining the labels of the queried elements
(without any extra space): if each vertex is required to have a unique label,
then we show that a label space of (\sum_{i=1}^n \lfloor {n \over i} \rfloor)
is necessary and sufficient. In other words, \lg n + \lg \lg n + O(1) bits of
space are necessary and sufficient for representing each of the labels. This
slightly strengthens the known lower bound and is in contrast to the known
necessary and sufficient bound of \lceil \lg n \rceil for the label length, if
each vertex need not get a unique label.
  --Concerning succinct data structures for the problem when the n elements are
to be uniquely assigned labels from label set {1, 2, ...n}, we first show that
\Theta(\sqrt n) bits are necessary and sufficient to represent the equivalence
class information. This space includes the space for implicitly encoding the
vertex labels. We can support the query in such a structure in O(\lg n) time in
the standard word RAM model. We then develop structures resulting in one where
the queries can be supported in constant time using O({\sqrt n} \lg n) bits of
space. We also develop space efficient structures where union operation along
with the equivalence query can be answered fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4302</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4302</id><created>2013-06-18</created><authors><author><keyname>Farczadi</keyname><forenames>Linda</forenames></author><author><keyname>Georgiou</keyname><forenames>Konstantinos</forenames></author><author><keyname>Koenemann</keyname><forenames>Jochen</forenames></author></authors><title>Network bargaining with general capacities</title><categories>cs.GT</categories><comments>This is an extended version of a paper to appear at the 21st European
  Symposium on Algorithms (ESA 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study balanced solutions for network bargaining games with general
capacities, where agents can participate in a fixed but arbitrary number of
contracts. We provide the first polynomial time algorithm for computing
balanced solutions for these games. In addition, we prove that an instance has
a balanced solution if and only if it has a stable one. Our methods use a new
idea of reducing an instance with general capacities to a network bargaining
game with unit capacities defined on an auxiliary graph. This represents a
departure from previous approaches, which rely on computing an allocation in
the intersection of the core and prekernel of a corresponding cooperative game,
and then proving that the solution corresponding to this allocation is
balanced. In fact, we show that such cooperative game methods do not extend to
general capacity games, since contrary to the case of unit capacities, there
exist allocations in the intersection of the core and prekernel with no
corresponding balanced solution. Finally, we identify two sufficient conditions
under which the set of balanced solutions corresponds to the intersection of
the core and prekernel, thereby extending the class of games for which this
result was previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4303</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4303</id><created>2013-06-18</created><authors><author><keyname>Xu</keyname><forenames>Songcen</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Distributed conjugate gradient strategies for parameter estimation over
  sensor networks</title><categories>cs.IT math.IT</categories><comments>5 figures, 5 pages</comments><journal-ref>SSPD 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents distributed adaptive algorithms based on the conjugate
gradient (CG) method for distributed networks. Both incremental and diffusion
adaptive solutions are all considered. The distributed conventional (CG) and
modified CG (MCG) algorithms have an improved performance in terms of mean
square error as compared with least-mean square (LMS)-based algorithms and a
performance that is close to recursive least-squares (RLS) algorithms . The
resulting algorithms are distributed, cooperative and able to respond in real
time to changes in the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4308</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4308</id><created>2013-06-18</created><authors><author><keyname>Sba&#xef;</keyname><forenames>Zohra</forenames></author><author><keyname>Barkaoui</keyname><forenames>Kamel</forenames></author></authors><title>V\'erification Formelle des Processus Workflow Collaboratifs</title><categories>cs.SE cs.LO</categories><comments>14 pages, 6 figures, In 1\`ere conf\'erence francophone sur les
  Syst\`emes Collaboratifs (SysCo), pp., 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a method of verification of collaborative workflow
processes based on model checking techniques. In particular, we propose to
verify soundness properties of these processes using SPIN model checker. First
we translate the adopted specification of workflows (i.e. the WF-net) to
Promela which is the description language of models to be verified by SPIN.
Then we express the soundness properties in Linear Temporal Logic (LTL) and use
SPIN to test whether each property is satisfied by the Promela model of the
WF-net in question. Finally, we express the properties of k-soundness for
WF-nets modeling multiple instances and (k,R)-soundness for workflow processes
with multiple instances and sharing resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4340</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4340</id><created>2013-06-18</created><updated>2013-12-21</updated><authors><author><keyname>Alc&#xe1;zar</keyname><forenames>Juan Gerardo</forenames></author><author><keyname>Hermoso</keyname><forenames>Carlos</forenames></author><author><keyname>Muntingh</keyname><forenames>Georg</forenames></author></authors><title>Detecting Similarity of Rational Plane Curves</title><categories>math.AG cs.CG cs.SC</categories><comments>22 pages</comments><msc-class>14Q05, 68W30</msc-class><journal-ref>Journal of Computational and Applied Mathematics, Volume 269
  (2014), Pages 1 - 13</journal-ref><doi>10.1016/j.cam.2014.03.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel and deterministic algorithm is presented to detect whether two given
rational plane curves are related by means of a similarity, which is a central
question in Pattern Recognition. As a by-product it finds all such
similarities, and the particular case of equal curves yields all symmetries. A
complete theoretical description of the method is provided, and the method has
been implemented and tested in the Sage system for curves of moderate degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4345</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4345</id><created>2013-06-18</created><authors><author><keyname>Metre</keyname><forenames>Vishakha</forenames></author><author><keyname>Ghorpade</keyname><forenames>Jayshree</forenames></author></authors><title>An Overview of the Research on Texture Based Plant Leaf Classification</title><categories>cs.CV</categories><comments>12 pages,5 figures and 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plant classification has a broad application prospective in agriculture and
medicine, and is especially significant to the biology diversity research. As
plants are vitally important for environmental protection, it is more important
to identify and classify them accurately. Plant leaf classification is a
technique where leaf is classified based on its different morphological
features. The goal of this paper is to provide an overview of different aspects
of texture based plant leaf classification and related things. At last we will
be concluding about the efficient method i.e. the method that gives better
performance compared to the other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4350</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4350</id><created>2013-06-18</created><authors><author><keyname>Khina</keyname><forenames>Anatoly</forenames></author><author><keyname>Hitron</keyname><forenames>Ayal</forenames></author><author><keyname>Livni</keyname><forenames>Idan</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author></authors><title>Joint Unitary Triangularization for Gaussian Multi-User MIMO Networks</title><categories>cs.IT math.IT</categories><comments>A shortened version was submitted to IEEE transactions on information
  theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicast is the general method of conveying the same information to multiple
users over a broadcast channel. In this work, the Gaussian multiple-input
multiple-output broadcast channel is considered, with multiple receive nodes,
each equipped with an arbitrary number of antennas. A &quot;closed loop&quot; scenario is
assumed, for which a practical multicast scheme is constructed which approaches
capacity, by applying judiciously chosen unitary operations at the transmit and
receives nodes that triangularize the channel matrices such that the resulting
matrices have equal diagonals. This, along with the utilization of successive
interference cancellation, reduces the coding and decoding tasks to those of
coding and decoding over the single-antenna additive white Gaussian noise
channel. Over the resulting effective channel, any &quot;off-the-shelf&quot; code may be
employed. For the two-user case, it was recently shown that such joint unitary
triangularization is always possible. In this work it is shown that for more
users, joint triangularization of the time extensions of the channel matrices
is necessary in general, which corresponds to carrying out the unitary
processing over multiple channel uses. It is further shown that exact
triangularization, where all resulting diagonals are equal, is not always
possible, and appropriate conditions for the existence of such are established
for certain cases. When exact triangularization is not possible, an approximate
construction is proposed, that achieves the desired equal diagonals up to
constant-length prefix and suffix. By enlarging the number of channel uses
processed together, the loss in rate due to the prefix and the suffix can be
made arbitrarily small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4353</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4353</id><created>2013-06-18</created><updated>2013-06-20</updated><authors><author><keyname>Chauve</keyname><forenames>Cedric</forenames></author><author><keyname>Patterson</keyname><forenames>Murray</forenames></author><author><keyname>Rajaraman</keyname><forenames>Ashok</forenames></author></authors><title>Hypergraph covering problems motivated by genome assembly questions</title><categories>cs.DS</categories><comments>13 pages+3 page appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Consecutive-Ones Property (C1P) is a classical concept in discrete
mathematics that has been used in several genomics applications, from physical
mapping of contemporary genomes to the assembly of ancient genomes. A common
issue in genome assembly concerns repeats, genomic sequences that appear in
several locations of a genome. Handling repeats leads to a variant of the C1P,
the C1P with multiplicity (mC1P), that can also be seen as the problem of
covering edges of hypergraphs by linear and circular walks. In the present
work, we describe variants of the mC1P that address specific issues of genome
assembly, and polynomial time or fixed-parameter algorithms to solve them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4355</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4355</id><created>2013-06-18</created><authors><author><keyname>Sch&#xfc;lke</keyname><forenames>Christophe</forenames></author><author><keyname>Caltagirone</keyname><forenames>Francesco</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Blind Calibration in Compressed Sensing using Message Passing Algorithms</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><journal-ref>Advances in Neural Information Processing Systems 26 (NIPS 2013),
  pp 566--574</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) is a concept that allows to acquire compressible
signals with a small number of measurements. As such it is very attractive for
hardware implementations. Therefore, correct calibration of the hardware is a
central is- sue. In this paper we study the so-called blind calibration, i.e.
when the training signals that are available to perform the calibration are
sparse but unknown. We extend the approximate message passing (AMP) algorithm
used in CS to the case of blind calibration. In the calibration-AMP, both the
gains on the sensors and the elements of the signals are treated as unknowns.
Our algorithm is also applica- ble to settings in which the sensors distort the
measurements in other ways than multiplication by a gain, unlike previously
suggested blind calibration algorithms based on convex relaxations. We study
numerically the phase diagram of the blind calibration problem, and show that
even in cases where convex relaxation is pos- sible, our algorithm requires a
smaller number of measurements and/or signals in order to perform well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4359</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4359</id><created>2013-06-18</created><authors><author><keyname>Bajuelos</keyname><forenames>Antonio Leslie</forenames></author><author><keyname>Canales</keyname><forenames>Santiago</forenames></author><author><keyname>Hern&#xe1;ndez</keyname><forenames>Gregorio</forenames></author><author><keyname>Martins</keyname><forenames>Mafalda</forenames></author><author><keyname>Matos</keyname><forenames>In&#xea;s</forenames></author></authors><title>Some Results on Open Edge and Open Mobile Guarding of Polygons and
  Triangulations</title><categories>cs.CG</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on a variation of the Art Gallery problem that considers
open edge guards and open mobile guards. A mobile guard can be placed on edges
and diagonals of a polygon, and the &quot;open&quot; prefix means that the endpoints of
such edge or diagonal are not taken into account for visibility purposes. This
paper studies the number of guards that are sufficient and sometimes necessary
to guard some classes of simple polygons for both open edge and open mobile
guards. This problem is also considered for planar triangulation graphs using
open edge guards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4361</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4361</id><created>2013-06-18</created><authors><author><keyname>Anderson</keyname><forenames>Collin</forenames></author></authors><title>Dimming the Internet: Detecting Throttling as a Mechanism of Censorship
  in Iran</title><categories>cs.NI</categories><comments>Working Draft</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the days immediately following the contested June 2009 Presidential
election, Iranians attempting to reach news content and social media platforms
were subject to unprecedented levels of the degradation, blocking and jamming
of communications channels. Rather than shut down networks, which would draw
attention and controversy, the government was rumored to have slowed connection
speeds to rates that would render the Internet nearly unusable, especially for
the consumption and distribution of multimedia content. Since, political
upheavals elsewhere have been associated with headlines such as &quot;High usage
slows down Internet in Bahrain&quot; and &quot;Syrian Internet slows during Friday
protests once again,&quot; with further rumors linking poor connectivity with
political instability in Myanmar and Tibet. For governments threatened by
public expression, the throttling of Internet connectivity appears to be an
increasingly preferred and less detectable method of stifling the free flow of
information. In order to assess this perceived trend and begin to create
systems of accountability and transparency on such practices, we attempt to
outline an initial strategy for utilizing a ubiquitious set of network
measurements as a monitoring service, then apply such methodology to shed light
on the recent history of censorship in Iran.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4363</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4363</id><created>2013-06-18</created><authors><author><keyname>Merritt</keyname><forenames>Sears</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Social Network Dynamics in a Massive Online Game: Network Turnover,
  Non-densification, and Team Engagement in Halo Reach</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>8 pages, 13 figures</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online multiplayer games are a popular form of social interaction, used by
hundreds of millions of individuals. However, little is known about the social
networks within these online games, or how they evolve over time. Understanding
human social dynamics within massive online games can shed new light on social
interactions in general and inform the development of more engaging systems.
Here, we study a novel, large friendship network, inferred from nearly 18
billion social interactions over 44 weeks between 17 million individuals in the
popular online game Halo: Reach. This network is one of the largest, most
detailed temporal interaction networks studied to date, and provides a novel
perspective on the dynamics of online friendship networks, as opposed to mere
interaction graphs. Initially, this network exhibits strong structural turnover
and decays rapidly from a peak size. In the following period, however, both
network size and turnover stabilize, producing a dynamic structural
equilibrium. In contrast to other studies, we find that the Halo friendship
network is non-densifying: both the mean degree and the average pairwise
distance are stable, suggesting that densification cannot occur when
maintaining friendships is costly. Finally, players with greater long-term
engagement exhibit stronger local clustering, suggesting a group-level social
engagement process. These results demonstrate the utility of online games for
studying social networks, shed new light on empirical temporal graph patterns,
and clarify the claims of universality of network densification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4384</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4384</id><created>2013-06-18</created><updated>2013-10-07</updated><authors><author><keyname>Louis</keyname><forenames>Anand</forenames></author><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author></authors><title>Approximation Algorithm for Sparsest k-Partitioning</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G$, the sparsest-cut problem asks to find the set of vertices
$S$ which has the least expansion defined as $$\phi_G(S) :=
\frac{w(E(S,\bar{S}))}{\min \set{w(S), w(\bar{S})}}, $$ where $w$ is the total
edge weight of a subset. Here we study the natural generalization of this
problem: given an integer $k$, compute a $k$-partition $\set{P_1, \ldots, P_k}$
of the vertex set so as to minimize $$ \phi_k(\set{P_1, \ldots, P_k}) := \max_i
\phi_G(P_i). $$ Our main result is a polynomial time bi-criteria approximation
algorithm which outputs a $(1 - \e)k$-partition of the vertex set such that
each piece has expansion at most $O_{\varepsilon}(\sqrt{\log n \log k})$ times
$OPT$. We also study balanced versions of this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4391</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4391</id><created>2013-06-18</created><updated>2013-10-15</updated><authors><author><keyname>Soni</keyname><forenames>Akshay</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis</forenames></author></authors><title>On the Fundamental Limits of Recovering Tree Sparse Vectors from Noisy
  Linear Measurements</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>33 pages, 5 figures, IEEE Transactions on Information Theory
  (accepted for publication)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent breakthrough results in compressive sensing (CS) have established that
many high dimensional signals can be accurately recovered from a relatively
small number of non-adaptive linear observations, provided that the signals
possess a sparse representation in some basis. Subsequent efforts have shown
that the performance of CS can be improved by exploiting additional structure
in the locations of the nonzero signal coefficients during inference, or by
utilizing some form of data-dependent adaptive measurement focusing during the
sensing process. To our knowledge, our own previous work was the first to
establish the potential benefits that can be achieved when fusing the notions
of adaptive sensing and structured sparsity -- that work examined the task of
support recovery from noisy linear measurements, and established that an
adaptive sensing strategy specifically tailored to signals that are tree-sparse
can significantly outperform adaptive and non-adaptive sensing strategies that
are agnostic to the underlying structure. In this work we establish fundamental
performance limits for the task of support recovery of tree-sparse signals from
noisy measurements, in settings where measurements may be obtained either
non-adaptively (using a randomized Gaussian measurement strategy motivated by
initial CS investigations) or by any adaptive sensing strategy. Our main
results here imply that the adaptive tree sensing procedure analyzed in our
previous work is nearly optimal, in the sense that no other sensing and
estimation strategy can perform fundamentally better for identifying the
support of tree-sparse signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4401</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4401</id><created>2013-06-18</created><updated>2013-11-05</updated><authors><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Voter models with contrarian agents</title><categories>physics.soc-ph cs.SI</categories><comments>3 figures, 1 table</comments><journal-ref>Physical Review E, 88, 052803 (2013)</journal-ref><doi>10.1103/PhysRevE.88.052803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the voter and many other opinion formation models, agents are assumed to
behave as congregators (also called the conformists); they are attracted to the
opinions of others. In this study, I investigate linear extensions of the voter
model with contrarian agents. An agent is either congregator or contrarian and
assumes a binary opinion. I investigate three models that differ in the
behavior of the contrarian toward other agents. In model 1, contrarians mimic
the opinions of other contrarians and oppose (i.e., try to select the opinion
opposite to) those of congregators. In model 2, contrarians mimic the opinions
of congregators and oppose those of other contrarians. In model 3, contrarians
oppose anybody. In all models, congregators are assumed to like anybody. I show
that even a small number of contrarians prohibits the consensus in the entire
population to be reached in all three models. I also obtain the equilibrium
distributions using the van Kampen small-fluctuation approximation and the
Fokker-Planck equation for the case of many contrarians and a single
contrarian, respectively. I show that the fluctuation around the symmetric
coexistence equilibrium is much larger in model 2 than in models 1 and 3 when
contrarians are rare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4409</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4409</id><created>2013-06-18</created><authors><author><keyname>Saadi</keyname><forenames>Mostafa</forenames></author><author><keyname>Hasnaoui</keyname><forenames>Moulay Lahcen</forenames></author><author><keyname>Hssane</keyname><forenames>Abderrahim Beni</forenames></author><author><keyname>Benkirane</keyname><forenames>Said</forenames></author><author><keyname>Laghdir</keyname><forenames>Mohamed</forenames></author></authors><title>Energy-Aware Scheme used in Multi-level Heterogeneous Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>7 pages, 8 Figures</comments><journal-ref>International Journal of Computer Science Issues (IJCSI), 10(1) :
  pages 96-102, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wireless sensor networks (WSNs) is a power constrained system, since
nodes run on limited power batteries which shorten its lifespan.The main
challenge facing us in the design and conception of Wireless Sensor Networks
(WSNs) is to find the best way to extend their life span. The clustering
algorithm is a key technique used to increase the scalability and life span of
the network in general. In this paper, we propose and evaluate a distributed
energy-efficient clustering algorithm for WSNs. This heterogeneous-energy
protocol is a new clustering algorithm to decrease probability of failure nodes
and in which we introduce the node's remaining energy so as to determine the
cluster heads. We study the impact of heterogeneity of nodes on WSNs that are
hierarchically clustered. Finally, simulation results show that the proposed
algorithm increases the life span of the whole network and performs better than
LEACH and EEHC according to the metric:first node dies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4410</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4410</id><created>2013-06-18</created><authors><author><keyname>Wang</keyname><forenames>Junhui</forenames></author></authors><title>Joint estimation of sparse multivariate regression and conditional
  graphical models</title><categories>stat.ML cs.LG</categories><comments>29 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate regression model is a natural generalization of the classical
univari- ate regression model for ?tting multiple responses. In this paper, we
propose a high- dimensional multivariate conditional regression model for
constructing sparse estimates of the multivariate regression coe?cient matrix
that accounts for the dependency struc- ture among the multiple responses. The
proposed method decomposes the multivariate regression problem into a series of
penalized conditional log-likelihood of each response conditioned on the
covariates and other responses. It allows simultaneous estimation of the sparse
regression coe?cient matrix and the sparse inverse covariance matrix. The
asymptotic selection consistency and normality are established for the
diverging dimension of the covariates and number of responses. The e?ectiveness
of the pro- posed method is also demonstrated in a variety of simulated
examples as well as an application to the Glioblastoma multiforme cancer data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4411</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4411</id><created>2013-06-18</created><updated>2013-06-19</updated><authors><author><keyname>Baral</keyname><forenames>Chitta</forenames></author><author><keyname>Vo</keyname><forenames>Nguyen H.</forenames></author></authors><title>Event-Object Reasoning with Curated Knowledge Bases: Deriving Missing
  Information</title><categories>cs.AI</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broader goal of our research is to formulate answers to why and how
questions with respect to knowledge bases, such as AURA. One issue we face when
reasoning with many available knowledge bases is that at times needed
information is missing. Examples of this include partially missing information
about next sub-event, first sub-event, last sub-event, result of an event,
input to an event, destination of an event, and raw material involved in an
event. In many cases one can recover part of the missing knowledge through
reasoning. In this paper we give a formal definition about how such missing
information can be recovered and then give an ASP implementation of it. We then
discuss the implication of this with respect to answering why and how
questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4414</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4414</id><created>2013-06-18</created><authors><author><keyname>Chang</keyname><forenames>Ronald Y.</forenames></author><author><keyname>Lin</keyname><forenames>Sian-Jheng</forenames></author><author><keyname>Chung</keyname><forenames>Wei-Ho</forenames></author></authors><title>Symbol and Bit Mapping Optimization for Physical-Layer Network Coding
  with Pulse Amplitude Modulation</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Wireless
  Communications, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a two-way relay network in which two users
exchange messages through a single relay using a physical-layer network coding
(PNC) based protocol. The protocol comprises two phases of communication. In
the multiple access (MA) phase, two users transmit their modulated signals
concurrently to the relay, and in the broadcast (BC) phase, the relay
broadcasts a network-coded (denoised) signal to both users. Nonbinary and
binary network codes are considered for uniform and nonuniform pulse amplitude
modulation (PAM) adopted in the MA phase, respectively. We examine the effect
of different choices of symbol mapping (i.e., mapping from the denoised signal
to the modulation symbols at the relay) and bit mapping (i.e., mapping from the
modulation symbols to the source bits at the user) on the system error-rate
performance. A general optimization framework is proposed to determine the
optimal symbol/bit mappings with joint consideration of noisy transmissions in
both communication phases. Complexity-reduction techniques are developed for
solving the optimization problems. It is shown that the optimal symbol/bit
mappings depend on the signal-to-noise ratio (SNR) of the channel and the
modulation scheme. A general strategy for choosing good symbol/bit mappings is
also presented based on a high-SNR analysis, which suggests using a symbol
mapping that aligns the error patterns in both communication phases and Gray
and binary bit mappings for uniform and nonuniform PAM, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4418</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4418</id><created>2013-06-19</created><authors><author><keyname>Chu</keyname><forenames>Geoffrey</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author></authors><title>Structure Based Extended Resolution for Constraint Programming</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nogood learning is a powerful approach to reducing search in Constraint
Programming (CP) solvers. The current state of the art, called Lazy Clause
Generation (LCG), uses resolution to derive nogoods expressing the reasons for
each search failure. Such nogoods can prune other parts of the search tree,
producing exponential speedups on a wide variety of problems. Nogood learning
solvers can be seen as resolution proof systems. The stronger the proof system,
the faster it can solve a CP problem. It has recently been shown that the proof
system used in LCG is at least as strong as general resolution. However,
stronger proof systems such as \emph{extended resolution} exist. Extended
resolution allows for literals expressing arbitrary logical concepts over
existing variables to be introduced and can allow exponentially smaller proofs
than general resolution. The primary problem in using extended resolution is to
figure out exactly which literals are useful to introduce. In this paper, we
show that we can use the structural information contained in a CP model in
order to introduce useful literals, and that this can translate into
significant speedups on a range of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4427</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4427</id><created>2013-06-19</created><authors><author><keyname>Anil</keyname><forenames>Nithin K.</forenames></author><author><keyname>Kurian</keyname><forenames>Sharath Basil</forenames></author><author><keyname>T</keyname><forenames>Aby Abahai</forenames></author><author><keyname>Varghese</keyname><forenames>Surekha Mariam</forenames></author></authors><title>Multidimensional User Data Model for Web Personalization</title><categories>cs.IR</categories><comments>6 pages, 3 figures -&quot;Published with International Journal of Computer
  Applications (IJCA)&quot;</comments><msc-class>68U35, 68N99, 68T01</msc-class><acm-class>H.3.3; H.3.4</acm-class><journal-ref>International Journal of Computer Applications, Volume 69, No.12,
  May 2013</journal-ref><doi>10.5120/11896-7955 10.5120/11896-7955 10.5120/11896-7955</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalization is being applied to great extend in many systems. This paper
presents a multi-dimensional user data model and its application in web search.
Online and Offline activities of the user are tracked for creating the user
model. The main phases are identification of relevant documents and the
representation of relevance and similarity of the documents. The concepts
Keywords, Topics, URLs and clusters are used in the implementation. The
algorithms for profiling, grading and clustering the concepts in the user model
and algorithm for determining the personalized search results by re-ranking the
results in a search bank are presented in this paper. Simple experiments for
evaluation of the model and their results are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4447</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4447</id><created>2013-06-19</created><authors><author><keyname>Ateniese</keyname><forenames>Giuseppe</forenames></author><author><keyname>Felici</keyname><forenames>Giovanni</forenames></author><author><keyname>Mancini</keyname><forenames>Luigi V.</forenames></author><author><keyname>Spognardi</keyname><forenames>Angelo</forenames></author><author><keyname>Villani</keyname><forenames>Antonio</forenames></author><author><keyname>Vitali</keyname><forenames>Domenico</forenames></author></authors><title>Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data
  from Machine Learning Classifiers</title><categories>cs.CR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Learning (ML) algorithms are used to train computers to perform a
variety of complex tasks and improve with experience. Computers learn how to
recognize patterns, make unintended decisions, or react to a dynamic
environment. Certain trained machines may be more effective than others because
they are based on more suitable ML algorithms or because they were trained
through superior training sets. Although ML algorithms are known and publicly
released, training sets may not be reasonably ascertainable and, indeed, may be
guarded as trade secrets. While much research has been performed about the
privacy of the elements of training sets, in this paper we focus our attention
on ML classifiers and on the statistical information that can be unconsciously
or maliciously revealed from them. We show that it is possible to infer
unexpected but useful information from ML classifiers. In particular, we build
a novel meta-classifier and train it to hack other classifiers, obtaining
meaningful information about their training sets. This kind of information
leakage can be exploited, for example, by a vendor to build more effective
classifiers or to simply acquire trade secrets from a competitor's apparatus,
potentially violating its intellectual property rights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4450</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4450</id><created>2013-06-19</created><authors><author><keyname>Faouzi</keyname><forenames>Ahmed</forenames></author><author><keyname>Mabrouki</keyname><forenames>Charif</forenames></author><author><keyname>Semma</keyname><forenames>Alami</forenames></author></authors><title>Modeling a repository of modules for ports Terminals Operating System
  (TOS)</title><categories>cs.SE</categories><comments>8 pages</comments><journal-ref>IJCSI-2013-10-1-5041</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is the modeling of a repository for modules and
interfaces that must include all integrated information system management of a
port terminal.Modules will provide a basic framework necessary for automatic
management of internal operations and activities of all Port Terminals
worldwide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4454</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4454</id><created>2013-06-19</created><updated>2013-09-17</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author></authors><title>Which percentile-based approach should be preferred for calculating
  normalized citation impact values? An empirical comparison of five approaches
  including a newly developed citation-rank approach (P100)</title><categories>cs.DL stat.AP</categories><comments>Accepted for publication in the Journal of Informetrics</comments><msc-class>62Pxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Percentile-based approaches have been proposed as a non-parametric
alternative to parametric central-tendency statistics to normalize observed
citation counts. Percentiles are based on an ordered set of citation counts in
a reference set, whereby the fraction of papers at or below the citation counts
of a focal paper is used as an indicator for its relative citation impact in
the set. In this study, we pursue two related objectives: (1) although
different percentile-based approaches have been developed, an approach is
hitherto missing that satisfies a number of criteria such as scaling of the
percentile ranks from zero (all other papers perform better) to 100 (all other
papers perform worse), and solving the problem with tied citation ranks
unambiguously. We introduce a new citation-rank approach having these
properties, namely P100. (2) We compare the reliability of P100 empirically
with other percentile-based approaches, such as the approaches developed by the
SCImago group, the Centre for Science and Technology Studies (CWTS), and
Thomson Reuters (InCites), using all papers published in 1980 in Thomson
Reuters Web of Science (WoS). How accurately can the different approaches
predict the long-term citation impact in 2010 (in year 31) using citation
impact measured in previous time windows (years 1 to 30)? The comparison of the
approaches shows that the method used by InCites overestimates citation impact
(because of using the highest percentile rank when papers are assigned to more
than a single subject category) whereas the SCImago indicator shows higher
power in predicting the long-term citation impact on the basis of citation
rates in early years. Since the results show a disadvantage in this predictive
ability for P100 against the other approaches, there is still room for further
improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4460</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4460</id><created>2013-06-19</created><authors><author><keyname>Certicky</keyname><forenames>Michal</forenames></author></authors><title>Implementing a Wall-In Building Placement in StarCraft with Declarative
  Programming</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real-time strategy games like StarCraft, skilled players often block the
entrance to their base with buildings to prevent the opponent's units from
getting inside. This technique, called &quot;walling-in&quot;, is a vital part of
player's skill set, allowing him to survive early aggression. However, current
artificial players (bots) do not possess this skill, due to numerous
inconveniences surfacing during its implementation in imperative languages like
C++ or Java. In this text, written as a guide for bot programmers, we address
the problem of finding an appropriate building placement that would block the
entrance to player's base, and present a ready to use declarative solution
employing the paradigm of answer set programming (ASP). We also encourage the
readers to experiment with different declarative approaches to this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4466</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4466</id><created>2013-06-19</created><updated>2013-06-24</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Gao</keyname><forenames>Yihan</forenames></author><author><keyname>Mao</keyname><forenames>Jieming</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zuo</keyname><forenames>Song</forenames></author></authors><title>New upper bound on block sensitivity and certificate complexity in terms
  of sensitivity</title><categories>cs.CC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensitivity \cite{CD82,CDR86} and block sensitivity \cite{Nisan91} are two
important complexity measures of Boolean functions. A longstanding open problem
in decision tree complexity, the &quot;Sensitivity versus Block Sensitivity&quot;
question, proposed by Nisan and Szegedy \cite{Nisan94} in 1992, is whether
these two complexity measures are polynomially related, i.e., whether
$bs(f)=O(s(f)^{O(1)})$.
  We prove an new upper bound on block sensitivity in terms of sensitivity:
$bs(f) \leq 2^{s(f)-1} s(f)$. Previously, the best upper bound on block
sensitivity was $bs(f) \leq (\frac{e}{\sqrt{2\pi}}) e^{s(f)} \sqrt{s(f)}$ by
Kenyon and Kutin \cite{KK}. We also prove that if $\min\{s_0(f),s_1(f)\}$ is a
constant, then sensitivity and block sensitivity are linearly related, i.e.
$bs(f)=O(s(f))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4473</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4473</id><created>2013-06-19</created><updated>2013-06-26</updated><authors><author><keyname>Pacheco</keyname><forenames>Hugo</forenames></author><author><keyname>Macedo</keyname><forenames>Nuno</forenames></author><author><keyname>Cunha</keyname><forenames>Alcino</forenames></author><author><keyname>Voigtl&#xe4;nder</keyname><forenames>Janis</forenames></author></authors><title>A Generic Scheme and Properties of Bidirectional Transformations</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent rise of interest in bidirectional transformations (BXs) has led to
the development of many BX frameworks, originating in diverse computer science
disciplines. From a user perspective, these frameworks vary significantly in
both interface and predictability of the underlying bidirectionalization
technique. In this paper we start by presenting a generic BX scheme that can be
instantiated to different concrete interfaces, by plugging-in the desired
notion of update and traceability. Based on that scheme, we then present
several desirable generic properties that may characterize a BX framework, and
show how they can be instantiated to concrete interfaces. This generic
presentation is useful when exploring the BX design space: it might help
developers when designing new frameworks and end-users when comparing existing
ones. We support the latter claim, by applying it in a comparative survey of
popular existing BX frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4478</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4478</id><created>2013-06-19</created><updated>2014-10-28</updated><authors><author><keyname>Wuhrer</keyname><forenames>Stefanie</forenames></author><author><keyname>Lang</keyname><forenames>Jochen</forenames></author><author><keyname>Tekieh</keyname><forenames>Motahareh</forenames></author><author><keyname>Shu</keyname><forenames>Chang</forenames></author></authors><title>Finite Element Based Tracking of Deforming Surfaces</title><categories>cs.CV cs.GR</categories><comments>additional experiments</comments><journal-ref>Graphical Models, 77(1), pp. 1-17, 2015</journal-ref><doi>10.1016/j.gmod.2014.10.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to robustly track the geometry of an object that
deforms over time from a set of input point clouds captured from a single
viewpoint. The deformations we consider are caused by applying forces to known
locations on the object's surface. Our method combines the use of prior
information on the geometry of the object modeled by a smooth template and the
use of a linear finite element method to predict the deformation. This allows
the accurate reconstruction of both the observed and the unobserved sides of
the object. We present tracking results for noisy low-quality point clouds
acquired by either a stereo camera or a depth camera, and simulations with
point clouds corrupted by different error terms. We show that our method is
also applicable to large non-linear deformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4479</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4479</id><created>2013-06-19</created><authors><author><keyname>Talel</keyname><forenames>Bessaoudi</forenames></author><author><keyname>Fay&#xe7;al</keyname><forenames>Ben Hmida</forenames></author></authors><title>Robust State and fault Estimation of Linear Discrete Time Systems with
  Unknown Disturbances</title><categories>cs.SY</categories><comments>6 pages, 3 figures, CIET'13 conference</comments><msc-class>93C05, 93C55</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new robust fault and state estimation based on
recursive least square filter for linear stochastic systems with unknown
disturbances. The novel elements of the algorithm are : a simple, easily
implementable, square root method which is shown to solve the numerical
problems affecting the unknown input filter algorithm and related information
filter and smoothing algorithms; an iterative framework, where information and
covariance filters and smoothing are sequentially run in order to estimate the
state and fault. This method provides a direct estimate of the state and fault
in a single block with a simple formulation. A numerical example is given in
order to illustrate the performance of the proposed filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4493</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4493</id><created>2013-06-19</created><authors><author><keyname>Bartocci</keyname><forenames>Ezio</forenames></author><author><keyname>Bortolussi</keyname><forenames>Luca</forenames></author><author><keyname>Nenzi</keyname><forenames>Laura</forenames></author></authors><title>A temporal logic approach to modular design of synthetic biological
  circuits</title><categories>cs.LO cs.ET q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach for the design of a synthetic biological circuit
whose behaviour is specified in terms of signal temporal logic (STL) formulae.
We first show how to characterise with STL formulae the input/output behaviour
of biological modules miming the classical logical gates (AND, NOT, OR). Hence,
we provide the regions of the parameter space for which these specifications
are satisfied. Given a STL specification of the target circuit to be designed
and the networks of its constituent components, we propose a methodology to
constrain the behaviour of each module, then identifying the subset of the
parameter space in which those constraints are satisfied, providing also a
measure of the robustness for the target circuit design. This approach, which
leverages recent results on the quantitative semantics of Signal Temporal
Logic, is illustrated by synthesising a biological implementation of an
half-adder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4495</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4495</id><created>2013-06-19</created><updated>2014-09-08</updated><authors><author><keyname>Pitarokoilis</keyname><forenames>Antonios</forenames></author><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Uplink Performance of Time-Reversal MRC in Massive MIMO Systems Subject
  to Phase Noise</title><categories>cs.IT math.IT</categories><comments>13 pages, 6 figures, 2 tables, IEEE Transactions on Wireless
  Communications (accepted)</comments><journal-ref>Wireless Communications, IEEE Transactions on , vol.14, no.2,
  pp.711--723, Feb. 2015</journal-ref><doi>10.1109/TWC.2014.2359018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-user multiple-input multiple-output (MU-MIMO) cellular systems with an
excess of base station (BS) antennas (Massive MIMO) offer unprecedented
multiplexing gains and radiated energy efficiency. Oscillator phase noise is
introduced in the transmitter and receiver radio frequency chains and severely
degrades the performance of communication systems. We study the effect of
oscillator phase noise in frequency-selective Massive MIMO systems with
imperfect channel state information (CSI). In particular, we consider two
distinct operation modes, namely when the phase noise processes at the $M$ BS
antennas are identical (synchronous operation) and when they are independent
(non-synchronous operation). We analyze a linear and low-complexity
time-reversal maximum-ratio combining (TR-MRC) reception strategy. For both
operation modes we derive a lower bound on the sum-capacity and we compare
their performance. Based on the derived achievable sum-rates, we show that with
the proposed receive processing an $O(\sqrt{M})$ array gain is achievable. Due
to the phase noise drift the estimated effective channel becomes progressively
outdated. Therefore, phase noise effectively limits the length of the interval
used for data transmission and the number of scheduled users. The derived
achievable rates provide insights into the optimum choice of the data interval
length and the number of scheduled users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4512</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4512</id><created>2013-06-19</created><authors><author><keyname>Nyman</keyname><forenames>Henrik J</forenames></author><author><keyname>Sarlin</keyname><forenames>Peter</forenames></author></authors><title>From Bits to Atoms: 3D Printing in the Context of Supply Chain
  Strategies</title><categories>cs.CY</categories><comments>Submitted to HICSS-47</comments><journal-ref>HICSS 2014: 4190-4199</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A lot of attention in supply chain management has been devoted to
understanding customer requirements. What are customer priorities in terms of
price and service level, and how can companies go about fulfilling these
requirements in an optimal way? New manufacturing technology in the form of 3D
printing is about to change some of the underlying assumptions for different
supply chain set-ups. This paper explores opportunities and barriers of 3D
printing technology, specifically in a supply chain context. We are proposing a
set of principles that can act to bridge existing research on different supply
chain strategies and 3D printing. With these principles, researchers and
practitioners alike can better understand the opportunities and limitations of
3D printing in a supply chain management context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4514</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4514</id><created>2013-06-19</created><authors><author><keyname>Yousefbeiki</keyname><forenames>Mohsen</forenames></author><author><keyname>Perruisseau-Carrier</keyname><forenames>Julien</forenames></author></authors><title>Towards Compact and Frequency-Tunable Antenna Solutions for MIMO
  Transmission with a Single RF Chain</title><categories>cs.IT math.IT</categories><comments>9 pages</comments><journal-ref>IEEE Transactions on Antennas and Propagation, vol. PP, no. 99,
  2013</journal-ref><doi>10.1109/TAP.2013.2267197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a technique called beam-space MIMO has been demonstrated as an
effective approach for transmitting multiple signals while using a single
RF-chain. In this work, we present novel design considerations and a compact
antenna solution to stimulate the deployment of beam-space MIMO in future
wireless applications. Targeting integration in small wireless devices, the
novel antenna is made of a single integrated radiator rather than an array of
physically-separated dipoles. It also drastically simplifies the implementation
of variable loads and DC bias circuits for BPSK modulated signals, and does not
require any external reconfigurable matching circuit. Finally, we show that
this antenna system could be reconfigured by dynamic adjustment of terminating
loads to preserve its beam-space multiplexing capabilities over a 1:2 tuning
range, thereby promoting the convergence of MIMO and dynamic spectrum
allocation via reduced-complexity hardware. A prototype achieving
single-RF-chain multiplexing at a fixed frequency is designed and measured,
showing excellent agreement between simulations and measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4521</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4521</id><created>2013-06-19</created><authors><author><keyname>Ajwani</keyname><forenames>Deepak</forenames></author><author><keyname>Sitchinava</keyname><forenames>Nodari</forenames></author></authors><title>Empirical Evaluation of the Parallel Distribution Sweeping Framework on
  Multicore Architectures</title><categories>cs.DS cs.DC</categories><comments>Longer version of ESA'13 paper</comments><acm-class>F.2.2; D.1.3; D.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we perform an empirical evaluation of the Parallel External
Memory (PEM) model in the context of geometric problems. In particular, we
implement the parallel distribution sweeping framework of Ajwani, Sitchinava
and Zeh to solve batched 1-dimensional stabbing max problem. While modern
processors consist of sophisticated memory systems (multiple levels of caches,
set associativity, TLB, prefetching), we empirically show that algorithms
designed in simple models, that focus on minimizing the I/O transfers between
shared memory and single level cache, can lead to efficient software on current
multicore architectures. Our implementation exhibits significantly fewer
accesses to slow DRAM and, therefore, outperforms traditional approaches based
on plane sweep and two-way divide and conquer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4532</identifier>
 <datestamp>2014-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4532</id><created>2013-06-19</created><updated>2014-12-29</updated><authors><author><keyname>Duncan</keyname><forenames>Ross</forenames><affiliation>University of Strathclyde, Glasgow, UK</affiliation></author><author><keyname>Lucas</keyname><forenames>Maxime</forenames><affiliation>Universit&#xe9; Libre de Bruxelles, Brussels, Belgium</affiliation></author></authors><title>Verifying the Steane code with Quantomatic</title><categories>quant-ph cs.AI cs.LO</categories><comments>In Proceedings QPL 2013, arXiv:1412.7917</comments><proxy>EPTCS</proxy><acm-class>F.3.1</acm-class><journal-ref>EPTCS 171, 2014, pp. 33-49</journal-ref><doi>10.4204/EPTCS.171.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a partially mechanized proof of the correctness of
Steane's 7-qubit error correcting code, using the tool Quantomatic. To the best
of our knowledge, this represents the largest and most complicated verification
task yet carried out using Quantomatic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4534</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4534</id><created>2013-06-19</created><authors><author><keyname>Lima</keyname><forenames>Antonio</forenames></author><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>Pejovic</keyname><forenames>Veljko</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Exploiting Cellular Data for Disease Containment and Information
  Campaigns Strategies in Country-Wide Epidemics</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 9 figures. Appeared in Proceedings of NetMob 2013. Boston,
  MA, USA. May 2013</comments><report-no>School of Computer Science University of Birmingham Technical Report
  CSR-13-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human mobility is one of the key factors at the basis of the spreading of
diseases in a population. Containment strategies are usually devised on
movement scenarios based on coarse-grained assumptions. Mobility phone data
provide a unique opportunity for building models and defining strategies based
on very precise information about the movement of people in a region or in a
country. Another very important aspect is the underlying social structure of a
population, which might play a fundamental role in devising information
campaigns to promote vaccination and preventive measures, especially in
countries with a strong family (or tribal) structure.
  In this paper we analyze a large-scale dataset describing the mobility and
the call patterns of a large number of individuals in Ivory Coast. We present a
model that describes how diseases spread across the country by exploiting
mobility patterns of people extracted from the available data. Then, we
simulate several epidemics scenarios and we evaluate mechanisms to contain the
epidemic spreading of diseases, based on the information about people mobility
and social ties, also gathered from the phone call data. More specifically, we
find that restricting mobility does not delay the occurrence of an endemic
state and that an information campaign based on one-to-one phone conversations
among members of social groups might be an effective countermeasure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4542</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4542</id><created>2013-06-19</created><authors><author><keyname>Sanabria-Russo</keyname><forenames>Luis</forenames></author></authors><title>CSMA/ECA in Non-Saturation</title><categories>cs.NI</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the past month we have been trying to understand both the behavior of
CSMA/ECA and how our simulator works. In this report I try to asses some of
those doubts and provide a groundwork for discussion of past and new ideas for
further develop the MAC protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4546</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4546</id><created>2013-06-19</created><authors><author><keyname>Perez</keyname><forenames>Alexandre</forenames></author></authors><title>Dynamic Code Coverage with Progressive Detail Levels</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, locating software components responsible for observed failures is
one of the most expensive and error-prone tasks in the software development
process. To improve the debugging process efficiency, some effort was already
made to automatically assist the detection and location of software faults.
This led to the creation of statistical debugging tools such as Tarantula,
Zoltar and GZoltar. These tools use information gathered from code coverage
data and the result of test executions to return a list of potential faulty
locations. Although helpful, fault localization tools have some scaling
problems because of the fine-grained coverage data they need to perform the
fault localization analysis. Instrumentation overhead, which in some cases can
be as high as 50% is the main cause for their inefficiency. This thesis
proposes a new approach to this problem, avoiding as much as possible the high
level of coverage detail, while still using the proven techniques these fault
localization tools employ. This approach, named DCC, consists of using a
coarser initial instrumentation, obtaining only coverage traces for large
components. Then, the instrumentation detail of certain components is
progressively increased, based on the intermediate results provided by the same
techniques employed in current fault localization tools. To assess the validity
of our proposed approach, an empirical evaluation was performed, injecting
faults in four real-world software projects. The empirical evaluation
demonstrates that the DCC approach reduces the execution overhead that exists
in spectrum-based fault localization, and even presents a more concise
potential fault ranking to the user. We have observed execution time reductions
of 27% on average and diagnostic report size reductions of 63% on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4549</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4549</id><created>2013-06-19</created><authors><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Saab</keyname><forenames>Rayan</forenames></author><author><keyname>Y&#x131;lmaz</keyname><forenames>&#xd6;zg&#xfc;r</forenames></author></authors><title>Sigma-Delta quantization of sub-Gaussian frame expansions and its
  application to compressed sensing</title><categories>cs.IT math.IT math.NA</categories><comments>22 pages</comments><msc-class>94A12, 94A20, 41A25, 15B52</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that the collection $\{e_i\}_{i=1}^m$ forms a frame for $\R^k$, where
each entry of the vector $e_i$ is a sub-Gaussian random variable. We consider
expansions in such a frame, which are then quantized using a Sigma-Delta
scheme. We show that an arbitrary signal in $\R^k$ can be recovered from its
quantized frame coefficients up to an error which decays root-exponentially in
the oversampling rate $m/k$. Here the quantization scheme is assumed to be
chosen appropriately depending on the oversampling rate and the quantization
alphabet can be coarse. The result holds with high probability on the draw of
the frame uniformly for all signals. The crux of the argument is a bound on the
extreme singular values of the product of a deterministic matrix and a
sub-Gaussian frame. For fine quantization alphabets, we leverage this bound to
show polynomial error decay in the context of compressed sensing. Our results
extend previous results for structured deterministic frame expansions and
Gaussian compressed sensing measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4552</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4552</id><created>2013-06-19</created><updated>2013-11-06</updated><authors><author><keyname>Sandell</keyname><forenames>Magnus</forenames></author><author><keyname>Tosato</keyname><forenames>Filippo</forenames></author></authors><title>A Novel Lowest Density MDS Array Code</title><categories>cs.IT math.IT</categories><comments>Withdrawn by the author due to a major revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a novel MDS array code with lowest density. In
contrast to existing codes, this one has no restrictions on the size or the
number of erasures it can correct. It is based on a simple matrix construction
involving totally nonsingular matrices. We also introduce a simple decoding
algorithm based on the structure of the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4592</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4592</id><created>2013-06-19</created><authors><author><keyname>Dash</keyname><forenames>Tirtharaj</forenames></author></authors><title>Time Efficient Approach To Offline Hand Written Character Recognition
  Using Associative Memory Net</title><categories>cs.NE cs.CV</categories><journal-ref>International Journal of Computing and Business Research (IJCBR)
  ISSN (Online) : 2229-6166; Volume 3, Issue 3; September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an efficient Offline Hand Written Character Recognition
algorithm is proposed based on Associative Memory Net (AMN). The AMN used in
this work is basically auto associative. The implementation is carried out
completely in 'C' language. To make the system perform to its best with minimal
computation time, a Parallel algorithm is also developed using an API package
OpenMP. Characters are mainly English alphabets (Small (26), Capital (26))
collected from system (52) and from different persons (52). The characters
collected from system are used to train the AMN and characters collected from
different persons are used for testing the recognition ability of the net. The
detailed analysis showed that the network recognizes the hand written
characters with recognition rate of 72.20% in average case. However, in best
case, it recognizes the collected hand written characters with 88.5%. The
developed network consumes 3.57 sec (average) in Serial implementation and 1.16
sec (average) in Parallel implementation using OpenMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4595</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4595</id><created>2013-06-19</created><authors><author><keyname>Diop</keyname><forenames>Abdoulaye</forenames></author><author><keyname>Qi</keyname><forenames>Yue</forenames></author><author><keyname>Wang</keyname><forenames>Qin</forenames></author><author><keyname>Hussain</keyname><forenames>Shariq</forenames></author></authors><title>An Advanced Survey on Secure Energy-Efficient Hierarchical Routing
  Protocols in Wireless Sensor Networks</title><categories>cs.CR</categories><comments>11 pages, 7 tables, 4 figures,30 refences. arXiv admin note: text
  overlap with arXiv:1011.1529, arXiv:1301.5065 by other authors</comments><journal-ref>IJCSI , Volume 10, Issue 1, January 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Wireless Sensor Networks (WSNs) are often deployed in hostile environments,
which make such networks highly vulnerable and increase the risk of attacks
against this type of network. WSN comprise of large number of sensor nodes with
different hardware abilities and functions. Due to the limited memory resources
and energy constraints, complex security algorithms cannot be used in sensor
networks. Therefore, it is necessary to balance between the security level and
the associated energy consumption overhead to mitigate the security risks.
Hierarchical routing protocol is more energy-efficient than other routing
protocols in WSNs. Many secure cluster-based routing protocols have been
proposed in the literature to overcome these constraints. In this paper, we
discuss Secure Energy-Efficient Hierarchical Routing Protocols in WSNs and
compare them in terms of security, performance and efficiency. Security issues
for WSNs and their solutions are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4598</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4598</id><created>2013-06-19</created><authors><author><keyname>Gliwa</keyname><forenames>Bogdan</forenames></author><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author><author><keyname>Ko&#x17a;lak</keyname><forenames>Jaros&#x142;aw</forenames></author></authors><title>Analysis of roles and groups in blogosphere</title><categories>cs.SI physics.soc-ph</categories><comments>8th International Conference on Computer Recognition Systems, CORES
  2013</comments><journal-ref>Advances in Intelligent and Soft Computing volume 226. Springer,
  2013, pp.299-308</journal-ref><doi>10.1007/978-3-319-00969-8_29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper different roles of users in social media, taking into
consideration their strength of influence and different degrees of
cooperativeness, are introduced. Such identified roles are used for the
analysis of characteristics of groups of strongly connected entities. The
different classes of groups, considering the distribution of roles of users
belonging to them, are presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4606</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4606</id><created>2013-06-19</created><authors><author><keyname>Marujo</keyname><forenames>Luis</forenames></author><author><keyname>Viveiros</keyname><forenames>M&#xe1;rcio</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o Paulo da Silva</forenames></author></authors><title>Keyphrase Cloud Generation of Broadcast News</title><categories>cs.IR</categories><comments>In Proceeding of Interspeech 2011: 12th Annual Conference of the
  International Speech Communication Association</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an enhanced automatic keyphrase extraction method
applied to Broadcast News. The keyphrase extraction process is used to create a
concept level for each news. On top of words resulting from a speech
recognition system output and news indexation and it contributes to the
generation of a tag/keyphrase cloud of the top news included in a Multimedia
Monitoring Solution system for TV and Radio news/programs, running daily, and
monitoring 12 TV channels and 4 Radios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4608</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4608</id><created>2013-06-19</created><authors><author><keyname>Marujo</keyname><forenames>Luis</forenames></author><author><keyname>Bugalho</keyname><forenames>Miguel</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o Paulo da Silva</forenames></author><author><keyname>Gershman</keyname><forenames>Anatole</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author></authors><title>Hourly Traffic Prediction of News Stories</title><categories>cs.IR</categories><comments>In 3rd International Workshop on Context-Aware Recommender Systems
  held as part of the 5th ACM RecSys Conference 2011</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of predicting news stories popularity from several news sources
has become a challenge of great importance for both news producers and readers.
In this paper, we investigate methods for automatically predicting the number
of clicks on a news story during one hour. Our approach is a combination of
additive regression and bagging applied over a M5P regression tree using a
logarithmic scale (log10). The features included are social-based (social
network metadata from Facebook), content-based (automatically extracted
keyphrases, and stylometric statistics from news titles), and time-based. In
1st Sapo Data Challenge we obtained 11.99% as mean relative error value which
put us in the 4th place out of 26 participants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4621</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4621</id><created>2013-06-19</created><authors><author><keyname>Dash</keyname><forenames>Tirtharaj</forenames></author><author><keyname>Nayak</keyname><forenames>Tanistha</forenames></author></authors><title>English Character Recognition using Artificial Neural Network</title><categories>cs.NE</categories><comments>appeared in Proceedings of National Conference on Artificial
  Intelligence, Robotics and Embedded Systems (AIRES-2012), Andhra University,
  Vishakhapatnam, India (29-30 June, 2012), pp. 7-9</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on development of a Offline Hand Written English Character
Recognition algorithm based on Artificial Neural Network (ANN). The ANN
implemented in this work has single output neuron which shows whether the
tested character belongs to a particular cluster or not. The implementation is
carried out completely in 'C' language. Ten sets of English alphabets
(small-26, capital-26) were used to train the ANN and 5 sets of English
alphabets were used to test the network. The characters were collected from
different persons over duration of about 25 days. The algorithm was tested with
5 capital letters and 5 small letter sets. However, the result showed that the
algorithm recognized English alphabet patterns with maximum accuracy of 92.59%
and False Rejection Rate (FRR) of 0%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4622</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4622</id><created>2013-06-19</created><authors><author><keyname>Nayak</keyname><forenames>Tanistha</forenames></author><author><keyname>Dash</keyname><forenames>Tirtharaj</forenames></author></authors><title>Solution to Quadratic Equation Using Genetic Algorithm</title><categories>cs.NE</categories><comments>appeared in: Conf. Proceedings of National Conference on Artificial
  Intelligence, Robotics and Embedded Systems (AIRES-2012), Andhra University,
  Vishakhapatnam, India (29-30 June, 2012), pp. 10-13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving Quadratic equation is one of the intrinsic interests as it is the
simplest nonlinear equations. A novel approach for solving Quadratic Equation
based on Genetic Algorithms (GAs) is presented. Genetic Algorithms (GAs) are a
technique to solve problems which need optimization. Generation of trial
solutions have been formed by this method. Many examples have been worked out,
and in most cases we find out the exact solution. We have discussed the effect
of different parameters on the performance of the developed algorithm. The
results are concluded after rigorous testing on different equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4623</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4623</id><created>2013-06-19</created><updated>2014-02-17</updated><authors><author><keyname>Fu</keyname><forenames>Tom Z. J.</forenames></author><author><keyname>Song</keyname><forenames>Qianqian</forenames></author><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author></authors><title>The Academic Social Network</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>A number of modifications have been made according to the reviewer's
  comments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through academic publications, the authors of these publications form a
social network. Instead of sharing casual thoughts and photos (as in Facebook),
authors pick co-authors and reference papers written by other authors. Thanks
to various efforts (such as Microsoft Libra and DBLP), the data necessary for
analyzing the academic social network is becoming more available on the
Internet. What type of information and queries would be useful for users to
find out, beyond the search queries already available from services such as
Google Scholar? In this paper, we explore this question by defining a variety
of ranking metrics on different entities -authors, publication venues and
institutions. We go beyond traditional metrics such as paper counts, citations
and h-index. Specifically, we define metrics such as influence, connections and
exposure for authors. An author gains influence by receiving more citations,
but also citations from influential authors. An author increases his/her
connections by co-authoring with other authors, and specially from other
authors with high connections. An author receives exposure by publishing in
selective venues where publications received high citations in the past, and
the selectivity of these venues also depends on the influence of the authors
who publish there. We discuss the computation aspects of these metrics, and
similarity between different metrics. With additional information of
author-institution relationships, we are able to study institution rankings
based on the corresponding authors' rankings for each type of metric as well as
different domains. We are prepared to demonstrate these ideas with a web site
(http://pubstat.org) built from millions of publications and authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4624</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4624</id><created>2013-06-19</created><updated>2013-06-20</updated><authors><author><keyname>Griesmayer</keyname><forenames>Andreas</forenames></author><author><keyname>Morisset</keyname><forenames>Charles</forenames></author></authors><title>Automated Certification of Authorisation Policy Resistance</title><categories>cs.CR cs.LO</categories><comments>20 pages, 4 figures, version including proofs of the paper that will
  be presented at ESORICS 2013</comments><acm-class>D.4.6; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attribute-based Access Control (ABAC) extends traditional Access Control by
considering an access request as a set of pairs attribute name-value, making it
particularly useful in the context of open and distributed systems, where
security relevant information can be collected from different sources. However,
ABAC enables attribute hiding attacks, allowing an attacker to gain some access
by withholding information. In this paper, we first introduce the notion of
policy resistance to attribute hiding attacks. We then propose the tool ATRAP
(Automatic Term Rewriting for Authorisation Policies), based on the recent
formal ABAC language PTaCL, which first automatically searches for resistance
counter-examples using Maude, and then automatically searches for an Isabelle
proof of resistance. We illustrate our approach with two simple examples of
policies and propose an evaluation of ATRAP performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4626</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4626</id><created>2013-06-19</created><updated>2013-10-31</updated><authors><author><keyname>Gauvin</keyname><forenames>Laetitia</forenames></author><author><keyname>Panisson</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Cattuto</keyname><forenames>Ciro</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author></authors><title>Activity clocks: spreading dynamics on temporal networks of human
  contact</title><categories>physics.soc-ph cs.SI nlin.AO</categories><journal-ref>Scientific Reports 3, 3099 (2013)</journal-ref><doi>10.1038/srep03099</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamical processes on time-varying complex networks are key to understanding
and modeling a broad variety of processes in socio-technical systems. Here we
focus on empirical temporal networks of human proximity and we aim at
understanding the factors that, in simulation, shape the arrival time
distribution of simple spreading processes. Abandoning the notion of wall-clock
time in favour of node-specific clocks based on activity exposes robust
statistical patterns in the arrival times across different social contexts.
Using randomization strategies and generative models constrained by data, we
show that these patterns can be understood in terms of heterogeneous
inter-event time distributions coupled with heterogeneous numbers of events per
edge. We also show, both empirically and by using a synthetic dataset, that
significant deviations from the above behavior can be caused by the presence of
edge classes with strong activity correlations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4627</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4627</id><created>2013-06-19</created><authors><author><keyname>Dash</keyname><forenames>Tirtharaj</forenames></author><author><keyname>Nayak</keyname><forenames>Tanistha</forenames></author></authors><title>Parallel Algorithm for Longest Common Subsequence in a String</title><categories>cs.DS</categories><comments>appeared in: Proceedings of National Conference on Artificial
  Intelligence, Robotics and Embedded Systems (AIRES) - 2012, Andhra
  University, Visakhapatnam (29-30 June, 2012), pp. 66-69</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the area of Pattern Recognition and Matching, finding a Longest Common
Subsequence plays an important role. In this paper, we have proposed one
algorithm based on parallel computation. We have used OpenMP API package as
middleware to send the data to different processors. We have tested our
algorithm in a system having four processors and 2 GB physical memory. The best
result showed that the parallel algorithm increases the performance (speed of
computation) by 3.22.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4629</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4629</id><created>2013-06-19</created><authors><author><keyname>Dash</keyname><forenames>Tirtharaj</forenames></author><author><keyname>Nayak</keyname><forenames>Tanistha</forenames></author></authors><title>Non-Correlated Character Recognition using Artificial Neural Network</title><categories>cs.NE cs.CV</categories><comments>appeared in: proceedings of National Conference on Dynamics and
  Prospects of Data Mining: Theory and Practices (DPDM)-2012; September 30,
  2012, India; Publisher: OITS-BLS, Balasore Chapter; Proceeding ISBN:
  987-93-81361-31-6, pp. 79-83</comments><journal-ref>proc. National Conference on Dynamics and Prospects of Data
  Mining: Theory and Practices (DPDM)-2012; September 30, 2012, India; ISBN:
  987-93-81361-31-6, pp. 79-83</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a method of Handwritten English Character Recognition
using Artificial Neural Network (ANN). This work has been done in offline
Environment for non correlated characters, which do not possess any linear
relationships among them. We test that whether the particular tested character
belongs to a cluster or not. The implementation is carried out in Matlab
environment and successfully tested. Fifty-two sets of English alphabets are
used to train the ANN and test the network. The algorithms are tested with 26
capital letters and 26 small letters. The testing result showed that the
proposed ANN based algorithm showed a maximum recognition rate of 85%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4631</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4631</id><created>2013-06-06</created><authors><author><keyname>Parikh</keyname><forenames>Rachana</forenames></author><author><keyname>Vasant</keyname><forenames>Avani R.</forenames></author></authors><title>Table of Content detection using Machine Learning</title><categories>cs.LG cs.DL cs.IR</categories><comments>International Journal of Artificial Intelligence and Applications,
  May-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Table of content (TOC) detection has drawn attention now a day because it
plays an important role in digitization of multipage document. Generally book
document is multipage document. So it becomes necessary to detect Table of
Content page for easy navigation of multipage document and also to make
information retrieval faster for desirable data from the multipage document.
All the Table of content pages follow the different layout, different way of
presenting the contents of the document like chapter, section, subsection etc.
This paper introduces a new method to detect Table of content using machine
learning technique with different features. With the main aim to detect Table
of Content pages is to structure the document according to their contents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4633</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4633</id><created>2013-06-06</created><authors><author><keyname>Goswami</keyname><forenames>Sumit</forenames></author><author><keyname>Shishodia</keyname><forenames>Mayank Singh</forenames></author></authors><title>A Fuzzy Based Approach to Text Mining and Document Clustering</title><categories>cs.LG cs.IR</categories><comments>10 pages, 6 tables, 1 figure, review paper, International Journal of
  Data Mining &amp; Knowledge Management Process (IJDKP) ISSN : 2230 - 9608[Online]
  ; 2231 - 007X [Print]. Paper can be found at
  http://airccse.org/journal/ijdkp/current2013.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy logic deals with degrees of truth. In this paper, we have shown how to
apply fuzzy logic in text mining in order to perform document clustering. We
took an example of document clustering where the documents had to be clustered
into two categories. The method involved cleaning up the text and stemming of
words. Then, we chose m number of features which differ significantly in their
word frequencies (WF), normalized by document length, between documents
belonging to these two clusters. The documents to be clustered were represented
as a collection of m normalized WF values. Fuzzy c-means (FCM) algorithm was
used to cluster these documents into two clusters. After the FCM execution
finished, the documents in the two clusters were analysed for the values of
their respective m features. It was known that documents belonging to a
document type, say X, tend to have higher WF values for some particular
features. If the documents belonging to a cluster had higher WF values for
those same features, then that cluster was said to represent X. By fuzzy logic,
we not only get the cluster name, but also the degree to which a document
belongs to a cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4635</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4635</id><created>2013-06-19</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Towards Multistage Design of Modular Systems</title><categories>cs.AI cs.SY</categories><comments>13 pages, 25 figures, 14 tables</comments><msc-class>68T20, 93A13, 93B51, 90B50</msc-class><acm-class>I.2.8; J.6; K.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes multistage design of composite (modular) systems (i.e.,
design of a system trajectory). This design process consists of the following:
(i) definition of a set of time/logical points; (ii) modular design of the
system for each time/logical point (e.g., on the basis of combinatorial
synthesis as hierarchical morphological design or multiple choice problem) to
obtain several system solutions; (iii) selection of the system solution for
each time/logical point while taking into account their quality and the quality
of compatibility between neighbor selected system solutions (here,
combinatorial synthesis is used as well). Mainly, the examined time/logical
points are based on a time chain. In addition, two complicated cases are
considered: (a) the examined logical points are based on a tree-like structure,
(b) the examined logical points are based on a digraph. Numerical examples
illustrate the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4636</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4636</id><created>2013-06-19</created><updated>2013-11-06</updated><authors><author><keyname>Babiak</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Blahoudek</keyname><forenames>Franti&#x161;ek</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Mojm&#xed;r</forenames></author><author><keyname>Strej&#x10d;ek</keyname><forenames>Jan</forenames></author></authors><title>Effective Translation of LTL to Deterministic Rabin Automata: Beyond the
  (F,G)-Fragment</title><categories>cs.FL cs.LO</categories><comments>Full version of the paper accepted to ATVA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some applications of linear temporal logic (LTL) require to translate
formulae of the logic to deterministic omega-automata. There are currently two
translators producing deterministic automata: ltl2dstar working for the whole
LTL and Rabinizer applicable to LTL(F,G) which is the LTL fragment using only
modalities F and G. We present a new translation to deterministic Rabin
automata via alternating automata and deterministic transition-based
generalized Rabin automata. Our translation applies to a fragment that is
strictly larger than LTL(F,G). Experimental results show that our algorithm can
produce significantly smaller automata compared to Rabinizer and ltl2dstar,
especially for more complex LTL formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4650</identifier>
 <datestamp>2013-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4650</id><created>2013-06-19</created><updated>2013-09-10</updated><authors><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author></authors><title>Stochastic Majorization-Minimization Algorithms for Large-Scale
  Optimization</title><categories>stat.ML cs.LG math.OC</categories><comments>accepted for publication for Neural Information Processing Systems
  (NIPS) 2013. This is the 9-pages version followed by 16 pages of appendices.
  The title has changed compared to the first technical report</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Majorization-minimization algorithms consist of iteratively minimizing a
majorizing surrogate of an objective function. Because of its simplicity and
its wide applicability, this principle has been very popular in statistics and
in signal processing. In this paper, we intend to make this principle scalable.
We introduce a stochastic majorization-minimization scheme which is able to
deal with large-scale or possibly infinite data sets. When applied to convex
optimization problems under suitable assumptions, we show that it achieves an
expected convergence rate of $O(1/\sqrt{n})$ after $n$ iterations, and of
$O(1/n)$ for strongly convex functions. Equally important, our scheme almost
surely converges to stationary points for a large class of non-convex problems.
We develop several efficient algorithms based on our framework. First, we
propose a new stochastic proximal gradient method, which experimentally matches
state-of-the-art solvers for large-scale $\ell_1$-logistic regression. Second,
we develop an online DC programming algorithm for non-convex sparse estimation.
Finally, we demonstrate the effectiveness of our approach for solving
large-scale structured matrix factorization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4652</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4652</id><created>2013-06-19</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Finding and Solving Contradictions of False Positives in Virus Scanning</title><categories>cs.CR</categories><comments>13 pages. Available at TRIZsite Journal, Apr 2012
  http://trizsite.tk/trizsite/articles/default.asp?month=Apr&amp;year=2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  False positives are equally dangerous as false negatives. Ideally the false
positive rate should remain 0 or very close to 0. Even a slightest increase in
false positive rate is considered as undesirable.
  Although the specific methods provide very accurate scanning by comparing
viruses with their exact signatures, they fail to detect the new and unknown
viruses. On the other hand the generic methods can detect even new viruses
without using virus signatures. But these methods are more likely to generate
false positives. There is a positive correlation between the capability to
detect new and unknown viruses and false positive rate.
  While a traditional approach tries to achieve a right balance between false
positives and false negatives a TRIZ approach looks forward to achieve the
Ideal Final Result. The Ideal final result is to 'detect and prevent viruses
with full certainty. The chances of error should be nil and the method should
not raise any false positive or false negative.' The article shows many
contradictions relating to false positives and their solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4653</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4653</id><created>2013-06-19</created><updated>2013-07-08</updated><authors><author><keyname>Kale</keyname><forenames>Satyen</forenames></author></authors><title>Multiarmed Bandits With Limited Expert Advice</title><categories>cs.LG</categories><comments>Updated with tighter upper bound based on PolyINF algorithm, lower
  bound nearly matching the upper bound, and fixed some typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the COLT 2013 open problem of \citet{SCB} on minimizing regret in
the setting of advice-efficient multiarmed bandits with expert advice. We give
an algorithm for the setting of K arms and N experts out of which we are
allowed to query and use only M experts' advices in each round, which has a
regret bound of \tilde{O}\bigP{\sqrt{\frac{\min\{K, M\} N}{M} T}} after T
rounds. We also prove that any algorithm for this problem must have expected
regret at least \tilde{\Omega}\bigP{\sqrt{\frac{\min\{K, M\} N}{M}T}}, thus
showing that our upper bound is nearly tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4660</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4660</id><created>2013-06-19</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Contradictions in Improving Speed of Virus Scanning</title><categories>cs.CR</categories><comments>Available at TRIZsite Journal, May 2012,
  http://trizsite.tk/trizsite/articles/default.asp?month=May&amp;year=2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although everything in computing industry moves faster including the
processor, memory speed, memory size, storage space etc. there is no
improvement in virus scanning time. Although the processing speed has
substantially increased, a typical full scanning is still taking several hours
for an average computer. There is a serious need to improve the scanning time.
  Contradiction is a stage of problem solving where the nature of the actual
problem is clearly explained in terms of at least two parameters, one improving
and another worsening. While emphasizing one parameter strengthens the system
position emphasizing another parameter weakens the system.
  In conventional methods a problem solver has to make a perfect balance
between these conflicting parameters, where the situation is neither too much
on one side nor too much on the other. The results of those methods, although
increase the speed of virus scanning, results in disadvantages like load on
processor, increase in false positives and compromise on security. The
objective of TRIZ is not to accept a tradeoff between the speed of scanning and
those other difficulties but to resolve the contradictions so that the speed of
scanning increases without compromising with security and other harmful
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4664</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4664</id><created>2013-06-19</created><authors><author><keyname>Huber</keyname><forenames>Michael</forenames></author></authors><title>Efficient Two-Stage Group Testing Algorithms for Genetic Screening</title><categories>cs.DS math.CO q-bio.QM</categories><comments>14 pages; to appear in &quot;Algorithmica&quot;. Part of this work has been
  presented at the ICALP 2011 Group Testing Workshop; arXiv:1106.3680</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient two-stage group testing algorithms that are particularly suited for
rapid and less-expensive DNA library screening and other large scale biological
group testing efforts are investigated in this paper. The main focus is on
novel combinatorial constructions in order to minimize the number of individual
tests at the second stage of a two-stage disjunctive testing procedure.
Building on recent work by Levenshtein (2003) and Tonchev (2008), several new
infinite classes of such combinatorial designs are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4666</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4666</id><created>2013-06-19</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Methods of Repairing Virus Infected Files, A TRIZ based Analysis</title><categories>cs.CR</categories><comments>18 pages, 20 references. (May 15, 2013). Available at SSRN:
  http://ssrn.com/abstract=2265576</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most viruses are capable of fixing up the first few bytes and repair the
original program because they have to return the control back to the infected
program. This fact is used by a heuristic cleaner to clean the infected file.
As the virus knows how to repair the it uses the same virus to repair the
infected file.
  There are some infections where parts of the files are damaged by the virus.
These types of infections are caused by 'file modifying viruses'. In these
cases, the chance of recovery is less, but the anti-virus has to apply various
methods with hope. The virus cleaner must know the characteristics of a virus
in order to remove that virus. It cannot remove an unknown virus whose methods
of infection are not known. If a virus is wrongly detected to be a different
virus, then the cleaner will do wrong operations and build a garbage file.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4672</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4672</id><created>2013-06-19</created><authors><author><keyname>Dash</keyname><forenames>Tirtharaj</forenames></author><author><keyname>Mishra</keyname><forenames>Goutam</forenames></author><author><keyname>Nayak</keyname><forenames>Tanistha</forenames></author></authors><title>A Novel Approach for Intelligent Robot Path Planning</title><categories>cs.RO</categories><comments>appeared in: Proceedings of National Conference on Artificial
  Intelligence, Robotics and Embedded Systems (AIRES) - 2012, Andhra
  University, Visakhapatnam (29-30 June, 2012), pp. 388-391</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path planning of Robot is one of the challenging fields in the area of
Robotics research. In this paper, we proposed a novel algorithm to find path
between starting and ending position for an intelligent system. An intelligent
system is considered to be a device/robot having an antenna connected with
sensor-detector system. The proposed algorithm is based on Neural Network
training concept. The considered neural network is Adapti ve to the knowledge
bases. However, implementation of this algorithm is slightly expensive due to
hardware it requires. From detailed analysis, it can be proved that the
resulted path of this algorithm is efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4713</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4713</id><created>2013-06-19</created><updated>2013-12-10</updated><authors><author><keyname>Tobin-Hochstadt</keyname><forenames>Sam</forenames><affiliation>Northeastern University</affiliation></author><author><keyname>Van Horn</keyname><forenames>David</forenames><affiliation>Northeastern University</affiliation></author></authors><title>From Principles to Practice with Class in the First Year</title><categories>cs.PL</categories><comments>In Proceedings TFPIE 2013, arXiv:1312.2216</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 136, 2013, pp. 1-15</journal-ref><doi>10.4204/EPTCS.136.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a bridge between functional and object-oriented programming in the
first-year curriculum. Traditionally, curricula that begin with functional
programming transition to a professional, usually object-oriented, language in
the second course. This transition poses obstacles for students, and often
results in confusing the details of development environments, syntax, and
libraries with the fundamentals of OO programming that the course should focus
on. Instead, we propose to begin the second course with a sequence of custom
teaching languages which minimize the transition from the first course, and
allow students to focus on core ideas. After working through the sequence of
pedagogical languages, we then transition to Java, at which point students have
a strong command of the basic principles. We have 3 years of experience with
this course, with notable success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4714</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4714</id><created>2013-06-19</created><authors><author><keyname>Sarraute</keyname><forenames>Carlos</forenames><affiliation>Core Security Technologies</affiliation><affiliation>ITBA</affiliation></author><author><keyname>Buffet</keyname><forenames>Olivier</forenames><affiliation>INRIA</affiliation></author><author><keyname>Hoffmann</keyname><forenames>Joerg</forenames><affiliation>INRIA</affiliation></author></authors><title>Penetration Testing == POMDP Solving?</title><categories>cs.AI cs.CR</categories><comments>Proceedings of the 3rd Workshop on Intelligent Security (SecArt'11),
  at IJCAI'11</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Penetration Testing is a methodology for assessing network security, by
generating and executing possible attacks. Doing so automatically allows for
regular and systematic testing without a prohibitive amount of human labor. A
key question then is how to generate the attacks. This is naturally formulated
as a planning problem. Previous work (Lucangeli et al. 2010) used classical
planning and hence ignores all the incomplete knowledge that characterizes
hacking. More recent work (Sarraute et al. 2011) makes strong independence
assumptions for the sake of scaling, and lacks a clear formal concept of what
the attack planning problem actually is. Herein, we model that problem in terms
of partially observable Markov decision processes (POMDP). This grounds
penetration testing in a well-researched formalism, highlighting important
aspects of this problem's nature. POMDPs allow to model information gathering
as an integral part of the problem, thus providing for the first time a means
to intelligently mix scanning actions with actual exploits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4721</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4721</id><created>2013-06-19</created><updated>2014-01-18</updated><authors><author><keyname>Shoari</keyname><forenames>Arian</forenames></author><author><keyname>Seyedi</keyname><forenames>Alireza</forenames></author></authors><title>On Localization of A Non-Cooperative Target with Non-Coherent Binary
  Detectors</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Signal Processing Letters, vol. 21, no. 6, pp. 746-750, June
  2014</journal-ref><doi>10.1109/LSP.2014.2314220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization of a non-cooperative target with binary detectors is considered.
A general expression for the Fisher information for estimation of target
location and power is developed. This general expression is then used to derive
closed-form approximations for the Cramer-Rao bound for the case of
non-coherent detectors. Simulations show that the approximations are quite
consistent with the exact bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4724</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4724</id><created>2013-06-19</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author></authors><title>Computer simulation based parameter selection for resistance exercise</title><categories>cs.CV cs.HC</categories><comments>In Modelling and Simulation, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast to most scientific disciplines, sports science research has been
characterized by comparatively little effort investment in the development of
relevant phenomenological models. Scarcer yet is the application of said models
in practice. We present a framework which allows resistance training
practitioners to employ a recently proposed neuromuscular model in actual
training program design. The first novelty concerns the monitoring aspect of
coaching. A method for extracting training performance characteristics from
loosely constrained video sequences, effortlessly and with minimal human input,
using computer vision is described. The extracted data is subsequently used to
fit the underlying neuromuscular model. This is achieved by solving an inverse
dynamics problem corresponding to a particular exercise. Lastly, a computer
simulation of hypothetical training bouts, using athlete-specific capability
parameters, is used to predict the effected adaptation and changes in
performance. The software described here allows the practitioner to manipulate
hypothetical training parameters and immediately see their effect on predicted
adaptation for a specific athlete. Thus, this work presents a holistic view of
the monitoring-assessment-adjustment loop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4726</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4726</id><created>2013-06-19</created><authors><author><keyname>Zhao</keyname><forenames>Dawei</forenames></author><author><keyname>Peng</keyname><forenames>Haipeng</forenames></author><author><keyname>Li</keyname><forenames>Lixiang</forenames></author><author><keyname>Yang</keyname><forenames>Yixian</forenames></author></authors><title>A secure and effective anonymous authentication scheme for roaming
  service in global mobility networks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Mun et al. analyzed Wu et al.'s authentication scheme and proposed
a enhanced anonymous authentication scheme for roaming service in global
mobility networks. However, through careful analysis, we find that Mun et al.'s
scheme is vulnerable to impersonation attacks, off-line password guessing
attacks and insider attacks, and cannot provide user friendliness, user's
anonymity, proper mutual authentication and local verification. To remedy these
weaknesses, in this paper we propose a novel anonymous authentication scheme
for roaming service in global mobility networks. Security and performance
analyses show the proposed scheme is more suitable for the low-power and
resource-limited mobile devices, and is secure against various attacks and has
many excellent features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4727</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4727</id><created>2013-06-19</created><updated>2013-08-26</updated><authors><author><keyname>Carvalho</keyname><forenames>Cicero</forenames></author></authors><title>On the second Hamming weight of some Reed-Muller type codes</title><categories>cs.IT math.AC math.IT</categories><comments>Added citations</comments><msc-class>11T71 (primary) 13P25, 94B60 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study affine cartesian codes, which are a Reed-Muller type of evaluation
codes, where polynomials are evaluated at the cartesian product of n subsets of
a finite field F_q. These codes appeared recently in a work by H. Lopez, C.
Renteria-Marquez and R. Villareal and, in a generalized form, in a work by O.
Geil and C. Thomsen. Using methods from Gr\&quot;obner basis theory we determine the
second Hamming weight (also called next-to-minimal weight) for particular cases
of affine cartesian codes and also some higher Hamming weights of this type of
code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4746</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4746</id><created>2013-06-19</created><authors><author><keyname>Barrett</keyname><forenames>Daniel Paul</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>Felzenszwalb-Baum-Welch: Event Detection by Changing Appearance</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method which can detect events in videos by modeling the change
in appearance of the event participants over time. This method makes it
possible to detect events which are characterized not by motion, but by the
changing state of the people or objects involved. This is accomplished by using
object detectors as output models for the states of a hidden Markov model
(HMM). The method allows an HMM to model the sequence of poses of the event
participants over time, and is effective for poses of humans and inanimate
objects. The ability to use existing object-detection methods as part of an
event model makes it possible to leverage ongoing work in the object-detection
community. A novel training method uses an EM loop to simultaneously learn the
temporal structure and object models automatically, without the need to specify
either the individual poses to be modeled or the frames in which they occur.
The E-step estimates the latent assignment of video frames to HMM states, while
the M-step estimates both the HMM transition probabilities and state output
models, including the object detectors, which are trained on the weighted
subset of frames assigned to their state. A new dataset was gathered because
little work has been done on events characterized by changing object pose, and
suitable datasets are not available. Our method produced results superior to
that of comparison systems on this dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4748</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4748</id><created>2013-06-19</created><updated>2014-05-01</updated><authors><author><keyname>Eftekhari</keyname><forenames>Armin</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>New Analysis of Manifold Embeddings and Signal Recovery from Compressive
  Measurements</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1002.1247</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive Sensing (CS) exploits the surprising fact that the information
contained in a sparse signal can be preserved in a small number of compressive,
often random linear measurements of that signal. Strong theoretical guarantees
have been established concerning the embedding of a sparse signal family under
a random measurement operator and on the accuracy to which sparse signals can
be recovered from noisy compressive measurements. In this paper, we address
similar questions in the context of a different modeling framework. Instead of
sparse models, we focus on the broad class of manifold models, which can arise
in both parametric and non-parametric signal families. Using tools from the
theory of empirical processes, we improve upon previous results concerning the
embedding of low-dimensional manifolds under random measurement operators. We
also establish both deterministic and probabilistic instance-optimal bounds in
$\ell_2$ for manifold-based signal recovery and parameter estimation from noisy
compressive measurements. In line with analogous results for sparsity-based CS,
we conclude that much stronger bounds are possible in the probabilistic
setting. Our work supports the growing evidence that manifold-based models can
be used with high accuracy in compressive signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4753</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4753</id><created>2013-06-20</created><authors><author><keyname>Gordon</keyname><forenames>Geoffrey J.</forenames></author></authors><title>Galerkin Methods for Complementarity Problems and Variational
  Inequalities</title><categories>cs.LG cs.AI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complementarity problems and variational inequalities arise in a wide variety
of areas, including machine learning, planning, game theory, and physical
simulation. In all of these areas, to handle large-scale problem instances, we
need fast approximate solution methods. One promising idea is Galerkin
approximation, in which we search for the best answer within the span of a
given set of basis functions. Bertsekas proposed one possible Galerkin method
for variational inequalities. However, this method can exhibit two problems in
practice: its approximation error is worse than might be expected based on the
ability of the basis to represent the desired solution, and each iteration
requires a projection step that is not always easy to implement efficiently.
So, in this paper, we present a new Galerkin method with improved behavior: our
new error bounds depend directly on the distance from the true solution to the
subspace spanned by our basis, and the only projections we require are onto the
feasible region or onto the span of our basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4754</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4754</id><created>2013-06-20</created><authors><author><keyname>Gong</keyname><forenames>Chen</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>On Finite Block-Length Quantization Distortion</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the upper and lower bounds on the quantization distortions for
independent and identically distributed sources in the finite block-length
regime. Based on the convex optimization framework of the rate-distortion
theory, we derive a lower bound on the quantization distortion under finite
block-length, which is shown to be greater than the asymptotic distortion given
by the rate-distortion theory. We also derive two upper bounds on the
quantization distortion based on random quantization codebooks, which can
achieve any distortion above the asymptotic one. Moreover, we apply the new
upper and lower bounds to two types of sources, the discrete binary symmetric
source and the continuous Gaussian source. For the binary symmetric source, we
obtain the closed-form expressions of the upper and lower bounds. For the
Gaussian source, we propose a computational tractable method to numerically
compute the upper and lower bounds, for both bounded and unbounded quantization
codebooks.Numerical results show that the gap between the upper and lower
bounds is small for reasonable block length and hence the bounds are tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4755</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4755</id><created>2013-06-20</created><authors><author><keyname>Li</keyname><forenames>Shuying</forenames></author><author><keyname>Gong</keyname><forenames>Chen</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Hybrid Group Decoding for Scalable Video over MIMO-OFDM Downlink Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a scalable video broadcasting scheme over MIMO-OFDM systems. The
scalable video source layers are channel encoded and modulated into independent
signal streams, which are then transmitted from the allocated antennas in
certain time-frequency blocks. Each receiver employs the successive group
decoder to decode the signal streams of interest by treating other signal
streams as interference. The transmitter performs adaptive coding and
modulation, and transmission antenna and subcarrier allocation, based on the
rate feedback from the receivers. We also propose a hybrid receiver that
switches between the successive group decoder and the MMSE decoder depending on
the rate. Extensive simulations are provided to demonstrate the performance
gain of the proposed group-decoding-based scalable video broadcasting scheme
over the one based on the conventional MMSE decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4758</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4758</id><created>2013-06-20</created><authors><author><keyname>Gulati</keyname><forenames>Payal</forenames></author><author><keyname>Sharma</keyname><forenames>A. K.</forenames></author></authors><title>Analysing Word Importance for Image Annotation</title><categories>cs.IR cs.CV</categories><comments>4 pages, 3 figures, Published in IJCSI (International Journal of
  Computer Science Issues) Journal, Volume 10, Issue 1, No 2, January 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image annotation provides several keywords automatically for a given image
based on various tags to describe its contents which is useful in Image
retrieval. Various researchers are working on text based and content based
image annotations [7,9]. It is seen, in traditional Image annotation
approaches, annotation words are treated equally without considering the
importance of each word in real world. In context of this, in this work, images
are annotated with keywords based on their frequency count and word
correlation. Moreover this work proposes an approach to compute importance
score of candidate keywords, having same frequency count.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4773</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4773</id><created>2013-06-20</created><authors><author><keyname>Jiang</keyname><forenames>Yuming</forenames></author></authors><title>Performance Bounds for Multiclass FIFO in Communication Networks: A
  Deterministic Case</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiclass FIFO is used in communication networks such as in input-queueing
routers/switches and in wireless networks. For the concern of providing service
guarantees in such networks, it is crucial to have analytical results, e.g.
bounds, on the performance of multi-class FIFO. Surprisingly, there are few
such results in the literature. This paper is devoted to filling the gap.
Specifically, a single hop deterministic case is studied, for which, delay and
backlog bounds are derived, in addition to guaranteed rate and service curve
characterizations that may be exploited to extend the analysis to network
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4774</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4774</id><created>2013-06-20</created><authors><author><keyname>Wang</keyname><forenames>Anyu</forenames></author><author><keyname>Zhang</keyname><forenames>Zhifang</forenames></author></authors><title>Repair Locality with Multiple Erasure Tolerance</title><categories>cs.IT math.IT</categories><comments>14 pages</comments><journal-ref>IEEE Transactions on Information Theory, Volume 60, Issue 11, pp.
  6979 - 6987, Oct 2014</journal-ref><doi>10.1109/TIT.2014.2351404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed storage systems, erasure codes with locality $r$ is preferred
because a coordinate can be recovered by accessing at most $r$ other
coordinates which in turn greatly reduces the disk I/O complexity for small
$r$. However, the local repair may be ineffective when some of the $r$
coordinates accessed for recovery are also erased.
  To overcome this problem, we propose the $(r,\delta)_c$-locality providing
$\delta -1$ local repair options for a coordinate. Consequently, the repair
locality $r$ can tolerate $\delta-1$ erasures in total. We derive an upper
bound on the minimum distance $d$ for any linear $[n,k]$ code with information
$(r,\delta)_c$-locality. For general parameters, we prove existence of the
codes that attain this bound when $n\geq k(r(\delta-1)+1)$, implying tightness
of this bound. Although the locality $(r,\delta)$ defined by Prakash et al
provides the same level of locality and local repair tolerance as our
definition, codes with $(r,\delta)_c$-locality are proved to have more
advantage in the minimum distance. In particular, we construct a class of codes
with all symbol $(r,\delta)_c$-locality where the gain in minimum distance is
$\Omega(\sqrt{r})$ and the information rate is close to 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4779</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4779</id><created>2013-06-20</created><authors><author><keyname>Altaany</keyname><forenames>Fawzi H.</forenames></author><author><keyname>Jassim</keyname><forenames>Firas A.</forenames></author></authors><title>Impact of Facebook Usage on Undergraduate Students Performance in Irbid
  National University: Case Study</title><categories>cs.CY</categories><comments>6 pages, 10 tables</comments><journal-ref>International Journal of Engineering Research and Applications
  (IJERA), Vol. 3, Issue 4, 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The aim of this study is to investigate the style of Facebook usage between
undergraduate students and the impact on their academics performance. Also,
this paper was evaluated in the view of student the using of Facebook. A
questioner was design for collecting data from a sample of 480 undergraduate
students in Irbid National University. The survey revealed that 77% of the
students have an account on Facebook. One of the main findings is that there
was a significant relationship between gender and Facebook usage. Moreover, the
survey revealed that whenever the less time spent on Facebook, the higher the
performance will be in grade point average. This was conducted by the negative
correlation between time spent on Facebook and the performance of undergraduate
student. Statistically speaking, the study has seven hypotheses; two of them
were rejecting against five acceptable hypotheses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4793</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4793</id><created>2013-06-20</created><authors><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Evolving Boolean Regulatory Networks with Epigenetic Control</title><categories>cs.NE q-bio.MN</categories><comments>18 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:1303.7220</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The significant role of epigenetic mechanisms within natural systems has
become increasingly clear. This paper uses a recently presented abstract,
tunable Boolean genetic regulatory network model to explore aspects of
epigenetics. It is shown how dynamically controlling transcription via a DNA
methylation-inspired mechanism can be selected for by simulated evolution under
various single and multiple cell scenarios. Further, it is shown that the
effects of such control can be inherited without detriment to fitness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4807</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4807</id><created>2013-06-20</created><updated>2015-05-14</updated><authors><author><keyname>Wang</keyname><forenames>Xinhua</forenames></author><author><keyname>Shirinzadeh</keyname><forenames>Bijan</forenames></author></authors><title>Nonlinear continuous integral-derivative observer</title><categories>cs.SY math.DS</categories><comments>21 pages, 12 figures</comments><journal-ref>Nonlinear Dynamics, vol. 77, no. 3, 2014, 793-806</journal-ref><doi>10.1007/s11071-014-1341-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a high-order nonlinear continuous integral-derivative observer
is presented based on finite-time stability and singular perturbation
technique. The proposed integral-derivative observer can not only obtain the
multiple integrals of a signal, but can also estimate the derivatives.
Conditions are given ensuring finite-time stability for the presented
integral-derivative observer, and the stability and robustness in time domain
are analysed. The merits of the presented integral-derivative observer include
its synchronous estimation of integrals and derivatives, finite-time stability,
ease of parameters selection, sufficient stochastic noises rejection and almost
no drift phenomenon. The theoretical results are confirmed by computational
analysis and simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4828</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4828</id><created>2013-06-20</created><authors><author><keyname>Asghar</keyname><forenames>Muhammad Rizwan</forenames></author><author><keyname>Ion</keyname><forenames>Mihaela</forenames></author><author><keyname>Russello</keyname><forenames>Giovanni</forenames></author><author><keyname>Crispo</keyname><forenames>Bruno</forenames></author></authors><title>ESPOON: Enforcing Encrypted Security Policies in Outsourced Environments</title><categories>cs.CR</categories><comments>The final version of this paper has been published at ARES 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enforcement of security policies in outsourced environments is still an
open challenge for policy-based systems. On the one hand, taking the
appropriate security decision requires access to the policies. However, if such
access is allowed in an untrusted environment then confidential information
might be leaked by the policies. Current solutions are based on cryptographic
operations that embed security policies with the security mechanism. Therefore,
the enforcement of such policies is performed by allowing the authorised
parties to access the appropriate keys. We believe that such solutions are far
too rigid because they strictly intertwine authorisation policies with the
enforcing mechanism.
  In this paper, we want to address the issue of enforcing security policies in
an untrusted environment while protecting the policy confidentiality. Our
solution ESPOON is aiming at providing a clear separation between security
policies and the enforcement mechanism. However, the enforcement mechanism
should learn as less as possible about both the policies and the requester
attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4845</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4845</id><created>2013-06-20</created><authors><author><keyname>Menahem</keyname><forenames>Eitan</forenames></author><author><keyname>Nakibly</keyname><forenames>Gabi</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>ACTIDS: An Active Strategy For Detecting And Localizing Network Attacks</title><categories>cs.CR</categories><comments>Full fledged paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate a new approach for detecting attacks which aim to
degrade the network's Quality of Service (QoS). To this end, a new
network-based intrusion detection system (NIDS) is proposed. Most contemporary
NIDSs take a passive approach by solely monitoring the network's production
traffic. This paper explores a complementary approach in which distributed
agents actively send out periodic probes. The probes are continuously monitored
to detect anomalous behavior of the network. The proposed approach takes away
much of the variability of the network's production traffic that makes it so
difficult to classify. This enables the NIDS to detect more subtle attacks
which would not be detected using the passive approach alone. Furthermore, the
active probing approach allows the NIDS to be effectively trained using only
examples of the network's normal states, hence enabling an effective detection
of zero-day attacks. Using realistic experiments, we show that an NIDS which
also leverages the active approach is considerably more effective in detecting
attacks which aim to degrade the network's QoS when compared to an NIDS which
relies solely on the passive approach. Lastly, we show that the false positives
rate remains very low even in the face of Byzantine faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4849</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4849</id><created>2013-06-20</created><authors><author><keyname>Piva</keyname><forenames>Matteo</forenames></author><author><keyname>Sala</keyname><forenames>Massimiliano</forenames></author></authors><title>A generalization of bounds for cyclic codes, including the HT and BS
  bounds</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the algebraic structure of cyclic codes and some properties of the
discrete Fourier transform to give a reformulation of several classical bounds
for the distance of cyclic codes, by extending techniques of linear algebra. We
propose a bound, whose computational complexity is polynomial bounded, which is
a generalization of the Hartmann-Tzeng bound and the Betti-Sala bound. In the
majority of computed cases, our bound is the tightest among all known
polynomial-time bounds, including the Roos bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4856</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4856</id><created>2013-06-20</created><authors><author><keyname>Aman</keyname><forenames>Valeria</forenames></author></authors><title>The potential of preprints to accelerate scholarly communication - A
  bibliometric analysis based on selected journals</title><categories>cs.DL</categories><comments>Master Thesis from 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper quantifies to which extent preprints in arXiv accelerate scholarly
communication. The following subject fields were investigated up to the year
2012: High Energy Physics (HEP), Mathematics, Astrophysics, Quantitative
Biology, and Library and Information Science (LIS). Publication and citation
data was downloaded from Scopus and matched with corresponding preprints in
arXiv. Furthermore, the INSPIRE HEP database was used to retrieve citation data
for papers related to HEP. The bibliometric analysis deals with the growth in
numbers of articles published having a previous preprint in arXiv and the
publication delay, which is defined as the chronological distance between the
deposit of a preprint in arXiv and its formal journal publication. Likewise,
the citation delay is analyzed, which describes the time it takes until the
first citation of preprints, and articles, respectively. Total citation numbers
are compared for sets of articles with a previous preprint and those without.
The results show that in all fields but biology a significant citation
advantage exists in terms of speed and citation rates for articles with a
previous preprint version on arXiv.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4883</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4883</id><created>2013-06-20</created><authors><author><keyname>Bouguerra</keyname><forenames>Abderrahmen</forenames></author><author><keyname>Saigaa</keyname><forenames>Djamel</forenames></author><author><keyname>Kara</keyname><forenames>Kamel</forenames></author><author><keyname>Zeghlache</keyname><forenames>Samir</forenames></author><author><keyname>Loukal</keyname><forenames>Keltoum</forenames></author></authors><title>Fault-Tolerant Control of a 2 DOF Helicopter (TRMS System) Based on
  H_infinity</title><categories>cs.SY</categories><comments>6 pages, 11 figures, conference. In CEIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a Fault-Tolerant control of 2 DOF Helicopter (TRMS System)
Based on H-infinity is presented. In particular, the introductory part of the
paper presents a Fault-Tolerant Control (FTC), the first part of this paper
presents a description of the mathematical model of TRMS, and the last part of
the paper presented and a polytypic Unknown Input Observer (UIO) is synthesized
using equalities and LMIs. This UIO is used to observe the faults and then
compensate them, in this part the shown how to design a fault-tolerant control
strategy for this particular class of non-linear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4884</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4884</id><created>2013-06-20</created><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Collette</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Ito</keyname><forenames>Hiro</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Sakaidani</keyname><forenames>Hikaru</forenames></author><author><keyname>Taslakian</keyname><forenames>Perouz</forenames></author></authors><title>Cannibal Animal Games: a new variant of Tic-Tac-Toe</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new partial two-player game, called the \emph{cannibal
animal game}, which is a variant of Tic-Tac-Toe. The game is played on the
infinite grid, where in each round a player chooses and occupies free cells.
The first player Alice can occupy a cell in each turn and wins if she occupies
a set of cells, the union of a subset of which is a translated, reflected
and/or rotated copy of a previously agreed upon polyomino $P$ (called an
\emph{animal}). The objective of the second player Bob is to prevent Alice from
creating her animal by occupying in each round a translated, reflected and/or
rotated copy of $P$. An animal is a \emph{cannibal} if Bob has a winning
strategy, and a \emph{non-cannibal} otherwise. This paper presents some new
tools, such as the \emph{bounding strategy} and the \emph{punching lemma}, to
classify animals into cannibals or non-cannibals. We also show that the
\emph{pairing strategy} works for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4886</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4886</id><created>2013-06-20</created><authors><author><keyname>Marujo</keyname><forenames>Luis</forenames></author><author><keyname>Gershman</keyname><forenames>Anatole</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author><author><keyname>Frederking</keyname><forenames>Robert</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o P.</forenames></author></authors><title>Supervised Topical Key Phrase Extraction of News Stories using
  Crowdsourcing, Light Filtering and Co-reference Normalization</title><categories>cs.CL cs.IR</categories><comments>In 8th International Conference on Language Resources and Evaluation
  (LREC 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast and effective automated indexing is critical for search and personalized
services. Key phrases that consist of one or more words and represent the main
concepts of the document are often used for the purpose of indexing. In this
paper, we investigate the use of additional semantic features and
pre-processing steps to improve automatic key phrase extraction. These features
include the use of signal words and freebase categories. Some of these features
lead to significant improvements in the accuracy of the results. We also
experimented with 2 forms of document pre-processing that we call light
filtering and co-reference normalization. Light filtering removes sentences
from the document, which are judged peripheral to its main content.
Co-reference normalization unifies several written forms of the same named
entity into a unique form. We also needed a &quot;Gold Standard&quot; - a set of labeled
documents for training and evaluation. While the subjective nature of key
phrase selection precludes a true &quot;Gold Standard&quot;, we used Amazon's Mechanical
Turk service to obtain a useful approximation. Our data indicates that the
biggest improvements in performance were due to shallow semantic features, news
categories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion of
deeper semantic features such as Freebase sub-categories was not beneficial by
itself, but in combination with pre-processing, did cause slight improvements
in the nDCG scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4890</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4890</id><created>2013-06-20</created><authors><author><keyname>Marujo</keyname><forenames>Luis</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o P.</forenames></author><author><keyname>Gershman</keyname><forenames>Anatole</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author></authors><title>Key Phrase Extraction of Lightly Filtered Broadcast News</title><categories>cs.CL cs.IR</categories><comments>In 15th International Conference on Text, Speech and Dialogue (TSD
  2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the impact of light filtering on automatic key phrase
extraction (AKE) applied to Broadcast News (BN). Key phrases are words and
expressions that best characterize the content of a document. Key phrases are
often used to index the document or as features in further processing. This
makes improvements in AKE accuracy particularly important. We hypothesized that
filtering out marginally relevant sentences from a document would improve AKE
accuracy. Our experiments confirmed this hypothesis. Elimination of as little
as 10% of the document sentences lead to a 2% improvement in AKE precision and
recall. AKE is built over MAUI toolkit that follows a supervised learning
approach. We trained and tested our AKE method on a gold standard made of 8 BN
programs containing 110 manually annotated news stories. The experiments were
conducted within a Multimedia Monitoring Solution (MMS) system for TV and radio
news/programs, running daily, and monitoring 12 TV and 4 radio channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4895</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4895</id><created>2013-06-20</created><updated>2013-08-02</updated><authors><author><keyname>Leelaruji</keyname><forenames>R.</forenames></author><author><keyname>Vanfretti</keyname><forenames>L.</forenames></author><author><keyname>Gjerde</keyname><forenames>J. O.</forenames></author><author><keyname>Lovlund</keyname><forenames>S.</forenames></author></authors><title>PMU-based Voltage Instability Detection through Linear Regression</title><categories>cs.SY</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Timely recognition of voltage instability is crucial to allow for effective
control and protection interventions. Phasor measurements units (PMUs) can be
utilized to provide high sampling rate time-synchronized voltage and current
phasors suitable for wide-area voltage instability detection. However, PMU data
contains unwanted measurement errors and noise, which may affect the results of
applications using these measurements for voltage instability detection. The
aim of this article is to revisit a sensitivities calculation to detect voltage
instability by applying a method utilizing linear regression for preprocessing
PMU data. The methodology is validated using both real-time
hardware-in-the-loop simulation and real PMU measurements from Norwegian
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4898</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4898</id><created>2013-06-20</created><updated>2013-10-07</updated><authors><author><keyname>Blindell</keyname><forenames>Gabriel S. Hjort</forenames></author></authors><title>Survey on Instruction Selection: An Extensive and Modern Literature
  Review</title><categories>cs.PL</categories><comments>Major changes: - Merged simulation chapter with macro expansion
  chapter - Addressed misunderstandings of several approaches - Completely
  rewrote many parts of the chapters; strengthened the discussion of many
  approaches - Revised the drawing of all trees and graphs to put the root at
  the top instead of at the bottom - Added appendix for listing the approaches
  in a table See doc for more info</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Instruction selection is one of three optimisation problems involved in the
code generator backend of a compiler. The instruction selector is responsible
of transforming an input program from its target-independent representation
into a target-specific form by making best use of the available machine
instructions. Hence instruction selection is a crucial part of efficient code
generation.
  Despite on-going research since the late 1960s, the last, comprehensive
survey on the field was written more than 30 years ago. As new approaches and
techniques have appeared since its publication, this brings forth a need for a
new, up-to-date review of the current body of literature. This report addresses
that need by performing an extensive review and categorisation of existing
research. The report therefore supersedes and extends the previous surveys, and
also attempts to identify where future research should be directed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4905</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4905</id><created>2013-06-20</created><authors><author><keyname>Belohlavek</keyname><forenames>Radim</forenames></author><author><keyname>Trnecka</keyname><forenames>Martin</forenames></author></authors><title>From-Below Approximations in Boolean Matrix Factorization: Geometry and
  New Algorithm</title><categories>cs.NA cs.LG</categories><doi>10.1016/j.jcss.2015.06.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new results on Boolean matrix factorization and a new algorithm
based on these results. The results emphasize the significance of
factorizations that provide from-below approximations of the input matrix.
While the previously proposed algorithms do not consider the possibly different
significance of different matrix entries, our results help measure such
significance and suggest where to focus when computing factors. An experimental
evaluation of the new algorithm on both synthetic and real data demonstrates
its good performance in terms of good coverage by the first k factors as well
as a small number of factors needed for exact decomposition and indicates that
the algorithm outperforms the available ones in these terms. We also propose
future research topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4908</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4908</id><created>2013-06-20</created><authors><author><keyname>Marujo</keyname><forenames>Luis</forenames></author><author><keyname>Ling</keyname><forenames>Wang</forenames></author><author><keyname>Gershman</keyname><forenames>Anatole</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o P.</forenames></author><author><keyname>Matos</keyname><forenames>David</forenames></author></authors><title>Recognition of Named-Event Passages in News Articles</title><categories>cs.CL cs.IR</categories><comments>In 25th International Conference on Computational Linguistics (COLING
  2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the concept of Named Entities to Named Events - commonly occurring
events such as battles and earthquakes. We propose a method for finding
specific passages in news articles that contain information about such events
and report our preliminary evaluation results. Collecting &quot;Gold Standard&quot; data
presents many problems, both practical and conceptual. We present a method for
obtaining such data using the Amazon Mechanical Turk service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4917</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4917</id><created>2013-06-20</created><authors><author><keyname>Boland</keyname><forenames>Natashia</forenames></author><author><keyname>Kalinowski</keyname><forenames>Thomas</forenames></author><author><keyname>Kapoor</keyname><forenames>Reena</forenames></author><author><keyname>Kaur</keyname><forenames>Simranjit</forenames></author></authors><title>Scheduling unit processing time arc shutdown jobs to maximize network
  flow over time: complexity results</title><categories>cs.DM cs.CC</categories><msc-class>90C10, 90B10, 68Q25</msc-class><journal-ref>Networks 63(2):196-202, 2014</journal-ref><doi>10.1002/net.21536</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of scheduling maintenance on arcs of a capacitated
network so as to maximize the total flow from a source node to a sink node over
a set of time periods. Maintenance on an arc shuts down the arc for the
duration of the period in which its maintenance is scheduled, making its
capacity zero for that period. A set of arcs is designated to have maintenance
during the planning period, which will require each to be shut down for exactly
one time period. In general this problem is known to be NP-hard. Here we
identify a number of characteristics that are relevant for the complexity of
instance classes. In particular, we discuss instances with restrictions on the
set of arcs that have maintenance to be scheduled; series parallel networks;
capacities that are balanced, in the sense that the total capacity of arcs
entering a (non-terminal) node equals the total capacity of arcs leaving the
node; and identical capacities on all arcs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4919</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4919</id><created>2013-06-20</created><authors><author><keyname>Alewijnse</keyname><forenames>Sander P. A.</forenames></author><author><keyname>Bouts</keyname><forenames>Quirijn W.</forenames></author><author><keyname>Brink</keyname><forenames>Alex P. ten</forenames></author><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author></authors><title>Computing the Greedy Spanner in Linear Space</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The greedy spanner is a high-quality spanner: its total weight, edge count
and maximal degree are asymptotically optimal and in practice significantly
better than for any other spanner with reasonable construction time.
Unfortunately, all known algorithms that compute the greedy spanner of n points
use Omega(n^2) space, which is impractical on large instances. To the best of
our knowledge, the largest instance for which the greedy spanner was computed
so far has about 13,000 vertices.
  We present a O(n)-space algorithm that computes the same spanner for points
in R^d running in O(n^2 log^2 n) time for any fixed stretch factor and
dimension. We discuss and evaluate a number of optimizations to its running
time, which allowed us to compute the greedy spanner on a graph with a million
vertices. To our knowledge, this is also the first algorithm for the greedy
spanner with a near-quadratic running time guarantee that has actually been
implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4925</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4925</id><created>2013-06-20</created><authors><author><keyname>Maratea</keyname><forenames>Marco</forenames></author><author><keyname>Pulina</keyname><forenames>Luca</forenames></author><author><keyname>Ricca</keyname><forenames>Francesco</forenames></author></authors><title>A Multi-Engine Approach to Answer Set Programming</title><categories>cs.AI cs.LO</categories><comments>26 pages, 8 figures</comments><msc-class>68T20</msc-class><doi>10.1017/S1471068413000094</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is a truly-declarative programming paradigm
proposed in the area of non-monotonic reasoning and logic programming, that has
been recently employed in many applications. The development of efficient ASP
systems is, thus, crucial. Having in mind the task of improving the solving
methods for ASP, there are two usual ways to reach this goal: $(i)$ extending
state-of-the-art techniques and ASP solvers, or $(ii)$ designing a new ASP
solver from scratch. An alternative to these trends is to build on top of
state-of-the-art solvers, and to apply machine learning techniques for choosing
automatically the &quot;best&quot; available solver on a per-instance basis.
  In this paper we pursue this latter direction. We first define a set of
cheap-to-compute syntactic features that characterize several aspects of ASP
programs. Then, we apply classification methods that, given the features of the
instances in a {\sl training} set and the solvers' performance on these
instances, inductively learn algorithm selection strategies to be applied to a
{\sl test} set. We report the results of a number of experiments considering
solvers and different training and test sets of instances taken from the ones
submitted to the &quot;System Track&quot; of the 3rd ASP Competition. Our analysis shows
that, by applying machine learning techniques to ASP solving, it is possible to
obtain very robust performance: our approach can solve more instances compared
with any solver that entered the 3rd ASP Competition. (To appear in Theory and
Practice of Logic Programming (TPLP).)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4934</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4934</id><created>2013-06-20</created><updated>2015-04-07</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>On the Corner Points of the Capacity Region of a Two-User Gaussian
  Interference Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Trans. on Information Theory in July 17, 2014,
  and revised in April 5, 2015. Presented in part at Allerton 2013, and also
  presented in part with improved results at ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the corner points of the capacity region of a two-user
Gaussian interference channel (GIC). In a two-user GIC, the rate pairs where
one user transmits its data at the single-user capacity (without interference),
and the other at the largest rate for which reliable communication is still
possible are called corner points. This paper relies on existing outer bounds
on the capacity region of a two-user GIC that are used to derive informative
bounds on the corner points of the capacity region. The new bounds refer to a
weak two-user GIC (i.e., when both cross-link gains in standard form are
positive and below 1), and a refinement of these bounds is obtained for the
case where the transmission rate of one user is within $\varepsilon &gt; 0$ of the
single-user capacity. The bounds on the corner points are asymptotically tight
as the transmitted powers tend to infinity, and they are also useful for the
case of moderate SNR and INR. Upper and lower bounds on the gap (denoted by
$\Delta$) between the sum-rate and the maximal achievable total rate at the two
corner points are derived. This is followed by an asymptotic analysis analogous
to the study of the generalized degrees of freedom (where the SNR and INR
scalings are coupled such that $\frac{\log(\text{INR})}{\log(\text{SNR})} =
\alpha \geq 0$), leading to an asymptotic characterization of this gap which is
exact for the whole range of $\alpha$. The upper and lower bounds on $\Delta$
are asymptotically tight in the sense that they achieve the exact asymptotic
characterization. Improved bounds on $\Delta$ are derived for finite SNR and
INR, and their improved tightness is exemplified numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4947</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4947</id><created>2013-06-20</created><updated>2013-10-03</updated><authors><author><keyname>Zhu</keyname><forenames>Xiaojin</forenames></author></authors><title>Machine Teaching for Bayesian Learners in the Exponential Family</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What if there is a teacher who knows the learning goal and wants to design
good training data for a machine learner? We propose an optimal teaching
framework aimed at learners who employ Bayesian models. Our framework is
expressed as an optimization problem over teaching examples that balance the
future loss of the learner and the effort of the teacher. This optimization
problem is in general hard. In the case where the learner employs conjugate
exponential family models, we present an approximate algorithm for finding the
optimal teaching set. Our algorithm optimizes the aggregate sufficient
statistics, then unpacks them into actual teaching examples. We give several
examples to illustrate our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4949</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4949</id><created>2013-06-20</created><updated>2013-11-03</updated><authors><author><keyname>Clark</keyname><forenames>Andrew</forenames></author><author><keyname>Alomair</keyname><forenames>Basel</forenames></author><author><keyname>Bushnell</keyname><forenames>Linda</forenames></author><author><keyname>Poovendran</keyname><forenames>Radha</forenames></author></authors><title>Minimizing Convergence Error in Multi-Agent Systems via Leader
  Selection: A Supermodular Optimization Approach</title><categories>cs.SY</categories><comments>33 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a leader-follower multi-agent system (MAS), the leader agents act as
control inputs and influence the states of the remaining follower agents. The
rate at which the follower agents converge to their desired states, as well as
the errors in the follower agent states prior to convergence, are determined by
the choice of leader agents. In this paper, we study leader selection in order
to minimize convergence errors experienced by the follower agents, which we
define as a norm of the distance between the follower agents' intermediate
states and the convex hull of the leader agent states. By introducing a novel
connection to random walks on the network graph, we show that the convergence
error has an inherent supermodular structure as a function of the leader set.
Supermodularity enables development of efficient discrete optimization
algorithms that directly approximate the optimal leader set, provide provable
performance guarantees, and do not rely on continuous relaxations. We formulate
two leader selection problems within the supermodular optimization framework,
namely, the problem of selecting a fixed number of leader agents in order to
minimize the convergence error, as well as the problem of selecting the
minimum-size set of leader agents to achieve a given bound on the convergence
error. We introduce algorithms for approximating the optimal solution to both
problems in static networks, dynamic networks with known topology
distributions, and dynamic networks with unknown and unpredictable topology
distributions. Our approach is shown to provide significantly lower convergence
errors than existing random and degree-based leader selection methods in a
numerical study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4956</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4956</id><created>2013-06-20</created><authors><author><keyname>Abolfazli</keyname><forenames>Saeid</forenames></author><author><keyname>Sanaei</keyname><forenames>Zohreh</forenames></author><author><keyname>Ahmed</keyname><forenames>Ejaz</forenames></author><author><keyname>Gani</keyname><forenames>Abdullah</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Cloud-Based Augmentation for Mobile Devices: Motivation, Taxonomies, and
  Open Challenges</title><categories>cs.DC</categories><comments>Accepted for Publication in IEEE Communications Surveys &amp; Tutorials</comments><journal-ref>IEEE Communications Surveys &amp; Tutorials (2013) 1-32</journal-ref><doi>10.1109/SURV.2013.070813.00285</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Cloud-based Mobile Augmentation (CMA) approaches have gained
remarkable ground from academia and industry. CMA is the state-of-the-art
mobile augmentation model that employs resource-rich clouds to increase,
enhance, and optimize computing capabilities of mobile devices aiming at
execution of resource-intensive mobile applications. Augmented mobile devices
envision to perform extensive computations and to store big data beyond their
intrinsic capabilities with least footprint and vulnerability. Researchers
utilize varied cloud-based computing resources (e.g., distant clouds and nearby
mobile nodes) to meet various computing requirements of mobile users. However,
employing cloud-based computing resources is not a straightforward panacea.
Comprehending critical factors that impact on augmentation process and optimum
selection of cloud-based resource types are some challenges that hinder CMA
adaptability. This paper comprehensively surveys the mobile augmentation domain
and presents taxonomy of CMA approaches. The objectives of this study is to
highlight the effects of remote resources on the quality and reliability of
augmentation processes and discuss the challenges and opportunities of
employing varied cloud-based resources in augmenting mobile devices. We present
augmentation definition, motivation, and taxonomy of augmentation types,
including traditional and cloud-based. We critically analyze the
state-of-the-art CMA approaches and classify them into four groups of distant
fixed, proximate fixed, proximate mobile, and hybrid to present a taxonomy.
Vital decision making and performance limitation factors that influence on the
adoption of CMA approaches are introduced and an exemplary decision making
flowchart for future CMA approaches are presented. Impacts of CMA approaches on
mobile computing is discussed and open challenges are presented as the future
research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4962</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4962</id><created>2013-06-20</created><updated>2014-01-31</updated><authors><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author></authors><title>An Exploratory Ethnographic Study of Issues and Concerns with Whole
  Genome Sequencing</title><categories>cs.CR cs.CY q-bio.GN</categories><comments>A preliminary version of this paper appears in USEC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Progress in Whole Genome Sequencing (WGS) will soon allow a large number of
individuals to have their genome fully sequenced. This lays the foundations to
improve modern healthcare, enabling a new era of personalized medicine where
diagnosis and treatment is tailored to the patient's genetic makeup. It also
allows individuals motivated by personal curiosity to have access to their
genetic information, and use it, e.g., to trace their ancestry. However, the
very same progress also amplifies a number of ethical and privacy concerns,
that stem from the unprecedented sensitivity of genomic information and that
are not well studied. This paper presents an exploratory ethnographic study of
users' perception of privacy and ethical issues with WGS, as well as their
attitude toward different WGS programs. We report on a series of
semi-structured interviews, involving 16 participants, and analyze the results
both quantitatively and qualitatively. Our analysis shows that users exhibit
common trust concerns and fear of discrimination, and demand to retain strict
control over their genetic information. Finally, we highlight the need for
further research in the area and follow-up studies that build on our initial
findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4966</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4966</id><created>2013-06-20</created><authors><author><keyname>Hu</keyname><forenames>Rui</forenames></author><author><keyname>Watt</keyname><forenames>Stephen M.</forenames></author></authors><title>Determining Points on Handwritten Mathematical Symbols</title><categories>cs.CV cs.CY</categories><comments>16 pages; 19 figures; Conferences on Intelligent Computer Mathematics
  (CICM2013), July 8-12, 2013, University of Bath, Bath, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a variety of applications, such as handwritten mathematics and diagram
labelling, it is common to have symbols of many different sizes in use and for
the writing not to follow simple baselines. In order to understand the scale
and relative positioning of individual characters, it is necessary to identify
the location of certain expected features. These are typically identified by
particular points in the symbols, for example, the baseline of a lower case &quot;p&quot;
would be identified by the lowest part of the bowl, ignoring the descender. We
investigate how to find these special points automatically so they may be used
in a number of problems, such as improving two-dimensional mathematical
recognition and in handwriting neatening, while preserving the original style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4997</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4997</id><created>2013-06-20</created><authors><author><keyname>Zhang</keyname><forenames>Shenqiu</forenames></author><author><keyname>Seyedi</keyname><forenames>Alireza</forenames></author></authors><title>Harvesting Resource Allocation in Energy Harvesting Wireless Sensor
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering an energy harvesting sensor network, the overall probability of
event loss is derived. Based on this result, a variety of harvesting resource
allocation schemes (sizing the energy storages and the harvesting devices,
under a total cost constraint) are provided. Their performances are verified
and compared through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.4999</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.4999</id><created>2013-06-20</created><authors><author><keyname>Zhang</keyname><forenames>Lizi</forenames></author></authors><title>Safeguarding E-Commerce against Advisor Cheating Behaviors: Towards More
  Robust Trust Models for Handling Unfair Ratings</title><categories>cs.SI cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In electronic marketplaces, after each transaction buyers will rate the
products provided by the sellers. To decide the most trustworthy sellers to
transact with, buyers rely on trust models to leverage these ratings to
evaluate the reputation of sellers. Although the high effectiveness of
different trust models for handling unfair ratings have been claimed by their
designers, recently it is argued that these models are vulnerable to more
intelligent attacks, and there is an urgent demand that the robustness of the
existing trust models has to be evaluated in a more comprehensive way. In this
work, we classify the existing trust models into two broad categories and
propose an extendable e-marketplace testbed to evaluate their robustness
against different unfair rating attacks comprehensively. On top of highlighting
the robustness of the existing trust models for handling unfair ratings is far
from what they were claimed to be, we further propose and validate a novel
combination mechanism for the existing trust models, Discount-then-Filter, to
notably enhance their robustness against the investigated attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5003</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5003</id><created>2013-06-20</created><authors><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Vardi</keyname><forenames>Shai</forenames></author></authors><title>A Local Computation Approximation Scheme to Maximum Matching</title><categories>cs.DS</categories><comments>Appears in Approx 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polylogarithmic local computation matching algorithm which
guarantees a $(1-\eps)$-approximation to the maximum matching in graphs of
bounded degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5005</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5005</id><created>2013-06-20</created><updated>2013-12-13</updated><authors><author><keyname>Fochtman</keyname><forenames>Tyler</forenames></author><author><keyname>Hendricks</keyname><forenames>Jacob</forenames></author><author><keyname>Padilla</keyname><forenames>Jennifer E.</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Rogers</keyname><forenames>Trent A.</forenames></author></authors><title>Signal Transmission Across Tile Assemblies: 3D Static Tiles Simulate
  Active Self-Assembly by 2D Signal-Passing Tiles</title><categories>cs.ET</categories><comments>A condensed version of this paper will appear in a special issue of
  Natural Computing for papers from DNA 19. This full version contains proofs
  not seen in the published version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 2-Handed Assembly Model (2HAM) is a tile-based self-assembly model in
which, typically beginning from single tiles, arbitrarily large aggregations of
static tiles combine in pairs to form structures. The Signal-passing Tile
Assembly Model (STAM) is an extension of the 2HAM in which the tiles are
dynamically changing components which are able to alter their binding domains
as they bind together. For our first result, we demonstrate useful techniques
and transformations for converting an arbitrarily complex STAM$^+$ tile set
into an STAM$^+$ tile set where every tile has a constant, low amount of
complexity, in terms of the number and types of ``signals'' they can send, with
a trade off in scale factor.
  Using these simplifications, we prove that for each temperature $\tau&gt;1$
there exists a 3D tile set in the 2HAM which is intrinsically universal for the
class of all 2D STAM$^+$ systems at temperature $\tau$ (where the STAM$^+$ does
not make use of the STAM's power of glue deactivation and assembly breaking, as
the tile components of the 2HAM are static and unable to change or break
bonds). This means that there is a single tile set $U$ in the 3D 2HAM which
can, for an arbitrarily complex STAM$^+$ system $S$, be configured with a
single input configuration which causes $U$ to exactly simulate $S$ at a scale
factor dependent upon $S$. Furthermore, this simulation uses only two planes of
the third dimension. This implies that there exists a 3D tile set at
temperature $2$ in the 2HAM which is intrinsically universal for the class of
all 2D STAM$^+$ systems at temperature $1$. Moreover, we show that for each
temperature $\tau&gt;1$ there exists an STAM$^+$ tile set which is intrinsically
universal for the class of all 2D STAM$^+$ systems at temperature $\tau$,
including the case where $\tau = 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5013</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5013</id><created>2013-06-20</created><updated>2013-12-16</updated><authors><author><keyname>Biagioni</keyname><forenames>David J.</forenames></author><author><keyname>Beylkin</keyname><forenames>Daniel</forenames></author><author><keyname>Beylkin</keyname><forenames>Gregory</forenames></author></authors><title>Randomized Interpolative Decomposition of Separated Representations</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce tensor Interpolative Decomposition (tensor ID) for the reduction
of the separation rank of Canonical Tensor Decompositions (CTDs). Tensor ID
selects, for a user-defined accuracy \epsilon, a near optimal subset of terms
of a CTD to represent the remaining terms via a linear combination of the
selected terms. Tensor ID can be used as an alternative to or a step of the
Alternating Least Squares (ALS) algorithm. In addition, we briefly discuss
Q-factorization to reduce the size of components within an ALS iteration.
Combined, tensor ID and Q-factorization lead to a new paradigm for the
reduction of the separation rank of CTDs. In this context, we also discuss the
spectral norm as a computational alternative to the Frobenius norm.
  We reduce the problem of finding tensor IDs to that of constructing
Interpolative Decompositions of certain matrices. These matrices are generated
via either randomized projection or randomized sampling of the given tensor. We
provide cost estimates and several examples of the new approach to the
reduction of separation rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5018</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5018</id><created>2013-06-20</created><authors><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author><author><keyname>Wagner</keyname><forenames>Aaron B.</forenames></author><author><keyname>Sahai</keyname><forenames>Anant</forenames></author></authors><title>Information embedding and the triple role of control</title><categories>cs.IT math.IT</categories><comments>Revised version submitted to IEEE Trans. Info Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of information embedding where the encoder modifies a
white Gaussian host signal in a power-constrained manner to encode a message,
and the decoder recovers both the embedded message and the modified host
signal. This partially extends the recent work of Sumszyk and Steinberg to the
continuous-alphabet Gaussian setting. Through a control-theoretic lens, we
observe that the problem is a minimalist example of what is called the &quot;triple
role&quot; of control actions. We show that a dirty-paper-coding strategy achieves
the optimal rate for perfect recovery of the modified host and the message for
any message rate. For imperfect recovery of the modified host, by deriving
bounds on the minimum mean-square error (MMSE) in recovering the modified host
signal, we show that DPC-based strategies are guaranteed to attain within a
uniform constant factor of 16 of the optimal weighted sum of power required in
host signal modification and the MMSE in the modified host signal
reconstruction for all weights and all message rates. When specialized to the
zero-rate case, our results provide the tightest known lower bounds on the
asymptotic costs for the vector version of a famous open problem in
decentralized control: the Witsenhausen counterexample. Numerically, this
tighter bound helps us characterize the asymptotically optimal costs for the
vector Witsenhausen problem to within a factor of 1.3 for all problem
parameters, improving on the earlier best known bound of 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5029</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5029</id><created>2013-06-20</created><authors><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author><author><keyname>Vitter</keyname><forenames>Jeffrey Scott</forenames></author></authors><title>Optimal Color Range Reporting in One Dimension</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Color (or categorical) range reporting is a variant of the orthogonal range
reporting problem in which every point in the input is assigned a \emph{color}.
While the answer to an orthogonal point reporting query contains all points in
the query range $Q$, the answer to a color reporting query contains only
distinct colors of points in $Q$. In this paper we describe an O(N)-space data
structure that answers one-dimensional color reporting queries in optimal
$O(k+1)$ time, where $k$ is the number of colors in the answer and $N$ is the
number of points in the data structure. Our result can be also dynamized and
extended to the external memory model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5039</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5039</id><created>2013-06-20</created><authors><author><keyname>Iriyama</keyname><forenames>S.</forenames></author><author><keyname>Ohya</keyname><forenames>M.</forenames></author><author><keyname>Volovich</keyname><forenames>I. V.</forenames></author></authors><title>On Quantum Algorithm for Binary Search and Its Computational Complexity</title><categories>quant-ph cs.IT math.IT</categories><comments>8 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new quantum algorithm for a search problem and its computational complexity
are discussed. It is shown in the search problem containing 2^n objects that
our algorithm runs in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5041</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5041</id><created>2013-06-20</created><updated>2013-09-27</updated><authors><author><keyname>Ishii</keyname><forenames>Toshimasa</forenames></author><author><keyname>Ono</keyname><forenames>Hirotaka</forenames></author><author><keyname>Uno</keyname><forenames>Yushi</forenames></author></authors><title>(Total) Vector Domination for Graphs with Bounded Branchwidth</title><categories>cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G=(V,E)$ of order $n$ and an $n$-dimensional non-negative
vector $d=(d(1),d(2),\ldots,d(n))$, called demand vector, the vector domination
(resp., total vector domination) is the problem of finding a minimum
$S\subseteq V$ such that every vertex $v$ in $V\setminus S$ (resp., in $V$) has
at least $d(v)$ neighbors in $S$. The (total) vector domination is a
generalization of many dominating set type problems, e.g., the dominating set
problem, the $k$-tuple dominating set problem (this $k$ is different from the
solution size), and so on, and its approximability and inapproximability have
been studied under this general framework. In this paper, we show that a
(total) vector domination of graphs with bounded branchwidth can be solved in
polynomial time. This implies that the problem is polynomially solvable also
for graphs with bounded treewidth. Consequently, the (total) vector domination
problem for a planar graph is subexponential fixed-parameter tractable with
respectto $k$, where $k$ is the size of solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5042</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5042</id><created>2013-06-21</created><updated>2013-11-28</updated><authors><author><keyname>Li</keyname><forenames>Qian</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Lv</keyname><forenames>Linyuan</forenames></author><author><keyname>Chen</keyname><forenames>Duanbing</forenames></author></authors><title>Identifying Influential Spreaders by Weighted LeaderRank</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>15 pages and 8 figures</comments><journal-ref>Physica A 404 (2014) 47-55</journal-ref><doi>10.1016/j.physa.2014.02.041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying influential spreaders is crucial for understanding and
controlling spreading processes on social networks. Via assigning
degree-dependent weights onto links associated with the ground node, we
proposed a variant to a recent ranking algorithm named LeaderRank [L. Lv et
al., PLoS ONE 6 (2011) e21202]. According to the simulations on the standard
SIR model, the weighted LeaderRank performs better than LeaderRank in three
aspects: (i) the ability to find out more influential spreaders, (ii) the
higher tolerance to noisy data, and (iii) the higher robustness to intentional
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5044</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5044</id><created>2013-06-21</created><updated>2014-01-13</updated><authors><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Wu</keyname><forenames>Fuke</forenames></author><author><keyname>Zhang</keyname><forenames>Ji-Feng</forenames></author></authors><title>Multi-Agent Consensus With Relative-State-Dependent Measurement Noises</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, the distributed consensus corrupted by relative-state-dependent
measurement noises is considered. Each agent can measure or receive its
neighbors' state information with random noises, whose intensity is a vector
function of agents' relative states. By investigating the structure of this
interaction and the tools of stochastic differential equations, we develop
several small consensus gain theorems to give sufficient conditions in terms of
the control gain, the number of agents and the noise intensity function to
ensure mean square (m. s.) and almost sure (a. s.) consensus and quantify the
convergence rate and the steady-state error. Especially, for the case with
homogeneous communication and control channels, a necessary and sufficient
condition to ensure m. s. consensus on the control gain is given and it is
shown that the control gain is independent of the specific network topology,
but only depends on the number of nodes and the noise coefficient constant. For
symmetric measurement models, the almost sure convergence rate is estimated by
the Iterated Logarithm Law of Brownian motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5053</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5053</id><created>2013-06-21</created><authors><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Breaking Symmetry with Different Orderings</title><categories>cs.AI cs.CC</categories><comments>To appear in Proceedings of CP 2013, 19th International Conference on
  Principles and Practice of Constraint Programming. Slightly longer version
  with a proof sketch expanded compared to official LNCS conference proceedings</comments><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We can break symmetry by eliminating solutions within each symmetry class.
For instance, the Lex-Leader method eliminates all but the smallest solution in
the lexicographical ordering. Unfortunately, the Lex-Leader method is
intractable in general. We prove that, under modest assumptions, we cannot
reduce the worst case complexity of breaking symmetry by using other orderings
on solutions. We also prove that a common type of symmetry, where rows and
columns in a matrix of decision variables are interchangeable, is intractable
to break when we use two promising alternatives to the lexicographical
ordering: the Gray code ordering (which uses a different ordering on
solutions), and the Snake-Lex ordering (which is a variant of the
lexicographical ordering that re-orders the variables). Nevertheless, we show
experimentally that using other orderings like the Gray code to break symmetry
can be beneficial in practice as they may better align with the objective
function and branching heuristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5056</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5056</id><created>2013-06-21</created><updated>2014-02-22</updated><authors><author><keyname>Sanderson</keyname><forenames>Tyler</forenames></author><author><keyname>Scott</keyname><forenames>Clayton</forenames></author></authors><title>Class Proportion Estimation with Application to Multiclass Anomaly
  Rejection</title><categories>stat.ML cs.LG</categories><comments>Accepted to AISTATS 2014. 15 pages. 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses two classification problems that fall under the heading
of domain adaptation, wherein the distributions of training and testing
examples differ. The first problem studied is that of class proportion
estimation, which is the problem of estimating the class proportions in an
unlabeled testing data set given labeled examples of each class. Compared to
previous work on this problem, our approach has the novel feature that it does
not require labeled training data from one of the classes. This property allows
us to address the second domain adaptation problem, namely, multiclass anomaly
rejection. Here, the goal is to design a classifier that has the option of
assigning a &quot;reject&quot; label, indicating that the instance did not arise from a
class present in the training data. We establish consistent learning strategies
for both of these domain adaptation problems, which to our knowledge are the
first of their kind. We also implement the class proportion estimation
technique and demonstrate its performance on several benchmark data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5061</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5061</id><created>2013-06-21</created><authors><author><keyname>Jakob</keyname><forenames>Robert</forenames></author><author><keyname>Thiemann</keyname><forenames>Peter</forenames></author></authors><title>Towards Tree Automata-based Success Types</title><categories>cs.PL</categories><comments>Abstract presented at HOPA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error detection facilities for dynamic languages are often based on unit
testing. Thus, the advantage of rapid prototyping and flexibility must be
weighed against cumbersome and time consuming test suite development. Lindahl
and Sagonas' success typings provide a means of static must-fail detection in
Erlang. Due to the constraint-based nature of the approach, some errors
involving nested tuples and recursion cannot be detected.
  We propose an approach that uses an extension of model checking for
pattern-matching recursion schemes with context-aware ranked tree automata to
provide improved success typings for a constructor-based first-order prototype
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5066</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5066</id><created>2013-06-21</created><authors><author><keyname>Kim</keyname><forenames>Chol-Su</forenames></author><author><keyname>Jong</keyname><forenames>Kwang-Hyok</forenames></author><author><keyname>Im</keyname><forenames>Song-Jin</forenames></author></authors><title>Document watermarking based on digital holographic principle</title><categories>physics.optics cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for document watermarking based on the digital Fourier hologram
is proposed. It applies the methods of digital image watermarking based on
holographic principle presented previously in several papers into printed
documents. Experimental results show that the proposed method can not only meet
the demand on invisibility, robustness and non-reproducibility of the document
watermark, and but also has other advantages compared with the conventional
methods for document securities such as embossed hologram, Lippmann photograph
and halftone modulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5070</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5070</id><created>2013-06-21</created><authors><author><keyname>Lotfi</keyname><forenames>Nasser</forenames></author><author><keyname>Tamouk</keyname><forenames>Jamshid</forenames></author><author><keyname>Farmanbar</keyname><forenames>Mina</forenames></author></authors><title>3-SAT Problem A New Memetic-PSO Algorithm</title><categories>cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3-SAT problem is of great importance to many technical and scientific
applications. This paper presents a new hybrid evolutionary algorithm for
solving this satisfiability problem. 3-SAT problem has the huge search space
and hence it is known as a NP-hard problem. So, deterministic approaches are
not applicable in this context. Thereof, application of evolutionary processing
approaches and especially PSO will be very effective for solving these kinds of
problems. In this paper, we introduce a new evolutionary optimization technique
based on PSO, Memetic algorithm and local search approaches. When some
heuristics are mixed, their advantages are collected as well and we can reach
to the better outcomes. Finally, we test our proposed algorithm over some
benchmarks used by some another available algorithms. Obtained results show
that our new method leads to the suitable results by the appropriate time.
Thereby, it achieves a better result in compared with the existent approaches
such as pure genetic algorithm and some verified types
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5076</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5076</id><created>2013-06-21</created><authors><author><keyname>Sitchinava</keyname><forenames>Nodari</forenames></author><author><keyname>Weichert</keyname><forenames>Volker</forenames></author></authors><title>Provably Efficient GPU Algorithms</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an abstract model for algorithm design on GPUs by
extending the parallel external memory (PEM) model with computations in
internal memory (commonly known as shared memory in GPU literature) defined in
the presence of memory banks and bank conflicts. We also present a framework
for designing bank conflict free algorithms on GPUs.
  Using our framework we develop the first shared memory sorting algorithm that
incurs no bank conflicts. Our sorting algorithm can be used as a subroutine for
comparison-based GPU sorting algorithms to replace current use of sorting
networks in shared memory. We show experimentally that such substitution
improves the runtime of the mergesort implementation of the THRUST library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5081</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5081</id><created>2013-06-21</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Hackl</keyname><forenames>Thomas</forenames></author><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author><author><keyname>Ramos</keyname><forenames>Pedro A.</forenames></author><author><keyname>Sacrist&#xe1;n</keyname><forenames>Vera</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>Empty triangles in good drawings of the complete graph</title><categories>cs.CG cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A good drawing of a simple graph is a drawing on the sphere or, equivalently,
in the plane in which vertices are drawn as distinct points, edges are drawn as
Jordan arcs connecting their end vertices, and any pair of edges intersects at
most once. In any good drawing, the edges of three pairwise connected vertices
form a Jordan curve which we call a triangle. We say that a triangle is empty
if one of the two connected components it induces does not contain any of the
remaining vertices of the drawing of the graph. We show that the number of
empty triangles in any good drawing of the complete graph $K_n$ with $n$
vertices is at least $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5088</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5088</id><created>2013-06-21</created><updated>2013-10-10</updated><authors><author><keyname>Artale</keyname><forenames>A.</forenames></author><author><keyname>Kontchakov</keyname><forenames>R.</forenames></author><author><keyname>Ryzhikov</keyname><forenames>V.</forenames></author><author><keyname>Zakharyaschev</keyname><forenames>M.</forenames></author></authors><title>The Complexity of Clausal Fragments of LTL</title><categories>cs.LO cs.CC</categories><comments>arXiv admin note: text overlap with arXiv:1209.5571</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and investigate a number of fragments of propo- sitional
temporal logic LTL over the flow of time (Z, &lt;). The fragments are defined in
terms of the available temporal operators and the structure of the clausal
normal form of the temporal formulas. We determine the computational complexity
of the satisfiability problem for each of the fragments, which ranges from
NLogSpace to PTime, NP and PSpace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5089</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5089</id><created>2013-06-21</created><updated>2014-01-08</updated><authors><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames></author><author><keyname>Dowek</keyname><forenames>Gilles</forenames></author></authors><title>Normalisation of a Non-deterministic Type Isomorphic {\lambda}-calculus</title><categories>cs.LO</categories><comments>This paper has been withdrawn by the authors due to a crucial error
  in Definition 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a proof of strong normalisation for lambda+, a recently
introduced, explicitly typed, non-deterministic lambda-calculus where
isomorphic propositions are identified. Such a proof is a non-trivial
adaptation of the reducibility candidates technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5091</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5091</id><created>2013-06-21</created><authors><author><keyname>Tonisson</keyname><forenames>Eno</forenames></author></authors><title>Students' Comparison of Their Trigonometric Answers with the Answers of
  a Computer Algebra System</title><categories>cs.CY</categories><comments>14 pages, Conferences on Intelligent Computer Mathematics CICM 2013
  8.-12. July 2013 Bath, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparison of answers offered by a computer algebra system (CAS) with answers
derived by a student without a CAS is relevant, for instance, in the context of
computer-aided assessment (CAA). The issues of identity, equivalence and
correctness emerge in different ways and are important for CAA. These issues
are also interesting if a student is charged with the task of comparing the
answers. What will happen when students themselves are encouraged to analyse
differences, equivalence and correctness of their own answers and CAS answers?
What differences do they notice foremost? Would they recognise
equivalence/non-equivalence? How do they explain equivalence/non-equivalence?
The paper discusses these questions on the basis of lessons where the students
solved trigonometric equations. Ten equations were chosen with the aim to
ensure that the expected school answer and the CAS answer would differ in
various ways. Three of them are discussed more thoroughly in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5093</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5093</id><created>2013-06-21</created><authors><author><keyname>Ciuonzo</keyname><forenames>D.</forenames></author><author><keyname>Romano</keyname><forenames>G.</forenames></author><author><keyname>Rossi</keyname><forenames>P. Salvo</forenames></author></authors><title>Performance Analysis and Design of Maximum Ratio Combining in
  Channel-Aware MIMO Decision Fusion</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><journal-ref>IEEE Transactions on Wireless Communications, vol. 12, no. 9, pp.
  4716 - 4728 September 2013</journal-ref><doi>10.1109/TWC.2013.071913.130269</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a theoretical performance analysis of the maximum
ratio combining (MRC) rule for channel-aware decision fusion over
multiple-input multiple-output (MIMO) channels for (conditionally) dependent
and independent local decisions. The system probabilities of false alarm and
detection conditioned on the channel realization are derived in closed form and
an approximated threshold choice is given. Furthermore, the channel-averaged
(CA) performances are evaluated in terms of the CA system probabilities of
false alarm and detection and the area under the receiver operating
characteristic (ROC) through the closed form of the conditional moment
generating function (MGF) of the MRC statistic, along with Gauss-Chebyshev (GC)
quadrature rules. Furthermore, we derive the deflection coefficients in closed
form, which are used for sensor threshold design. Finally, all the results are
confirmed through Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5096</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5096</id><created>2013-06-21</created><authors><author><keyname>Velic</keyname><forenames>Marko</forenames></author><author><keyname>Padavic</keyname><forenames>Ivan</forenames></author><author><keyname>Car</keyname><forenames>Sinisa</forenames></author></authors><title>Computer Aided ECG Analysis - State of the Art and Upcoming Challenges</title><categories>cs.CV</categories><comments>7 pages, 3 figures, IEEE EUROCON 2013 International conference on
  computer as a tool, 1-4 July 2013, Zagreb, Croatia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present current achievements in computer aided ECG analysis
and their applicability in real world medical diagnosis process. Most of the
current work is covering problems of removing noise, detecting heartbeats and
rhythm-based analysis. There are some advancements in particular ECG segments
detection and beat classifications but with limited evaluations and without
clinical approvals. This paper presents state of the art advancements in those
areas till present day. Besides this short computer science and signal
processing literature review, paper covers future challenges regarding the ECG
signal morphology analysis deriving from the medical literature review. Paper
is concluded with identified gaps in current advancements and testing, upcoming
challenges for future research and a bullseye test is suggested for morphology
analysis evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5098</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5098</id><created>2013-06-21</created><authors><author><keyname>Velic</keyname><forenames>Marko</forenames></author><author><keyname>Grzinic</keyname><forenames>Toni</forenames></author><author><keyname>Padavic</keyname><forenames>Ivan</forenames></author></authors><title>Wisdom of Crowds Algorithm for Stock Market Predictions</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, International Conference on Information Technology
  Interfaces ITI 2013, June 24-27, 2013, Cavtat/Dubrovnik, Croatia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a mathematical model for collaborative filtering
implementation in stock market predictions. In popular literature collaborative
filtering, also known as Wisdom of Crowds, assumes that group has a greater
knowledge than the individual while each individual can improve group's
performance by its specific information input. There are commercially available
tools for collaborative stock market predictions and patent protected web-based
software solutions. Mathematics that lies behind those algorithms is not
disclosed in the literature, so the presented model and algorithmic
implementation are the main contributions of this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5099</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5099</id><created>2013-06-21</created><authors><author><keyname>Rabhi</keyname><forenames>Emna</forenames></author><author><keyname>Lachiri</keyname><forenames>Zied</forenames></author></authors><title>SVM based on personal identification system using Electrocardiograms</title><categories>cs.SY</categories><comments>Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new algorithm for personal identification from their
Electrocardiograms (ECG) which is based on morphological descriptors and
Hermite Polynomials Expansion coefficients (HPEc). After preprocessing, we
extracted ten morphological descriptors which were divided into homogeneous
groups (amplitude, surface interval and slope) and we extracted sixty Hermite
Polynomials Expansion coefficients(HPEc) from each heartbeat. For the
classification, we employed a binary Support Vector Machines with Gaussian
kernel and we adopted a particular strategy: we first classified groups of
morphological descriptors separately then we combined them in one system. On
the other hand, we classified the Hermite Polynomials Expansion coefficients
apart and we associated them with all groups of morphological descriptors in a
single system in order to improve overall performance. We tested our algorithm
on 18 different healthy signals of the MIT_BIH database. The analysis of
different groups separately showed that the best recognition performance is
96.45% for all morphological descriptors and the results of experiments showed
that the proposed hybrid approach has led to an overall maximum of 98.97%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5109</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5109</id><created>2013-06-21</created><authors><author><keyname>Messaoudi</keyname><forenames>Imen</forenames></author><author><keyname>Oueslati</keyname><forenames>Afef Elloumi</forenames></author><author><keyname>Lachiri</keyname><forenames>Zied</forenames></author></authors><title>Complex Morlet Wavelet Analysis of the DNA Frequency Chaos Game Signal
  and Revealing Specific Motifs of Introns in C.elegans</title><categories>cs.SY q-bio.GN</categories><comments>Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, studying introns is becoming a very promising field in the
genomics. Even though they play a role in the dynamic regulation of gene and in
the organism's evolution, introns have not attracted enough attention like
exons did; especially of digital signal processing researchers. Thus, we focus
on analysis of the C.elegans introns. In this paper, we propose the complex
Morlet wavelet analysis to investigate introns' characterization in the
C.elegans genes. However, catching the change in frequency response with
respect to time of the gene sequences is hindered by their presence in the form
of strings of characters. This can only be counteracted by assigning numerical
values to each of the DNA characters. This operation defines the so called &quot;DNA
coding approach&quot;. In this context, we propose a new coding technique based on
the Frequency Chaos Game Representation (FCGR) that we name the &quot;Frequency
Chaos Game Signal&quot; (FCGS). Results of the complex Morlet wavelet Analysis
applied to the Celegans FCGS are showing a very distinguished texture. The
visual interpretation of the colour scalograms is proved to be an efficient
tool for revealing significant information about intronic sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5111</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5111</id><created>2013-06-21</created><authors><author><keyname>Gruner</keyname><forenames>Alexander</forenames></author><author><keyname>Huber</keyname><forenames>Michael</forenames></author></authors><title>Low-Density Parity-Check Codes From Transversal Designs With Improved
  Stopping Set Distributions</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>11 pages; to appear in &quot;IEEE Transactions on Communications&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the construction of low-density parity-check (LDPC) codes
from transversal designs based on sets of mutually orthogonal Latin squares
(MOLS). By transferring the concept of configurations in combinatorial designs
to the level of Latin squares, we thoroughly investigate the occurrence and
avoidance of stopping sets for the arising codes. Stopping sets are known to
determine the decoding performance over the binary erasure channel and should
be avoided for small sizes. Based on large sets of simple-structured MOLS, we
derive powerful constraints for the choice of suitable subsets, leading to
improved stopping set distributions for the corresponding codes. We focus on
LDPC codes with column weight 4, but the results are also applicable for the
construction of codes with higher column weights. Finally, we show that a
subclass of the presented codes has quasi-cyclic structure which allows
low-complexity encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5142</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5142</id><created>2013-06-21</created><authors><author><keyname>Langr</keyname><forenames>Daniel</forenames></author><author><keyname>Tvrd&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Dytrych</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Draayer</keyname><forenames>Jerry P.</forenames></author></authors><title>Fake Run-Time Selection of Template Arguments in C++</title><categories>cs.PL</categories><comments>Objects, Models, Components, Patterns (50th International Conference,
  TOOLS 2012)</comments><journal-ref>Lecture Notes in Computer Science, pages 140-154, volume 7304.
  2012</journal-ref><doi>10.1007/978-3-642-30561-0_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  C++ does not support run-time resolution of template type arguments. To
circumvent this restriction, we can instantiate a template for all possible
combinations of type arguments at compile time and then select the proper
instance at run time by evaluation of some provided conditions. However, for
templates with multiple type parameters such a solution may easily result in a
branching code bloat. We present a template metaprogramming algorithm called
for_id that allows the user to select the proper template instance at run time
with theoretical minimum sustained complexity of the branching code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5151</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5151</id><created>2013-06-21</created><authors><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author><author><keyname>Rahtu</keyname><forenames>Esa</forenames></author><author><keyname>Kannala</keyname><forenames>Juho</forenames></author><author><keyname>Blaschko</keyname><forenames>Matthew</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>Fine-Grained Visual Classification of Aircraft</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces FGVC-Aircraft, a new dataset containing 10,000 images
of aircraft spanning 100 aircraft models, organised in a three-level hierarchy.
At the finer level, differences between models are often subtle but always
visually measurable, making visual recognition challenging but possible. A
benchmark is obtained by defining corresponding classification tasks and
evaluation protocols, and baseline results are presented. The construction of
this dataset was made possible by the work of aircraft enthusiasts, a strategy
that can extend to the study of number of other object classes. Compared to the
domains usually considered in fine-grained visual classification (FGVC), for
example animals, aircraft are rigid and hence less deformable. They, however,
present other interesting modes of variation, including purpose, size,
designation, structure, historical style, and branding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5156</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5156</id><created>2013-06-21</created><authors><author><keyname>Kobeissi</keyname><forenames>Nadim</forenames></author><author><keyname>Breault</keyname><forenames>Arlo</forenames></author></authors><title>Cryptocat: Adopting Accessibility and Ease of Use as Security Properties</title><categories>cs.CR cs.CY</categories><comments>Working Draft</comments><acm-class>C.2.0; E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptocat is a Free and Open Source Software (FL/OSS) browser extension that
makes use of web technologies in order to provide easy to use, accessible,
encrypted instant messaging to the general public. We aim to investigate how to
best leverage the accessibility and portability offered by web technologies in
order to allow encrypted instant messaging an opportunity to better permeate on
a social level. We have found that encrypted communications, while in many
cases technically well-implemented, suffer from a lack of usage due to their
being unappealing and inaccessible to the &quot;average end-user&quot;. Our position is
that accessibility and ease of use must be treated as security properties. Even
if a cryptographic system is technically highly qualified, securing user
privacy is not achieved without addressing the problem of accessibility. Our
goal is to investigate the feasibility of implementing cryptographic systems in
highly accessible mediums, and to address the technical and social challenges
of making encrypted instant messaging accessible and portable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5158</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5158</id><created>2013-06-21</created><authors><author><keyname>Sherman</keyname><forenames>Galina</forenames></author><author><keyname>Siebers</keyname><forenames>Peer-Olaf</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Menachof</keyname><forenames>David</forenames></author></authors><title>Scenario Analysis, Decision Trees and Simulation for Cost Benefit
  Analysis of the Cargo Screening Process</title><categories>cs.CE stat.AP</categories><comments>International Workshop of Applied Modelling and Simulation (WAMS),
  5-7 May, Buizos, Brasil, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present our ideas for conducting a cost benefit analysis by
using three different methods: scenario analysis, decision trees and
simulation. Then we introduce our case study and examine these methods in a
real world situation. We show how these tools can be used and what the results
are for each of them. Our aim is to conduct a comparison of these different
probabilistic methods of estimating costs for port security risk assessment
studies. Methodologically, we are trying to understand the limits of all the
tools mentioned above by focusing on rare events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5160</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5160</id><created>2013-06-21</created><authors><author><keyname>Sherman</keyname><forenames>Galina</forenames></author><author><keyname>Menachof</keyname><forenames>David</forenames></author><author><keyname>Siebers</keyname><forenames>Peer-Olaf</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Towards modelling cost and risks of infrequent events in the cargo
  screening process</title><categories>cs.CE</categories><comments>UK OR Society Simulation Workshop 2010 (SW10), 23-24 March,
  Worcestershire, UK, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simulation model of the port of Calais with a focus on the
operation of immigration controls. Our aim is to compare the cost and benefits
of different screening policies. Methodologically, we are trying to understand
the limits of discrete event simulation of rare events. When will they become
'too rare' for simulation to give meaningful results?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5166</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5166</id><created>2013-06-21</created><authors><author><keyname>Hegarty</keyname><forenames>Peter</forenames></author><author><keyname>Martinsson</keyname><forenames>Anders</forenames></author><author><keyname>Zhelezov</keyname><forenames>Dmitry</forenames></author></authors><title>A variant of the multi-agent rendezvous problem</title><categories>cs.MA cs.CG cs.DS cs.RO math.PR</categories><comments>18 pages, 3 figures. None of the authors has any previous experience
  in this area of research (multi-agent systems), hence we welcome any feedback
  from specialists</comments><msc-class>68W20, 68M14, 68T40, 60D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical multi-agent rendezvous problem asks for a deterministic
algorithm by which $n$ points scattered in a plane can move about at constant
speed and merge at a single point, assuming each point can use only the
locations of the others it sees when making decisions and that the visibility
graph as a whole is connected. In time complexity analyses of such algorithms,
only the number of rounds of computation required are usually considered, not
the amount of computation done per round. In this paper, we consider
$\Omega(n^2 \log n)$ points distributed independently and uniformly at random
in a disc of radius $n$ and, assuming each point can not only see but also, in
principle, communicate with others within unit distance, seek a randomised
merging algorithm which asymptotically almost surely (a.a.s.) runs in time
O(n), in other words in time linear in the radius of the disc rather than in
the number of points. Under a precise set of assumptions concerning the
communication capabilities of neighboring points, we describe an algorithm
which a.a.s. runs in time O(n) provided the number of points is $o(n^3)$.
Several questions are posed for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5170</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5170</id><created>2013-06-21</created><authors><author><keyname>Abdel-moneim</keyname><forenames>Wafaa Tawfik</forenames></author><author><keyname>Abdel-Aziz</keyname><forenames>Mohamed Hashem</forenames></author><author><keyname>Hassan</keyname><forenames>Mohamed Monier</forenames></author></authors><title>Clinical Relationships Extraction Techniques from Patient Narratives</title><categories>cs.IR cs.CL</categories><comments>15 pages 13 figures 7 tables</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol.10,
  Issue 1, January 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Clinical E-Science Framework (CLEF) project was used to extract important
information from medical texts by building a system for the purpose of clinical
research, evidence-based healthcare and genotype-meets-phenotype informatics.
The system is divided into two parts, one part concerns with the identification
of relationships between clinically important entities in the text. The full
parses and domain-specific grammars had been used to apply many approaches to
extract the relationship. In the second part of the system, statistical machine
learning (ML) approaches are applied to extract relationship. A corpus of
oncology narratives that hand annotated with clinical relationships can be used
to train and test a system that has been designed and implemented by supervised
machine learning (ML) approaches. Many features can be extracted from these
texts that are used to build a model by the classifier. Multiple supervised
machine learning algorithms can be applied for relationship extraction. Effects
of adding the features, changing the size of the corpus, and changing the type
of the algorithm on relationship extraction are examined. Keywords: Text
mining; information extraction; NLP; entities; and relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5173</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5173</id><created>2013-06-21</created><updated>2013-07-11</updated><authors><author><keyname>Kuo</keyname><forenames>Kao-Yueh</forenames></author><author><keyname>Lu</keyname><forenames>Chung-Chin</forenames></author></authors><title>On the Hardnesses of Several Quantum Decoding Problems</title><categories>quant-ph cs.IT math.IT</categories><comments>There are six pages in this paper. Part of this paper was presented
  in the 2012 International Symposium on Information Theory and its
  Applications (ISITA 2012), Hawaii, USA, October 28--31, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify the time complexities of three important decoding problems for
quantum stabilizer codes. First, regardless of the channel model, quantum
bounded distance decoding is shown to be NP-hard, like what Berlekamp, McEliece
and Tilborg did for classical binary linear codes in 1978. Then over the
depolarizing channel, the decoding problems for finding a most likely error and
for minimizing the decoding error probability are also shown to be NP-hard. Our
results indicate that finding a polynomial-time decoding algorithm for general
stabilizer codes may be impossible, but this, on the other hand, strengthens
the foundation of quantum code-based cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5176</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5176</id><created>2013-06-21</created><updated>2015-04-17</updated><authors><author><keyname>G&#xf6;bel</keyname><forenames>Andreas</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>McQuillan</keyname><forenames>Colin</forenames></author><author><keyname>Richerby</keyname><forenames>David</forenames></author><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>Counting list matrix partitions of graphs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a symmetric D*D matrix M over {0,1,*}, a list M-partition of a graph G
is a partition of G's vertices into D parts associated with the rows of M. The
part of each vertex is chosen from a given list so that no edge of G maps to a
0 in M and no non-edge of G maps to a 1 in M. Many important graph-theoretic
structures can be represented as list M-partitions, such as graph colourings,
split graphs and homogeneous sets and pairs, which arise in the proofs of the
weak and strong perfect graph conjectures. There has been quite a bit of work
on determining for which matrices M computations involving list M-partitions
are tractable. We focus on counting list M-partitions, given a graph G and a
list for each vertex of G. We identify a set of &quot;tractable&quot; matrices and give
an algorithm that counts list M-partitions in polynomial time for every (fixed)
matrix M in this set. The algorithm uses data structures such as sparse-dense
partitions and subcube decompositions to reduce each instance to a sequence of
instances in which the lists restrict access to portions of M in which the
interaction of 0s and 1s is controlled. We solve the resulting restricted
instances by converting them into counting constraint satisfaction problems
(#CSPs) which we solve using arc-consistency. For every matrix M for which our
algorithm fails, we show that counting list M-partitions is #P-complete.
Further, we give an explicit characterisation of the dichotomy theorem:
counting list M-partitions is in FP if M has a structure called a
derectangularising sequence; otherwise, counting list M-partitions is #P-hard.
We show that the meta-problem of determining whether a given matrix has a
derectangularising sequence is NP-complete. Finally, we show that lists can be
used to encode cardinality restrictions in M-partitions problems and use this
to give a polynomial-time algorithm for counting homogeneous pairs in graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5180</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5180</id><created>2013-06-20</created><authors><author><keyname>Abdessamad</keyname><forenames>Benlafkih</forenames></author><author><keyname>Salah-ddine</keyname><forenames>Krit</forenames></author><author><keyname>Mohamed</keyname><forenames>Chafik Elidrissi</forenames></author></authors><title>A Comparative study of Analog and digital Controller On DC/DC Buck-Boost
  Converter Four Switch for Mobile Device Applications</title><categories>cs.OH</categories><comments>6 pages, IJCSI International Journal of Computer Science Issues, Vol.
  10, Issue 1, No 2, January 2013</comments><acm-class>B.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents comparative performance between Analog and digital
controller on DC/DC buck-boost converter four switch. The design of power
electronic converter circuit with the use of closed loop scheme needs modeling
and then simulating the converter using the modeled equations. This can easily
be done with the help of state equations and MATLAB/SIMULINK as a tool for
simulation of those state equations. DC/DC Buckboost converter in this study is
operated in buck (step-down) and boost (step-up) modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5192</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5192</id><created>2013-06-19</created><authors><author><keyname>Chitikela</keyname><forenames>Sindhu</forenames></author></authors><title>State Decoding in Multi-Stage Cryptography Protocols</title><categories>quant-ph cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a practical method of quantum tomography for decoding the
state of photons in a multistage cryptography protocol. This method works if
the polarization angles are defined on a fixed plane, as is assumed in several
quantum cryptography protocols. We show if there are 2m polarization angles in
a fixed plane, we need m number of filters and m2 number of photons through
each filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5204</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5204</id><created>2013-06-21</created><authors><author><keyname>Morstatter</keyname><forenames>Fred</forenames></author><author><keyname>Pfeffer</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Liu</keyname><forenames>Huan</forenames></author><author><keyname>Carley</keyname><forenames>Kathleen M.</forenames></author></authors><title>Is the Sample Good Enough? Comparing Data from Twitter's Streaming API
  with Twitter's Firehose</title><categories>cs.SI physics.soc-ph</categories><comments>Published in ICWSM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is a social media giant famous for the exchange of short,
140-character messages called &quot;tweets&quot;. In the scientific community, the
microblogging site is known for openness in sharing its data. It provides a
glance into its millions of users and billions of tweets through a &quot;Streaming
API&quot; which provides a sample of all tweets matching some parameters preset by
the API user. The API service has been used by many researchers, companies, and
governmental institutions that want to extract knowledge in accordance with a
diverse array of questions pertaining to social media. The essential drawback
of the Twitter API is the lack of documentation concerning what and how much
data users get. This leads researchers to question whether the sampled data is
a valid representation of the overall activity on Twitter. In this work we
embark on answering this question by comparing data collected using Twitter's
sampled API service with data collected using the full, albeit costly, Firehose
stream that includes every single published tweet. We compare both datasets
using common statistical metrics as well as metrics that allow us to compare
topics, networks, and locations of tweets. The results of our work will help
researchers and practitioners understand the implications of using the
Streaming API.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5209</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5209</id><created>2013-06-21</created><authors><author><keyname>Saini</keyname><forenames>Gurpreet Singh</forenames></author><author><keyname>Dubey</keyname><forenames>Priyanka</forenames></author><author><keyname>Rahman</keyname><forenames>Md Tanzilur</forenames></author></authors><title>Review Study For Inter-Operability Of Manet Protocols In Wireless Sensor
  Networks</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Trends and Technology (IJCTT),
  Volume 4, Issue 6 May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Networks are most appealing in terms of deployment over a wide range
of applications. The key areas are disaster management, industrial unit
automation and battlefield surveillance. The paper presents a study over
inter-operability of MANET (Mobile Ad-Hoc Network) protocols i.e DSDV, OLSR,
ZRP, AODV over WSN (Wireless Sensor Network) [10]. The review here covers all
the prevailing protocol solutions for WSN and deployment of MANET protocols
over them. The need of moving to MANET protocols lie in situation when we talk
about mobile sensory nodes which are a compulsion when we talk about the above
mentioned three areas. However, the deployment may not be limited to these
only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5215</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5215</id><created>2013-06-21</created><authors><author><keyname>Tolk</keyname><forenames>Andreas</forenames></author><author><keyname>Diallo</keyname><forenames>Saikou Y.</forenames></author><author><keyname>Padilla</keyname><forenames>Jose J.</forenames></author><author><keyname>Gore</keyname><forenames>Ross</forenames></author></authors><title>Epistemology of Modeling and Simulation: How can we gain Knowledge from
  Simulations?</title><categories>cs.GL cs.AI</categories><comments>MODSIM World 2013</comments><msc-class>00</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epistemology is the branch of philosophy that deals with gaining knowledge.
It is closely related to ontology. The branch that deals with questions like
&quot;What is real?&quot; and &quot;What do we know?&quot; as it provides these components. When
using modeling and simulation, we usually imply that we are doing so to either
apply knowledge, in particular when we are using them for training and
teaching, or that we want to gain new knowledge, for example when doing
analysis or conducting virtual experiments. This paper looks at the history of
science to give a context to better cope with the question, how we can gain
knowledge from simulation. It addresses aspects of computability and the
general underlying mathematics, and applies the findings to validation and
verification and development of federations. As simulations are understood as
computable executable hypotheses, validation can be understood as hypothesis
testing and theory building. The mathematical framework allows furthermore
addressing some challenges when developing federations and the potential
introduction of contradictions when composing different theories, as they are
represented by the federated simulation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5216</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5216</id><created>2013-06-21</created><authors><author><keyname>Chang</keyname><forenames>J.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author></authors><title>Modification to Darcy model for high pressure and high velocity
  applications and associated mixed finite element formulations</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Darcy model is based on a plethora of assumptions. One of the most
important assumptions is that the Darcy model assumes the drag coefficient to
be constant. However, there is irrefutable experimental evidence that
viscosities of organic liquids and carbon-dioxide depend on the pressure.
Experiments have also shown that the drag varies nonlinearly with respect to
the velocity at high flow rates. In important technological applications like
enhanced oil recovery and geological carbon-dioxide sequestration, one
encounters both high pressures and high flow rates. It should be emphasized
that flow characteristics and pressure variation under varying drag are both
quantitatively and qualitatively different from that of constant drag.
Motivated by experimental evidence, we consider the drag coefficient to depend
on both the pressure and velocity. We consider two major modifications to the
Darcy model based on the Barus formula and Forchheimer approximation. The
proposed modifications to the Darcy model result in nonlinear partial
differential equations, which are not amenable to analytical solutions. To this
end, we present mixed finite element formulations based on least-squares
formalism and variational multiscale formalism for the resulting governing
equations. The proposed modifications to the Darcy model and its associated
finite element formulations are used to solve realistic problems with relevance
to enhanced oil recovery. We also study the competition between the nonlinear
dependence of drag on the velocity and the dependence of viscosity on the
pressure. To the best of the authors' knowledge such a systematic study has not
been performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5219</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5219</id><created>2013-06-21</created><updated>2013-07-05</updated><authors><author><keyname>Lopez-Medrano</keyname><forenames>Alvaro</forenames></author></authors><title>How Information Transfer works: interpretation of Information Contents
  in Bayes Theorem. Understanding Negative Information</title><categories>cs.IT math.IT q-bio.NC</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a given space of models or hypothesis the individual information content
of each of them is considered as opposed to the Shannon entropy that measures
the average information content of the mentioned space. In particular
expressing Bayes Theorem in terms of the information contents associated to its
probabilities allows understanding how bits of information, introduced in the
system by an observation, are transferred to each of the models in the space.
It is shown how, from a single observation not one, but two causal information
sources are generated: the Information Content Associated to the Evidence that
always introduces positive information, and the Information Content Associated
to the Bayes Likelihood that always introduces negative bits; therefore the
evidence contributes to increase the probability of occurrence of the model and
the likelihood to decrease it; depending on the net value of the difference
between these two mentioned information contents, the information that arrives
to a given model will be positive or negative. Thus, we propose a novel metric,
given by the difference of the two mentioned information contents called
transfer information content which measures the information transferred to each
of the single models in the space. The resolution of the Monty Hall Problem
(MHP) and some of its variants in the Information Theory framework proposed
allows to confirm the validity of the formulas derived and to understand the
counterintuitive and theoretically problematic concept of negative information.
The implications of the concepts introduced in terms of information transfer to
the emergent field of Local Information Dynamics, to Computational Neuroscience
(particularly to Directed Information Theory and Neural Coding) are proposed as
further work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5224</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5224</id><created>2013-06-21</created><authors><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Prutkin</keyname><forenames>Roman</forenames></author></authors><title>Euclidean Greedy Drawings of Trees</title><categories>cs.CG</categories><comments>Expanded version of a paper to appear in the 21st European Symposium
  on Algorithms (ESA 2013). 24 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Greedy embedding (or drawing) is a simple and efficient strategy to route
messages in wireless sensor networks. For each source-destination pair of nodes
s, t in a greedy embedding there is always a neighbor u of s that is closer to
t according to some distance metric. The existence of greedy embeddings in the
Euclidean plane R^2 is known for certain graph classes such as 3-connected
planar graphs. We completely characterize the trees that admit a greedy
embedding in R^2. This answers a question by Angelini et al. (Graph Drawing
2009) and is a further step in characterizing the graphs that admit Euclidean
greedy embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5226</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5226</id><created>2013-06-21</created><updated>2014-12-23</updated><authors><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author><author><keyname>Khoo</keyname><forenames>Yuehaw</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Global registration of multiple point clouds using semidefinite
  programming</title><categories>cs.CV cs.NA math.NA math.OC</categories><comments>33 pages, 12 figures. To appear in SIAM Journal on Optimization</comments><msc-class>90C22, 52C25, 05C50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider $N$ points in $\mathbb{R}^d$ and $M$ local coordinate systems that
are related through unknown rigid transforms. For each point we are given
(possibly noisy) measurements of its local coordinates in some of the
coordinate systems. Alternatively, for each coordinate system, we observe the
coordinates of a subset of the points. The problem of estimating the global
coordinates of the $N$ points (up to a rigid transform) from such measurements
comes up in distributed approaches to molecular conformation and sensor network
localization, and also in computer vision and graphics.
  The least-squares formulation of this problem, though non-convex, has a well
known closed-form solution when $M=2$ (based on the singular value
decomposition). However, no closed form solution is known for $M\geq 3$.
  In this paper, we demonstrate how the least-squares formulation can be
relaxed into a convex program, namely a semidefinite program (SDP). By setting
up connections between the uniqueness of this SDP and results from rigidity
theory, we prove conditions for exact and stable recovery for the SDP
relaxation. In particular, we prove that the SDP relaxation can guarantee
recovery under more adversarial conditions compared to earlier proposed
spectral relaxations, and derive error bounds for the registration error
incurred by the SDP relaxation.
  We also present results of numerical experiments on simulated data to confirm
the theoretical findings. We empirically demonstrate that (a) unlike the
spectral relaxation, the relaxation gap is mostly zero for the semidefinite
program (i.e., we are able to solve the original non-convex least-squares
problem) up to a certain noise threshold, and (b) the semidefinite program
performs significantly better than spectral and manifold-optimization methods,
particularly at large noise levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5229</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5229</id><created>2013-06-20</created><authors><author><keyname>Tian</keyname><forenames>Shuang</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Shirvanimoghaddam</keyname><forenames>Mahyar</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>A Physical-layer Rateless Code for Wireless Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a physical-layer rateless code for wireless
channels. A novel rateless encoding scheme is developed to overcome the high
error floor problem caused by the low-density generator matrix (LDGM)-like
encoding scheme in conventional rateless codes. This is achieved by providing
each symbol with approximately equal protection in the encoding process. An
extrinsic information transfer (EXIT) chart based optimization approach is
proposed to obtain a robust check node degree distribution, which can achieve
near-capacity performances for a wide range of signal to noise ratios (SNR).
Simulation results show that, under the same channel conditions and
transmission overheads, the bit-error-rate (BER) performance of the proposed
scheme considerably outperforms the existing rateless codes in additive white
Gaussian noise (AWGN) channels, particularly at low BER regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5247</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5247</id><created>2013-06-21</created><authors><author><keyname>Kinnaird</keyname><forenames>Peter</forenames></author></authors><title>A Modest Proposal: (Possible) Implications for Appropriation</title><categories>cs.CY</categories><comments>4 pages</comments><acm-class>H.5.m</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  As technologies are developed and constructed, designers may or may not be
aware that they are embedding politics and values into their artifacts.
Computer scientists operate and advance their field by building layers of
abstraction into software and hardware to reduce the complexity of interfaces,
making the artifacts they create zuhanden for others in part by imposing
constraints. The increasing reliance of global populations and economies on
communication mediated by many information and communications technologies
(ICTs) transforms them from applications into communications infrastructure,
elevating the importance of considering the values embodied in those
infrastructures. I argue that the status quo bias and economic inertia of
built-infrastructure requires a reevaluation of research in light of the de
facto global technocracy to consider a nouveau social contract between
infrastructural theorists, scientists, designers, and engineers and the current
and future generations who will be constrained by that infrastructure as it is
reified. The CHI community is uniquely situated to establish a norm of
including a discussion of the implications of appropriation as first class
topic in research output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5263</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5263</id><created>2013-06-21</created><authors><author><keyname>Yu</keyname><forenames>Haonan</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>Discriminative Training: Learning to Describe Video with Sentences, from
  Video Described with Sentences</title><categories>cs.CV cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for learning word meanings from complex and realistic
video clips by discriminatively training (DT) positive sentential labels
against negative ones, and then use the trained word models to generate
sentential descriptions for new video. This new work is inspired by recent work
which adopts a maximum likelihood (ML) framework to address the same problem
using only positive sentential labels. The new method, like the ML-based one,
is able to automatically determine which words in the sentence correspond to
which concepts in the video (i.e., ground words to meanings) in a weakly
supervised fashion. While both DT and ML yield comparable results with
sufficient training data, DT outperforms ML significantly with smaller training
sets because it can exploit negative training labels to better constrain the
learning problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5264</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5264</id><created>2013-06-21</created><authors><author><keyname>Bjorner</keyname><forenames>Nikolaj</forenames></author><author><keyname>McMillan</keyname><forenames>Ken</forenames></author><author><keyname>Rybalchenko</keyname><forenames>Andrey</forenames></author></authors><title>Higher-order Program Verification as Satisfiability Modulo Theories with
  Algebraic Data-types</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on work in progress on automatic procedures for proving properties
of programs written in higher-order functional languages. Our approach encodes
higher-order programs directly as first-order SMT problems over Horn clauses.
It is straight-forward to reduce Hoare-style verification of first-order
programs into satisfiability of Horn clauses. The presence of closures offers
several challenges: relatively complete proof systems have to account for
closures; and in practice, the effectiveness of search procedures depend on
encoding strategies and capabilities of underlying solvers. We here use
algebraic data-types to encode closures and rely on solvers that support
algebraic data-types. The viability of the approach is examined using examples
from the literature on higher-order program verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5268</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5268</id><created>2013-06-21</created><authors><author><keyname>Staudt</keyname><forenames>Christian</forenames></author><author><keyname>Schumm</keyname><forenames>Andrea</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>G&#xf6;rke</keyname><forenames>Robert</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Static and Dynamic Aspects of Scientific Collaboration Networks</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>ASONAM 2012: IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Collaboration networks arise when we map the connections between scientists
which are formed through joint publications. These networks thus display the
social structure of academia, and also allow conclusions about the structure of
scientific knowledge. Using the computer science publication database DBLP, we
compile relations between authors and publications as graphs and proceed with
examining and quantifying collaborative relations with graph-based methods. We
review standard properties of the network and rank authors and publications by
centrality. Additionally, we detect communities with modularity-based
clustering and compare the resulting clusters to a ground-truth based on
conferences and thus topical similarity. In a second part, we are the first to
combine DBLP network data with data from the Dagstuhl Seminars: We investigate
whether seminars of this kind, as social and academic events designed to
connect researchers, leave a visible track in the structure of the
collaboration network. Our results suggest that such single events are not
influential enough to change the network structure significantly. However, the
network structure seems to influence a participant's decision to accept or
decline an invitation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5277</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5277</id><created>2013-06-21</created><authors><author><keyname>Li</keyname><forenames>Chengju</forenames></author><author><keyname>Yue</keyname><forenames>Qin</forenames></author></authors><title>Weight distribution of two classes of cyclic codes with respect to two
  distinct order elements</title><categories>cs.IT math.IT math.NT</categories><msc-class>94B15, 11T71, 11T24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are an interesting type of linear codes and have wide
applications in communication and storage systems due to their efficient
encoding and decoding algorithms. Cyclic codes have been studied for many
years, but their weight distribution are known only for a few cases. In this
paper, let $\Bbb F_r$ be an extension of a finite field $\Bbb F_q$ and $r=q^m$,
we determine the weight distribution of the cyclic codes $\mathcal C=\{c(a, b):
a, b \in \Bbb F_r\},$ $$c(a, b)=(\mbox {Tr}_{r/q}(ag_1^0+bg_2^0), \ldots, \mbox
{Tr}_{r/q}(ag_1^{n-1}+bg_2^{n-1})), g_1, g_2\in \Bbb F_r,$$ in the following
two cases: (1) $\ord(g_1)=n, n|r-1$ and $g_2=1$; (2) $\ord(g_1)=n$,
$g_2=g_1^2$, $\ord(g_2)=\frac n 2$, $m=2$ and $\frac{2(r-1)}n|(q+1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5279</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5279</id><created>2013-06-21</created><updated>2014-04-03</updated><authors><author><keyname>Hoey</keyname><forenames>Jesse</forenames></author><author><keyname>Schroeder</keyname><forenames>Tobias</forenames></author><author><keyname>Alhothali</keyname><forenames>Areej</forenames></author></authors><title>Affect Control Processes: Intelligent Affective Interaction using a
  Partially Observable Markov Decision Process</title><categories>cs.HC cs.AI</categories><acm-class>I.2; I.2.0; J.4</acm-class><doi>10.1016/j.artint.2015.09.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel method for building affectively intelligent
human-interactive agents. The method is based on a key sociological insight
that has been developed and extensively verified over the last twenty years,
but has yet to make an impact in artificial intelligence. The insight is that
resource bounded humans will, by default, act to maintain affective
consistency. Humans have culturally shared fundamental affective sentiments
about identities, behaviours, and objects, and they act so that the transient
affective sentiments created during interactions confirm the fundamental
sentiments. Humans seek and create situations that confirm or are consistent
with, and avoid and supress situations that disconfirm or are inconsistent
with, their culturally shared affective sentiments. This &quot;affect control
principle&quot; has been shown to be a powerful predictor of human behaviour. In
this paper, we present a probabilistic and decision-theoretic generalisation of
this principle, and we demonstrate how it can be leveraged to build affectively
intelligent artificial agents. The new model, called BayesAct, can maintain
multiple hypotheses about sentiments simultaneously as a probability
distribution, and can make use of an explicit utility function to make
value-directed action choices. This allows the model to generate affectively
intelligent interactions with people by learning about their identity,
predicting their behaviours using the affect control principle, and taking
actions that are simultaneously goal-directed and affect-sensitive. We
demonstrate this generalisation with a set of simulations. We then show how our
model can be used as an emotional &quot;plug-in&quot; for artificially intelligent
systems that interact with humans in two different settings: an exam practice
assistant (tutor) and an assistive device for persons with a cognitive
disability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5283</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5283</id><created>2013-06-21</created><authors><author><keyname>Kierstead</keyname><forenames>Hal</forenames></author><author><keyname>Lidick&#xfd;</keyname><forenames>Bernard</forenames></author></authors><title>On choosability with separation of planar graphs with lists of different
  sizes</title><categories>math.CO cs.DM</categories><comments>7 pages, 2 figures</comments><msc-class>05C10, 05C15</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A (k,d)-list assignment L of a graph G is a mapping that assigns to each
vertex v a list L(v) of at least k colors and for any adjacent pair xy, the
lists L(x) and L(y) share at most d colors. A graph G is (k,d)-choosable if
there exists an L-coloring of G for every (k,d)-list assignment L. This concept
is also known as choosability with separation.
  It is known that planar graphs are (4,1)-choosable but it is not known if
planar graphs are (3,1)-choosable. We strengthen the result that planar graphs
are (4,1)-choosable by allowing an independent set of vertices to have lists of
size 3 instead of 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5288</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5288</id><created>2013-06-22</created><updated>2014-03-27</updated><authors><author><keyname>Wang</keyname><forenames>Pinghui</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Zhao</keyname><forenames>Junzhou</forenames></author><author><keyname>Guan</keyname><forenames>Xiaohong</forenames></author></authors><title>Efficiently Estimating Motif Statistics of Large Networks</title><categories>cs.SI physics.soc-ph</categories><comments>TKDD 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploring statistics of locally connected subgraph patterns (also known as
network motifs) has helped researchers better understand the structure and
function of biological and online social networks (OSNs). Nowadays the massive
size of some critical networks -- often stored in already overloaded relational
databases -- effectively limits the rate at which nodes and edges can be
explored, making it a challenge to accurately discover subgraph statistics. In
this work, we propose sampling methods to accurately estimate subgraph
statistics from as few queried nodes as possible. We present sampling
algorithms that efficiently and accurately estimate subgraph properties of
massive networks. Our algorithms require no pre-computation or complete network
topology information. At the same time, we provide theoretical guarantees of
convergence. We perform experiments using widely known data sets, and show that
for the same accuracy, our algorithms require an order of magnitude less
queries (samples) than the current state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5291</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5291</id><created>2013-06-22</created><authors><author><keyname>Shariatpanahi</keyname><forenames>Seyed Pooya</forenames></author><author><keyname>Khalaj</keyname><forenames>Babak Hossein</forenames></author><author><keyname>Alishahi</keyname><forenames>Kasra</forenames></author><author><keyname>Shah-Mansouri</keyname><forenames>Hamed</forenames></author></authors><title>Throughput of Large One-hop Wireless Networks with General Fading</title><categories>cs.IT math.IT</categories><comments>27 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider $n$ source-destination pairs randomly located in a shared wireless
medium, resulting in interference between different transmissions. All wireless
links are modeled by independently and identically distributed (i.i.d.) random
variables, indicating that the dominant channel effect is the random fading
phenomenon. We characterize the throughput of one-hop communication in such
network. First, we present a closed-form expression for throughput scaling of a
heuristic strategy, for a completely general channel power distribution. This
heuristic strategy is based on activating the source-destination pairs with the
best direct links, and forcing the others to be silent. Then, we present the
results for several common examples, namely, Gamma (Nakagami-$m$ fading),
Weibull, Pareto, and Log-normal channel power distributions. Finally -- by
proposing an upper bound on throughput of all possible strategies for
super-exponential distributions -- we prove that the aforementioned heuristic
method is order-optimal for Nakagami-$m$ fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5293</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5293</id><created>2013-06-22</created><authors><author><keyname>Mastani</keyname><forenames>S. Aruna</forenames></author><author><keyname>Shilpa</keyname><forenames>K.</forenames></author></authors><title>New Approach of Estimating PSNR-B For De-blocked Images</title><categories>cs.CV</categories><comments>7 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Measurement of image quality is very crucial to many image processing
applications. Quality metrics are used to measure the quality of improvement in
the images after they are processed and compared with the original images.
Compression is one of the applications where it is required to monitor the
quality of decompressed or decoded image. JPEG compression is the lossy
compression which is most prevalent technique for image codecs. But it suffers
from blocking artifacts. Various deblocking filters are used to reduce blocking
artifacts. The efficiency of deblocking filters which improves visual signals
degraded by blocking artifacts from compression will also be studied. Objective
quality metrics like PSNR, SSIM, and PSNRB for analyzing the quality of
deblocked images will be studied. We introduce a new approach of PSNR-B for
analyzing quality of deblocked images. Simulation results show that new
approach of PSNR-B called modified PSNR-B. it gives even better results
compared to existing well known blockiness specific indices
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5296</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5296</id><created>2013-06-22</created><updated>2014-03-12</updated><authors><author><keyname>Banerji</keyname><forenames>Sourangsu</forenames></author></authors><title>Design and Implementation of an Unmanned Vehicle using a GSM Network
  without Microcontrollers</title><categories>cs.SY</categories><comments>8 pages, 6 figures, 6 tables</comments><journal-ref>Journal of Electrical Engineering, Volume 14, Issue 1, April 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent past, wireless controlled vehicles had been extensively used in
a lot of areas like unmanned rescue missions, military usage for unmanned
combat and many others. But the major disadvantage of these wireless unmanned
robots is that they typically make use of RF circuits for maneuver and control.
Essentially RF circuits suffer from a lot of drawbacks such as limited
frequency range i.e. working range, and limited control. To overcome such
problems associated with RF control, few papers have been written, describing
methods which make use of the GSM network and the DTMF function of a cell phone
to control the robotic vehicle. This paper although uses the same principle
technology of the GSM network and the DTMF based mobile phone but it
essentially shows the construction of a circuit using only 4 bits of wireless
data communication to control the motion of the vehicle without the use of any
microcontroller. This improvement results in considerable reduction of circuit
complexity and of manpower for software development as the circuit built using
this system does not require any form of programming. Moreover, practical
results obtained showed an appreciable degree of accuracy of the system and
friendliness without the use of any microcontroller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5299</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5299</id><created>2013-06-22</created><authors><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu R.</forenames></author></authors><title>Secret key generation from Gaussian sources using lattice hashing</title><categories>cs.IT math.IT</categories><comments>5 pages, Conference (ISIT 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple yet complete lattice-based scheme for secret key
generation from Gaussian sources in the presence of an eavesdropper, and show
that it achieves strong secret key rates up to 1/2 nat from the optimal in the
case of &quot;degraded&quot; source models. The novel ingredient of our scheme is a
lattice-hashing technique, based on the notions of flatness factor and channel
intrinsic randomness. The proposed scheme does not require dithering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5305</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5305</id><created>2013-06-22</created><authors><author><keyname>Fodor</keyname><forenames>Gabor</forenames></author><author><keyname>Belleschi</keyname><forenames>Marco</forenames></author><author><keyname>Penda</keyname><forenames>Demia D.</forenames></author><author><keyname>Pradini</keyname><forenames>Aidilla</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author><author><keyname>Abrardo</keyname><forenames>Andrea</forenames></author></authors><title>Benchmarking Practical RRM Algorithms for D2D Communications in LTE
  Advanced</title><categories>cs.IT cs.NI math.IT</categories><comments>30 pages, submitted for review April-2013. See also: G. Fodor, M.
  Johansson, D. P. Demia, B. Marco, and A. Abrardo, A joint power control and
  resource allocation algorithm for D2D communications, KTH, Automatic Control,
  Tech. Rep., 2012, qC 20120910,
  http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-102057</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communication integrated into cellular networks is a
means to take advantage of the proximity of devices and allow for reusing
cellular resources and thereby to increase the user bitrates and the system
capacity. However, when D2D (in the 3rd Generation Partnership Project also
called Long Term Evolution (LTE) Direct) communication in cellular spectrum is
supported, there is a need to revisit and modify the existing radio resource
management (RRM) and power control (PC) techniques to realize the potential of
the proximity and reuse gains and to limit the interference at the cellular
layer. In this paper, we examine the performance of the flexible LTE PC tool
box and benchmark it against a utility optimal iterative scheme. We find that
the open loop PC scheme of LTE performs well for cellular users both in terms
of the used transmit power levels and the achieved
signal-to-interference-and-noise-ratio (SINR) distribution. However, the
performance of the D2D users as well as the overall system throughput can be
boosted by the utility optimal scheme, because the utility maximizing scheme
takes better advantage of both the proximity and the reuse gains. Therefore, in
this paper we propose a hybrid PC scheme, in which cellular users employ the
open loop path compensation method of LTE, while D2D users use the utility
optimizing distributed PC scheme. In order to protect the cellular layer, the
hybrid scheme allows for limiting the interference caused by the D2D layer at
the cost of having a small impact on the performance of the D2D layer. To
ensure feasibility, we limit the number of iterations to a practically feasible
level. We make the point that the hybrid scheme is not only near optimal, but
it also allows for a distributed implementation for the D2D users, while
preserving the LTE PC scheme for the cellular users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5308</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5308</id><created>2013-06-22</created><authors><author><keyname>Bhatt</keyname><forenames>Mehul</forenames></author><author><keyname>Suchan</keyname><forenames>Jakob</forenames></author><author><keyname>Schultz</keyname><forenames>Carl</forenames></author></authors><title>Cognitive Interpretation of Everyday Activities: Toward Perceptual
  Narrative Based Visuo-Spatial Scene Interpretation</title><categories>cs.AI cs.CV cs.HC cs.RO</categories><comments>To appear at: Computational Models of Narrative (CMN) 2013., a
  satellite event of CogSci 2013: The 35th meeting of the Cognitive Science
  Society</comments><acm-class>I.2; I.2.0; I.2.4; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We position a narrative-centred computational model for high-level knowledge
representation and reasoning in the context of a range of assistive
technologies concerned with &quot;visuo-spatial perception and cognition&quot; tasks. Our
proposed narrative model encompasses aspects such as \emph{space, events,
actions, change, and interaction} from the viewpoint of commonsense reasoning
and learning in large-scale cognitive systems. The broad focus of this paper is
on the domain of &quot;human-activity interpretation&quot; in smart environments, ambient
intelligence etc. In the backdrop of a &quot;smart meeting cinematography&quot; domain,
we position the proposed narrative model, preliminary work on perceptual
narrativisation, and the immediate outlook on constructing general-purpose
open-source tools for perceptual narrativisation.
  ACM Classification: I.2 Artificial Intelligence: I.2.0 General -- Cognitive
Simulation, I.2.4 Knowledge Representation Formalisms and Methods, I.2.10
Vision and Scene Understanding: Architecture and control structures, Motion,
Perceptual reasoning, Shape, Video analysis
  General keywords: cognitive systems; human-computer interaction; spatial
cognition and computation; commonsense reasoning; spatial and temporal
reasoning; assistive technologies
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5323</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5323</id><created>2013-06-22</created><authors><author><keyname>Wang</keyname><forenames>Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Haonan</forenames></author><author><keyname>Scharf</keyname><forenames>Louis</forenames></author></authors><title>The Geometry of Fusion Inspired Channel Design</title><categories>cs.IT math.IT</categories><comments>Manuscript has been submitted to Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is motivated by the problem of integrating multiple sources of
measurements. We consider two multiple-input-multiple-output (MIMO) channels, a
primary channel and a secondary channel, with dependent input signals. The
primary channel carries the signal of interest, and the secondary channel
carries a signal that shares a joint distribution with the primary signal. The
problem of particular interest is designing the secondary channel matrix, when
the primary channel matrix is fixed. We formulate the problem as an
optimization problem, in which the optimal secondary channel matrix maximizes
an information-based criterion. An analytical solution is provided in a special
case. Two fast-to-compute algorithms, one extrinsic and the other intrinsic,
are proposed to approximate the optimal solutions in general cases. In
particular, the intrinsic algorithm exploits the geometry of the unit sphere, a
manifold embedded in Euclidean space. The performances of the proposed
algorithms are examined through a simulation study. A discussion of the choice
of dimension for the secondary channel is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5326</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5326</id><created>2013-06-22</created><authors><author><keyname>Micheli</keyname><forenames>Giacomo</forenames></author></authors><title>Cryptanalysis of a non-commutative key exchange protocol</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the papers by Alvarez et al. and Pathak and Sanghi a non-commutative based
public key exchange is described. A similiar version of it has also been
patented (US7184551). In this paper we present a polynomial time attack that
breaks the variants of the protocol presented in the two papers. Moreover we
show that breaking the patented cryptosystem US7184551 can be easily reduced to
factoring. We also give some examples to show how efficiently the attack works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5338</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5338</id><created>2013-06-22</created><authors><author><keyname>Summers</keyname><forenames>Tyler H.</forenames></author><author><keyname>Shames</keyname><forenames>Iman</forenames></author></authors><title>Active influence in dynamical models of structural balance in social
  networks</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 3 figures, to appear in Europhysics Letters
  (http://www.epletters.net)</comments><doi>10.1209/0295-5075/103/18001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a nonlinear dynamical system on a signed graph, which can be
interpreted as a mathematical model of social networks in which the links can
have both positive and negative connotations. In accordance with a concept from
social psychology called structural balance, the negative links play a key role
in both the structure and dynamics of the network. Recent research has shown
that in a nonlinear dynamical system modeling the time evolution of
&quot;friendliness levels&quot; in the network, two opposing factions emerge from almost
any initial condition. Here we study active external influence in this
dynamical model and show that any agent in the network can achieve any desired
structurally balanced state from any initial condition by perturbing its own
local friendliness levels. Based on this result, we also introduce a new
network centrality measure for signed networks. The results are illustrated in
an international relations network using United Nations voting record data from
1946 to 2008 to estimate friendliness levels amongst various countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5349</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5349</id><created>2013-06-22</created><authors><author><keyname>Stattner</keyname><forenames>Erick</forenames></author><author><keyname>Segretier</keyname><forenames>Wilfried</forenames></author><author><keyname>Collard</keyname><forenames>Martine</forenames></author><author><keyname>Hunel</keyname><forenames>Philippe</forenames></author><author><keyname>Vidot</keyname><forenames>Nicolas</forenames></author></authors><title>Song-based Classification techniques for Endangered Bird Conservation</title><categories>cs.LG</categories><comments>6 pages, 4 figures. In ICML 2013 Workshop on Machine Learning for
  Bioacoustics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work presented in this paper is part of a global framework which long
term goal is to design a wireless sensor network able to support the
observation of a population of endangered birds. We present the first stage for
which we have conducted a knowledge discovery approach on a sample of
acoustical data. We use MFCC features extracted from bird songs and we exploit
two knowledge discovery techniques. One that relies on clustering-based
approaches, that highlights the homogeneity in the songs of the species. The
other, based on predictive modeling, that demonstrates the good performances of
various machine learning techniques for the identification process. The
knowledge elicited provides promising results to consider a widespread study
and to elicit guidelines for designing a first version of the automatic
approach for data collection based on acoustic sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5350</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5350</id><created>2013-06-22</created><authors><author><keyname>Miller</keyname><forenames>Daniel L.</forenames></author></authors><title>Error Correction for NOR Memory Devices with Exponentially Distributed
  Read Noise</title><categories>cs.IT math.IT</categories><comments>4 pages, Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scaling of high density NOR Flash memory devices with multi level cell
(MLC) hits the reliability break wall because of relatively high intrinsic bit
error rate (IBER). The chip maker companies offer two solutions to meet the
output bit error rate (OBER) specification: either partial coverage with error
correction code (ECC) or data storage in single level cell (SLC) with
significant increase of the die cost. The NOR flash memory allows to write
information in small portions, therefore the full error protection becomes
costly due to high required redundancy, e.g. $\sim$50%. This is very different
from the NAND flash memory writing at once large chunks of information; NAND
ECC requires just $\sim$10% redundancy. This paper gives an analysis of a novel
error protection scheme applicable to NOR storage of one byte. The method does
not require any redundant cells, but assumes 5th program level. The information
is mapped to states in the 4-dimensional space separated by the minimal
Manhattan distance equal 2. This code preserves the information capacity: one
byte occupies four memory cells. We demonstrate the OBER $\sim$ IBER$^{3/2}$
scaling law, where IBER is calculated for the 4-level MLC memory. As an
example, the 4-level MLC with IBER $\sim10^{-9}$, which is unacceptable for
high density products, can be converted to OBER $\sim10^{-12}$. We assume that
the IBER is determined by the exponentially distributed read noise. This is the
case for NOR Flash memory devices, since the exponential tails are typical for
the random telegraph signal (RTS) noise and for most of the charge loss, charge
gain, and charge sharing data losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5358</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5358</id><created>2013-06-22</created><updated>2013-10-01</updated><authors><author><keyname>Frank</keyname><forenames>Rupert L.</forenames></author><author><keyname>Lieb</keyname><forenames>Elliott H.</forenames></author></authors><title>Monotonicity of a relative R\'enyi entropy</title><categories>math-ph cs.IT math.FA math.IT math.MP quant-ph</categories><comments>6 pages; minor revisions</comments><doi>10.1063/1.4838835</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a recent definition of relative R\'enyi entropy is monotone
under completely positive, trace preserving maps. This proves a recent
conjecture of M\&quot;uller-Lennert et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5362</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5362</id><created>2013-06-22</created><authors><author><keyname>Ma</keyname><forenames>Ping</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author></authors><title>A Statistical Perspective on Algorithmic Leveraging</title><categories>stat.ME cs.LG stat.ML</categories><comments>44 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One popular method for dealing with large-scale data sets is sampling. For
example, by using the empirical statistical leverage scores as an importance
sampling distribution, the method of algorithmic leveraging samples and
rescales rows/columns of data matrices to reduce the data size before
performing computations on the subproblem. This method has been successful in
improving computational efficiency of algorithms for matrix problems such as
least-squares approximation, least absolute deviations approximation, and
low-rank matrix approximation. Existing work has focused on algorithmic issues
such as worst-case running times and numerical issues associated with providing
high-quality implementations, but none of it addresses statistical aspects of
this method.
  In this paper, we provide a simple yet effective framework to evaluate the
statistical properties of algorithmic leveraging in the context of estimating
parameters in a linear regression model with a fixed number of predictors. We
show that from the statistical perspective of bias and variance, neither
leverage-based sampling nor uniform sampling dominates the other. This result
is particularly striking, given the well-known result that, from the
algorithmic perspective of worst-case analysis, leverage-based sampling
provides uniformly superior worst-case algorithmic results, when compared with
uniform sampling. Based on these theoretical results, we propose and analyze
two new leveraging algorithms. A detailed empirical evaluation of existing
leverage-based methods as well as these two new methods is carried out on both
synthetic and real data sets. The empirical results indicate that our theory is
a good predictor of practical performance of existing and new leverage-based
algorithms and that the new algorithms achieve improved performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5365</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5365</id><created>2013-06-22</created><authors><author><keyname>Takahashi</keyname><forenames>Hirotaka</forenames></author><author><keyname>Oohara</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Kaneyama</keyname><forenames>Masato</forenames></author><author><keyname>Hiranuma</keyname><forenames>Yuta</forenames></author><author><keyname>Camp</keyname><forenames>Jordan B</forenames></author></authors><title>On Investigating EMD Parameters to Search for Gravitational Waves</title><categories>gr-qc cs.CE math.NA</categories><comments>20 pages, 5 figures</comments><journal-ref>Advances in Adaptive Data Analysis (AADA) Vol. 5 No. 2, 1350010
  (2013)</journal-ref><doi>10.1142/S1793536913500106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hilbert-Huang transform (HHT) is a novel, adaptive approach to time
series analysis. It does not impose a basis set on the data or otherwise make
assumptions about the data form, and so the time--frequency decomposition is
not limited by spreading due to uncertainty. Because of the high resolution of
the time--frequency, we investigate the possibility of the application of the
HHT to the search for gravitational waves. It is necessary to determine some
parameters in the empirical mode decomposition (EMD), which is a component of
the HHT, and in this paper we propose and demonstrate a method to determine the
optimal values of the parameters to use in the search for gravitational waves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5377</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5377</id><created>2013-06-23</created><updated>2013-07-05</updated><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Lin</keyname><forenames>Liren</forenames></author></authors><title>Thresholds of Random Quasi-Abelian Codes</title><categories>cs.IT math.IT</categories><comments>20 pages</comments><msc-class>15B52, 68Q87, 94B05, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a random quasi-abelian code of rate $r$, it is shown that the GV-bound is
a threshold point: if $r$ is less than the GV-bound at $\delta$, then the
probability of the relative distance of the random code being greater than
$\delta$ is almost 1; whereas, if $r$ is bigger than the GV-bound at $\delta$,
then the probability is almost 0. As a consequence, there exist many
asymptotically good quasi-abelian codes with any parameters attaining the
GV-bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5381</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5381</id><created>2013-06-23</created><authors><author><keyname>Nainan</keyname><forenames>Sumita</forenames></author><author><keyname>Parekh</keyname><forenames>Romin</forenames></author><author><keyname>Shah</keyname><forenames>Tanvi</forenames></author></authors><title>RFID Technology Based Attendance Management System</title><categories>cs.ET</categories><comments>6 pages, 8 figures, 2 tables</comments><msc-class>68U04</msc-class><acm-class>B.1.1; B.2.3; B.4.1; B.4.2; C.5.3; D.3.2; D.4.4; E.1; H.4.1</acm-class><journal-ref>International Journal of Computer Science Issues bearing paper ID
  'IJCSI-2013-10-1-4801' was published in IJCSI Volume 10, Issue 1, January
  2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  RFID is a nascent technology, deeply rooted by its early developments in
using radar1 as a harbinger of adversary planes during World War II. A plethora
of industries have leveraged the benefits of RFID technology for enhancements
in sectors like military, sports, security, airline, animal farms, healthcare
and other areas. Industry specific key applications of this technology include
vehicle tracking, automated inventory management, animal monitoring, secure
store checkouts, supply chain management, automatic payment, sport timing
technologies, etc. This paper introduces the distinctive components of RFID
technology and focuses on its core competencies: scalability and security. It
will be then supplemented by a detailed synopsis of an investigation conducted
to test the feasibility and practicality of RFID technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5383</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5383</id><created>2013-06-23</created><authors><author><keyname>Lungu</keyname><forenames>Eliza-Olivia</forenames></author><author><keyname>Zamfir</keyname><forenames>Ana-Maria</forenames></author><author><keyname>Mocanu</keyname><forenames>Cristina</forenames></author></authors><title>Patterns in the occupational mobility network of the higher education
  graduates. Comparative study in 12 EU countries</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 3 figures</comments><journal-ref>Proceedings of the European Conference on Complex Systems 2012,
  Springer-Verlag New York, LLC, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article investigates the properties of the occupational mobility network
(OMN) in 12 EU countries. Using REFLEX database we construct for each country
an empirical OMN that reflects the job movements of the university graduates,
during the first five years after graduation (1999 - 2005). The nodes are
represented by the occupations coded at 3 digits according to ISCO-88 and the
links are weighted with the number of graduates switching from one occupation
to another. We construct the networks as weighted and directed. This
comparative study allows us to see what are the common patterns in the OMN over
different EU labor markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5390</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5390</id><created>2013-06-23</created><updated>2013-06-28</updated><authors><author><keyname>Agarwal</keyname><forenames>Tejaswi</forenames></author><author><keyname>Jha</keyname><forenames>Saurabh</forenames></author><author><keyname>Kanna</keyname><forenames>B. Rajesh</forenames></author></authors><title>P-HGRMS: A Parallel Hypergraph Based Root Mean Square Algorithm for
  Image Denoising</title><categories>cs.DC cs.CV</categories><comments>2 pages, 2 figures. Published as poster at the 22nd ACM International
  Symposium on High Performance Parallel and Distributed Systems, HPDC 2013,
  New York, USA. Won the Best Poster Award at HPDC 2013</comments><acm-class>I.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a parallel Salt and Pepper (SP) noise removal algorithm
in a grey level digital image based on the Hypergraph Based Root Mean Square
(HGRMS) approach. HGRMS is generic algorithm for identifying noisy pixels in
any digital image using a two level hierarchical serial approach. However, for
SP noise removal, we reduce this algorithm to a parallel model by introducing a
cardinality matrix and an iteration factor, k, which helps us reduce the
dependencies in the existing approach. We also observe that the performance of
the serial implementation is better on smaller images, but once the threshold
is achieved in terms of image resolution, its computational complexity
increases drastically. We test P-HGRMS using standard images from the Berkeley
Segmentation dataset on NVIDIAs Compute Unified Device Architecture (CUDA) for
noise identification and attenuation. We also compare the noise removal
efficiency of the proposed algorithm using Peak Signal to Noise Ratio (PSNR) to
the existing approach. P-HGRMS maintains the noise removal efficiency and
outperforms its sequential counterpart by 6 to 18 times (6x - 18x) in
computational efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5391</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5391</id><created>2013-06-23</created><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Harutyunyan</keyname><forenames>Anna</forenames></author></authors><title>Boundary-to-boundary flows in planar graphs</title><categories>cs.DS</categories><comments>In Proc. IWOCA, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an iterative algorithm for finding the maximum flow between a set of
sources and sinks that lie on the boundary of a planar graph. Our algorithm
uses only O(n) queries to simple data structures, achieving an O(n log n)
running time that we expect to be practical given the use of simple primitives.
The only existing algorithm for this problem uses divide and conquer and, in
order to achieve an O(n log n) running time, requires the use of the
(complicated) linear-time shortest-paths algorithm for planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5406</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5406</id><created>2013-06-23</created><authors><author><keyname>Pereszl&#xe9;nyi</keyname><forenames>Attila</forenames></author></authors><title>One-Sided Error QMA with Shared EPR Pairs -- A Simpler Proof</title><categories>quant-ph cs.CC</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a simpler proof of one of the results of Kobayashi, Le Gall, and
Nishimura [arXiv:1210.1290v2], which shows that any QMA protocol can be
converted to a one-sided error protocol, in which Arthur and Merlin initially
share a constant number of EPR pairs and then Merlin sends his proof to Arthur.
Our protocol is similar but somewhat simpler than the original. Our main
contribution is a simpler and more direct analysis of the soundness property
that uses well-known results in quantum information such as properties of the
trace distance and the fidelity, and the quantum de Finetti theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5412</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5412</id><created>2013-06-23</created><authors><author><keyname>Singh</keyname><forenames>Sajai Vir</forenames></author><author><keyname>Gupta</keyname><forenames>Gungan</forenames></author><author><keyname>Chhabra</keyname><forenames>Rahul</forenames></author><author><keyname>Nagpal</keyname><forenames>Kanika</forenames></author><author><keyname>Devansh</keyname></author></authors><title>Electronically Tunable Voltage-Mode Biquad Filter/Oscillator Based On
  CCCCTAs</title><categories>cs.SY</categories><comments>5 pages, 7 figures, 1 table, Authors profile</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a circuit employing current controlled current conveyor
trans-conductance amplifiers (CCCCTAs) as active element is proposed which can
function both as biquad filter and oscillator. It uses two CCCCTAs and two
capacitors. As a biquad filter it can realizes all the standard filtering
functions (low pass, band pass, high pass, band reject and all pass) in
voltage-mode and provides the feature of electronically and orthogonal control
of pole frequency and quality factor through biasing current(s) of CCCCTAs. The
proposed circuit can also be worked as oscillator without changing the circuit
topology. Without any resistors and using capacitors, the proposed circuit is
suitable for IC fabrication. The validity of proposed filter is verified
through PSPICE simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5424</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5424</id><created>2013-06-23</created><updated>2013-06-25</updated><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Moritz</forenames></author></authors><title>The Fine Classification of Conjunctive Queries and Parameterized
  Logarithmic Space Complexity</title><categories>cs.CC cs.DB cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform a fundamental investigation of the complexity of conjunctive query
evaluation from the perspective of parameterized complexity. We classify sets
of boolean conjunctive queries according to the complexity of this problem.
Previous work showed that a set of conjunctive queries is fixed-parameter
tractable precisely when the set is equivalent to a set of queries having
bounded treewidth. We present a fine classification of query sets up to
parameterized logarithmic space reduction. We show that, in the bounded
treewidth regime, there are three complexity degrees and that the properties
that determine the degree of a query set are bounded pathwidth and bounded tree
depth. We also engage in a study of the two higher degrees via logarithmic
space machine characterizations and complete problems. Our work yields a
significantly richer perspective on the complexity of conjunctive queries and,
at the same time, suggests new avenues of research in parameterized complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5434</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5434</id><created>2013-06-23</created><updated>2014-07-18</updated><authors><author><keyname>Mendel</keyname><forenames>Manor</forenames></author><author><keyname>Naor</keyname><forenames>Assaf</forenames></author></authors><title>Expanders with respect to Hadamard spaces and random graphs</title><categories>math.MG cs.DS math.CO math.FA</categories><comments>incorporated Referees' comments</comments><journal-ref>Duke Math. J. 164, no. 8 (2015), 1471-1548</journal-ref><doi>10.1215/00127094-3119525</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that there exists a sequence of 3-regular graphs
$\{G_n\}_{n=1}^\infty$ and a Hadamard space $X$ such that
$\{G_n\}_{n=1}^\infty$ forms an expander sequence with respect to $X$, yet
random regular graphs are not expanders with respect to $X$. This answers a
question of \cite{NS11}. $\{G_n\}_{n=1}^\infty$ are also shown to be expanders
with respect to random regular graphs, yielding a deterministic sublinear time
constant factor approximation algorithm for computing the average squared
distance in subsets of a random graph. The proof uses the Euclidean cone over a
random graph, an auxiliary continuous geometric object that allows for the
implementation of martingale methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5441</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5441</id><created>2013-06-23</created><authors><author><keyname>Cai</keyname><forenames>Kai</forenames></author><author><keyname>Wonham</keyname><forenames>W. M.</forenames></author></authors><title>Supervisor Localization of Discrete-Event Systems based on State Tree
  Structures</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently we developed supervisor localization, a top-down approach to
distributed control of discrete-event systems in the Ramadge-Wonham supervisory
control framework. Its essence is the decomposition of monolithic (global)
control action into local control strategies for the individual agents. In this
paper, we establish a counterpart supervisor localization theory in the
framework of State Tree Structures, known to be efficient for control design of
very large systems. In the new framework, we introduce the new concepts of
local state tracker, local control function, and state-based local-global
control equivalence. As before, we prove that the collective localized control
behavior is identical to the monolithic optimal (i.e. maximally permissive) and
nonblocking controlled behavior. In addition, we propose a new and more
efficient localization algorithm which exploits BDD computation. Finally we
demonstrate our localization approach on a model for a complex semiconductor
manufacturing system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5445</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5445</id><created>2013-06-23</created><authors><author><keyname>Poss</keyname><forenames>Raphael 'kena'</forenames></author></authors><title>Extrinsically adaptable systems</title><categories>cs.CY</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Are there qualitative and quantitative traits of system design that
contribute to the ability of people to further innovate? We propose that
extrinsic adaptability, the ability given to secondary parties to change a
system to match new requirements not envisioned by the primary provider, is
such a trait. &quot;Extrinsic adaptation&quot; encompasses the popular concepts of
&quot;workaround&quot;, &quot;fast prototype extension&quot; or &quot;hack&quot;, and extrinsic adaptability
is thus a measure of how friendly a system is to tinkering by curious minds. In
this report, we give &quot;hackability&quot; or &quot;hacker-friendliness&quot; scientific
credentials by formulating and studying a generalization of the concept. During
this exercise, we find that system changes by secondary parties fall on a
subjective gradient of acceptability, with extrinsic adaptations on one side
which confidently preserve existing system features, and invasive modifications
on the other side which are perceived to be disruptive to existing system
features. Where a change is positioned on this gradient is dependent on how an
external observer perceives component boundaries within the changed system. We
also find that the existence of objective cost functions can alleviate but not
fully eliminate this subjectiveness. The study also enables us to formulate an
ethical imperative for system designers to promote extrinsic adaptability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5460</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5460</id><created>2013-06-23</created><authors><author><keyname>Alamdari</keyname><forenames>Soroush</forenames></author><author><keyname>Chan</keyname><forenames>Timothy M.</forenames></author><author><keyname>Grant</keyname><forenames>Elyot</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Pathak</keyname><forenames>Vinayak</forenames></author></authors><title>Self-Approaching Graphs</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce self-approaching graph drawings. A straight-line
drawing of a graph is self-approaching if, for any origin vertex s and any
destination vertex t, there is an st-path in the graph such that, for any point
q on the path, as a point p moves continuously along the path from the origin
to q, the Euclidean distance from p to q is always decreasing. This is a more
stringent condition than a greedy drawing (where only the distance between
vertices on the path and the destination vertex must decrease), and guarantees
that the drawing is a 5.33-spanner. We study three topics: (1) recognizing
self-approaching drawings; (2) constructing self-approaching drawings of a
given graph; (3) constructing a self-approaching Steiner network connecting a
given set of points. We show that: (1) there are e?fficient algorithms to test
if a polygonal path is self-approaching in R^2 and R^3, but it is NP-hard to
test if a given graph drawing in R^3 has a self-approaching uv-path; (2) we can
characterize the trees that have self-approaching drawings; (3) for any given
set of terminal points in the plane, we can ?find a linear sized network that
has a self-approaching path between any ordered pair of terminals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5473</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5473</id><created>2013-06-23</created><authors><author><keyname>Conover</keyname><forenames>Michael D.</forenames></author><author><keyname>Davis</keyname><forenames>Clayton</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>McKelvey</keyname><forenames>Karissa</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>The Geospatial Characteristics of a Social Movement Communication
  Network</title><categories>cs.CY cs.SI physics.data-an physics.soc-ph</categories><comments>Open access available at:
  http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0064679</comments><journal-ref>PLoS ONE 8(3):e55957 2013</journal-ref><doi>10.1371/journal.pone.0055957</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social movements rely in large measure on networked communication
technologies to organize and disseminate information relating to the movements'
objectives. In this work we seek to understand how the goals and needs of a
protest movement are reflected in the geographic patterns of its communication
network, and how these patterns differ from those of stable political
communication. To this end, we examine an online communication network
reconstructed from over 600,000 tweets from a thirty-six week period covering
the birth and maturation of the American anticapitalist movement, Occupy Wall
Street. We find that, compared to a network of stable domestic political
communication, the Occupy Wall Street network exhibits higher levels of
locality and a hub and spoke structure, in which the majority of non-local
attention is allocated to high-profile locations such as New York, California,
and Washington D.C. Moreover, we observe that information flows across state
boundaries are more likely to contain framing language and references to the
media, while communication among individuals in the same state is more likely
to reference protest action and specific places and and times. Tying these
results to social movement theory, we propose that these features reflect the
movement's efforts to mobilize resources at the local level and to develop
narrative frames that reinforce collective purpose at the national level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5474</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5474</id><created>2013-06-23</created><authors><author><keyname>Conover</keyname><forenames>Michael D.</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>The Digital Evolution of Occupy Wall Street</title><categories>cs.CY cs.SI physics.data-an physics.soc-ph</categories><comments>Open access available at:
  http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0064679</comments><journal-ref>PLoS ONE 8(5):e64679 2013</journal-ref><doi>10.1371/journal.pone.0064679</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the temporal evolution of digital communication activity relating
to the American anti-capitalist movement Occupy Wall Street. Using a
high-volume sample from the microblogging site Twitter, we investigate changes
in Occupy participant engagement, interests, and social connectivity over a
fifteen month period starting three months prior to the movement's first
protest action. The results of this analysis indicate that, on Twitter, the
Occupy movement tended to elicit participation from a set of highly
interconnected users with pre-existing interests in domestic politics and
foreign social movements. These users, while highly vocal in the months
immediately following the birth of the movement, appear to have lost interest
in Occupy related communication over the remainder of the study period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5480</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5480</id><created>2013-06-23</created><authors><author><keyname>Kunsberg</keyname><forenames>Benjamin</forenames></author><author><keyname>Zucker</keyname><forenames>Steven W.</forenames></author></authors><title>Characterizing Ambiguity in Light Source Invariant Shape from Shading</title><categories>cs.CV q-bio.NC</categories><comments>34 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shape from shading is a classical inverse problem in computer vision. This
shape reconstruction problem is inherently ill-defined; it depends on the
assumed light source direction. We introduce a novel mathematical formulation
for calculating local surface shape based on covariant derivatives of the
shading flow field, rather than the customary integral minimization or P.D.E
approaches. On smooth surfaces, we show second derivatives of brightness are
independent of the light sources and can be directly related to surface
properties. We use these measurements to define the matching local family of
surfaces that can result from any given shading patch, changing the emphasis to
characterizing ambiguity in the problem. We give an example of how these local
surface ambiguities collapse along certain image contours and how this can be
used for the reconstruction problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5487</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5487</id><created>2013-06-23</created><authors><author><keyname>Maguedong-Djoumessi</keyname><forenames>Celestine-Periale</forenames></author></authors><title>Model Reframing by Feature Context Change</title><categories>cs.LG</categories><comments>MSc Thesis, 126 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The feature space (including both input and output variables) characterises a
data mining problem. In predictive (supervised) problems, the quality and
availability of features determines the predictability of the dependent
variable, and the performance of data mining models in terms of
misclassification or regression error. Good features, however, are usually
difficult to obtain. It is usual that many instances come with missing values,
either because the actual value for a given attribute was not available or
because it was too expensive. This is usually interpreted as a utility or
cost-sensitive learning dilemma, in this case between misclassification (or
regression error) costs and attribute tests costs. Both misclassification cost
(MC) and test cost (TC) can be integrated into a single measure, known as joint
cost (JC). We introduce methods and plots (such as the so-called JROC plots)
that can work with any of-the-shelf predictive technique, including ensembles,
such that we re-frame the model to use the appropriate subset of attributes
(the feature configuration) during deployment time. In other words, models are
trained with the available attributes (once and for all) and then deployed by
setting missing values on the attributes that are deemed ineffective for
reducing the joint cost. As the number of feature configuration combinations
grows exponentially with the number of features we introduce quadratic methods
that are able to approximate the optimal configuration and model choices, as
shown by the experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5501</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5501</id><created>2013-06-23</created><authors><author><keyname>Li</keyname><forenames>Hu</forenames></author><author><keyname>Liu</keyname><forenames>Yuan`an</forenames></author><author><keyname>Yuan</keyname><forenames>Dongming</forenames></author><author><keyname>Hu</keyname><forenames>Hefei</forenames></author></authors><title>A Wrapper of PCI Express with FIFO Interfaces based on FPGA</title><categories>cs.AR</categories><comments>5 pages, 8 figures</comments><journal-ref>Proceedings of the 2012 International Conference on Industrial
  Control and Electronics Engineering, ICICEE 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a PCI Express (PCIE) Wrapper core named PWrapper with
FIFO interfaces. Compared with other PCIE solutions, PWrapper has several
advantages such as flexibility, isolation of clock domain, etc. PWrapper is
implemented and verified on Vertex -5-FX70T which is a development board
provided by Xilinx Inc. Architecture of PWrapper and design of two key modules
are illustrated, which timing optimization methods have been adopted. Then we
explained the advantages and challenges of on-chip interfaces technology based
on FIFOs. The verification results show that PWrapper can achieve the speed of
1.8Gbps (Giga bits per second).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5507</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5507</id><created>2013-06-24</created><authors><author><keyname>Dutta</keyname><forenames>Pallab</forenames></author></authors><title>Java Card for PayTv Application</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart cards are widely used along with PayTV receivers to store secret user
keys and to perform security functions to prevent any unauthorized viewing of
PayTV channels. Java Card technology enables programs written in the Java
programming language to run on smart cards. Smart cards represent one of the
smallest computing platforms in use today. The memory configuration of a smart
card are of the order of 4K of RAM, 72K of EEPROM, and 24K of ROM. Using Java
card provides advantages to the industry in terms of ease of coding, faster
time to market and faster upgrades as compared to plain smart cards . Also
different applications like payTV, e-commerce, health-card can easily be
implemented in a single java card as multiple applets corresponding to each
application can coexists in a single java card. But there are security concerns
in java cards and also the performance issues. In this paper, we analyse the
suitability of using Java card for PayTV applications as part of conditional
access system in place of plain smart cards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5511</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5511</id><created>2013-06-24</created><updated>2013-09-30</updated><authors><author><keyname>Lucas</keyname><forenames>Andrew</forenames></author></authors><title>Binary decision making with very heterogeneous influence</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>19 pages, 7 figures; v2: published version</comments><journal-ref>Journal of Statistical Mechanics 1309 P09024 (2013)</journal-ref><doi>10.1088/1742-5468/2013/09/P09024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an extension of a binary decision model in which nodes make
decisions based on influence-biased averages of their neighbors' states,
similar to Ising spin glasses with on-site random fields. In the limit where
these influences become very heavy-tailed, the behavior of the model
dramatically changes. On complete graphs, or graphs where nodes with large
influence have large degree, this model is characterized by a new &quot;phase&quot; with
an unpredictable number of macroscopic shocks, with no associated critical
phenomena. On random graphs where the degree of the most influential nodes is
small compared to population size, a predictable glassy phase without phase
transitions emerges. Analytic results about both of these new phases are
obtainable in limiting cases. We use numerical simulations to explore the model
for more general scenarios. The phases associated with very influential
decision makers are easily distinguishable experimentally from a homogeneous
influence phase in many circumstances, in the context of our simple model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5513</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5513</id><created>2013-06-24</created><updated>2014-09-03</updated><authors><author><keyname>Fang</keyname><forenames>Z.</forenames></author><author><keyname>Wang</keyname><forenames>X.</forenames></author><author><keyname>Yuan</keyname><forenames>X.</forenames></author></authors><title>Power Minimization in Multi-pair Two-Way Relaying</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors as this article provides
  some proofs for the results in the paper arXiv:1307.0052</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This doc provides some proofs in our submitted journal paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5526</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5526</id><created>2013-06-24</created><authors><author><keyname>Ivan</keyname><forenames>Mihai</forenames></author><author><keyname>Ivan</keyname><forenames>Gheorghe</forenames></author></authors><title>Programs in C++ for matrix computations in min plus algebra</title><categories>math.RA cs.MS</categories><comments>73 pages, no figures</comments><msc-class>15A80, 68-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this paper is to propose six programs in C++ for matrix
computations and solving recurrent equations systems with entries in min plus
algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5527</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5527</id><created>2013-06-24</created><authors><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author><author><keyname>Buchin</keyname><forenames>Maike</forenames></author><author><keyname>van Leusden</keyname><forenames>Rolf</forenames></author><author><keyname>Meulemans</keyname><forenames>Wouter</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author></authors><title>Computing the Fr\'echet Distance with a Retractable Leash</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All known algorithms for the Fr\'echet distance between curves proceed in two
steps: first, they construct an efficient oracle for the decision version; then
they use this oracle to find the optimum among a finite set of critical values.
We present a novel approach that avoids the detour through the decision
version. We demonstrate its strength by presenting a quadratic time algorithm
for the Fr\'echet distance between polygonal curves in R^d under polyhedral
distance functions, including L_1 and L_infty. We also get a
(1+epsilon)-approximation of the Fr\'echet distance under the Euclidean metric.
For the exact Euclidean case, our framework currently gives an algorithm with
running time O(n^2 log^2 n). However, we conjecture that it may eventually lead
to a faster exact algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5530</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5530</id><created>2013-06-24</created><updated>2013-12-02</updated><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>A Formal Model of QoS-Aware Web Service Orchestration Engine</title><categories>cs.SE cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  QoS-aware applications can satisfy not only the functional requirements of
the customers, but also the QoS requirements. QoS-aware Web Service
orchestration translates the QoS requirements of the customers into those of
its component Web Services. In a system viewpoint, we discuss issues on
QoS-aware Web Service orchestration and design a typical QoS-aware Web Service
orchestration engine called QoS-WSOE. More importantly, we establish a formal
model of QoS-WSOE based on actor systems theory. Within the formal model, we
use a three-layered pyramidal structure to capture the requirements of the
customers with a concept named QoS-Aware WSO Service, characteristics of
QoS-WSOE with a concept named QoS-Aware WSO System, and structures and
behaviors of QoS-WSOE with a concept named QoS-Aware WSO Behavior. Conclusions
showing that a system with QoS-Aware WSO Behavior is a QoS-Aware WSO System and
further can provide QoS-Aware WSO Service are drawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5532</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5532</id><created>2013-06-24</created><updated>2015-06-25</updated><authors><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Waldspurger</keyname><forenames>Ir&#xe8;ne</forenames></author></authors><title>Deep Learning by Scattering</title><categories>cs.LG stat.ML</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce general scattering transforms as mathematical models of deep
neural networks with l2 pooling. Scattering networks iteratively apply complex
valued unitary operators, and the pooling is performed by a complex modulus. An
expected scattering defines a contractive representation of a high-dimensional
probability distribution, which preserves its mean-square norm. We show that
unsupervised learning can be casted as an optimization of the space contraction
to preserve the volume occupied by unlabeled examples, at each layer of the
network. Supervised learning and classification are performed with an averaged
scattering, which provides scattering estimations for multiple classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5533</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5533</id><created>2013-06-24</created><authors><author><keyname>Bull</keyname><forenames>Larry</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Evolving Gene Regulatory Networks with Mobile DNA Mechanisms</title><categories>cs.CE nlin.AO q-bio.MN</categories><comments>7 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:1303.7220</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses a recently presented abstract, tuneable Boolean regulatory
network model extended to consider aspects of mobile DNA, such as transposons.
The significant role of mobile DNA in the evolution of natural systems is
becoming increasingly clear. This paper shows how dynamically controlling
network node connectivity and function via transposon-inspired mechanisms can
be selected for in computational intelligence tasks to give improved
performance. The designs of dynamical networks intended for implementation
within the slime mould Physarum polycephalum and for the distributed control of
a smart surface are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5535</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5535</id><created>2013-06-24</created><updated>2013-12-02</updated><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>A Survey on Formal Methods for Web Service Composition</title><categories>cs.SE cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Service Composition creates new composite Web Services from existing Web
Services which embodies the added values of Web Service technology and is a key
technology to solve cross-organizational business process integrations. We do a
survey on formal methods for Web Service Composition in the following way.
Through analyses of Web Service Composition, we establish a reference model
called RM-WSComposition to capture elements of Web Service Composition. Based
on the RM-WSComposition, issues on formalization for Web Service Composition
are pointed out and state-of-the-art on formal methods for Web Service
Composition is introduced. Finally, we point out the trends on this topic. For
convenience, we use an example called BuyingBooks to illustrate the concepts
and mechanisms in Web Service Composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5538</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5538</id><created>2013-06-24</created><authors><author><keyname>Zhu</keyname><forenames>Yu-Xiao</forenames></author><author><keyname>Zhang</keyname><forenames>Xiao-Guang</forenames></author><author><keyname>Sun</keyname><forenames>Gui-Quan</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author></authors><title>Influence of Reciprocal links in Social Networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1371/journal.pone.0103007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this Letter, we empirically study the influence of reciprocal links, in
order to understand its role in affecting the structure and function of
directed social networks. Experimental results on two representative datesets,
Sina Weibo and Douban, demonstrate that the reciprocal links indeed play a more
important role than non-reciprocal ones in both spreading information and
maintaining the network robustness. In particular, the information spreading
process can be significantly enhanced by considering the reciprocal effect. In
addition, reciprocal links are largely responsible for the connectivity and
efficiency of directed networks. This work may shed some light on the in-depth
understanding and application of the reciprocal effect in directed online
social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5539</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5539</id><created>2013-06-24</created><updated>2013-09-30</updated><authors><author><keyname>Suda</keyname><forenames>Martin</forenames></author></authors><title>Variable and clause elimination for LTL satisfiability checking</title><categories>cs.LO</categories><comments>20 pages (5 page appendix)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study preprocessing techniques for clause normal forms of LTL formulas.
Applying the mechanism of labelled clauses enables us to reinterpret LTL
satisfiability as a set of purely propositional problems and thus to transfer
simplification ideas from SAT to LTL. We demonstrate this by adapting variable
and clause elimination, a very effective preprocessing technique used by modern
SAT solvers. Our experiments confirm that even in the temporal setting
substantial reductions in formula size and subsequent decrease of solver
runtime can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5547</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5547</id><created>2013-06-24</created><authors><author><keyname>Lee</keyname><forenames>Chae Chang</forenames></author><author><keyname>yoon</keyname><forenames>Ji Won</forenames></author></authors><title>A data mining approach using transaction patterns for card fraud
  detection</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Credit and debit cards, rather than actual money, have become the universal
payment means. With these cards, it has become possible to buy expensive items
easily without an additional complex authentication procedure being conducted.
However, card transaction features are targeted by criminals seeking to use a
lost or stolen card and looking for a chance to replicate it. Accidents,
whether caused by the negligence of users or not, that lead to a transaction
being performed by a criminal rather than the authorized card user should be
prevented. Therefore, card companies are providing their clients with a variety
of policies and standards to cover this eventuality. Card companies must
therefore be able to distinguish between the rightful user and illegal users
according to these standards in order to minimize damage resulting from
unauthorized transactions.
  However, there is a limit to applying the same fixed standards to all card
users, since the transaction patterns of people differ and even individuals'
transaction patterns may change frequently due to changes income and
consumption preference. Therefore, when only a specific threshold is applied,
it is difficult to distinguish a fraudulent card transaction from a legitimate
one.
  In this paper, we present methods for learning the individual patterns of a
card user's transaction amount and the region in which he or she uses the card,
for a given period, and for determining whether the specified transaction is
allowable in accordance with these learned user transaction patterns. Then, we
classify legitimate transactions and fraudulent transactions by setting
thresholds based on the learned individual patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5550</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5550</id><created>2013-06-24</created><updated>2013-08-23</updated><authors><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Neeman</keyname><forenames>Joe</forenames></author><author><keyname>Sly</keyname><forenames>Allan</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author><author><keyname>Zhang</keyname><forenames>Pan</forenames></author></authors><title>Spectral redemption: clustering sparse networks</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph stat.ML</categories><comments>11 pages, 6 figures. Clarified to what extent our claims are
  rigorous, and to what extent they are conjectures; also added an
  interpretation of the eigenvectors of the 2n-dimensional version of the
  non-backtracking matrix</comments><journal-ref>Proceedings of the National Academy of Sciences 110, no. 52
  (2013): 20935-20940</journal-ref><doi>10.1073/pnas.1312486110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral algorithms are classic approaches to clustering and community
detection in networks. However, for sparse networks the standard versions of
these algorithms are suboptimal, in some cases completely failing to detect
communities even when other algorithms such as belief propagation can do so.
Here we introduce a new class of spectral algorithms based on a
non-backtracking walk on the directed edges of the graph. The spectrum of this
operator is much better-behaved than that of the adjacency matrix or other
commonly used matrices, maintaining a strong separation between the bulk
eigenvalues and the eigenvalues relevant to community structure even in the
sparse case. We show that our algorithm is optimal for graphs generated by the
stochastic block model, detecting communities all the way down to the
theoretical limit. We also show the spectrum of the non-backtracking operator
for some real-world networks, illustrating its advantages over traditional
spectral clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5554</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5554</id><created>2013-06-24</created><updated>2013-11-05</updated><authors><author><keyname>McWilliams</keyname><forenames>Brian</forenames></author><author><keyname>Balduzzi</keyname><forenames>David</forenames></author><author><keyname>Buhmann</keyname><forenames>Joachim M.</forenames></author></authors><title>Correlated random features for fast semi-supervised learning</title><categories>stat.ML cs.LG</categories><comments>15 pages, 3 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Correlated Nystrom Views (XNV), a fast semi-supervised
algorithm for regression and classification. The algorithm draws on two main
ideas. First, it generates two views consisting of computationally inexpensive
random features. Second, XNV applies multiview regression using Canonical
Correlation Analysis (CCA) on unlabeled data to bias the regression towards
useful features. It has been shown that, if the views contains accurate
estimators, CCA regression can substantially reduce variance with a minimal
increase in bias. Random views are justified by recent theoretical and
empirical work showing that regression with random features closely
approximates kernel regression, implying that random views can be expected to
contain accurate estimators. We show that XNV consistently outperforms a
state-of-the-art algorithm for semi-supervised learning: substantially
improving predictive performance and reducing the variability of performance on
a wide variety of real-world datasets, whilst also reducing runtime by orders
of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5568</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5568</id><created>2013-06-24</created><authors><author><keyname>Gorodecky</keyname><forenames>Danila A.</forenames></author></authors><title>Combinatorial method of polynomial expansion of symmetric Boolean
  functions</title><categories>cs.DM</categories><comments>12 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel polynomial expansion method of symmetric Boolean functions is
described. The method is efficient for symmetric Boolean function with small
set of valued numbers and has the linear complexity for elementary symmetric
Boolean functions, while the complexity of the known methods for this class of
functions is quadratic. The proposed method is based on the consequence of the
combinatorial Lucas theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5571</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5571</id><created>2013-06-24</created><authors><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Obdr&#x17e;&#xe1;lek</keyname><forenames>Jan</forenames></author></authors><title>Expanding the expressive power of Monadic Second-Order logic on
  restricted graph classes</title><categories>cs.DS cs.LO</categories><comments>Accepted for IWOCA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We combine integer linear programming and recent advances in Monadic
Second-Order model checking to obtain two new algorithmic meta-theorems for
graphs of bounded vertex-cover. The first shows that cardMSO1, an extension of
the well-known Monadic Second-Order logic by the addition of cardinality
constraints, can be solved in FPT time parameterized by vertex cover. The
second meta-theorem shows that the MSO partitioning problems introduced by Rao
can also be solved in FPT time with the same parameter. The significance of our
contribution stems from the fact that these formalisms can describe problems
which are W[1]-hard and even NP-hard on graphs of bounded tree-width.
Additionally, our algorithms have only an elementary dependence on the
parameter and formula. We also show that both results are easily extended from
vertex cover to neighborhood diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5583</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5583</id><created>2013-06-24</created><authors><author><keyname>Zhou</keyname><forenames>Yan</forenames></author></authors><title>vSMC: Parallel Sequential Monte Carlo in C++</title><categories>stat.CO cs.MS</categories><comments>44 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Sequential Monte Carlo is a family of algorithms for sampling from a sequence
of distributions. Some of these algorithms, such as particle filters, are
widely used in the physics and signal processing researches. More recent
developments have established their application in more general inference
problems such as Bayesian modeling.
  These algorithms have attracted considerable attentions in recent years as
they admit natural and scalable parallelizations. However, these algorithms are
perceived to be difficult to implement. In addition, parallel programming is
often unfamiliar to many researchers though conceptually appealing, especially
for sequential Monte Carlo related fields.
  A C++ template library is presented for the purpose of implementing general
sequential Monte Carlo algorithms on parallel hardware. Two examples are
presented: a simple particle filter and a classic Bayesian modeling problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5585</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5585</id><created>2013-06-24</created><updated>2013-08-18</updated><authors><author><keyname>Breuer</keyname><forenames>Peter T.</forenames></author><author><keyname>Pickin</keyname><forenames>Simon J.</forenames></author></authors><title>Soundness and Completeness of the NRB Verification Logic</title><categories>cs.LO</categories><comments>To appear in OpenCert 2013 Workshop, Sept 23, Madrid, 15p</comments><acm-class>B.1.2; D.2.4</acm-class><doi>10.1007/978-3-319-05032-4_28</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper gives a model for and a proof of completeness of the NRB
verification logic for deterministic imperative programs, the logic having been
used in the past as the basis for automated semantic checks of large,
fast-changing, open source C code archives, such as that of the Linux kernel
source. The model is a colored state transitions model that approximates from
above the set of transitions possible for a program. Correspondingly, the logic
catches all traces that may trigger a particular defect at a given point in the
program, but may also flag false positives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5586</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5586</id><created>2013-06-24</created><authors><author><keyname>Primmer</keyname><forenames>Robert</forenames></author><author><keyname>Nyman</keyname><forenames>Scott</forenames></author><author><keyname>Lin</keyname><forenames>Wayzen</forenames></author></authors><title>Creating a Relational Distributed Object Store</title><categories>cs.DB cs.DC</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In and of itself, data storage has apparent business utility. But when we can
convert data to information, the utility of stored data increases dramatically.
It is the layering of relation atop the data mass that is the engine for such
conversion. Frank relation amongst discrete objects sporadically ingested is
rare, making the process of synthesizing such relation all the more
challenging, but the challenge must be met if we are ever to see an equivalent
business value for unstructured data as we already have with structured data.
This paper describes a novel construct, referred to as a relational distributed
object store (RDOS), that seeks to solve the twin problems of how to
persistently and reliably store petabytes of unstructured data while
simultaneously creating and persisting relations amongst billions of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5596</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5596</id><created>2013-06-24</created><authors><author><keyname>Li</keyname><forenames>Nan</forenames></author><author><keyname>Dubrova</keyname><forenames>Elena</forenames></author></authors><title>An Algorithm for Constructing a Smallest Register with Non-Linear Update
  Generating a Given Binary Sequence</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Registers with Non-Linear Update (RNLUs) are a generalization of Non-Linear
Feedback Shift Registers (NLFSRs) in which both, feedback and feedforward,
connections are allowed and no chain connection between the stages is required.
In this paper, a new algorithm for constructing RNLUs generating a given binary
sequence is presented. Expected size of RNLUs constructed by the presented
algorithm is proved to be O(n/log(n/p)), where n is the sequence length and p
is the degree of parallelization. This is asymptotically smaller than the
expected size of RNLUs constructed by previous algorithms and the expected size
of LFSRs and NLFSRs generating the same sequence. The presented algorithm can
potentially be useful for many applications, including testing, wireless
communications, and cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5601</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5601</id><created>2013-06-24</created><updated>2013-08-25</updated><authors><author><keyname>M&#xfc;hlenthaler</keyname><forenames>Moritz</forenames></author><author><keyname>Wanka</keyname><forenames>Rolf</forenames></author></authors><title>A Decomposition of the Max-min Fair Curriculum-based Course Timetabling
  Problem</title><categories>cs.AI</categories><comments>revised version (fixed problems in the notation and general
  improvements); original paper: 16 pages, accepted for publication at the
  Multidisciplinary International Scheduling Conference 2013 (MISTA 2013)</comments><acm-class>I.2.8; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a decomposition of the max-min fair curriculum-based course
timetabling (MMF-CB-CTT) problem. The decomposition models the room assignment
subproblem as a generalized lexicographic bottleneck optimization problem
(LBOP). We show that the generalized LBOP can be solved efficiently if the
corresponding sum optimization problem can be solved efficiently. As a
consequence, the room assignment subproblem of the MMF-CB-CTT problem can be
solved efficiently. We use this insight to improve a previously proposed
heuristic algorithm for the MMF-CB-CTT problem. Our experimental results
indicate that using the new decomposition improves the performance of the
algorithm on most of the 21 ITC2007 test instances with respect to the quality
of the best solution found. Furthermore, we introduce a measure of the quality
of a solution to a max-min fair optimization problem. This measure helps to
overcome some limitations imposed by the qualitative nature of max-min fairness
and aids the statistical evaluation of the performance of randomized algorithms
for such problems. We use this measure to show that using the new decomposition
the algorithm outperforms the original one on most instances with respect to
the average solution quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5606</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5606</id><created>2013-06-24</created><updated>2014-02-17</updated><authors><author><keyname>Hurley</keyname><forenames>Barry</forenames></author><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author><author><keyname>Malitsky</keyname><forenames>Yuri</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Barry</forenames></author></authors><title>Proteus: A Hierarchical Portfolio of Solvers and Transformations</title><categories>cs.AI</categories><comments>11th International Conference on Integration of AI and OR Techniques
  in Constraint Programming for Combinatorial Optimization Problems. The final
  publication is available at link.springer.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, portfolio approaches to solving SAT problems and CSPs have
become increasingly common. There are also a number of different encodings for
representing CSPs as SAT instances. In this paper, we leverage advances in both
SAT and CSP solving to present a novel hierarchical portfolio-based approach to
CSP solving, which we call Proteus, that does not rely purely on CSP solvers.
Instead, it may decide that it is best to encode a CSP problem instance into
SAT, selecting an appropriate encoding and a corresponding SAT solver. Our
experimental evaluation used an instance of Proteus that involved four CSP
solvers, three SAT encodings, and six SAT solvers, evaluated on the most
challenging problem instances from the CSP solver competitions, involving
global and intensional constraints. We show that significant performance
improvements can be achieved by Proteus obtained by exploiting alternative
view-points and solvers for combinatorial problem-solving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5609</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5609</id><created>2013-06-24</created><authors><author><keyname>Gorla</keyname><forenames>Elisa</forenames></author><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>Partial Spreads in Random Network Coding</title><categories>cs.IT math.IT</categories><msc-class>11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following the approach by R. K\&quot;otter and F. R. Kschischang, we study network
codes as families of k-dimensional linear subspaces of a vector space F_q^n, q
being a prime power and F_q the finite field with q elements. In particular,
following an idea in finite projective geometry, we introduce a class of
network codes which we call &quot;partial spread codes&quot;. Partial spread codes
naturally generalize spread codes. In this paper we provide an easy description
of such codes in terms of matrices, discuss their maximality, and provide an
efficient decoding algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5615</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5615</id><created>2013-06-24</created><updated>2013-11-14</updated><authors><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Liu</keyname><forenames>Yuansheng</forenames></author><author><keyname>Zhang</keyname><forenames>Leo Yu</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-wo</forenames></author></authors><title>Breaking a compression and encryption scheme based on Chinese Remainder
  Theorem</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a fundamental of number theory, Chinese Reminder Theorem (CRT) is widely
used to construct cryptographic primitive. This paper studies security of a
compression and encryption scheme based on CRT, called CECRT. Based on some
properties of CRT, the equivalent secret key of CECRT can be reconstructed
efficiently: the pair number of required of chosen plaintext and the
corresponding ciphertext is only $(1+\lceil (\log_2L)/l \rceil)$; the attack
complexity is only $O(L)$, where $L$ is the size of the plaintext, and $l$ is
binary size of every plain-element. In addition, some other defects of CECRT,
including invalid compression function, are also reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5646</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5646</id><created>2013-06-24</created><updated>2015-12-23</updated><authors><author><keyname>Mullan</keyname><forenames>Ciaran</forenames></author><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author></authors><title>SL2 homomorphic hash functions: Worst case to average case reduction and
  short collision search</title><categories>cs.CR math.GR</categories><comments>Final version. To appear in Design Codes Cryptography</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study homomorphic hash functions into SL(2,q), the 2x2 matrices with
determinant 1 over the field with $q$ elements. Modulo a well supported number
theoretic hypothesis, which holds in particular for concrete homomorphisms
proposed thus far, we provide a worst case to average case reduction for these
hash functions: upto a logarithmic factor, a random homomorphism is as secure
as _any_ concrete homomorphism. For a family of homomorphisms containing
several concrete proposals in the literature, we prove that collisions of
length O(log(q)) can be found in running time O(sqrt(q)). For general
homomorphisms we offer an algorithm that, heuristically and according to
experiments, in running time O(sqrt(q)) finds collisions of length O(log(q))
for q even, and length O(log^2(q)/loglog(q))$ for arbitrary q. While exponetial
time, our algorithms are faster in practice than all earlier generic
algorithms, and produce much shorter collisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5648</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5648</id><created>2013-06-20</created><authors><author><keyname>Chen</keyname><forenames>Zhixiong</forenames></author></authors><title>Trace representation and linear complexity of binary sequences derived
  from Fermat quotients</title><categories>math.NT cs.CR</categories><comments>14 pages, no figures</comments><msc-class>94A55, 94A60, 65C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the trace representations of two families of binary sequences
derived from Fermat quotients modulo an odd prime $p$ (one is the binary
threshold sequences, the other is the Legendre-Fermat quotient sequences) via
determining the defining pairs of all binary characteristic sequences of
cosets, which coincide with the sets of pre-images modulo $p^2$ of each fixed
value of Fermat quotients. From the defining pairs, we can obtain an earlier
result of linear complexity for the binary threshold sequences and a new result
of linear complexity for the Legendre-Fermat quotient sequences under the
assumption of $2^{p-1}\not\equiv 1 \bmod {p^2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5667</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5667</id><created>2013-06-24</created><authors><author><keyname>Langdon</keyname><forenames>W. B.</forenames></author><author><keyname>Harman</keyname><forenames>M.</forenames></author></authors><title>Using Genetic Programming to Model Software</title><categories>cs.NE cs.AI</categories><comments>As UCL computer science Technical Report RN/13/12</comments><report-no>RN/13/12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a generic program to investigate the scope for automatically
customising it for a vital current task, which was not considered when it was
first written. In detail, we show genetic programming (GP) can evolve models of
aspects of BLAST's output when it is used to map Solexa Next-Gen DNA sequences
to the human genome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5677</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5677</id><created>2013-06-24</created><authors><author><keyname>Zhao</keyname><forenames>Dong</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Ma</keyname><forenames>Huadong</forenames></author></authors><title>OMG: How Much Should I Pay Bob in Truthful Online Mobile Crowdsourced
  Sensing?</title><categories>cs.GT cs.NI</categories><comments>14 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile crowdsourced sensing (MCS) is a new paradigm which takes advantage of
the pervasive smartphones to efficiently collect data, enabling numerous novel
applications. To achieve good service quality for a MCS application, incentive
mechanisms are necessary to attract more user participation. Most of existing
mechanisms apply only for the offline scenario where all users' information are
known a priori. On the contrary, we focus on a more real scenario where users
arrive one by one online in a random order. We model the problem as an online
auction in which the users submit their private types to the crowdsourcer over
time, and the crowdsourcer aims to select a subset of users before a specified
deadline for maximizing the total value of the services provided by selected
users under a budget constraint. We design two online mechanisms, OMZ and OMG,
satisfying the computational efficiency, individual rationality, budget
feasibility, truthfulness, consumer sovereignty and constant competitiveness
under the zero arrival-departure interval case and a more general case,
respectively. Through extensive simulations, we evaluate the performance and
validate the theoretical properties of our online mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5678</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5678</id><created>2013-06-24</created><updated>2013-07-09</updated><authors><author><keyname>Koleini</keyname><forenames>Masoud</forenames></author><author><keyname>Clarkson</keyname><forenames>Michael R.</forenames></author><author><keyname>Micinski</keyname><forenames>Kristopher K.</forenames></author></authors><title>A Temporal Logic of Security</title><categories>cs.LO cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new logic for verification of security policies is proposed. The logic,
HyperLTL, extends linear-time temporal logic (LTL) with connectives for
explicit and simultaneous quantification over multiple execution paths, thereby
enabling HyperLTL to express information-flow security policies that LTL
cannot. A model-checking algorithm for a fragment of HyperLTL is given, and the
algorithm is implemented in a prototype model checker. The class of security
policies expressible in HyperLTL is characterized by an arithmetic hierarchy of
hyperproperties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5690</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5690</id><created>2013-06-21</created><authors><author><keyname>Pieris</keyname><forenames>Dhammika</forenames></author></authors><title>Modifying the Entity relationship modelling notation: towards high
  quality relational databases from better notated ER models</title><categories>cs.DB</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The entity relationship modelling using the original ER notation has been
applauded providing a natural view of data in conceptual modelling of
information systems. However, the current ER to relational model transformation
algorithm is known to be insufficient in providing a complete and accurate
representation of the ER model undertaken for transformation. In an effort to
derive better transformations from ER models, we have understood that
modifications should be introduced to both of the existing transformation
algorithm as well as to the ER notation. Introducing some new concepts, we have
adapted the original ER notation and developed a new transformation algorithm
based on the existing one. This paper presents the modified ER notation with an
ER diagram drawn based on the new notation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5702</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5702</id><created>2013-06-24</created><authors><author><keyname>Janakiraman</keyname><forenames>Vijay Manikandan</forenames></author><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author><author><keyname>Sterniak</keyname><forenames>Jeff</forenames></author><author><keyname>Assanis</keyname><forenames>Dennis</forenames></author></authors><title>Modeling The Stable Operating Envelope For Partially Stable Combustion
  Engines Using Class Imbalance Learning</title><categories>cs.NE</categories><comments>In a Journal review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced combustion technologies such as homogeneous charge compression
ignition (HCCI) engines have a narrow stable operating region defined by
complex control strategies such as exhaust gas recirculation (EGR) and variable
valve timing among others. For such systems, it is important to identify the
operating envelope or the boundary of stable operation for diagnostics and
control purposes. Obtaining a good model of the operating envelope using
physics becomes intractable owing to engine transient effects. In this paper, a
machine learning based approach is employed to identify the stable operating
boundary of HCCI combustion directly from experimental data. Owing to imbalance
in class proportions in the data, two approaches are considered. A re-sampling
(under-sampling, over-sampling) based approach is used to develop models using
existing algorithms while a cost-sensitive approach is used to modify the
learning algorithm without modifying the data set. Support vector machines and
recently developed extreme learning machines are used for model development and
results compared against linear classification methods show that cost-sensitive
versions of ELM and SVM algorithms are well suited to model the HCCI operating
envelope. The prediction results indicate that the models have the potential to
be used for predicting HCCI instability based on sensor measurement history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5707</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5707</id><created>2013-06-24</created><updated>2014-06-24</updated><authors><author><keyname>Sung</keyname><forenames>Jaeyong</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Synthesizing Manipulation Sequences for Under-Specified Tasks using
  Unrolled Markov Random Fields</title><categories>cs.RO cs.AI cs.LG</categories><comments>To Appear in IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2014 (A preliminary version of this work was presented at
  International Conference of Machine Learning (ICML) workshop on Prediction
  with Sequential Models, 2013)</comments><doi>10.1109/IROS.2014.6942972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many tasks in human environments require performing a sequence of navigation
and manipulation steps involving objects. In unstructured human environments,
the location and configuration of the objects involved often change in
unpredictable ways. This requires a high-level planning strategy that is robust
and flexible in an uncertain environment. We propose a novel dynamic planning
strategy, which can be trained from a set of example sequences. High level
tasks are expressed as a sequence of primitive actions or controllers (with
appropriate parameters). Our score function, based on Markov Random Field
(MRF), captures the relations between environment, controllers, and their
arguments. By expressing the environment using sets of attributes, the approach
generalizes well to unseen scenarios. We train the parameters of our MRF using
a maximum margin learning method. We provide a detailed empirical validation of
our overall framework demonstrating successful plan strategies for a variety of
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5720</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5720</id><created>2013-06-24</created><authors><author><keyname>Perkins</keyname><forenames>Will</forenames></author><author><keyname>Reyzin</keyname><forenames>Lev</forenames></author></authors><title>On the Resilience of Bipartite Networks</title><categories>cs.DS cs.SI</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by problems modeling the spread of infections in networks, in this
paper we explore which bipartite graphs are most resilient to widespread
infections under various parameter settings. Namely, we study bipartite
networks with a requirement of a minimum degree $d$ on one side under an
independent infection, independent transmission model. We completely
characterize the optimal graphs in the case $d=1$, which already produces
non-trivial behavior, and we give some extremal results for the more general
cases. We also show that determining the subgraph of an arbitrary bipartite
graph most resilient to infection is NP-hard for any one-sided minimal degree
$d \ge 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5726</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5726</id><created>2013-06-24</created><updated>2013-07-07</updated><authors><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author><author><keyname>Meel</keyname><forenames>Kuldeep S.</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>A Scalable Approximate Model Counter</title><categories>cs.LO</categories><comments>Conference version will appear in CP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propositional model counting} (#SAT), i.e., counting the number of satisfying
assignments of a propositional formula, is a problem of significant theoretical
and practical interest. Due to the inherent complexity of the problem,
approximate model counting, which counts the number of satisfying assignments
to within given tolerance and confidence level, was proposed as a practical
alternative to exact model counting. Yet, approximate model counting has been
studied essentially only theoretically. The only reported implementation of
approximate model counting, due to Karp and Luby, worked only for DNF formulas.
A few existing tools for CNF formulas are bounding model counters; they can
handle realistic problem sizes, but fall short of providing counts within given
tolerance and confidence, and, thus, are not approximate model counters.
  We present here a novel algorithm, as well as a reference implementation,
that is the first scalable approximate model counter for CNF formulas. The
algorithm works by issuing a polynomial number of calls to a SAT solver. Our
tool, ApproxMC, scales to formulas with tens of thousands of variables. Careful
experimental comparisons show that ApproxMC reports, with high confidence,
bounds that are close to the exact count, and also succeeds in reporting bounds
with small tolerance and high confidence in cases that are too large for
computing exact model counts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5771</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5771</id><created>2013-06-24</created><authors><author><keyname>Jenkins</keyname><forenames>Adrian</forenames><affiliation>ICC, Durham</affiliation></author><author><keyname>Booth</keyname><forenames>Stephen</forenames><affiliation>EPCC, Edinburgh</affiliation></author></authors><title>Panphasia: a user guide</title><categories>astro-ph.IM astro-ph.CO cs.MS</categories><comments>11 pages, 2 figures. Software to calculate Panphasia is available
  from: http://icc.dur.ac.uk/Panphasia.php</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We make a very large realisation of a Gaussian white noise field, called
PANPHASIA, public by releasing software that computes this field. Panphasia is
designed specifically for setting up Gaussian initial conditions for
cosmological simulations and resimulations of structure formation. We make
available both software to compute the field itself and codes to illustrate
applications including a modified version of a public serial initial conditions
generator. We document the software and present the results of a few basic
tests of the field. The properties and method of construction of Panphasia are
described in full in a companion paper Jenkins 2013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5776</identifier>
 <datestamp>2013-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5776</id><created>2013-06-24</created><updated>2013-09-11</updated><authors><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author></authors><title>Two-Part Reconstruction in Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures, submitted to GlobalSIP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-part reconstruction is a framework for signal recovery in compressed
sensing (CS), in which the advantages of two different algorithms are combined.
Our framework allows to accelerate the reconstruction procedure without
compromising the reconstruction quality. To illustrate the efficacy of our
two-part approach, we extend the author's previous Sudocodes algorithm and make
it robust to measurement noise. In a 1-bit CS setting, promising numerical
results indicate that our algorithm offers both a reduction in run-time and
improvement in reconstruction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5781</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5781</id><created>2013-06-24</created><updated>2014-11-01</updated><authors><author><keyname>Carmon</keyname><forenames>Yair</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Comparison of the Achievable Rates in OFDM and Single Carrier Modulation
  with I.I.D. Inputs</title><categories>cs.IT math.IT</categories><comments>Revised version of IEEE IT submission. Includes new results on
  uniform inputs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare the maximum achievable rates in single-carrier and OFDM modulation
schemes, under the practical assumptions of i.i.d. finite alphabet inputs and
linear ISI with additive Gaussian noise. We show that the Shamai-Laroia
approximation serves as a bridge between the two rates: while it is well known
that this approximation is often a lower bound on the single-carrier achievable
rate, it is revealed to also essentially upper bound the OFDM achievable rate.
We apply Information-Estimation relations in order to rigorously establish this
result for both general input distributions and to sharpen it for commonly used
PAM and QAM constellations. To this end, novel bounds on MMSE estimation of PAM
inputs to a scalar Gaussian channel are derived, which may be of general
interest. Our results show that, under reasonable assumptions, optimal
single-carrier schemes may offer spectral efficiency significantly superior to
that of OFDM, motivating further research of such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5782</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5782</id><created>2013-06-24</created><authors><author><keyname>Dazzi</keyname><forenames>Patrizio</forenames></author></authors><title>A Tool for Programming Embarrassingly Task Parallel Applications on CoW
  and NoW</title><categories>cs.DC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embarrassingly parallel problems can be split in parts that are characterized
by a really low (or sometime absent) exchange of information during their
computation in parallel. As a consequence they can be effectively computed in
parallel exploiting commodity hardware, hence without particularly
sophisticated interconnection networks. Basically, this means Clusters,
Networks of Workstations and Desktops as well as Computational Clouds. Despite
the simplicity of this computational model, it can be exploited to compute a
quite large range of problems. This paper describes JJPF, a tool for developing
task parallel applications based on Java and Jini that showed to be an
effective and efficient solution in environment like Clusters and Networks of
Workstations and Desktops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5787</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5787</id><created>2013-06-24</created><updated>2014-01-01</updated><authors><author><keyname>Thakur</keyname><forenames>Gaurav</forenames></author></authors><title>Spread Spectrum Codes for Continuous-Phase Modulated Systems</title><categories>cs.IT math.IT</categories><msc-class>94A14, 60G35</msc-class><journal-ref>IEEE Transactions on Communications 62(3):952-960 (2014)</journal-ref><doi>10.1109/TCOMM.2014.011514.130842</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the theoretical performance of a combined approach to demodulation
and decoding of binary continuous-phase modulated signals under repetition-like
codes. This technique is motivated by a need to transmit packetized or framed
data bursts in high noise regimes where many powerful, short-length codes are
ineffective. In channels with strong noise, we mathematically study the
asymptotic bit error rates of this combined approach and quantify the
performance improvement over performing demodulation and decoding separately as
the code rate increases. In this context, we also discuss a simple variant of
repetition coding involving pseudorandom code words, based on direct-sequence
spread spectrum methods, that preserves the spectral density of the encoded
signal in order to maintain resistance to narrowband interference. We describe
numerical simulations that demonstrate the advantages of this approach as an
inner code which can be used underneath modern coding schemes in high noise
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5793</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5793</id><created>2013-06-24</created><authors><author><keyname>Kallitsis</keyname><forenames>Michael</forenames></author><author><keyname>Stoev</keyname><forenames>Stilian</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author></authors><title>A State-Space Approach for Optimal Traffic Monitoring via Network Flow
  Sampling</title><categories>cs.SY cs.NI stat.AP stat.ML</categories><comments>preliminary work, short paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robustness and integrity of IP networks require efficient tools for
traffic monitoring and analysis, which scale well with traffic volume and
network size. We address the problem of optimal large-scale flow monitoring of
computer networks under resource constraints. We propose a stochastic
optimization framework where traffic measurements are done by exploiting the
spatial (across network links) and temporal relationship of traffic flows.
Specifically, given the network topology, the state-space characterization of
network flows and sampling constraints at each monitoring station, we seek an
optimal packet sampling strategy that yields the best traffic volume estimation
for all flows of the network. The optimal sampling design is the result of a
concave minimization problem; then, Kalman filtering is employed to yield a
sequence of traffic estimates for each network flow. We evaluate our algorithm
using real-world Internet2 data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5794</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5794</id><created>2013-06-24</created><updated>2014-07-10</updated><authors><author><keyname>Darst</keyname><forenames>Richard K.</forenames></author><author><keyname>Reichman</keyname><forenames>David R.</forenames></author><author><keyname>Ronhovde</keyname><forenames>Peter</forenames></author><author><keyname>Nussinov</keyname><forenames>Zohar</forenames></author></authors><title>Algorithm independent bounds on community detection problems and
  associated transitions in stochastic block model graphs</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>21 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive rigorous bounds for well-defined community structure in complex
networks for a stochastic block model (SBM) benchmark. In particular, we
analyze the effect of inter-community &quot;noise&quot; (inter-community edges) on any
&quot;community detection&quot; algorithm's ability to correctly group nodes assigned to
a planted partition, a problem which has been proven to be NP complete in a
standard rendition. Our result does not rely on the use of any one particular
algorithm nor on the analysis of the limitations of inference. Rather, we turn
the problem on its head and work backwards to examine when, in the first place,
well defined structure may exist in SBMs.The method that we introduce here
could potentially be applied to other computational problems. The objective of
community detection algorithms is to partition a given network into optimally
disjoint subgraphs (or communities). Similar to k-SAT and other combinatorial
optimization problems, &quot;community detection&quot; exhibits different phases.
Networks that lie in the &quot;unsolvable phase&quot; lack well-defined structure and
thus have no partition that is meaningful. Solvable systems splinter into two
disparate phases: those in the &quot;hard&quot; phase and those in the &quot;easy&quot; phase. As
befits its name, within the easy phase, a partition is easy to achieve by known
algorithms. When a network lies in the hard phase, it still has an underlying
structure yet finding a meaningful partition which can be checked in polynomial
time requires an exhaustive computational effort that rapidly increases with
the size of the graph. When taken together, (i) the rigorous results that we
report here on when graphs have an underlying structure and (ii) recent results
concerning the limits of rather general algorithms, suggest bounds on the hard
phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5796</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5796</id><created>2013-06-24</created><updated>2014-10-08</updated><authors><author><keyname>Maftuleac</keyname><forenames>Daniela</forenames></author></authors><title>Algorithms for distance problems in planar complexes of global
  nonpositive curvature</title><categories>cs.CG</categories><doi>10.1142/S0218195914500010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CAT(0) metric spaces and hyperbolic spaces play an important role in
combinatorial and geometric group theory. In this paper, we present efficient
algorithms for distance problems in CAT(0) planar complexes. First of all, we
present an algorithm for answering single-point distance queries in a CAT(0)
planar complex. Namely, we show that for a CAT(0) planar complex K with n
vertices, one can construct in O(n^2 log n) time a data structure D of size
O(n^2) so that, given a point x in K, the shortest path gamma(x,y) between x
and the query point y can be computed in linear time. Our second algorithm
computes the convex hull of a finite set of points in a CAT(0) planar complex.
This algorithm is based on Toussaint's algorithm for computing the convex hull
of a finite set of points in a simple polygon and it constructs the convex hull
of a set of k points in O(n^2 log n + nk log k) time, using a data structure of
size O(n^2 + k).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5797</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5797</id><created>2013-06-24</created><authors><author><keyname>Chen</keyname><forenames>Xiaomin</forenames></author><author><keyname>Jukan</keyname><forenames>Admela</forenames></author><author><keyname>Gumaste</keyname><forenames>Ashwin</forenames></author></authors><title>Optimized Parallel Transmission in Elastic Optical Networks to Support
  High-Speed Ethernet</title><categories>cs.NI</categories><comments>12 pages, submitted to IEEE Journal of Lightwave Technology, under
  review</comments><doi>10.1109/JLT.2013.2291318</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The need for optical parallelization is driven by the imminent optical
capacity crunch, where the spectral efficiency required in the coming decades
will be beyond the Shannon limit. To this end, the emerging high-speed Ethernet
services at 100 Gbps, have already standardized options to utilize parallel
optics to parallelize interfaces referred to as Multi-lane Distribution (MLD).
OFDM-based optical network is a promising transmission option towards the goal
of Ethernet parallelization. It can allocate optical resource tailored for a
variety of bandwidth requirements and that in a fundamentally parallel fashion
with each sub-carrier utilizing a frequency slot at a lower rate than if serial
transmission was used. In this paper, we propose a novel parallel transmission
framework designed for elastic (OFDM-based) optical networks to support
high-speed Ethernet services, in-line with IEEE and ITU-T standards. We
formulate an ILP optimization model based on integer linear programming, with
consideration of various constraints, including spectrum fragmentation,
differential delay and guard-band constraints. We also propose a heuristic
algorithm which can be applied when the optimization model becomes intractable.
The numerical results show the effectiveness and high suitability of elastic
optical networks to support parallel transmission in high-speed Ethernet. To
the best of our knowledge, this is the first attempt to investigate the
parallel transmission in elastic optical networks to support standardized
high-speed Ethernet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5809</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5809</id><created>2013-06-24</created><updated>2013-06-29</updated><authors><author><keyname>Li</keyname><forenames>Chengju</forenames></author><author><keyname>Yue</keyname><forenames>Qin</forenames></author><author><keyname>Li</keyname><forenames>Fengwei</forenames></author></authors><title>Weight distributions of cyclic codes with respect to pairwise coprime
  order elements</title><categories>cs.IT math.IT</categories><comments>18 pages. arXiv admin note: substantial text overlap with
  arXiv:1306.5277</comments><msc-class>94B15, 11T71, 11T24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\Bbb F_r$ be an extension of a finite field $\Bbb F_q$ with $r=q^m$. Let
each $g_i$ be of order $n_i$ in $\Bbb F_r^*$ and $\gcd(n_i, n_j)=1$ for $1\leq
i \neq j \leq u$.
  We define a cyclic code over $\Bbb F_q$ by
  $$\mathcal C_{(q, m, n_1,n_2, ..., n_u)}=\{c(a_1, a_2, ..., a_u) : a_1, a_2,
..., a_u \in \Bbb F_r\},$$ where
  $$c(a_1, a_2, ..., a_u)=({Tr}_{r/q}(\sum_{i=1}^ua_ig_i^0), ...,
{Tr}_{r/q}(\sum_{i=1}^ua_ig_i^{n-1}))$$ and $n=n_1n_2... n_u$. In this paper,
we present a method to compute the weights of $\mathcal C_{(q, m, n_1,n_2, ...,
n_u)}$. Further, we determine the weight distributions of the cyclic codes
$\mathcal C_{(q, m, n_1,n_2)}$ and $\mathcal C_{(q, m, n_1,n_2,1)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5815</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5815</id><created>2013-06-24</created><authors><author><keyname>Shinn</keyname><forenames>Tong-Wook</forenames></author><author><keyname>Takaoka</keyname><forenames>Tadao</forenames></author></authors><title>Some Extensions of the All Pairs Bottleneck Paths Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the well known bottleneck paths problem in two directions for
directed unweighted (unit edge cost) graphs with positive real edge capacities.
Firstly we narrow the problem domain and compute the bottleneck of the entire
network in $O(n^{\omega}\log{n})$ time, where $O(n^{\omega})$ is the time taken
to multiply two $n$-by-$n$ matrices over ring. Secondly we enlarge the domain
and compute the shortest paths for all possible flow amounts. We present a
combinatorial algorithm to solve the Single Source Shortest Paths for All Flows
(SSSP-AF) problem in $O(mn)$ worst case time, followed by an algorithm to solve
the All Pairs Shortest Paths for All Flows (APSP-AF) problem in
$O(\sqrt{d}n^{(\omega+9)/4})$ time, where $d$ is the number of distinct edge
capacities. We also discuss real life applications for these new problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5825</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5825</id><created>2013-06-24</created><updated>2014-06-27</updated><authors><author><keyname>Goyal</keyname><forenames>Navin</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author><author><keyname>Xiao</keyname><forenames>Ying</forenames></author></authors><title>Fourier PCA and Robust Tensor Decomposition</title><categories>cs.LG cs.DS stat.ML</categories><comments>Extensively revised; details added; minor errors corrected;
  exposition improved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fourier PCA is Principal Component Analysis of a matrix obtained from higher
order derivatives of the logarithm of the Fourier transform of a
distribution.We make this method algorithmic by developing a tensor
decomposition method for a pair of tensors sharing the same vectors in rank-$1$
decompositions. Our main application is the first provably polynomial-time
algorithm for underdetermined ICA, i.e., learning an $n \times m$ matrix $A$
from observations $y=Ax$ where $x$ is drawn from an unknown product
distribution with arbitrary non-Gaussian components. The number of component
distributions $m$ can be arbitrarily higher than the dimension $n$ and the
columns of $A$ only need to satisfy a natural and efficiently verifiable
nondegeneracy condition. As a second application, we give an alternative
algorithm for learning mixtures of spherical Gaussians with linearly
independent means. These results also hold in the presence of Gaussian noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5829</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5829</id><created>2013-06-24</created><updated>2013-07-11</updated><authors><author><keyname>Cousins</keyname><forenames>Ben</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>A Cubic Algorithm for Computing Gaussian Volume</title><categories>cs.DS math.FA math.PR</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present randomized algorithms for sampling the standard Gaussian
distribution restricted to a convex set and for estimating the Gaussian measure
of a convex set, in the general membership oracle model. The complexity of
integration is $O^*(n^3)$ while the complexity of sampling is $O^*(n^3)$ for
the first sample and $O^*(n^2)$ for every subsequent sample. These bounds
improve on the corresponding state-of-the-art by a factor of $n$. Our
improvement comes from several aspects: better isoperimetry, smoother
annealing, avoiding transformation to isotropic position and the use of the
&quot;speedy walk&quot; in the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5836</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5836</id><created>2013-06-24</created><authors><author><keyname>Ma</keyname><forenames>Shan</forenames></author><author><keyname>Xiong</keyname><forenames>Junlin</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery A.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Robust Decentralized Stabilization of Markovian Jump Large-Scale
  Systems: A Neighboring Mode Dependent Control Approach</title><categories>cs.SY</categories><comments>accepted by Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the decentralized stabilization problem for a
class of uncertain large-scale systems with Markovian jump parameters. The
controllers use local subsystem states and neighboring mode information to
generate local control inputs. A sufficient condition involving rank
constrained linear matrix inequalities is proposed for the design of such
controllers. A numerical example is given to illustrate the developed theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5838</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5838</id><created>2013-06-24</created><updated>2013-12-16</updated><authors><author><keyname>Cheilaris</keyname><forenames>Panagiotis</forenames></author><author><keyname>Khramtcova</keyname><forenames>Elena</forenames></author><author><keyname>Papadopoulou</keyname><forenames>Evanthia</forenames></author></authors><title>Randomized incremental construction of the Hausdorff Voronoi diagram of
  non-crossing clusters</title><categories>cs.CG</categories><comments>This paper has been withdrawn by the author because the substantially
  updated version (improved results, major text revision) is now submitted
  (arXiv:1312.3904)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Hausdorff Voronoi diagram of a set of clusters of points in the plane,
the distance between a point t and a cluster P is the maximum Euclidean
distance between t and a point in P. This diagram has direct applications in
VLSI design. We consider so-called &quot;non-crossing&quot; clusters. The complexity of
the Hausdorff diagram of m such clusters is linear in the total number n of
points in the convex hulls of all clusters. We present randomized incremental
constructions for computing efficiently the diagram, improving considerably
previous results. Our best complexity algorithm runs in expected time O((n +
m(log log(n))^2)log^2(n)) and worst-case space O(n). We also provide a more
practical algorithm whose expected running time is O((n + m log(n))log^2(n))
and expected space complexity is O(n). To achieve these bounds, we augment the
randomized incremental paradigm for the construction of Voronoi diagrams with
the ability to efficiently handle non-standard characteristics of generalized
Voronoi diagrams, such as sites of non-constant complexity, sites that are not
enclosed in their Voronoi regions, and empty Voronoi regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5850</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5850</id><created>2013-06-25</created><updated>2013-11-04</updated><authors><author><keyname>Liu</keyname><forenames>Shuiyin</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Practical Secrecy: Bridging the Gap between Cryptography and Physical
  Layer Security</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current security techniques can be implemented either by requiring a secret
key exchange or depending on assumptions about the communication channels. In
this paper, we show that, by using a physical layer technique known as
artificial noise, it is feasible to protect secret data without any form of
secret key exchange and any restriction on the communication channels.
Specifically, we analyze how the artificial noise can achieve practical
secrecy. By treating the artificial noise as an unshared one-time pad secret
key, we show that the proposed scheme also achieves Shannon's perfect secrecy.
Moreover, we show that achieving perfect secrecy is much easier than ensuring
non-zero secrecy capacity, especially when the eavesdropper has more antennas
than the transmitter. Focusing on the practical applications, we show that
practical secrecy and strong secrecy can be guaranteed even if the eavesdropper
attempts to remove the artificial noise. We finally show the connections
between traditional cryptography and physical layer security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5855</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5855</id><created>2013-06-25</created><authors><author><keyname>Meir</keyname><forenames>Reshef</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>Equilibrium in Labor Markets with Few Firms</title><categories>cs.GT</categories><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study competition between firms in labor markets, following a
combinatorial model suggested by Kelso and Crawford [1982]. In this model, each
firm is trying to recruit workers by offering a higher salary than its
competitors, and its production function defines the utility generated from any
actual set of recruited workers. We define two natural classes of production
functions for firms, where the first one is based on additive capacities
(weights), and the second on the influence of workers in a social network. We
then analyze the existence of pure subgame perfect equilibrium (PSPE) in the
labor market and its properties. While neither class holds the gross
substitutes condition, we show that in both classes the existence of PSPE is
guaranteed under certain restrictions, and in particular when there are only
two competing firms. As a corollary, there exists a Walrasian equilibrium in a
corresponding combinatorial auction, where bidders' valuation functions belong
to these classes.
  While a PSPE may not exist when there are more than two firms, we perform an
empirical study of equilibrium outcomes for the case of weight-based games with
three firms, which extend our analytical results. We then show that stability
can in some cases be extended to coalitional stability, and study the
distribution of profit between firms and their workers in weight-based games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5858</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5858</id><created>2013-06-25</created><authors><author><keyname>Nissim</keyname><forenames>Raz</forenames></author><author><keyname>Brafman</keyname><forenames>Ronen</forenames></author></authors><title>Distributed Heuristic Forward Search for Multi-Agent Systems</title><categories>cs.AI cs.DC</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a number of distributed forward search algorithms for
solving multi-agent planning problems. We introduce a distributed formulation
of non-optimal forward search, as well as an optimal version, MAD-A*. Our
algorithms exploit the structure of multi-agent problems to not only distribute
the work efficiently among different agents, but also to remove symmetries and
reduce the overall workload. The algorithms ensure that private information is
not shared among agents, yet computation is still efficient -- outperforming
current state-of-the-art distributed planners, and in some cases even
centralized search -- despite the fact that each agent has access only to
partial information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5863</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5863</id><created>2013-06-25</created><updated>2015-03-11</updated><authors><author><keyname>Yang</keyname><forenames>Li</forenames></author></authors><title>Quantum oblivious transfer and bit commitment protocols based on two
  non-orthogonal states coding</title><categories>quant-ph cs.CR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oblivious transfer (OT) protocol is presented based on non-orthogonal states
transmission. Then, the bit commitment protocols on the top of it are
constructed. Contrast to classical cryptography, the bit commitment protocols
may insecure even if the OT and the composition itself are secure. Finally, we
present some interactive bit commitment protocols which are probably beyond the
famous no-go theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5883</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5883</id><created>2013-06-25</created><authors><author><keyname>Zachariah</keyname><forenames>Dave</forenames></author><author><keyname>Wirf&#xe4;lt</keyname><forenames>Petter</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author></authors><title>Line Spectrum Estimation with Probabilistic Priors</title><categories>math.ST cs.IT math.IT stat.TH</categories><journal-ref>Signal Processing, Volume 93, Issue 11, November 2013, Pages
  2969-2974</journal-ref><doi>10.1016/j.sigpro.2013.03.038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For line spectrum estimation, we derive the maximum a posteriori probability
estimator where prior knowledge of frequencies is modeled probabilistically.
Since the spectrum is periodic, an appropriate distribution is the circular von
Mises distribution that can parameterize the entire range of prior certainty of
the frequencies. An efficient alternating projections method is used to solve
the resulting optimization problem. The estimator is evaluated numerically and
compared with other estimators and the Cram\'er-Rao bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5884</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5884</id><created>2013-06-25</created><updated>2014-01-01</updated><authors><author><keyname>Venkatesh</keyname><forenames>Sandeep</forenames></author><author><keyname>Patil</keyname><forenames>Meera V</forenames></author><author><keyname>Swamy</keyname><forenames>Nanditha</forenames></author></authors><title>Design of an Agent for Answering Back in Smart Phones</title><categories>cs.AI cs.HC cs.LG</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  erro</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of the paper is to design an agent which provides efficient
response to the caller when a call goes unanswered in smartphones. The agent
provides responses through text messages, email etc stating the most likely
reason as to why the callee is unable to answer a call. Responses are composed
taking into consideration the importance of the present call and the situation
the callee is in at the moment like driving, sleeping, at work etc. The agent
makes decisons in the compostion of response messages based on the patterns it
has come across in the learning environment. Initially the user helps the agent
to compose response messages. The agent associates this message to the percept
it recieves with respect to the environment the callee is in. The user may
thereafter either choose to make to response system automatic or choose to
recieve suggestions from the agent for responses messages and confirm what is
to be sent to the caller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5898</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5898</id><created>2013-06-25</created><authors><author><keyname>Lampesberger</keyname><forenames>Harald</forenames></author></authors><title>A Grammatical Inference Approach to Language-Based Anomaly Detection in
  XML</title><categories>cs.CR cs.DB</categories><comments>Paper accepted at First Int. Workshop on Emerging Cyberthreats and
  Countermeasures ECTCM 2013</comments><doi>10.1109/ARES.2013.90</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  False-positives are a problem in anomaly-based intrusion detection systems.
To counter this issue, we discuss anomaly detection for the eXtensible Markup
Language (XML) in a language-theoretic view. We argue that many XML-based
attacks target the syntactic level, i.e. the tree structure or element content,
and syntax validation of XML documents reduces the attack surface. XML offers
so-called schemas for validation, but in real world, schemas are often
unavailable, ignored or too general. In this work-in-progress paper we describe
a grammatical inference approach to learn an automaton from example XML
documents for detecting documents with anomalous syntax.
  We discuss properties and expressiveness of XML to understand limits of
learnability. Our contributions are an XML Schema compatible lexical datatype
system to abstract content in XML and an algorithm to learn visibly pushdown
automata (VPA) directly from a set of examples. The proposed algorithm does not
require the tree representation of XML, so it can process large documents or
streams. The resulting deterministic VPA then allows stream validation of
documents to recognize deviations in the underlying tree structure or
datatypes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5918</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5918</id><created>2013-06-25</created><updated>2015-03-20</updated><authors><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author><author><keyname>Xiao</keyname><forenames>Lin</forenames></author></authors><title>A Randomized Nonmonotone Block Proximal Gradient Method for a Class of
  Structured Nonlinear Programming</title><categories>math.OC cs.LG cs.NA math.NA stat.ML</categories><comments>The previous title was &quot;Randomized Block Coordinate Non-Monotone
  Gradient Method for a Class of Nonlinear Programming&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a randomized nonmonotone block proximal gradient (RNBPG) method
for minimizing the sum of a smooth (possibly nonconvex) function and a
block-separable (possibly nonconvex nonsmooth) function. At each iteration,
this method randomly picks a block according to any prescribed probability
distribution and solves typically several associated proximal subproblems that
usually have a closed-form solution, until a certain progress on objective
value is achieved. In contrast to the usual randomized block coordinate descent
method [23,20], our method has a nonmonotone flavor and uses variable stepsizes
that can partially utilize the local curvature information of the smooth
component of objective function. We show that any accumulation point of the
solution sequence of the method is a stationary point of the problem {\it
almost surely} and the method is capable of finding an approximate stationary
point with high probability. We also establish a sublinear rate of convergence
for the method in terms of the minimal expected squared norm of certain
proximal gradients over the iterations. When the problem under consideration is
convex, we show that the expected objective values generated by RNBPG converge
to the optimal value of the problem. Under some assumptions, we further
establish a sublinear and linear rate of convergence on the expected objective
values generated by a monotone version of RNBPG. Finally, we conduct some
preliminary experiments to test the performance of RNBPG on the
$\ell_1$-regularized least-squares problem and a dual SVM problem in machine
learning. The computational results demonstrate that our method substantially
outperforms the randomized block coordinate {\it descent} method with fixed or
variable stepsizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5920</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5920</id><created>2013-06-25</created><updated>2013-11-22</updated><authors><author><keyname>Beigi</keyname><forenames>Salman</forenames></author></authors><title>Sandwiched R\'enyi Divergence Satisfies Data Processing Inequality</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>13 pages, title changed, fixed typos, results unchanged, to appear in
  J. Math. Phys</comments><journal-ref>J. Math. Phys. 54, 122202 (2013)</journal-ref><doi>10.1063/1.4838855</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sandwiched (quantum) $\alpha$-R\'enyi divergence has been recently defined in
the independent works of Wilde et al. (arXiv:1306.1586) and M\&quot;uller-Lennert et
al (arXiv:1306.3142v1). This new quantum divergence has already found
applications in quantum information theory. Here we further investigate
properties of this new quantum divergence. In particular we show that
sandwiched $\alpha$-R\'enyi divergence satisfies the data processing inequality
for all values of $\alpha&gt; 1$. Moreover we prove that $\alpha$-Holevo
information, a variant of Holevo information defined in terms of sandwiched
$\alpha$-R\'enyi divergence, is super-additive. Our results are based on
H\&quot;older's inequality, the Riesz-Thorin theorem and ideas from the theory of
complex interpolation. We also employ Sion's minimax theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5930</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5930</id><created>2013-06-25</created><authors><author><keyname>Guimar&#xe3;es</keyname><forenames>Jos&#xe9; de Oliveira</forenames></author></authors><title>The Green Language</title><categories>cs.PL</categories><comments>220 pages</comments><acm-class>D.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Green is a statically-typed object-oriented language that separates subtyping
from inheritance. It supports garbage collection, classes as first-class
objects, parameterized classes, introspective reflection and a kind of run-time
metaobjects called shells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5960</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5960</id><created>2013-06-25</created><authors><author><keyname>Hartati</keyname><forenames>Sri</forenames></author><author><keyname>'Uyun</keyname><forenames>Shofwatul</forenames></author></authors><title>Computation of Diet Composition for Patients Suffering from Kidney and
  Urinary Tract Diseases with the Fuzzy Genetic System</title><categories>cs.AI</categories><comments>8 pages</comments><journal-ref>International Journal of Computer Applications (0975-8887)-Volume
  36, No.6, December 2011</journal-ref><doi>10.5120/4499-6350</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Determination of dietary food consumed a day for patients with diseases in
general, greatly affect the health of the body and the healing process, is no
exception for people with kidney disease and urinary tract. This paper presents
the determination of diet composition in the form of food subtance for people
with kidney and urinary tract diseases with a genetic fuzzy approach. This
approach combines fuzzy logic and genetic algorithms, which utilizing fuzzy
logic fuzzy tools and techniques to model the components of the genetic
algorithm and adapting genetic algorithm control parameters, with the aim of
improving system performance. The Mamdani fuzzy inference model and fuzzy rules
based on population parameters and generation are used to determine the
probability of crossover and mutation, and was using In this study, 400 food
survey data along with their substances was used as test material. From the
data, a varying amount of population is established. Each chromosome has 10
genes in which the value of each gene indicates the index number of foodstuffs
in the database. The fuzzy genetic approach produces 10 best food substance and
their compositions. The composition of these foods has nutritional value in
accordance with the number of calories needed by people with kidney and urinary
tract diseases by type of food.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5961</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5961</id><created>2013-06-25</created><updated>2013-06-26</updated><authors><author><keyname>Stehl&#xe9;</keyname><forenames>J.</forenames></author><author><keyname>Charbonnier</keyname><forenames>F.</forenames></author><author><keyname>Picard</keyname><forenames>T.</forenames></author><author><keyname>Cattuto</keyname><forenames>C.</forenames></author><author><keyname>Barrat</keyname><forenames>A.</forenames></author></authors><title>Gender homophily from spatial behavior in a primary school: a
  sociometric study</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Social Networks 35(4) 604-613 (2013)</journal-ref><doi>10.1016/j.socnet.2013.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate gender homophily in the spatial proximity of children (6 to 12
years old) in a French primary school, using time-resolved data on face-to-face
proximity recorded by means of wearable sensors. For strong ties, i.e., for
pairs of children who interact more than a defined threshold, we find
statistical evidence of gender preference that increases with grade. For weak
ties, conversely, gender homophily is negatively correlated with grade for
girls, and positively correlated with grade for boys. This different evolution
with grade of weak and strong ties exposes a contrasted picture of gender
homophily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5972</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5972</id><created>2013-06-25</created><authors><author><keyname>Beame</keyname><forenames>Paul</forenames></author><author><keyname>Koutris</keyname><forenames>Paraschos</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Communication Steps for Parallel Query Processing</title><categories>cs.DB</categories><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing a relational query $q$ on a large input
database of size $n$, using a large number $p$ of servers. The computation is
performed in rounds, and each server can receive only $O(n/p^{1-\varepsilon})$
bits of data, where $\varepsilon \in [0,1]$ is a parameter that controls
replication. We examine how many global communication steps are needed to
compute $q$. We establish both lower and upper bounds, in two settings. For a
single round of communication, we give lower bounds in the strongest possible
model, where arbitrary bits may be exchanged; we show that any algorithm
requires $\varepsilon \geq 1-1/\tau^*$, where $\tau^*$ is the fractional vertex
cover of the hypergraph of $q$. We also give an algorithm that matches the
lower bound for a specific class of databases. For multiple rounds of
communication, we present lower bounds in a model where routing decisions for a
tuple are tuple-based. We show that for the class of tree-like queries there
exists a tradeoff between the number of rounds and the space exponent
$\varepsilon$. The lower bounds for multiple rounds are the first of their
kind. Our results also imply that transitive closure cannot be computed in O(1)
rounds of communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5977</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5977</id><created>2013-06-24</created><updated>2015-03-16</updated><authors><author><keyname>Hutchinson</keyname><forenames>Maxwell</forenames></author><author><keyname>Widom</keyname><forenames>Michael</forenames></author></authors><title>Enumeration of octagonal tilings</title><categories>math.CO cs.DM math-ph math.MP</categories><journal-ref>Theoretical Computer Science (2015), pp. 40-50</journal-ref><doi>10.1016/j.tcs.2015.03.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random tilings are interesting as idealizations of atomistic models of
quasicrystals and for their connection to problems in combinatorics and
algorithms. Of particular interest is the tiling entropy density, which
measures the relation of the number of distinct tilings to the number of
constituent tiles. Tilings by squares and 45 degree rhombi receive special
attention as presumably the simplest model that has not yet been solved exactly
in the thermodynamic limit. However, an exact enumeration formula can be
evaluated for tilings in finite regions with fixed boundaries. We implement
this algorithm in an efficient manner, enabling the investigation of larger
regions of parameter space than previously were possible. Our new results
appear to yield monotone increasing and decreasing lower and upper bounds on
the fixed boundary entropy density that converge toward S = 0.36021(3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5982</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5982</id><created>2013-06-25</created><authors><author><keyname>J</keyname><forenames>Menaka Gandhi.</forenames></author><author><keyname>Gayathri</keyname><forenames>K. S.</forenames></author></authors><title>Activity Modeling in Smart Home using High Utility Pattern Mining over
  Data Streams</title><categories>cs.AI cs.DB</categories><comments>This research paper consists of 7 pages, 7 figures and 4 algorithms</comments><journal-ref>&quot;Interactive mining of high utility patterns over data streams&quot;,
  Elsevier, Vol. 39, No. 15, 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Smart home technology is a better choice for the people to care about
security, comfort and power saving as well. It is required to develop
technologies that recognize the Activities of Daily Living (ADLs) of the
residents at home and detect the abnormal behavior in the individual's
patterns. Data mining techniques such as Frequent pattern mining (FPM), High
Utility Pattern (HUP) Mining were used to find those activity patterns from the
collected sensor data. But applying the above technique for Activity
Recognition from the temporal sensor data stream is highly complex and
challenging task. So, a new approach is proposed for activity recognition from
sensor data stream which is achieved by constructing Frequent Pattern Stream
tree (FPS - tree). FPS is a sliding window based approach to discover the
recent activity patterns over time from data streams. The proposed work aims at
identifying the frequent pattern of the user from the sensor data streams which
are later modeled for activity recognition. The proposed FPM algorithm uses a
data structure called Linked Sensor Data Stream (LSDS) for storing the sensor
data stream information which increases the efficiency of frequent pattern
mining algorithm through both space and time. The experimental results show the
efficiency of the proposed algorithm and this FPM is further extended for
applying for power efficiency using HUP to detect the high usage of power
consumption of residents at smart home.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.5998</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.5998</id><created>2013-06-25</created><authors><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Lakin</keyname><forenames>Matthew R.</forenames></author><author><keyname>Stefanovic</keyname><forenames>Darko</forenames></author></authors><title>DNA Reservoir Computing: A Novel Molecular Computing Approach</title><categories>cs.NE cs.ET nlin.AO nlin.CD physics.bio-ph</categories><comments>14 pages, 7 figure</comments><msc-class>q-bio.BM, q-bio.MN, q-bio.NC, nlin.AO, nlin.CD</msc-class><journal-ref>D. Soloveichik and B. Yurke (Eds.): DNA 2013, LNCS 8141, pp.
  76--89</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel molecular computing approach based on reservoir computing.
In reservoir computing, a dynamical core, called a reservoir, is perturbed with
an external input signal while a readout layer maps the reservoir dynamics to a
target output. Computation takes place as a transformation from the input space
to a high-dimensional spatiotemporal feature space created by the transient
dynamics of the reservoir. The readout layer then combines these features to
produce the target output. We show that coupled deoxyribozyme oscillators can
act as the reservoir. We show that despite using only three coupled
oscillators, a molecular reservoir computer could achieve 90% accuracy on a
benchmark temporal problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6017</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6017</id><created>2013-06-25</created><authors><author><keyname>Librino</keyname><forenames>Federico</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Relaying Strategies for Uplink in Wireless Cellular Networks</title><categories>cs.NI</categories><comments>30 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the impact of relays on the uplink performance of
FDMA cellular networks. We focus our analysis on Decode and Forward techniques,
with the aim of measuring the improvements which can be achieved in terms of
throughput and energy saving. We apply a stochastic geometry based approach to
a scenario with inter-cell interference and reuse factor equal to 1. The first
goal of this work is to observe what is the impact of various relay features,
such as transmission power, location and antenna pattern, when a half-duplex
constraint is imposed. The second goal is to determine how much relaying can be
beneficial also for users who are not at the cell edge, and who can therefore
use a direct link towards the base station. We show that if more refined
decoding techniques, such as Successive Interference Cancellation and
Superposition Coding, are properly used, considerable gains can be obtained for
these mobiles as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6020</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6020</id><created>2013-06-25</created><authors><author><keyname>Primmer</keyname><forenames>Robert</forenames></author><author><keyname>D'Halluin</keyname><forenames>Carl</forenames></author></authors><title>Collision and Preimage Resistance of the Centera Content Address</title><categories>cs.CR</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centera uses cryptographic hash functions as a means of addressing stored
objects, thus creating a new class of data storage referred to as CAS (content
addressed storage). Such hashing serves the useful function of providing a
means of uniquely identifying data and providing a global handle to that data,
referred to as the Content Address or CA. However, such a model begs the
question: how certain can one be that a given CA is indeed unique?
  In this paper we describe fundamental concepts of cryptographic hash
functions, such as collision resistance, pre-image resistance, and
second-preimage resistance. We then map these properties to the MD5 and SHA-256
hash algorithms, which are used to generate the Centera content address.
Finally, we present a proof of the collision resistance of the Centera Content
Address.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6023</identifier>
 <datestamp>2013-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6023</id><created>2013-06-25</created><updated>2013-08-21</updated><authors><author><keyname>Dell'Amico</keyname><forenames>Matteo</forenames></author></authors><title>A Simulator for Data-Intensive Job Scheduling</title><categories>cs.DC</categories><report-no>EURECOM RR-13-282</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the fact that size-based schedulers can give excellent results in
terms of both average response times and fairness, data-intensive computing
execution engines generally do not employ size-based schedulers, mainly because
of the fact that job size is not known a priori.
  In this work, we perform a simulation-based analysis of the performance of
size-based schedulers when they are employed with the workload of typical
data-intensive schedules and with approximated size estimations. We show
results that are very promising: even when size estimation is very imprecise,
response times of size-based schedulers can be definitely smaller than those of
simple scheduling techniques such as processor sharing or FIFO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6029</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6029</id><created>2013-06-25</created><updated>2014-06-22</updated><authors><author><keyname>Shafarenko</keyname><forenames>Alex</forenames></author></authors><title>AstraKahn: A Coordination Language for Streaming Networks</title><categories>cs.PL</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a preliminary version of the language report. It contains key
definitions, specifications and some examples, but lacks completeness. The full
document will include Chapter 3 (Data and Instrumentation Layer) and will
comprise an appendix giving the complete syntax and some whole program
examples. The purpose of the present document is to fix the concepts and major
features of the language and to enable the production of the definition
document that is required for implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6032</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6032</id><created>2013-06-25</created><authors><author><keyname>Dunfield</keyname><forenames>Joshua</forenames></author><author><keyname>Krishnaswami</keyname><forenames>Neelakantan R.</forenames></author></authors><title>Complete and Easy Bidirectional Typechecking for Higher-Rank
  Polymorphism</title><categories>cs.PL</categories><comments>13 pages + 78-page appendix, to appear in International Conference on
  Functional Programming (ICFP) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidirectional typechecking, in which terms either synthesize a type or are
checked against a known type, has become popular for its scalability (unlike
Damas-Milner type inference, bidirectional typing remains decidable even for
very expressive type systems), its error reporting, and its relative ease of
implementation. Following design principles from proof theory, bidirectional
typing can be applied to many type constructs. The principles underlying a
bidirectional approach to polymorphism, however, are less obvious. We give a
declarative, bidirectional account of higher-rank polymorphism, grounded in
proof theory; this calculus enjoys many properties such as eta-reduction and
predictability of annotations. We give an algorithm for implementing the
declarative system; our algorithm is remarkably simple and well-behaved,
despite being both sound and complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6041</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6041</id><created>2013-06-25</created><authors><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Teuscher</keyname><forenames>Christof</forenames></author><author><keyname>Gulbahce</keyname><forenames>Natali</forenames></author><author><keyname>Rohlf</keyname><forenames>Thimo</forenames></author></authors><title>Learning, Generalization, and Functional Entropy in Random Automata
  Networks</title><categories>cs.NE cond-mat.dis-nn nlin.AO nlin.CD physics.bio-ph</categories><msc-class>nlin.AO, nlin.CD, q-bio.NC, physics.bio-ph, cond-mat.dis-nn</msc-class><acm-class>C.1.3; I.2.6; I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown \citep{broeck90:physicalreview,patarnello87:europhys} that
feedforward Boolean networks can learn to perform specific simple tasks and
generalize well if only a subset of the learning examples is provided for
learning. Here, we extend this body of work and show experimentally that random
Boolean networks (RBNs), where both the interconnections and the Boolean
transfer functions are chosen at random initially, can be evolved by using a
state-topology evolution to solve simple tasks. We measure the learning and
generalization performance, investigate the influence of the average node
connectivity $K$, the system size $N$, and introduce a new measure that allows
to better describe the network's learning and generalization behavior. We show
that the connectivity of the maximum entropy networks scales as a power-law of
the system size $N$. Our results show that networks with higher average
connectivity $K$ (supercritical) achieve higher memorization and partial
generalization. However, near critical connectivity, the networks show a higher
perfect generalization on the even-odd task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6042</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6042</id><created>2013-06-25</created><updated>2014-04-18</updated><authors><author><keyname>Nadakuditi</keyname><forenames>Raj Rao</forenames></author></authors><title>OptShrink: An algorithm for improved low-rank signal matrix denoising by
  optimal, data-driven singular value shrinkage</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>Published version. The algorithm can be downloaded from
  http://www.eecs.umich.edu/~rajnrao/optshrink</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 6, pp. 1-17,
  May 2014</journal-ref><doi>10.1109/TIT.2014.2311661</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The truncated singular value decomposition (SVD) of the measurement matrix is
the optimal solution to the_representation_ problem of how to best approximate
a noisy measurement matrix using a low-rank matrix. Here, we consider the
(unobservable)_denoising_ problem of how to best approximate a low-rank signal
matrix buried in noise by optimal (re)weighting of the singular vectors of the
measurement matrix. We exploit recent results from random matrix theory to
exactly characterize the large matrix limit of the optimal weighting
coefficients and show that they can be computed directly from data for a large
class of noise models that includes the i.i.d. Gaussian noise case.
  Our analysis brings into sharp focus the shrinkage-and-thresholding form of
the optimal weights, the non-convex nature of the associated shrinkage function
(on the singular values) and explains why matrix regularization via singular
value thresholding with convex penalty functions (such as the nuclear norm)
will always be suboptimal. We validate our theoretical predictions with
numerical simulations, develop an implementable algorithm (OptShrink) that
realizes the predicted performance gains and show how our methods can be used
to improve estimation in the setting where the measured matrix has missing
entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6047</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6047</id><created>2013-06-25</created><updated>2013-08-13</updated><authors><author><keyname>Power</keyname><forenames>Russell</forenames></author><author><keyname>Rubinsteyn</keyname><forenames>Alex</forenames></author></authors><title>How fast can we make interpreted Python?</title><categories>cs.PL</categories><comments>Tech Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Python is a popular dynamic language with a large part of its appeal coming
from powerful libraries and extension modules. These augment the language and
make it a productive environment for a wide variety of tasks, ranging from web
development (Django) to numerical analysis (NumPy). Unfortunately, Python's
performance is quite poor when compared to modern implementations of languages
such as Lua and JavaScript.
  Why does Python lag so far behind these other languages? As we show, the very
same API and extension libraries that make Python a powerful language also make
it very difficult to efficiently execute. Given that we want to retain access
to the great extension libraries that already exist for Python, how fast can we
make it?
  To evaluate this, we designed and implemented Falcon, a high-performance
bytecode interpreter fully compatible with the standard CPython interpreter.
Falcon applies a number of well known optimizations and introduces several new
techniques to speed up execution of Python bytecode. In our evaluation, we
found Falcon an average of 25% faster than the standard Python interpreter on
most benchmarks and in some cases about 2.5X faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6054</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6054</id><created>2013-06-25</created><authors><author><keyname>Ganesh</keyname><forenames>Vijay</forenames></author><author><keyname>Minnes</keyname><forenames>Mia</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author><author><keyname>Rinard</keyname><forenames>Martin</forenames></author></authors><title>(Un)Decidability Results for Word Equations with Length and Regular
  Expression Constraints</title><categories>cs.LO</categories><comments>Invited Paper at ADDCT Workshop 2013 (co-located with CADE 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove several decidability and undecidability results for the
satisfiability and validity problems for languages that can express solutions
to word equations with length constraints. The atomic formulas over this
language are equality over string terms (word equations), linear inequality
over the length function (length constraints), and membership in regular sets.
These questions are important in logic, program analysis, and formal
verification. Variants of these questions have been studied for many decades by
mathematicians. More recently, practical satisfiability procedures (aka SMT
solvers) for these formulas have become increasingly important in the context
of security analysis for string-manipulating programs such as web applications.
  We prove three main theorems. First, we give a new proof of undecidability
for the validity problem for the set of sentences written as a forall-exists
quantifier alternation applied to positive word equations. A corollary of this
undecidability result is that this set is undecidable even with sentences with
at most two occurrences of a string variable. Second, we consider Boolean
combinations of quantifier-free formulas constructed out of word equations and
length constraints. We show that if word equations can be converted to a solved
form, a form relevant in practice, then the satisfiability problem for Boolean
combinations of word equations and length constraints is decidable. Third, we
show that the satisfiability problem for quantifier-free formulas over word
equations in regular solved form, length constraints, and the membership
predicate over regular expressions is also decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6056</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6056</id><created>2013-06-25</created><authors><author><keyname>Van Nguyen</keyname><forenames>Thuy</forenames></author><author><keyname>Nosratinia</keyname><forenames>Aria</forenames></author><author><keyname>Divsalar</keyname><forenames>Dariush</forenames></author></authors><title>Rate-Compatible Protograph-based LDPC Codes for Inter-Symbol
  Interference Channels</title><categories>cs.IT math.IT</categories><comments>4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter produces a family of rate-compatible protograph-based LDPC codes
approaching the independent and uniformly distributed (i.u.d.) capacity of
inter-symbol interference (ISI) channels. This problem is highly nontrivial due
to the joint design of structured (protograph-based) LDPC codes and the state
structure of ISI channels. We describe a method to design nested high-rate
protograph codes by adding variable nodes to the protograph of a lower rate
code. We then design a family of rate-compatible protograph codes using the
extension method. The resulting protograph codes have iterative decoding
thresholds close to the i.u.d. capacity. Our results are supported by numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6058</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6058</id><created>2013-06-25</created><updated>2013-06-26</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Chen</keyname><forenames>Shaohua</forenames></author><author><keyname>Hedjam</keyname><forenames>Rachid</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>A maximal-information color to gray conversion method for document
  images: Toward an optimal grayscale representation for document image
  binarization</title><categories>cs.CV</categories><comments>36 page, the uncompressed version is available on Synchromedia
  website</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel method to convert color/multi-spectral images to gray-level images is
introduced to increase the performance of document binarization methods. The
method uses the distribution of the pixel data of the input document image in a
color space to find a transformation, called the dual transform, which balances
the amount of information on all color channels. Furthermore, in order to
reduce the intensity variations on the gray output, a color reduction
preprocessing step is applied. Then, a channel is selected as the gray value
representation of the document image based on the homogeneity criterion on the
text regions. In this way, the proposed method can provide a
luminance-independent contrast enhancement. The performance of the method is
evaluated against various images from two databases, the ICDAR'03 Robust
Reading, the KAIST and the DIBCO'09 datasets, subjectively and objectively with
promising results. The ground truth images for the images from the ICDAR'03
Robust Reading dataset have been created manually by the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6078</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6078</id><created>2013-06-25</created><authors><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author><author><keyname>Sudhof</keyname><forenames>Moritz</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author><author><keyname>Potts</keyname><forenames>Christopher</forenames></author></authors><title>A Computational Approach to Politeness with Application to Social
  Factors</title><categories>cs.CL cs.SI physics.soc-ph</categories><comments>To appear at ACL 2013. 10pp, 3 fig. Data and other info available at
  http://www.mpi-sws.org/~cristian/Politeness.html</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a computational framework for identifying linguistic aspects of
politeness. Our starting point is a new corpus of requests annotated for
politeness, which we use to evaluate aspects of politeness theory and to
uncover new interactions between politeness markers and context. These findings
guide our construction of a classifier with domain-independent lexical and
syntactic features operationalizing key components of politeness theory, such
as indirection, deference, impersonalization and modality. Our classifier
achieves close to human performance and is effective across domains. We use our
framework to study the relationship between politeness and social power,
showing that polite Wikipedia editors are more likely to achieve high status
through elections, but, once elevated, they become less polite. We see a
similar negative correlation between politeness and power on Stack Exchange,
where users at the top of the reputation scale are less polite than those at
the bottom. Finally, we apply our classifier to a preliminary analysis of
politeness variation by gender and community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6109</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6109</id><created>2013-06-25</created><updated>2015-01-07</updated><authors><author><keyname>Anantharamu</keyname><forenames>Lakshmi</forenames></author><author><keyname>Chlebus</keyname><forenames>Bogdan S.</forenames></author></authors><title>Broadcasting in Ad Hoc Multiple Access Channels</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study dynamic broadcasting in multiple access channels in adversarial
settings. There is an unbounded supply of anonymous stations attached to a
synchronous channel. There is an adversary who injects packets into stations to
be broadcast on the channel. The adversary is restricted by injection rate,
burstiness, and by how many passive stations can be simultaneously activated by
providing them with packets. We consider deterministic distributed broadcast
algorithms, which are further categorized by their properties. We investigate
for which injection rates can algorithms attain bounded packet latency, when
adversaries are restricted to be able to activate at most one station per
round. The rates of algorithms we present make the increasing sequence
consisting of 1/3, 3/8 and 1/2, reflecting the additional features of
algorithms. We show that injection rate 3/4 cannot be handled with bounded
packet latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6111</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6111</id><created>2013-06-25</created><updated>2013-08-23</updated><authors><author><keyname>Darmon</keyname><forenames>David</forenames></author><author><keyname>Sylvester</keyname><forenames>Jared</forenames></author><author><keyname>Girvan</keyname><forenames>Michelle</forenames></author><author><keyname>Rand</keyname><forenames>William</forenames></author></authors><title>Understanding the Predictive Power of Computational Mechanics and Echo
  State Networks in Social Media</title><categories>cs.SI cs.LG physics.soc-ph stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a large amount of interest in understanding users of social media in
order to predict their behavior in this space. Despite this interest, user
predictability in social media is not well-understood. To examine this
question, we consider a network of fifteen thousand users on Twitter over a
seven week period. We apply two contrasting modeling paradigms: computational
mechanics and echo state networks. Both methods attempt to model the behavior
of users on the basis of their past behavior. We demonstrate that the behavior
of users on Twitter can be well-modeled as processes with self-feedback. We
find that the two modeling approaches perform very similarly for most users,
but that they differ in performance on a small subset of the users. By
exploring the properties of these performance-differentiated users, we
highlight the challenges faced in applying predictive models to dynamic social
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6115</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6115</id><created>2013-06-25</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author><author><keyname>Rue&#xdf;</keyname><forenames>Harald</forenames></author><author><keyname>Sch&#xe4;tz</keyname><forenames>Bernhard</forenames></author></authors><title>On Behavioral Types for OSGi: From Theory to Implementation</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1302.5175</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents our work on behavioral types for OSGi component systems.
It extends previously published work and presents features and details that
have not yet been published. In particular, we cover a discussion on behavioral
types in general, and Eclipse based implementation work on behavioral types .
The implementation work covers: editors, means for comparing types at
development and runtime, a tool connection to resolve incompatibilities, and an
AspectJ based infrastructure to ensure behavioral type correctness at runtime
of a system. Furthermore, the implementation comprises various auxiliary
operations. We present some evaluation work based on examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6116</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6116</id><created>2013-06-25</created><authors><author><keyname>Dasarathan</keyname><forenames>Sivaraman</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author></authors><title>Distributed Estimation and Detection with Bounded Transmissions over
  Gaussian Multiple Access Channels</title><categories>cs.DC cs.IT math.IT</categories><comments>24 Pages, 7 Figures, Will be submitted to an IEEE journal</comments><doi>10.1109/TSP.2014.2327573</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed inference scheme which uses bounded transmission functions over
a Gaussian multiple access channel is considered. When the sensor measurements
are decreasingly reliable as a function of the sensor index, the conditions on
the transmission functions under which consistent estimation and reliable
detection are possible is characterized. For the distributed estimation
problem, an estimation scheme that uses bounded transmission functions is
proved to be strongly consistent provided that the variance of the noise
samples are bounded and that the transmission function is one-to-one. The
proposed estimation scheme is compared with the amplify-and-forward technique
and its robustness to impulsive sensing noise distributions is highlighted. In
contrast to amplify-and-forward schemes, it is also shown that bounded
transmissions suffer from inconsistent estimates if the sensing noise variance
goes to infinity. For the distributed detection problem, similar results are
obtained by studying the deflection coefficient. Simulations corroborate our
analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6122</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6122</id><created>2013-06-25</created><authors><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Downlink Rate Distribution in Heterogeneous Cellular Networks under
  Generalized Cell Selection</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering both small-scale fading and long-term shadowing, we characterize
the downlink rate distribution at a typical user equipment (UE) in a
heterogeneous cellular network (HetNet), where shadowing, following any general
distribution, impacts cell selection while fading does not. Prior work either
ignores the impact of channel randomness on cell selection or lumps all the
sources of randomness into a single variable, with cell selection based on the
instantaneous signal strength, which is unrealistic. As an application of the
results, we study the impact of shadowing on load balancing in terms of the
optimal per-tier selection bias needed for rate maximization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6125</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6125</id><created>2013-06-26</created><updated>2014-03-12</updated><authors><author><keyname>Banerji</keyname><forenames>Sourangsu</forenames></author></authors><title>Design and Implementation of an Unmanned Vehicle using a GSM Network
  with Microcontrollers</title><categories>cs.SY</categories><comments>8 pages, 6 figures, 4 tables. A different version of the paper at
  arXiv:1306.5296</comments><journal-ref>International Journal of Science, Engineering and Technology
  Research (IJSETR), Volume 2, Issue 2, February 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now-a-days, a lot of research is being carried out in the development of USVs
(Unmanned surface vehicles), UAVs (Unmanned Aerial Vehicles) etc. Now in case
of USVs generally, we have seen that wireless controlled vehicles use RF
circuits which suffer from many drawbacks such as limited working range,
limited frequency range and limited control. Moreover shooting infrared
outdoors on a bright sunny day is often problematic, since sunlight can
interfere with the infrared signal. Use of a GSM network (in the form of a
mobile phone, a cordless phone) for robotic control can overcome these
limitations. It provides the advantages of robust control, working range as
large as the coverage area of the service provider in comparison with that of
an IR system, no interference with other controllers. This paper presents a
Global System for Mobile Telecommunication (GSM) network based system which can
be used to remotely send streams of 4 bit data for control of USVs.
Furthermore, this paper describes the usage of the Dual Tone Multi-Frequency
(DTMF) function of the phone, and builds a microcontroller based circuit to
control the vehicle to demonstrate wireless data communication. Practical
result obtained showed an appreciable degree of accuracy of the system and
friendliness through the use of a microcontroller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6130</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6130</id><created>2013-06-26</created><authors><author><keyname>Bishop</keyname><forenames>Robert</forenames><suffix>Jr</suffix></author></authors><title>Competency Tracking for English as a Second or Foreign Language Learners</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  My system utilizes the outcomes feature found in Moodle and other learning
content management systems (LCMSs) to keep track of where students are in terms
of what language competencies they have mastered and the competencies they need
to get where they want to go. These competencies are based on the Common
European Framework for (English) Language Learning. This data can be available
for everyone involved with a given student's progress (e.g. educators, parents,
supervisors and the students themselves). A given student's record of past
accomplishments can also be meshed with those of his classmates. Not only are a
student's competencies easily seen and tracked, educators can view competencies
of a group of students that were achieved prior to enrollment in the class.
This should make curriculum decision making easier and more efficient for
educators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6133</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6133</id><created>2013-06-26</created><updated>2014-03-17</updated><authors><author><keyname>Traversa</keyname><forenames>Fabio Lorenzo</forenames></author><author><keyname>Bonani</keyname><forenames>Fabrizio</forenames></author><author><keyname>Pershin</keyname><forenames>Yuriy V.</forenames></author><author><keyname>Di Ventra</keyname><forenames>Massimiliano</forenames></author></authors><title>Dynamic Computing Random Access Memory</title><categories>cs.ET cond-mat.mes-hall cs.AR</categories><journal-ref>Nanotechnology, vol. 25, is. 8, pg. 285201 (10pp), year 2014</journal-ref><doi>10.1088/0957-4484/25/28/285201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present von Neumann computing paradigm involves a significant amount of
information transfer between a central processing unit (CPU) and memory, with
concomitant limitations in the actual execution speed. However, it has been
recently argued that a different form of computation, dubbed memcomputing
[Nature Physics, 9, 200-202 (2013)] and inspired by the operation of our brain,
can resolve the intrinsic limitations of present day architectures by allowing
for computing and storing of information on the same physical platform. Here we
show a simple and practical realization of memcomputing that utilizes
easy-to-build memcapacitive systems. We name this architecture Dynamic
Computing Random Access Memory (DCRAM). We show that DCRAM provides
massively-parallel and polymorphic digital logic, namely it allows for
different logic operations with the same architecture, by varying only the
control signals. In addition, by taking into account realistic parameters, its
energy expenditures can be as low as a few fJ per operation. DCRAM is fully
compatible with CMOS technology, can be realized with current fabrication
facilities, and therefore can really serve as an alternative to the present
computing technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6141</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6141</id><created>2013-06-26</created><authors><author><keyname>Ciuonzo</keyname><forenames>D.</forenames></author><author><keyname>Papa</keyname><forenames>G.</forenames></author><author><keyname>Romano</keyname><forenames>G.</forenames></author><author><keyname>Rossi</keyname><forenames>P. Salvo</forenames></author><author><keyname>Willett</keyname><forenames>P. K.</forenames></author></authors><title>One-bit Decentralized Detection with a Rao Test for Multisensor Fusion</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Signal Processing Letters</comments><journal-ref>IEEE Signal Processing Letters, vol. 20, no. 9, pp. 861-864,
  September 2013</journal-ref><doi>10.1109/LSP.2013.2271847</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we propose the Rao test as a simpler alternative to the
generalized likelihood ratio test (GLRT) for multisensor fusion. We consider
sensors observing an unknown deterministic parameter with symmetric and
unimodal noise. A decision fusion center (DFC) receives quantized sensor
observations through error-prone binary symmetric channels and makes a global
decision. We analyze the optimal quantizer thresholds and we study the
performance of the Rao test in comparison to the GLRT. Also, a theoretical
comparison is made and asymptotic performance is derived in a scenario with
homogeneous sensors. All the results are confirmed through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6168</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6168</id><created>2013-06-26</created><updated>2013-10-21</updated><authors><author><keyname>Courcelle</keyname><forenames>Bruno</forenames><affiliation>LaBRI, IUF</affiliation></author></authors><title>Clique-width and edge contraction</title><categories>cs.DM cs.LO</categories><comments>Information Processinhgs Letters 2013, In press</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that edge contractions do not preserve the property that a set of
graphs has bounded clique-width. This property is preserved by contractions of
edges, one end of which is a vertex of degree 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6169</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6169</id><created>2013-06-26</created><authors><author><keyname>Li</keyname><forenames>C.</forenames></author><author><keyname>Zhang</keyname><forenames>J.</forenames></author><author><keyname>Letaief</keyname><forenames>K. B.</forenames></author></authors><title>Throughput and Energy Efficiency Analysis of Small Cell Networks with
  Multi-antenna Base Stations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cell networks have recently been proposed as an important evolution
path for the next-generation cellular networks. However, with more and more
irregularly deployed base stations (BSs), it is becoming increasingly difficult
to quantify the achievable network throughput or energy efficiency. In this
paper, we develop an analytical framework for downlink performance evaluation
of small cell networks, based on a random spatial network model, where BSs and
users are modeled as two independent spatial Poisson point processes. A new
simple expression of the outage probability is derived, which is analytically
tractable and is especially useful with multi-antenna transmissions. This new
result is then applied to evaluate the network throughput and energy
efficiency. It is analytically shown that deploying more BSs or more BS
antennas can always increase the network throughput, but the performance gain
critically depends on the BS-user density ratio and the number of BS antennas.
On the other hand, increasing the BS density or the number of transmit antennas
will first increase and then decrease the energy efficiency if different
components of BS power consumption satisfy certain conditions, and the optimal
BS density and the optimal number of BS antennas can be found. Otherwise, the
energy efficiency will always decrease. Simulation results shall demonstrate
that our conclusions based on the random network model are general and also
hold in a regular grid-based model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6189</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6189</id><created>2013-06-26</created><authors><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Scaling Up Robust MDPs by Reinforcement Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider large-scale Markov decision processes (MDPs) with parameter
uncertainty, under the robust MDP paradigm. Previous studies showed that robust
MDPs, based on a minimax approach to handle uncertainty, can be solved using
dynamic programming for small to medium sized problems. However, due to the
&quot;curse of dimensionality&quot;, MDPs that model real-life problems are typically
prohibitively large for such approaches. In this work we employ a reinforcement
learning approach to tackle this planning problem: we develop a robust
approximate dynamic programming method based on a projected fixed point
equation to approximately solve large scale robust MDPs. We show that the
proposed method provably succeeds under certain technical conditions, and
demonstrate its effectiveness through simulation of an option pricing problem.
To the best of our knowledge, this is the first attempt to scale up the robust
MDPs paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6192</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6192</id><created>2013-06-26</created><authors><author><keyname>Swierczewski</keyname><forenames>Lukasz</forenames></author></authors><title>Akceleracja obliczen algebry liniowej z wykorzystaniem masywnie
  rownoleglych, wielordzeniowych procesorow GPU</title><categories>cs.DC</categories><comments>10 pages in polish</comments><journal-ref>Prace Naukowe Studentow 2; Wyzwania XXI Wieku, Przyroda, Technika,
  Czlowiek; Materialy z III Ogolnopolskiej Sesji Kol Naukowych, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents the aspect of use of modern graphics accelerators
supporting CUDA technology for high-performance computing in the field of
linear algebra. Fully programmable graphic cards have been available for
several years for both ordinary users and research units. They provide the
capability of performing virtually any computing with high performance, which
is often beyond the reach of conventional CPUs. GPU architecture, also in case
of classical problems of linear algebra which is the basis for many
calculations, can bring many benefits to the developer. Performance increase,
observed during matrix multiplication on nVidia Tesla C2050, was more than
thousandfold compared to ordinary CPU, resulting in drastic reduction of
latency for some of the results, thus the cost of obtaining them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6193</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6193</id><created>2013-06-26</created><authors><author><keyname>Sena</keyname><forenames>P. Vasanth</forenames></author></authors><title>An Optimal Heuristic for Sum of All Prime Numbers Logic for Large Inputs
  using RAPTOR</title><categories>cs.DS</categories><comments>7 pages, 7 figures, 1 algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An optimal heuristic logic is an effective method for finding the sum of all
prime numbers up to a given number. This paper presents different approaches,
namely, general method and optimal method which facilitate to compare the
results and draw the optimal solution. The method adopted is to know the number
of symbols evaluated in each logic, construct better approaches based on
heuristics, proposals and implementations of human sequential development using
Rapid algorithmic prototyping tool for ordered reasoning (RAPTOR). In
traditional approach, task is complex in point of time and space; however, this
method reduces these prime factors by applying simple mathematical theorems and
heuristics. This model effectively works with large numeric inputs. It has been
tested on RAPTOR with flow charts, results indicates that algorithms are fast,
effective and scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6194</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6194</id><created>2013-06-26</created><authors><author><keyname>Taeib</keyname><forenames>Adel</forenames></author><author><keyname>Ltaeif</keyname><forenames>Ali</forenames></author><author><keyname>Chaari</keyname><forenames>Abdelkader</forenames></author></authors><title>A PSO Approach for Optimum Design of Multivariable PID Controller for
  nonlinear systems</title><categories>cs.SY</categories><comments>5 pages, 3 figures, 4 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The aim of this research is to design a PID Controller using particle swarm
optimization (PSO) algorithm for multiple-input multiple output (MIMO)
Takagi-Sugeno fuzzy model. The conventional gain tuning of PID controller (such
as Ziegler-Nichols (ZN) method) usually produces a big overshoot, and therefore
modern heuristics approach such as PSO are employed to enhance the capability
of traditional techniques. However, due to the computational efficiency, only
PSO will be used in this paper. The results show the advantage of the PID
tuning using PSO-based optimization approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6198</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6198</id><created>2013-06-26</created><authors><author><keyname>Santos</keyname><forenames>Augusto</forenames></author><author><keyname>Moura</keyname><forenames>Jos&#xe9; M. F.</forenames></author><author><keyname>Xavier</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Emergent Behavior in Multipartite Large Networks: Multi-virus Epidemics</title><categories>cs.SI physics.soc-ph</categories><comments>27 pages, submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemics in large complete networks is well established. In contrast, we
consider epidemics in non-complete networks. We establish the fluid limit
macroscopic dynamics of a multi-virus spread over a multipartite network as the
number of nodes at each partite or island grows large. The virus spread follows
a peer-to-peer random rule of infection in line with the Harris contact
process. The model conforms to an SIS (susceptible-infected-susceptible) type,
where a node is either infected or it is healthy and prone to be infected. The
local (at node level) random infection model induces the emergence of
structured dynamics at the macroscale. Namely, we prove that, as the
multipartite network grows large, the normalized Markov jump vector process
$\left(\bar{\mathbf{Y}}^\mathbf{N}(t)\right) =
\left(\bar{Y}_1^\mathbf{N}(t),\ldots, \bar{Y}_M^\mathbf{N}(t)\right)$
collecting the fraction of infected nodes at each island $i=1,\ldots,M$,
converges weakly (with respect to the Skorokhod topology on the space of
\emph{c\`{a}dl\`{a}g} sample paths) to the solution of an $M$-dimensional
vector nonlinear coupled ordinary differential equation. In the case of
multi-virus diffusion with $K\in\mathbb{N}$ distinct strains of virus, the
Markov jurmp matrix process $\left(\bar{\mathbf{Y}}^\mathbf{N}(t)\right)$,
stacking the fraction of nodes infected with virus type $j$, $j=1,\ldots,K$, at
each island $i=1,\ldots,M$, converges weakly as well to the solution of a
$\left(K\times M\right)$-dimensional vector differential equation that is also
characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6203</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6203</id><created>2013-06-26</created><updated>2013-10-14</updated><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>A Derivation of the Asymptotic Random-Coding Prefactor</title><categories>cs.IT math.IT</categories><comments>Published at Allerton Conference 2013. (v3) Final version uploaded</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the subexponential prefactor to the random-coding bound
for a given rate. Using a refinement of Gallager's bounding techniques, an
alternative proof of a recent result by Altu\u{g} and Wagner is given, and the
result is extended to the setting of mismatched decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6206</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6206</id><created>2013-06-26</created><authors><author><keyname>Figueredo</keyname><forenames>Grazziela P.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Investigating Immune System Aging: System Dynamics and Agent-Based
  Modeling</title><categories>cs.CE q-bio.QM</categories><comments>Proceedings of the International Summer Computer Simulation
  Conference 2010, 174-181, 2010. arXiv admin note: text overlap with
  arXiv:1306.2898</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  System dynamics and agent based simulation models can both be used to model
and understand interactions of entities within a population. Our modeling work
presented here is concerned with understanding the suitability of the different
types of simulation for the immune system aging problems and comparing their
results. We are trying to answer questions such as: How fit is the immune
system given a certain age? Would an immune boost be of therapeutic value, e.g.
to improve the effectiveness of a simultaneous vaccination? Understanding the
processes of immune system aging and degradation may also help in development
of therapies that reverse some of the damages caused thus improving life
expectancy. Therefore as a first step our research focuses on T cells; major
contributors to immune system functionality. One of the main factors
influencing immune system aging is the output rate of naive T cells. Of further
interest is the number and phenotypical variety of these cells in an
individual, which will be the case study focused on in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6224</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6224</id><created>2013-06-26</created><updated>2014-04-10</updated><authors><author><keyname>Endrullis</keyname><forenames>Joerg</forenames></author><author><keyname>Hansen</keyname><forenames>Helle Hvid</forenames></author><author><keyname>Hendriks</keyname><forenames>Dimitri</forenames></author><author><keyname>Polonsky</keyname><forenames>Andrew</forenames></author><author><keyname>Silva</keyname><forenames>Alexandra</forenames></author></authors><title>A Coinductive Treatment of Infinitary Rewriting</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a coinductive definition of infinitary term rewriting. The setup
is surprisingly simple, and has in contrast to the usual definitions of
infinitary rewriting, neither need for ordinals nor for metric convergence.
While the idea of a coinductive treatment of infinitary rewriting is not new,
all previous approaches were limited to reductions of length at most omega. The
approach presented in this paper is the first to capture the full infinitary
term rewriting with reductions of arbitrary ordinal length. Apart from an
elegant reformulation of known concepts, our approach gives rise, in a very
natural way, to a novel notion of infinitary equational reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6239</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6239</id><created>2013-06-26</created><updated>2014-04-29</updated><authors><author><keyname>Malloy</keyname><forenames>Matthew L.</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author></authors><title>Near-Optimal Adaptive Compressed Sensing</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple adaptive sensing and group testing algorithm for
sparse signal recovery. The algorithm, termed Compressive Adaptive Sense and
Search (CASS), is shown to be near-optimal in that it succeeds at the lowest
possible signal-to-noise-ratio (SNR) levels, improving on previous work in
adaptive compressed sensing. Like traditional compressed sensing based on
random non-adaptive design matrices, the CASS algorithm requires only k log n
measurements to recover a k-sparse signal of dimension n. However, CASS
succeeds at SNR levels that are a factor log n less than required by standard
compressed sensing. From the point of view of constructing and implementing the
sensing operation as well as computing the reconstruction, the proposed
algorithm is substantially less computationally intensive than standard
compressed sensing. CASS is also demonstrated to perform considerably better in
practice through simulation. To the best of our knowledge, this is the first
demonstration of an adaptive compressed sensing algorithm with near-optimal
theoretical guarantees and excellent practical performance. This paper also
shows that methods like compressed sensing, group testing, and pooling have an
advantage beyond simply reducing the number of measurements or tests --
adaptive versions of such methods can also improve detection and estimation
performance when compared to non-adaptive direct (uncompressed) sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6259</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6259</id><created>2013-06-26</created><updated>2013-10-08</updated><authors><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Highlighting Entanglement of Cultures via Ranking of Multilingual
  Wikipedia Articles</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>Published in PLoS ONE
  (http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0074554).
  Supporting information is available on the same webpage</comments><journal-ref>PLoS ONE 8(10): e74554 (2013)</journal-ref><doi>10.1371/journal.pone.0074554</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How different cultures evaluate a person? Is an important person in one
culture is also important in the other culture? We address these questions via
ranking of multilingual Wikipedia articles. With three ranking algorithms based
on network structure of Wikipedia, we assign ranking to all articles in 9
multilingual editions of Wikipedia and investigate general ranking structure of
PageRank, CheiRank and 2DRank. In particular, we focus on articles related to
persons, identify top 30 persons for each rank among different editions and
analyze distinctions of their distributions over activity fields such as
politics, art, science, religion, sport for each edition. We find that local
heroes are dominant but also global heroes exist and create an effective
network representing entanglement of cultures. The Google matrix analysis of
network of cultures shows signs of the Zipf law distribution. This approach
allows to examine diversity and shared characteristics of knowledge
organization between cultures. The developed computational, data driven
approach highlights cultural interconnections in a new perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6260</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6260</id><created>2013-06-26</created><authors><author><keyname>Nikitin</keyname><forenames>Oleksandr</forenames></author></authors><title>Information-Theoretic Security for the Masses</title><categories>cs.CR cs.CY cs.IT math.IT</categories><comments>4 pages</comments><msc-class>94A60, 68P25</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We combine interactive zero-knowledge protocols and weak physical layer
randomness properties to construct a protocol which allows bootstrapping an
IT-secure and PF-secure channel from a memorizable shared secret. The protocol
also tolerates failures of its components, still preserving most of its
security properties, which makes it accessible to regular users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6262</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6262</id><created>2013-06-26</created><authors><author><keyname>Teyton</keyname><forenames>C&#xe9;dric</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Falleri</keyname><forenames>Jean-R&#xe9;my</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Palyart</keyname><forenames>Marc</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Blanc</keyname><forenames>Xavier</forenames><affiliation>LaBRI</affiliation></author></authors><title>A Study of Library Migration in Java Software</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software intensively depends on external libraries whose relevance may change
during its life cycle. As a consequence, software developers must periodically
reconsider the libraries they depend on, and must think about \textit{library
migration}. To our knowledge, no existing study has been done to understand
library migration although it is known to be an expensive maintenance task. Are
library migrations frequent? For which software are they performed and when?
For which libraries? For what reasons? The purpose of this paper is to answer
these questions with the intent to help software developers that have to
replace their libraries. To that extent, we have performed a statistical
analysis of a large set of open source software to mine their library
migration. To perform this analysis we have defined an approach that identifies
library migrations in a pseudo-automatic fashion by analyzing the source code
of the software. We have implemented this approach for the Java programming
language and applied it on Java Open Source Software stored in large hosting
services. The main result of our study is that library migration is not a
frequent practice but depends a lot on the nature of the software as well as
the nature of the libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6263</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6263</id><created>2013-06-26</created><authors><author><keyname>Ayatollahi</keyname><forenames>Seyed Morteza</forenames></author><author><keyname>Nafchi</keyname><forenames>Hossein Ziaei</forenames></author></authors><title>Persian Heritage Image Binarization Competition (PHIBC 2012)</title><categories>cs.CV</categories><comments>4 pages, 2 figures, conference</comments><doi>10.1109/PRIA.2013.6528442</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first competition on the binarization of historical Persian documents and
manuscripts (PHIBC 2012) has been organized in conjunction with the first
Iranian conference on pattern recognition and image analysis (PRIA 2013). The
main objective of PHIBC 2012 is to evaluate performance of the binarization
methodologies, when applied on the Persian heritage images. This paper provides
a report on the methodology and performance of the three submitted algorithms
based on evaluation measures has been used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6264</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6264</id><created>2013-06-26</created><updated>2014-08-02</updated><authors><author><keyname>Forney</keyname><forenames>G. David</forenames><suffix>Jr</suffix></author></authors><title>Codes on Graphs: Fundamentals</title><categories>cs.IT math.IT</categories><comments>32 pages, 22 figures. To appear in IEEE Transactions on Information
  Theory. Part of this paper was presented at the 2012 Allerton Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a fundamental theory of realizations of linear and group
codes on general graphs using elementary group theory, including basic group
duality theory. Principal new and extended results include: normal realization
duality; analysis of systems-theoretic properties of fragments of realizations
and their connections; &quot;minimal = trim and proper&quot; theorem for cycle-free
codes; results showing that all constraint codes except interface nodes may be
assumed to be trim and proper, and that the interesting part of a cyclic
realization is its &quot;2-core;&quot; notions of observability and controllability for
fragments, and related tests; relations between state-trimness and
controllability, and dual state-trimness and observability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6265</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6265</id><created>2013-06-26</created><authors><author><keyname>Chabanne</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Cohen</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Patey</keyname><forenames>Alain</forenames></author></authors><title>Towards Secure Two-Party Computation from the Wire-Tap Channel</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new protocol for secure two-party computation of linear
functions in the semi-honest model, based on coding techniques. We first
establish a parallel between the second version of the wire-tap channel model
and secure two-party computation. This leads us to our protocol, that combines
linear coset coding and oblivious transfer techniques. Our construction
requires the use of binary intersecting codes or $q$-ary minimal codes, which
are also studied in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6269</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6269</id><created>2013-06-26</created><updated>2013-11-11</updated><authors><author><keyname>Bansal</keyname><forenames>Sumukh</forenames></author><author><keyname>Tatu</keyname><forenames>Aditya</forenames></author></authors><title>Active Contour Models for Manifold Valued Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image segmentation is the process of partitioning a image into different
regions or groups based on some characteristics like color, texture, motion or
shape etc. Active contours is a popular variational method for object
segmentation in images, in which the user initializes a contour which evolves
in order to optimize an objective function designed such that the desired
object boundary is the optimal solution. Recently, imaging modalities that
produce Manifold valued images have come up, for example, DT-MRI images, vector
fields. The traditional active contour model does not work on such images. In
this paper, we generalize the active contour model to work on Manifold valued
images. As expected, our algorithm detects regions with similar Manifold values
in the image. Our algorithm also produces expected results on usual gray-scale
images, since these are nothing but trivial examples of Manifold valued images.
As another application of our general active contour model, we perform texture
segmentation on gray-scale images by first creating an appropriate Manifold
valued image. We demonstrate segmentation results for manifold valued images
and texture images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6278</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6278</id><created>2013-06-26</created><authors><author><keyname>Le&#x15b;niak</keyname><forenames>Krzysztof</forenames></author></authors><title>Playing cooperatively with possibly treacherous partner</title><categories>cs.GT</categories><msc-class>91A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate an alternative concept of Nash equilibrium, m-equilibrium,
which slightly resembles Harsanyi-Selten risk dominant equilibrium although it
is a different notion. M-equilibria provide nontrivial solutions of normal form
games as shown by comparison of the Prisoner's Dilemma with the Traveler's
Dilemma. They are also resistant on the deep iterated elimination of dominated
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6281</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6281</id><created>2013-06-26</created><authors><author><keyname>Harmany</keyname><forenames>Zachary T.</forenames></author><author><keyname>Marcia</keyname><forenames>Roummel F.</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca M.</forenames></author></authors><title>Compressive Coded Aperture Keyed Exposure Imaging with Optical Flow
  Reconstruction</title><categories>cs.IT cs.CV math.IT stat.AP</categories><comments>13 pages, 4 figures, Submitted to IEEE Transactions on Image
  Processing. arXiv admin note: substantial text overlap with arXiv:1111.7247</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a coded aperture and keyed exposure approach to
compressive video measurement which admits a small physical platform, high
photon efficiency, high temporal resolution, and fast reconstruction
algorithms. The proposed projections satisfy the Restricted Isometry Property
(RIP), and hence compressed sensing theory provides theoretical guarantees on
the video reconstruction quality. Moreover, the projections can be easily
implemented using existing optical elements such as spatial light modulators
(SLMs). We extend these coded mask designs to novel dual-scale masks (DSMs)
which enable the recovery of a coarse-resolution estimate of the scene with
negligible computational cost. We develop fast numerical algorithms which
utilize both temporal correlations and optical flow in the video sequence as
well as the innovative structure of the projections. Our numerical experiments
demonstrate the efficacy of the proposed approach on short-wave infrared data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6288</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6288</id><created>2013-06-26</created><authors><author><keyname>Elkayam</keyname><forenames>Nir</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>Information Spectrum Approach to the Source Channel Separation Theorem</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A source-channel separation theorem for a general channel has recently been
shown by Aggrawal et. al. This theorem states that if there exist a coding
scheme that achieves a maximum distortion level d_{max} over a general channel
W, then reliable communication can be accomplished over this channel at rates
less then R(d_{max}), where R(.) is the rate distortion function of the source.
The source, however, is essentially constrained to be discrete and memoryless
(DMS). In this work we prove a stronger claim where the source is general,
satisfying only a &quot;sphere packing optimality&quot; feature, and the channel is
completely general. Furthermore, we show that if the channel satisfies the
strong converse property as define by Han &amp; verdu, then the same statement can
be made with d_{avg}, the average distortion level, replacing d_{max}. Unlike
the proofs there, we use information spectrum methods to prove the statements
and the results can be quite easily extended to other situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6291</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6291</id><created>2013-06-26</created><updated>2015-02-16</updated><authors><author><keyname>Kronenburg</keyname><forenames>M. J.</forenames></author></authors><title>A Method for Fast Diagonalization of a 2x2 or 3x3 Real Symmetric Matrix</title><categories>math.NA cs.MS math.RA</categories><comments>Corrected formula 4.12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method is presented for fast diagonalization of a 2x2 or 3x3 real symmetric
matrix, that is determination of its eigenvalues and eigenvectors. The Euler
angles of the eigenvectors are computed. A small computer algebra program is
used to compute some of the identities, and a C++ program for testing the
formulas has been uploaded to arXiv.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6294</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6294</id><created>2013-06-26</created><updated>2013-11-05</updated><authors><author><keyname>Jain</keyname><forenames>Ashesh</forenames></author><author><keyname>Wojcik</keyname><forenames>Brian</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Learning Trajectory Preferences for Manipulators via Iterative
  Improvement</title><categories>cs.RO cs.AI cs.HC</categories><comments>9 pages. To appear in NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning good trajectories for manipulation tasks.
This is challenging because the criterion defining a good trajectory varies
with users, tasks and environments. In this paper, we propose a co-active
online learning framework for teaching robots the preferences of its users for
object manipulation tasks. The key novelty of our approach lies in the type of
feedback expected from the user: the human user does not need to demonstrate
optimal trajectories as training data, but merely needs to iteratively provide
trajectories that slightly improve over the trajectory currently proposed by
the system. We argue that this co-active preference feedback can be more easily
elicited from the user than demonstrations of optimal trajectories, which are
often challenging and non-intuitive to provide on high degrees of freedom
manipulators. Nevertheless, theoretical regret bounds of our algorithm match
the asymptotic rates of optimal trajectory algorithms. We demonstrate the
generalizability of our algorithm on a variety of grocery checkout tasks, for
whom, the preferences were not only influenced by the object being manipulated
but also by the surrounding environment.\footnote{For more details and a
demonstration video, visit: \url{http://pr.cs.cornell.edu/coactive}}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6295</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6295</id><created>2013-06-26</created><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author></authors><title>Tight Lower Bound for Linear Sketches of Moments</title><categories>cs.DS cs.IT math.IT math.ST stat.TH</categories><comments>In Proceedings of the 40th International Colloquium on Automata,
  Languages and Programming (ICALP), Riga, Latvia, July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of estimating frequency moments of a data stream has attracted a
lot of attention since the onset of streaming algorithms [AMS99]. While the
space complexity for approximately computing the $p^{\rm th}$ moment, for
$p\in(0,2]$ has been settled [KNW10], for $p&gt;2$ the exact complexity remains
open. For $p&gt;2$ the current best algorithm uses $O(n^{1-2/p}\log n)$ words of
space [AKO11,BO10], whereas the lower bound is of $\Omega(n^{1-2/p})$ [BJKS04].
  In this paper, we show a tight lower bound of $\Omega(n^{1-2/p}\log n)$ words
for the class of algorithms based on linear sketches, which store only a sketch
$Ax$ of input vector $x$ and some (possibly randomized) matrix $A$. We note
that all known algorithms for this problem are linear sketches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6302</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6302</id><created>2013-06-26</created><updated>2013-06-27</updated><authors><author><keyname>Joshi</keyname><forenames>S.</forenames></author><author><keyname>Khardon</keyname><forenames>R.</forenames></author><author><keyname>Tadepalli</keyname><forenames>P.</forenames></author><author><keyname>Raghavan</keyname><forenames>A.</forenames></author><author><keyname>Fern</keyname><forenames>A.</forenames></author></authors><title>Solving Relational MDPs with Exogenous Events and Additive Rewards</title><categories>cs.AI cs.LG</categories><comments>This is an extended version of our ECML/PKDD 2013 paper including all
  proofs. (v2 corrects typos and updates ref [10] to cite this report as the
  full version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formalize a simple but natural subclass of service domains for relational
planning problems with object-centered, independent exogenous events and
additive rewards capturing, for example, problems in inventory control.
Focusing on this subclass, we present a new symbolic planning algorithm which
is the first algorithm that has explicit performance guarantees for relational
MDPs with exogenous events. In particular, under some technical conditions, our
planning algorithm provides a monotonic lower bound on the optimal value
function. To support this algorithm we present novel evaluation and reduction
techniques for generalized first order decision diagrams, a knowledge
representation for real-valued functions over relational world states. Our
planning algorithm uses a set of focus states, which serves as a training set,
to simplify and approximate the symbolic solution, and can thus be seen to
perform learning for planning. A preliminary experimental evaluation
demonstrates the validity of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6311</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6311</id><created>2013-06-26</created><updated>2014-01-29</updated><authors><author><keyname>Giard</keyname><forenames>Pascal</forenames></author><author><keyname>Sarkis</keyname><forenames>Gabi</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Fast Software Polar Decoders</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, submitted to ICASSP 2014</comments><doi>10.1109/ICASSP.2014.6855069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among error-correcting codes, polar codes are the first to provably achieve
channel capacity with an explicit construction. In this work, we present
software implementations of a polar decoder that leverage the capabilities of
modern general-purpose processors to achieve an information throughput in
excess of 200 Mbps, a throughput well suited for software-defined-radio
applications. We also show that, for a similar error-correction performance,
the throughput of polar decoders both surpasses that of LDPC decoders targeting
general-purpose processors and is competitive with that of state-of-the-art
software LDPC decoders running on graphic processing units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6316</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6316</id><created>2013-06-26</created><updated>2014-12-09</updated><authors><author><keyname>Bauer</keyname><forenames>Andrej</forenames><affiliation>University of Ljubljana</affiliation></author><author><keyname>Pretnar</keyname><forenames>Matija</forenames><affiliation>University of Ljubljana</affiliation></author></authors><title>An Effect System for Algebraic Effects and Handlers</title><categories>cs.PL cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  10, 2014) lmcs:1153</journal-ref><doi>10.2168/LMCS-10(4:9)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an effect system for core Eff, a simplified variant of Eff, which
is an ML-style programming language with first-class algebraic effects and
handlers. We define an expressive effect system and prove safety of operational
semantics with respect to it. Then we give a domain-theoretic denotational
semantics of core Eff, using Pitts's theory of minimal invariant relations, and
prove it adequate. We use this fact to develop tools for finding useful
contextual equivalences, including an induction principle. To demonstrate their
usefulness, we use these tools to derive the usual equations for mutable state,
including a general commutativity law for computations using non-interfering
references. We have formalized the effect system, the operational semantics,
and the safety theorem in Twelf.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6370</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6370</id><created>2013-06-26</created><authors><author><keyname>Nguyen</keyname><forenames>Tommy</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>Social Ranking Techniques for the Web</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>7 pages, ASONAM 2013</comments><journal-ref>Proc. 2013 IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining (ASONAM) Niagara Falls, Canada, August 25-28,
  2013, 49-55</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of social media has the potential for changing the
structure and organization of the web. In the past, scientists have looked at
the web as a large connected component to understand how the topology of
hyperlinks correlates with the quality of information contained in the page and
they proposed techniques to rank information contained in web pages. We argue
that information from web pages and network data on social relationships can be
combined to create a personalized and socially connected web. In this paper, we
look at the web as a composition of two networks, one consisting of information
in web pages and the other of personal data shared on social media web sites.
Together, they allow us to analyze how social media tunnels the flow of
information from person to person and how to use the structure of the social
network to rank, deliver, and organize information specifically for each
individual user. We validate our social ranking concepts through a ranking
experiment conducted on web pages that users shared on Google Buzz and Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6375</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6375</id><created>2013-06-26</created><authors><author><keyname>Bongolan</keyname><forenames>Vena Pearl</forenames></author><author><keyname>Ballesteros,</keyname><forenames>Florencio C.</forenames><suffix>Jr.</suffix></author><author><keyname>Banting</keyname><forenames>Joyce Anne M.</forenames></author><author><keyname>Olaes</keyname><forenames>Aina Marie Q.</forenames></author><author><keyname>Aquino</keyname><forenames>Charlymagne R.</forenames></author></authors><title>Metaheuristics in Flood Disaster Management and Risk Assessment</title><categories>cs.AI</categories><comments>UP ICE Centennial Conference Harmonizing Infrastructure with the
  Environment November 12, 2010 in Manila, Philippines 8th National conference
  on Information Technology Education (NCITE 2010) October 20-23, 2010 in
  Boracay, Philippines</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conceptual area is divided into units or barangays, each was allowed to
evolve under a physical constraint. A risk assessment method was then used to
identify the flood risk in each community using the following risk factors: the
area's urbanized area ratio, literacy rate, mortality rate, poverty incidence,
radio/TV penetration, and state of structural and non-structural measures.
Vulnerability is defined as a weighted-sum of these components. A penalty was
imposed for reduced vulnerability. Optimization comparison was done with
MatLab's Genetic Algorithms and Simulated Annealing; results showed 'extreme'
solutions and realistic designs, for simulated annealing and genetic algorithm,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6378</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6378</id><created>2013-06-26</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Yukawa</keyname><forenames>M.</forenames></author><author><keyname>Yamada</keyname><forenames>I.</forenames></author></authors><title>Robust Reduced-Rank Adaptive Processing Based on Parallel Subgradient
  Projection and Krylov Subspace Techniques</title><categories>cs.IT math.IT</categories><comments>10 figures. In IEEE Transactions on Signal Processing, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel reduced-rank adaptive filtering algorithm
by blending the idea of the Krylov subspace methods with the set-theoretic
adaptive filtering framework. Unlike the existing Krylov-subspace-based
reduced-rank methods, the proposed algorithm tracks the optimal point in the
sense of minimizing the \sinq{true} mean square error (MSE) in the Krylov
subspace, even when the estimated statistics become erroneous (e.g., due to
sudden changes of environments). Therefore, compared with those existing
methods, the proposed algorithm is more suited to adaptive filtering
applications. The algorithm is analyzed based on a modified version of the
adaptive projected subgradient method (APSM). Numerical examples demonstrate
that the proposed algorithm enjoys better tracking performance than the
existing methods for the interference suppression problem in code-division
multiple-access (CDMA) systems as well as for simple system identification
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6397</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6397</id><created>2013-06-26</created><authors><author><keyname>Mundra</keyname><forenames>Ankit</forenames></author><author><keyname>Rakesh</keyname><forenames>Nitin</forenames></author><author><keyname>Tyagi</keyname><forenames>Vipin</forenames></author></authors><title>Query Centric CPS (QCPS) Approach for Multiple Heterogeneous Systems</title><categories>cs.DC</categories><comments>5 pages, 5 Figures. In International Journal of Computer Science And
  Technology 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern scenario we need to have mechanisms which can provide better
interaction with physical world by an efficient and more effective
communication and computation approach for multiple heterogeneous sensor
networks. Previous work provides efficient communication approach between
sensor nodes and a query centric approach for multiple collaborative
heterogeneous sensor networks. Even there is energy issues involved in wireless
sensor network operation. In this paper we have proposed Query centric Cyber
Physical System (QCPS)model to implement query centric user request using Cyber
Physical System (CPS). CPS takes both communication and computation in parallel
to provide better interaction with physical world. This feature of CPS reduces
system cost and makes it more energy efficient. This paper provides an
efficient query processing approach for multiple heterogeneous sensor networks
using cyber physical system.This approach results in reduction of communication
and computation cost as sensor network communicates using centroid of
respective grids which reduces cost of communication while involvement of CPS
reduces the computation cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6399</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6399</id><created>2013-06-26</created><updated>2013-09-08</updated><authors><author><keyname>Chen</keyname><forenames>Xuemei</forenames></author><author><keyname>Wang</keyname><forenames>Haichao</forenames></author><author><keyname>Wang</keyname><forenames>Rongrong</forenames></author></authors><title>A null space analysis of the L1 synthesis method in dictionary-based
  compressed sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interesting topic in compressed sensing aims to recover signals with
sparse representations in a dictionary. Recently the performance of the
L1-analysis method has been a focus, while some fundamental problems for the
L1-synthesis method are still unsolved. For example, what are the conditions
for it to stably recover compressible signals under noise? Whether coherent
dictionaries allow the existence of sensing matrices that guarantee good
performances of the L1-synthesis method? To answer these questions, we build up
a framework for the L1-synthesis method. In particular, we propose a
dictionary-based null space property DNSP which, to the best of our knowledge,
is the first sufficient and necessary condition for the success of L1-synthesis
without measurement noise. With this new property, we show that when the
dictionary D is full spark, it cannot be too coherent otherwise the method
fails for all sensing matrices. We also prove that in the real case, DNSP is
equivalent to the stability of L1-synthesis under noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6410</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6410</id><created>2013-06-27</created><updated>2014-04-29</updated><authors><author><keyname>Zhou</keyname><forenames>Amelie Chi</forenames></author><author><keyname>He</keyname><forenames>Bingsheng</forenames></author><author><keyname>Liu</keyname><forenames>Cheng</forenames></author></authors><title>Monetary Cost Optimizations for Hosting Workflow-as-a-Service in IaaS
  Clouds</title><categories>cs.DC</categories><report-no>Technical Report 2013-12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, we have witnessed workflows from science and other data-intensive
applications emerging on Infrastructure-asa-Service (IaaS) clouds, and many
workflow service providers offering workflow as a service (WaaS). The major
concern of WaaS providers is to minimize the monetary cost of executing
workflows in the IaaS cloud. While there have been previous studies on this
concern, most of them assume static task execution time and static pricing
scheme, and have the QoS notion of satisfying a deterministic deadline.
However, cloud environment is dynamic, with performance dynamics caused by the
interference from concurrent executions and price dynamics like spot prices
offered by Amazon EC2. Therefore, we argue that WaaS providers should have the
notion of offering probabilistic performance guarantees for individual
workflows on IaaS clouds. We develop a probabilistic scheduling framework
called Dyna to minimize the monetary cost while offering probabilistic deadline
guarantees. The framework includes an A*-based instance configuration method
for performance dynamics, and a hybrid instance configuration refinement for
utilizing spot instances. Experimental results with three real-world scientific
workflow applications on Amazon EC2 demonstrate (1) the accuracy of our
framework on satisfying the probabilistic deadline guarantees required by the
users; (2) the effectiveness of our framework on reducing monetary cost in
comparison with the existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6412</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6412</id><created>2013-06-27</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author></authors><title>Decision Taking versus Promise Issuing</title><categories>cs.SE</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An alignment is developed between the terminology of outcome oriented
decision taking and a terminology for promise issuing. Differences and
correspondences are investigated between the concepts of decision and promise.
  For decision taking, two forms are distinguished: the external outcome
delivering form and internalized decision taking. Internalized decision taking
is brought in connection with Marc Slors' theory of self-programming.
  Examples are produced for decisions and promises in four different several
settings each connected with software technology: instruction sequence
effectuation, informational money transfer, budget announcement, and division
by zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6413</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6413</id><created>2013-06-27</created><authors><author><keyname>Meenakshi</keyname><forenames>S. P.</forenames></author><author><keyname>Raghavan</keyname><forenames>S. V.</forenames></author></authors><title>Forecasting and Event Detection in Internet Resource Dynamics using Time
  Series Models</title><categories>cs.NI</categories><comments>22 pages, 15 figures</comments><acm-class>C.2.6; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At present Internet has emerged as a country's predominant and viable data
communication infrastructure. The Autonomous System (AS) resources which are
building blocks of the Internet are AS numbers, IPv4 and IPv6 Prefixes. AS
number growth is one of Internet infrastructure development indicators. Hence
understanding on long term trend and stochastic variation behaviour are
essential to detect significant events during the growth. In this work, time
series based approximation is considered for mathematical modelling and
forecast the yearly AS growth. The AS data of five countries namely India,
China, Japan, South Korea and Taiwan are extracted from APNIC archive. ARIMA
models with different Auto Regressive and Moving Average parameters are
identified for forecasting. Model validation, parameter estimation, point
forecast and prediction intervals with 95 % confidence levels for the five
countries are reported in the paper. The significant level change in
variations, positive growth percentage in Inter Annual Absolute Variations
(IAAV) and higher percentage of advertised ASes when compared to other
countries indicate India's fast growth and wider global reachability of
Internet infrastructure from 2007 onwards. The correlation between IAAV change
point and GDP growth period indicates that service sector industry growth is
the driving force behind significant yearly changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6428</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6428</id><created>2013-06-27</created><authors><author><keyname>Meenakshi</keyname><forenames>S. P.</forenames></author><author><keyname>Raghavan</keyname><forenames>S. V.</forenames></author></authors><title>Internet Control Plane Event Identification using Model Based Change
  Point Detection Techniques</title><categories>cs.NI</categories><comments>19 pages, 8 figures</comments><acm-class>C.2.3; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the raise of many global organizations deploying their data centers and
content services in India, the prefix reachability performance study from
global destinations garners our attention. The events such as failures and
attacks occurring in the Internet topology have impact on Autonomous System
(AS) paths announced in the control plane and reachability of prefixes from
spatially distributed ASes. As a consequence the customer reachability to the
services in terms of increased latency and outages for a short or long time are
experienced. The challenge in control plane event detection is when the data
plane traffic is able to reach the intended destinations correctly. However
detection of such events are crucial for the operations of content and data
center industries. By monitoring the spatially distributed routing table
features like AS path length distributions, spatial prefix reachability
distribution and covering to overlap route ratio, we can detect the control
plane events. In our work, we study prefix AS paths from the publicly available
route-view data and analyze the global reachability as well as reachability to
Indian AS topology. To capture the spatial events in a single temporal pattern,
we propose a counting based measure using prefixes announced by x % of spatial
peers. Employing statistical characteristics change point detection and
temporal aberration algorithm on the time series of the proposed measure, we
identify the occurrence of long and stochastic control plane events. The impact
and duration of the events are also quantified. We validate the mechanisms over
the proposed measure using the SEA-Me-We4 cable cut event manifestations in the
control plane of Indian AS topology. The cable cut events occurred on 6th June
2012 (long term event) and 17th April 2012 (stochastic event) are considered
for validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6438</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6438</id><created>2013-06-27</created><updated>2013-12-12</updated><authors><author><keyname>Aldridge</keyname><forenames>Matthew</forenames></author><author><keyname>Baldassini</keyname><forenames>Leonardo</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author></authors><title>Group testing algorithms: bounds and simulations</title><categories>cs.IT math.IT math.PR</categories><journal-ref>IEEE Transactions on Information Theory, 60:6, 3671-3687, 2014</journal-ref><doi>10.1109/TIT.2014.2314472</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of non-adaptive noiseless group testing of $N$ items
of which $K$ are defective. We describe four detection algorithms: the COMP
algorithm of Chan et al.; two new algorithms, DD and SCOMP, which require
stronger evidence to declare an item defective; and an essentially optimal but
computationally difficult algorithm called SSS. By considering the asymptotic
rate of these algorithms with Bernoulli designs we see that DD outperforms
COMP, that DD is essentially optimal in regimes where $K \geq \sqrt N$, and
that no algorithm with a nonadaptive Bernoulli design can perform as well as
the best non-random adaptive designs when $K &gt; N^{0.35}$. In simulations, we
see that DD and SCOMP far outperform COMP, with SCOMP very close to the optimal
SSS, especially in cases with larger $K$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6458</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6458</id><created>2013-06-27</created><updated>2014-05-03</updated><authors><author><keyname>Stolzenburg</keyname><forenames>Frieder</forenames></author></authors><title>Harmony Perception by Periodicity Detection</title><categories>cs.SD</categories><comments>19 pages, 7 figures, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The perception of consonance/dissonance of musical harmonies is strongly
correlated to their periodicity. This is shown in this article by consistently
applying recent results from psychophysics and neuroacoustics, namely that the
just noticeable difference of human pitch perception is about 1% for the
musically important low frequency range and that periodicities of complex
chords can be detected in the human brain. The presented results correlate
significantly to empirical investigations on the perception of chords. Even for
scales, plausible results are obtained. For example, all classical church modes
appear in the front ranks of all theoretically possible seven-tone scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6482</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6482</id><created>2013-06-27</created><authors><author><keyname>Kataoka</keyname><forenames>Shun</forenames></author><author><keyname>Yasuda</keyname><forenames>Muneki</forenames></author><author><keyname>Furtlehner</keyname><forenames>Cyril</forenames></author><author><keyname>Tanaka</keyname><forenames>Kazuyuki</forenames></author></authors><title>Traffic data reconstruction based on Markov random field modeling</title><categories>stat.ML cond-mat.dis-nn cs.LG</categories><comments>12 pages, 4 figures</comments><journal-ref>Inverse Problems 30 (2014) 025003</journal-ref><doi>10.1088/0266-5611/30/2/025003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the traffic data reconstruction problem. Suppose we have the
traffic data of an entire city that are incomplete because some road data are
unobserved. The problem is to reconstruct the unobserved parts of the data. In
this paper, we propose a new method to reconstruct incomplete traffic data
collected from various traffic sensors. Our approach is based on Markov random
field modeling of road traffic. The reconstruction is achieved by using
mean-field method and a machine learning method. We numerically verify the
performance of our method using realistic simulated traffic data for the real
road network of Sendai, Japan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6489</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6489</id><created>2013-06-27</created><authors><author><keyname>'Uyun</keyname><forenames>Shofwatul</forenames></author><author><keyname>Riadi</keyname><forenames>Imam</forenames></author></authors><title>A Fuzzy Topsis Multiple-Attribute Decision Making for Scholarship
  Selection</title><categories>cs.AI</categories><comments>10 pages, 5 figures, arXiv admin note: substantial text overlap with
  arXiv:1306.5960</comments><journal-ref>TELKOMNIKA Journal Vol.9 No.1 April 2011</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  As the education fees are becoming more expensive, more students apply for
scholarships. Consequently, hundreds and even thousands of applications need to
be handled by the sponsor. To solve the problems, some alternatives based on
several attributes (criteria) need to be selected. In order to make a decision
on such fuzzy problems, Fuzzy Multiple Attribute Decision Making (FMDAM) can be
applied. In this study, Unified Modeling Language (UML) in FMADM with TOPSIS
and Weighted Product (WP) methods is applied to select the candidates for
academic and non-academic scholarships at Universitas Islam Negeri Sunan
Kalijaga. Data used were a crisp and fuzzy data. The results show that TOPSIS
and Weighted Product FMADM methods can be used to select the most suitable
candidates to receive the scholarships since the preference values applied in
this method can show applicants with the highest eligibility
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6510</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6510</id><created>2013-06-27</created><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>De Vos</keyname><forenames>Maarten</forenames></author><author><keyname>Gligorijevic</keyname><forenames>Ivan</forenames></author><author><keyname>Matic</keyname><forenames>Vladimir</forenames></author><author><keyname>Li</keyname><forenames>Yuqian</forenames></author><author><keyname>Van Huffel</keyname><forenames>Sabine</forenames></author></authors><title>Multi-Structural Signal Recovery for Biomedical Compressive Sensing</title><categories>cs.IT math.IT stat.AP</categories><comments>29 pages, 20 figures, accepted by IEEE Transactions on Biomedical
  Engineering. Online first version:
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6519288&amp;tag=1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing has shown significant promise in biomedical fields. It
reconstructs a signal from sub-Nyquist random linear measurements. Classical
methods only exploit the sparsity in one domain. A lot of biomedical signals
have additional structures, such as multi-sparsity in di?fferent domains,
piecewise smoothness, low rank, etc. We propose a framework to exploit all the
available structure information. A new convex programming problem is generated
with multiple convex structure-inducing constraints and the linear measurement
fitting constraint. With additional a priori information for solving the
underdetermined system, the signal recovery performance can be improved. In
numerical experiments, we compare the proposed method with classical methods.
Both simulated data and real-life biomedical data are used. Results show that
the newly proposed method achieves better reconstruction accuracy performance
in term of both L1 and L2 errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6526</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6526</id><created>2013-06-27</created><updated>2014-05-19</updated><authors><author><keyname>Zanardini</keyname><forenames>Damiano</forenames></author><author><keyname>Genaim</keyname><forenames>Samir</forenames></author></authors><title>Inference of Field-Sensitive Reachability and Cyclicity</title><categories>cs.PL</categories><comments>38 pages + appendix</comments><acm-class>D.2.4; F.3.1; F.3.2; F.4.1; I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In heap-based languages, knowing that a variable x points to an acyclic data
structure is useful for analyzing termination: this information guarantees that
the depth of the data structure to which x points is greater than the depth of
the structure pointed to by x.fld, and allows bounding the number of iterations
of a loop which traverses the data structure on fld. In general, proving
termination needs acyclicity, unless program-specific or non-automated
reasoning is performed. However, recent work could prove that certain loops
terminate even without inferring acyclicity, because they traverse data
structures &quot;acyclically&quot;. Consider a double-linked list: if it is possible to
demonstrate that every cycle involves both the &quot;next&quot; and the &quot;prev&quot; field,
then a traversal on &quot;next&quot; terminates since no cycle will be traversed
completely. This paper develops a static analysis inferring field-sensitive
reachability and cyclicity information, which is more general than existing
approaches. Propositional formulae are computed, which describe which fields
may or may not be traversed by paths in the heap. Consider a tree with edges
&quot;left&quot; and &quot;right&quot; to the left and right sub-trees, and &quot;parent&quot; to the parent
node: termination of a loop traversing leaf-up cannot be guaranteed by
state-of-the-art analyses. Instead, propositional formulae computed by this
analysis indicate that cycles must traverse &quot;parent&quot; and at least one between
&quot;left&quot; and &quot;right&quot;: termination is guaranteed as no cycle is traversed
completely. This paper defines the necessary abstract domains and builds an
abstract semantics on them. A prototypical implementation provides the expected
result on relevant examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6531</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6531</id><created>2013-06-27</created><updated>2013-10-20</updated><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author><author><keyname>Granqvist</keyname><forenames>Claes-Goran</forenames></author></authors><title>Critical analysis of the Bennett-Riedel attack on secure cryptographic
  key distributions via the Kirchhoff-law-Johnson-noise scheme</title><categories>cs.CR</categories><comments>Accepted for publication at PLOS ONE on October 16, 2013. (The simple
  explanation of why the technically-unlimited Eve is still information
  theoretically limited is given in Sec.1.1.4)</comments><journal-ref>PLoS ONE 8 (2013) e81810</journal-ref><doi>10.1371/journal.pone.0081810</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recently, Bennett and Riedel (BR) (http://arxiv.org/abs/1303.7435v1) argued
that thermodynamics is not essential in the Kirchhoff-law-Johnson-noise (KLJN)
classical physical cryptographic exchange method in an effort to disprove the
security of the KLJN scheme. They attempted to demonstrate this by introducing
a dissipation-free deterministic key exchange method with two batteries and two
switches. In the present paper, we first show that BR's scheme is unphysical
and that some elements of its assumptions violate basic protocols of secure
communication. All our analyses are based on a technically-unlimited Eve with
infinitely accurate and fast measurements limited only by the laws of physics
and statistics. For non-ideal situations and at active (invasive) attacks, the
uncertainly principle between measurement duration and statistical errors makes
it impossible for Eve to extract the key regardless of the accuracy or speed of
her measurements. To show that thermodynamics and noise are essential for the
security, we crack the BR system with 100% success via passive attacks, in ten
different ways, and demonstrate that the same cracking methods do not function
for the KLJN scheme that employs Johnson noise to provide security underpinned
by the Second Law of Thermodynamics. We also present a critical analysis of
some other claims by BR; for example, we prove that their equations for
describing zero security do not apply to the KLJN scheme. Finally we give
mathematical security proofs for each BR-attack against the KLJN scheme and
conclude that the information theoretic (unconditional) security of the KLJN
method has not been successfully challenged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6542</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6542</id><created>2013-06-27</created><authors><author><keyname>Yuan</keyname><forenames>Shuai</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Zhao</keyname><forenames>Xiaoxue</forenames></author></authors><title>Real-time Bidding for Online Advertising: Measurement and Analysis</title><categories>cs.GT cs.CE cs.IR</categories><comments>Accepted by ADKDD '13 workshop</comments><acm-class>H.3.5; J.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The real-time bidding (RTB), aka programmatic buying, has recently become the
fastest growing area in online advertising. Instead of bulking buying and
inventory-centric buying, RTB mimics stock exchanges and utilises computer
algorithms to automatically buy and sell ads in real-time; It uses per
impression context and targets the ads to specific people based on data about
them, and hence dramatically increases the effectiveness of display
advertising. In this paper, we provide an empirical analysis and measurement of
a production ad exchange. Using the data sampled from both demand and supply
side, we aim to provide first-hand insights into the emerging new impression
selling infrastructure and its bidding behaviours, and help identifying
research and design issues in such systems. From our study, we observed that
periodic patterns occur in various statistics including impressions, clicks,
bids, and conversion rates (both post-view and post-click), which suggest
time-dependent models would be appropriate for capturing the repeated patterns
in RTB. We also found that despite the claimed second price auction, the first
price payment in fact is accounted for 55.4% of total cost due to the
arrangement of the soft floor price. As such, we argue that the setting of soft
floor price in the current RTB systems puts advertisers in a less favourable
position. Furthermore, our analysis on the conversation rates shows that the
current bidding strategy is far less optimal, indicating the significant needs
for optimisation algorithms incorporating the facts such as the temporal
behaviours, the frequency and recency of the ad displays, which have not been
well considered in the past.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6572</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6572</id><created>2013-06-27</created><authors><author><keyname>Chernyak</keyname><forenames>Vladimir Y.</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Bierkens</keyname><forenames>Joris</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert J.</forenames></author></authors><title>Stochastic Optimal Control as Non-equilibrium Statistical Mechanics:
  Calculus of Variations over Density and Current</title><categories>cond-mat.stat-mech cs.SY math-ph math.MP math.OC</categories><comments>4 pages, 1 figure</comments><doi>10.1088/1751-8113/47/2/022001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Stochastic Optimal Control (SOC) one minimizes the average cost-to-go,
that consists of the cost-of-control (amount of efforts), cost-of-space (where
one wants the system to be) and the target cost (where one wants the system to
arrive), for a system participating in forced and controlled Langevin dynamics.
We extend the SOC problem by introducing an additional cost-of-dynamics,
characterized by a vector potential. We propose derivation of the generalized
gauge-invariant Hamilton-Jacobi-Bellman equation as a variation over density
and current, suggest hydrodynamic interpretation and discuss examples, e.g.,
ergodic control of a particle-within-a-circle, illustrating non-equilibrium
space-time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6578</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6578</id><created>2013-06-27</created><authors><author><keyname>Cagnoni</keyname><forenames>Davide</forenames></author><author><keyname>Agostini</keyname><forenames>Francesco</forenames></author><author><keyname>Christen</keyname><forenames>Thomas</forenames></author><author><keyname>de Falco</keyname><forenames>Carlo</forenames></author><author><keyname>Parolini</keyname><forenames>Nicola</forenames></author><author><keyname>Stevanovi&#x107;</keyname><forenames>Ivica</forenames></author></authors><title>Multiphysics simulation of corona discharge induced ionic wind</title><categories>physics.comp-ph cs.CE physics.flu-dyn</categories><comments>24 pages, 17 figures</comments><doi>10.1063/1.4843823</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ionic wind devices or electrostatic fluid accelerators are becoming of
increasing interest as tools for thermal management, in particular for
semiconductor devices. In this work, we present a numerical model for
predicting the performance of such devices, whose main benefit is the ability
to accurately predict the amount of charge injected at the corona electrode.
Our multiphysics numerical model consists of a highly nonlinear strongly
coupled set of PDEs including the Navier-Stokes equations for fluid flow,
Poisson's equation for electrostatic potential, charge continuity and heat
transfer equations. To solve this system we employ a staggered solution
algorithm that generalizes Gummel's algorithm for charge transport in
semiconductors. Predictions of our simulations are validated by comparison with
experimental measurements and are shown to closely match. Finally, our
simulation tool is used to estimate the effectiveness of the design of an
electrohydrodynamic cooling apparatus for power electronics applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6584</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6584</id><created>2013-06-27</created><updated>2014-02-26</updated><authors><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author><author><keyname>Mart&#xed;n-Mart&#xed;n</keyname><forenames>Alberto</forenames></author><author><keyname>Fuente-Guti&#xe9;rrez</keyname><forenames>Enrique</forenames></author></authors><title>An introduction to the coverage of the Data Citation Index
  (Thomson-Reuters): disciplines, document types and repositories</title><categories>cs.DL</categories><journal-ref>Rev. Esp. Doc. Cient., 37(1), enero-marzo 2014, e036. ISSN-L:
  0210-0614</journal-ref><doi>10.3989/redc.2014.1.1114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past years, the movement of data sharing has been enjoying great
popularity. Within this context, Thomson Reuters launched at the end of 2012 a
new product inside the Web of Knowledge family: the Data Citation Index. The
aim of this tool is to enable discovery and access, from a single place, to
data from a variety of data repositories from different subject areas and from
around the world. In this short note we present some preliminary results from
the analysis of the Data Citation Index. Specifically, we address the following
issues: discipline coverage, data types present in the database, and
repositories that were included at the time of the study
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6593</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6593</id><created>2013-06-27</created><updated>2014-04-03</updated><authors><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Sankowski</keyname><forenames>Piotr</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Erik Jan</forenames></author></authors><title>Network Sparsification for Steiner Problems on Planar and Bounded-Genus
  Graphs</title><categories>cs.DS</categories><comments>Major update. In particular: new overview of the proofs, weighted
  variant of the main theorem, a lower bound for Steiner Forest</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose polynomial-time algorithms that sparsify planar and bounded-genus
graphs while preserving optimal or near-optimal solutions to Steiner problems.
Our main contribution is a polynomial-time algorithm that, given an unweighted
graph $G$ embedded on a surface of genus $g$ and a designated face $f$ bounded
by a simple cycle of length $k$, uncovers a set $F \subseteq E(G)$ of size
polynomial in $g$ and $k$ that contains an optimal Steiner tree for any set of
terminals that is a subset of the vertices of $f$.
  We apply this general theorem to prove that: * given an unweighted graph $G$
embedded on a surface of genus $g$ and a terminal set $S \subseteq V(G)$, one
can in polynomial time find a set $F \subseteq E(G)$ that contains an optimal
Steiner tree $T$ for $S$ and that has size polynomial in $g$ and $|E(T)|$; * an
analogous result holds for an optimal Steiner forest for a set $S$ of terminal
pairs; * given an unweighted planar graph $G$ and a terminal set $S \subseteq
V(G)$, one can in polynomial time find a set $F \subseteq E(G)$ that contains
an optimal (edge) multiway cut $C$ separating $S$ and that has size polynomial
in $|C|$.
  In the language of parameterized complexity, these results imply the first
polynomial kernels for Steiner Tree and Steiner Forest on planar and
bounded-genus graphs (parameterized by the size of the tree and forest,
respectively) and for (Edge) Multiway Cut on planar graphs (parameterized by
the size of the cutset). Steiner Tree and similar &quot;subset&quot; problems were
identified in [Demaine, Hajiaghayi, Computer J., 2008] as important to the
quest to widen the reach of the theory of bidimensionality ([Demaine et al,
JACM 2005], [Fomin et al, SODA 2010]). Therefore, our results can be seen as a
leap forward to achieve this broader goal.
  Additionally, we obtain a weighted variant of our main contribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6595</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6595</id><created>2013-06-27</created><authors><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author><author><keyname>Cabezas-Clavijo</keyname><forenames>Alvaro</forenames></author><author><keyname>Jimenez-Contreras</keyname><forenames>Evaristo</forenames></author></authors><title>Altmetrics: New Indicators for Scientific Communication in Web 2.0</title><categories>cs.DL cs.SI physics.soc-ph</categories><doi>10.3916/C41-2013-05</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we review the socalled altmetrics or alternative metrics. This
concept raises from the development of new indicators based on Web 2.0, for the
evaluation of the research and academic activity. The basic assumption is that
variables such as mentions in blogs, number of twits or of researchers
bookmarking a research paper for instance, may be legitimate indicators for
measuring the use and impact of scientific publications. In this sense, these
indicators are currently the focus of the bibliometric community and are being
discussed and debated. We describe the main platforms and indicators and we
analyze as a sample the Spanish research output in Communication Studies.
Comparing traditional indicators such as citations with these new indicators.
The results show that the most cited papers are also the ones with a highest
impact according to the altmetrics. We conclude pointing out the main
shortcomings these metrics present and the role they may play when measuring
the research impact through 2.0 platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6597</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6597</id><created>2013-06-27</created><updated>2014-01-27</updated><authors><author><keyname>Singh</keyname><forenames>Sukhpal</forenames></author><author><keyname>Singh</keyname><forenames>Rishideep</forenames></author></authors><title>Earthquake Disaster based Efficient Resource Utilization Technique in
  IaaS Cloud</title><categories>cs.DC</categories><comments>6 Pages including 4 figures, Published by IJARCET</comments><report-no>IJARCET - 6831</report-no><msc-class>97P70</msc-class><acm-class>J.7</acm-class><journal-ref>International Journal of Advanced Research in Computer Engineering
  &amp; Technology (IJARCET) Volume 2, Issue 6, 2013, 1933-1938</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Cloud Computing is an emerging area. The main aim of the initial
search-and-rescue period after strong earthquakes is to reduce the whole number
of mortalities. One main trouble rising in this period is to and the greatest
assignment of available resources to functioning zones. For this issue a
dynamic optimization model is presented. The model uses thorough descriptions
of the operational zones and of the available resources to determine the
resource performance and efficiency for different workloads related to the
response. A suitable solution method for the model is offered as well. In this
paper, Earthquake Disaster Based Resource Scheduling (EDBRS) Framework has been
proposed. The allocation of resources to cloud workloads based on urgency
(emergency during Earthquake Disaster). Based on this criterion, the resource
scheduling algorithm has been proposed. The performance of the proposed
algorithm has been assessed with the existing common scheduling algorithms
through the CloudSim. The experimental results show that the proposed algorithm
outperforms the existing algorithms by reducing execution cost and time of
cloud consumer workloads submitted to the cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6598</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6598</id><created>2013-06-27</created><authors><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>A More Complicated Hardness Proof for Finding Densest Subgraphs in
  Bounded Degree Graphs</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Densest-Subgraph problem, where a graph and an integer k is
given and we search for a subgraph on exactly k vertices that induces the
maximum number of edges. We prove that this problem is NP-hard even when the
input graph has maximum degree three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6615</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6615</id><created>2013-06-27</created><updated>2015-08-11</updated><authors><author><keyname>Okayama</keyname><forenames>Tomoaki</forenames></author></authors><title>Explicit error bound for modified numerical iterated integration by
  means of Sinc methods</title><categories>math.NA cs.NA</categories><comments>Keyword: Sinc quadrature, Sinc indefinite integration, repeated
  integral, verified numerical integration, double-exponential transformation</comments><msc-class>65D30, 65D32, 65G99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reinforces numerical iterated integration developed by
Muhammad--Mori in the following two points: 1) the approximation formula is
modified so that it can achieve a better convergence rate in more general
cases, and 2) explicit error bound is given in a computable form for the
modified formula. The formula works quite efficiently, especially if the
integrand is of a product type. Numerical examples that confirm it are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6649</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6649</id><created>2013-06-27</created><authors><author><keyname>Halmes</keyname><forenames>Michel</forenames></author></authors><title>Measurements of collective machine intelligence</title><categories>cs.AI cs.MA</categories><comments>78 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independent from the still ongoing research in measuring individual
intelligence, we anticipate and provide a framework for measuring collective
intelligence. Collective intelligence refers to the idea that several
individuals can collaborate in order to achieve high levels of intelligence. We
present thus some ideas on how the intelligence of a group can be measured and
simulate such tests. We will however focus here on groups of artificial
intelligence agents (i.e., machines). We will explore how a group of agents is
able to choose the appropriate problem and to specialize for a variety of
tasks. This is a feature which is an important contributor to the increase of
intelligence in a group (apart from the addition of more agents and the
improvement due to common decision making). Our results reveal some interesting
results about how (collective) intelligence can be modeled, about how
collective intelligence tests can be designed and about the underlying dynamics
of collective intelligence. As it will be useful for our simulations, we
provide also some improvements of the threshold allocation model originally
used in the area of swarm intelligence but further generalized here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6657</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6657</id><created>2013-06-27</created><authors><author><keyname>Finkbeiner</keyname><forenames>Bernd</forenames></author><author><keyname>Rabe</keyname><forenames>Markus N.</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>C&#xe9;sar</forenames></author></authors><title>A Temporal Logic for Hyperproperties</title><categories>cs.LO cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperproperties, as introduced by Clarkson and Schneider, characterize the
correctness of a computer program as a condition on its set of computation
paths. Standard temporal logics can only refer to a single path at a time, and
therefore cannot express many hyperproperties of interest, including
noninterference and other important properties in security and coding theory.
In this paper, we investigate an extension of temporal logic with explicit path
variables. We show that the quantification over paths naturally subsumes other
extensions of temporal logic with operators for information flow and knowledge.
The model checking problem for temporal logic with path quantification is
decidable. For alternation depth 1, the complexity is PSPACE in the length of
the formula and NLOGSPACE in the size of the system, as for linear-time
temporal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6659</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6659</id><created>2013-06-27</created><authors><author><keyname>Hur</keyname><forenames>Sooyoung</forenames></author><author><keyname>Kim</keyname><forenames>Taejoon</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Krogmeier</keyname><forenames>James V.</forenames></author><author><keyname>Thomas</keyname><forenames>Timothy A.</forenames></author><author><keyname>Ghosh</keyname><forenames>Amitava</forenames></author></authors><title>Millimeter Wave Beamforming for Wireless Backhaul and Access in Small
  Cell Networks</title><categories>cs.IT math.IT</categories><comments>34 pages, 15 figures, Submitted to IEEE Transactions on
  Communications for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been considerable interest in new tiered network cellular
architectures, which would likely use many more cell sites than found today.
Two major challenges will be i) providing backhaul to all of these cells and
ii) finding efficient techniques to leverage higher frequency bands for mobile
access and backhaul. This paper proposes the use of outdoor millimeter wave
communications for backhaul networking between cells and mobile access within a
cell. To overcome the outdoor impairments found in millimeter wave propagation,
this paper studies beamforming using large arrays. However, such systems will
require narrow beams, increasing sensitivity to movement caused by pole sway
and other environmental concerns. To overcome this, we propose an efficient
beam alignment technique using adaptive subspace sampling and hierarchical beam
codebooks. A wind sway analysis is presented to establish a notion of beam
coherence time. This highlights a previously unexplored tradeoff between array
size and wind-induced movement. Generally, it is not possible to use larger
arrays without risking a corresponding performance loss from wind-induced beam
misalignment. The performance of the proposed alignment technique is analyzed
and compared with other search and alignment methods. The results show
significant performance improvement with reduced search time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6670</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6670</id><created>2013-06-27</created><authors><author><keyname>Cur&#xe9;</keyname><forenames>Olivier</forenames></author><author><keyname>Faye</keyname><forenames>David</forenames></author><author><keyname>Blin</keyname><forenames>Guillaume</forenames></author></authors><title>Towards a better insight of RDF triples Ontology-guided Storage system
  abilities</title><categories>cs.DB</categories><comments>15 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vision of the Semantic Web is becoming a reality with billions of RDF
triples being distributed over multiple queryable end-points (e.g. Linked
Data). Although there has been a body of work on RDF triples persistent
storage, it seems that, considering reasoning dependent queries, the problem of
providing an efficient, in terms of performance, scalability and data
redundancy, partitioning of the data is still open. In regards to recent data
partitioning studies, it seems reasonable to think that data partitioning
should be guided considering several directions (e.g. ontology, data,
application queries). This paper proposes several contributions: describe an
overview of what a road map for data partitioning for RDF data efficient and
persistent storage should contain, present some preliminary results and
analysis on the particular case of ontology-guided (property hierarchy)
partitioning and finally introduce a set of semantic query rewriting rules to
support querying RDF data needing OWL inferences
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6671</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6671</id><created>2013-06-27</created><authors><author><keyname>Vaezi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Extended Subspace Error Localization for Rate-Adaptive Distributed
  Source Coding</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures. To appear in the IEEE International Symposium on
  Information Theory (ISIT 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A subspace-based approach for rate-adaptive distributed source coding (DSC)
based on discrete Fourier transform (DFT) codes is developed. Punctured DFT
codes can be used to implement rate-adaptive source coding, however they
perform poorly after even moderate puncturing since the performance of the
subspace error localization degrades severely. The proposed subspace-based
error localization extends and improves the existing one, based on additional
syndrome, and is naturally suitable for rate-adaptive distributed source coding
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6675</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6675</id><created>2013-06-27</created><authors><author><keyname>Chekanov</keyname><forenames>S. V.</forenames></author></authors><title>Next generation input-output data format for HEP using Google's protocol
  buffers</title><categories>cs.CE cs.MS hep-ph</categories><comments>7 pages, 1 figure, 2 tables. Contributed to the Snowmass 2013 Study</comments><report-no>ANL-HEP-CP-13-32</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a data format for Monte Carlo (MC) events, or any structural data,
including experimental data, in a compact binary form using variable-size
integer encoding as implemented in the Google's Protocol Buffers package. This
approach is implemented in the so-called ProMC library which produces smaller
file sizes for MC records compared to the existing input-output libraries used
in high-energy physics (HEP). Other important features are a separation of
abstract data layouts from concrete programming implementations,
self-description and random access. Data stored in ProMC files can be written,
read and manipulated in a number of programming languages, such C++, Java and
Python.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6686</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6686</id><created>2013-06-27</created><updated>2014-07-18</updated><authors><author><keyname>Babichenko</keyname><forenames>Yakov</forenames></author></authors><title>Query Complexity of Approximate Nash Equilibria</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the query complexity of approximate notions of Nash equilibrium in
games with a large number of players $n$. Our main result states that for
$n$-player binary-action games and for constant $\varepsilon$, the query
complexity of an $\varepsilon$-well-supported Nash equilibrium is exponential
in $n$. One of the consequences of this result is an exponential lower bound on
the rate of convergence of adaptive dynamics to approxiamte Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6709</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6709</id><created>2013-06-27</created><updated>2014-02-12</updated><authors><author><keyname>Bellet</keyname><forenames>Aur&#xe9;lien</forenames></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames></author><author><keyname>Sebban</keyname><forenames>Marc</forenames></author></authors><title>A Survey on Metric Learning for Feature Vectors and Structured Data</title><categories>cs.LG stat.ML</categories><comments>Technical report, 59 pages. Changes in v2: fixed typos and improved
  presentation. Changes in v3: fixed typos. Changes in v4: fixed typos and new
  methods</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for appropriate ways to measure the distance or similarity between
data is ubiquitous in machine learning, pattern recognition and data mining,
but handcrafting such good metrics for specific problems is generally
difficult. This has led to the emergence of metric learning, which aims at
automatically learning a metric from data and has attracted a lot of interest
in machine learning and related fields for the past ten years. This survey
paper proposes a systematic review of the metric learning literature,
highlighting the pros and cons of each approach. We pay particular attention to
Mahalanobis distance metric learning, a well-studied and successful framework,
but additionally present a wide range of methods that have recently emerged as
powerful alternatives, including nonlinear metric learning, similarity learning
and local metric learning. Recent trends and extensions, such as
semi-supervised metric learning, metric learning for histogram data and the
derivation of generalization guarantees, are also covered. Finally, this survey
addresses metric learning for structured data, in particular edit distance
learning, and attempts to give an overview of the remaining challenges in
metric learning for the years to come.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6710</identifier>
 <datestamp>2014-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6710</id><created>2013-06-28</created><updated>2014-08-20</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Rogers</keyname><forenames>Trent A.</forenames></author><author><keyname>Schweller</keyname><forenames>Robert T.</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author><author><keyname>Woods</keyname><forenames>Damien</forenames></author></authors><title>The two-handed tile assembly model is not intrinsically universal</title><categories>cs.CG cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The well-studied Two-Handed Tile Assembly Model (2HAM) is a model of tile
assembly in which pairs of large assemblies can bind, or self-assemble,
together. In order to bind, two assemblies must have matching glues that can
simultaneously touch each other, and stick together with strength that is at
least the temperature $\tau$, where $\tau$ is some fixed positive integer. We
ask whether the 2HAM is intrinsically universal, in other words we ask: is
there a single universal 2HAM tile set $U$ which can be used to simulate any
instance of the model? Our main result is a negative answer to this question.
We show that for all $\tau' &lt; \tau$, each temperature-$\tau'$ 2HAM tile system
does not simulate at least one temperature-$\tau$ 2HAM tile system. This
impossibility result proves that the 2HAM is not intrinsically universal, in
stark contrast to the simpler (single-tile addition only) abstract Tile
Assembly Model which is intrinsically universal (&quot;The tile assembly model is
intrinsically universal&quot;, FOCS 2012). However, on the positive side, we prove
that, for every fixed temperature $\tau \geq 2$, temperature-$\tau$ 2HAM tile
systems are indeed intrinsically universal: in other words, for each $\tau$
there is a single universal 2HAM tile set $U$ that, when appropriately
initialized, is capable of simulating the behavior of any temperature-$\tau$
2HAM tile system. As a corollary of these results we find an infinite set of
infinite hierarchies of 2HAM systems with strictly increasing simulation power
within each hierarchy. Finally, we show that for each $\tau$, there is a
temperature-$\tau$ 2HAM system that simultaneously simulates all
temperature-$\tau$ 2HAM systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6726</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6726</id><created>2013-06-28</created><authors><author><keyname>Tatu</keyname><forenames>Aditya</forenames></author><author><keyname>Bansal</keyname><forenames>Sumukh</forenames></author></authors><title>A Novel Active Contour Model for Texture Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texture is intuitively defined as a repeated arrangement of a basic pattern
or object in an image. There is no mathematical definition of a texture though.
The human visual system is able to identify and segment different textures in a
given image. Automating this task for a computer is far from trivial. There are
three major components of any texture segmentation algorithm: (a) The features
used to represent a texture, (b) the metric induced on this representation
space and (c) the clustering algorithm that runs over these features in order
to segment a given image into different textures. In this paper, we propose an
active contour based novel unsupervised algorithm for texture segmentation. We
use intensity covariance matrices of regions as the defining feature of
textures and find regions that have the most inter-region dissimilar covariance
matrices using active contours. Since covariance matrices are symmetric
positive definite, we use geodesic distance defined on the manifold of
symmetric positive definite matrices PD(n) as a measure of dissimlarity between
such matrices. We demonstrate performance of our algorithm on both artificial
and real texture images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6728</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6728</id><created>2013-06-28</created><authors><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Nussbaum</keyname><forenames>Yahav</forenames></author></authors><title>Min-Cost Flow Duality in Planar Networks</title><categories>cs.DM cs.DS</categories><comments>13 pages, 7 figures</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the min-cost flow problem in planar networks. We start
with the min-cost flow problem and apply two transformations, one is based on
geometric duality of planar graphs and the other on linear programming duality.
The result is a min-cost flow problem in a related planar network whose balance
constraints are defined by the costs of the original problem and whose costs
are defined by the capacities of the original problem. We use this
transformation to show an O(n log^2 n) time algorithm for the min-cost flow
problem in an n-vertex outerplanar network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6729</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6729</id><created>2013-06-28</created><authors><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>Dragoni</keyname><forenames>Nicola</forenames></author><author><keyname>Gottardo</keyname><forenames>Sebastiano</forenames></author></authors><title>MITHYS: Mind The Hand You Shake - Protecting mobile devices from SSL
  usage vulnerabilities</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have shown that a significant number of mobile applications,
often handling sensitive data such as bank accounts and login credentials,
suffers from SSL vulnerabilities. Most of the time, these vulnerabilities are
due to improper use of the SSL protocol (in particular, in its \emph{handshake}
phase), resulting in applications exposed to man-in-the-middle attacks. In this
paper, we present MITHYS, a system able to: (i) detect applications vulnerable
to man-in-the-middle attacks, and (ii) protect them against these attacks. We
demonstrate the feasibility of our proposal by means of a prototype
implementation in Android, named MITHYSApp. A thorough set of experiments
assesses the validity of our solution in detecting and protecting mobile
applications from man-in-the-middle attacks, without introducing significant
overheads. Finally, MITHYSApp does not require any special permissions nor OS
modifications, as it operates at the application level. These features make
MITHYSApp immediately deployable on a large user base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6734</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6734</id><created>2013-06-28</created><authors><author><keyname>Pieris</keyname><forenames>Dhammika</forenames></author></authors><title>A novel ER model to relational model transformation algorithm for
  semantically clear high quality database design</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conceptual modelling using the entity relationship (ER) model has been widely
used for database design for a long period of time. However, studies indicate
that creating a satisfactory relational model representation from an ER model
is uncertain due to the insufficiencies both in the transformation methods used
and in the relational model itself. In an effort to solve the issue the
original ER notation has been modified, and accordingly, a new transformation
algorithm has been developed. This paper presents the proposed transformation
algorithm. Using a real world example it shows how the algorithm can be applied
in practice. The paper also discusses how to validate the resulted database and
reclaim the information that it represents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6735</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6735</id><created>2013-06-28</created><updated>2013-10-20</updated><authors><author><keyname>Torrieri</keyname><forenames>Don</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author></authors><title>An Analysis of the DS-CDMA Cellular Uplink for Arbitrary and Constrained
  Topologies</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communications, vol. 61, no. 8, pp. 3318 - 3326,
  August 2013</comments><doi>10.1109/TCOMM.2013.13.120911</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new analysis is presented for the direct-sequence code-division multiple
access (DS-CDMA) cellular uplink. For a given network topology, closed-form
expressions are found for the outage probability and rate of each uplink in the
presence of path-dependent Nakagami fading and shadowing. The topology may be
arbitrary or modeled by a random spatial distribution with a fixed number of
base stations and mobiles placed over a finite area. The analysis is more
detailed and accurate than existing ones and facilitates the resolution of
network design issues including the influence of the minimum base-station
separation, the role of the spreading factor, and the impact of various
power-control and rate-control policies. It is shown that once power control is
established, the rate can be allocated according to a fixed-rate or
variable-rate policy with the objective of either meeting an outage constraint
or maximizing throughput. An advantage of variable-rate power control is that
it allows an outage constraint to be enforced on every uplink, which is
impossible when a fixed rate is used throughout the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6737</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6737</id><created>2013-06-28</created><authors><author><keyname>Mishra</keyname><forenames>Minati</forenames></author><author><keyname>Adhikary</keyname><forenames>Flt. Lt. Dr. M. C.</forenames></author></authors><title>Digital Image Tamper Detection Techniques - A Comprehensive Study</title><categories>cs.CR cs.CV</categories><comments>12 pages available online via
  http://ijcsbi.org/index.php/ijcsbi/article/view/50</comments><journal-ref>IJCSBI Vol. 2, No. 1. June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photographs are considered to be the most powerful and trustworthy media of
expression. For a long time, those were accepted as proves of evidences in
varied fields such as journalism, forensic investigations, military
intelligence, scientific research and publications, crime detection and legal
proceedings, investigation of insurance claims, medical imaging etc. Today,
digital images have completely replaced the conventional photographs from every
sphere of life but unfortunately, they seldom enjoy the credibility of their
conventional counterparts, thanks to the rapid advancements in the field of
digital image processing. The increasing availability of low cost and sometimes
free of cost image editing software such as Photoshop, Corel Paint Shop,
Photoscape, PhotoPlus, GIMP and Pixelmator have made the tampering of digital
images even more easier and a common practice. Now it has become quite
impossible to say whether a photograph is a genuine camera output or a
manipulated version of it just by looking at it. As a result, photographs have
almost lost their reliability and place as proves of evidences in all fields.
This is why digital image tamper detection has emerged as an important research
area to establish the authenticity of digital photographs by separating the
tampered lots from the original ones. This paper gives a brief history of image
tampering and a state-of-the-art review of the tamper detection techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6744</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6744</id><created>2013-06-28</created><authors><author><keyname>Billera</keyname><forenames>Louis J.</forenames></author><author><keyname>Levine</keyname><forenames>Lionel</forenames></author><author><keyname>Meszaros</keyname><forenames>Karola</forenames></author></authors><title>How to decompose a permutation into a pair of labeled Dyck paths by
  playing a game</title><categories>math.CO cs.GT math.PR</categories><comments>9 pages, 1 figure</comments><msc-class>05A05, 05A15, 91A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a bijection between permutations of length 2n and certain pairs of
Dyck paths with labels on the down steps. The bijection arises from a game in
which two players alternate selecting from a set of 2n items: the permutation
encodes the players' preference ordering of the items, and the Dyck paths
encode the order in which items are selected under optimal play. We enumerate
permutations by certain statistics, AA inversions and BB inversions, which have
natural interpretations in terms of the game. We give new proofs of classical
identities such as \sum_p \prod_{i=1}^n q^{h_i -1} [h_i]_q = [1]_q [3]_q ...
[2n-1]_q where the sum is over all Dyck paths p of length 2n, and the h_i are
the heights of the down steps of p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6749</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6749</id><created>2013-06-28</created><authors><author><keyname>Prank</keyname><forenames>Rein</forenames></author></authors><title>Software for Evaluating Relevance of Steps in Algebraic Transformations</title><categories>cs.SC</categories><comments>CICM 2013, Bath</comments><acm-class>K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Students of our department solve algebraic exercises in mathematical logic in
a computerized environment. They construct transformations step by step and the
program checks the syntax, equivalence of expressions and completion of the
task. With our current project, we add a program component for checking
relevance of the steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6755</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6755</id><created>2013-06-28</created><authors><author><keyname>Darwish</keyname><forenames>Kareem</forenames></author></authors><title>Arabizi Detection and Conversion to Arabic</title><categories>cs.CL cs.IR</categories><acm-class>I.2.7</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Arabizi is Arabic text that is written using Latin characters. Arabizi is
used to present both Modern Standard Arabic (MSA) or Arabic dialects. It is
commonly used in informal settings such as social networking sites and is often
with mixed with English. In this paper we address the problems of: identifying
Arabizi in text and converting it to Arabic characters. We used word and
sequence-level features to identify Arabizi that is mixed with English. We
achieved an identification accuracy of 98.5%. As for conversion, we used
transliteration mining with language modeling to generate equivalent Arabic
text. We achieved 88.7% conversion accuracy, with roughly a third of errors
being spelling and morphological variants of the forms in ground truth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6802</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6802</id><created>2013-06-28</created><updated>2013-07-01</updated><authors><author><keyname>Kosmopoulos</keyname><forenames>Aris</forenames></author><author><keyname>Partalas</keyname><forenames>Ioannis</forenames></author><author><keyname>Gaussier</keyname><forenames>Eric</forenames></author><author><keyname>Paliouras</keyname><forenames>Georgios</forenames></author><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author></authors><title>Evaluation Measures for Hierarchical Classification: a unified view and
  novel approaches</title><categories>cs.AI cs.LG</categories><comments>Submitted to journal</comments><doi>10.1007/s10618-014-0382-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical classification addresses the problem of classifying items into a
hierarchy of classes. An important issue in hierarchical classification is the
evaluation of different classification algorithms, which is complicated by the
hierarchical relations among the classes. Several evaluation measures have been
proposed for hierarchical classification using the hierarchy in different ways.
This paper studies the problem of evaluation in hierarchical classification by
analyzing and abstracting the key components of the existing performance
measures. It also proposes two alternative generic views of hierarchical
evaluation and introduces two corresponding novel measures. The proposed
measures, along with the state-of-the art ones, are empirically tested on three
large datasets from the domain of text classification. The empirical results
illustrate the undesirable behavior of existing approaches and how the proposed
methods overcome most of these methods across a range of cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6805</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6805</id><created>2013-06-28</created><authors><author><keyname>Hajian</keyname><forenames>Sara</forenames></author></authors><title>Simultaneous Discrimination Prevention and Privacy Protection in Data
  Publishing and Mining</title><categories>cs.DB cs.CR</categories><comments>PhD Thesis defended on June 10, 2013, at the Department of Computer
  Engineering and Mathematics of Universitat Rovira i Virgili. Advisors: Josep
  Domingo-Ferrer and Dino Pedreschi</comments><msc-class>68P15, 68P20, 68P99</msc-class><acm-class>K.4.1; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining is an increasingly important technology for extracting useful
knowledge hidden in large collections of data. There are, however, negative
social perceptions about data mining, among which potential privacy violation
and potential discrimination. Automated data collection and data mining
techniques such as classification have paved the way to making automated
decisions, like loan granting/denial, insurance premium computation. If the
training datasets are biased in what regards discriminatory attributes like
gender, race, religion, discriminatory decisions may ensue. In the first part
of this thesis, we tackle discrimination prevention in data mining and propose
new techniques applicable for direct or indirect discrimination prevention
individually or both at the same time. We discuss how to clean training
datasets and outsourced datasets in such a way that direct and/or indirect
discriminatory decision rules are converted to legitimate (non-discriminatory)
classification rules. In the second part of this thesis, we argue that privacy
and discrimination risks should be tackled together. We explore the
relationship between privacy preserving data mining and discrimination
prevention in data mining to design holistic approaches capable of addressing
both threats simultaneously during the knowledge discovery process. As part of
this effort, we have investigated for the first time the problem of
discrimination and privacy aware frequent pattern discovery, i.e. the
sanitization of the collection of patterns mined from a transaction database in
such a way that neither privacy-violating nor discriminatory inferences can be
inferred on the released patterns. Moreover, we investigate the problem of
discrimination and privacy aware data publishing, i.e. transforming the data,
instead of patterns, in order to simultaneously fulfill privacy preservation
and discrimination prevention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6811</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6811</id><created>2013-06-28</created><authors><author><keyname>Gerdt</keyname><forenames>Vladimir P.</forenames></author><author><keyname>Hashemi</keyname><forenames>Amir</forenames></author><author><keyname>-Alizadeh</keyname><forenames>Benyamin M.</forenames></author></authors><title>Involutive Bases Algorithm Incorporating F5 Criterion</title><categories>math.AC cs.SC math.RA</categories><comments>24 pages, 2 figures</comments><msc-class>13P10</msc-class><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Faugere's F5 algorithm is the fastest known algorithm to compute Groebner
bases. It has a signature-based and an incremental structure that allow to
apply the F5 criterion for deletion of unnecessary reductions. In this paper,
we present an involutive completion algorithm which outputs a minimal
involutive basis. Our completion algorithm has a nonincremental structure and
in addition to the involutive form of Buchberger's criteria it applies the F5
criterion whenever this criterion is applicable in the course of completion to
involution. In doing so, we use the G2V form of the F5 criterion developed by
Gao, Guan and Volny IV. To compare the proposed algorithm, via a set of
benchmarks, with the Gerdt-Blinkov involutive algorithm (which does not apply
the F5 criterion) we use implementations of both algorithms done on the same
platform in Maple.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6812</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6812</id><created>2013-06-28</created><authors><author><keyname>Santos</keyname><forenames>Augusto</forenames></author><author><keyname>Moura</keyname><forenames>Jos&#xe9; M. F.</forenames></author><author><keyname>Xavier</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Epidemics in Multipartite Networks: Emergent Dynamics</title><categories>cs.SI physics.soc-ph q-bio.PE</categories><comments>32 pages, Submitted to the IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single virus epidemics over complete networks are widely explored in the
literature as the fraction of infected nodes is, under appropriate microscopic
modeling of the virus infection, a Markov process. With non-complete networks,
this macroscopic variable is no longer Markov. In this paper, we study virus
diffusion, in particular, multi-virus epidemics, over non-complete stochastic
networks. We focus on multipartite networks. In companying work
http://arxiv.org/abs/1306.6198, we show that the peer-to-peer local random
rules of virus infection lead, in the limit of large multipartite networks, to
the emergence of structured dynamics at the macroscale. The exact fluid limit
evolution of the fraction of nodes infected by each virus strain across islands
obeys a set of nonlinear coupled differential equations, see
http://arxiv.org/abs/1306.6198. In this paper, we develop methods to analyze
the qualitative behavior of these limiting dynamics, establishing conditions on
the virus micro characteristics and network structure under which a virus
persists or a natural selection phenomenon is observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6815</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6815</id><created>2013-06-28</created><updated>2013-10-28</updated><authors><author><keyname>Sundman</keyname><forenames>Dennis</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Distributed Greedy Pursuit Algorithms</title><categories>cs.IT math.IT</categories><comments>Submitted to EURASIP Signal Processing Dec. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For compressed sensing over arbitrarily connected networks, we consider the
problem of estimating underlying sparse signals in a distributed manner. We
introduce a new signal model that helps to describe inter-signal correlation
among connected nodes. Based on this signal model along with a brief survey of
existing greedy algorithms, we develop distributed greedy algorithms with low
communication overhead. Incorporating appropriate modifications, we design two
new distributed algorithms where the local algorithms are based on
appropriately modified existing orthogonal matching pursuit and subspace
pursuit. Further, by combining advantages of these two local algorithms, we
design a new greedy algorithm that is well suited for a distributed scenario.
By extensive simulations we demonstrate that the new algorithms in a sparsely
connected network provide good performance, close to the performance of a
centralized greedy solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6834</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6834</id><created>2013-06-28</created><authors><author><keyname>Paulo</keyname><forenames>Damon</forenames></author><author><keyname>Fischl</keyname><forenames>Bradley</forenames></author><author><keyname>Markow</keyname><forenames>Tanya</forenames></author><author><keyname>Martin</keyname><forenames>Michael</forenames></author><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author></authors><title>Social Network Intelligence Analysis to Combat Street Gang Violence</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper we introduce the Organization, Relationship, and Contact
Analyzer (ORCA) that is designed to aide intelligence analysis for law
enforcement operations against violent street gangs. ORCA is designed to
address several police analytical needs concerning street gangs using new
techniques in social network analysis. Specifically, it can determine &quot;degree
of membership&quot; for individuals who do not admit to membership in a street gang,
quickly identify sets of influential individuals (under the tipping model), and
identify criminal ecosystems by decomposing gangs into sub-groups. We describe
this software and the design decisions considered in building an intelligence
analysis tool created specifically for countering violent street gangs as well
as provide results based on conducting analysis on real-world police data
provided by a major American metropolitan police department who is partnering
with us and currently deploying this system for real-world use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6839</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6839</id><created>2013-06-24</created><authors><author><keyname>R</keyname><forenames>Karthik</forenames></author><author><keyname>Karthik</keyname><forenames>Raghavendra</forenames></author><author><keyname>S</keyname><forenames>Pramod</forenames></author><author><keyname>Kamath</keyname><forenames>Sowmya</forenames></author></authors><title>W3-Scrape - A Windows based Reconnaissance Tool for Web Application
  Fingerprinting</title><categories>cs.CR</categories><comments>International Conference on Emerging Trends in Electrical,
  Communication and Information Technologies (ICECIT 2012), 6 pages; Organised
  by SRIT, Ananthpur, India during Dec 21 - 23, 2012. (Publisher - Elsevier
  Science &amp; Technology; ISBN 8131234118, 9788131234112)</comments><acm-class>D.4.6; E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Application finger printing is a quintessential part of the Information
Gathering phase of (ethical) hacking. It allows narrowing down the specifics
instead of looking for all clues. Also an application that has been correctly
recognized can help in quickly analyzing known weaknesses and then moving ahead
with remaining aspects. This step is also essential to allow a pen tester to
customize its payload or exploitation techniques based on the identification so
to increase the chances of successful intrusion. This paper presents a new tool
&quot;W3-Scrape&quot; for the relatively nascent field of Web Application finger printing
that helps automate web application fingerprinting when performed in the
current scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6842</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6842</id><created>2013-06-28</created><authors><author><keyname>Arabadjis</keyname><forenames>Dimitris</forenames></author><author><keyname>Giannopoulos</keyname><forenames>Fotios</forenames></author><author><keyname>Papaodysseus</keyname><forenames>Constantin</forenames></author><author><keyname>Zannos</keyname><forenames>Solomon</forenames></author><author><keyname>Rousopoulos</keyname><forenames>Panayiotis</forenames></author><author><keyname>Panagopoulos</keyname><forenames>Michail</forenames></author><author><keyname>Blackwell</keyname><forenames>Christopher</forenames></author></authors><title>New Mathematical and Algorithmic Schemes for Pattern Classification with
  Application to the Identification of Writers of Important Ancient Documents</title><categories>cs.CV</categories><journal-ref>Pattern Recognition, Volume 46, Issue 8, Pages 2278-2296, August
  2013</journal-ref><doi>10.1016/j.patcog.2013.01.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel approach is introduced for classifying curves into
proper families, according to their similarity. First, a mathematical quantity
we call plane curvature is introduced and a number of propositions are stated
and proved. Proper similarity measures of two curves are introduced and a
subsequent statistical analysis is applied. First, the efficiency of the curve
fitting process has been tested on 2 shapes datasets of reference. Next, the
methodology has been applied to the very important problem of classifying 23
Byzantine codices and 46 Ancient inscriptions to their writers, thus achieving
correct dating of their content. The inscriptions have been attributed to ten
individual hands and the Byzantine codices to four writers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6843</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6843</id><created>2013-06-28</created><updated>2013-09-29</updated><authors><author><keyname>Pe&#xf1;a</keyname><forenames>Jose M.</forenames></author></authors><title>Error AMP Chain Graphs</title><categories>stat.ML cs.AI</categories><comments>In Proceedings of the 12th Scandinavian Conference on Artificial
  Intelligence (SCAI 2013), to appear. Changes from v1 to v2: Minor correction
  in Theorem 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any regular Gaussian probability distribution that can be represented by an
AMP chain graph (CG) can be expressed as a system of linear equations with
correlated errors whose structure depends on the CG. However, the CG represents
the errors implicitly, as no nodes in the CG correspond to the errors. We
propose in this paper to add some deterministic nodes to the CG in order to
represent the errors explicitly. We call the result an EAMP CG. We will show
that, as desired, every AMP CG is Markov equivalent to its corresponding EAMP
CG under marginalization of the error nodes. We will also show that every EAMP
CG under marginalization of the error nodes is Markov equivalent to some LWF CG
under marginalization of the error nodes, and that the latter is Markov
equivalent to some directed and acyclic graph (DAG) under marginalization of
the error nodes and conditioning on some selection nodes. This is important
because it implies that the independence model represented by an AMP CG can be
accounted for by some data generating process that is partially observed and
has selection bias. Finally, we will show that EAMP CGs are closed under
marginalization. This is a desirable feature because it guarantees parsimonious
models under marginalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6852</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6852</id><created>2013-06-28</created><authors><author><keyname>Brunelli</keyname><forenames>Matteo</forenames></author><author><keyname>Fedrizzi</keyname><forenames>Michele</forenames></author></authors><title>Axiomatic properties of inconsistency indices for pairwise comparisons</title><categories>cs.AI</categories><comments>25 pages, 3 figures</comments><msc-class>90B08 (Primary) 90B06, 90B50 (Secondary)</msc-class><journal-ref>Journal of the Operational Research Society, 66(1), 1-15, (2015)</journal-ref><doi>10.1057/jors.2013.135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pairwise comparisons are a well-known method for the representation of the
subjective preferences of a decision maker. Evaluating their inconsistency has
been a widely studied and discussed topic and several indices have been
proposed in the literature to perform this task. Since an acceptable level of
consistency is closely related with the reliability of preferences, a suitable
choice of an inconsistency index is a crucial phase in decision making
processes. The use of different methods for measuring consistency must be
carefully evaluated, as it can affect the decision outcome in practical
applications. In this paper, we present five axioms aimed at characterizing
inconsistency indices. In addition, we prove that some of the indices proposed
in the literature satisfy these axioms, while others do not, and therefore, in
our view, they may fail to correctly evaluate inconsistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6856</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6856</id><created>2013-06-28</created><authors><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author></authors><title>Linear Dependent Types for Domain Specific Program Analysis (Extended
  Abstract)</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this tutorial I will present how a combination of linear and dependent
type can be useful to describe different properties about higher order
programs. Linear types have been proved particularly useful to express
properties of functions; dependent types are useful to describe the behavior of
the program in terms of its control flow. This two ideas fits together well
when one is interested in analyze properties of functions depending on the
control flow of the program. I will present these ideas with example taken by
complexity analysis and sensitivity analysis. I will conclude the tutorial by
arguing about the generality of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6909</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6909</id><created>2013-06-28</created><updated>2014-09-15</updated><authors><author><keyname>Duval</keyname><forenames>Vincent</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author></authors><title>Exact Support Recovery for Sparse Spikes Deconvolution</title><categories>math.OC cs.IT math.IT math.NA</categories><comments>Foundations of Computational Mathematics (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies sparse spikes deconvolution over the space of measures. We
focus our attention to the recovery properties of the support of the measure,
i.e. the location of the Dirac masses. For non-degenerate sums of Diracs, we
show that, when the signal-to-noise ratio is large enough, total variation
regularization (which is the natural extension of the L1 norm of vectors to the
setting of measures) recovers the exact same number of Diracs. We also show
that both the locations and the heights of these Diracs converge toward those
of the input measure when the noise drops to zero. The exact speed of
convergence is governed by a specific dual certificate, which can be computed
by solving a linear system. We draw connections between the support of the
recovered measure on a continuous domain and on a discretized grid. We show
that when the signal-to-noise level is large enough, the solution of the
discretized problem is supported on pairs of Diracs which are neighbors of the
Diracs of the input measure. This gives a precise description of the
convergence of the solution of the discretized problem toward the solution of
the continuous grid-free problem, as the grid size tends to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6920</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6920</id><created>2013-06-03</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Beniwal</keyname><forenames>Nidhi</forenames></author><author><keyname>Mandal</keyname><forenames>Gargi</forenames></author><author><keyname>Talwar</keyname><forenames>Saloni</forenames></author></authors><title>Enhanced Tiny Encryption Algorithm with Embedding (ETEA)</title><categories>cs.MM cs.CR</categories><comments>9 pages, 3 figures, International Journal of Computers &amp; Technology,
  Vol 7, No 1, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As computer systems become more pervasive and complex, security is
increasingly important. Secure Transmission refers to the transfer of data such
as confidential or proprietary information over a secure channel. Many secure
transmission methods require a type of encryption. Secure transmissions are put
in place to prevent attacks such as ARP spoofing and general data loss. Hence,
in order to provide a better security mechanism, in this paper we propose
Enhanced Tiny Encryption Algorithm with Embedding (ETEA), a data hiding
technique called steganography along with the technique of encryption
(Cryptography). The advantage of ETEA is that it incorporates cryptography and
steganography. The advantage proposed algorithm is that it hides the messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6924</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6924</id><created>2013-06-28</created><authors><author><keyname>Wu</keyname><forenames>Peiran</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay K.</forenames></author></authors><title>Optimal Tx-BF for MIMO SC-FDE Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmit beamforming (Tx-BF) for multiple-input multiple-output (MIMO)
channels is an effective means to improve system performance. In
frequency-selective channels, Tx-BF can be implemented in combination with
single-carrier frequency-domain equalization (SC-FDE) to combat inter-symbol
interference. In this paper, we consider the optimal design of the Tx-BF matrix
for a MIMO SC-FDE system employing a linear minimum mean square error (MSE)
receiver. We formulate the Tx-BF optimization problem as the minimization of a
general function of the stream MSEs, subject to a transmit power constraint.
The optimal structure of the Tx-BF matrix is obtained in closed form and an
efficient algorithm is proposed for computing the optimal power allocation. Our
simulation results validate the excellent performance of the proposed scheme in
terms of uncoded bit-error rate and achievable bit rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6929</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6929</id><created>2013-06-28</created><authors><author><keyname>Molinero</keyname><forenames>Xavier</forenames></author><author><keyname>Riquelme</keyname><forenames>Fabi&#xe1;n</forenames></author><author><keyname>Serna</keyname><forenames>Maria</forenames></author></authors><title>Power indices of influence games and new centrality measures for social
  networks</title><categories>cs.GT cs.SI physics.soc-ph</categories><comments>14 pages, 6 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social network analysis, there is a common perception that influence is
relevant to determine the global behavior of the society and thus it can be
used to enforce cooperation by targeting an adequate initial set of individuals
or to analyze global choice processes. Here we propose centrality measures that
can be used to analyze the relevance of the actors in process related to spread
of influence. In [39] it was considered a multiagent system in which the agents
are eager to perform a collective task depending on the perception of the
willingness to perform the task of other individuals. The setting is modeled
using a notion of simple games called influence games. Those games are defined
on graphs were the nodes are labeled by their influence threshold and the
spread of influence between its nodes is used to determine whether a coalition
is winning or not. Influence games provide tools to measure the importance of
the actors of a social network by means of classic power indices and provide a
framework to consider new centrality criteria. In this paper we consider two of
the most classical power indices, i.e., Banzhaf and Shapley-Shubik indices, as
centrality measures for social networks in influence games. Although there is
some work related to specific scenarios of game-theoretic networks, here we use
such indices as centrality measures in any social network where the spread of
influence phenomenon can be applied. Further, we define new centrality measures
such as satisfaction and effort that, as far as we know, have not been
considered so far. We also perform a comparison of the proposed measures with
other three classic centrality measures, degree, closeness and betweenness,
considering three social networks. We show that in some cases our measurements
provide centrality hierarchies similar to those of other measures, while in
other cases provide different hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6943</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6943</id><created>2013-06-20</created><updated>2013-07-01</updated><authors><author><keyname>Saket</keyname><forenames>Rishi</forenames></author></authors><title>A PTAS for the Classical Ising Spin Glass Problem on the Chimera Graph
  Structure</title><categories>cs.DS quant-ph</categories><comments>6 pages, corrected PTAS running time</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial time approximation scheme (PTAS) for the minimum
value of the classical Ising Hamiltonian with linear terms on the Chimera graph
structure as defined in the recent work of McGeoch and Wang. The result follows
from a direct application of the techniques used by Bansal, Bravyi and Terhal
who gave a PTAS for the same problem on planar and, in particular, grid graphs.
We also show that on Chimera graphs, the trivial lower bound is within a
constant factor of the optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1306.6944</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1306.6944</id><created>2013-06-07</created><authors><author><keyname>Sch&#xf6;neberg</keyname><forenames>Ulf</forenames></author><author><keyname>Sperber</keyname><forenames>Wolfram</forenames></author></authors><title>The DeLiVerMATH project - Text analysis in mathematics</title><categories>cs.CL cs.DL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A high-quality content analysis is essential for retrieval functionalities
but the manual extraction of key phrases and classification is expensive.
Natural language processing provides a framework to automatize the process.
Here, a machine-based approach for the content analysis of mathematical texts
is described. A prototype for key phrase extraction and classification of
mathematical texts is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0024</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0024</id><created>2013-06-28</created><authors><author><keyname>Wilmer</keyname><forenames>Daan</forenames></author></authors><title>Investigation of &quot;Enhancing flexibility and robustness in multi-agent
  task scheduling&quot;</title><categories>cs.DS cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wilson et al. propose a measure of flexibility in project scheduling problems
and propose several ways of distributing flexibility over tasks without
overrunning the deadline. These schedules prove quite robust: delays of some
tasks do not necessarily lead to delays of subsequent tasks. The number of
tasks that finish late depends, among others, on the way of distributing
flexibility.
  In this paper I study the different flexibility distributions proposed by
Wilson et al. and the differences in number of violations (tasks that finish
too late). I show one factor in the instances that causes differences in the
number of violations, as well as two properties of the flexibility distribution
that cause them to behave differently. Based on these findings, I propose three
new flexibility distributions. Depending on the nature of the delays, these new
flexibility distributions perform as good as or better than the distributions
by Wilson et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0027</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0027</id><created>2013-06-28</created><authors><author><keyname>Jel&#xed;nek</keyname><forenames>V&#xed;t</forenames></author><author><keyname>Valtr</keyname><forenames>Pavel</forenames></author></authors><title>Splittings and Ramsey Properties of Permutation Classes</title><categories>math.CO cs.DM</categories><comments>34 pages, 6 figures</comments><msc-class>05A05, 05C55</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We say that a permutation p is 'merged' from permutations q and r, if we can
color the elements of p red and blue so that the red elements are
order-isomorphic to q and the blue ones to r. A 'permutation class' is a set of
permutations closed under taking subpermutations. A permutation class C is
'splittable' if it has two proper subclasses A and B such that every element of
C can be obtained by merging an element of A with an element of B.
  Several recent papers use splittability as a tool in deriving enumerative
results for specific permutation classes. The goal of this paper is to study
splittability systematically. As our main results, we show that if q is a
sum-decomposable permutation of order at least four, then the class Av(q) of
all q-avoiding permutations is splittable, while if q is a simple permutation,
then Av(q) is unsplittable.
  We also show that there is a close connection between splittings of certain
permutation classes and colorings of circle graphs of bounded clique size.
Indeed, our splittability results can be interpreted as a generalization of a
theorem of Gy\'arf\'as stating that circle graphs of bounded clique size have
bounded chromatic number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0029</identifier>
 <datestamp>2013-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0029</id><created>2013-06-11</created><updated>2013-09-25</updated><authors><author><keyname>Rout</keyname><forenames>Ranjeet Kumar</forenames></author><author><keyname>Choudhury</keyname><forenames>Pabitra Pal</forenames></author><author><keyname>Sagar</keyname><forenames>B. S. Daya</forenames></author><author><keyname>Hassan</keyname><forenames>Sk. Sarif</forenames></author></authors><title>Fractal and Mathematical Morphology in Intricate Comparison between
  Tertiary Protein Structures</title><categories>cs.CG cs.CE</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Intricate comparison between two given tertiary structures of proteins is as
important as the comparison of their functions. Several algorithms have been
devised to compute the similarity and dissimilarity among protein structures.
But, these algorithms compare protein structures by structural alignment of the
protein backbones which are usually unable to determine precise differences. In
this paper, an attempt has been made to compute the similarities and
dissimilarities among 3D protein structures using the fundamental mathematical
morphology operations and fractal geometry which can resolve the problem of
real differences. In doing so, two techniques are being used here in
determining the superficial structural (global similarity) and local similarity
in atomic level of the protein molecules. This intricate structural difference
would provide insight to Biologists to understand the protein structures and
their functions more precisely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0031</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0031</id><created>2013-06-28</created><authors><author><keyname>Kennedy</keyname><forenames>W. Sean</forenames></author><author><keyname>Narayan</keyname><forenames>Onuttom</forenames></author><author><keyname>Saniee</keyname><forenames>Iraj</forenames></author></authors><title>On the Hyperbolicity of Large-Scale Networks</title><categories>physics.soc-ph cs.SI</categories><comments>22 pages, 25 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through detailed analysis of scores of publicly available data sets
corresponding to a wide range of large-scale networks, from communication and
road networks to various forms of social networks, we explore a little-studied
geometric characteristic of real-life networks, namely their hyperbolicity. In
smooth geometry, hyperbolicity captures the notion of negative curvature;
within the more abstract context of metric spaces, it can be generalized as
d-hyperbolicity. This generalized definition can be applied to graphs, which we
explore in this report. We provide strong evidence that communication and
social networks exhibit this fundamental property, and through extensive
computations we quantify the degree of hyperbolicity of each network in
comparison to its diameter. By contrast, and as evidence of the validity of the
methodology, applying the same methods to the road networks shows that they are
not hyperbolic, which is as expected. Finally, we present practical
computational means for detection of hyperbolicity and show how the test itself
may be scaled to much larger graphs than those we examined via renormalization
group methodology. Using well-understood mechanisms, we provide evidence
through synthetically generated graphs that hyperbolicity is preserved and
indeed amplified by renormalization. This allows us to detect hyperbolicity in
large networks efficiently, through much smaller renormalized versions. These
observations indicate that d-hyperbolicity is a common feature of large-scale
networks. We propose that d-hyperbolicity in conjunction with other local
characteristics of networks, such as the degree distribution and clustering
coefficients, provide a more complete unifying picture of networks, and helps
classify in a parsimonious way what is otherwise a bewildering and complex
array of features and characteristics specific to each natural and man-made
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0032</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0032</id><created>2013-06-28</created><authors><author><keyname>Mitliagkas</keyname><forenames>Ioannis</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author></authors><title>Memory Limited, Streaming PCA</title><categories>stat.ML cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider streaming, one-pass principal component analysis (PCA), in the
high-dimensional regime, with limited memory. Here, $p$-dimensional samples are
presented sequentially, and the goal is to produce the $k$-dimensional subspace
that best approximates these points. Standard algorithms require $O(p^2)$
memory; meanwhile no algorithm can do better than $O(kp)$ memory, since this is
what the output itself requires. Memory (or storage) complexity is most
meaningful when understood in the context of computational and sample
complexity. Sample complexity for high-dimensional PCA is typically studied in
the setting of the {\em spiked covariance model}, where $p$-dimensional points
are generated from a population covariance equal to the identity (white noise)
plus a low-dimensional perturbation (the spike) which is the signal to be
recovered. It is now well-understood that the spike can be recovered when the
number of samples, $n$, scales proportionally with the dimension, $p$. Yet, all
algorithms that provably achieve this, have memory complexity $O(p^2)$.
Meanwhile, algorithms with memory-complexity $O(kp)$ do not have provable
bounds on sample complexity comparable to $p$. We present an algorithm that
achieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is able
to compute the $k$-dimensional spike with $O(p \log p)$ sample-complexity --
the first algorithm of its kind. While our theoretical analysis focuses on the
spiked covariance model, our simulations show that our algorithm is successful
on much more general models for the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0036</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0036</id><created>2013-06-28</created><authors><author><keyname>Jassim</keyname><forenames>Firas A.</forenames></author></authors><title>Increasing Compression Ratio in PNG Images by k-Modulus Method for Image
  Transformation</title><categories>cs.CV cs.MM</categories><comments>10 pages, 7 figures, 2 tables</comments><journal-ref>International Journal of Advanced Research in Computer Science and
  Software Engineering, Vol. 3, issue 6, pp. 45-52,June 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Image compression is an important filed in image processing. The science
welcomes any tinny contribution that may increase the compression ratio by
whichever insignificant percentage. Therefore, the essential contribution in
this paper is to increase the compression ratio for the well known Portable
Network Graphics (PNG) image file format. The contribution starts with
converting the original PNG image into k-Modulus Method (k-MM). Practically,
taking k equals to ten, and then the pixels in the constructed image will be
integers divisible by ten. Since PNG uses Lempel-Ziv compression algorithm,
then the ability to reduce file size will increase according to the repetition
in pixels in each k-by-k window according to the transformation done by k-MM.
Experimental results show that the proposed technique (k-PNG) produces high
compression ratio with smaller file size in comparison to the original PNG
file.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0044</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0044</id><created>2013-06-28</created><updated>2014-05-14</updated><authors><author><keyname>Gorlatova</keyname><forenames>Maria</forenames></author><author><keyname>Sarik</keyname><forenames>John</forenames></author><author><keyname>Grebla</keyname><forenames>Guy</forenames></author><author><keyname>Cong</keyname><forenames>Mina</forenames></author><author><keyname>Kymissis</keyname><forenames>Ioannis</forenames></author><author><keyname>Zussman</keyname><forenames>Gil</forenames></author></authors><title>Movers and Shakers: Kinetic Energy Harvesting for the Internet of Things</title><categories>cs.ET cs.NI cs.PF</categories><comments>15 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous energy harvesting wireless devices that will serve as building
blocks for the Internet of Things (IoT) are currently under development.
However, there is still only limited understanding of the properties of various
energy sources and their impact on energy harvesting adaptive algorithms.
Hence, we focus on characterizing the kinetic (motion) energy that can be
harvested by a wireless node with an IoT form factor and on developing energy
allocation algorithms for such nodes. In this paper, we describe methods for
estimating harvested energy from acceleration traces. To characterize the
energy availability associated with specific human activities (e.g., relaxing,
walking, cycling), we analyze a motion dataset with over 40 participants. Based
on acceleration measurements that we collected for over 200 hours, we study
energy generation processes associated with day-long human routines. We also
briefly summarize our experiments with moving objects. We develop energy
allocation algorithms that take into account practical IoT node design
considerations, and evaluate the algorithms using the collected measurements.
Our observations provide insights into the design of motion energy harvesters,
IoT nodes, and energy harvesting adaptive algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0048</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0048</id><created>2013-06-28</created><authors><author><keyname>Yang</keyname><forenames>Kun</forenames></author></authors><title>Simple one-pass algorithm for penalized linear regression with
  cross-validation on MapReduce</title><categories>stat.ML cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a one-pass algorithm on MapReduce for penalized
linear regression
  \[f_\lambda(\alpha, \beta) = \|Y - \alpha\mathbf{1} - X\beta\|_2^2 +
p_{\lambda}(\beta)\] where $\alpha$ is the intercept which can be omitted
depending on application; $\beta$ is the coefficients and $p_{\lambda}$ is the
penalized function with penalizing parameter $\lambda$. $f_\lambda(\alpha,
\beta)$ includes interesting classes such as Lasso, Ridge regression and
Elastic-net. Compared to latest iterative distributed algorithms requiring
multiple MapReduce jobs, our algorithm achieves huge performance improvement;
moreover, our algorithm is exact compared to the approximate algorithms such as
parallel stochastic gradient decent. Moreover, what our algorithm distinguishes
with others is that it trains the model with cross validation to choose optimal
$\lambda$ instead of user specified one.
  Key words: penalized linear regression, lasso, elastic-net, ridge, MapReduce
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0049</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0049</id><created>2013-06-28</created><authors><author><keyname>Dudev</keyname><forenames>Minko</forenames></author><author><keyname>Gerling</keyname><forenames>Sebastian</forenames></author><author><keyname>Peter</keyname><forenames>Philip</forenames></author></authors><title>SAHER: Secure and Efficient Routing in Sensor Networks</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an increasing amount of research is being done on various applications of
sensor networks in adversarial environments, ensuring secure routing becomes of
critical importance for the success of such deployments. The problem of
designing a secure routing protocol for ad hoc networks has been already
addressed, yet, there exists no complete solution that meets the specific
requirements of sensor networks, where nodes are extremely constrained in terms
of both power and computational resources. Thus, we propose a new protocol that
is not built solely around security but also has efficiency and simplicity
among its main goals. We propose the Secure Ad Hoc Efficient Routing protocol
(SAHER) which employs a two-tier architecture based on node clustering. Also,
we combine mechanisms like localscale geographic routing, per-node reputation
tables, credit based alternate route enforcement and cumulative authentication.
Using these techniques we examine ways to efficiently defend against the two
most common network layer attacks: selective packet dropping and message
flooding. Further, we consider join/leave operations which have not yet been
studied in sufficient depth for sensor networks from a security standpoint. We
provide a description of the protocol along with comprehensive experimental
evaluation under different node distributions, different proportions of
non-malicious vs. malicious nodes and different types of activity that
malicious nodes could exhibit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0052</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0052</id><created>2013-06-28</created><authors><author><keyname>Fang</keyname><forenames>Zhaoxi</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author></authors><title>Beamforming Design for Multiuser Two-Way Relaying: A Unified Approach
  via Max-Min SINR</title><categories>cs.IT math.IT</categories><comments>31 pages, 9 figures, submitted to IEEE Trans. Signal Process</comments><doi>10.1109/TSP.2013.2281032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a unified framework for beamforming designs in
non-regenerative multiuser two-way relaying (TWR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0060</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0060</id><created>2013-06-28</created><authors><author><keyname>Mansinghka</keyname><forenames>Vikash K.</forenames></author><author><keyname>Kulkarni</keyname><forenames>Tejas D.</forenames></author><author><keyname>Perov</keyname><forenames>Yura N.</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Approximate Bayesian Image Interpretation using Generative Probabilistic
  Graphics Programs</title><categories>cs.AI cs.CV stat.ML</categories><comments>The first two authors contributed equally to this work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of computer vision as the Bayesian inverse problem to computer
graphics has a long history and an appealing elegance, but it has proved
difficult to directly implement. Instead, most vision tasks are approached via
complex bottom-up processing pipelines. Here we show that it is possible to
write short, simple probabilistic graphics programs that define flexible
generative models and to automatically invert them to interpret real-world
images. Generative probabilistic graphics programs consist of a stochastic
scene generator, a renderer based on graphics software, a stochastic likelihood
model linking the renderer's output and the data, and latent variables that
adjust the fidelity of the renderer and the tolerance of the likelihood model.
Representations and algorithms from computer graphics, originally designed to
produce high-quality images, are instead used as the deterministic backbone for
highly approximate and stochastic generative models. This formulation combines
probabilistic programming, computer graphics, and approximate Bayesian
computation, and depends only on general-purpose, automatic inference
techniques. We describe two applications: reading sequences of degraded and
adversarially obscured alphanumeric characters, and inferring 3D road models
from vehicle-mounted camera images. Each of the probabilistic graphics programs
we present relies on under 20 lines of probabilistic code, and supports
accurate, approximately Bayesian inferences about ambiguous real-world images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0067</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0067</id><created>2013-06-29</created><updated>2015-01-05</updated><authors><author><keyname>Naghshvar</keyname><forenames>Mohammad</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Extrinsic Jensen-Shannon Divergence: Applications to Variable-Length
  Coding</title><categories>cs.IT math.IT math.OC math.ST stat.TH</categories><comments>17 pages (two-column), 4 figures, to appear in IEEE Transactions on
  Information Theory</comments><msc-class>94A24, 93E35, 93E20, 68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of variable-length coding over a discrete
memoryless channel (DMC) with noiseless feedback. The paper provides a
stochastic control view of the problem whose solution is analyzed via a newly
proposed symmetrized divergence, termed extrinsic Jensen-Shannon (EJS)
divergence. It is shown that strictly positive lower bounds on EJS divergence
provide non-asymptotic upper bounds on the expected code length. The paper
presents strictly positive lower bounds on EJS divergence, and hence
non-asymptotic upper bounds on the expected code length, for the following two
coding schemes: variable-length posterior matching and MaxEJS coding scheme
which is based on a greedy maximization of the EJS divergence.
  As an asymptotic corollary of the main results, this paper also provides a
rate-reliability test. Variable-length coding schemes that satisfy the
condition(s) of the test for parameters $R$ and $E$, are guaranteed to achieve
rate $R$ and error exponent $E$. The results are specialized for posterior
matching and MaxEJS to obtain deterministic one-phase coding schemes achieving
capacity and optimal error exponent. For the special case of symmetric
binary-input channels, simpler deterministic schemes of optimal performance are
proposed and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0072</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0072</id><created>2013-06-29</created><authors><author><keyname>Riadi</keyname><forenames>Imam</forenames></author><author><keyname>Istiyanto</keyname><forenames>Jazi Eko</forenames></author><author><keyname>Ashari</keyname><forenames>Ahmad</forenames></author><author><keyname>Subanar</keyname></author></authors><title>Log Analysis Techniques using Clustering in Network Forensics</title><categories>cs.CY cs.CR</categories><comments>8 pages, 13 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Internet crimes are now increasing. In a row with many crimes using
information technology, in particular those using Internet, some crimes are
often carried out in the form of attacks that occur within a particular agency
or institution. To be able to find and identify the types of attacks, requires
a long process that requires time, human resources and utilization of
information technology to solve these problems. The process of identifying
attacks that happened also needs the support of both hardware and software as
well. The attack happened in the Internet network can generally be stored in a
log file that has a specific data format. Clustering technique is one of
methods that can be used to facilitate the identification process. Having
grouped the data log file using K-means clustering technique, then the data is
grouped into three categories of attack, and will be continued with the
forensic process that can later be known to the source and target of attacks
that exist in the network. It is concluded that the framework proposed can help
the investigator in the trial process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0076</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0076</id><created>2013-06-29</created><authors><author><keyname>James</keyname><forenames>Joshua I.</forenames></author><author><keyname>Jang</keyname><forenames>Yunsik Jake</forenames></author></authors><title>An Assessment Model for Cybercrime Investigation Capacity</title><categories>cs.CY</categories><comments>1 figure, World Crime Forum 1st Asian Regional Conference -
  Information Society and Cybercrime: Challenges for Criminology and Criminal
  Justice</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Digital technologies are constantly changing, and with it criminals are
finding new ways to abuse these technologies. Cybercrime investigators, then,
must also keep their skills and knowledge up to date. This work proposes a
holistic training development model - specifically focused on cybercrime
investigation - that is based on improving investigator capability while also
considering the capacity of the investigator or unit. Along with a training
development model, a cybercrime investigation capacity assessment framework is
given for attempting to measure capacity throughout the education process.
First, a training development model is proposed that focuses on the expansion
of investigation capability as well as capacity of investigators and units.
Next, a capacity assessment model is given to evaluate the effectiveness of the
training program. A description of how the proposed model is being applied to
the development of training programs for cybercrime investigators in developing
countries will then be given, as well as already observed challenges. Finally,
concluding remarks as well as proposed future work is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0082</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0082</id><created>2013-06-29</created><authors><author><keyname>Khlaif</keyname><forenames>M.</forenames></author><author><keyname>Talb</keyname><forenames>M.</forenames></author></authors><title>Digital Data Security and Copyright Protection Using Cellular Automata</title><categories>cs.CR</categories><comments>04 pages, 2 figures, IJCSN Journal</comments><report-no>IJCSN-2013-2-3-13</report-no><journal-ref>International Journal of Computer Science and Network-IJCSN ,
  Volume 2, Issue 3, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of many challenges and the rapid development of the means of
communications and computer networks and the Internet. Digital information
revolution has affected a lot on human societies. Data today has become
available in digital format (text, image, audio, and video), which led to the
emergence of many opportunities for creativity for innovation as well as the
emergence of a new kind of challenges
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0084</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0084</id><created>2013-06-29</created><updated>2013-07-07</updated><authors><author><keyname>Kaltiokallio</keyname><forenames>Ossi</forenames></author><author><keyname>Yigitler</keyname><forenames>H&#xfc;seyin</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author></authors><title>Catch a Breath: Non-invasive Respiration Rate Monitoring via Wireless
  Communication</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio signals are sensitive to changes in the environment, which for example
is reflected on the received signal strength (RSS) measurements of low-cost
wireless devices. This information has been used effectively in the past years
e.g. in device-free localization and tracking. Recent literature has also shown
that the fading information of the wireless channel can be exploited to
estimate the breathing rate of a person in a non-invasive manner; a research
topic we address in this paper. To the best of our knowledge, we demonstrate
for the first time that the respiration rate of a person can be accurately
estimated using only a single IEEE 802.15.4 compliant TX-RX pair. We exploit
channel diversity, low-jitter periodic communication, and oversampling to
enhance the breathing estimates, and make use of a decimation filter to
decrease the computational requirements of breathing estimation. In addition,
we develop a hidden Markov model (HMM) to identify the time instances when
breathing estimation is not possible, i.e., during times when other motion than
breathing occurs. We experimentally validate the accuracy of the system and the
results suggest that the respiration rate can be estimated with a mean error of
0.03 breaths per minute, the lowest breathing rate error reported to date using
IEEE 802.15.4 compliant transceivers. We also demonstrate that the breathing of
two people can be monitored simultaneously, a result not reported in earlier
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0085</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0085</id><created>2013-06-29</created><authors><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Coded Slotted ALOHA with Varying Packet Loss Rate across Users</title><categories>cs.IT math.IT</categories><comments>4 pages, submitted to GlobalSIP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent research has established an analogy between successive
interference cancellation in slotted ALOHA framework and iterative
belief-propagation erasure-decoding, which has opened the possibility to
enhance random access protocols by utilizing theory and tools of
erasure-correcting codes. In this paper we present a generalization of the
and-or tree evaluation, adapted for the asymptotic analysis of the slotted
ALOHA-based random-access protocols, for the case when the contending users
experience different channel conditions, resulting in packet loss probability
that varies across users. We apply the analysis to the example of frameless
ALOHA, where users contend on a slot basis. We present results regarding the
optimal access probabilities and contention period lengths, such that the
throughput and probability of user resolution are maximized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0087</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0087</id><created>2013-06-29</created><authors><author><keyname>Lolli</keyname><forenames>Fabrizio M. A.</forenames></author></authors><title>Semantics and pragmatics in actual software applications and in web
  search engines: exploring innovations</title><categories>cs.IR cs.CL cs.HC</categories><journal-ref>International Symposium on Language and Communication: Exploring
  Novelties, vol.3, 955-962. IICS, 2013, Turkey</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While new ways to use the Semantic Web are developed every week, which allow
the user to find information on web more accurately - for example in search
engines - some sophisticated pragmatic tools are becoming more important - for
example in web interfaces known as Social Intelligence, or in the most famous
Siri by Apple. The work aims to analyze whether and where we can identify the
boundary between semantics and pragmatics in the software used by analyzed
systems. examining how the linguistic disciplines are fundamental in their
progress. Is it possible to assume that the tools of social intelligence have a
pragmatic approach to the questions of the user, or it is just a use of a very
rich vocabulary, with the use of semantic tools?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0099</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0099</id><created>2013-06-29</created><authors><author><keyname>Fredriksson</keyname><forenames>Kimmo</forenames></author><author><keyname>Giaquinta</keyname><forenames>Emanuele</forenames></author></authors><title>On a compact encoding of the swap automaton</title><categories>cs.FL cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a string $P$ of length $m$ over an alphabet $\Sigma$ of size $\sigma$,
a swapped version of $P$ is a string derived from $P$ by a series of local
swaps, i.e., swaps of adjacent symbols, such that each symbol can participate
in at most one swap. We present a theoretical analysis of the nondeterministic
finite automaton for the language $\bigcup_{P'\in\Pi_P}\Sigma^*P'$ (swap
automaton for short), where $\Pi_P$ is the set of swapped versions of $P$. Our
study is based on the bit-parallel simulation of the same automaton due to
Fredriksson, and reveals an interesting combinatorial property that links the
automaton to the one for the language $\Sigma^*P$. By exploiting this property
and the method presented by Cantone et al. (2010), we obtain a bit-parallel
encoding of the swap automaton which takes $O(\sigma^2\ceil{k/w})$ space and
allows one to simulate the automaton on a string of length $n$ in time
$O(n\ceil{k/w})$, where $\ceil{m/\sigma}\le k\le m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0100</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0100</id><created>2013-06-29</created><authors><author><keyname>Solnushkin</keyname><forenames>Konstantin S.</forenames></author><author><keyname>Tsujita</keyname><forenames>Yuichi</forenames></author></authors><title>Marrying Many-core Accelerators and InfiniBand for a New Commodity
  Processor</title><categories>cs.DC cs.AR</categories><comments>5 pages, 4 figures. The work was presented at one of the workshops at
  the International Conference on Computational Science (ICCS 2013) in
  Barcelona, Spain. However, due to prolonged deadlines, papers from this
  workshop didn't get into conference proceedings; we rectify this by
  submitting the paper to arXiv.org</comments><acm-class>C.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last 15 years, the supercomputing industry has been using
mass-produced, off-the-shelf components to build cluster computers. Such
components are not perfect for HPC purposes, but are cheap due to effect of
scale in their production. The coming exa-scale era changes the landscape:
exa-scale computers will contain components in quantities large enough to
justify their custom development and production.
  We propose a new heterogeneous processor, equipped with a network controller
and designed specifically for HPC. We then show how it can be used for
enterprise computing market, guaranteeing its widespread adoption and therefore
low production costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0108</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0108</id><created>2013-06-29</created><authors><author><keyname>Benini</keyname><forenames>Marco</forenames></author></authors><title>Intuitionistic First-Order Logic: Categorical Semantics via the
  Curry-Howard Isomorphism</title><categories>math.LO cs.LO</categories><comments>92 pages</comments><msc-class>03F55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This reports introduces a novel sound and complete semantics for first order
intuitionistic logic, in the framework of category theory and by the
computational interpretation of the logic based on the so-called Curry-Howard
isomorphism. Aside, a sound and complete semantics for the corresponding
lambda-calculus is derived, too. This semantics extends, in a way, the more
traditional meanings given by Heyting categories, the topos-theoretic
interpretation, and Kripke models. The feature which justifies the introduction
of this novel semantics is the fact that it is 'point-free', i.e., there is no
universe whose elements are used to interpret the logical terms. In other
words, terms do not denote individuals of some collection but, instead, they
denote the 'glue' which keeps together the interpretations of statements,
similarly to what happens in formal topology. Since the proposed semantics can
be trivially extended to all the first-order logical theories based on the
intuitionistic system (and, with some care, to minimal systems as well), the
semantics covers also all the predicative theories, even if some peculiar
aspects of these theories should be remarked.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0110</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0110</id><created>2013-06-29</created><authors><author><keyname>Cabezas-Clavijo</keyname><forenames>Alvaro</forenames></author><author><keyname>Robinson-Garcia</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Escabias</keyname><forenames>Manuel</forenames></author><author><keyname>Jim&#xe9;nez-Contreras</keyname><forenames>Evaristo</forenames></author></authors><title>Reviewers' ratings and bibliometric indicators: hand in hand when
  assessing over research proposals?</title><categories>cs.DL</categories><journal-ref>PLoS ONE 8(6): e68258</journal-ref><doi>10.1371/journal.pone.0068258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The peer review system has been traditionally challenged due to its many
limitations especially for allocating funding. Bibliometric indicators may well
present themselves as a complement. Objective: We analyze the relationship
between peers' ratings and bibliometric indicators for Spanish researchers in
the 2007 National R&amp;D Plan for 23 research fields. We analyze peers' ratings
for 2333 applications. We also gathered principal investigators' research
output and impact and studied the differences between accepted and rejected
applications. We used the Web of Science database and focused on the 2002-2006
period. First, we analyzed the distribution of granted and rejected proposals
considering a given set of bibliometric indicators to test if there are
significant differences. Then, we applied a multiple logistic regression
analysis to determine if bibliometric indicators can explain by themselves the
concession of grant proposals. 63.4% of the applications were funded.
Bibliometric indicators for accepted proposals showed a better previous
performance than for those rejected; however the correlation between peer
review and bibliometric indicators is very heterogeneous among most areas. The
logistic regression analysis showed that the main bibliometric indicators that
explain the granting of research proposals in most cases are the output (number
of published articles) and the number of papers published in journals that
belong to the first quartile ranking of the Journal Citations Report.
Bibliometric indicators predict the concession of grant proposals at least as
well as peer ratings. Social Sciences and Education are the only areas where no
relation was found, although this may be due to the limitations of the Web of
Science's coverage. These findings encourage the use of bibliometric indicators
as a complement to peer review in most of the analyzed areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0118</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0118</id><created>2013-06-29</created><updated>2014-05-10</updated><authors><author><keyname>Zhu</keyname><forenames>Yanshu</forenames></author><author><keyname>Sun</keyname><forenames>Feng</forenames></author><author><keyname>Choi</keyname><forenames>Yi-King</forenames></author><author><keyname>J&#xfc;ttler</keyname><forenames>Bert</forenames></author><author><keyname>Wang</keyname><forenames>Wenping</forenames></author></authors><title>Computing a Compact Spline Representation of the Medial Axis Transform
  of a 2D Shape</title><categories>cs.GR</categories><comments>GMP14 (Geometric Modeling and Processing)</comments><doi>10.1016/j.gmod.2014.03.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a full pipeline for computing the medial axis transform of an
arbitrary 2D shape. The instability of the medial axis transform is overcome by
a pruning algorithm guided by a user-defined Hausdorff distance threshold. The
stable medial axis transform is then approximated by spline curves in 3D to
produce a smooth and compact representation. These spline curves are computed
by minimizing the approximation error between the input shape and the shape
represented by the medial axis transform. Our results on various 2D shapes
suggest that our method is practical and effective, and yields faithful and
compact representations of medial axis transforms of 2D shapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0127</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0127</id><created>2013-06-29</created><authors><author><keyname>Lattimore</keyname><forenames>Tor</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Sunehag</keyname><forenames>Peter</forenames></author></authors><title>Concentration and Confidence for Discrete Bayesian Sequence Predictors</title><categories>cs.LG stat.ML</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian sequence prediction is a simple technique for predicting future
symbols sampled from an unknown measure on infinite sequences over a countable
alphabet. While strong bounds on the expected cumulative error are known, there
are only limited results on the distribution of this error. We prove tight
high-probability bounds on the cumulative error, which is measured in terms of
the Kullback-Leibler (KL) divergence. We also consider the problem of
constructing upper confidence bounds on the KL and Hellinger errors similar to
those constructed from Hoeffding-like bounds in the i.i.d. case. The new
results are applied to show that Bayesian sequence prediction can be used in
the Knows What It Knows (KWIK) framework with bounds that match the
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0129</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0129</id><created>2013-06-29</created><authors><author><keyname>Rajabi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Ghassemian</keyname><forenames>Hassan</forenames></author></authors><title>Hyperspectral Data Unmixing Using GNMF Method and Sparseness Constraint</title><categories>cs.CV</categories><comments>4 pages, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral images contain mixed pixels due to low spatial resolution of
hyperspectral sensors. Mixed pixels are pixels containing more than one
distinct material called endmembers. The presence percentages of endmembers in
mixed pixels are called abundance fractions. Spectral unmixing problem refers
to decomposing these pixels into a set of endmembers and abundance fractions.
Due to nonnegativity constraint on abundance fractions, nonnegative matrix
factorization methods (NMF) have been widely used for solving spectral unmixing
problem. In this paper we have used graph regularized (GNMF) method with
sparseness constraint to unmix hyperspectral data. This method applied on
simulated data using AVIRIS Indian Pines dataset and USGS library and results
are quantified based on AAD and SAD measures. Results in comparison with other
methods show that the proposed method can unmix data more effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0147</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0147</id><created>2013-06-29</created><updated>2013-11-03</updated><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Zhao</keyname><forenames>Xin</forenames></author><author><keyname>Qin</keyname><forenames>Hong</forenames></author></authors><title>4-Dimensional Geometry Lens: A Novel Volumetric Magnification Approach</title><categories>cs.GR</categories><comments>12 pages. In CGF 2013. This paper has been withdrawn by the author
  due to publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel methodology that utilizes 4-Dimensional (4D) space
deformation to simulate a magnification lens on versatile volume datasets and
textured solid models. Compared with other magnification methods (e.g.,
geometric optics, mesh editing), 4D differential geometry theory and its
practices are much more flexible and powerful for preserving shape features
(i.e., minimizing angle distortion), and easier to adapt to versatile solid
models. The primary advantage of 4D space lies at the following fact: we can
now easily magnify the volume of regions of interest (ROIs) from the additional
dimension, while keeping the rest region unchanged. To achieve this primary
goal, we first embed a 3D volumetric input into 4D space and magnify ROIs in
the 4th dimension. Then we flatten the 4D shape back into 3D space to
accommodate other typical applications in the real 3D world. In order to
enforce distortion minimization, in both steps we devise the high dimensional
geometry techniques based on rigorous 4D geometry theory for 3D/4D mapping back
and forth to amend the distortion. Our system can preserve not only focus
region, but also context region and global shape. We demonstrate the
effectiveness, robustness, and efficacy of our framework with a variety of
models ranging from tetrahedral meshes to volume datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0150</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0150</id><created>2013-06-29</created><updated>2014-01-13</updated><authors><author><keyname>Willard</keyname><forenames>Dan E.</forenames></author></authors><title>On the Significance of Self-Justifying Axiom Systems from the
  Perspective of Analytic Tableaux</title><categories>math.LO cs.LO</categories><comments>Cover page + 33 pages of text. All the theorems in this January 2014
  manuscript also had appeared in the earlier &quot;Version 1&quot; June 2013 draft. The
  main difference between these two drafts is that I substantially spiced up
  the accompanying narrative, so as to better explain the motivation for this
  research endeavor</comments><msc-class>03B52, 03F25, 03F45, 03H13</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article will be a continuation of our research into self-justifying
systems. It will introduce several new theorems and their applications. (One of
these results will transform our previous infinite-sized self-verifying
formalisms into tighter systems, with only a finite number of axioms.) It will
explain how self-justification is useful, even when the Incompleteness Theorem
limits its reach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0155</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0155</id><created>2013-06-29</created><authors><author><keyname>Pe&#xf1;a</keyname><forenames>Norberto</forenames></author><author><keyname>Credidio</keyname><forenames>Bruno Cec&#xed;lio</forenames></author><author><keyname>Corr&#xea;a</keyname><forenames>Lorena Peixoto Nogueira Rodriguez Martinez Salles</forenames></author><author><keyname>Fran&#xe7;a</keyname><forenames>Lucas Gabriel Souza</forenames></author><author><keyname>Cunha</keyname><forenames>Marcelo do Vale</forenames></author><author><keyname>de Sousa</keyname><forenames>Marcos Cavalcanti</forenames></author><author><keyname>Vieira</keyname><forenames>Jo&#xe3;o Paulo Bomfim Cruz</forenames></author><author><keyname>Miranda</keyname><forenames>Jos&#xe9; Garcia Vivas</forenames></author></authors><title>Free Instrument for Movement Measure</title><categories>physics.ins-det cs.GR physics.pop-ph</categories><comments>Accepted for publication at the RBEF - Revista Brasileira de Ensino
  de F\'isica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the validation of a computational tool that serves to
obtain continuous measurements of moving objects. The software uses techniques
of computer vision, pattern recognition and optical flow, to enable tracking of
objects in videos, generating data trajectory, velocity, acceleration and
angular movement. The program was applied to track a ball around a simple
pendulum. The methodology used to validate it, taking as a basis to compare the
values measured by the program, as well as the theoretical values expected
according to the model of a simple pendulum. The experiment is appropriate to
the method because it was built within the limits of the linear harmonic
oscillator and energy losses due to friction had been minimized, making it the
most ideal possible. The results indicate that the tool is sensitive and
accurate. Deviations of less than a millimeter to the extent of the trajectory,
ensures the applicability of the software on physics, whether in research or in
teaching topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0156</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0156</id><created>2013-06-29</created><updated>2015-04-08</updated><authors><author><keyname>Murta</keyname><forenames>Gl&#xe1;ucia</forenames></author><author><keyname>Cunha</keyname><forenames>Marcelo Terra</forenames></author><author><keyname>Cabello</keyname><forenames>Ad&#xe1;n</forenames></author></authors><title>Quantum nonlocality as the route for ever-lasting unconditionally secure
  bit commitment</title><categories>quant-ph cs.CR</categories><comments>This paper has been withdrawn by the authors due to an insecurity
  found in the protocol</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a bit commitment protocol based on quantum nonlocality that seems
to bring ever-lasting unconditional security. Although security is not
rigorously proved, physical arguments and numerical simulations support this
conclusion. The key point is that the proof of the commitment is forced to
become classical data uncorrelated with anything else. This allows us to
circumvent previous impossibility proofs in which it is assumed that classical
data can be replaced by quantum data that may be entangled with the committer.
The proposed protocol also recovers two features missing in recent
&quot;relativistic&quot; quantum bit commitment protocols: (i) the committer can decide
if and when she wants to reveal the commitment and (ii) the security of the
commitment lasts for arbitrary long time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0180</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0180</id><created>2013-06-30</created><authors><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Kong</keyname><forenames>Qiong</forenames></author></authors><title>One generator $(1+u)$-quasi twisted codes over $F_2+uF_2$</title><categories>cs.IT math.IT</categories><comments>7 pages. In Mathematical Computation, 2013(1)</comments><msc-class>94B05</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper gives the minimum generating sets of three types of one generator
$(1+u)$-quasi twisted (QT) codes over $F_2+uF_2$, $u^2=0$. Moreover, it
discusses the generating sets and the lower bounds on the minimum Lee distance
of a special class of $A_2$ type one generator $(1+u)$-QT codes. Some good
(optimal or suboptimal) linear codes over $F_2$ are obtained by these types of
one generator $(1+u)$-QT codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0187</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0187</id><created>2013-06-30</created><authors><author><keyname>Ahmed</keyname><forenames>Qasim Zeeshan</forenames></author><author><keyname>Park</keyname><forenames>Ki-Hong</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Aissa</keyname><forenames>Sonia</forenames></author></authors><title>Compression and Combining Based on Channel Shortening and Rank Reduction
  Techniques for Cooperative Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>In IEEE Transactions on Vehicular Technology, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates and compares the performance of wireless sensor
networks where sensors operate on the principles of cooperative communications.
We consider a scenario where the source transmits signals to the destination
with the help of $L$ sensors. As the destination has the capacity of processing
only $U$ out of these $L$ signals, the strongest $U$ signals are selected while
the remaining $(L-U)$ signals are suppressed. A preprocessing block similar to
channel-shortening is proposed in this contribution. However, this
preprocessing block employs a rank-reduction technique instead of
channel-shortening. By employing this preprocessing, we are able to decrease
the computational complexity of the system without affecting the bit error rate
(BER) performance. From our simulations, it can be shown that these schemes
outperform the channel-shortening schemes in terms of computational complexity.
In addition, the proposed schemes have a superior BER performance as compared
to channel-shortening schemes when sensors employ fixed gain amplification.
However, for sensors which employ variable gain amplification, a tradeoff
exists in terms of BER performance between the channel-shortening and these
schemes. These schemes outperform channel-shortening scheme for lower
signal-to-noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0189</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0189</id><created>2013-06-30</created><authors><author><keyname>Dumas</keyname><forenames>Philippe</forenames></author></authors><title>Rational series and asymptotic expansion for linear homogeneous
  divide-and-conquer recurrences</title><categories>cs.CC</categories><msc-class>11A63, 41A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among all sequences that satisfy a divide-and-conquer recurrence, the
sequences that are rational with respect to a numeration system are certainly
the most immediate and most essential. Nevertheless, until recently they have
not been studied from the asymptotic standpoint. We show how a mechanical
process permits to compute their asymptotic expansion. It is based on linear
algebra, with Jordan normal form, joint spectral radius, and dilation
equations. The method is compared with the analytic number theory approach,
based on Dirichlet series and residues, and new ways to compute the Fourier
series of the periodic functions involved in the expansion are developed. The
article comes with an extended bibliography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0191</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0191</id><created>2013-06-30</created><authors><author><keyname>Moniruzzaman</keyname><forenames>A B M</forenames></author><author><keyname>Hossain</keyname><forenames>Syed Akhter</forenames></author></authors><title>NoSQL Database: New Era of Databases for Big data Analytics -
  Classification, Characteristics and Comparison</title><categories>cs.DB</categories><comments>14 pages, 10 figures, 44 references used and with authors biographies</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Digital world is growing very fast and become more complex in the volume
(terabyte to petabyte), variety (structured and un-structured and hybrid),
velocity (high speed in growth) in nature. This refers to as Big Data that is a
global phenomenon. This is typically considered to be a data collection that
has grown so large it can not be effectively managed or exploited using
conventional data management tools: e.g., classic relational database
management systems (RDBMS) or conventional search engines. To handle this
problem, traditional RDBMS are complemented by specifically designed a rich set
of alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This
paper motivation is to provide - classification, characteristics and evaluation
of NoSQL databases in Big Data Analytics. This report is intended to help
users, especially to the organizations to obtain an independent understanding
of the strengths and weaknesses of various NoSQL database approaches to
supporting applications that process huge volumes of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0193</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0193</id><created>2013-06-30</created><authors><author><keyname>Nirkhiwale</keyname><forenames>Supriya</forenames></author><author><keyname>Dobra</keyname><forenames>Alin</forenames></author><author><keyname>Jermaine</keyname><forenames>Chris</forenames></author></authors><title>A Sampling Algebra for Aggregate Estimation</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As of 2005, sampling has been incorporated in all major database systems.
While efficient sampling techniques are realizable, determining the accuracy of
an estimate obtained from the sample is still an unresolved problem. In this
paper, we present a theoretical framework that allows an elegant treatment of
the problem. We base our work on generalized uniform sampling (GUS), a class of
sampling methods that subsumes a wide variety of sampling techniques. We
introduce a key notion of equivalence that allows GUS sampling operators to
commute with selection and join, and derivation of confidence intervals. We
illustrate the theory through extensive examples and give indications on how to
use it to provide meaningful estimations in database systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0194</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0194</id><created>2013-06-30</created><authors><author><keyname>Liang</keyname><forenames>Wang</forenames></author><author><keyname>KaiYong</keyname><forenames>Zhao</forenames></author></authors><title>A new DNA alignment method based on inverted index</title><categories>q-bio.GN cs.CE</categories><comments>7 pages;5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel DNA sequences alignment method based on inverted
index. Now most large scale information retrieval system are all use inverted
index as the basic data structure. But its application in DNA sequence
alignment is still not found. This paper just discuss such applications. Three
main problems, DNA segmenting, long DNA query search, DNA search ranking
algorithm and evaluation method are detailed respectively. This research
presents a new avenue to build more effective DNA alignment methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0201</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0201</id><created>2013-06-30</created><updated>2013-07-26</updated><authors><author><keyname>Hetland</keyname><forenames>Magnus Lie</forenames></author></authors><title>Simulating Ability: Representing Skills in Games</title><categories>cs.GT cs.AI</categories><journal-ref>Serious Games Development and Applications. Lecture Notes in
  Computer Science Volume 8101, 2013, pp 226-238</journal-ref><doi>10.1007/978-3-642-40790-1_22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Throughout the history of games, representing the abilities of the various
agents acting on behalf of the players has been a central concern. With
increasingly sophisticated games emerging, these simulations have become more
realistic, but the underlying mechanisms are still, to a large extent, of an ad
hoc nature. This paper proposes using a logistic model from psychometrics as a
unified mechanism for task resolution in simulation-oriented games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0204</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0204</id><created>2013-06-30</created><updated>2013-09-14</updated><authors><author><keyname>Bruni</keyname><forenames>Roberto</forenames><affiliation>Dipartimento di Informatica, Universita di Pisa</affiliation></author><author><keyname>Melgratti</keyname><forenames>Hernan</forenames><affiliation>FCEyN, Universidad de Buenos Aires - Conicet</affiliation></author><author><keyname>Montanari</keyname><forenames>Ugo</forenames><affiliation>Dipartimento di Informatica, Universita di Pisa</affiliation></author><author><keyname>Sobocinski</keyname><forenames>Pawel</forenames><affiliation>ECS, University of Southampton</affiliation></author></authors><title>Connector algebras for C/E and P/T nets' interactions</title><categories>cs.FL</categories><comments>64 pages, 21 figures</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (September
  17, 2013) lmcs:883</journal-ref><doi>10.2168/LMCS-9(3:16)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quite flourishing research thread in the recent literature on
component-based systems is concerned with the algebraic properties of different
classes of connectors. In a recent paper, an algebra of stateless connectors
was presented that consists of five kinds of basic connectors, namely symmetry,
synchronization, mutual exclusion, hiding and inaction, plus their duals, and
it was shown how they can be freely composed in series and in parallel to model
sophisticated 'glues'. In this paper we explore the expressiveness of stateful
connectors obtained by adding one-place buffers or unbounded buffers to the
stateless connectors. The main results are: i) we show how different classes of
connectors exactly correspond to suitable classes of Petri nets equipped with
compositional interfaces, called nets with boundaries; ii) we show that the
difference between strong and weak semantics in stateful connectors is
reflected in the semantics of nets with boundaries by moving from the classic
step semantics (strong case) to a novel banking semantics (weak case), where a
step can be executed by taking some 'debit' tokens to be given back during the
same step; iii) we show that the corresponding bisimilarities are congruences
(w.r.t. composition of connectors in series and in parallel); iv) we show that
suitable monoidality laws, like those arising when representing stateful
connectors in the tile model, can nicely capture concurrency (in the sense of
step semantics) aspects; and v) as a side result, we provide a basic algebra,
with a finite set of symbols, out of which we can compose all P/T nets with
boundaries, fulfilling a long standing quest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0214</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0214</id><created>2013-06-30</created><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author></authors><title>Dynamic Traitor Tracing Schemes, Revisited</title><categories>cs.CR</categories><comments>7 pages, 1 figure (6 subfigures), 1 table</comments><journal-ref>IEEE Workshop on Information Forensics and Security (WIFS), pp.
  191-196, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit recent results from the area of collusion-resistant traitor
tracing, and show how they can be combined and improved to obtain more
efficient dynamic traitor tracing schemes. In particular, we show how the
dynamic Tardos scheme of Laarhoven et al. can be combined with the optimized
score functions of Oosterwijk et al. to trace coalitions much faster. If the
attack strategy is known, in many cases the order of the code length goes down
from quadratic to linear in the number of colluders, while if the attack is not
known, we show how the interleaving defense may be used to catch all colluders
about twice as fast as in the dynamic Tardos scheme. Some of these results also
apply to the static traitor tracing setting where the attack strategy is known
in advance, and to group testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0219</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0219</id><created>2013-06-30</created><updated>2013-11-16</updated><authors><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author></authors><title>Ornitolog\'ia Virtual: Caracterizando a #Chile en Twitter</title><categories>cs.SI</categories><comments>24 pages, technical report. In spanish</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Este art\'iculo presenta un an\'alisis de los tweets recolectados el 28 de
Octubre de 2012, en el contexto de las elecciones municipales de 2012 en Chile.
Dicho an\'alisis se realiza mediante una metodolog\'ia basada en literatura
previa, en particular en t\'ecnicas de recuperaci\'on de la informaci\'on y de
an\'alisis de espacios de informaci\'on. Como resultado, se determinan: 1)
caracter\'isticas demogr\'aficas b\'asicas de la poblaci\'on virtual chilena,
incluyendo su distribuci\'on geogr\'afica, 2) el contenido que caracteriza a
cada regi\'on, y c\'omo fluye informaci\'on entre regiones, y 3) el grado de
representatividad de la poblaci\'on virtual participante en el evento con
respecto a la poblaci\'on f\'isica. Se determina que la muestra obtenida es
representativa de la poblaci\'on en t\'erminos de distribuci\'on geogr\'afica,
que el centralismo que afecta al pa\'is se ve reflejado en Twitter, y que, a
pesar de los sesgos poblacionales, es posible identificar el contenido que
caracteriza a cada regi\'on. Se finaliza con una discusi\'on de las
implicaciones y conclusiones pr\'acticas de este trabajo, as\'i como futuras
aplicaciones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0220</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0220</id><created>2013-06-30</created><updated>2013-07-06</updated><authors><author><keyname>Formato</keyname><forenames>Richard A.</forenames></author></authors><title>Dipole-Loaded Monopole Optimized Using VSO, v.3</title><categories>cs.OH</categories><comments>arXiv admin note: substantial text overlap with arXiv:1107.1437,
  arXiv:1103.5629, arXiv:1108.0901, arXiv:1003.1039. Version 2, 02 Jul 2013:
  minor typos corrected; hill climber material added; source code listing
  updated. Version 3, 06 Jul 2013: replaces VSO diagram/pseudocode to clarify
  algorithm's elitist nature; other minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dipole-loaded monopole antenna is optimized for uniform hemispherical
coverage using VSO, a new global search design and optimization algorithm. The
antenna's performance is compared to genetic algorithm and hill-climber
optimized loaded monopoles, and VSO is tested against two suites of benchmark
functions and several other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0247</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0247</id><created>2013-06-30</created><authors><author><keyname>J&#xe4;hn</keyname><forenames>Claudius</forenames></author></authors><title>Progressive Blue Surfels</title><categories>cs.GR</categories><comments>Please note that this paper represents an early working draft, which
  will be subsequently replaced by refined versions! 3 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a new technique to generate and use surfels for
rendering of highly complex, polygonal 3D scenes in real time. The basic idea
is to approximate complex parts of the scene by rendering a set of points
(surfels). The points are computed in a preprocessing step and offer two
important properties: They are placed only on the visible surface of the
scene's geometry and they are distributed and sorted in such a way, that every
prefix of points is a good visual representation of the approximated part of
the scene. An early evaluation of the method shows that it is capable of
rendering scenes consisting of several billions of triangles with high image
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0252</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0252</id><created>2013-06-30</created><authors><author><keyname>Bair</keyname><forenames>Eric</forenames></author></authors><title>Semi-supervised clustering methods</title><categories>stat.ME cs.LG stat.ML</categories><comments>28 pages, 5 figures</comments><journal-ref>WIREs Comp Stat, 2013, 5(5): 349-361</journal-ref><doi>10.1002/wics.1270</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cluster analysis methods seek to partition a data set into homogeneous
subgroups. It is useful in a wide variety of applications, including document
processing and modern genetics. Conventional clustering methods are
unsupervised, meaning that there is no outcome variable nor is anything known
about the relationship between the observations in the data set. In many
situations, however, information about the clusters is available in addition to
the values of the features. For example, the cluster labels of some
observations may be known, or certain observations may be known to belong to
the same cluster. In other cases, one may wish to identify clusters that are
associated with a particular outcome variable. This review describes several
clustering algorithms (known as &quot;semi-supervised clustering&quot; methods) that can
be applied in these situations. The majority of these methods are modifications
of the popular k-means clustering method, and several of them will be described
in detail. A brief description of some other semi-supervised clustering
algorithms is also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0253</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0253</id><created>2013-06-30</created><authors><author><keyname>Dalvi</keyname><forenames>Bhavana</forenames></author><author><keyname>Cohen</keyname><forenames>William W.</forenames></author><author><keyname>Callan</keyname><forenames>Jamie</forenames></author></authors><title>Exploratory Learning</title><categories>cs.LG</categories><comments>16 pages; European Conference on Machine Learning and Principles and
  Practice of Knowledge Discovery in Databases, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multiclass semi-supervised learning (SSL), it is sometimes the case that
the number of classes present in the data is not known, and hence no labeled
examples are provided for some classes. In this paper we present variants of
well-known semi-supervised multiclass learning methods that are robust when the
data contains an unknown number of classes. In particular, we present an
&quot;exploratory&quot; extension of expectation-maximization (EM) that explores
different numbers of classes while learning. &quot;Exploratory&quot; SSL greatly improves
performance on three datasets in terms of F1 on the classes with seed examples
i.e., the classes which are expected to be in the data. Our Exploratory EM
algorithm also outperforms a SSL method based non-parametric Bayesian
clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0258</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0258</id><created>2013-06-30</created><authors><author><keyname>Wu</keyname><forenames>Xiaofu</forenames></author><author><keyname>Yang</keyname><forenames>Zhen</forenames></author></authors><title>Verification-Based Interval-Passing Algorithm for Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, accepted for publication in IEEE Signal
  Processing Letters, with an additional response to one of the reviewers in
  the current form</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a verification-based Interval-Passing (IP) algorithm for
iteratively reconstruction of nonnegative sparse signals using parity check
matrices of low-density parity check (LDPC) codes as measurement matrices. The
proposed algorithm can be considered as an improved IP algorithm by further
incorporation of the mechanism of verification algorithm. It is proved that the
proposed algorithm performs always better than either the IP algorithm or the
verification algorithm. Simulation results are also given to demonstrate the
superior performance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0261</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0261</id><created>2013-06-30</created><authors><author><keyname>Dalvi</keyname><forenames>Bhavana</forenames></author><author><keyname>Cohen</keyname><forenames>William W.</forenames></author><author><keyname>Callan</keyname><forenames>Jamie</forenames></author></authors><title>WebSets: Extracting Sets of Entities from the Web Using Unsupervised
  Information Extraction</title><categories>cs.LG cs.CL cs.IR</categories><comments>10 pages; International Conference on Web Search and Data Mining 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a open-domain information extraction method for extracting
concept-instance pairs from an HTML corpus. Most earlier approaches to this
problem rely on combining clusters of distributionally similar terms and
concept-instance pairs obtained with Hearst patterns. In contrast, our method
relies on a novel approach for clustering terms found in HTML tables, and then
assigning concept names to these clusters using Hearst patterns. The method can
be efficiently applied to a large corpus, and experimental results on several
datasets show that our method can accurately extract large numbers of
concept-instance pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0264</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0264</id><created>2013-06-30</created><authors><author><keyname>Zhang</keyname><forenames>Jialiang</forenames></author><author><keyname>Wu</keyname><forenames>Gang</forenames></author><author><keyname>Xiong</keyname><forenames>Wenhui</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>Utility-maximization Resource Allocation for Device-to-Device
  Communication Underlaying Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device(D2D) underlaying communication brings great benefits to the
cellular networks from the improvement of coverage and spectral efficiency at
the expense of complicated transceiver design. With frequency spectrum sharing
mode, the D2D user generates interference to the existing cellular networks
either in downlink or uplink. Thus the resource allocation for D2D pairs should
be designed properly in order to reduce possible interference, in particular
for uplink. In this paper, we introduce a novel bandwidth allocation scheme to
maximize the utilities of both D2D users and cellular users. Since the
allocation problem is strongly NP-hard, we apply a relaxation to the
association indicators. We propose a low-complexity distributed algorithm and
prove the convergence in a static environment. The numerical result shows that
the proposed scheme can significant improve the performance in terms of
utilities.The performance of D2D communications depends on D2D user locations,
the number of D2D users and QoS(Quality of Service) parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0276</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0276</id><created>2013-07-01</created><updated>2014-03-02</updated><authors><author><keyname>Du</keyname><forenames>Guang-Xun</forenames></author><author><keyname>Quan</keyname><forenames>Quan</forenames></author><author><keyname>Cai</keyname><forenames>Kai-Yuan</forenames></author></authors><title>Controllability Analysis and Degraded Control for a Class of Hexacopters
  Subject to Rotor Failures</title><categories>cs.SY cs.RO</categories><comments>21 pages, 7 figures, submitted to Journal of Intelligent &amp; Robotic
  Systems</comments><doi>10.1007/s10846-014-0103-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the controllability analysis and fault tolerant control
problem for a class of hexacopters. It is shown that the considered hexacopter
is uncontrollable when one rotor fails, even though the hexacopter is
over-actuated and its controllability matrix is row full rank. According to
this, a fault tolerant control strategy is proposed to control a degraded
system, where the yaw states of the considered hexacopter are ignored.
Theoretical analysis indicates that the degraded system is controllable if and
only if the maximum lift of each rotor is greater than a certain value. The
simulation and experiment results on a prototype hexacopter show the
feasibility of our controllability analysis and degraded control strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0277</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0277</id><created>2013-07-01</created><authors><author><keyname>Samantaa</keyname><forenames>Sourav</forenames></author><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author><author><keyname>Das</keyname><forenames>Poulami</forenames></author><author><keyname>Acharjee</keyname><forenames>Suvojit</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Sheli Sinha</forenames></author></authors><title>Multilevel Threshold Based Gray Scale Image Segmentation using Cuckoo
  Search</title><categories>cs.CV</categories><comments>8 Pages,7 figures,ICECIT2012,Anatapur,India. arXiv admin note: text
  overlap with arXiv:1003.1594, arXiv:1005.2908 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image Segmentation is a technique of partitioning the original image into
some distinct classes. Many possible solutions may be available for segmenting
an image into a certain number of classes, each one having different quality of
segmentation. In our proposed method, multilevel thresholding technique has
been used for image segmentation. A new approach of Cuckoo Search (CS) is used
for selection of optimal threshold value. In other words, the algorithm is used
to achieve the best solution from the initial random threshold values or
solutions and to evaluate the quality of a solution correlation function is
used. Finally, MSE and PSNR are measured to understand the segmentation
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0278</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0278</id><created>2013-07-01</created><authors><author><keyname>Malyshev</keyname><forenames>Dmitriy</forenames></author></authors><title>The coloring problem for classes with two small obstructions</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coloring problem is studied in the paper for graph classes defined by two
small forbidden induced subgraphs. We prove some sufficient conditions for
effective solvability of the problem in such classes. As their corollary we
determine the computational complexity for all sets of two connected forbidden
induced subgraphs with at most five vertices except 13 explicitly enumerated
cases
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0284</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0284</id><created>2013-07-01</created><updated>2013-07-03</updated><authors><author><keyname>Chebotarev</keyname><forenames>Pavel</forenames></author><author><keyname>Lezina</keyname><forenames>Zoya</forenames></author><author><keyname>Loginov</keyname><forenames>Anton</forenames></author><author><keyname>Tsodikova</keyname><forenames>Yana</forenames></author></authors><title>The effectiveness of altruistic lobbying: A model study</title><categories>cs.MA cs.SI cs.SY math.OC</categories><comments>2 pages, 1 figure. In: Proceedings of the The Seventh International
  Conference &quot;Game Theory and Management&quot; (GTM2013), ed. by L.A. Petrosyan and
  N.A. Zenkevich, St. Petersburg University, St. Petersburg, 2013. P.50-53</comments><msc-class>91B12 91B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Altruistic lobbying is lobbying in the public interest or in the interest of
the least protected part of the society. In fact, an altruist has a wide range
of strategies, from behaving in the interest of the society as a whole to the
support of the most disadvantaged ones. How can we compare the effectiveness of
such strategies? Another question is: &quot;Given a strategy, is it possible to
estimate the optimal number of participants choosing it?&quot; Finally, do the
answers to these questions depend on the level of well-being in the society?
Can we say that the poorer the society, the more important is to focus on the
support of the poorest? We answer these questions within the framework of the
model of social dynamics determined by voting in a stochastic environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0289</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0289</id><created>2013-07-01</created><updated>2013-07-18</updated><authors><author><keyname>Clouston</keyname><forenames>Ranald</forenames></author><author><keyname>Dawson</keyname><forenames>Jeremy</forenames></author><author><keyname>Gore</keyname><forenames>Rajeev</forenames></author><author><keyname>Tiu</keyname><forenames>Alwen</forenames></author></authors><title>Annotation-Free Sequent Calculi for Full Intuitionistic Linear Logic --
  Extended Version</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full Intuitionistic Linear Logic (FILL) is multiplicative intuitionistic
linear logic extended with par. Its proof theory has been notoriously difficult
to get right, and existing sequent calculi all involve inference rules with
complex annotations to guarantee soundness and cut-elimination. We give a
simple and annotation-free display calculus for FILL which satisfies Belnap's
generic cut-elimination theorem. To do so, our display calculus actually
handles an extension of FILL, called Bi-Intuitionistic Linear Logic (BiILL),
with an `exclusion' connective defined via an adjunction with par. We refine
our display calculus for BiILL into a cut-free nested sequent calculus with
deep inference in which the explicit structural rules of the display calculus
become admissible. A separation property guarantees that proofs of FILL
formulae in the deep inference calculus contain no trace of exclusion. Each
such rule is sound for the semantics of FILL, thus our deep inference calculus
and display calculus are conservative over FILL. The deep inference calculus
also enjoys the subformula property and terminating backward proof search,
which gives the NP-completeness of BiILL and FILL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0309</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0309</id><created>2013-07-01</created><authors><author><keyname>Bogdanov</keyname><forenames>Petko</forenames></author><author><keyname>Busch</keyname><forenames>Michael</forenames></author><author><keyname>Moehli</keyname><forenames>Jeff</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj K.</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>The Social Media Genome: Modeling Individual Topic-Specific Behavior in
  Social Media</title><categories>cs.SI physics.soc-ph</categories><comments>ASONAM 2013, 7 pages</comments><journal-ref>Proc. 2013 IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining ASONAM, Niagara Falls, Canada, August 25-28,
  2013, pp. 236-242</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information propagation in social media depends not only on the static
follower structure but also on the topic-specific user behavior. Hence novel
models incorporating dynamic user behavior are needed. To this end, we propose
a model for individual social media users, termed a genotype. The genotype is a
per-topic summary of a user's interest, activity and susceptibility to adopt
new information. We demonstrate that user genotypes remain invariant within a
topic by adopting them for classification of new information spread in
large-scale real networks. Furthermore, we extract topic-specific influence
backbone structures based on information adoption and show that they differ
significantly from the static follower network. When employed for influence
prediction of new content spread, our genotype model and influence backbones
enable more than $20% improvement, compared to purely structural features. We
also demonstrate that knowledge of user genotypes and influence backbones allow
for the design of effective strategies for latency minimization of
topic-specific information spread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0317</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0317</id><created>2013-07-01</created><authors><author><keyname>&#x160;peh</keyname><forenames>Jaka</forenames></author><author><keyname>Muhi&#x10d;</keyname><forenames>Andrej</forenames></author><author><keyname>Rupnik</keyname><forenames>Jan</forenames></author></authors><title>Algorithms of the LDA model [REPORT]</title><categories>cs.LG cs.IR stat.ML</categories><comments>5 pages, 4 figures, report</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We review three algorithms for Latent Dirichlet Allocation (LDA). Two of them
are variational inference algorithms: Variational Bayesian inference and Online
Variational Bayesian inference and one is Markov Chain Monte Carlo (MCMC)
algorithm -- Collapsed Gibbs sampling. We compare their time complexity and
performance. We find that online variational Bayesian inference is the fastest
algorithm and still returns reasonably good results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0320</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0320</id><created>2013-07-01</created><authors><author><keyname>Gao</keyname><forenames>Wanling</forenames></author><author><keyname>Zhu</keyname><forenames>Yuqing</forenames></author><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Luo</keyname><forenames>Chunjie</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Zhiguo</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Qi</keyname><forenames>Yong</forenames></author><author><keyname>He</keyname><forenames>Yongqiang</forenames></author><author><keyname>Gong</keyname><forenames>Shiming</forenames></author><author><keyname>Li</keyname><forenames>Xiaona</forenames></author><author><keyname>Zhang</keyname><forenames>Shujie</forenames></author><author><keyname>Qiu</keyname><forenames>Bizhu</forenames></author></authors><title>BigDataBench: a Big Data Benchmark Suite from Web Search Engines</title><categories>cs.IR cs.DB</categories><comments>7 pages, 5 figures, The Third Workshop on Architectures and Systems
  for Big Data(ASBD 2013) in conjunction with The 40th International Symposium
  on Computer Architecture, May 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our joint research efforts on big data benchmarking with
several industrial partners. Considering the complexity, diversity, workload
churns, and rapid evolution of big data systems, we take an incremental
approach in big data benchmarking. For the first step, we pay attention to
search engines, which are the most important domain in Internet services in
terms of the number of page views and daily visitors. However, search engine
service providers treat data, applications, and web access logs as business
confidentiality, which prevents us from building benchmarks. To overcome those
difficulties, with several industry partners, we widely investigated the open
source solutions in search engines, and obtained the permission of using
anonymous Web access logs. Moreover, with two years' great efforts, we created
a sematic search engine named ProfSearch (available from
http://prof.ict.ac.cn). These efforts pave the path for our big data benchmark
suite from search engines---BigDataBench, which is released on the web page
(http://prof.ict.ac.cn/BigDataBench). We report our detailed analysis of search
engine workloads, and present our benchmarking methodology. An innovative data
generation methodology and tool are proposed to generate scalable volumes of
big data from a small seed of real data, preserving semantics and locality of
data. Also, we preliminarily report two case studies using BigDataBench for
both system and architecture researches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0332</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0332</id><created>2013-07-01</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>de Keijzer</keyname><forenames>Bart</forenames></author></authors><title>Shapley Meets Shapley</title><categories>cs.GT</categories><comments>17 pages</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns the analysis of the Shapley value in matching games.
Matching games constitute a fundamental class of cooperative games which help
understand and model auctions and assignments. In a matching game, the value of
a coalition of vertices is the weight of the maximum size matching in the
subgraph induced by the coalition. The Shapley value is one of the most
important solution concepts in cooperative game theory.
  After establishing some general insights, we show that the Shapley value of
matching games can be computed in polynomial time for some special cases:
graphs with maximum degree two, and graphs that have a small modular
decomposition into cliques or cocliques (complete k-partite graphs are a
notable special case of this). The latter result extends to various other
well-known classes of graph-based cooperative games.
  We continue by showing that computing the Shapley value of unweighted
matching games is #P-complete in general. Finally, a fully polynomial-time
randomized approximation scheme (FPRAS) is presented. This FPRAS can be
considered the best positive result conceivable, in view of the #P-completeness
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0339</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0339</id><created>2013-07-01</created><updated>2013-07-01</updated><authors><author><keyname>Liou</keyname><forenames>Cheng-Yuan</forenames></author><author><keyname>Huang</keyname><forenames>Bo-Shiang</forenames></author><author><keyname>Liou</keyname><forenames>Daw-Ran</forenames></author><author><keyname>Simak</keyname><forenames>Alex A.</forenames></author></authors><title>Syntactic sensitive complexity for symbol-free sequence</title><categories>cs.AI</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work uses the L-system to construct a tree structure for the text
sequence and derives its complexity. It serves as a measure of structural
complexity of the text. It is applied to anomaly detection in data
transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0345</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0345</id><created>2013-07-01</created><updated>2013-12-06</updated><authors><author><keyname>Esfahani</keyname><forenames>Peyman Mohajerin</forenames></author><author><keyname>Sutter</keyname><forenames>Tobias</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Performance Bounds for the Scenario Approach and an Extension to a Class
  of Non-convex Programs</title><categories>math.OC cs.SY</categories><comments>19 pages, revised version</comments><msc-class>90C34</msc-class><doi>10.1109/TAC.2014.2330702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Scenario Convex Program (SCP) for two classes of optimization
problems that are not tractable in general: Robust Convex Programs (RCPs) and
Chance-Constrained Programs (CCPs). We establish a probabilistic bridge from
the optimal value of SCP to the optimal values of RCP and CCP in which the
uncertainty takes values in a general, possibly infinite dimensional, metric
space. We then extend our results to a certain class of non-convex problems
that includes, for example, binary decision variables. In the process, we also
settle a measurability issue for a general class of scenario programs, which to
date has been addressed by an assumption. Finally, we demonstrate the
applicability of our results on a benchmark problem and a problem in fault
detection and isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0349</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0349</id><created>2013-07-01</created><authors><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Zhang</keyname><forenames>Chunhong</forenames></author><author><keyname>Qiu</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Zeng</keyname><forenames>Zhimin</forenames></author></authors><title>Replacing Network Coordinate System with Internet Delay Matrix Service
  (IDMS): A Case Study in Chinese Internet</title><categories>cs.NI</categories><comments>15 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network distance (Round Trip Time, RTT) is an important parameter for many
Internet distributed systems to optimize their performances. Network Coordinate
System (NCS) is assumed as a lightweight and scalable mechanism to predict
network distance between any two Internet hosts without explicit measurements.
Though many NCSes have been proposed in the literatures, they are not
satisfactory in terms of accuracy. In this paper, we propose to use delay
matrix to replace the NCS. This paper makes three contributions. First, we show
that not all the hosts need an independent network coordinate (NC). On the
contrary, most hosts can be represented as one or several nodes in NCS. Second,
we present an Internet Delay Matrix Service (IDMS) for representing network
distances without explicit measurements in Internet. Third, we describe two
delay matrices up to date delay matrix (UDM) and previous delay matrices (PDM)
for representing the network distances. Extensive simulations on our collected
Chinese Internet data sets show that IDMS is an accurate, efficient, scalable
and practical method for Chinese Internet. The performance of IDMS is better
than other existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0366</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0366</id><created>2013-07-01</created><updated>2014-02-24</updated><authors><author><keyname>Raskutti</keyname><forenames>Garvesh</forenames></author><author><keyname>Uhler</keyname><forenames>Caroline</forenames></author></authors><title>Learning directed acyclic graphs based on sparsest permutations</title><categories>math.ST cs.LG stat.TH</categories><comments>22 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a Bayesian network or directed acyclic
graph (DAG) model from observational data. A number of constraint-based,
score-based and hybrid algorithms have been developed for this purpose. For
constraint-based methods, statistical consistency guarantees typically rely on
the faithfulness assumption, which has been show to be restrictive especially
for graphs with cycles in the skeleton. However, there is only limited work on
consistency guarantees for score-based and hybrid algorithms and it has been
unclear whether consistency guarantees can be proven under weaker conditions
than the faithfulness assumption.
  In this paper, we propose the sparsest permutation (SP) algorithm. This
algorithm is based on finding the causal ordering of the variables that yields
the sparsest DAG. We prove that this new score-based method is consistent under
strictly weaker conditions than the faithfulness assumption. We also
demonstrate through simulations on small DAGs that the SP algorithm compares
favorably to the constraint-based PC and SGS algorithms as well as the
score-based Greedy Equivalence Search and hybrid Max-Min Hill-Climbing method.
In the Gaussian setting, we prove that our algorithm boils down to finding the
permutation of the variables with sparsest Cholesky decomposition for the
inverse covariance matrix. Using this connection, we show that in the oracle
setting, where the true covariance matrix is known, the SP algorithm is in fact
equivalent to $\ell_0$-penalized maximum likelihood estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0396</identifier>
 <datestamp>2014-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0396</id><created>2013-07-01</created><updated>2014-08-08</updated><authors><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author></authors><title>On Optimal Zero-Delay Coding of Vector Markov Sources</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>IEEE Transactions on Information Theory, accepted for publication</comments><msc-class>93E20, 94A29, 60J05</msc-class><doi>10.1109/TIT.2014.2346780</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal zero-delay coding (quantization) of a vector-valued Markov source
driven by a noise process is considered. Using a stochastic control problem
formulation, the existence and structure of optimal quantization policies are
studied. For a finite-horizon problem with bounded per-stage distortion
measure, the existence of an optimal zero-delay quantization policy is shown
provided that the quantizers allowed are ones with convex codecells. The
bounded distortion assumption is relaxed to cover cases that include the linear
quadratic Gaussian problem. For the infinite horizon problem and a stationary
Markov source the optimality of deterministic Markov coding policies is shown.
The existence of optimal stationary Markov quantization policies is also shown
provided randomization that is shared by the encoder and the decoder is
allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0412</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0412</id><created>2013-07-01</created><authors><author><keyname>LaRocca</keyname><forenames>Sarah</forenames></author><author><keyname>Guikema</keyname><forenames>Seth</forenames></author></authors><title>Characterizing and Predicting the Robustness of Power-law Networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1016/j.ress.2014.07.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power-law networks such as the Internet, terrorist cells, species
relationships, and cellular metabolic interactions are susceptible to node
failures, yet maintaining network connectivity is essential for network
functionality. Disconnection of the network leads to fragmentation and, in some
cases, collapse of the underlying system. However, the influences of the
topology of networks on their ability to withstand node failures are poorly
understood. Based on a study of the response of 2,000 power-law networks to
node failures, we find that networks with higher nodal degree and clustering
coefficient, lower betweenness centrality, and lower variability in path length
and clustering coefficient maintain their cohesion better during such events.
We also find that network robustness, i.e., the ability to withstand node
failures, can be accurately predicted a priori for power-law networks across
many fields. These results provide a basis for designing new, more robust
networks, improving the robustness of existing networks such as the Internet
and cellular metabolic pathways, and efficiently degrading networks such as
terrorist cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0414</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0414</id><created>2013-07-01</created><authors><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Carrier</keyname><forenames>Pierre Luc</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Mirza</keyname><forenames>Mehdi</forenames></author><author><keyname>Hamner</keyname><forenames>Ben</forenames></author><author><keyname>Cukierski</keyname><forenames>Will</forenames></author><author><keyname>Tang</keyname><forenames>Yichuan</forenames></author><author><keyname>Thaler</keyname><forenames>David</forenames></author><author><keyname>Lee</keyname><forenames>Dong-Hyun</forenames></author><author><keyname>Zhou</keyname><forenames>Yingbo</forenames></author><author><keyname>Ramaiah</keyname><forenames>Chetan</forenames></author><author><keyname>Feng</keyname><forenames>Fangxiang</forenames></author><author><keyname>Li</keyname><forenames>Ruifan</forenames></author><author><keyname>Wang</keyname><forenames>Xiaojie</forenames></author><author><keyname>Athanasakis</keyname><forenames>Dimitris</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author><author><keyname>Milakov</keyname><forenames>Maxim</forenames></author><author><keyname>Park</keyname><forenames>John</forenames></author><author><keyname>Ionescu</keyname><forenames>Radu</forenames></author><author><keyname>Popescu</keyname><forenames>Marius</forenames></author><author><keyname>Grozea</keyname><forenames>Cristian</forenames></author><author><keyname>Bergstra</keyname><forenames>James</forenames></author><author><keyname>Xie</keyname><forenames>Jingjing</forenames></author><author><keyname>Romaszko</keyname><forenames>Lukasz</forenames></author><author><keyname>Xu</keyname><forenames>Bing</forenames></author><author><keyname>Chuang</keyname><forenames>Zhang</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Challenges in Representation Learning: A report on three machine
  learning contests</title><categories>stat.ML cs.LG</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ICML 2013 Workshop on Challenges in Representation Learning focused on
three challenges: the black box learning challenge, the facial expression
recognition challenge, and the multimodal learning challenge. We describe the
datasets created for these challenges and summarize the results of the
competitions. We provide suggestions for organizers of future challenges and
some comments on what kind of knowledge can be gained from machine learning
competitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0417</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0417</id><created>2013-07-01</created><updated>2013-12-04</updated><authors><author><keyname>Kurz</keyname><forenames>Alexander A</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>Palmigiano</keyname><forenames>Alessandra A</forenames><affiliation>University of Amsterdam</affiliation></author></authors><title>Epistemic Updates on Algebras</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  5, 2013) lmcs:897</journal-ref><doi>10.2168/LMCS-9(4:17)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the mathematical theory of epistemic updates with the tools of
duality theory. We focus on the Logic of Epistemic Actions and Knowledge (EAK),
introduced by Baltag-Moss- Solecki, without the common knowledge operator. We
dually characterize the product update construction of EAK as a certain
construction transforming the complex algebras associated with the given model
into the complex algebra associated with the updated model. This dual
characterization naturally generalizes to much wider classes of algebras, which
include, but are not limited to, arbitrary BAOs and arbitrary modal expansions
of Heyting algebras (HAOs). As an application of this dual characterization, we
axiomatize the intuitionistic analogue of the logic of epistemic knowledge and
actions, which we refer to as IEAK, prove soundness and completeness of IEAK
w.r.t. both algebraic and relational models, and illustrate how IEAK encodes
the reasoning of agents in a concrete epistemic scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0426</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0426</id><created>2013-07-01</created><updated>2013-10-26</updated><authors><author><keyname>Lampert</keyname><forenames>Thomas A.</forenames></author><author><keyname>Stumpf</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Gan&#xe7;arski</keyname><forenames>Pierre</forenames></author></authors><title>An Empirical Study into Annotator Agreement, Ground Truth Estimation,
  and Algorithm Evaluation</title><categories>cs.CV cs.AI</categories><comments>23 pages</comments><acm-class>I.4.6; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although agreement between annotators has been studied in the past from a
statistical viewpoint, little work has attempted to quantify the extent to
which this phenomenon affects the evaluation of computer vision (CV) object
detection algorithms. Many researchers utilise ground truth (GT) in experiments
and more often than not this GT is derived from one annotator's opinion. How
does the difference in opinion affect an algorithm's evaluation? Four examples
of typical CV problems are chosen, and a methodology is applied to each to
quantify the inter-annotator variance and to offer insight into the mechanisms
behind agreement and the use of GT. It is found that when detecting linear
objects annotator agreement is very low. The agreement in object position,
linear or otherwise, can be partially explained through basic image properties.
Automatic object detectors are compared to annotator agreement and it is found
that a clear relationship exists. Several methods for calculating GTs from a
number of annotations are applied and the resulting differences in the
performance of the object detectors are quantified. It is found that the rank
of a detector is highly dependent upon the method used to form the GT. It is
also found that although the STAPLE and LSML GT estimation methods appear to
represent the mean of the performance measured using the individual
annotations, when there are few annotations, or there is a large variance in
them, these estimates tend to degrade. Furthermore, one of the most commonly
adopted annotation combination methods--consensus voting--accentuates more
obvious features, which results in an overestimation of the algorithm's
performance. Finally, it is concluded that in some datasets it may not be
possible to state with any confidence that one algorithm outperforms another
when evaluating upon one GT and a method for calculating confidence bounds is
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0433</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0433</id><created>2013-07-01</created><updated>2013-07-02</updated><authors><author><keyname>Ammendola</keyname><forenames>Roberto</forenames></author><author><keyname>Biagioni</keyname><forenames>Andrea</forenames></author><author><keyname>Frezza</keyname><forenames>Ottorino</forenames></author><author><keyname>Cicero</keyname><forenames>Francesca Lo</forenames></author><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Paolucci</keyname><forenames>Pier Stanislao</forenames></author><author><keyname>Rossetti</keyname><forenames>Davide</forenames></author><author><keyname>Simula</keyname><forenames>Francesco</forenames></author><author><keyname>Tosoratto</keyname><forenames>Laura</forenames></author><author><keyname>Vicini</keyname><forenames>Piero</forenames></author></authors><title>'Mutual Watch-dog Networking': Distributed Awareness of Faults and
  Critical Events in Petascale/Exascale systems</title><categories>cs.DC cs.NI</categories><comments>Technical Report, Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many tile systems require techniques to be applied to increase components
resilience and control the FIT (Failures In Time) rate. When scaling to peta-
exa-scale systems the FIT rate may become unacceptable due to component
numerosity, requiring more systemic countermeasures. Thus, the ability to be
fault aware, i.e. to detect and collect information about fault and critical
events, is a necessary feature that large scale distributed architectures must
provide in order to apply systemic fault tolerance techniques. In this context,
the LO|FA|MO approach is a way to obtain systemic fault awareness, by
implementing a mutual watchdog mechanism and guaranteeing fault detection in a
no-single-point-of-failure fashion. This document contains specification and
implementation details about this approach, in the shape of a technical report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0441</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0441</id><created>2013-07-01</created><authors><author><keyname>Bakibayev</keyname><forenames>Nurzhan</forenames></author><author><keyname>Ko&#x10d;isk&#xfd;</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Olteanu</keyname><forenames>Dan</forenames></author><author><keyname>Z&#xe1;vodn&#xfd;</keyname><forenames>Jakub</forenames></author></authors><title>Aggregation and Ordering in Factorised Databases</title><categories>cs.DB cs.DS</categories><comments>12 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common approach to data analysis involves understanding and manipulating
succinct representations of data. In earlier work, we put forward a succinct
representation system for relational data called factorised databases and
reported on the main-memory query engine FDB for select-project-join queries on
such databases.
  In this paper, we extend FDB to support a larger class of practical queries
with aggregates and ordering. This requires novel optimisation and evaluation
techniques. We show how factorisation coupled with partial aggregation can
effectively reduce the number of operations needed for query evaluation. We
also show how factorisations of query results can support enumeration of tuples
in desired orders as efficiently as listing them from the unfactorised, sorted
results.
  We experimentally observe that FDB can outperform off-the-shelf relational
engines by orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0445</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0445</id><created>2013-07-01</created><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Shirazinia</keyname><forenames>Amirpasha</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Networked Estimation using Sparsifying Basis Prediction</title><categories>cs.SY math.OC</categories><comments>Proceedings of the 4th IFAC Workshop on Distributed Estimation and
  Control in Networked Systems (NecSys), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for networked state estimation, where systems encode
their (possibly high dimensional) state vectors using a mutually agreed basis
between the system and the estimator (in a remote monitoring unit). The basis
sparsifies the state vectors, i.e., it represents them using vectors with few
non-zero components, and as a result, the systems might need to transmit only a
fraction of the original information to be able to recover the non-zero
components of the transformed state vector. Hence, the estimator can recover
the state vector of the system from an under-determined linear set of
equations. We use a greedy search algorithm to calculate the sparsifying basis.
Then, we present an upper bound for the estimation error. Finally, we
demonstrate the results on a numerical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0449</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0449</id><created>2013-07-01</created><authors><author><keyname>Lerner</keyname><forenames>Vladimir S.</forenames></author></authors><title>Arising information regularities in an observer</title><categories>nlin.AO cs.IT math.IT</categories><comments>29 pages,3 figures. arXiv admin note: text overlap with
  arXiv:1212.1710</comments><msc-class>58J65, 60J65, 93B52, 93E02, 93E15, 93E30</msc-class><acm-class>H.1.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Considering conversion of observed uncertainty to the observer certainty, the
paper verifies the minimax principle of both the optimal extracting and
spending of information, which generally refers to getting maximum of
information from each of its observed minimum and minimize the maximum while
consuming it. This dual complimentary principle functionally unifies observer
regularities: integral measuring each observing process under multiple trial
actions; converting the observed uncertainty to information-certainty by
generation of internal information micro and macrodynamics and verification of
trial information; enclosing the internal dynamics in information network (IN),
whose logic integrates the observer requested information in the IN code;
building concurrently the IN temporary hierarchy, whose high level enfolds
information logic that requests new information for the running observer IN,
extending the logic code; self-forming the observer inner dynamical and
geometrical structures with a limited boundary, shaped by the IN information
geometry during the time-space cooperative processes. These regularities
establish united information mechanism, whose integral logic self-operates this
mechanism, transforming observed uncertainty to physical reality-matter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0468</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0468</id><created>2013-07-01</created><updated>2013-11-18</updated><authors><author><keyname>Sandryhaila</keyname><forenames>Aliaksei</forenames></author><author><keyname>Moura</keyname><forenames>Jose M. F.</forenames></author></authors><title>Discrete Signal Processing on Graphs: Frequency Analysis</title><categories>cs.SI math.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signals and datasets that arise in physical and engineering applications, as
well as social, genetics, biomolecular, and many other domains, are becoming
increasingly larger and more complex. In contrast to traditional time and image
signals, data in these domains are supported by arbitrary graphs. Signal
processing on graphs extends concepts and techniques from traditional signal
processing to data indexed by generic graphs. This paper studies the concepts
of low and high frequencies on graphs, and low-, high-, and band-pass graph
filters. In traditional signal processing, there concepts are easily defined
because of a natural frequency ordering that has a physical interpretation. For
signals residing on graphs, in general, there is no obvious frequency ordering.
We propose a definition of total variation for graph signals that naturally
leads to a frequency ordering on graphs and defines low-, high-, and band-pass
graph signals and filters. We study the design of graph filters with specified
frequency response, and illustrate our approach with applications to sensor
malfunction detection and data classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0471</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0471</id><created>2013-07-01</created><updated>2014-07-10</updated><authors><author><keyname>Rebentrost</keyname><forenames>Patrick</forenames></author><author><keyname>Mohseni</keyname><forenames>Masoud</forenames></author><author><keyname>Lloyd</keyname><forenames>Seth</forenames></author></authors><title>Quantum support vector machine for big data classification</title><categories>quant-ph cs.LG</categories><comments>5 pages</comments><journal-ref>Phys. Rev. Lett. 113, 130503 (2014)</journal-ref><doi>10.1103/PhysRevLett.113.130503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised machine learning is the classification of new data based on
already classified training examples. In this work, we show that the support
vector machine, an optimized binary classifier, can be implemented on a quantum
computer, with complexity logarithmic in the size of the vectors and the number
of training examples. In cases when classical sampling algorithms require
polynomial time, an exponential speed-up is obtained. At the core of this
quantum big data algorithm is a non-sparse matrix exponentiation technique for
efficiently performing a matrix inversion of the training data inner-product
(kernel) matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0473</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0473</id><created>2013-07-01</created><updated>2015-01-28</updated><authors><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author><author><keyname>Nedi&#x107;</keyname><forenames>Angelia</forenames></author></authors><title>Online discrete optimization in social networks in the presence of
  Knightian uncertainty</title><categories>math.OC cs.DC cs.LG</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a model of collective real-time decision-making (or learning) in a
social network operating in an uncertain environment, for which no a priori
probabilistic model is available. Instead, the environment's impact on the
agents in the network is seen through a sequence of cost functions, revealed to
the agents in a causal manner only after all the relevant actions are taken.
There are two kinds of costs: individual costs incurred by each agent and
local-interaction costs incurred by each agent and its neighbors in the social
network. Moreover, agents have inertia: each agent has a default mixed strategy
that stays fixed regardless of the state of the environment, and must expend
effort to deviate from this strategy in order to respond to cost signals coming
from the environment. We construct a decentralized strategy, wherein each agent
selects its action based only on the costs directly affecting it and on the
decisions made by its neighbors in the network. In this setting, we quantify
social learning in terms of regret, which is given by the difference between
the realized network performance over a given time horizon and the best
performance that could have been achieved in hindsight by a fictitious
centralized entity with full knowledge of the environment's evolution. We show
that our strategy achieves the regret that scales polylogarithmically with the
time horizon and polynomially with the number of agents and the maximum number
of neighbors of any agent in the social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0475</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0475</id><created>2013-07-01</created><authors><author><keyname>Ahmed</keyname><forenames>Faraz</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Liu</keyname><forenames>Alex X.</forenames></author></authors><title>A Random Matrix Approach to Differential Privacy and Structure Preserved
  Social Network Graph Publishing</title><categories>cs.CR cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks are being increasingly used for analyzing various
societal phenomena such as epidemiology, information dissemination, marketing
and sentiment flow. Popular analysis techniques such as clustering and
influential node analysis, require the computation of eigenvectors of the real
graph's adjacency matrix. Recent de-anonymization attacks on Netflix and AOL
datasets show that an open access to such graphs pose privacy threats. Among
the various privacy preserving models, Differential privacy provides the
strongest privacy guarantees.
  In this paper we propose a privacy preserving mechanism for publishing social
network graph data, which satisfies differential privacy guarantees by
utilizing a combination of theory of random matrix and that of differential
privacy. The key idea is to project each row of an adjacency matrix to a low
dimensional space using the random projection approach and then perturb the
projected matrix with random noise. We show that as compared to existing
approaches for differential private approximation of eigenvectors, our approach
is computationally efficient, preserves the utility and satisfies differential
privacy. We evaluate our approach on social network graphs of Facebook, Live
Journal and Pokec. The results show that even for high values of noise variance
sigma=1 the clustering quality given by normalized mutual information gain is
as low as 0.74. For influential node discovery, the propose approach is able to
correctly recover 80 of the most influential nodes. We also compare our results
with an approach presented in [43], which directly perturbs the eigenvector of
the original data by a Laplacian noise. The results show that this approach
requires a large random perturbation in order to preserve the differential
privacy, which leads to a poor estimation of eigenvectors for large social
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0489</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0489</id><created>2013-07-01</created><authors><author><keyname>Duval</keyname><forenames>Dominique</forenames><affiliation>LJK</affiliation></author></authors><title>Scalability using effects</title><categories>cs.LO math.CT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note is about using computational effects for scalability. With this
method, the specification gets more and more complex while its semantics gets
more and more correct. We show, from two fundamental examples, that it is
possible to design a deduction system for a specification involving an effect
without expliciting this effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0516</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0516</id><created>2013-07-01</created><updated>2013-11-17</updated><authors><author><keyname>Hooper</keyname><forenames>Paul L.</forenames></author><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author><author><keyname>Hooper</keyname><forenames>Ann E. Caldwell</forenames></author><author><keyname>Gurven</keyname><forenames>Michael</forenames></author><author><keyname>Kaplan</keyname><forenames>Hillard S.</forenames></author></authors><title>Dynamical Structure of a Traditional Amazonian Social Network</title><categories>cs.SI nlin.AO physics.soc-ph q-bio.PE</categories><comments>24 pages, 6 figures, 1 table; expanded results and discussion
  sections; matches published version</comments><journal-ref>Entropy 2013, 15, 4932-4955</journal-ref><doi>10.3390/e15114932</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reciprocity is a vital feature of social networks, but relatively little is
known about its temporal structure or the mechanisms underlying its persistence
in real world behavior. In pursuit of these two questions, we study the
stationary and dynamical signals of reciprocity in a network of manioc beer
(Spanish: chicha; Tsimane': shocdye') drinking events in a Tsimane' village in
lowland Bolivia. At the stationary level, our analysis reveals that social
exchange within the community is heterogeneously patterned according to kinship
and spatial proximity. A positive relationship between the frequencies at which
two families host each other, controlling for kinship and proximity, provides
evidence for stationary reciprocity. Our analysis of the dynamical structure of
this network presents a novel method for the study of conditional, or
non-stationary, reciprocity effects. We find evidence that short-timescale
reciprocity (within three days) is present among non- and distant-kin pairs;
conversely, we find that levels of cooperation among close kin can be accounted
for on the stationary hypothesis alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0531</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0531</id><created>2013-07-01</created><authors><author><keyname>Abousamra</keyname><forenames>Ahmed</forenames></author><author><keyname>Bunde</keyname><forenames>David P.</forenames></author><author><keyname>Pruhs</keyname><forenames>Kirk</forenames></author></authors><title>An Experimental Comparison of Speed Scaling Algorithms with Deadline
  Feasibility Constraints</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the first, and most well studied, speed scaling problem in the
algorithmic literature: where the scheduling quality of service measure is a
deadline feasibility constraint, and where the power objective is to minimize
the total energy used. Four online algorithms for this problem have been
proposed in the algorithmic literature. Based on the best upper bound that can
be proved on the competitive ratio, the ranking of the online algorithms from
best to worst is: $\qOA$, $\OA$, $\AVR$, $\BKP$. As a test case on the
effectiveness of competitive analysis to predict the best online algorithm, we
report on an experimental &quot;horse race&quot; between these algorithms using instances
based on web server traces. Our main conclusion is that the ranking of our
algorithms based on their performance in our experiments is identical to the
order predicted by competitive analysis. This ranking holds over a large range
of possible power functions, and even if the power objective is temperature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0539</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0539</id><created>2013-07-01</created><updated>2015-08-14</updated><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author><author><keyname>Baras</keyname><forenames>John S.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>The Evolution of Beliefs over Signed Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the evolution of opinions (or beliefs) over a social network modeled
as a signed graph. The sign attached to an edge in this graph characterizes
whether the corresponding individuals or end nodes are friends (positive links)
or enemies (negative links). Pairs of nodes are randomly selected to interact
over time, and when two nodes interact, each of them updates its opinion based
on the opinion of the other node and the sign of the corresponding link. This
model generalizes DeGroot model to account for negative links: when two enemies
interact, their opinions go in opposite directions. We provide conditions for
convergence and divergence in expectation, in mean-square, and in almost sure
sense, and exhibit phase transition phenomena for these notions of convergence
depending on the parameters of the opinion update model and on the structure of
the underlying graph. We establish a {\it no-survivor} theorem, stating that
the difference in opinions of any two nodes diverges whenever opinions in the
network diverge as a whole. We also prove a {\it live-or-die} lemma, indicating
that almost surely, the opinions either converge to an agreement or diverge.
Finally, we extend our analysis to cases where opinions have hard lower and
upper limits. In these cases, we study when and how opinions may become
asymptotically clustered to the belief boundaries, and highlight the crucial
influence of (strong or weak) structural balance of the underlying network on
this clustering phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0550</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0550</id><created>2013-07-01</created><updated>2013-11-13</updated><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Zorzi</keyname><forenames>Margherita</forenames></author></authors><title>Wave-Style Token Machines and Quantum Lambda Calculi (Long Version)</title><categories>cs.LO</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle-style token machines are a way to interpret proofs and programs,
when the latter are defined according to the principles of linear logic. In
this paper, we show that token machines also make sense when the programs at
hand are those of a simple linear quantum $\lambda$-calculus. This, however,
requires generalizing the concept of a token machine to one in which more than
one particle can possibly travel around the term at the same time. This is
intimately related to entanglement and allows to give a simple operational
semantics to the calculus coherently with the principles of quantum
computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0555</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0555</id><created>2013-07-01</created><authors><author><keyname>Bokharaie</keyname><forenames>Vahid</forenames></author><author><keyname>Parsaee</keyname><forenames>Gholamreza</forenames></author></authors><title>An Application of Joint Spectral Radius in Power Control Problem for
  Wireless Communications</title><categories>math.DS cs.IT math.IT</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource management, including power control, is one of the most essential
functionalities of any wireless telecommunication system. Various transmitter
power-control methods have been developed to deliver a desired quality of
service in wireless networks. We consider two of these methods: Distributed
Power Control and Distributed Balancing Algorithm schemes. We use the concept
of joint spectral radius to come up with conditions for convergence of the
transmitted power in these two schemes when the gains on all the communications
links are assumed to vary at each time-step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0556</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0556</id><created>2013-07-01</created><updated>2014-04-25</updated><authors><author><keyname>G&#xf6;bel</keyname><forenames>Andreas</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Richerby</keyname><forenames>David</forenames></author></authors><title>The Complexity of Counting Homomorphisms to Cactus Graphs Modulo 2</title><categories>cs.CC math.CO</categories><comments>minor changes</comments><doi>10.1145/2635825</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A homomorphism from a graph G to a graph H is a function from V(G) to V(H)
that preserves edges. Many combinatorial structures that arise in mathematics
and computer science can be represented naturally as graph homomorphisms and as
weighted sums of graph homomorphisms. In this paper, we study the complexity of
counting homomorphisms modulo 2. The complexity of modular counting was
introduced by Papadimitriou and Zachos and it has been pioneered by Valiant who
famously introduced a problem for which counting modulo 7 is easy but counting
modulo 2 is intractable. Modular counting provides a rich setting in which to
study the structure of homomorphism problems. In this case, the structure of
the graph H has a big influence on the complexity of the problem. Thus, our
approach is graph-theoretic. We give a complete solution for the class of
cactus graphs, which are connected graphs in which every edge belongs to at
most one cycle. Cactus graphs arise in many applications such as the modelling
of wireless sensor networks and the comparison of genomes. We show that, for
some cactus graphs H, counting homomorphisms to H modulo 2 can be done in
polynomial time. For every other fixed cactus graph H, the problem is complete
for the complexity class parity-P which is a wide complexity class to which
every problem in the polynomial hierarchy can be reduced (using randomised
reductions). Determining which H lead to tractable problems can be done in
polynomial time. Our result builds upon the work of Faben and Jerrum, who gave
a dichotomy for the case in which H is a tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0571</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0571</id><created>2013-07-01</created><authors><author><keyname>Nicolae</keyname><forenames>Marius</forenames></author><author><keyname>Rajasekaran</keyname><forenames>Sanguthevar</forenames></author></authors><title>Efficient Sequential and Parallel Algorithms for Planted Motif Search</title><categories>cs.DS cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motif searching is an important step in the detection of rare events
occurring in a set of DNA or protein sequences. One formulation of the problem
is known as (l,d)-motif search or Planted Motif Search (PMS). In PMS we are
given two integers l and d and n biological sequences. We want to find all
sequences of length l that appear in each of the input sequences with at most d
mismatches. The PMS problem is NP-complete. PMS algorithms are typically
evaluated on certain instances considered challenging. This paper presents an
exact parallel PMS algorithm called PMS8. PMS8 is the first algorithm to solve
the challenging (l,d) instances (25,10) and (26,11). PMS8 is also efficient on
instances with larger l and d such as (50,21). This paper also introduces
necessary and sufficient conditions for 3 l-mers to have a common d-neighbor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0578</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0578</id><created>2013-07-01</created><authors><author><keyname>Bargi</keyname><forenames>Ava</forenames></author><author><keyname>Da Xu</keyname><forenames>Richard Yi</forenames></author><author><keyname>Piccardi</keyname><forenames>Massimo</forenames></author></authors><title>A non-parametric conditional factor regression model for
  high-dimensional input and response</title><categories>stat.ML cs.LG</categories><comments>9 pages, 3 figures, NIPS submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a non-parametric conditional factor regression
(NCFR)model for domains with high-dimensional input and response. NCFR enhances
linear regression in two ways: a) introducing low-dimensional latent factors
leading to dimensionality reduction and b) integrating an Indian Buffet Process
as a prior for the latent factors to derive unlimited sparse dimensions.
Experimental results comparing NCRF to several alternatives give evidence to
remarkable prediction performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0585</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0585</id><created>2013-07-01</created><updated>2013-10-02</updated><authors><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Huang</keyname><forenames>Howard C.</forenames></author><author><keyname>Viswanathan</keyname><forenames>Harish</forenames></author><author><keyname>Valenzuela</keyname><forenames>Reinaldo A.</forenames></author></authors><title>Fundamentals of Throughput Maximization with Random Arrivals for M2M
  Communications</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For wireless systems in which randomly arriving devices attempt to transmit a
fixed payload to a central receiver, we develop a framework to characterize the
system throughput as a function of arrival rate and per-user data rate. The
framework considers both coordinated transmission (where devices are scheduled)
and uncoordinated transmission (where devices communicate on a random access
channel and a provision is made for retransmissions). Our main contribution is
a novel characterization of the optimal throughput for the case of
uncoordinated transmission and a strategy for achieving this throughput that
relies on overlapping transmissions and joint decoding. Simulations for a
noise-limited cellular network show that the optimal strategy provides a factor
of four improvement in throughput compared to slotted aloha. We apply our
framework to evaluate more general system-level designs that account for
overhead signaling. We demonstrate that, for small payload sizes relevant for
machine-to-machine (M2M) communications (200 bits or less), a one-stage
strategy, where identity and data are transmitted optimally over the random
access channel, can support at least twice the number of devices compared to a
conventional strategy, where identity is established over an initial
random-access stage and data transmission is scheduled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0589</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0589</id><created>2013-07-02</created><authors><author><keyname>Ness</keyname><forenames>Steven</forenames></author><author><keyname>Symonds</keyname><forenames>Helena</forenames></author><author><keyname>Spong</keyname><forenames>Paul</forenames></author><author><keyname>Tzanetakis</keyname><forenames>George</forenames></author></authors><title>The Orchive : Data mining a massive bioacoustic archive</title><categories>cs.LG cs.DB cs.SD</categories><comments>ICML 2013 Workshop on Machine Learning for Bioacoustics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Orchive is a large collection of over 20,000 hours of audio recordings
from the OrcaLab research facility located off the northern tip of Vancouver
Island. It contains recorded orca vocalizations from the 1980 to the present
time and is one of the largest resources of bioacoustic data in the world. We
have developed a web-based interface that allows researchers to listen to these
recordings, view waveform and spectral representations of the audio, label
clips with annotations, and view the results of machine learning classifiers
based on automatic audio features extraction. In this paper we describe such
classifiers that discriminate between background noise, orca calls, and the
voice notes that are present in most of the tapes. Furthermore we show
classification results for individual calls based on a previously existing orca
call catalog. We have also experimentally investigated the scalability of
classifiers over the entire Orchive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0596</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0596</id><created>2013-07-02</created><authors><author><keyname>Damani</keyname><forenames>Om P.</forenames></author></authors><title>Improving Pointwise Mutual Information (PMI) by Incorporating
  Significant Co-occurrence</title><categories>cs.CL</categories><comments>To appear in the proceedings of 17th Conference on Computational
  Natural Language Learning, CoNLL 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a new co-occurrence based word association measure by incorporating
the concept of significant cooccurrence in the popular word association measure
Pointwise Mutual Information (PMI). By extensive experiments with a large
number of publicly available datasets we show that the newly introduced measure
performs better than other co-occurrence based measures and despite being
resource-light, compares well with the best known resource-heavy distributional
similarity and knowledge based word association measures. We investigate the
source of this performance improvement and find that of the two types of
significant co-occurrence - corpus-level and document-level, the concept of
corpus level significance combined with the use of document counts in place of
word counts is responsible for all the performance gains observed. The concept
of document level significance is not helpful for PMI adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0608</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0608</id><created>2013-07-02</created><updated>2014-08-19</updated><authors><author><keyname>Han</keyname><forenames>Te Sun</forenames></author><author><keyname>Endo</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Sasaki</keyname><forenames>Masahide</forenames></author></authors><title>Reliability and Secrecy Functions of the Wiretap Channel under Cost
  Constraint</title><categories>cs.IT cs.CR math.IT</categories><comments>60 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wiretap channel has been devised and studied first by Wyner, and
subsequently extended to the case with non-degraded general wiretap channels by
Csiszar and Korner. Focusing mainly on the Poisson wiretap channel with cost
constraint, we newly introduce the notion of reliability and security functions
as a fundamental tool to analyze and/or design the performance of an efficient
wiretap channel system. Compact formulae for those functions are explicitly
given for stationary memoryless wiretap channels. It is also demonstrated that,
based on such a pair of reliability and security functions, we can control the
tradeoff between reliability and security (usually conflicting), both with
exponentially decreasing rates as block length n becomes large. Two ways to do
so are given on the basis of concatenation and rate exchange. In this
framework, the notion of the {\delta} secrecy capacity is defined and shown to
attain the strongest security standard among others. The maximized vs. averaged
security measures is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0616</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0616</id><created>2013-07-02</created><authors><author><keyname>Tewari</keyname><forenames>Jyoti</forenames></author><author><keyname>Arya</keyname><forenames>Swati</forenames></author></authors><title>Evolution of Gi Fi Technology Over Other Technologies</title><categories>cs.NI</categories><comments>4 pages, 3 Figures, 1 Table, Volume 2, Issue 3, IJCSN - International
  Journal of Computer Science and Network, June 2013</comments><report-no>IJCSN-2013-2-3-17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gi-Fi stands for Gigabit Wireless. Gi-Fi is a wireless transmission system
which is ten times faster than other technology and its chip delivers
short-range multigigabit data transfer in a local environment. Gi-Fi is a
wireless technology which promises high speed short range data transfers with
speeds of up to 5 Gbps within a range of 10 meters. The Gi-Fi operates on the
60GHz frequency band. This frequency band is currently mostly unused. It is
manufactured using (CMOS) technology. This wireless technology named as Gi-Fi.
The benefits and features of this new technology can be helpful for use in
development of the next generation of devices and places. In this paper, the
comparison is perform between Gi-Fi and some of existing technologies with very
high speed large files transfers within seconds it is expected that Gi-Fi to be
the preferred wireless technology used in home and office of future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0624</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0624</id><created>2013-07-02</created><authors><author><keyname>Chan</keyname><forenames>T-H. Hubert</forenames></author><author><keyname>Chen</keyname><forenames>Fei</forenames></author></authors><title>A Primal-Dual Continuous LP Method on the Multi-choice Multi-best
  Secretary Problem</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The J-choice K-best secretary problem, also known as the (J,K)-secretary
problem, is a generalization of the classical secretary problem. An algorithm
for the (J,K)-secretary problem is allowed to make J choices and the payoff to
be maximized is the expected number of items chosen among the K best items.
  Previous works analyzed the case when the total number n of items is finite,
and considered what happens when n grows. However, for general J and K, the
optimal solution for finite n is difficult to analyze. Instead, we prove a
formal connection between the finite model and the infinite model, where there
are countably infinite number of items, each attached with a random arrival
time drawn independently and uniformly from [0,1].
  We use primal-dual continuous linear programming techniques to analyze a
class of infinite algorithms, which are general enough to capture the
asymptotic behavior of the finite model with large number of items. Our
techniques allow us to prove that the optimal solution can be achieved by the
(J,K)-Threshold Algorithm, which has a nice &quot;rational description&quot; for the case
K = 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0626</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0626</id><created>2013-07-02</created><authors><author><keyname>Bandhekar</keyname><forenames>S. F.</forenames></author><author><keyname>Dhurvey</keyname><forenames>S. N.</forenames></author><author><keyname>Ashtankar</keyname><forenames>P. P.</forenames></author></authors><title>Simulation Un-Symmetrical 2 Phase Induction Motor</title><categories>cs.SY</categories><comments>4 pages, 8 Figures, IJCSN Journal Volume 2, Issue 3, June 2013</comments><report-no>IJCSN-2013-2-3-18</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The equations of unsymmetrical 2-phase induction motors are established and a
computer representation is developed from these equations. Computer
representation of single phase motors are developed by extension and
modification of the unsymmetrical 2-phase induction motors representation.
These equations of an unsymmetrical 2-phase induction motors are describe the
dynamic performance of equations of unsymmetrical 2-phase induction motors. The
system is simulated to verify its capability such as input phase voltage,
stator and rotor currents, electromagnetic torque and rotor speed. The
performance of an unsymmetrical 2-p
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0635</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0635</id><created>2013-07-02</created><authors><author><keyname>&#xc9;sik</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Quaas</keyname><forenames>Karin</forenames></author></authors><title>Kleene Algebras and Semimodules for Energy Problems</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the purpose of unifying a number of approaches to energy problems found
in the literature, we introduce generalized energy automata. These are finite
automata whose edges are labeled with energy functions that define how energy
levels evolve during transitions. Uncovering a close connection between energy
problems and reachability and B\&quot;uchi acceptance for semiring-weighted
automata, we show that these generalized energy problems are decidable. We also
provide complexity results for important special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0642</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0642</id><created>2013-07-02</created><authors><author><keyname>Jassim</keyname><forenames>Firas A.</forenames></author></authors><title>A Novel Steganography Algorithm for Hiding Text in Image using Five
  Modulus Method</title><categories>cs.MM cs.CR</categories><journal-ref>International Journal of Computer Applications, Vol.72, No.17, pp.
  39-44, June 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The needs for steganographic techniques for hiding secret message inside
images have been arise. This paper is to create a practical steganographic
implementation to hide text inside grey scale images. The secret message is
hidden inside the cover image using Five Modulus Method. The novel algorithm is
called (ST-FMM. FMM which consists of transforming all the pixels within the
5X5 window size into its corresponding multiples of 5. After that, the secret
message is hidden inside the 5X5 window as a non-multiples of 5. Since the
modulus of non-multiples of 5 are 1,2,3 and 4, therefore; if the reminder is
one of these, then this pixel represents a secret character. The secret key
that has to be sent is the window size. The main advantage of this novel
algorithm is to keep the size of the cover image constant while the secret
message increased in size. Peak signal-to-noise ratio is captured for each of
the images tested. Based on the PSNR value of each images, the stego image has
high PSNR value. Hence this new steganography algorithm is very efficient to
hide the data inside the image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0643</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0643</id><created>2013-07-02</created><authors><author><keyname>Kov&#xe1;cs</keyname><forenames>Edith</forenames></author><author><keyname>Sz&#xe1;ntai</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Discovering the Markov network structure</title><categories>cs.IT cs.LG math.IT</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new proof is given for the supermodularity of information
content. Using the decomposability of the information content an algorithm is
given for discovering the Markov network graph structure endowed by the
pairwise Markov property of a given probability distribution. A discrete
probability distribution is given for which the equivalence of
Hammersley-Clifford theorem is fulfilled although some of the possible vector
realizations are taken on with zero probability. Our algorithm for discovering
the pairwise Markov network is illustrated on this example, too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0667</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0667</id><created>2013-07-02</created><updated>2013-10-10</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Mutz</keyname><forenames>Ruediger</forenames></author></authors><title>From P100 to P100_: Conception and improvement of a new citation-rank
  approach in bibliometrics</title><categories>cs.DL</categories><comments>Accepted for publication in the Journal of the American Society for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Properties of a percentile-based rating scale needed in bibliometrics are
formulated. Based on these properties, P100 was recently introduced as a new
citation-rank approach (Bornmann, Leydesdorff, &amp; Wang, in press). In this
paper, we conceptualize P100 and propose an improvement which we call P100_.
Advantages and disadvantages of citation-rank indicators are noted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0679</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0679</id><created>2013-07-02</created><authors><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author><author><keyname>Meo</keyname><forenames>Maria Chiara</forenames></author><author><keyname>Tacchella</keyname><forenames>Paolo</forenames></author><author><keyname>Wiklicky</keyname><forenames>Herbert</forenames></author></authors><title>Unfolding for CHR programs</title><categories>cs.PL</categories><comments>49 pages</comments><journal-ref>Theory and Practice of Logic Programming, 15(3), 264-311, 2015</journal-ref><doi>10.1017/S1471068413000288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program transformation is an appealing technique which allows to improve
run-time efficiency, space-consumption, and more generally to optimize a given
program. Essentially, it consists of a sequence of syntactic program
manipulations which preserves some kind of semantic equivalence. Unfolding is
one of the basic operations which is used by most program transformation
systems and which consists in the replacement of a procedure call by its
definition. While there is a large body of literature on transformation and
unfolding of sequential programs, very few papers have addressed this issue for
concurrent languages.
  This paper defines an unfolding system for CHR programs. We define an
unfolding rule, show its correctness and discuss some conditions which can be
used to delete an unfolded rule while preserving the program meaning. We also
prove that, under some suitable conditions, confluence and termination are
preserved by the above transformation.
  To appear in Theory and Practice of Logic Programming (TPLP)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0685</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0685</id><created>2013-07-02</created><updated>2014-02-28</updated><authors><author><keyname>Zewail</keyname><forenames>Ahmed A.</forenames></author><author><keyname>Nafie</keyname><forenames>M.</forenames></author><author><keyname>Mohasseb</keyname><forenames>Y.</forenames></author><author><keyname>Gamal</keyname><forenames>H. EL</forenames></author></authors><title>Achievable Degrees of Freedom Region of the MIMO Relay Networks using
  the Detour Schemes</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figure, to appear in ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the degrees of freedom (DoF) of the MIMO relay
networks. We start with a general Y channel, where each user has $M_i$ antennas
and aims to exchange messages with the other two users via a relay equipped
with $N$ antennas. Then, we extend our work to a general 4-user MIMO relay
network. Unlike most previous work which focused on the total DoF of the
network, our aim here is to characterize the achievable DoF region as well. We
develop an outer bound on the DoF region based on the notion of one sided
genie. Then, we define a new achievable region using the Signal Space Alignment
(SSA) and the Detour Schemes. Our achievable scheme achieves the upper bound
for certain conditions relating $M_i$'s and $N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0687</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0687</id><created>2013-07-02</created><authors><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author><author><keyname>Gorbil</keyname><forenames>Gokce</forenames></author><author><keyname>Tzovaras</keyname><forenames>Dimitrios</forenames></author><author><keyname>Liebergeld</keyname><forenames>Steffen</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Baltatu</keyname><forenames>Madalina</forenames></author><author><keyname>Lyberopoulos</keyname><forenames>George</forenames></author></authors><title>Security for Smart Mobile Networks: The NEMESYS Approach</title><categories>cs.NI cs.CR</categories><comments>Accepted for publication in PRISMS'13; 8 pages. arXiv admin note:
  text overlap with arXiv:1305.5483</comments><acm-class>C.2.3; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing popularity of smart mobile devices such as smartphones and
tablets has made them an attractive target for cyber-criminals, resulting in a
rapidly growing and evolving mobile threat as attackers experiment with new
business models by targeting mobile users. With the emergence of the first
large-scale mobile botnets, the core network has also become vulnerable to
distributed denial-of-service attacks such as the signaling attack.
Furthermore, complementary access methods such as Wi-Fi and femtocells
introduce additional vulnerabilities for the mobile users as well as the core
network. In this paper, we present the NEMESYS approach to smart mobile network
security. The goal of the NEMESYS project is to develop novel security
technologies for seamless service provisioning in the smart mobile ecosystem,
and to improve mobile network security through a better understanding of the
threat landscape. To this purpose, NEMESYS will collect and analyze information
about the nature of cyber-attacks targeting smart mobile devices and the core
network so that appropriate counter-measures can be taken. We are developing a
data collection infrastructure that incorporates virtualized mobile honeypots
and honeyclients in order to gather, detect and provide early warning of mobile
attacks and understand the modus operandi of cyber-criminals that target mobile
devices. By correlating the extracted information with known attack patterns
from wireline networks, we plan to reveal and identify the possible shift in
the way that cyber-criminals launch attacks against smart mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0720</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0720</id><created>2013-07-02</created><authors><author><keyname>Berend</keyname><forenames>Daniel</forenames></author><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author></authors><title>The state complexity of random DFAs</title><categories>math.PR cs.FL math.CO</categories><msc-class>60C05, 68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state complexity of a Deterministic Finite-state automaton (DFA) is the
number of states in its minimal equivalent DFA. We study the state complexity
of random $n$-state DFAs over a $k$-symbol alphabet, drawn uniformly from the
set $[n]^{[n]\times[k]}\times2^{[n]}$ of all such automata. We show that, with
high probability, the latter is $\alpha_k n + O(\sqrt n\log n)$ for a certain
explicit constant $\alpha_k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0725</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0725</id><created>2013-07-02</created><updated>2013-07-31</updated><authors><author><keyname>Droste</keyname><forenames>M.</forenames></author><author><keyname>Esik</keyname><forenames>Z.</forenames></author><author><keyname>Kuich</keyname><forenames>W.</forenames></author></authors><title>Conway and iteration hemirings</title><categories>cs.FL</categories><msc-class>68Q70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conway hemirings are Conway semirings without a multiplicative unit. We also
define iteration hemirings as Conway hemirings satisfying certain identities
associated with the finite groups. Iteration hemirings are iteration semirings
without a multiplicative unit. We provide an analysis of the relationship
between Conway hemirings and (partial) Conway semirings and describe several
free constructions. In the second part of the paper we define and study
hemimodules of Conway and iteration hemirings, and show their applicability in
the analysis of quantitative aspects of the infinitary behavior of weighted
transition systems. These include discounted and average computations of
weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0747</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0747</id><created>2013-07-02</created><authors><author><keyname>Foan</keyname><forenames>Stephanie</forenames></author><author><keyname>Jackson</keyname><forenames>Andrew</forenames></author><author><keyname>Spendlove</keyname><forenames>Ian</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Simulating the Dynamics of T Cell Subsets Throughout the Lifetime</title><categories>cs.CE</categories><comments>Proceedings of the 10th International Conference on Artificial Immune
  Systems (ICARIS 2011), LNCS Volume 6825, Cambridge, UK, pp 71-76</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely accepted that the immune system undergoes age-related changes
correlating with increased disease in the elderly. T cell subsets have been
implicated. The aim of this work is firstly to implement and validate a
simulation of T regulatory cell (Treg) dynamics throughout the lifetime, based
on a model by Baltcheva. We show that our initial simulation produces an
inversion between precursor and mature Treys at around 20 years of age, though
the output differs significantly from the original laboratory dataset.
Secondly, this report discusses development of the model to incorporate new
data from a cross-sectional study of healthy blood donors addressing balance
between Treys and Th17 cells with novel markers for Treg. The potential for
simulation to add insight into immune aging is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0749</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0749</id><created>2013-07-02</created><authors><author><keyname>Siebers</keyname><forenames>Peer-Olaf</forenames></author><author><keyname>Sherman</keyname><forenames>Galina</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Menachof</keyname><forenames>David</forenames></author></authors><title>Comparing Decison Support Tools for Cargo Screening Processes</title><categories>cs.CE</categories><comments>The 10th International Conference on Modeling and Applied Simulation
  (MAS), 12-14 September, Rome, Italy, 31-39</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When planning to change operations at ports there are two key stake holders
with very different interests involved in the decision making processes. Port
operators are attentive to their standards, a smooth service flow and economic
viability while border agencies are concerned about national security. The time
taken for security checks often interferes with the compliance to service
standards that port operators would like to achieve. Decision support tools as
for example Cost-Benefit Analysis or Multi Criteria Analysis are useful helpers
to better understand the impact of changes to a system. They allow
investigating future scenarios and helping to find solutions that are
acceptable for all parties involved in port operations. In this paper we
evaluate two different modelling methods, namely scenario analysis and discrete
event simulation. These are useful for driving the decision support tools (i.e.
they provide the inputs the decision support tools require). Our aims are, on
the one hand, to guide the reader through the modelling processes and, on the
other hand, to demonstrate what kind of decision support information one can
obtain from the different modelling methods presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0772</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0772</id><created>2013-07-02</created><authors><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>How to Build an RSS Feed using ASP</title><categories>cs.OH</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RSS is a XML based format. The Current popular version of RSS is RSS version
2.0. The purpose of adding an RSS feed to your site is to show if anything new
is added to the site. For example, if a new article or blog or news item is
added to your site that should automatically appear in the RSS feed so that the
visitors/ RSS readers will automatically get updated about this new addition.
The RSS feed is also called RSS channel.
  There are two main elements of the RSS XML file, one is the header or channel
element that describes the details about the site/feeder and other is the body
or item element that describes the consists of individual articles/entries
updated in the site. As the format of the RSS feed file is pretty simple, it
can be coded in any language, ASP, PHP or anything of that sort. We will build
an RSS feeder using classical ASP (Active Server Pages) code in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0776</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0776</id><created>2013-07-02</created><authors><author><keyname>Cheng</keyname><forenames>Jian</forenames></author><author><keyname>Jiang</keyname><forenames>Tianzi</forenames></author><author><keyname>Deriche</keyname><forenames>Rachid</forenames></author><author><keyname>Shen</keyname><forenames>Dinggang</forenames></author><author><keyname>Yap</keyname><forenames>Pew-Thian</forenames></author></authors><title>Regularized Spherical Polar Fourier Diffusion MRI with Optimal
  Dictionary Learning</title><categories>cs.CV</categories><comments>Accepted by MICCAI 2013. Abstract shortened to respect the arXiv
  limit of 1920 characters</comments><doi>10.1007/978-3-642-40811-3_80</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed Sensing (CS) takes advantage of signal sparsity or compressibility
and allows superb signal reconstruction from relatively few measurements. Based
on CS theory, a suitable dictionary for sparse representation of the signal is
required. In diffusion MRI (dMRI), CS methods were proposed to reconstruct
diffusion-weighted signal and the Ensemble Average Propagator (EAP), and there
are two kinds of Dictionary Learning (DL) methods: 1) Discrete Representation
DL (DR-DL), and 2) Continuous Representation DL (CR-DL). DR-DL is susceptible
to numerical inaccuracy owing to interpolation and regridding errors in a
discretized q-space. In this paper, we propose a novel CR-DL approach, called
Dictionary Learning - Spherical Polar Fourier Imaging (DL-SPFI) for effective
compressed-sensing reconstruction of the q-space diffusion-weighted signal and
the EAP. In DL-SPFI, an dictionary that sparsifies the signal is learned from
the space of continuous Gaussian diffusion signals. The learned dictionary is
then adaptively applied to different voxels using a weighted LASSO framework
for robust signal reconstruction. The adaptive dictionary is proved to be
optimal. Compared with the start-of-the-art CR-DL and DR-DL methods proposed by
Merlet et al. and Bilgic et al., espectively, our work offers the following
advantages. First, the learned dictionary is proved to be optimal for Gaussian
diffusion signals. Second, to our knowledge, this is the first work to learn a
voxel-adaptive dictionary. The importance of the adaptive dictionary in EAP
reconstruction will be demonstrated theoretically and empirically. Third,
optimization in DL-SPFI is only performed in a small subspace resided by the
SPF coefficients, as opposed to the q-space approach utilized by Merlet et al.
The experiment results demonstrate the advantages of DL-SPFI over the original
SPF basis and Bilgic et al.'s method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0781</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0781</id><created>2013-07-02</created><authors><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Distributed Online Big Data Classification Using Context Information</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed, online data mining systems have emerged as a result of
applications requiring analysis of large amounts of correlated and
high-dimensional data produced by multiple distributed data sources. We propose
a distributed online data classification framework where data is gathered by
distributed data sources and processed by a heterogeneous set of distributed
learners which learn online, at run-time, how to classify the different data
streams either by using their locally available classification functions or by
helping each other by classifying each other's data. Importantly, since the
data is gathered at different locations, sending the data to another learner to
process incurs additional costs such as delays, and hence this will be only
beneficial if the benefits obtained from a better classification will exceed
the costs. We model the problem of joint classification by the distributed and
heterogeneous learners from multiple data sources as a distributed contextual
bandit problem where each data is characterized by a specific context. We
develop a distributed online learning algorithm for which we can prove
sublinear regret. Compared to prior work in distributed online data mining, our
work is the first to provide analytic regret results characterizing the
performance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0788</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0788</id><created>2013-07-02</created><updated>2013-12-15</updated><authors><author><keyname>Sienkiewicz</keyname><forenames>Julian</forenames></author><author><keyname>Soja</keyname><forenames>Krzysztof</forenames></author><author><keyname>Holyst</keyname><forenames>Janusz A.</forenames></author><author><keyname>Sloot</keyname><forenames>Peter M. A.</forenames></author></authors><title>Categorical and Geographical Separation in Science</title><categories>physics.soc-ph cs.DL</categories><comments>17 pages, 12 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform the analysis of scientific collaboration at the level of
universities. The scope of this study is to answer two fundamental questions:
(i) can one indicate a category (i.e., a scientific discipline) that has the
greatest impact on the rank of the university and (ii) do the best universities
collaborate with the best ones only? Using two university ranking lists (ARWU
and QS) as well as data from the Science Citation Index we show how the number
of publications in certain categories correlates with the university rank.
Moreover, using complex networks analysis, we give hints that the scientific
collaboration is highly embedded in the physical space and the number of common
papers decays with the distance between them. We also show the strength of the
ties between universities is proportional to product of their total number of
publications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0794</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0794</id><created>2013-07-02</created><updated>2013-07-03</updated><authors><author><keyname>Grandl</keyname><forenames>Reinhard</forenames></author><author><keyname>Su</keyname><forenames>Kai</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author></authors><title>On the Interaction of Adaptive Video Streaming with Content-Centric
  Networking</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two main trends in today's internet are of major interest for video streaming
services: most content delivery platforms coincide towards using adaptive video
streaming over HTTP and new network architectures allowing caching at
intermediate points within the network. We investigate one of the most popular
streaming service in terms of rate adaptation and opportunistic caching. Our
experimental study shows that the streaming client's rate selection trajectory,
i.e., the set of selected segments of varied bit rates which constitute a
complete video, is not repetitive across separate downloads. Also, the
involvement of caching could lead to frequent alternation between cache and
server when serving back client's requests for video segments. These
observations warrant cautions for rate adaption algorithm design and trigger
our analysis to characterize the performance of in-network caching for HTTP
streaming. Our analytic results show: (i) a significant degradation of cache
hit rate for adaptive streaming, with a typical file popularity distribution in
nowadays internet; (ii) as a result of the (usually) higher throughput at the
client-cache connection compared to client-server one, cache-server
oscillations due to misjudgments of the rate adaptation algorithm occur.
Finally, we introduce DASH-INC, a framework for improved video streaming in
caching networks including transcoding and multiple throughput estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0802</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0802</id><created>2013-07-02</created><updated>2014-02-07</updated><authors><author><keyname>Huggins</keyname><forenames>Jonathan H.</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>A Statistical Learning Theory Framework for Supervised Pattern Discovery</title><categories>stat.ML cs.AI</categories><comments>12 pages, 1 figure. Title change. Full version. Appearing at the SIAM
  International Conference on Data Mining (SDM) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper formalizes a latent variable inference problem we call {\em
supervised pattern discovery}, the goal of which is to find sets of
observations that belong to a single ``pattern.'' We discuss two versions of
the problem and prove uniform risk bounds for both. In the first version,
collections of patterns can be generated in an arbitrary manner and the data
consist of multiple labeled collections. In the second version, the patterns
are assumed to be generated independently by identically distributed processes.
These processes are allowed to take an arbitrary form, so observations within a
pattern are not in general independent of each other. The bounds for the second
version of the problem are stated in terms of a new complexity measure, the
quasi-Rademacher complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0803</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0803</id><created>2013-07-02</created><updated>2015-02-06</updated><authors><author><keyname>&#x17d;itnik</keyname><forenames>Marinka</forenames></author><author><keyname>Zupan</keyname><forenames>Bla&#x17e;</forenames></author></authors><title>Data Fusion by Matrix Factorization</title><categories>cs.LG cs.AI cs.DB stat.ML</categories><comments>Short preprint, 13 pages, 3 Figures, 3 Tables. Full paper in
  10.1109/TPAMI.2014.2343973</comments><msc-class>15A83, 15A23, 40C05, 65F30</msc-class><acm-class>H.2.8; G.1.3; I.2.6; H.3.3</acm-class><journal-ref>Marinka Zitnik and Blaz Zupan. IEEE Transactions on Pattern
  Analysis and Machine Intelligence, 37(1):41-53 (2015)</journal-ref><doi>10.1109/TPAMI.2014.2343973</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For most problems in science and engineering we can obtain data sets that
describe the observed system from various perspectives and record the behavior
of its individual components. Heterogeneous data sets can be collectively mined
by data fusion. Fusion can focus on a specific target relation and exploit
directly associated data together with contextual data and data about system's
constraints. In the paper we describe a data fusion approach with penalized
matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to
reveal hidden associations. The approach can directly consider any data that
can be expressed in a matrix, including those from feature-based
representations, ontologies, associations and networks. We demonstrate the
utility of DFMF for gene function prediction task with eleven different data
sources and for prediction of pharmacologic actions by fusing six data sources.
Our data fusion algorithm compares favorably to alternative data integration
approaches and achieves higher accuracy than can be obtained from any single
data source alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0805</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0805</id><created>2013-07-02</created><updated>2013-10-30</updated><authors><author><keyname>Zhang</keyname><forenames>Zemin</forenames></author><author><keyname>Ely</keyname><forenames>Gregory</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author><author><keyname>Hao</keyname><forenames>Ning</forenames></author><author><keyname>Kilmer</keyname><forenames>Misha</forenames></author></authors><title>Novel Factorization Strategies for Higher Order Tensors: Implications
  for Compression and Recovery of Multi-linear Data</title><categories>cs.IT cs.CV math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose novel methods for compression and recovery of
multilinear data under limited sampling. We exploit the recently proposed
tensor- Singular Value Decomposition (t-SVD)[1], which is a group theoretic
framework for tensor decomposition. In contrast to popular existing tensor
decomposition techniques such as higher-order SVD (HOSVD), t-SVD has optimality
properties similar to the truncated SVD for matrices. Based on t-SVD, we first
construct novel tensor-rank like measures to characterize informational and
structural complexity of multilinear data. Following that we outline a
complexity penalized algorithm for tensor completion from missing entries. As
an application, 3-D and 4-D (color) video data compression and recovery are
considered. We show that videos with linear camera motion can be represented
more efficiently using t-SVD compared to traditional approaches based on
vectorizing or flattening of the tensors. Application of the proposed tensor
completion algorithm for video recovery from missing entries is shown to yield
a superior performance over existing methods. In conclusion we point out
several research directions and implications to online prediction of
multilinear data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0813</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0813</id><created>2013-07-02</created><updated>2014-02-12</updated><authors><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author><author><keyname>Englert</keyname><forenames>Peter</forenames></author><author><keyname>Peters</keyname><forenames>Jan</forenames></author><author><keyname>Fox</keyname><forenames>Dieter</forenames></author></authors><title>Multi-Task Policy Search</title><categories>stat.ML cs.AI cs.LG cs.RO</categories><comments>8 pages, double column. IEEE International Conference on Robotics and
  Automation, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning policies that generalize across multiple tasks is an important and
challenging research topic in reinforcement learning and robotics. Training
individual policies for every single potential task is often impractical,
especially for continuous task variations, requiring more principled approaches
to share and transfer knowledge among similar tasks. We present a novel
approach for learning a nonlinear feedback policy that generalizes across
multiple tasks. The key idea is to define a parametrized policy as a function
of both the state and the task, which allows learning a single policy that
generalizes across multiple known and unknown tasks. Applications of our novel
approach to reinforcement and imitation learning in real-robot experiments are
shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0814</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0814</id><created>2013-07-02</created><authors><author><keyname>Asgari</keyname><forenames>Fereshteh</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Becker</keyname><forenames>Monique</forenames></author></authors><title>A survey on Human Mobility and its applications</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human Mobility has attracted attentions from different fields of studies such
as epidemic modeling, traffic engineering, traffic prediction and urban
planning. In this survey we review major characteristics of human mobility
studies including from trajectory-based studies to studies using graph and
network theory. In trajectory-based studies statistical measures such as jump
length distribution and radius of gyration are analyzed in order to investigate
how people move in their daily life, and if it is possible to model this
individual movements and make prediction based on them. Using graph in mobility
studies, helps to investigate the dynamic behavior of the system, such as
diffusion and flow in the network and makes it easier to estimate how much one
part of the network influences another by using metrics like centrality
measures. We aim to study population flow in transportation networks using
mobility data to derive models and patterns, and to develop new applications in
predicting phenomena such as congestion. Human Mobility studies with the new
generation of mobility data provided by cellular phone networks, arise new
challenges such as data storing, data representation, data analysis and
computation complexity. A comparative review of different data types used in
current tools and applications of Human Mobility studies leads us to new
approaches for dealing with mentioned challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0836</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0836</id><created>2013-07-02</created><authors><author><keyname>Jordan</keyname><forenames>Stephen P.</forenames></author></authors><title>Strong equivalence of reversible circuits is coNP-complete</title><categories>cs.CC quant-ph</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that deciding equivalence of logic circuits is a
coNP-complete problem. As a corollary, the problem of deciding weak equivalence
of reversible circuits, i.e. ignoring the ancilla bits, is also coNP-complete.
The complexity of deciding strong equivalence, including the ancilla bits, is
less obvious and may depend on gate set. Here we use Barrington's theorem to
show that deciding strong equivalence of reversible circuits built from the
Fredkin gate is coNP-complete. This implies coNP-completeness of deciding
strong equivalence for other commonly used universal reversible gate sets,
including any gate set that includes the Toffoli or Fredkin gate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0841</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0841</id><created>2013-07-02</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Brest</keyname><forenames>Janez</forenames></author></authors><title>Comparing various regression methods on ensemble strategies in
  differential evolution</title><categories>cs.NE</categories><journal-ref>In Proceedings of 19th International Conference on Soft Computing
  MENDEL 2013, Brno, 2013, pp. 87-92</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential evolution possesses a multitude of various strategies for
generating new trial solutions. Unfortunately, the best strategy is not known
in advance. Moreover, this strategy usually depends on the problem to be
solved. This paper suggests using various regression methods (like random
forest, extremely randomized trees, gradient boosting, decision trees, and a
generalized linear model) on ensemble strategies in differential evolution
algorithm by predicting the best differential evolution strategy during the
run. Comparing the preliminary results of this algorithm by optimizing a suite
of five well-known functions from literature, it was shown that using the
random forest regression method substantially outperformed the results of the
other regression methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0844</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0844</id><created>2013-07-02</created><authors><author><keyname>Todor</keyname><forenames>Andrei</forenames></author><author><keyname>Dobra</keyname><forenames>Alin</forenames></author><author><keyname>Kahveci</keyname><forenames>Tamer</forenames></author><author><keyname>Dudley</keyname><forenames>Christopher</forenames></author></authors><title>Making massive probabilistic databases practical</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existence of incomplete and imprecise data has moved the database paradigm
from deterministic to proba- babilistic information. Probabilistic databases
contain tuples that may or may not exist with some probability. As a result,
the number of possible deterministic database instances that can be observed
from a probabilistic database grows exponentially with the number of
probabilistic tuples. In this paper, we consider the problem of answering both
aggregate and non-aggregate queries on massive probabilistic databases. We
adopt the tuple independence model, in which each tuple is assigned a
probability value. We develop a method that exploits Probability Generating
Functions (PGF) to answer such queries efficiently. Our method maintains a
polynomial for each tuple. It incrementally builds a master polynomial that
expresses the distribution of the possible result values precisely. We also
develop an approximation method that finds the distribution of the result value
with negligible errors. Our experiments suggest that our methods are orders of
magnitude faster than the most recent systems that answer such queries,
including MayBMS and SPROUT. In our experiments, we were able to scale up to
several terabytes of data on TPC- H queries, while existing methods could only
run for a few gigabytes of data on the same queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0845</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0845</id><created>2013-06-13</created><updated>2013-12-23</updated><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>The SP theory of intelligence: benefits and applications</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1212.0229</comments><journal-ref>J G Wolff, Information, 5 (1), 1-27, 2014</journal-ref><doi>10.3390/info5010001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes existing and expected benefits of the &quot;SP theory of
intelligence&quot;, and some potential applications. The theory aims to simplify and
integrate ideas across artificial intelligence, mainstream computing, and human
perception and cognition, with information compression as a unifying theme. It
combines conceptual simplicity with descriptive and explanatory power across
several areas of computing and cognition. In the &quot;SP machine&quot; -- an expression
of the SP theory which is currently realized in the form of a computer model --
there is potential for an overall simplification of computing systems,
including software. The SP theory promises deeper insights and better solutions
in several areas of application including, most notably, unsupervised learning,
natural language processing, autonomous robots, computer vision, intelligent
databases, software engineering, information compression, medical diagnosis and
big data. There is also potential in areas such as the semantic web,
bioinformatics, structuring of documents, the detection of computer viruses,
data fusion, new kinds of computer, and the development of scientific theories.
The theory promises seamless integration of structures and functions within and
between different areas of application. The potential value, worldwide, of
these benefits and applications is at least $190 billion each year. Further
development would be facilitated by the creation of a high-parallel,
open-source version of the SP machine, available to researchers everywhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0846</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0846</id><created>2013-07-02</created><authors><author><keyname>Tsivtsivadze</keyname><forenames>Evgeni</forenames></author><author><keyname>Heskes</keyname><forenames>Tom</forenames></author></authors><title>Semi-supervised Ranking Pursuit</title><categories>stat.ML cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel sparse preference learning/ranking algorithm. Our
algorithm approximates the true utility function by a weighted sum of basis
functions using the squared loss on pairs of data points, and is a
generalization of the kernel matching pursuit method. It can operate both in a
supervised and a semi-supervised setting and allows efficient search for
multiple, near-optimal solutions. Furthermore, we describe the extension of the
algorithm suitable for combined ranking and regression tasks. In our
experiments we demonstrate that the proposed algorithm outperforms several
state-of-the-art learning methods when taking into account unlabeled data and
performs comparably in a supervised learning scenario, while providing sparser
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0849</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0849</id><created>2013-07-02</created><authors><author><keyname>Yang</keyname><forenames>James Yifei</forenames></author><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author></authors><title>Single Video Performance Analysis for Video-on-Demand Systems</title><categories>cs.NI cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the content placement problem for cache delivery video-on-demand
systems under static random network topologies with fixed heavy-tailed video
demand. The performance measure is the amount of server load; we wish to
minimize the total download rate for all users from the server and maximize the
rate from caches. Our approach reduces the analysis for multiple videos to
consideration of decoupled systems with a single video each. For each placement
policy, insights gained from the single video analysis carry back to the
original multiple video content placement problem. Finally, we propose a hybrid
placement technique that achieves near optimal performance with low complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0855</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0855</id><created>2013-07-02</created><updated>2013-07-31</updated><authors><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author><author><keyname>Domiguez-Garcia</keyname><forenames>Alejandro D.</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>A Local Control Approach to Voltage Regulation in Distribution Networks</title><categories>math.OC cs.SY</categories><comments>shorter version submitted to NAPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper address the problem of voltage regulation in power distribution
networks with deep penetration of distributed energy resources (DERs) without
any explicit communication between the buses in the network. We cast the
problem as an optimization problem with the objective of minimizing the
distance between the bus voltage magnitudes and some reference voltage profile.
We present an iterative algorithm where each bus updates the reactive power
injection provided by their DER. The update at a bus only depends on the
voltage magnitude at that bus, and for this reason, we call the algorithm a
local control algorithm. We provide sufficient conditions that guarantee the
convergence of the algorithm and these conditions can be checked a priori for a
set of feasible power injections. We also provide necessary conditions
establishing that longer and more heavily loaded networks are inherently more
difficult to control. We illustrate the operation of the algorithm through case
studies involving 8-,34- and 123-bus test distribution systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0861</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0861</id><created>2013-07-02</created><updated>2014-03-17</updated><authors><author><keyname>Renna</keyname><forenames>Francesco</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Reconstruction of Signals Drawn from a Gaussian Mixture from Noisy
  Compressive Measurements</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2014.2309560</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper determines to within a single measurement the minimum number of
measurements required to successfully reconstruct a signal drawn from a
Gaussian mixture model in the low-noise regime. The method is to develop upper
and lower bounds that are a function of the maximum dimension of the linear
subspaces spanned by the Gaussian mixture components. The method not only
reveals the existence or absence of a minimum mean-squared error (MMSE) error
floor (phase transition) but also provides insight into the MMSE decay via
multivariate generalizations of the MMSE dimension and the MMSE power offset,
which are a function of the interaction between the geometrical properties of
the kernel and the Gaussian mixture. These results apply not only to standard
linear random Gaussian measurements but also to linear kernels that minimize
the MMSE. It is shown that optimal kernels do not change the number of
measurements associated with the MMSE phase transition, rather they affect the
sensed power required to achieve a target MMSE in the low-noise regime.
Overall, our bounds are tighter and sharper than standard bounds on the minimum
number of measurements needed to recover sparse signals associated with a union
of subspaces model, as they are not asymptotic in the signal dimension or
signal sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0870</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0870</id><created>2013-07-02</created><updated>2014-04-04</updated><authors><author><keyname>Charalambides</keyname><forenames>Marcos</forenames></author></authors><title>Distinct Distances on Curves via Rigidity</title><categories>math.MG cs.CG</categories><comments>33 pages, to appear in Discrete &amp; Computational Geometry</comments><doi>10.1007/s00454-014-9586-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that $N$ points on a real algebraic curve of degree $n$ in
$\mathbb{R}^d$ always determine $\gtrsim_{n,d}N^{1+\frac{1}{4}}$ distinct
distances, unless the curve is a straight line or the closed geodesic of a flat
torus. In the latter case, there are arrangements of $N$ points which determine
$\lesssim N$ distinct distances. The method may be applied to other quantities
of interest to obtain analogous exponent gaps. An important step in the proof
involves understanding the structural rigidity of certain frameworks on curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0885</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0885</id><created>2013-07-02</created><authors><author><keyname>Hu</keyname><forenames>Honggang</forenames></author><author><keyname>Shao</keyname><forenames>Shuai</forenames></author><author><keyname>Gong</keyname><forenames>Guang</forenames></author><author><keyname>Helleseth</keyname><forenames>Tor</forenames></author></authors><title>The Proof of Lin's Conjecture via the Decimation-Hadamard Transform</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1998, Lin presented a conjecture on a class of ternary sequences with
ideal 2-level autocorrelation in his Ph.D thesis. Those sequences have a very
simple structure, i.e., their trace representation has two trace monomial
terms. In this paper, we present a proof for the conjecture. The mathematical
tools employed are the second-order multiplexing decimation-Hadamard transform,
Stickelberger's theorem, the Teichm\&quot;{u}ller character, and combinatorial
techniques for enumerating the Hamming weights of ternary numbers. As a
by-product, we also prove that the Lin conjectured ternary sequences are
Hadamard equivalent to ternary $m$-sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0905</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0905</id><created>2013-07-02</created><updated>2014-06-13</updated><authors><author><keyname>Kunkler</keyname><forenames>Sarah J.</forenames></author><author><keyname>LaMar</keyname><forenames>M. Drew</forenames></author><author><keyname>Kincaid</keyname><forenames>Rex K.</forenames></author><author><keyname>Phillips</keyname><forenames>David</forenames></author></authors><title>Algorithm and Complexity for a Network Assortativity Measure</title><categories>math.CO cs.DM</categories><comments>Added additional section on applications</comments><msc-class>90C27</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that finding a graph realization with the minimum Randi\'c index for
a given degree sequence is solvable in polynomial time by formulating the
problem as a minimum weight perfect b-matching problem. However, the
realization found via this reduction is not guaranteed to be connected.
Approximating the minimum weight b-matching problem subject to a connectivity
constraint is shown to be NP-Hard. For instances in which the optimal solution
to the minimum Randi\'c index problem is not connected, we describe a heuristic
to connect the graph using pairwise edge exchanges that preserves the degree
sequence. In our computational experiments, the heuristic performs well and the
Randi\'c index of the realization after our heuristic is within 3% of the
unconstrained optimal value on average. Although we focus on minimizing the
Randi\'c index, our results extend to maximizing the Randi\'c index as well.
Applications of the Randi\'c index to synchronization of neuronal networks
controlling respiration in mammals and to normalizing cortical thickness
networks in diagnosing individuals with dementia are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0914</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0914</id><created>2013-07-03</created><updated>2015-08-27</updated><authors><author><keyname>Amodio</keyname><forenames>P.</forenames></author><author><keyname>Blinkov</keyname><forenames>Yu.</forenames></author><author><keyname>Gerdt</keyname><forenames>V.</forenames></author><author><keyname>La Scala</keyname><forenames>R.</forenames></author></authors><title>On Consistency of Finite Difference Approximations to the Navier-Stokes
  Equations</title><categories>math.NA cs.NA physics.flu-dyn</categories><comments>15 pages, 4 figures</comments><msc-class>65M06, 76D05</msc-class><journal-ref>LNCS 8136, Springer-Verlag, 2013, pp.46-60</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the given paper, we confront three finite difference approximations to the
Navier--Stokes equations for the two-dimensional viscous incomressible fluid
flows. Two of these approximations were generated by the computer algebra
assisted method proposed based on the finite volume method, numerical
integration, and difference elimination. The third approximation was derived by
the standard replacement of the temporal derivatives with the forward
differences and the spatial derivatives with the central differences. We prove
that only one of these approximations is strongly consistent with the
Navier--Stokes equations and present our numerical tests which show that this
approximation has a better behavior than the other two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0920</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0920</id><created>2013-07-03</created><authors><author><keyname>Ilambharathi</keyname><forenames>K.</forenames></author><author><keyname>Manik</keyname><forenames>G. S. N. V. Venkata</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author><author><keyname>Sivaselvan</keyname><forenames>B.</forenames></author></authors><title>Domain Specific Hierarchical Huffman Encoding</title><categories>cs.IT cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the classical data compression problem for domain
specific texts. It is well-known that classical Huffman algorithm is optimal
with respect to prefix encoding and the compression is done at character level.
Since many data transfer are domain specific, for example, downloading of
lecture notes, web-blogs, etc., it is natural to think of data compression in
larger dimensions (i.e. word level rather than character level). Our framework
employs a two-level compression scheme in which the first level identifies
frequent patterns in the text using classical frequent pattern algorithms. The
identified patterns are replaced with special strings and to acheive a better
compression ratio the length of a special string is ensured to be shorter than
the length of the corresponding pattern. After this transformation, on the
resultant text, we employ classical Huffman data compression algorithm. In
short, in the first level compression is done at word level and in the second
level it is at character level. Interestingly, this two level compression
technique for domain specific text outperforms classical Huffman technique. To
support our claim, we have presented both theoretical and simulation results
for domain specific texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0927</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0927</id><created>2013-07-03</created><authors><author><keyname>Liu</keyname><forenames>Xiaogang</forenames></author><author><keyname>Luo</keyname><forenames>Yuan</forenames></author></authors><title>On the bounds and achievability about the ODPC of $\mathcal{GRM}(2,m)^*$
  over prime field for increasing message length</title><categories>cs.IT math.IT</categories><comments>28 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The optimum distance profiles of linear block codes were studied for
increasing or decreasing message length while keeping the minimum distances as
large as possible, especially for Golay codes and the second-order Reed-Muller
codes, etc. Cyclic codes have more efficient encoding and decoding algorithms.
In this paper, we investigate the optimum distance profiles with respect to the
cyclic subcode chains (ODPCs) of the punctured generalized second-order
Reed-Muller codes $\mathcal{GRM}(2,m)^*$ which were applied in Power Control in
OFDM Modulations in channels with synchronization, and so on. For this, two
standards are considered in the inverse dictionary order, i.e., for increasing
message length. Four lower bounds and upper bounds on ODPC are presented, where
the lower bounds almost achieve the corresponding upper bounds in some sense.
The discussions are over nonbinary prime field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0937</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0937</id><created>2013-07-03</created><authors><author><keyname>Ayadi</keyname><forenames>Mouhamed Gaith</forenames></author><author><keyname>Bouslimi</keyname><forenames>Riadh</forenames></author><author><keyname>Akaichi</keyname><forenames>Jalel</forenames></author></authors><title>Extending UML for Conceptual Modeling of Annotation of Medical Images</title><categories>cs.CV</categories><comments>9 pages, 6 figures. In International Journal of Computer
  Applications, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imaging has occupied a huge role in the management of patients, whether
hospitalized or not. Depending on the patients clinical problem, a variety of
imaging modalities were available for use. This gave birth of the annotation of
medical image process. The annotation is intended to image analysis and solve
the problem of semantic gap. The reason for image annotation is due to increase
in acquisition of images. Physicians and radiologists feel better while using
annotation techniques for faster remedy in surgery and medicine due to the
following reasons: giving details to the patients, searching the present and
past records from the larger databases, and giving solutions to them in a
faster and more accurate way. However, classical conceptual modeling does not
incorporate the specificity of medical domain specially the annotation of
medical image. The design phase is the most important activity in the
successful building of annotation process. For this reason, we focus in this
paper on presenting the conceptual modeling of the annotation of medical image
by defining a new profile using the StarUML extensibility mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0951</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0951</id><created>2013-07-03</created><authors><author><keyname>Arakistain</keyname><forenames>Ivan</forenames></author><author><keyname>Barrado</keyname><forenames>Mikel</forenames></author></authors><title>WoodTouch, a new interaction interface for wooden furniture</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If wood has been with us since time immemorial, being part of our
environment, housing and tools, now wood has gain momentum, as it is clear that
wood improves our life style. Because of the healthiness, resistance, ecology
and comfort, wood is important for all of us, no matter what our life style is.
WOODTOUCH Project aims to open a completely new market for furniture and
interior design companies, enabling touch interaction between the user and
wooden furniture surfaces. Why not switch on or dim the lights touching a
wooden table? Why not turn on the heating system? Why not use wood as a touch
sensitive surface for domotic control? The furniture designed with this novel
technology, offers a wooden outer image and has different touch sensitive areas
over the ones the user is able to control all sorts of electric appliances
touching over a wooden surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0952</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0952</id><created>2013-07-03</created><authors><author><keyname>Arakistain</keyname><forenames>Ivan</forenames></author><author><keyname>Abascal</keyname><forenames>Jose Miguel</forenames></author><author><keyname>Munne</keyname><forenames>Oriol</forenames></author></authors><title>Wireless sensor network technology for moisture monitoring of wood</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leaks represent a very important hazard for the buildings and they can affect
all sorts of building materials and specially wood due to its hygroscopic
properties. Excessive moisture content can affect in a negative way building
processes such as the installation of wooden floors or the use of wood as a
structural material. Moisture meters can provide prompt and non-destructive
determination of wood moisture, and as such are among the most useful tools
available to wood products manufacturers and scientists. However, a continuous
monitoring system is needed in order to avoid excessive moisture content which
can damage wooden floors as well as structural wood. Data and procedures are
presented in order to develop a suitable monitoring tool based on wireless
sensor networks to provide an electronic tool of active security both for the
installation of wooden floors and for the proper maintenance of existent
buildings which have a timber structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0957</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0957</id><created>2013-07-03</created><updated>2013-07-05</updated><authors><author><keyname>Pucci</keyname><forenames>L.</forenames></author><author><keyname>Gravino</keyname><forenames>P.</forenames></author><author><keyname>Servedio</keyname><forenames>V. D. P.</forenames></author></authors><title>Modeling the emergence of a new language: Naming Game with hybridization</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 10 figures, presented at IWSOS 2013 Palma de Mallorca, the
  final publication will be available at LNCS http://www.springer.com/lncs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent times, the research field of language dynamics has focused on the
investigation of language evolution, dividing the work in three evolutive
steps, according to the level of complexity: lexicon, categories and grammar.
The Naming Game is a simple model capable of accounting for the emergence of a
lexicon, intended as the set of words through which objects are named. We
introduce a stochastic modification of the Naming Game model with the aim of
characterizing the emergence of a new language as the result of the interaction
of agents. We fix the initial phase by splitting the population in two sets
speaking either language A or B. Whenever the result of the interaction of two
individuals results in an agent able to speak both A and B, we introduce a
finite probability that this state turns into a new idiom C, so to mimic a sort
of hybridization process. We study the system in the space of parameters
defining the interaction, and show that the proposed model displays a rich
variety of behaviours, despite the simple mean field topology of interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0962</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0962</id><created>2013-07-03</created><updated>2013-12-05</updated><authors><author><keyname>Manickam</keyname><forenames>Saravana</forenames></author><author><keyname>Marina</keyname><forenames>Mahesh K.</forenames></author><author><keyname>Pediaditaki</keyname><forenames>Sofia</forenames></author><author><keyname>Nekovee</keyname><forenames>Maziar</forenames></author></authors><title>Auctioning based Coordinated TV White Space Spectrum Sharing for Home
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of having the geolocation database monitor the secondary use of TV
white space (TVWS) spectrum and assist in coordinating the secondary usage is
gaining ground. Considering the home networking use case, we leverage the
geolocation database for interference-aware coordinated TVWS sharing among
secondary users (home networks) using {\em short-term auctions}, thereby
realize a dynamic secondary market. To enable this auctioning based coordinated
TVWS sharing framework, we propose an enhanced {\em market-driven TVWS spectrum
access model}. For the short-term auctions, we propose an online multi-unit,
iterative truthful mechanism called VERUM that takes into consideration
spatially heterogeneous spectrum availability, an inherent characteristic in
the TVWS context. We prove that VERUM is truthful (i.e., the best strategy for
every bidder is to bid based on its true valuation) and is also efficient in
that it allocates spectrum to users who value it the most. Evaluation results
from scenarios with real home distributions in urban and dense-urban
environments and using realistic TVWS spectrum availability maps show that
VERUM performs close to optimal allocation in terms of revenue for the
coordinating spectrum manager. Comparison with two existing efficient and
truthful multi-unit spectrum auction schemes, VERITAS and SATYA, shows that
VERUM fares better in terms of revenue, spectrum utilisation and percentage of
winning bidders in diverse conditions. Taking all of the above together, VERUM
can be seen to offer incentives to subscribed users encouraging them to use
TVWS spectrum through greater spectrum availability (as measured by percentage
of winning bidders) as well as to the coordinating spectrum manager through
revenue generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0966</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0966</id><created>2013-07-03</created><authors><author><keyname>Soria-Comas</keyname><forenames>Jordi</forenames></author></authors><title>Improving data utility in differential privacy and k-anonymity</title><categories>cs.CR cs.DB</categories><comments>Ph.D. Thesis defended on June 14, 2013, at the Department of Computer
  Engineering and Mathematics of Universitat Rovira i Virgili. Advisor: Josep
  Domingo-Ferrer</comments><acm-class>K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on two mainstream privacy models: k-anonymity and differential
privacy. Once a privacy model has been selected, the goal is to enforce it
while preserving as much data utility as possible. The main objective of this
thesis is to improve the data utility in k-anonymous and differentially private
data releases. k-Anonymity has several drawbacks. On the disclosure limitation
side, there is a lack of protection against attribute disclosure and against
informed intruders. On the data utility side, dealing with a large number of
quasi-identifier attributes is problematic. We propose a relaxation of
k-anonymity that deals with these issues.
  Differential privacy limits disclosure risk through noise addition. The
Laplace distribution is commonly used for the random noise. We show that the
Laplace distribution is not optimal: the same disclosure limitation guarantee
can be attained by adding less noise. Optimal univariate and multivariate
noises are characterized and constructed.
  Common mechanisms to attain differential privacy do not take into account the
users prior knowledge; they implicitly assume zero initial knowledge about the
query response. We propose a mechanism that focuses on limiting the knowledge
gain over the prior knowledge.
  Microaggregation-based k-anonymity and differential privacy can be combined
to produce microdata releases with the strong privacy guarantees of
differential privacy and improved data accuracy.
  The last contribution delves into the relation between t-closeness and
differential privacy. We see that for a specific distance and under some
reasonable assumptions on the intruders knowledge, t-closeness leads to
differential privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0974</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0974</id><created>2013-07-03</created><authors><author><keyname>Chia</keyname><forenames>Yeow-Khiang</forenames></author><author><keyname>Kittichokechai</keyname><forenames>Kittipong</forenames></author></authors><title>On Secure Source Coding with Side Information at the Encoder</title><categories>cs.IT math.IT</categories><comments>38 pages, 4 figures. Short version to be presented at ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a secure source coding problem with side information (S.I.) at
the decoder and the eavesdropper. The encoder has a source that it wishes to
describe with limited distortion through a rate limited link to a legitimate
decoder. The message sent is also observed by the eavesdropper. The encoder
aims to minimize both the distortion incurred by the legitimate decoder; and
the information leakage rate at the eavesdropper. When the encoder has access
to the uncoded S.I. at the decoder, we characterize the
rate-distortion-information leakage rate (R.D.I.) region under a Markov chain
assumption and when S.I. at the encoder does not improve the rate-distortion
region as compared to the case when S.I. is absent. When the decoder also has
access to the eavesdroppers S.I., we characterize the R.D.I. region without the
Markov Chain condition. We then consider a related setting where the encoder
and decoder obtain coded S.I. through a rate limited helper, and characterize
the R.D.I. region for several special cases, including special cases under
logarithmic loss distortion and for special cases of the Quadratic Gaussian
setting. Finally, we consider the amplification measures of list or entropy
constraint at the decoder, and show that the R.D.I. regions for the settings
considered in this paper under these amplification measures coincide with
R.D.I. regions under per symbol logarithmic loss distortion constraint at the
decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0991</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0991</id><created>2013-07-03</created><updated>2014-10-19</updated><authors><author><keyname>Behboodi</keyname><forenames>Arash</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author></authors><title>Mixed Noisy Network Coding and Cooperative Unicasting in Wireless
  Networks</title><categories>cs.IT math.IT</categories><comments>33 pages, 11 figures, To appear in IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of communicating a single message to a destination in presence of
multiple relay nodes, referred to as cooperative unicast network, is
considered. First, we introduce &quot;Mixed Noisy Network Coding&quot; (MNNC) scheme
which generalizes &quot;Noisy Network Coding&quot; (NNC) where relays are allowed to
decode-and-forward (DF) messages while all of them (without exception) transmit
noisy descriptions of their observations. These descriptions are exploited at
the destination and the DF relays aim to decode the transmitted messages while
creating full cooperation among the nodes. Moreover, the destination and the DF
relays can independently select the set of descriptions to be decoded or
treated as interference. This concept is further extended to multi-hopping
scenarios, referred to as &quot;Layered MNNC&quot; (LMNNC), where DF relays are organized
into disjoint groups representing one hop in the network. For cooperative
unicast additive white Gaussian noise (AWGN) networks we show that -provided DF
relays are properly chosen- MNNC improves over all previously established
constant gaps to the cut-set bound. Secondly, we consider the composite
cooperative unicast network where the channel parameters are randomly drawn
before communication starts and remain fixed during the transmission. Each draw
is assumed to be unknown at the source and fully known at the destination but
only partly known at the relays. We introduce through MNNC scheme the concept
of &quot;Selective Coding Strategy&quot; (SCS) that enables relays to decide dynamically
whether, in addition to communicate noisy descriptions, is possible to decode
and forward messages. It is demonstrated through slow-fading AWGN relay
networks that SCS clearly outperforms conventional coding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0995</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0995</id><created>2013-07-03</created><authors><author><keyname>Yoon</keyname><forenames>Ji Won</forenames></author></authors><title>An Efficient Model Selection for Gaussian Mixture Model in a Bayesian
  Framework</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to cluster or partition data, we often use
Expectation-and-Maximization (EM) or Variational approximation with a Gaussian
Mixture Model (GMM), which is a parametric probability density function
represented as a weighted sum of $\hat{K}$ Gaussian component densities.
However, model selection to find underlying $\hat{K}$ is one of the key
concerns in GMM clustering, since we can obtain the desired clusters only when
$\hat{K}$ is known. In this paper, we propose a new model selection algorithm
to explore $\hat{K}$ in a Bayesian framework. The proposed algorithm builds the
density of the model order which any information criterions such as AIC and BIC
basically fail to reconstruct. In addition, this algorithm reconstructs the
density quickly as compared to the time-consuming Monte Carlo simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.0998</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.0998</id><created>2013-07-03</created><updated>2014-06-30</updated><authors><author><keyname>Lu</keyname><forenames>F.</forenames></author><author><keyname>Chen</keyname><forenames>Z.</forenames></author></authors><title>A Unified Framework of Elementary Geometric Transformation
  Representation</title><categories>cs.CV</categories><comments>26 pages, 11 figures, 1 table, 21 referneces</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an extension of projective homology, stereohomology is proposed via an
extension of Desargues theorem and the extended Desargues configuration.
Geometric transformations such as reflection, translation, central symmetry,
central projection, parallel projection, shearing, central dilation, scaling,
and so on are all included in stereohomology and represented as
Householder-Chen elementary matrices. Hence all these geometric transformations
are called elementary. This makes it possible to represent these elementary
geometric transformations in homogeneous square matrices independent of a
particular choice of coordinate system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1019</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1019</id><created>2013-07-03</created><authors><author><keyname>Ralph</keyname><forenames>Paul</forenames></author></authors><title>Software Engineering Process Theory: A Multi-Method Comparison of
  Sensemaking-CoevoIution-Implementation Theory and Function-Behavior-Structure
  Theory</title><categories>cs.SE cs.HC</categories><comments>49 pages, 73 references, 12 tables, 4 figures, 4 appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many academics have called for increasing attention to theory in software
engineering. Consequently, this paper empirically evaluates two dissimilar
software development process theories - one expressing a more traditional,
methodical view (FBS) and one expressing an alternative, more improvisational
view (SCI). A primarily quantitative survey of more than 1300 software
developers is combined with four qualitative case studies to achieve a
simultaneously broad and deep empirical evaluation. Case data analysis using a
closed-ended, a priori coding scheme based on the two theories strongly
supports SCI, as does analysis of questionnaire response distributions
(p&lt;0.001; chi-square goodness of fit test). Furthermore, case-questionnaire
triangulation found no evidence that support for SCI varied by participants'
gender, education, experience, nationality or the size or nature of their
projects. This suggests that instead of iteration between weakly-coupled phases
(analysis, design, coding, testing), it is more accurate and useful to
conceptualize development as ad hoc oscillation between organizing perceptions
of the project context (Sensemaking), simultaneously improving mental pictures
of the context and design artifact (Coevolution) and constructing, debugging
and deploying software artifacts (Implementation).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1024</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1024</id><created>2013-07-02</created><authors><author><keyname>Herrouz</keyname><forenames>Abdelhakim</forenames></author><author><keyname>Khentout</keyname><forenames>Chabane</forenames></author><author><keyname>Djoudi</keyname><forenames>Mahieddine</forenames></author></authors><title>Overview of Web Content Mining Tools</title><categories>cs.IR</categories><comments>06 pages</comments><journal-ref>The International Journal of Engineering And Science (IJES),
  Vol.2, Issue 6, June 2013, pp. 106-110, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the Web has become one of the most widespread platforms for
information change and retrieval. As it becomes easier to publish documents, as
the number of users, and thus publishers, increases and as the number of
documents grows, searching for information is turning into a cumbersome and
time-consuming operation. Due to heterogeneity and unstructured nature of the
data available on the WWW, Web mining uses various data mining techniques to
discover useful knowledge from Web hyperlinks, page content and usage log. The
main uses of web content mining are to gather, categorize, organize and provide
the best possible information available on the Web to the user requesting the
information. The mining tools are imperative to scanning the many HTML
documents, images, and text. Then, the result is used by the search engines. In
this paper, we first introduce the concepts related to web mining; we then
present an overview of different Web Content Mining tools. We conclude by
presenting a comparative table of these tools based on some pertinent criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1028</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1028</id><created>2013-07-03</created><updated>2015-03-28</updated><authors><author><keyname>Quatrini</keyname><forenames>Myriam</forenames><affiliation>IML--FRE 3529 Aix-Marseille Universit&#xe9;</affiliation></author><author><keyname>Fouquer&#xe9;</keyname><forenames>Christophe</forenames><affiliation>Universit&#xe9; Paris 13, Sorbonne Paris Cit&#xe9;</affiliation></author></authors><title>Incarnation in Ludics and maximal cliques of paths</title><categories>cs.LO</categories><comments>33 pages, revised and corrected version</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (October
  16, 2013) lmcs:926</journal-ref><doi>10.2168/LMCS-9(4:6)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ludics is a reconstruction of logic with interaction as a primitive notion,
in the sense that the primary logical concepts are no more formulas and proofs
but cut-elimination interpreted as an interaction between objects called
designs. When the interaction between two designs goes well, such two designs
are said to be orthogonal. A behaviour is a set of designs closed under
bi-orthogonality. Logical formulas are then denoted by behaviours. Finally
proofs are interpreted as designs satisfying particular properties. In that
way, designs are more general than proofs and we may notice in particular that
they are not typed objects. Incarnation is introduced by Girard in Ludics as a
characterization of &quot;useful&quot; designs in a behaviour. The incarnation of a
design is defined as its subdesign that is the smallest one in the behaviour
ordered by inclusion. It is useful in particular because being &quot;incarnated&quot; is
one of the conditions for a design to denote a proof of a formula. The
computation of incarnation is important also as it gives a minimal denotation
for a formula, and more generally for a behaviour. We give here a constructive
way to capture the incarnation of the behaviour of a set of designs, without
computing the behaviour itself. The method we follow uses an alternative
definition of designs: rather than defining them as sets of chronicles, we
consider them as sets of paths, a concept very close to that of play in game
semantics that allows an easier handling of the interaction: the unfolding of
interaction is a path common to two interacting designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1051</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1051</id><created>2013-07-03</created><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Byzantine Convex Consensus: Preliminary Version</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of the past work on asynchronous approximate Byzantine consensus has
assumed scalar inputs at the nodes [3, 7]. Recent work has yielded approximate
Byzantine consensus algorithms for the case when the input at each node is a
d-dimensional vector, and the nodes must reach consensus on a vector in the
convex hull of the input vectors at the fault-free nodes [8, 12]. The
d-dimensional vectors can be equivalently viewed as points in the d-dimensional
Euclidean space. Thus, the algorithms in [8, 12] require the fault-free nodes
to decide on a point in the d-dimensional space.
  In this paper, we generalize the problem to allow the decision to be a convex
polytope in the d-dimensional space, such that the decided polytope is within
the convex hull of the input vectors at the fault-free nodes. We name this
problem as Byzantine convex consensus (BCC), and present an asynchronous
approximate BCC algorithm with optimal fault tolerance. Ideally, the goal here
is to agree on a convex polytope that is as large as possible. While we do not
claim that our algorithm satisfies this goal, we show a bound on the output
convex polytope chosen by our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1058</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1058</id><created>2013-07-03</created><updated>2014-07-19</updated><authors><author><keyname>Alekseyev</keyname><forenames>Max A.</forenames></author><author><keyname>Basova</keyname><forenames>Marina G.</forenames></author><author><keyname>Zolotykh</keyname><forenames>Nikolai Yu.</forenames></author></authors><title>On the minimal teaching sets of two-dimensional threshold functions</title><categories>math.CO cs.LG math.NT</categories><comments>11 pages, 4 figures</comments><journal-ref>SIAM J. Discrete Math. 29(1), pp. 157-165 (2015)</journal-ref><doi>10.1137/140978090</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that a minimal teaching set of any threshold function on the
twodimensional rectangular grid consists of 3 or 4 points. We derive exact
formulae for the numbers of functions corresponding to these values and further
refine them in the case of a minimal teaching set of size 3. We also prove that
the average cardinality of the minimal teaching sets of threshold functions is
asymptotically 7/2.
  We further present corollaries of these results concerning some special
arrangements of lines in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1061</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1061</id><created>2013-07-03</created><authors><author><keyname>Nilsson</keyname><forenames>John-Olof</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>Recursive Bayesian Initialization of Localization Based on Ranging and
  Dead Reckoning</title><categories>cs.RO cs.MA</categories><acm-class>I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The initialization of the state estimation in a localization scenario based
on ranging and dead reckoning is studied. Specifically, we start with a
cooperative localization setup and consider the problem of recursively arriving
at a uni-modal state estimate with sufficiently low covariance such that
covariance based filters can be used to estimate an agent's state subsequently.
A number of simplifications/assumptions are made such that the estimation
problem can be seen as that of estimating the initial agent state given a
deterministic surrounding and dead reckoning. This problem is solved by means
of a particle filter and it is described how continual states and covariance
estimates are derived from the solution. Finally, simulations are used to
illustrate the characteristics of the method and experimental data are briefly
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1070</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1070</id><created>2013-07-03</created><authors><author><keyname>Benatar</keyname><forenames>Naisan</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author></authors><title>A Comparison of Non-stationary, Type-2 and Dual Surface Fuzzy Control</title><categories>cs.AI cs.NE</categories><comments>2011 IEEE International Conference on Fuzzy Systems, pp 1193-1200</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Type-1 fuzzy logic has frequently been used in control systems. However this
method is sometimes shown to be too restrictive and unable to adapt in the
presence of uncertainty. In this paper we compare type-1 fuzzy control with
several other fuzzy approaches under a range of uncertain conditions. Interval
type-2 and non-stationary fuzzy controllers are compared, along with 'dual
surface' type-2 control, named due to utilising both the lower and upper values
produced from standard interval type-2 systems. We tune a type-1 controller,
then derive the membership functions and footprints of uncertainty from the
type-1 system and evaluate them using a simulated autonomous sailing problem
with varying amounts of environmental uncertainty. We show that while these
more sophisticated controllers can produce better performance than the type-1
controller, this is not guaranteed and that selection of Footprint of
Uncertainty (FOU) size has a large effect on this relative performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1073</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1073</id><created>2013-07-03</created><authors><author><keyname>Majid</keyname><forenames>Mazlina Abdul</forenames></author><author><keyname>Siebers</keyname><forenames>Peer-Olaf</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Modelling Reactive and Proactive Behaviour in Simulation: A Case Study
  in a University Organisation</title><categories>cs.CE</categories><comments>Gameon-Arabia, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation is a well established what-if scenario analysis tool in
Operational Research (OR). While traditionally Discrete Event Simulation (DES)
and System Dynamics Simulation (SDS) are the predominant simulation techniques
in OR, a new simulation technique, namely Agent-Based Simulation (ABS), has
emerged and is gaining more attention. In our research we focus on discrete
simulation methods (i.e. DES and ABS). The contribution made by this paper is
the comparison of DES and combined DES/ABS for modelling human reactive and
different level of detail of human proactive behaviour in service systems. The
results of our experiments show that the level of proactiveness considered in
the model has a big impact on the simulation output. However, there is not a
big difference between the results from the DES and the combined DES/ABS
simulation models. Therefore, for service systems of the type we investigated
we would suggest to use DES as the preferred analysis tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1078</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1078</id><created>2013-07-03</created><authors><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Feyereisl</keyname><forenames>Jan</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Gibson</keyname><forenames>Jack E.</forenames></author><author><keyname>Hubbard</keyname><forenames>Richard B.</forenames></author></authors><title>Investigating the Detection of Adverse Drug Events in a UK General
  Practice Electronic Health-Care Database</title><categories>cs.CE cs.LG</categories><comments>UKCI 2011, the 11th Annual Workshop on Computational Intelligence,
  Manchester, pp 167-173</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-mining techniques have frequently been developed for Spontaneous
reporting databases. These techniques aim to find adverse drug events
accurately and efficiently. Spontaneous reporting databases are prone to
missing information, under reporting and incorrect entries. This often results
in a detection lag or prevents the detection of some adverse drug events. These
limitations do not occur in electronic health-care databases. In this paper,
existing methods developed for spontaneous reporting databases are implemented
on both a spontaneous reporting database and a general practice electronic
health-care database and compared. The results suggests that the application of
existing methods to the general practice database may help find signals that
have gone undetected when using the spontaneous reporting system database. In
addition the general practice database provides far more supplementary
information, that if incorporated in analysis could provide a wealth of
information for identifying adverse events more accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1079</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1079</id><created>2013-07-03</created><authors><author><keyname>Dent</keyname><forenames>Ian</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Rodden</keyname><forenames>Tom</forenames></author></authors><title>Application of a clustering framework to UK domestic electricity data</title><categories>cs.CE cs.LG</categories><comments>UKCI 2011, the 11th Annual Workshop on Computational Intelligence,
  Manchester, pp 161-166</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper takes an approach to clustering domestic electricity load profiles
that has been successfully used with data from Portugal and applies it to UK
data. Clustering techniques are applied and it is found that the preferred
technique in the Portuguese work (a two stage process combining Self Organised
Maps and Kmeans) is not appropriate for the UK data. The work shows that up to
nine clusters of households can be identified with the differences in usage
profiles being visually striking. This demonstrates the appropriateness of
breaking the electricity usage patterns down to more detail than the two load
profiles currently published by the electricity industry. The paper details
initial results using data collected in Milton Keynes around 1990. Further work
is described and will concentrate on building accurate and meaningful clusters
of similar electricity users in order to better direct demand side management
initiatives to the most relevant target customers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1088</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1088</id><created>2013-07-03</created><authors><author><keyname>Nather</keyname><forenames>Mohammed F.</forenames></author><author><keyname>Saleem</keyname><forenames>Dr. Nada N.</forenames></author></authors><title>Suggest an Aspect-Oriented Design Approach for UML Communication Diagram</title><categories>cs.SE cs.PL</categories><comments>6 pager,7 figures,journal</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  More and more works are done on the design of the Unified Modeling Language
(UML) which is designed to help us for modeling effective object oriented
software, Existing Object-Oriented design methods are not mature enough to
capture non-functional requirement such as concurrency, fault tolerance,
distribution and persistence of a software approach. Our approach proposed to
use aspect-oriented software development (AOSD) mechanisms to solve the issues
for interactions of the communication diagram in UML that support only the
Object-Oriented mechanisms,thus AOSD allow to design programs that are out of
reach of strict Object-Orientation and could possibly improve the structures
and implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1101</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1101</id><created>2013-06-11</created><updated>2013-11-07</updated><authors><author><keyname>Liu</keyname><forenames>An</forenames></author><author><keyname>Lau</keyname><forenames>Vincent</forenames></author></authors><title>Mixed-Timescale Precoding and Cache Control in Cached MIMO Interference
  Network</title><categories>cs.IT math.IT</categories><comments>accepted by IEEE Transactions on Signal Processing. arXiv admin note:
  text overlap with arXiv:1306.2701</comments><journal-ref>Liu, A.; Lau, V.K.N., &quot;Mixed-Timescale Precoding and Cache Control
  in Cached MIMO Interference Network,&quot; IEEE Transactions on Signal Processing,
  vol.61, no.24, pp.6320,6332, Dec.15, 2013</journal-ref><doi>10.1109/TSP.2013.2282273</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider media streaming in MIMO interference networks whereby multiple base
stations (BS) simultaneously deliver media to their associated users using
fixed data rates. The performance is fundamentally limited by the cross-link
interference. We propose a cache-induced opportunistic cooperative MIMO (CoMP)
for interference mitigation. By caching a portion of the media files, the BSs
opportunistically employ CoMP to transform the cross-link interference into
spatial multiplexing gain. We study a mixed-timescale optimization of MIMO
precoding and cache control to minimize the transmit power under the rate
constraint. The cache control is to create more CoMP opportunities and is
adaptive to the long-term popularity of the media files. The precoding is to
guarantee the rate requirement and is adaptive to the channel state information
and cache state at the BSs. The joint stochastic optimization problem is
decomposed into a short-term precoding and a long-term cache control problem.
We propose a precoding algorithm which converges to a stationary point of the
short-term problem. Based on this, we exploit the hidden convexity of the
long-term problem and propose a low complexity and robust solution using
stochastic subgradient. The solution has significant gains over various
baselines and does not require explicit knowledge of the media popularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1144</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1144</id><created>2013-07-03</created><authors><author><keyname>Nabi</keyname><forenames>Zubair</forenames></author></authors><title>The Anatomy of Web Censorship in Pakistan</title><categories>cs.CY</categories><comments>To appear in: Proceedings of the 3rd USENIX Workshop on Free and Open
  Communications on the Internet (FOCI '13), Washington DC, August 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, the Internet has democratized the flow of information.
Unfortunately, in parallel, authoritarian regimes and other entities (such as
ISPs) for their vested interests have curtailed this flow by partially or fully
censoring the web. The policy, mechanism, and extent of this censorship varies
from country to country.
  We present the first study of the cause, effect, and mechanism of web
censorship in Pakistan. Specifically, we use a publicly available list of
blocked websites and check their accessibility from multiple networks within
the country. Our results indicate that the censorship mechanism varies across
websites: some are blocked at the DNS level while others at the HTTP level.
Interestingly, the government shifted to a centralized, Internet exchange level
censorship system during the course of our study, enabling our findings to
compare two generations of blocking systems. Furthermore, we report the outcome
of a controlled survey to ascertain the mechanisms that are being actively
employed by people to circumvent censorship. Finally, we discuss some simple
but surprisingly unexplored methods of bypassing restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1157</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1157</id><created>2013-07-02</created><authors><author><keyname>Zou</keyname><forenames>Yi Ming</forenames></author></authors><title>Representing Boolean Functions Using Polynomials: More Can Offer Less</title><categories>cs.CC math.CO math.NT</categories><comments>A shorter version of this article appeared in LNCS 6677, 2011</comments><journal-ref>LNCS 6677, 2011, pp. 290-296</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial threshold gates are basic processing units of an artificial neural
network. When the input vectors are binary vectors, these gates correspond to
Boolean functions and can be analyzed via their polynomial representations. In
practical applications, it is desirable to find a polynomial representation
with the smallest number of terms possible, in order to use the least possible
number of input lines to the unit under consideration. For this purpose,
instead of an exact polynomial representation, usually the sign representation
of a Boolean function is considered. The non-uniqueness of the sign
representation allows the possibility for using a smaller number of monomials
by solving a minimization problem. This minimization problem is combinatorial
in nature, and so far the best known deterministic algorithm claims the use of
at most $0.75\times 2^n$ of the $2^n$ total possible monomials. In this paper,
the basic methods of representing a Boolean function by polynomials are
examined, and an alternative approach to this problem is proposed. It is shown
that it is possible to use at most $0.5\times 2^n = 2^{n-1}$ monomials based on
the $\{0, 1\}$ binary inputs by introducing extra variables, and at the same
time keeping the degree upper bound at $n$. An algorithm for further reduction
of the number of terms that used in a polynomial representation is provided.
Examples show that in certain applications, the improvement achieved by the
proposed method over the existing methods is significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1166</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1166</id><created>2013-07-03</created><authors><author><keyname>Jassim</keyname><forenames>Firas A.</forenames></author></authors><title>A Novel Robust Method to Add Watermarks to Bitmap Images by Fading
  Technique</title><categories>cs.CV cs.MM</categories><comments>5 pages, 6 figures, 1 table</comments><journal-ref>International Journal of Multidisciplinary Sciences and
  Engineering, Vol. 4, No. 5, pp. 43-47, June 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Digital water marking is one of the essential fields in image security and
copyright protection. The proposed technique in this paper was based on the
principle of protecting images by hide an invisible watermark in the image. The
technique starts with merging the cover image and the watermark image with
suitable ratios, i.e., 99% from the cover image will be merged with 1% from the
watermark image. Technically, the fading process is irreversible but with the
proposed technique, the probability to reconstruct the original watermark image
is great. There is no perceptible difference between the original and
watermarked image by human eye. The experimental results show that the proposed
technique proven its ability to hide images that have the same size of the
cover image. Three performance measures were implemented to support the
proposed techniques which are MSE, PSNR, and SSIM. Fortunately, all the three
measures have excellent values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1170</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1170</id><created>2013-06-05</created><authors><author><keyname>Giuffrida</keyname><forenames>Giovanni</forenames></author><author><keyname>Zarba</keyname><forenames>Calogero G.</forenames></author></authors><title>A Formal Sociologic Study of Free Will</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We make a formal sociologic study of the concept of free will. By using the
language of mathematics and logic, we define what we call everlasting
societies. Everlasting societies never age: persons never age, and the goods of
the society are indestructible. The infinite history of an everlasting society
unfolds by following deterministic and probabilistic laws that do their best to
satisfy the free will of all the persons of the society.
  We define three possible kinds of histories for everlasting societies:
primitive histories, good histories, and golden histories. In primitive
histories, persons are inherently selfish, and they use their free will to
obtain the personal ownerships of all the goods of the society. In good
histories, persons are inherently good, and they use their free will to
distribute the goods of the society. In good histories, a person is not only
able to desire the personal ownership of goods, but is also able to desire that
a good be owned by another person. In golden histories, free will is bound by
the ethic of reciprocity, which states that &quot;you should wish upon others as you
would like others to wish upon yourself&quot;. In golden societies, the ethic of
reciprocity becomes a law that partially binds free will, and that must be
abided at all times. In other words, the verb &quot;should&quot; becomes the verb &quot;must&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1179</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1179</id><created>2013-07-03</created><authors><author><keyname>Trotman</keyname><forenames>Andrew</forenames></author><author><keyname>Zhang</keyname><forenames>Jinglan</forenames></author></authors><title>Future Web Growth and its Consequences for Web Search Architectures</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Introduction: Before embarking on the design of any computer system it is
first necessary to assess the magnitude of the problem. In the case of a web
search engine this assessment amounts to determining the current size of the
web, the growth rate of the web, and the quantity of computing resource
necessary to search it, and projecting the historical growth of this into the
future. Method: The over 20 year history of the web makes it possible to make
short-term projections on future growth. The longer history of hard disk drives
(and smart phone memory card) makes it possible to make short-term hardware
projections. Analysis: Historical data on Internet uptake and hardware growth
is extrapolated. Results: It is predicted that within a decade the storage
capacity of a single hard drive will exceed the size of the index of the web at
that time. Within another decade it will be possible to store the entire
searchable text on the same hard drive. Within another decade the entire
searchable web (including images) will also fit. Conclusion: This result raises
questions about the future architecture of search engines. Several new models
are proposed. In one model the user's computer is an active part of the
distributed search architecture. They search a pre-loaded snapshot (back-file)
of the web on their local device which frees up the online data centre for
searching just the difference between the snapshot and the current time.
Advantageously this also makes it possible to search when the user is
disconnected from the Internet. In another model all changes to all files are
broadcast to all users (forming a star-like network) and no data centre is
needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1192</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1192</id><created>2013-07-03</created><authors><author><keyname>Freund</keyname><forenames>Robert M.</forenames></author><author><keyname>Grigas</keyname><forenames>Paul</forenames></author><author><keyname>Mazumder</keyname><forenames>Rahul</forenames></author></authors><title>AdaBoost and Forward Stagewise Regression are First-Order Convex
  Optimization Methods</title><categories>stat.ML cs.LG math.OC</categories><msc-class>68Q32, 68T05, 62J05, 90C25</msc-class><acm-class>I.2.6; I.5.1; G.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boosting methods are highly popular and effective supervised learning methods
which combine weak learners into a single accurate model with good statistical
performance. In this paper, we analyze two well-known boosting methods,
AdaBoost and Incremental Forward Stagewise Regression (FS$_\varepsilon$), by
establishing their precise connections to the Mirror Descent algorithm, which
is a first-order method in convex optimization. As a consequence of these
connections we obtain novel computational guarantees for these boosting
methods. In particular, we characterize convergence bounds of AdaBoost, related
to both the margin and log-exponential loss function, for any step-size
sequence. Furthermore, this paper presents, for the first time, precise
computational complexity results for FS$_\varepsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1201</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1201</id><created>2013-07-04</created><updated>2013-10-19</updated><authors><author><keyname>Budney</keyname><forenames>Ryan</forenames></author><author><keyname>Sethares</keyname><forenames>William</forenames></author></authors><title>Topology of Musical Data</title><categories>math.AT cs.SD math.ST stat.TH</categories><comments>32 pages, 42 figures, v1-&gt;v2 reference updates, typos and many small
  touch-ups</comments><msc-class>55U10, 65D-18</msc-class><doi>10.1080/17459737.2013.850597</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The musical realm is a promising area in which to expect to find nontrivial
topological structures. This paper describes several kinds of metrics on
musical data, and explores the implications of these metrics in two ways: via
techniques of classical topology where the metric space of all-possible musical
data can be described explicitly, and via modern data-driven ideas of
persistent homology which calculates the Betti-number bar-codes of individual
musical works. Both analyses are able to recover three well known topological
structures in music: the circle of notes (octave-reduced scalar structures),
the circle of fifths, and the rhythmic repetition of timelines. Applications to
a variety of musical works (for example, folk music in the form of standard
MIDI files) are presented, and the bar codes show many interesting features.
Examples show that individual pieces may span the complete space (in which case
the classical and the data-driven analyses agree), or they may span only part
of the space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1204</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1204</id><created>2013-07-04</created><authors><author><keyname>Xu</keyname><forenames>Qin</forenames></author><author><keyname>Li</keyname><forenames>Fan</forenames></author><author><keyname>Sun</keyname><forenames>Jinsheng</forenames></author><author><keyname>Zukerman</keyname><forenames>Moshe</forenames></author></authors><title>A New TCP/AQM System Analysis</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MGT fluid model has been used extensively to guide designs of AQM schemes
aiming to alleviate adverse effects of Internet congestion. In this paper, we
provide a new analysis of a TCP/AQM system that aims to improve the accuracy of
the MGT fluid model especially in heavy traffic conditions. The analysis is
based on the consideration of two extreme congestion scenarios that leads to
the derivation of upper and lower bounds for the queue length and marking
probability dynamics and showing that they approach each other in steady state.
Both discrete and continuous time models are provided. Simulation results
demonstrate that the new model achieves a significantly higher level of
accuracy than a simplified version of the MGT fluid model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1212</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1212</id><created>2013-07-04</created><authors><author><keyname>Nasri</keyname><forenames>Ridha</forenames></author><author><keyname>Altman</keyname><forenames>Zwi</forenames></author></authors><title>Handover adaptation for dynamic load balancing in 3gpp long term
  evolution systems</title><categories>cs.NI cs.RO</categories><comments>numero de Pages dans le proceeding 145-154</comments><proxy>ccsd</proxy><journal-ref>5th International Conference on Advances in Mobile Computing \&amp;
  Multimedia (MoMM2007), Jakarta : Indon\'esie (2007)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The long-Term Evolution (LTE) of the 3GPP (3rd Generation Partnership
Project) radio access network is in early stage of specification. Self-tuning
and self-optimisation algorithms are currently studied with the aim of
enriching the LTE standard. This paper investigates auto-tuning of LTE mobility
algorithm. The auto-tuning is carried out by adapting handover parameters of
each base station according to its radio load and the load of its adjacent
cells. The auto-tuning alleviates cell congestion and balances the traffic and
the load between cells by handing off mobiles close to the cell border from the
congested cell to its neighbouring cells. Simulation results show that the
auto-tuning process brings an important gain in both call admission rate and
user throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1215</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1215</id><created>2013-07-04</created><authors><author><keyname>Tapie</keyname><forenames>Laurent</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Mawussi</keyname><forenames>Bernardin</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Rubio</keyname><forenames>Walter</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Machining of complex-shaped parts with guidance curves</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>International Journal of Advanced Manufacturing Technology (2013)
  1-11</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, high-speed machining is usually used for production of hardened
material parts with complex shapes such as dies and molds. In such parts, tool
paths generated for bottom machining feature with the conventional parallel
plane strategy induced many feed rate reductions, especially when boundaries of
the feature have a lot of curvatures and are not parallel. Several machining
experiments on hardened material lead to the conclusion that a tool path
implying stable cutting conditions might guarantee a better part surface
integrity. To ensure this stability, the shape machined must be decomposed when
conventional strategies are not suitable. In this paper, an experimental
approach based on high-speed performance simulation is conducted on a master
bottom machining feature in order to highlight the influence of the curvatures
towards a suitable decomposition of machining area. The decomposition is
achieved through the construction of intermediate curves between the closed
boundaries of the feature. These intermediate curves are used as guidance curve
for the tool paths generation with an alternative machining strategy called
&quot;guidance curve strategy&quot;. For the construction of intermediate curves, key
parameters reflecting the influence of their proximity with each closed
boundary and the influence of the curvatures of this latter are introduced.
Based on the results, a method for defining guidance curves in four steps is
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1217</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1217</id><created>2013-07-04</created><authors><author><keyname>Olivier</keyname><forenames>Pierre</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Boukhobza</keyname><forenames>Jalil</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Senn</keyname><forenames>Eric</forenames><affiliation>Lab-STICC</affiliation></author></authors><title>Toward a Unified Performance and Power Consumption NAND Flash Memory
  Model of Embedded and Solid State Secondary Storage Systems</title><categories>cs.PF</categories><proxy>ccsd</proxy><journal-ref>GDR Soc-Sip 2013 Meeting, Lyon : France (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a set of models dedicated to describe a flash storage
subsystem structure, functions, performance and power consumption behaviors.
These models cover a large range of today's NAND flash memory applications.
They are designed to be implemented in simulation tools allowing to estimate
and compare performance and power consumption of I/O requests on flash memory
based storage systems. Such tools can also help in designing and validating new
flash storage systems and management mechanisms. This work is integrated in a
global project aiming to build a framework simulating complex flash storage
hierarchies for performance and power consumption analysis. This tool will be
highly configurable and modular with various levels of usage complexity
according to the required aim: from a software user point of view for
simulating storage systems, to a developer point of view for designing, testing
and validating new flash storage management systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1251</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1251</id><created>2013-07-04</created><authors><author><keyname>Bhattacharjee</keyname><forenames>Sukanta</forenames></author><author><keyname>Banerjee</keyname><forenames>Ansuman</forenames></author><author><keyname>Ho</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Chakrabarty</keyname><forenames>Krishnendu</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Bhargab B.</forenames></author></authors><title>Algorithms for Producing Linear Dilution Gradient with Digital
  Microfluidics</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital microfluidic (DMF) biochips are now being extensively used to
automate several biochemical laboratory protocols such as clinical analysis,
point-of-care diagnostics, and polymerase chain reaction (PCR). In many
biological assays, e.g., in bacterial susceptibility tests, samples and
reagents are required in multiple concentration (or dilution) factors,
satisfying certain &quot;gradient&quot; patterns such as linear, exponential, or
parabolic. Dilution gradients are usually prepared with continuous-flow
microfluidic devices; however, they suffer from inflexibility,
non-programmability, and from large requirement of costly stock solutions. DMF
biochips, on the other hand, are shown to produce, more efficiently, a set of
random dilution factors. However, all existing algorithms fail to optimize the
cost or performance when a certain gradient pattern is required. In this work,
we present an algorithm to generate any arbitrary linear gradient, on-chip,
with minimum wastage, while satisfying a required accuracy in the concentration
factor. We present new theoretical results on the number of mix-split
operations and waste computation, and prove an upper bound on the storage
requirement. The corresponding layout design of the biochip is also proposed.
Simulation results on different linear gradients show a significant improvement
in sample cost over three earlier algorithms used for the generation of
multiple concentrations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1252</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1252</id><created>2013-07-04</created><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Yu</keyname><forenames>Lan</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Elkind</keyname><forenames>Edith</forenames></author></authors><title>The Complexity of Fully Proportional Representation for Single-Crossing
  Electorates</title><categories>cs.GT cs.MA</categories><comments>23 pages</comments><msc-class>68Q17</msc-class><acm-class>I.2.11; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of winner determination in single-crossing elections
under two classic fully proportional representation
rules---Chamberlin--Courant's rule and Monroe's rule. Winner determination for
these rules is known to be NP-hard for unrestricted preferences. We show that
for single-crossing preferences this problem admits a polynomial-time algorithm
for Chamberlin--Courant's rule, but remains NP-hard for Monroe's rule. Our
algorithm for Chamberlin--Courant's rule can be modified to work for elections
with bounded single-crossing width. To circumvent the hardness result for
Monroe's rule, we consider single-crossing elections that satisfy an additional
constraint, namely, ones where each candidate is ranked first by at least one
voter (such elections are called narcissistic). For single-crossing
narcissistic elections, we provide an efficient algorithm for the egalitarian
version of Monroe's rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1253</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1253</id><created>2013-07-04</created><updated>2014-04-08</updated><authors><author><keyname>Min</keyname><forenames>Byungjoon</forenames></author><author><keyname>Yi</keyname><forenames>Su Do</forenames></author><author><keyname>Lee</keyname><forenames>Kyu-Min</forenames></author><author><keyname>Goh</keyname><forenames>K. -I.</forenames></author></authors><title>Network robustness of multiplex networks with interlayer degree
  correlations</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>9 pages, 9 figures, accepted for publication in Phys. Rev. E</comments><journal-ref>Phys. Rev. E 89, 042811 (2014)</journal-ref><doi>10.1103/PhysRevE.89.042811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the robustness properties of multiplex networks consisting of
multiple layers of distinct types of links, focusing on the role of
correlations between degrees of a node in different layers. We use generating
function formalism to address various notions of the network robustness
relevant to multiplex networks such as the resilience of ordinary- and mutual
connectivity under random or targeted node removals as well as the
biconnectivity. We found that correlated coupling can affect the structural
robustness of multiplex networks in diverse fashion. For example, for
maximally-correlated duplex networks, all pairs of nodes in the giant component
are connected via at least two independent paths and network structure is
highly resilient to random failure. In contrast, anti-correlated duplex
networks are on one hand robust against targeted attack on high-degree nodes,
but on the other hand they can be vulnerable to random failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1270</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1270</id><created>2013-07-04</created><authors><author><keyname>Ammendola</keyname><forenames>Roberto</forenames></author><author><keyname>Biagioni</keyname><forenames>Andrea</forenames></author><author><keyname>Frezza</keyname><forenames>Ottorino</forenames></author><author><keyname>Geurts</keyname><forenames>Werner</forenames></author><author><keyname>Goossens</keyname><forenames>Gert</forenames></author><author><keyname>Cicero</keyname><forenames>Francesca Lo</forenames></author><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Paolucci</keyname><forenames>Pier Stanislao</forenames></author><author><keyname>Rossetti</keyname><forenames>Davide</forenames></author><author><keyname>Simula</keyname><forenames>Francesco</forenames></author><author><keyname>Tosoratto</keyname><forenames>Laura</forenames></author><author><keyname>Vicini</keyname><forenames>Piero</forenames></author></authors><title>A heterogeneous many-core platform for experiments on scalable custom
  interconnects and management of fault and critical events, applied to
  many-process applications: Vol. II, 2012 technical report</title><categories>cs.DC</categories><comments>119 pages</comments><msc-class>68M10, 68M14, 68M15</msc-class><acm-class>B.8.1; C.1.4; C.3; C.4; C.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the second of a planned collection of four yearly volumes describing
the deployment of a heterogeneous many-core platform for experiments on
scalable custom interconnects and management of fault and critical events,
applied to many-process applications. This volume covers several topics, among
which: 1- a system for awareness of faults and critical events (named LO|FA|MO)
on experimental heterogeneous many-core hardware platforms; 2- the integration
and test of the experimental hardware heterogeneous many-core platform QUoNG,
based on the APEnet+ custom interconnect; 3- the design of a
Software-Programmable Distributed Network Processor architecture (DNP) using
ASIP technology; 4- the initial stages of design of a new DNP generation onto a
28nm FPGA. These developments were performed in the framework of the EURETILE
European Project under the Grant Agreement no. 247846.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1271</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1271</id><created>2013-07-04</created><authors><author><keyname>Gimenez-Toledo</keyname><forenames>Elea</forenames></author><author><keyname>Manana-Rodriguez</keyname><forenames>Jorge</forenames></author><author><keyname>Delgado-Lopez-Cozar</keyname><forenames>Emilio</forenames></author></authors><title>Quality indicators for scientific journals based on experts opinion</title><categories>cs.DL stat.AP</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the results and further development of a survey sent to
11,799 Spanish faculty members and researchers from various fields of the
social sciences and the humanities, obtaining a total of 45.6% (5,368
responses) usable answers. Respondents were asked (a) to indicate the three
most important journals in their field and (b) to rate them on a 0-10 scale
according to their quality. The information obtained has been synthesized in
two indicators which reflect the perceived quality of journals. Once the values
were obtained, the journals were categorized according to each indicator and
the ordinal positions were compared. Different profiles of journals are
analyzed in connection with experts opinion, such as regional orientation, and
the consensus among researchers is studied. Finally, the possibilities of
extending the research and indicators to sets of international journals are
explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1275</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1275</id><created>2013-07-04</created><authors><author><keyname>Feng</keyname><forenames>Fangxiang</forenames></author><author><keyname>Li</keyname><forenames>Ruifan</forenames></author><author><keyname>Wang</keyname><forenames>Xiaojie</forenames></author></authors><title>Constructing Hierarchical Image-tags Bimodal Representations for Word
  Tags Alternative Choice</title><categories>cs.LG cs.NE</categories><comments>6 pages, 1 figure, Presented at the Workshop on Representation
  Learning, ICML 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes our solution to the multi-modal learning challenge of
ICML. This solution comprises constructing three-level representations in three
consecutive stages and choosing correct tag words with a data-specific
strategy. Firstly, we use typical methods to obtain level-1 representations.
Each image is represented using MPEG-7 and gist descriptors with additional
features released by the contest organizers. And the corresponding word tags
are represented by bag-of-words model with a dictionary of 4000 words.
Secondly, we learn the level-2 representations using two stacked RBMs for each
modality. Thirdly, we propose a bimodal auto-encoder to learn the
similarities/dissimilarities between the pairwise image-tags as level-3
representations. Finally, during the test phase, based on one observation of
the dataset, we come up with a data-specific strategy to choose the correct tag
words leading to a leap of an improved overall performance. Our final average
accuracy on the private test set is 100%, which ranks the first place in this
challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1277</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1277</id><created>2013-07-04</created><authors><author><keyname>van Benthem</keyname><forenames>Johan</forenames></author><author><keyname>Fern&#xe1;ndez-Duque</keyname><forenames>David</forenames></author><author><keyname>Pacuit</keyname><forenames>Eric</forenames></author></authors><title>Evidence and plausibility in neighborhood structures</title><categories>math.LO cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intuitive notion of evidence has both semantic and syntactic features. In
this paper, we develop an {\em evidence logic} for epistemic agents faced with
possibly contradictory evidence from different sources. The logic is based on a
neighborhood semantics, where a neighborhood $N$ indicates that the agent has
reason to believe that the true state of the world lies in $N$. Further notions
of relative plausibility between worlds and beliefs based on the latter
ordering are then defined in terms of this evidence structure, yielding our
intended models for evidence-based beliefs. In addition, we also consider a
second more general flavor, where belief and plausibility are modeled using
additional primitive relations, and we prove a representation theorem showing
that each such general model is a $p$-morphic image of an intended one. This
semantics invites a number of natural special cases, depending on how uniform
we make the evidence sets, and how coherent their total structure. We give a
structural study of the resulting `uniform' and `flat' models. Our main result
are sound and complete axiomatizations for the logics of all four major model
classes with respect to the modal language of evidence, belief and safe belief.
We conclude with an outlook toward logics for the dynamics of changing
evidence, and the resulting language extensions and connections with logics of
plausibility change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1289</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1289</id><created>2013-07-04</created><authors><author><keyname>Veganzones</keyname><forenames>Miguel Angel</forenames><affiliation>GIPSA</affiliation></author><author><keyname>Datcu</keyname><forenames>Mihai</forenames><affiliation>DLR</affiliation></author><author><keyname>Gra&#xf1;a</keyname><forenames>Manuel</forenames><affiliation>GIC</affiliation></author></authors><title>Further results on dissimilarity spaces for hyperspectral images RF-CBIR</title><categories>cs.IR cs.CV</categories><comments>In Pattern Recognition Letters (2013)</comments><proxy>ccsd</proxy><report-no>veganzones_PRL2013</report-no><journal-ref>Pattern Recognition Letters 34, 14 (2013) 1659-1668</journal-ref><doi>10.1016/j.patrec.2013.05.025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-Based Image Retrieval (CBIR) systems are powerful search tools in
image databases that have been little applied to hyperspectral images.
Relevance feedback (RF) is an iterative process that uses machine learning
techniques and user's feedback to improve the CBIR systems performance. We
pursued to expand previous research in hyperspectral CBIR systems built on
dissimilarity functions defined either on spectral and spatial features
extracted by spectral unmixing techniques, or on dictionaries extracted by
dictionary-based compressors. These dissimilarity functions were not suitable
for direct application in common machine learning techniques. We propose to use
a RF general approach based on dissimilarity spaces which is more appropriate
for the application of machine learning algorithms to the hyperspectral
RF-CBIR. We validate the proposed RF method for hyperspectral CBIR systems over
a real hyperspectral dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1303</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1303</id><created>2013-07-02</created><authors><author><keyname>Parag</keyname><forenames>Toufiq</forenames><affiliation>Janelia Farm Research Campus-HHMI</affiliation></author></authors><title>Submodularity of a Set Label Disagreement Function</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set label disagreement function is defined over the number of variables
that deviates from the dominant label. The dominant label is the value assumed
by the largest number of variables within a set of binary variables. The
submodularity of a certain family of set label disagreement function is
discussed in this manuscript. Such disagreement function could be utilized as a
cost function in combinatorial optimization approaches for problems defined
over hypergraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1307</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1307</id><created>2013-07-04</created><authors><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>Leistedt</keyname><forenames>B.</forenames></author></authors><title>Fourier-Laguerre transform, convolution and wavelets on the ball</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>4 pages, 2 figures, Proceedings of 10th International Conference on
  Sampling Theory and Applications (SampTA), Codes are publicly available at
  http://www.s2let.org and http://www.flaglets.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the Fourier-Laguerre transform, an alternative harmonic analysis on
the three-dimensional ball to the usual Fourier-Bessel transform. The
Fourier-Laguerre transform exhibits an exact quadrature rule and thus leads to
a sampling theorem on the ball. We study the definition of convolution on the
ball in this context, showing explicitly how translation on the radial line may
be viewed as convolution with a shifted Dirac delta function. We review the
exact Fourier-Laguerre wavelet transform on the ball, coined flaglets, and show
that flaglets constitute a tight frame.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1330</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1330</id><created>2013-07-04</created><updated>2013-07-29</updated><authors><author><keyname>Lozano</keyname><forenames>George A.</forenames></author></authors><title>The elephant in the room: multi-authorship and the assessment of
  individual researchers</title><categories>physics.soc-ph cs.DL</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a group of individuals creates something, credit is usually divided
among them. Oddly, that does not apply to scientific papers. The most commonly
used performance measure for individual researchers is the h-index, which does
not correct for multi-authorship. Each author claims full credit for each paper
and each ensuing citation. This mismeasure of achievement is fuelling a
flagrant increase in multi-authorship. Several alternatives to the h-index have
been devised, and one of them, the individual h-index (hI), is logical,
intuitive and easily calculated. Correcting for multi-authorship would end
gratuitous authorship and allow proper attribution and unbiased comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1332</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1332</id><created>2013-07-04</created><updated>2013-07-09</updated><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Byzantine Convex Consensus: An Optimal Algorithm</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.1051</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of the past work on asynchronous approximate Byzantine consensus has
assumed scalar inputs at the nodes [4, 8]. Recent work has yielded approximate
Byzantine consensus algorithms for the case when the input at each node is a
d-dimensional vector, and the nodes must reach consensus on a vector in the
convex hull of the input vectors at the fault-free nodes [9, 13]. The
d-dimensional vectors can be equivalently viewed as points in the d-dimensional
Euclidean space. Thus, the algorithms in [9, 13] require the fault-free nodes
to decide on a point in the d-dimensional space.
  In our recent work [arXiv:/1307.1051], we proposed a generalization of the
consensus problem, namely Byzantine convex consensus (BCC), which allows the
decision to be a convex polytope in the d-dimensional space, such that the
decided polytope is within the convex hull of the input vectors at the
fault-free nodes. We also presented an asynchronous approximate BCC algorithm.
  In this paper, we propose a new BCC algorithm with optimal fault-tolerance
that also agrees on a convex polytope that is as large as possible under
adversarial conditions. Our prior work [arXiv:/1307.1051] does not guarantee
the optimality of the output polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1335</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1335</id><created>2013-07-04</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author></authors><title>Investigating independent subsets of graphs, with Mathematica</title><categories>cs.MS cs.DM math.CO</categories><journal-ref>Mathematica Italia User Group Meeting (UGM) 2013, ISBN
  9788896810033. (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With this work we aim to show how Mathematica can be a useful tool to
investigate properties of combinatorial structures. Specifically, we will face
enumeration problems on independent subsets of powers of paths and cycles,
trying to highlight the correspondence with other combinatorial objects with
the same cardinality. Then we will study the structures obtained by ordering
properly independent subsets of paths and cycles. We will approach some
enumeration problems on the resulting partially ordered sets, putting in
evidence the correspondences with structures known as Fibonacci and Lucas
Cubes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1343</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1343</id><created>2013-07-04</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author><author><keyname>Filaretti</keyname><forenames>Daniele</forenames></author></authors><title>Building Bricks with Bricks, with Mathematica</title><categories>cs.MS cs.DM</categories><journal-ref>Mathematica Italia User Group Meeting (UGM) 2011, ISBN
  9788896810026. (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we solve a special case of the problem of building an
n-dimensional parallelepiped using a given set of n-dimensional
parallelepipeds. Consider the identity x^3 = x(x-1)(x-2)+3x(x-1+x). For
sufficiently large x, we associate with x^3 a cube with edges of size x, with
x(x-1)(x-2) a parallelepiped with edges x, x-1, x-2, with 3x(x-1+x) three
parallelepipeds of edges x, x-1, 1, and with x a parallelepiped of edges x, 1,
1. The problem we takle is the actual construction of the cube using the given
parallelepipeds. In [DDNP90] it was shown how to solve this specific problem
and all similar instances in which a (monic) polynomial is expressed as a
linear combination of a persistent basis. That is to say a sequence of
polynomials q_0 = 1, and q_k(x) = q_{k-1}(x)(x-r_k) for k &gt; 0. Here, after
[Fil10], we deal with a multivariate version of the problem with respect to a
basis of polynomials of the same degree (binomial basis). We show that it is
possible to build the parallelepiped associated with a multivariate polynomial
P(x_1, ..., x_n)=(x_1- s_1)...(x_n-s_n) with integer roots, using the
parallelepipeds described by the elements of the basis. We provide an algorithm
in Mathematica to solve the problem for each n. Moreover, for n = 2, 3, 4 (in
the latter case, only when a projection is possible) we use Mathematica to
display a step by step construction of the parallelepiped P(x1,...,x_n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1348</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1348</id><created>2013-07-04</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author><author><keyname>Marigo</keyname><forenames>Francesco</forenames></author><author><keyname>Monti</keyname><forenames>Corrado</forenames></author></authors><title>Making simple proofs simpler</title><categories>cs.MS cs.DM</categories><journal-ref>Mathematica Italia User Group Meeting (UGM) 2013, ISBN
  9788896810033. (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open partition \pi{} [Cod09a, Cod09b] of a tree T is a partition of the
vertices of T with the property that, for each block B of \pi, the upset of B
is a union of blocks of \pi. This paper deals with the number, NP(n), of open
partitions of the tree, V_n, made of two chains with n points each, that share
the root.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1352</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1352</id><created>2013-07-04</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author></authors><title>A Mathematica package to cope with partially ordered sets</title><categories>cs.MS cs.DM</categories><journal-ref>Mathematica Italia User Group Meeting (UGM) 2010, ISBN
  9788896810002. (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematica offers, by way of the package Combinatorics, many useful
functions to work on graphs and ordered structures, but none of these functions
was specific enough to meet the needs of our research group. Moreover, the
existing functions are not always helpful when one has to work on new concepts.
  In this paper we present a package of features developed in Mathematica which
we consider particularly useful for the study of certain categories of
partially ordered sets. Among the features offered, the package includes: (1)
some basic features to treat partially ordered sets; (2) the ability to
enumerate, create, and display monotone and regular partitions of partially
ordered sets; (3) the capability of constructing the lattices of partitions of
a poset, and of doing some useful computations on these structures; (4) the
possibility of computing products and coproducts in the category of partially
ordered sets and monotone maps; (5) the possibility of computing products and
coproducts in the category of forests (disjoint union of trees) and open maps
(cf. [DM06] for the product between forests).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1353</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1353</id><created>2013-07-04</created><updated>2016-03-01</updated><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Moritz</forenames></author></authors><title>One Hierarchy Spawns Another: Graph Deconstructions and the Complexity
  Classification of Conjunctive Queries</title><categories>cs.CC</categories><msc-class>03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of conjunctive query evaluation relative to a class of
queries; this problem is formulated here as the relational homomorphism problem
relative to a class of structures A, wherein each instance must be a pair of
structures such that the first structure is an element of A. We present a
comprehensive complexity classification of these problems, which strongly links
graph-theoretic properties of A to the complexity of the corresponding
homomorphism problem. In particular, we define a binary relation on graph
classes, which is a preorder, and completely describe the resulting hierarchy
given by this relation. This relation is defined in terms of a notion which we
call graph deconstruction and which is a variant of the well-known notion of
tree decomposition. We then use this hierarchy of graph classes to infer a
complexity hierarchy of homomorphism problems which is comprehensive up to a
computationally very weak notion of reduction, namely, a parameterized version
of quantifier-free first-order reduction. In doing so, we obtain a
significantly refined complexity classification of homomorphism problems, as
well as a unifying, modular, and conceptually clean treatment of existing
complexity classifications. We then present and develop the theory of
Ehrenfeucht-Fraisse-style pebble games which solve the homomorphism problems
where the cores of the structures in A have bounded tree depth. Finally, we use
our framework to classify the complexity of model checking existential
sentences having bounded quantifier rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1354</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1354</id><created>2013-07-04</created><updated>2014-01-27</updated><authors><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author></authors><title>Modeling and Predicting the Growth and Death of Membership-based
  Websites</title><categories>physics.soc-ph cs.SI</categories><comments>Changed title to &quot;Modeling and Predicting the Growth and Death of
  Membership-based Websites&quot;. WWW Conference, 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Driven by outstanding success stories of Internet startups such as Facebook
and The Huffington Post, recent studies have thoroughly described their growth.
These highly visible online success stories, however, overshadow an untold
number of similar ventures that fail. The study of website popularity is
ultimately incomplete without general mechanisms that can describe both
successes and failures. In this work we present six years of the daily number
of users (DAU) of twenty-two membership-based websites - encompassing online
social networks, grassroots movements, online forums, and membership-only
Internet stores - well balanced between successes and failures. We then propose
a combination of reaction-diffusion-decay processes whose resulting equations
seem not only to describe well the observed DAU time series but also provide
means to roughly predict their evolution. This model allows an approximate
automatic DAU-based classification of websites into self-sustainable v.s.
unsustainable and whether the startup growth is mostly driven by marketing &amp;
media campaigns or word-of-mouth adoptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1360</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1360</id><created>2013-07-04</created><authors><author><keyname>Carrillo</keyname><forenames>Rafael E.</forenames></author><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>On sparsity averaging</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>4 pages, 3 figures, Proceedings of 10th International Conference on
  Sampling Theory and Applications (SampTA), Code available at
  https://github.com/basp-group/sopt, Full journal letter available at
  http://arxiv.org/abs/arXiv:1208.2330</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in Carrillo et al. (2012) and Carrillo et al. (2013)
introduced a novel regularization method for compressive imaging in the context
of compressed sensing with coherent redundant dictionaries. The approach relies
on the observation that natural images exhibit strong average sparsity over
multiple coherent frames. The associated reconstruction algorithm, based on an
analysis prior and a reweighted $\ell_1$ scheme, is dubbed Sparsity Averaging
Reweighted Analysis (SARA). We review these advances and extend associated
simulations establishing the superiority of SARA to regularization methods
based on sparsity in a single frame, for a generic spread spectrum acquisition
and for a Fourier acquisition of particular interest in radio astronomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1370</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1370</id><created>2013-07-04</created><updated>2013-07-05</updated><authors><author><keyname>Sweeney</keyname><forenames>Latanya</forenames></author></authors><title>Matching Known Patients to Health Records in Washington State Data</title><categories>cs.CY cs.DB</categories><comments>13 pages</comments><acm-class>K.4; K.4.1; K.5; K.5.2; K.6.5; E.0; H.2.0; H.2.7; H.3.5; J.1; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The State of Washington sells patient-level health data for $50. This
publicly available dataset has virtually all hospitalizations occurring in the
State in a given year, including patient demographics, diagnoses, procedures,
attending physician, hospital, a summary of charges, and how the bill was paid.
It does not contain patient names or addresses (only ZIPs). Newspaper stories
printed in the State for the same year that contain the word &quot;hospitalized&quot;
often include a patient's name and residential information and explain why the
person was hospitalized, such as vehicle accident or assault. News information
uniquely and exactly matched medical records in the State database for 35 of
the 81 cases (or 43 percent) found in 2011, thereby putting names to patient
records. A news reporter verified matches by contacting patients. Employers,
financial organizations and others know the same kind of information as
reported in news stories making it just as easy for them to identify the
medical records of employees, debtors, and others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1371</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1371</id><created>2013-07-04</created><authors><author><keyname>Evako</keyname><forenames>Alexander</forenames></author></authors><title>On digital simply connected spaces and manifolds: a digital simply
  connected 3-manifold is the digital 3-sphere</title><categories>cs.DM math.CO</categories><comments>Key words: Graph; Dimension; Digital manifold; Simply connected
  space; Sphere</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of digital topology, we study structural and topological
properties of digital n-dimensional manifolds. We introduce the notion of
simple connectedness of a digital space and prove that if M and N are homotopy
equivalent digital spaces and M is simply connected, then so is N. We show that
a simply connected digital 2-manifold is the digital 2-sphere and a simply
connected digital 3-manifold is the digital 3-sphere. This property can be
considered as a digital form of the Poincar\'e conjecture for continuous
three-manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1372</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1372</id><created>2013-07-04</created><updated>2013-08-19</updated><authors><author><keyname>Kumar</keyname><forenames>G. Kishore</forenames></author><author><keyname>Jayaraman</keyname><forenames>V. K.</forenames></author></authors><title>Clustering of Complex Networks and Community Detection Using Group
  Search Optimization</title><categories>cs.NE cs.DS</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group Search Optimizer(GSO) is one of the best algorithms, is very new in the
field of Evolutionary Computing. It is very robust and efficient algorithm,
which is inspired by animal searching behaviour. The paper describes an
application of GSO to clustering of networks. We have tested GSO against five
standard benchmark datasets, GSO algorithm is proved very competitive in terms
of accuracy and convergence speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1380</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1380</id><created>2013-07-04</created><authors><author><keyname>Dent</keyname><forenames>Ian</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Rodden</keyname><forenames>Tom</forenames></author></authors><title>The Application of a Data Mining Framework to Energy Usage Profiling in
  Domestic Residences using UK data</title><categories>cs.CE cs.LG stat.AP</categories><comments>Buildings Do Not Use Energy, People Do Research Student Conference,
  Bath, UK, 2011. arXiv admin note: text overlap with arXiv:1307.1079</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a method for defining representative load profiles for
domestic electricity users in the UK. It considers bottom up and clustering
methods and then details the research plans for implementing and improving
existing framework approaches based on the overall usage profile. The work
focuses on adapting and applying analysis framework approaches to UK energy
data in order to determine the effectiveness of creating a few (single figures)
archetypical users with the intention of improving on the current methods of
determining usage profiles. The work is currently in progress and the paper
details initial results using data collected in Milton Keynes around 1990.
Various possible enhancements to the work are considered including a split
based on temperature to reflect the varying UK weather conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1385</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1385</id><created>2013-07-04</created><authors><author><keyname>Dent</keyname><forenames>Ian</forenames></author><author><keyname>Wagner</keyname><forenames>Christian</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Rodden</keyname><forenames>Tom</forenames></author></authors><title>Creating Personalised Energy Plans. From Groups to Individuals using
  Fuzzy C Means Clustering</title><categories>cs.CE cs.LG</categories><comments>Digital Engagement 11, Newcastle, November 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Changes in the UK electricity market mean that domestic users will be
required to modify their usage behaviour in order that supplies can be
maintained. Clustering allows usage profiles collected at the household level
to be clustered into groups and assigned a stereotypical profile which can be
used to target marketing campaigns. Fuzzy C Means clustering extends this by
allowing each household to be a member of many groups and hence provides the
opportunity to make personalised offers to the household dependent on their
degree of membership of each group. In addition, feedback can be provided on
how user's changing behaviour is moving them towards more &quot;green&quot; or cost
effective stereotypical usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1387</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1387</id><created>2013-07-04</created><authors><author><keyname>Helmi</keyname><forenames>Hala</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jon M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Examining the Classification Accuracy of TSVMs with ?Feature Selection
  in Comparison with the GLAD Algorithm</title><categories>cs.LG cs.CE</categories><comments>UKCI 2011, the 11th Annual Workshop on Computational Intelligence,
  Manchester, pp 7-12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gene expression data sets are used to classify and predict patient diagnostic
categories. As we know, it is extremely difficult and expensive to obtain gene
expression labelled examples. Moreover, conventional supervised approaches
cannot function properly when labelled data (training examples) are
insufficient using Support Vector Machines (SVM) algorithms. Therefore, in this
paper, we suggest Transductive Support Vector Machines (TSVMs) as
semi-supervised learning algorithms, learning with both labelled samples data
and unlabelled samples to perform the classification of microarray data. To
prune the superfluous genes and samples we used a feature selection method
called Recursive Feature Elimination (RFE), which is supposed to enhance the
output of classification and avoid the local optimization problem. We examined
the classification prediction accuracy of the TSVM-RFE algorithm in comparison
with the Genetic Learning Across Datasets (GLAD) algorithm, as both are
semi-supervised learning methods. Comparing these two methods, we found that
the TSVM-RFE surpassed both a SVM using RFE and GLAD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1388</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1388</id><created>2013-07-04</created><authors><author><keyname>Hong</keyname><forenames>Qiao</forenames></author><author><keyname>Yinlin</keyname><forenames>Li</forenames></author><author><keyname>Tang</keyname><forenames>Tang</forenames></author><author><keyname>Peng</keyname><forenames>Wang</forenames></author></authors><title>Introducing Memory and Association Mechanism into a Biologically
  Inspired Visual Model</title><categories>cs.AI</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A famous biologically inspired hierarchical model firstly proposed by
Riesenhuber and Poggio has been successfully applied to multiple visual
recognition tasks. The model is able to achieve a set of position- and
scale-tolerant recognition, which is a central problem in pattern recognition.
In this paper, based on some other biological experimental results, we
introduce the Memory and Association Mechanisms into the above biologically
inspired model. The main motivations of the work are (a) to mimic the active
memory and association mechanism and add the 'top down' adjustment to the above
biologically inspired hierarchical model and (b) to build up an algorithm which
can save the space and keep a good recognition performance. The new model is
also applied to object recognition processes. The primary experimental results
show that our method is efficient with much less memory requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1389</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1389</id><created>2013-07-04</created><authors><author><keyname>Khachatryan</keyname><forenames>A. M.</forenames></author><author><keyname>Kamalian</keyname><forenames>R. R.</forenames></author></authors><title>On the extremal values of the number of vertices with an interval
  spectrum on the set of proper edge colorings of the graph of the
  $n$-dimensional cube</title><categories>math.CO cs.DM</categories><comments>9 pages. arXiv admin note: substantial text overlap with
  arXiv:1205.0125</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  For an undirected, simple, finite, connected graph $G$, we denote by $V(G)$
and $E(G)$ the sets of its vertices and edges, respectively. A function
$\varphi:E(G)\rightarrow \{1,...,t\}$ is called a proper edge $t$-coloring of a
graph $G$, if adjacent edges are colored differently and each of $t$ colors is
used. The least value of $t$ for which there exists a proper edge $t$-coloring
of a graph $G$ is denoted by $\chi'(G)$. For any graph $G$, and for any integer
$t$ satisfying the inequality $\chi'(G)\leq t\leq |E(G)|$, we denote by
$\alpha(G,t)$ the set of all proper edge $t$-colorings of $G$. Let us also
define a set $\alpha(G)$ of all proper edge colorings of a graph $G$: $$
\alpha(G)\equiv\bigcup_{t=\chi'(G)}^{|E(G)|}\alpha(G,t). $$
  An arbitrary nonempty finite subset of consecutive integers is called an
interval. If $\varphi\in\alpha(G)$ and $x\in V(G)$, then the set of colors of
edges of $G$ which are incident with $x$ is denoted by $S_G(x,\varphi)$ and is
called a spectrum of the vertex $x$ of the graph $G$ at the proper edge
coloring $\varphi$. If $G$ is a graph and $\varphi\in\alpha(G)$, then define
$f_G(\varphi)\equiv|\{x\in V(G)/S_G(x,\varphi) \textrm{is an interval}\}|$.
  For a graph $G$ and any integer $t$, satisfying the inequality $\chi'(G)\leq
t\leq |E(G)|$, we define: $$
\mu_1(G,t)\equiv\min_{\varphi\in\alpha(G,t)}f_G(\varphi),\qquad
\mu_2(G,t)\equiv\max_{\varphi\in\alpha(G,t)}f_G(\varphi). $$
  For any graph $G$, we set: $$ \mu_{11}(G)\equiv\min_{\chi'(G)\leq
t\leq|E(G)|}\mu_1(G,t),\qquad \mu_{12}(G)\equiv\max_{\chi'(G)\leq
t\leq|E(G)|}\mu_1(G,t), $$ $$ \mu_{21}(G)\equiv\min_{\chi'(G)\leq
t\leq|E(G)|}\mu_2(G,t),\qquad \mu_{22}(G)\equiv\max_{\chi'(G)\leq
t\leq|E(G)|}\mu_2(G,t). $$
  For any positive integer $n$, the exact values of the parameters $\mu_{11}$,
$\mu_{12}$, $\mu_{21}$ and $\mu_{22}$ are found for the graph of the
$n$-dimensional cube.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1390</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1390</id><created>2013-07-04</created><authors><author><keyname>Figueredo</keyname><forenames>Grazziela P</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Siebers</keyname><forenames>Peer-Olaf</forenames></author></authors><title>Systems Dynamics or Agent-Based Modelling for Immune Simulation?</title><categories>cs.CE cs.MA</categories><comments>Proceedings of the 10th International Conference on Artificial Immune
  Systems (ICARIS 2011), LNCS Volume 6825, Cambridge, UK, pp 81-94, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In immune system simulation there are two competing simulation approaches:
System Dynamics Simulation (SDS) and Agent-Based Simulation (ABS). In the
literature there is little guidance on how to choose the best approach for a
specific immune problem. Our overall research aim is to develop a framework
that helps researchers with this choice. In this paper we investigate if it is
possible to easily convert simulation models between approaches. With no
explicit guidelines available from the literature we develop and test our own
set of guidelines for converting SDS models into ABS models in a non-spacial
scenario. We also define guidelines to convert ABS into SDS considering a
non-spatial and a spatial scenario. After running some experiments with the
developed models we found that in all cases there are significant differences
between the results produced by the different simulation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1391</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1391</id><created>2013-07-04</created><authors><author><keyname>Gu</keyname><forenames>Feng</forenames></author><author><keyname>Feyereisl</keyname><forenames>Jan</forenames></author><author><keyname>Oates</keyname><forenames>Robert</forenames></author><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Greensmith</keyname><forenames>Julie</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Quiet in Class: Classification, Noise and the Dendritic Cell Algorithm</title><categories>cs.LG cs.CR</categories><comments>Proceedings of the 10th International Conference on Artificial Immune
  Systems (ICARIS 2011), LNCS Volume 6825, Cambridge, UK, pp 173-186, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theoretical analyses of the Dendritic Cell Algorithm (DCA) have yielded
several criticisms about its underlying structure and operation. As a result,
several alterations and fixes have been suggested in the literature to correct
for these findings. A contribution of this work is to investigate the effects
of replacing the classification stage of the DCA (which is known to be flawed)
with a traditional machine learning technique. This work goes on to question
the merits of those unique properties of the DCA that are yet to be thoroughly
analysed. If none of these properties can be found to have a benefit over
traditional approaches, then &quot;fixing&quot; the DCA is arguably less efficient than
simply creating a new algorithm. This work examines the dynamic filtering
property of the DCA and questions the utility of this unique feature for the
anomaly detection problem. It is found that this feature, while advantageous
for noisy, time-ordered classification, is not as useful as a traditional
static filter for processing a synthetic dataset. It is concluded that there
are still unique features of the DCA left to investigate. Areas that may be of
benefit to the Artificial Immune Systems community are suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1392</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1392</id><created>2013-07-04</created><authors><author><keyname>Davtyan</keyname><forenames>N. N.</forenames></author><author><keyname>Kamalian</keyname><forenames>R. R.</forenames></author></authors><title>An inequality for the number of vertices with an interval spectrum in
  edge labelings of regular graphs</title><categories>math.CO cs.DM</categories><comments>3 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We consider undirected simple finite graphs. The sets of vertices and edges
of a graph $G$ are denoted by $V(G)$ and $E(G)$, respectively. For a graph $G$,
we denote by $\delta(G)$ and $\eta(G)$ the least degree of a vertex of $G$ and
the number of connected components of $G$, respectively. For a graph $G$ and an
arbitrary subset $V_0\subseteq V(G)$ $G[V_0]$ denotes the subgraph of the graph
$G$ induced by the subset $V_0$ of its vertices. An arbitrary nonempty finite
subset of consecutive integers is called an interval. A function
$\varphi:E(G)\rightarrow \{1,2,\dots,|E(G)|\}$ is called an edge labeling of
the graph $G$, if for arbitrary different edges $e'\in E(G)$ and $e''\in E(G)$,
the inequality $\varphi(e')\neq \varphi(e'')$ holds. If $G$ is a graph, $x$ is
its arbitrary vertex, and $\varphi$ is its arbitrary edge labeling, then the
set $S_G(x,\varphi)\equiv\{\varphi(e)/ e\in E(G), e \textrm{is incident with}
x$\} is called a spectrum of the vertex $x$ of the graph $G$ at its edge
labeling $\varphi$. If $G$ is a graph and $\varphi$ is its arbitrary edge
labeling, then $V_{int}(G,\varphi)\equiv\{x\in V(G)/\;S_G(x,\varphi)\textrm{is
an interval}\}$. For an arbitrary $r$-regular graph $G$ with $r\geq2$ and its
arbitrary edge labeling $\varphi$, the inequality $$
|V_{int}(G,\varphi)|\leq\bigg\lfloor\frac{3\cdot|V(G)|-2\cdot\eta(G[V_{int}(G,\varphi)])}{4}\bigg\rfloor.
$$ is proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1394</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1394</id><created>2013-07-04</created><authors><author><keyname>Liu</keyname><forenames>Yihui</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Detect adverse drug reactions for drug Alendronate</title><categories>cs.CE cs.LG</categories><comments>Second International Conference on Business Computing and Global
  Informatization (BCGIN), pp 820-823, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adverse drug reaction (ADR) is widely concerned for public health issue. In
this study we propose an original approach to detect the ADRs using feature
matrix and feature selection. The experiments are carried out on the drug
Simvastatin. Major side effects for the drug are detected and better
performance is achieved compared to other computerized methods. The detected
ADRs are based on the computerized method, further investigation is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1397</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1397</id><created>2013-07-04</created><authors><author><keyname>Kittichokechai</keyname><forenames>Kittipong</forenames></author><author><keyname>Chia</keyname><forenames>Yeow-Khiang</forenames></author><author><keyname>Oechtering</keyname><forenames>Tobias J.</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Secure Source Coding with a Public Helper</title><categories>cs.IT math.IT</categories><comments>45 pages, 12 figures, a short version to be presented at ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider secure multi-terminal source coding problems in the presence of a
public helper. Two main scenarios are studied: 1) source coding with a helper
where the coded side information from the helper is eavesdropped by an external
eavesdropper; 2) triangular source coding with a helper where the helper is
considered as a public terminal. We are interested in how the helper can
support the source transmission subject to a constraint on the amount of
information leaked due to its public nature. We characterize the tradeoff
between transmission rate, incurred distortion, and information leakage rate at
the helper/eavesdropper in the form of a rate-distortion-leakage region for
various classes of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1406</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1406</id><created>2013-07-04</created><authors><author><keyname>Nicolae</keyname><forenames>Marius</forenames></author><author><keyname>Rajasekaran</keyname><forenames>Sanguthevar</forenames></author></authors><title>On string matching with k mismatches</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider several variants of the pattern matching problem.
In particular, we investigate the following problems: 1) Pattern matching with
k mismatches; 2) Approximate counting of mismatches; and 3) Pattern matching
with mismatches. The distance metric used is the Hamming distance. We present
some novel algorithms and techniques for solving these problems. Both
deterministic and randomized algorithms are offered. Variants of these problems
where there could be wild cards in either the text or the pattern or both are
considered. An experimental evaluation of these algorithms is also presented.
The source code is available at http://www.engr.uconn.edu/~man09004/kmis.zip.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1408</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1408</id><created>2013-07-04</created><updated>2013-07-08</updated><authors><author><keyname>Benatar</keyname><forenames>Naisan</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author></authors><title>An investigation into the relationship between type-2 FOU size and
  environmental uncertainty in robotic control</title><categories>cs.RO cs.AI</categories><comments>2012 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), pp
  1-8, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been suggested that, when faced with large amounts of uncertainty in
situations of automated control, type-2 fuzzy logic based controllers will
out-perform the simpler type-1 varieties due to the latter lacking the
flexibility to adapt accordingly. This paper aims to investigate this problem
in detail in order to analyse when a type-2 controller will improve upon type-1
performance. A robotic sailing boat is subjected to several experiments in
which the uncertainty and difficulty of the sailing problem is increased in
order to observe the effects on measured performance. Improved performance is
observed but not in every case. The size of the FOU is shown to be have a large
effect on performance with potentially severe performance penalties for
incorrectly sized footprints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1411</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1411</id><created>2013-07-04</created><authors><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Soria</keyname><forenames>Daniele</forenames></author><author><keyname>Gibson</keyname><forenames>Jack E.</forenames></author><author><keyname>Hubbard</keyname><forenames>Richard B.</forenames></author></authors><title>Discovering Sequential Patterns in a UK General Practice Database</title><categories>cs.LG cs.CE stat.AP</categories><comments>2012 IEEE-EMBS International Conference on Biomedical and Health
  Informatics, pp 960-963, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wealth of computerised medical information becoming readily available
presents the opportunity to examine patterns of illnesses, therapies and
responses. These patterns may be able to predict illnesses that a patient is
likely to develop, allowing the implementation of preventative actions. In this
paper sequential rule mining is applied to a General Practice database to find
rules involving a patients age, gender and medical history. By incorporating
these rules into current health-care a patient can be highlighted as
susceptible to a future illness based on past or current illnesses, gender and
year of birth. This knowledge has the ability to greatly improve health-care
and reduce health-care costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1417</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1417</id><created>2013-07-04</created><authors><author><keyname>Rajasekaran</keyname><forenames>Sanguthevar</forenames></author><author><keyname>Nicolae</keyname><forenames>Marius</forenames></author></authors><title>An Elegant Algorithm for the Construction of Suffix Arrays</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The suffix array is a data structure that finds numerous applications in
string processing problems for both linguistic texts and biological data. It
has been introduced as a memory efficient alternative for suffix trees. The
suffix array consists of the sorted suffixes of a string. There are several
linear time suffix array construction algorithms (SACAs) known in the
literature. However, one of the fastest algorithms in practice has a worst case
run time of $O(n^2)$. The problem of designing practically and theoretically
efficient techniques remains open. In this paper we present an elegant
algorithm for suffix array construction which takes linear time with high
probability; the probability is on the space of all possible inputs. Our
algorithm is one of the simplest of the known SACAs and it opens up a new
dimension of suffix array construction that has not been explored until now.
Our algorithm is easily parallelizable. We offer parallel implementations on
various parallel models of computing. We prove a lemma on the $\ell$-mers of a
random string which might find independent applications. We also present
another algorithm that utilizes the above algorithm. This algorithm is called
RadixSA and has a worst case run time of $O(n\log{n})$. RadixSA introduces an
idea that may find independent applications as a speedup technique for other
SACAs. An empirical comparison of RadixSA with other algorithms on various
datasets reveals that our algorithm is one of the fastest algorithms to date.
The C++ source code is freely available at
http://www.engr.uconn.edu/~man09004/radixSA.zip
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1424</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1424</id><created>2013-07-04</created><updated>2014-06-30</updated><authors><author><keyname>Samer</keyname><forenames>Phillippe</forenames></author><author><keyname>Urrutia</keyname><forenames>Sebasti&#xe1;n</forenames></author></authors><title>A branch and cut algorithm for minimum spanning trees under conflict
  constraints</title><categories>cs.DS math.CO math.OC</categories><msc-class>90C27, 90C57</msc-class><doi>10.1007/s11590-014-0750-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study approaches for the exact solution of the \NP--hard minimum spanning
tree problem under conflict constraints. Given a graph $G(V,E)$ and a set $C
\subset E \times E$ of conflicting edge pairs, the problem consists of finding
a conflict-free minimum spanning tree, i.e. feasible solutions are allowed to
include at most one of the edges from each pair in $C$. The problem was
introduced recently in the literature, with several results on its complexity
and approximability. Some formulations and both exact and heuristic algorithms
were also discussed, but computational results indicate considerably large
duality gaps and a lack of optimality certificates for benchmark instances. In
this paper, we build on the representation of conflict constraints using an
auxiliary conflict graph $\hat{G}(E,C)$, where stable sets correspond to
conflict-free subsets of $E$. We introduce a general preprocessing method and a
branch and cut algorithm using an IP formulation with exponentially sized
classes of valid inequalities for both the spanning tree and the stable set
polytopes. Encouraging computational results indicate that the dual bounds of
our approach are significantly stronger than those previously available,
already in the initial LP relaxation, and we are able to provide new
feasibility and optimality certificates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1428</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1428</id><created>2013-07-04</created><authors><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Juha</forenames></author><author><keyname>Kempa</keyname><forenames>Dominik</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>Lempel-Ziv Parsing in External Memory</title><categories>cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For decades, computing the LZ factorization (or LZ77 parsing) of a string has
been a requisite and computationally intensive step in many diverse
applications, including text indexing and data compression. Many algorithms for
LZ77 parsing have been discovered over the years; however, despite the
increasing need to apply LZ77 to massive data sets, no algorithm to date scales
to inputs that exceed the size of internal memory. In this paper we describe
the first algorithm for computing the LZ77 parsing in external memory. Our
algorithm is fast in practice and will allow the next generation of text
indexes to be realised for massive strings and string collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1437</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1437</id><created>2013-07-04</created><authors><author><keyname>Zhang</keyname><forenames>Yuqian</forenames></author><author><keyname>Mu</keyname><forenames>Cun</forenames></author><author><keyname>Kuo</keyname><forenames>Han-wen</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>Toward Guaranteed Illumination Models for Non-Convex Objects</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Illumination variation remains a central challenge in object detection and
recognition. Existing analyses of illumination variation typically pertain to
convex, Lambertian objects, and guarantee quality of approximation in an
average case sense. We show that it is possible to build V(vertex)-description
convex cone models with worst-case performance guarantees, for non-convex
Lambertian objects. Namely, a natural verification test based on the angle to
the constructed cone guarantees to accept any image which is sufficiently
well-approximated by an image of the object under some admissible lighting
condition, and guarantees to reject any image that does not have a sufficiently
good approximation. The cone models are generated by sampling point
illuminations with sufficient density, which follows from a new perturbation
bound for point images in the Lambertian model. As the number of point images
required for guaranteed verification may be large, we introduce a new
formulation for cone preserving dimensionality reduction, which leverages tools
from sparse and low-rank decomposition to reduce the complexity, while
controlling the approximation error with respect to the original cone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1447</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1447</id><created>2013-07-04</created><updated>2015-07-29</updated><authors><author><keyname>Corr&#xea;a</keyname><forenames>Ricardo C.</forenames></author><author><keyname>Farias</keyname><forenames>Pablo M. S.</forenames></author></authors><title>Linear Time Computation of the Maximal Linear and Circular Sums of
  Multiple Independent Insertions into a Sequence</title><categories>cs.DS</categories><comments>20 pages, 4 figures, 2 tables. Will be submitted for journal
  publication shortly</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximal sum of a sequence &quot;A&quot; of &quot;n&quot; real numbers is the greatest sum of
all elements of any strictly contiguous and possibly empty subsequence of &quot;A&quot;,
and it can be computed in &quot;O(n)&quot; time by means of Kadane's algorithm. Letting
&quot;A^(x -&gt; p)&quot; denote the sequence which results from inserting a real number &quot;x&quot;
between elements &quot;A[p-1]&quot; and &quot;A[p]&quot;, we show how the maximal sum of &quot;A^(x -&gt;
p)&quot; can be computed in &quot;O(1)&quot; worst-case time for any given &quot;x&quot; and &quot;p&quot;,
provided that an &quot;O(n)&quot; time preprocessing step has already been executed on
&quot;A&quot;. In particular, this implies that, given &quot;m&quot; pairs &quot;(x_0, p_0), ...,
(x_{m-1}, p_{m-1})&quot;, we can compute the maximal sums of sequences &quot;A^(x_0 -&gt;
p_0), ..., A^(x_{m-1} -&gt; p_{m-1})&quot; in &quot;O(n+m)&quot; time, which matches the lower
bound imposed by the problem input size, and also improves on the
straightforward strategy of applying Kadane's algorithm to each sequence
&quot;A^(x_i -&gt; p_i)&quot;, which takes a total of &quot;Theta(n.m)&quot; time. Our main
contribution, however, is to obtain the same time bound for the more
complicated problem of computing the greatest sum of all elements of any
strictly or circularly contiguous and possibly empty subsequence of &quot;A^(x -&gt;
p)&quot;. Our algorithms are easy to implement in practice, and they were motivated
by and find application in a buffer minimization problem on wireless mesh
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1448</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1448</id><created>2013-07-04</created><updated>2013-07-05</updated><authors><author><keyname>Barbarossa</keyname><forenames>Sergio</forenames></author><author><keyname>Sardellitti</keyname><forenames>Stefania</forenames></author><author><keyname>Di Lorenzo</keyname><forenames>Paolo</forenames></author></authors><title>Distributed Detection and Estimation in Wireless Sensor Networks</title><categories>cs.DC cs.IT math.IT</categories><comments>92 pages, 24 figures. To appear in E-Reference Signal Processing, R.
  Chellapa and S. Theodoridis, Eds., Elsevier, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we consider the problems of distributed detection and
estimation in wireless sensor networks. In the first part, we provide a general
framework aimed to show how an efficient design of a sensor network requires a
joint organization of in-network processing and communication. Then, we recall
the basic features of consensus algorithm, which is a basic tool to reach
globally optimal decisions through a distributed approach. The main part of the
paper starts addressing the distributed estimation problem. We show first an
entirely decentralized approach, where observations and estimations are
performed without the intervention of a fusion center. Then, we consider the
case where the estimation is performed at a fusion center, showing how to
allocate quantization bits and transmit powers in the links between the nodes
and the fusion center, in order to accommodate the requirement on the maximum
estimation variance, under a constraint on the global transmit power. We extend
the approach to the detection problem. Also in this case, we consider the
distributed approach, where every node can achieve a globally optimal decision,
and the case where the decision is taken at a central node. In the latter case,
we show how to allocate coding bits and transmit power in order to maximize the
detection probability, under constraints on the false alarm rate and the global
transmit power. Then, we generalize consensus algorithms illustrating a
distributed procedure that converges to the projection of the observation
vector onto a signal subspace. We then address the issue of energy consumption
in sensor networks, thus showing how to optimize the network topology in order
to minimize the energy necessary to achieve a global consensus. Finally, we
address the problem of matching the topology of the network to the graph
describing the statistical dependencies among the observed variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1461</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1461</id><created>2013-07-04</created><updated>2013-07-05</updated><authors><author><keyname>Chae</keyname><forenames>Sung Ho</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author></authors><title>Degrees of Freedom of the Rank-deficient Interference Channel with
  Feedback</title><categories>cs.IT math.IT</categories><comments>The material in this paper will be presented in part at the IEEE
  International Symposium on Information Theory (ISIT) 2013 and was in part
  submitted to the Allerton Conference on Communication, Control, and Computing
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the total degrees of freedom (DoF) of the K-user
rank-deficient interference channel with feedback. For the two-user case, we
characterize the total DoF by developing an achievable scheme and deriving a
matching upper bound. For the three-user case, we develop a new achievable
scheme which employs interference alignment to efficiently utilize the
dimension of the received signal space. In addition, we derive an upper bound
for the general K-user case and show the tightness of the bound when the number
of antennas at each node is sufficiently large. As a consequence of these
results, we show that feedback can increase the DoF when the number of antennas
at each node is large enough as compared to the ranks of channel matrices. This
finding is in contrast to the full-rank interference channel where feedback
provides no DoF gain. The gain comes from using feedback to provide alternative
signal paths, thereby effectively increasing the ranks of desired channel
matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1466</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1466</id><created>2013-07-04</created><authors><author><keyname>Liu</keyname><forenames>Yihui</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Detect adverse drug reactions for the drug Pravastatin</title><categories>cs.CE</categories><comments>5th International Conference on Biomedical Engineering and
  Informatics (BMEI), pp 1188-1192, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adverse drug reaction (ADR) is widely concerned for public health issue. ADRs
are one of most common causes to withdraw some drugs from market. Prescription
event monitoring (PEM) is an important approach to detect the adverse drug
reactions. The main problem to deal with this method is how to automatically
extract the medical events or side effects from high-throughput medical data,
which are collected from day to day clinical practice. In this study we propose
an original approach to detect the ADRs using feature matrix and feature
selection. The experiments are carried out on the drug Pravastatin. Major side
effects for the drug are detected. The detected ADRs are based on computerized
method, further investigation is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1482</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1482</id><created>2013-07-04</created><authors><author><keyname>de Silva</keyname><forenames>Lavindra</forenames></author><author><keyname>Pandey</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Gharbi</keyname><forenames>Mamoun</forenames></author><author><keyname>Alami</keyname><forenames>Rachid</forenames></author></authors><title>Towards Combining HTN Planning and Geometric Task Planning</title><categories>cs.AI</categories><comments>RSS Workshop on Combined Robot Motion Planning and AI Planning for
  Practical Applications, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an interface between a symbolic planner and a
geometric task planner, which is different to a standard trajectory planner in
that the former is able to perform geometric reasoning on abstract
entities---tasks. We believe that this approach facilitates a more principled
interface to symbolic planning, while also leaving more room for the geometric
planner to make independent decisions. We show how the two planners could be
interfaced, and how their planning and backtracking could be interleaved. We
also provide insights for a methodology for using the combined system, and
experimental results to use as a benchmark with future extensions to both the
combined system, as well as to the geometric task planner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1493</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1493</id><created>2013-07-04</created><updated>2013-11-01</updated><authors><author><keyname>Wager</keyname><forenames>Stefan</forenames></author><author><keyname>Wang</keyname><forenames>Sida</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Dropout Training as Adaptive Regularization</title><categories>stat.ML cs.LG stat.ME</categories><comments>11 pages. Advances in Neural Information Processing Systems (NIPS),
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dropout and other feature noising schemes control overfitting by artificially
corrupting the training data. For generalized linear models, dropout performs a
form of adaptive regularization. Using this viewpoint, we show that the dropout
regularizer is first-order equivalent to an L2 regularizer applied after
scaling the features by an estimate of the inverse diagonal Fisher information
matrix. We also establish a connection to AdaGrad, an online learning
algorithm, and find that a close relative of AdaGrad operates by repeatedly
solving linear dropout-regularized problems. By casting dropout as
regularization, we develop a natural semi-supervised algorithm that uses
unlabeled data to create a better adaptive regularizer. We apply this idea to
document classification tasks, and show that it consistently boosts the
performance of dropout training, improving on state-of-the-art results on the
IMDB reviews dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1508</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1508</id><created>2013-07-04</created><updated>2013-12-12</updated><authors><author><keyname>Chen</keyname><forenames>Zhong</forenames></author><author><keyname>Gao</keyname><forenames>Feifei</forenames></author><author><keyname>Zhang</keyname><forenames>Zhenwei</forenames></author><author><keyname>Li</keyname><forenames>James C. F.</forenames></author><author><keyname>Lei</keyname><forenames>Ming</forenames></author></authors><title>Multiple-Level Power Allocation Strategy for Secondary Users in
  Cognitive Radio Networks</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><acm-class>C.2.1</acm-class><doi>10.1186/1687-6180-2014-51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a multiple-level power allocation strategy for the
secondary user (SU) in cognitive radio (CR) networks. Different from the
conventional strategies, where SU either stays silent or transmit with a
constant/binary power depending on the busy/idle status of the primary user
(PU), the proposed strategy allows SU to choose different power levels
according to a carefully designed function of the receiving energy. The way of
the power level selection is optimized to maximize the achievable rate of SU
under the constraints of average transmit power at SU and average interference
power at PU. Simulation results demonstrate that the proposed strategy can
significantly improve the performance of SU compared to the conventional
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1514</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1514</id><created>2013-07-04</created><authors><author><keyname>Lu</keyname><forenames>Lu</forenames></author><author><keyname>You</keyname><forenames>Lizhao</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author></authors><title>Network-Coded Multiple Access</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes and experimentally demonstrates a first wireless local
area network (WLAN) system that jointly exploits physical-layer network coding
(PNC) and multiuser decoding (MUD) to boost system throughput. We refer to this
multiple access mode as Network-Coded Multiple Access (NCMA). Prior studies on
PNC mostly focused on relay networks. NCMA is the first realized multiple
access scheme that establishes the usefulness of PNC in a non-relay setting.
NCMA allows multiple nodes to transmit simultaneously to the access point (AP)
to boost throughput. In the non-relay setting, when two nodes A and B transmit
to the AP simultaneously, the AP aims to obtain both packet A and packet B
rather than their network-coded packet. An interesting question is whether
network coding, specifically PNC which extracts packet (A XOR B), can still be
useful in such a setting. We provide an affirmative answer to this question
with a novel two-layer decoding approach amenable to real-time implementation.
Our USRP prototype indicates that NCMA can boost throughput by 100% in the
medium-high SNR regime (&gt;=10dB). We believe further throughput enhancement is
possible by allowing more than two users to transmit together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1516</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1516</id><created>2013-07-05</created><authors><author><keyname>Abraham</keyname><forenames>Ittai</forenames></author><author><keyname>Chechik</keyname><forenames>Shiri</forenames></author></authors><title>Dynamic Decremental Approximate Distance Oracles with $(1+\epsilon, 2)$
  stretch</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a decremental approximate Distance Oracle that obtains stretch of
$1+\epsilon$ multiplicative and 2 additive and has $\hat{O}(n^{5/2})$ total
cost (where $\hat{O}$ notation suppresses polylogarithmic and
$n^{O(1)/\sqrt{n}}$ factors). The best previous results with $\hat{O}(n^{5/2})$
total cost obtained stretch $3+\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1517</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1517</id><created>2013-07-05</created><authors><author><keyname>Mirajkar</keyname><forenames>Nandan</forenames></author><author><keyname>Bhujbal</keyname><forenames>Sandeep</forenames></author><author><keyname>Deshmukh</keyname><forenames>Aaradhana</forenames></author></authors><title>Perform wordcount Map-Reduce Job in Single Node Apache Hadoop cluster
  and compress data using Lempel-Ziv-Oberhumer (LZO) algorithm</title><categories>cs.DC</categories><comments>10 pages, 17 figures, Journal</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 1, No 2, January 2013 ISSN (Print): 1694-0784 | ISSN (Online):
  1694-0814 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications like Yahoo, Facebook, Twitter have huge data which has to be
stored and retrieved as per client access. This huge data storage requires huge
database leading to increase in physical storage and becomes complex for
analysis required in business growth. This storage capacity can be reduced and
distributed processing of huge data can be done using Apache Hadoop which uses
Map-reduce algorithm and combines the repeating data so that entire data is
stored in reduced format. The paper describes performing a wordcount Map-Reduce
Job in Single Node Apache Hadoop cluster and compress data using
Lempel-Ziv-Oberhumer (LZO) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1524</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1524</id><created>2013-07-05</created><authors><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Li</keyname><forenames>Ying</forenames></author><author><keyname>Nuggehalli</keyname><forenames>Pavan</forenames></author><author><keyname>Pi</keyname><forenames>Zhouyue</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Fundamentals of Heterogeneous Cellular Networks with Energy Harvesting</title><categories>cs.IT cs.NI math.IT stat.AP</categories><comments>submitted to IEEE Transactions on Wireless Communications, July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new tractable model for K-tier heterogeneous cellular networks
(HetNets), where each base station (BS) is powered solely by a self-contained
energy harvesting module. The BSs across tiers differ in terms of the energy
harvesting rate, energy storage capacity, transmit power and deployment
density. Since a BS may not always have enough energy, it may need to be kept
OFF and allowed to recharge while nearby users are served by neighboring BSs
that are ON. We show that the fraction of time a k^{th} tier BS can be kept ON,
termed availability \rho_k, is a fundamental metric of interest. Using tools
from random walk theory, fixed point analysis and stochastic geometry, we
characterize the set of K-tuples (\rho_1, \rho_2, ... \rho_K), termed the
availability region, that is achievable by general uncoordinated operational
strategies, where the decision to toggle the current ON/OFF state of a BS is
taken independently of the other BSs. If the availability vector corresponding
to the optimal system performance, e.g., in terms of rate, lies in this
availability region, there is no performance loss due to the presence of
unreliable energy sources. As a part of our analysis, we model the temporal
dynamics of the energy level at each BS as a birth-death process, derive the
energy utilization rate, and use hitting/stopping time analysis to prove that
there exists a fundamental limit on \rho_k that cannot be surpassed by any
uncoordinated strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1528</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1528</id><created>2013-07-05</created><authors><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames><affiliation>University of Innsbruck, Austria</affiliation></author><author><keyname>L&#xfc;th</keyname><forenames>Christoph</forenames><affiliation>DFKI and University of Bremen, Germany</affiliation></author></authors><title>Proceedings 10th International Workshop On User Interfaces for Theorem
  Provers</title><categories>cs.LO cs.HC cs.MS</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 118, 2013</journal-ref><doi>10.4204/EPTCS.118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This EPTCS volume collects the post-proceedings of the 10th International
Workshop On User Interfaces for Theorem Provers (UITP 2012), held as part of
the Conferences on Intelligent Computer Mathematics (CICM 2012) in Bremen on
July 11th 2012. The UITP workshop series aims at bringing together reasearchers
interested in designing, developing and evaluating interfaces for interactive
proof systems, such as theorem provers, formal method tools, and other tools
manipulating and presenting mathematical formulae. Started in 1995, it can look
back on seventeen years of history by now.
  The papers in the present volume give a good indication of the range of
questions currently addressed in the UITP community; this ranges from interface
design (Windsteiger; Dunchev et al) to using technologies such as machine
learning to assist the user (Komendantskaya et al). The web features
prominently (Tankink), and new technology necessitates changes right down to
the very basic modes of interaction (Wenzel) - the old REPL (read, evaluate,
print, loop) mode of interaction can not take advantage of modern technology,
such as the web and multi-core machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1532</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1532</id><created>2013-07-05</created><authors><author><keyname>Zocca</keyname><forenames>Alessandro</forenames></author><author><keyname>Borst</keyname><forenames>Sem C.</forenames></author><author><keyname>van Leeuwaarden</keyname><forenames>Johan S. H.</forenames></author><author><keyname>Nardi</keyname><forenames>Francesca R.</forenames></author></authors><title>Delay performance in random-access grid networks</title><categories>math.PR cond-mat.stat-mech cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the impact of torpid mixing and meta-stability issues on the delay
performance in wireless random-access networks. Focusing on regular meshes as
prototypical scenarios, we show that the mean delays in an $L\times L$ toric
grid with normalized load $\rho$ are of the order $(\frac{1}{1-\rho})^L$. This
superlinear delay scaling is to be contrasted with the usual linear growth of
the order $\frac{1}{1-\rho}$ in conventional queueing networks. The intuitive
explanation for the poor delay characteristics is that (i) high load requires a
high activity factor, (ii) a high activity factor implies extremely slow
transitions between dominant activity states, and (iii) slow transitions cause
starvation and hence excessively long queues and delays. Our proof method
combines both renewal and conductance arguments. A critical ingredient in
quantifying the long transition times is the derivation of the communication
height of the uniformized Markov chain associated with the activity process. We
also discuss connections with Glauber dynamics, conductance and mixing times.
Our proof framework can be applied to other topologies as well, and is also
relevant for the hard-core model in statistical physics and the sampling from
independent sets using single-site update Markov chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1537</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1537</id><created>2013-07-05</created><updated>2013-07-17</updated><authors><author><keyname>Muharar</keyname><forenames>Rusdha</forenames></author><author><keyname>Zakhour</keyname><forenames>Randa</forenames></author><author><keyname>Evans</keyname><forenames>Jamie</forenames></author></authors><title>Optimal Power Allocation and User Loading for Multiuser MISO Channels
  with Regularized Channel Inversion</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiuser system where a single transmitter equipped with
multiple antennas (the base station) communicates with multiple users each with
a single antenna. Regularized channel inversion is employed as the precoding
strategy at the base station. Within this scenario we are interested in the
problems of power allocation and user admission control so as to maximize the
system throughput, i.e., which users should we communicate with and what power
should we use for each of the admitted users so as to get the highest sum rate.
This is in general a very difficult problem but we do two things to allow some
progress to be made. Firstly we consider the large system regime where the
number of antennas at the base station is large along with the number of users.
Secondly we cluster the downlink path gains of users into a finite number of
groups. By doing this we are able to show that the optimal power allocation
under an average transmit power constraint follows the well-known water filling
scheme. We also investigate the user admission problem which reduces in the
large system regime to optimization of the user loading in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1542</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1542</id><created>2013-07-05</created><authors><author><keyname>von der Weth</keyname><forenames>Christian</forenames></author><author><keyname>Hauswirth</keyname><forenames>Manfred</forenames></author></authors><title>DOBBS: Towards a Comprehensive Dataset to Study the Browsing Behavior of
  Online Users</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The investigation of the browsing behavior of users provides useful
information to optimize web site design, web browser design, search engines
offerings, and online advertisement. This has been a topic of active research
since the Web started and a large body of work exists. However, new online
services as well as advances in Web and mobile technologies clearly changed the
meaning behind &quot;browsing the Web&quot; and require a fresh look at the problem and
research, specifically in respect to whether the used models are still
appropriate. Platforms such as YouTube, Netflix or last.fm have started to
replace the traditional media channels (cinema, television, radio) and media
distribution formats (CD, DVD, Blu-ray). Social networks (e.g., Facebook) and
platforms for browser games attracted whole new, particularly less tech-savvy
audiences. Furthermore, advances in mobile technologies and devices made
browsing &quot;on-the-move&quot; the norm and changed the user behavior as in the mobile
case browsing is often being influenced by the user's location and context in
the physical world. Commonly used datasets, such as web server access logs or
search engines transaction logs, are inherently not capable of capturing the
browsing behavior of users in all these facets. DOBBS (DERI Online Behavior
Study) is an effort to create such a dataset in a non-intrusive, completely
anonymous and privacy-preserving way. To this end, DOBBS provides a browser
add-on that users can install, which keeps track of their browsing behavior
(e.g., how much time they spent on the Web, how long they stay on a website,
how often they visit a website, how they use their browser, etc.). In this
paper, we outline the motivation behind DOBBS, describe the add-on and captured
data in detail, and present some first results to highlight the strengths of
DOBBS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1543</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1543</id><created>2013-07-05</created><authors><author><keyname>von der Weth</keyname><forenames>Christian</forenames></author><author><keyname>Hauswirth</keyname><forenames>Manfred</forenames></author></authors><title>Finding Information Through Integrated Ad-Hoc Socializing in the Virtual
  and Physical World</title><categories>cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the services of sophisticated search engines like Google, there are a
number of interesting information sources which are useful but largely
inaccessible to current Web users. These information sources are often ad-hoc,
location-specific and only useful for users over short periods of time, or
relate to tacit knowledge of users or implicit knowledge in crowds. The
solution presented in this paper addresses these problems by introducing an
integrated concept of &quot;location&quot; and &quot;presence&quot; across the physical and virtual
worlds enabling ad-hoc socializing of users interested in, or looking for
similar information. While the definition of presence in the physical world is
straightforward - through a spatial location and vicinity at a certain point in
time - their definitions in the virtual world are neither obvious nor trivial.
Based on a detailed analysis we provide an integrated spatial model spanning
both worlds which enables us to define presence of users in a unified way. This
integrated model allows us to enable ad-hoc socializing of users browsing the
Web with users in the physical world specific to their joint information needs
and allows us to unlock the untapped information sources mentioned above. We
describe a proof-of-concept implementation of our model and provide an
empirical analysis based on real-world experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1560</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1560</id><created>2013-07-05</created><authors><author><keyname>G&#x142;owinski</keyname><forenames>Rados&#x142;aw</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author></authors><title>Compressed Pattern-Matching with Ranked Variables in Zimin Words</title><categories>cs.DS</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zimin words are very special finite words which are closely related to the
pattern-avoidability problem. This problem consists in testing if an instance
of a given pattern with variables occurs in almost all words over any finite
alphabet. The problem is not well understood, no polynomial time algorithm is
known and its NP-hardness is also not known. The pattern-avoidability problem
is equivalent to searching for a pattern (with variables) in a Zimin word. The
main difficulty is potentially exponential size of Zimin words. We use special
properties of Zimin words, especially that they are highly compressible, to
design efficient algorithms for special version of the pattern-matching, called
here ranked matching. It gives a new interpretation of Zimin algorithm in
compressed setting. We discuss the structure of rankings of variables and
compressed representations of values of variables. Moreover, for a ranked
matching we present efficient algorithms to find the shortest instance and the
number of valuations of instances of the pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1561</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1561</id><created>2013-07-05</created><authors><author><keyname>Vimina</keyname><forenames>E. R.</forenames></author><author><keyname>Jacob</keyname><forenames>K. Poulose</forenames></author></authors><title>A Sub-block Based Image Retrieval Using Modified Integrated Region
  Matching</title><categories>cs.IR cs.CV</categories><comments>7 pages</comments><journal-ref>International Journal of Computer Science Issues, Vol.10, Issue
  1,No 2, January 2013, pp. 686-692</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a content based image retrieval (CBIR) system using the
local colour and texture features of selected image sub-blocks and global
colour and shape features of the image. The image sub-blocks are roughly
identified by segmenting the image into partitions of different configuration,
finding the edge density in each partition using edge thresholding followed by
morphological dilation. The colour and texture features of the identified
regions are computed from the histograms of the quantized HSV colour space and
Gray Level Co- occurrence Matrix (GLCM) respectively. The colour and texture
feature vectors is computed for each region. The shape features are computed
from the Edge Histogram Descriptor (EHD). A modified Integrated Region Matching
(IRM) algorithm is used for finding the minimum distance between the sub-blocks
of the query and target image. Experimental results show that the proposed
method provides better retrieving result than retrieval using some of the
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1562</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1562</id><created>2013-07-05</created><updated>2015-01-29</updated><authors><author><keyname>Schubert</keyname><forenames>Michael</forenames></author><author><keyname>Steffen</keyname><forenames>Eckhard</forenames></author></authors><title>Nowhere-zero flows on signed regular graphs</title><categories>math.CO cs.DM</categories><comments>24 pages, 4 figures; final version; to appear in European J.
  Combinatorics</comments><msc-class>05C21, 05C22</msc-class><journal-ref>European Journal of Combinatorics 48 (2015) 34-47</journal-ref><doi>10.1016/j.ejc.2015.02.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the flow spectrum ${\cal S}(G)$ and the integer flow spectrum
$\overline{{\cal S}}(G)$ of signed $(2t+1)$-regular graphs. We show that if $r
\in {\cal S}(G)$, then $r = 2+\frac{1}{t}$ or $r \geq 2 + \frac{2}{2t-1}$.
Furthermore, $2 + \frac{1}{t} \in {\cal S}(G)$ if and only if $G$ has a
$t$-factor. If $G$ has a 1-factor, then $3 \in \overline{{\cal S}}(G)$, and for
every $t \geq 2$, there is a signed $(2t+1)$-regular graph $(H,\sigma)$ with $
3 \in \overline{{\cal S}}(H)$ and $H$ does not have a 1-factor.
  If $G$ $(\not = K_2^3)$ is a cubic graph which has a 1-factor, then $\{3,4\}
\subseteq {\cal S}(G) \cap \overline{{\cal S}}(G)$. Furthermore, the following
four statements are equivalent: (1) $G$ has a 1-factor. (2) $3 \in {\cal
S}(G)$. (3) $3 \in \overline{{\cal S}}(G)$. (4) $4 \in \overline{{\cal S}}(G)$.
There are cubic graphs whose integer flow spectrum does not contain 5 or 6, and
we construct an infinite family of bridgeless cubic graphs with integer flow
spectrum $\{3,4,6\}$.
  We show that there are signed graphs where the difference between the integer
flow number and the flow number is greater than or equal to 1, disproving a
conjecture of Raspaud and Zhu.
  The paper concludes with a proof of Bouchet's 6-flow conjecture for
Kotzig-graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1568</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1568</id><created>2013-07-05</created><authors><author><keyname>Do</keyname><forenames>Chau</forenames></author><author><keyname>Pauwels</keyname><forenames>Eric J.</forenames></author></authors><title>Using MathML to Represent Units of Measurement for Improved Ontology
  Alignment</title><categories>cs.AI</categories><comments>Conferences on Intelligent Computer Mathematics (CICM 2013), Bath,
  England</comments><journal-ref>CICM 2013, LNAI (7961), Springer, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontologies provide a formal description of concepts and their relationships
in a knowledge domain. The goal of ontology alignment is to identify
semantically matching concepts and relationships across independently developed
ontologies that purport to describe the same knowledge. In order to handle the
widest possible class of ontologies, many alignment algorithms rely on
terminological and structural meth- ods, but the often fuzzy nature of concepts
complicates the matching process. However, one area that should provide clear
matching solutions due to its mathematical nature, is units of measurement.
Several on- tologies for units of measurement are available, but there has been
no attempt to align them, notwithstanding the obvious importance for tech-
nical interoperability. We propose a general strategy to map these (and
similar) ontologies by introducing MathML to accurately capture the semantic
description of concepts specified therein. We provide mapping results for three
ontologies, and show that our approach improves on lexical comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1584</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1584</id><created>2013-07-05</created><authors><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Soria</keyname><forenames>Daniele</forenames></author><author><keyname>Gibson</keyname><forenames>Jack E.</forenames></author><author><keyname>Hubbard</keyname><forenames>Richard B.</forenames></author></authors><title>Comparing Data-mining Algorithms Developed for Longitudinal
  Observational Databases</title><categories>cs.LG cs.CE cs.DB</categories><comments>UKCI 2012, the 12th Annual Workshop on Computational Intelligence,
  Heriot-Watt University, pp 1-8, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Longitudinal observational databases have become a recent interest in the
post marketing drug surveillance community due to their ability of presenting a
new perspective for detecting negative side effects. Algorithms mining
longitudinal observation databases are not restricted by many of the
limitations associated with the more conventional methods that have been
developed for spontaneous reporting system databases. In this paper we
investigate the robustness of four recently developed algorithms that mine
longitudinal observational databases by applying them to The Health Improvement
Network (THIN) for six drugs with well document known negative side effects.
Our results show that none of the existing algorithms was able to consistently
identify known adverse drug reactions above events related to the cause of the
drug and no algorithm was superior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1589</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1589</id><created>2013-07-05</created><authors><author><keyname>Labb&#xe9;</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>A counterexample to a question of Hof, Knill and Simon</title><categories>math.CO cs.FL</categories><comments>11 pages, 0 figures</comments><msc-class>68R15</msc-class><journal-ref>Electronic Journal of Combinatorics 21 (2014) #P3.11</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this article, we give a negative answer to a question of Hof, Knill and
Simon (1995) concerning purely morphic sequences obtained from primitive
morphism containing an infinite number of palindromes. Proven for the binary
alphabet by B. Tan in 2007, we show the existence of a counterexample on the
ternary alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1597</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1597</id><created>2013-07-05</created><updated>2013-07-08</updated><authors><author><keyname>Figueredo</keyname><forenames>Grazziela P.</forenames></author><author><keyname>Siebers</keyname><forenames>Peer-Olaf</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Foan</keyname><forenames>Stephanie</forenames></author></authors><title>A Beginners Guide to Systems Simulation in Immunology</title><categories>cs.CE</categories><comments>Proceedings of the 11th Int. Conf. on Artificial Immune Systems, pp
  57-71, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some common systems modelling and simulation approaches for immune problems
are Monte Carlo simulations, system dynamics, discrete-event simulation and
agent-based simulation. These methods, however, are still not widely adopted in
immunology research. In addition, to our knowledge, there is few research on
the processes for the development of simulation models for the immune system.
Hence, for this work, we have two contributions to knowledge. The first one is
to show the importance of systems simulation to help immunological research and
to draw the attention of simulation developers to this research field. The
second contribution is the introduction of a quick guide containing the main
steps for modelling and simulation in immunology, together with challenges that
occur during the model development. Further, this paper introduces an example
of a simulation problem, where we test our guidelines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1598</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1598</id><created>2013-07-05</created><authors><author><keyname>Roadknight</keyname><forenames>Christopher M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Extending a Microsimulation of the Port of Dover</title><categories>cs.CE</categories><comments>ORS SW12 Simulation Conference, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling and simulating the traffic of heavily used but secure environments
such as seaports and airports is of increasing importance. This paper discusses
issues and problems that may arise when extending an existing microsimulation
strategy. We also discuss how extensions of these simulations can aid planners
with optimal physical and operational feedback. Conclusions are drawn about how
microsimulations can be moved forward as a robust planning tool for the 21st
century.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1599</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1599</id><created>2013-07-05</created><authors><author><keyname>Roadknight</keyname><forenames>Chris</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Qiu</keyname><forenames>Guoping</forenames></author><author><keyname>Scholefield</keyname><forenames>John</forenames></author><author><keyname>Durrant</keyname><forenames>Lindy</forenames></author></authors><title>Supervised Learning and Anti-learning of Colorectal Cancer Classes and
  Survival Rates from Cellular Biology Parameters</title><categories>cs.LG cs.CE stat.ML</categories><comments>IEEE International Conference on Systems, Man, and Cybernetics, pp
  797-802, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a dataset relating to cellular and physical
conditions of patients who are operated upon to remove colorectal tumours. This
data provides a unique insight into immunological status at the point of tumour
removal, tumour classification and post-operative survival. Attempts are made
to learn relationships between attributes (physical and immunological) and the
resulting tumour stage and survival. Results for conventional machine learning
approaches can be considered poor, especially for predicting tumour stages for
the most important types of cancer. This poor performance is further
investigated and compared with a synthetic, dataset based on the logical
exclusive-OR function and it is shown that there is a significant level of
'anti-learning' present in all supervised methods used and this can be
explained by the highly dimensional, complex and sparsely representative
dataset. For predicting the stage of cancer from the immunological attributes,
anti-learning approaches outperform a range of popular algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1601</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1601</id><created>2013-07-05</created><authors><author><keyname>Roadknight</keyname><forenames>Chris</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Ladas</keyname><forenames>Alex</forenames></author><author><keyname>Soria</keyname><forenames>Daniele</forenames></author><author><keyname>Scholefield</keyname><forenames>John</forenames></author><author><keyname>Durrant</keyname><forenames>Lindy</forenames></author></authors><title>Biomarker Clustering of Colorectal Cancer Data to Complement Clinical
  Classification</title><categories>cs.LG cs.CE</categories><comments>Federated Conference on Computer Science and Information Systems
  (FedCSIS), pp 187-191, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a dataset relating to cellular and physical
conditions of patients who are operated upon to remove colorectal tumours. This
data provides a unique insight into immunological status at the point of tumour
removal, tumour classification and post-operative survival. Attempts are made
to cluster this dataset and important subsets of it in an effort to
characterize the data and validate existing standards for tumour
classification. It is apparent from optimal clustering that existing tumour
classification is largely unrelated to immunological factors within a patient
and that there may be scope for re-evaluating treatment options and survival
estimates based on a combination of tumour physiology and patient
histochemistry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1615</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1615</id><created>2013-07-05</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author></authors><title>Partitions of a Finite Partially Ordered Set</title><categories>cs.DM math.CO math.CT</categories><comments>Preprint submitted for publication in the book &quot;From Combinatorics to
  Philosophy: The Legacy of G.-C. Rota&quot;</comments><doi>10.1007/978-0-387-88753-1_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the notion of partition of a finite partially
ordered set (poset, for short). We will define three different notions of
partition of a poset, namely, monotone, regular, and open partition. For each
of these notions we will find three equivalent definitions, that will be shown
to be equivalent. We start by defining partitions of a poset in terms of fibres
of some surjection having the poset as domain. We then obtain combinatorial
characterisations of such notions in terms of blocks, without reference to
surjection. Finally, we give a further, equivalent definition of each kind of
partition by means of analogues of equivalence relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1625</identifier>
 <datestamp>2014-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1625</id><created>2013-07-05</created><updated>2014-08-13</updated><authors><author><keyname>Triverio</keyname><forenames>Piero</forenames></author></authors><title>Robust Causality Check for Sampled Scattering Parameters via a Filtered
  Fourier Transform</title><categories>cs.CE</categories><journal-ref>IEEE Microwave and Wireless Components Letters, vol.24, no.2,
  pp.72,74, Feb. 2014</journal-ref><doi>10.1109/LMWC.2013.2290218</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a robust numerical technique to verify the causality of sampled
scattering parameters given on a finite bandwidth. The method is based on a
filtered Fourier transform and includes a rigorous estimation of the errors
caused by missing out-of-band samples. Compared to existing techniques, the
method is simpler to implement and provides a useful insight on the time-domain
characteristics of the detected violation. Through an applicative example, we
shows its usefulness to improve the accuracy and reliability of macromodeling
techniques used to convert sampled scattering parameters into models for
transient analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1627</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1627</id><created>2013-07-05</created><authors><author><keyname>van Hoeij</keyname><forenames>Mark</forenames></author></authors><title>Computing Puiseux Expansions at Cusps of the Modular Curve X0(N)</title><categories>math.NT cs.SC</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal in this preprint is to give an efficient algorithm to compute
Puiseux expansions at cusps of X0(N). It is based on a relation with a
hypergeometric function that holds for any N.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1630</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1630</id><created>2013-07-05</created><updated>2013-07-15</updated><authors><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Perlaza</keyname><forenames>Samir M.</forenames></author><author><keyname>Esnaola</keyname><forenames>Inaki</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Power Allocation Strategies in Energy Harvesting Wireless Cooperative
  Networks</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE TWireless</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a wireless cooperative network is considered, in which
multiple source-destination pairs communicate with each other via an energy
harvesting relay. The focus of this paper is on the relay's strategies to
distribute the harvested energy among the multiple users and their impact on
the system performance. Specifically, a non-cooperative strategy is to use the
energy harvested from the i-th source as the relay transmission power to the
i-th destination, to which asymptotic results show that its outage performance
decays as logSNR over SNR. A faster decaying rate, 1 over SNR, can be achieved
by the two centralized strategies proposed this the paper, where the water
filling based one can achieve optimal performance with respect to several
criteria, with a price of high complexity. An auction based power allocation
scheme is also proposed to achieve a better tradeoff between the system
performance and complexity. Simulation results are provided to confirm the
accuracy of the developed analytical results and facilitate a better
performance comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1650</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1650</id><created>2013-07-05</created><authors><author><keyname>Anta</keyname><forenames>Antonio Fernandez</forenames></author><author><keyname>Georgiou</keyname><forenames>Chryssis</forenames></author><author><keyname>Mosteiro</keyname><forenames>Miguel A.</forenames></author></authors><title>Algorithmic Mechanisms for Reliable Internet-based Computing under
  Collusion</title><categories>cs.DC cs.GT</categories><comments>23 pages. A preliminary version of this work appears in the
  Proceedings of NCA 2008</comments><msc-class>68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, using a game-theoretic approach, cost-sensitive mechanisms that
lead to reliable Internet-based computing are designed. In particular, we
consider Internet-based master-worker computations, where a master processor
assigns, across the Internet, a computational task to a set of potentially
untrusted worker processors and collects their responses. Workers may collude
in order to increase their benefit. Several game-theoretic models that capture
the nature of the problem are analyzed, and algorithmic mechanisms that, for
each given set of cost and system parameters, achieve high reliability are
designed. Additionally, two specific realistic system scenarios are studied.
These scenarios are a system of volunteer computing like SETI, and a company
that buys computing cycles from Internet computers and sells them to its
customers in the form of a task- computation service. Notably, under certain
conditions, non redundant allocation yields the best trade-off between cost and
reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1656</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1656</id><created>2013-07-05</created><updated>2013-10-28</updated><authors><author><keyname>Cozzo</keyname><forenames>E.</forenames></author><author><keyname>Ba&#xf1;os</keyname><forenames>R. A.</forenames></author><author><keyname>Meloni</keyname><forenames>S.</forenames></author><author><keyname>Moreno</keyname><forenames>Y.</forenames></author></authors><title>Contact-based Social Contagion in Multiplex Networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>5 pages and 3 figures. Final version to appear in PRE RC</comments><doi>10.1103/PhysRevE.88.050801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a theoretical framework for the study of epidemic-like social
contagion in large scale social systems. We consider the most general setting
in which different communication platforms or categories form multiplex
networks. Specifically, we propose a contact-based information spreading model,
and show that the critical point of the multiplex system associated to the
active phase is determined by the layer whose contact probability matrix has
the largest eigenvalue. The framework is applied to a number of different
situations, including a real multiplex system. Finally, we also show that when
the system through which information is disseminating is inherently multiplex,
working with the graph that results from the aggregation of the different
layers is flawed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1662</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1662</id><created>2013-07-05</created><updated>2014-06-27</updated><authors><author><keyname>Al-Rfou</keyname><forenames>Rami</forenames></author><author><keyname>Perozzi</keyname><forenames>Bryan</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>Polyglot: Distributed Word Representations for Multilingual NLP</title><categories>cs.CL cs.LG</categories><comments>10 pages, 2 figures, Proceedings of Conference on Computational
  Natural Language Learning CoNLL'2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed word representations (word embeddings) have recently contributed
to competitive performance in language modeling and several NLP tasks. In this
work, we train word embeddings for more than 100 languages using their
corresponding Wikipedias. We quantitatively demonstrate the utility of our word
embeddings by using them as the sole features for training a part of speech
tagger for a subset of these languages. We find their performance to be
competitive with near state-of-art methods in English, Danish and Swedish.
Moreover, we investigate the semantic features captured by these embeddings
through the proximity of word groupings. We will release these embeddings
publicly to help researchers in the development and enhancement of multilingual
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1670</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1670</id><created>2013-07-05</created><authors><author><keyname>Madeo</keyname><forenames>Dario</forenames></author><author><keyname>Mocenni</keyname><forenames>Chiara</forenames></author></authors><title>A New Mathematical Model for Evolutionary Games on Finite Networks of
  Players</title><categories>math.DS cs.GT q-bio.PE</categories><comments>26 pages, 7 figures, 1 table</comments><msc-class>91A22, 91A43, 92D25 (Primary) 91A80, 91A06, 37N25 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new mathematical model for evolutionary games on graphs is proposed to
extend the classical replicator equation to finite populations of players
organized on a network with generic topology. Classical results from game
theory, evolutionary game theory and graph theory are used. More specifically,
each player is placed in a vertex of the graph and he is seen as an infinite
population of replicators which replicate within the vertex. At each time
instant, a game is played by two replicators belonging to different connected
vertices, and the outcome of the game influences their ability of producing
offspring. Then, the behavior of a vertex player is determined by the
distribution of strategies used by the internal replicators. Under suitable
hypotheses, the proposed model is equivalent to the classical replicator
equation. Extended simulations are performed to show the dynamical behavior of
the solutions and the potentialities of the developed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1674</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1674</id><created>2013-07-05</created><authors><author><keyname>Arora</keyname><forenames>Raman</forenames></author><author><keyname>Cotter</keyname><forenames>Andrew</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Stochastic Optimization of PCA with Capped MSG</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study PCA as a stochastic optimization problem and propose a novel
stochastic approximation algorithm which we refer to as &quot;Matrix Stochastic
Gradient&quot; (MSG), as well as a practical variant, Capped MSG. We study the
method both theoretically and empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1681</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1681</id><created>2013-06-18</created><authors><author><keyname>Liu</keyname><forenames>Lianggui</forenames></author></authors><title>Extracting the trustworthiest way to service provider in complex online
  social networks</title><categories>cs.SI cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In complex online social networks, it is crucial for a service consumer to
extract the trustworthiest way to a target service provider from numerous
social trust paths between them. The extraction of the trustworthiest way
(namely, optimal social trust path (OSTP)) with multiple end-to-end quality of
trust (QoT) constraints has been proved to be NP-Complete. Heuristic algorithms
with polynomial and pseudo-polynomial-time complexities are often used to deal
with this challenging problem. However, existing solutions cannot guarantee the
efficiency of searching, that is, they can hardly avoid obtaining partial
optimal solutions during searching process. Quantum annealing uses
delocalization and tunneling to avoid falling into local minima without
sacrifying execution time. It has been proved to be a promising way to many
optimization problems in recently published literatures. In this paper, for the
first time, QA based OSTP algorithms (QA_OSTP) is applied to the extraction of
the trustworthiest way. The experiment results show that QA based algorithms
have better performance than its heuristic opponents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1684</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1684</id><created>2013-07-05</created><authors><author><keyname>Vadhwani</keyname><forenames>Diya Naresh</forenames></author><author><keyname>Singh</keyname><forenames>Megha</forenames></author><author><keyname>Kulhare</keyname><forenames>Deepak</forenames></author></authors><title>Simulation of wireless dynamic source routing protocol with IP traffic
  flow analysis, memory efficiency and increased throughput</title><categories>cs.NI</categories><comments>5 pages, 11 figures, 8 tables</comments><journal-ref>ACEEE International Journal on Communication (IJCom), Issue. 3,
  Vol. 4, Nov 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today in fast technology development in wireless mobile adhoc network there
is vast scope for research. As it is known that wireless communication for
mobile network has many application areas like routing services, security
services etc. The mobile adhoc network is the wireless network for
communication in which the mobile nodes are organized without any centralized
administrator. There are many Manet routing protocols like reactive, proactive,
hybrid etc. In this paper the reactive Manet routing protocol like DSR is
simulated for traffic analysis for 50 mobile nodes for IP traffic flows. Also
throughput is analyzed for DSR and ER-DSR protocol. And finally the memory
utilized during simulation of DSR and ER-DSR is evaluated in order to compare
both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1690</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1690</id><created>2013-07-05</created><updated>2013-11-19</updated><authors><author><keyname>Korula</keyname><forenames>Nitish</forenames></author><author><keyname>Lattanzi</keyname><forenames>Silvio</forenames></author></authors><title>An efficient reconciliation algorithm for social networks</title><categories>cs.DS cs.SI</categories><comments>23 pages, 4 figures. To appear in VLDB 2014</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People today typically use multiple online social networks (Facebook,
Twitter, Google+, LinkedIn, etc.). Each online network represents a subset of
their &quot;real&quot; ego-networks. An interesting and challenging problem is to
reconcile these online networks, that is, to identify all the accounts
belonging to the same individual. Besides providing a richer understanding of
social dynamics, the problem has a number of practical applications. At first
sight, this problem appears algorithmically challenging. Fortunately, a small
fraction of individuals explicitly link their accounts across multiple
networks; our work leverages these connections to identify a very large
fraction of the network.
  Our main contributions are to mathematically formalize the problem for the
first time, and to design a simple, local, and efficient parallel algorithm to
solve it. We are able to prove strong theoretical guarantees on the algorithm's
performance on well-established network models (Random Graphs, Preferential
Attachment). We also experimentally confirm the effectiveness of the algorithm
on synthetic and real social network data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1694</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1694</id><created>2013-07-04</created><authors><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Siebers</keyname><forenames>Peer-Olaf</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Modelling the Effects of User Learning on Forced Innovation Diffusion</title><categories>cs.CY</categories><comments>ORS SW12 Simulation Conference, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technology adoption theories assume that users' acceptance of an innovative
technology is on a voluntary basis. However, sometimes users are force to
accept an innovation. In this case users have to learn what it is useful for
and how to use it. This learning process will enable users to transit from zero
knowledge about the innovation to making the best use of it. So far the effects
of user learning on technology adoption have received little research
attention. In this paper - for the first time - we investigate the effects of
user learning on forced innovation adoption by using an agent-based simulation
approach using the case of forced smart metering deployments in the city of
Leeds
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1718</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1718</id><created>2013-07-05</created><updated>2014-04-28</updated><authors><author><keyname>Treeratpituk</keyname><forenames>Pucktada</forenames></author><author><keyname>Khabsa</keyname><forenames>Madian</forenames></author><author><keyname>Giles</keyname><forenames>C. Lee</forenames></author></authors><title>Graph-based Approach to Automatic Taxonomy Generation (GraBTax)</title><categories>cs.IR</categories><acm-class>H.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel graph-based approach for constructing concept hierarchy
from a large text corpus. Our algorithm, GraBTax, incorporates both statistical
co-occurrences and lexical similarity in optimizing the structure of the
taxonomy. To automatically generate topic-dependent taxonomies from a large
text corpus, GraBTax first extracts topical terms and their relationships from
the corpus. The algorithm then constructs a weighted graph representing topics
and their associations. A graph partitioning algorithm is then used to
recursively partition the topic graph into a taxonomy. For evaluation, we apply
GraBTax to articles, primarily computer science, in the CiteSeerX digital
library and search engine. The quality of the resulting concept hierarchy is
assessed by both human judges and comparison with Wikipedia categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1719</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1719</id><created>2013-07-05</created><authors><author><keyname>Dagit</keyname><forenames>Jason</forenames></author><author><keyname>Sottile</keyname><forenames>Matthew</forenames></author></authors><title>Identifying change patterns in software history</title><categories>cs.SE</categories><comments>7 pages, submitted to document changes 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional algorithms for detecting differences in source code focus on
differences between lines. As such, little can be learned about abstract
changes that occur over time within a project. Structural differencing on the
program's abstract syntax tree reveals changes at the syntactic level within
code, which allows us to further process the differences to understand their
meaning. We propose that grouping of changes by some metric of similarity,
followed by pattern extraction via antiunification will allow us to identify
patterns of change within a software project from the sequence of changes
contained within a Version Control System (VCS). Tree similarity metrics such
as a tree edit distance can be used to group changes in order to identify
groupings that may represent a single class of change (e.g., adding a parameter
to a function call). By applying antiunification within each group we are able
to generalize from families of concrete changes to patterns of structural
change. Studying patterns of change at the structural level, instead of
line-by-line, allows us to gain insight into the evolution of software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1728</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1728</id><created>2013-07-05</created><authors><author><keyname>Bassino</keyname><forenames>Frederique</forenames></author><author><keyname>Sportiello</keyname><forenames>Andrea</forenames></author></authors><title>Linear-time generation of specifiable combinatorial structures: general
  theory and first examples</title><categories>cs.DS</categories><comments>10 pages + 5 title/biblio/append., submitted to SODA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various specifiable combinatorial structures, with d extensive parameters,
can be exactly sampled both by the recursive method, with linear arithmetic
complexity if a heavy preprocessing is performed, or by the Boltzmann method,
with complexity Theta(n^{1+d/2}). We discuss a modified recursive method,
crucially based on the asymptotic expansion of the associated saddle-point
integrals, which can be adopted for a large number of such structures (e.g.
partitions, permutations, lattice walks, trees, random graphs, all with a
variety of prescribed statistics and/or constraints). The new algorithm
requires no preprocessing, still it has linear complexity on average. In terms
of bit complexity, instead of arithmetic, we only have extra logarithmic
factors. For many families of structures, this provides, at our knowledge, the
only known quasi-linear generators. We present the general theory, and detail a
specific example: the partitions of n elements into k non-empty blocks, counted
by the Stirling numbers of the second kind. These objects are involved in the
exact sampling of minimal automata with prescribed alphabet size and number of
states, which is thus performed here with average Theta(n ln n) bit complexity,
outbreaking all previously known Theta(n^{3/2}) algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1730</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1730</id><created>2013-07-05</created><authors><author><keyname>Ageyev</keyname><forenames>D.</forenames><affiliation>TCS Department, Kharkiv National University of Radioelectronics, UKRAINE</affiliation></author><author><keyname>Ignatenko</keyname><forenames>A.</forenames><affiliation>TCS Department, Kharkiv National University of Radioelectronics, UKRAINE</affiliation></author><author><keyname>Wehbe</keyname><forenames>Fouad</forenames><affiliation>TCS Department, Kharkiv National University of Radioelectronics, UKRAINE</affiliation></author></authors><title>Design of Information and Telecommunication Systems with the Usage of
  the Multi-Layer Graph Model</title><categories>cs.NI</categories><comments>Keywords - Multilayer graph, Overlay network, Information and
  Telecommunication system, Design, Flow model</comments><journal-ref>in Proc. of XIIth International Conference The Experience of
  Designing and Application of CAD Systems in Microelectronics (CADSM),
  Lviv-Polyana, Ukraine, 2013. pp. 1-4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considered in the paper is the overlaid nature of modern telecommunication
networks and expediency of usage the multi-layer graph model for its design.
General statement of design problem is given. The design method in general
using multi-layer graph model and its application for a typical information and
telecommunication system is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1738</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1738</id><created>2013-07-05</created><authors><author><keyname>Wang</keyname><forenames>Yuting</forenames></author><author><keyname>Nadathur</keyname><forenames>Gopalan</forenames></author></authors><title>Towards Extracting Explicit Proofs from Totality Checking in Twelf</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Edinburgh Logical Framework (LF) is a dependently type lambda calculus
that can be used to encode formal systems. The versatility of LF allows
specifications to be constructed also about the encoded systems. The Twelf
system exploits the correspondence between formulas and types to give
specifications in LF a logic programming interpretation. By interpreting
particular arguments as input and others as output, specifications can be seen
as describing non-deterministic functions. If particular such functions can be
shown to be total, they represent constructive proofs of meta-theorems of the
encoded systems. Twelf provides a suite of tools for establishing totality.
However, all the resulting proofs of meta-theorems are implicit: Twelf's
totality checking does not yield a certificate that can be given to a proof
checker. We begin the process here of making these proofs explicit. We treat
the restricted situation in Twelf where context definitions (regular worlds)
and lemmas are not used. In this setting we describe and prove correct a
translation of the steps in totality checking into an actual proof in the
companion logic M2. We intend in the long term to extend our translation to all
of Twelf and to use this work as the basis for producing proofs in the related
Abella system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1739</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1739</id><created>2013-07-05</created><updated>2013-11-03</updated><authors><author><keyname>Zhao</keyname><forenames>Xin</forenames></author><author><keyname>Kaufman</keyname><forenames>Arie</forenames></author></authors><title>Anatomical Feature-guided Volumeric Registration of Multimodal Prostate
  MRI</title><categories>cs.CV cs.GR</categories><comments>This paper has been withdrawn by the author due to publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radiological imaging of prostate is becoming more popular among researchers
and clinicians in searching for diseases, primarily cancer. Scans might be
acquired at different times, with patient movement between scans, or with
different equipment, resulting in multiple datasets that need to be registered.
For this issue, we introduce a registration method using anatomical
feature-guided mutual information. Prostate scans of the same patient taken in
three different orientations are first aligned for the accurate detection of
anatomical features in 3D. Then, our pipeline allows for multiple modalities
registration through the use of anatomical features, such as the interior
urethra of prostate and gland utricle, in a bijective way. The novelty of this
approach is the application of anatomical features as the pre-specified
corresponding landmarks for prostate registration. We evaluate the registration
results through both artificial and clinical datasets. Registration accuracy is
evaluated by performing statistical analysis of local intensity differences or
spatial differences of anatomical landmarks between various MR datasets.
Evaluation results demonstrate that our method statistics-significantly
improves the quality of registration. Although this strategy is tested for
MRI-guided brachytherapy, the preliminary results from these experiments
suggest that it can be also applied to other settings such as transrectal
ultrasound-guided or CT-guided therapy, where the integration of preoperative
MRI may have a significant impact upon treatment planning and guidance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1743</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1743</id><created>2013-07-05</created><authors><author><keyname>Gow</keyname><forenames>Richard</forenames></author><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author><author><keyname>Ray</keyname><forenames>Pradeep</forenames></author></authors><title>&quot;The tail wags the dog&quot;: A study of anomaly detection in commercial
  application performance</title><categories>cs.PF cs.DC</categories><comments>10 pages; Longer version of the short paper accepted for MASCOTS 2013</comments><report-no>UNSW-CSE-TR-201317</report-no><acm-class>C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IT industry needs systems management models that leverage available
application information to detect quality of service, scalability and health of
service. Ideally this technique would be common for varying application types
with different n-tier architectures under normal production conditions of
varying load, user session traffic, transaction type, transaction mix, and
hosting environment.
  This paper shows that a whole of service measurement paradigm utilizing a
black box M/M/1 queuing model and auto regression curve fitting of the
associated CDF are an accurate model to characterize system performance
signatures. This modeling method is also used to detect application slow down
events. The technique was shown to work for a diverse range of workloads
ranging from 76 Tx/ 5min to 19,025 Tx/ 5min. The method did not rely on
customizations specific to the n-tier architecture of the systems being
analyzed and so the performance anomaly detection technique was shown to be
platform and configuration agnostic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1746</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1746</id><created>2013-07-06</created><authors><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author><author><keyname>Shen</keyname><forenames>Linzhi</forenames></author></authors><title>Generalized Quasi-Cyclic Codes Over $\mathbb{F}_q+u\mathbb{F}_q$</title><categories>cs.IT math.IT</categories><comments>17 pages</comments><msc-class>11T71, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized quasi-cyclic (GQC) codes with arbitrary lengths over the ring
$\mathbb{F}_{q}+u\mathbb{F}_{q}$, where $u^2=0$, $q=p^n$, $n$ a positive
integer and $p$ a prime number, are investigated. By the Chinese Remainder
Theorem, structural properties and the decomposition of GQC codes are given.
For 1-generator GQC codes, minimal generating sets and lower bounds on the
minimum distance are given. As a special class of GQC codes, quasi-cyclic (QC)
codes over $\mathbb{F}_q+u\mathbb{F}_q$ are also discussed briefly in this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1751</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1751</id><created>2013-07-06</created><updated>2014-05-20</updated><authors><author><keyname>Banerji</keyname><forenames>Sourangsu</forenames></author></authors><title>Study and Development of a Data Acquisition &amp; Control (DAQ) System using
  TCP/Modbus Protocol</title><categories>cs.SY cs.HC physics.ins-det</categories><comments>111 pages, 38 figures, 25 tables. Project Report of the winter
  research work done at Variable Energy Cyclotron Centre,India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the project was to develop a HMI (Human-Machine Interface) with
the help of which a person could remotely control and monitor the Vacuum
measurement system. The Vacuum measurement system was constructed using a DAQ
(Data Acquisition &amp; Control) implementation instead of a PLC based
implementation because of the cost involvement and complexity involved in
deployment when only one basic parameter i.e. vacuum is required to be
measured. The system is to be installed in the Superconducting Cyclotron
section of VECC. The need for remote monitoring arises as during the operation
of the K500 Superconducting Cyclotron, people are not allowed to enter within a
certain specified range due to effective ion radiation. Using the designed
software i.e. HMI the following objective of remote monitoring could be
achieved effortlessly from any area which is in the safe zone. Moreover the
software was designed in a way that data could be recorded real time and in an
unmanned way. The hardware is also easy to setup and overcomes the complexity
involved in interfacing a PLC with other hardware. The deployment time is also
quite fast. Lastly, the practical results obtained showed an appreciable degree
of accuracy of the system and friendliness with the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1756</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1756</id><created>2013-07-06</created><authors><author><keyname>Bo</keyname><forenames>Cheng</forenames></author><author><keyname>Jian</keyname><forenames>Xuesi</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author></authors><title>TEXIVE: Detecting Drivers Using Personal Smart Phones by Leveraging
  Inertial Sensors</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address a fundamental and critical task of detecting the
behavior of driving and texting using smartphones carried by users. We propose,
design, and implement TEXIVE that leverages various sensors integrated in the
smartphone and realizes our goal of distinguishing drivers and passengers and
detecting texting using rich user micro-movements and irregularities that can
be detected by sensors in the phone before and during driving and texting.
Without relying on external infrastructure, TEXIVE has an advantage of being
readily implemented and adopted, while at the same time raising a number of
challenges that need to be carefully addressed for achieving a successful
detection with good sensitivity, specificity, accuracy, and precision. Our
system distinguishes the driver and passengers by detecting whether a user is
entering a vehicle or not, inferring which side of the vehicle s/he is
entering, reasoning whether the user is siting in front or rear seats, and
discovering if a user is texting by fusing multiple evidences collected from
accelerometer, magnetometer, and gyroscope sensors. To validate our approach,
we conduct extensive experiments with several users on various vehicles and
smartphones. Our evaluation results show that TEXIVE has a classification
accuracy of 87.18%, and precision of 96.67%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1759</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1759</id><created>2013-07-06</created><updated>2013-07-09</updated><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Huang</keyname><forenames>Dayu</forenames></author><author><keyname>Kulkarni</keyname><forenames>Ankur A.</forenames></author><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author><author><keyname>Mehta</keyname><forenames>Prashant</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>Approximate dynamic programming using fluid and diffusion approximations
  with applications to power management</title><categories>cs.LG math.OC</categories><comments>Submitted to SIAM Journal on Control and Optimization (SICON), July
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuro-dynamic programming is a class of powerful techniques for approximating
the solution to dynamic programming equations. In their most computationally
attractive formulations, these techniques provide the approximate solution only
within a prescribed finite-dimensional function class. Thus, the question that
always arises is how should the function class be chosen? The goal of this
paper is to propose an approach using the solutions to associated fluid and
diffusion approximations. In order to illustrate this approach, the paper
focuses on an application to dynamic speed scaling for power management in
computer processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1766</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1766</id><created>2013-07-06</created><updated>2014-09-11</updated><authors><author><keyname>Filos-Ratsikas</keyname><forenames>Aris</forenames></author><author><keyname>Miltersen</keyname><forenames>Peter Bro</forenames></author></authors><title>Truthful approximations to range voting</title><categories>cs.GT</categories><msc-class>91A40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fundamental mechanism design problem of approximate social
welfare maximization under general cardinal preferences on a finite number of
alternatives and without money. The well-known range voting scheme can be
thought of as a non-truthful mechanism for exact social welfare maximization in
this setting. With m being the number of alternatives, we exhibit a randomized
truthful-in-expectation ordinal mechanism implementing an outcome whose
expected social welfare is at least an Omega(m^{-3/4}) fraction of the social
welfare of the socially optimal alternative. On the other hand, we show that
for sufficiently many agents and any truthful-in-expectation ordinal mechanism,
there is a valuation profile where the mechanism achieves at most an
O(m^{-{2/3}) fraction of the optimal social welfare in expectation. We get
tighter bounds for the natural special case of m = 3, and in that case
furthermore obtain separation results concerning the approximation ratios
achievable by natural restricted classes of truthful-in-expectation mechanisms.
In particular, we show that for m = 3 and a sufficiently large number of
agents, the best mechanism that is ordinal as well as mixed-unilateral has an
approximation ratio between 0.610 and 0.611, the best ordinal mechanism has an
approximation ratio between 0.616 and 0.641, while the best mixed-unilateral
mechanism has an approximation ratio bigger than 0.660. In particular, the best
mixed-unilateral non-ordinal (i.e., cardinal) mechanism strictly outperforms
all ordinal ones, even the non-mixed-unilateral ordinal ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1769</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1769</id><created>2013-07-06</created><authors><author><keyname>Rokach</keyname><forenames>Lior</forenames></author><author><keyname>Schclar</keyname><forenames>Alon</forenames></author><author><keyname>Itach</keyname><forenames>Ehud</forenames></author></authors><title>Ensemble Methods for Multi-label Classification</title><categories>stat.ML cs.LG</categories><msc-class>68T05, 68Q32</msc-class><acm-class>I.5; I.2.6; K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensemble methods have been shown to be an effective tool for solving
multi-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm,
each member of the ensemble is associated with a small randomly-selected subset
of k labels. Then, a single label classifier is trained according to each
combination of elements in the subset. In this paper we adopt a similar
approach, however, instead of randomly choosing subsets, we select the minimum
required subsets of k labels that cover all labels and meet additional
constraints such as coverage of inter-label correlations. Construction of the
cover is achieved by formulating the subset selection as a minimum set covering
problem (SCP) and solving it by using approximation algorithms. Every cover
needs only to be prepared once by offline algorithms. Once prepared, a cover
may be applied to the classification of any given multi-label dataset whose
properties conform with those of the cover. The contribution of this paper is
two-fold. First, we introduce SCP as a general framework for constructing label
covers while allowing the user to incorporate cover construction constraints.
We demonstrate the effectiveness of this framework by proposing two
construction constraints whose enforcement produces covers that improve the
prediction performance of random selection. Second, we provide theoretical
bounds that quantify the probabilities of random selection to produce covers
that meet the proposed construction criteria. The experimental results indicate
that the proposed methods improve multi-label classification accuracy and
stability compared with the RAKEL algorithm and to other state-of-the-art
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1770</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1770</id><created>2013-07-06</created><updated>2015-07-10</updated><authors><author><keyname>Karahanoglu</keyname><forenames>Nazim Burak</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author></authors><title>Improving A*OMP: Theoretical and Empirical Analyses With a Novel Dynamic
  Cost Model</title><categories>cs.IT math.IT</categories><journal-ref>Signal Processing 118 (2016) 62-74</journal-ref><doi>10.1016/j.sigpro.2015.06.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Best-first search has been recently utilized for compressed sensing (CS) by
the A* orthogonal matching pursuit (A*OMP) algorithm. In this work, we
concentrate on theoretical and empirical analyses of A*OMP. We present a
restricted isometry property (RIP) based general condition for exact recovery
of sparse signals via A*OMP. In addition, we develop online guarantees which
promise improved recovery performance with the residue-based termination
instead of the sparsity-based one. We demonstrate the recovery capabilities of
A*OMP with extensive recovery simulations using the adaptive-multiplicative
(AMul) cost model, which e?ectively compensates for the path length differences
in the search tree. The presented results, involving phase transitions for
di?erent nonzero element distributions as well as recovery rates and average
error, reveal not only the superior recovery accuracy of A*OMP, but also the
improvements with the residue-based termination and the AMul cost model.
Comparison of the run times indicate the speed up by the AMul cost model. We
also demonstrate a hybrid of OMP and A?OMP to accelerate the search further.
Finally, we run A*OMP on a sparse image to illustrate its recovery performance
for more realistic coefcient distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1772</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1772</id><created>2013-07-06</created><authors><author><keyname>Jain</keyname><forenames>Surabhi</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author></authors><title>Simpler Sequential and Parallel Biconnectivity Augmentation</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a connected graph, a vertex separator is a set of vertices whose removal
creates at least two components and a minimum vertex separator is a vertex
separator of least cardinality. The vertex connectivity refers to the size of a
minimum vertex separator. For a connected graph $G$ with vertex connectivity $k
(k \geq 1)$, the connectivity augmentation refers to a set $S$ of edges whose
augmentation to $G$ increases its vertex connectivity by one. A minimum
connectivity augmentation of $G$ is the one in which $S$ is minimum. In this
paper, we focus our attention on connectivity augmentation of trees. Towards
this end, we present a new sequential algorithm for biconnectivity augmentation
in trees by simplifying the algorithm reported in \cite{nsn}. The simplicity is
achieved with the help of edge contraction tool. This tool helps us in getting
a recursive subproblem preserving all connectivity information. Subsequently,
we present a parallel algorithm to obtain a minimum connectivity augmentation
set in trees. Our parallel algorithm essentially follows the overall structure
of sequential algorithm. Our implementation is based on CREW PRAM model with
$O(\Delta)$ processors, where $\Delta$ refers to the maximum degree of a tree.
We also show that our parallel algorithm is optimal whose processor-time
product is O(n) where $n$ is the number of vertices of a tree, which is an
improvement over the parallel algorithm reported in \cite{hsu}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1774</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1774</id><created>2013-07-06</created><authors><author><keyname>Adamaszek</keyname><forenames>Anna</forenames></author><author><keyname>Wiese</keyname><forenames>Andreas</forenames></author></authors><title>Approximation Schemes for Maximum Weight Independent Set of Rectangles</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Maximum Weight Independent Set of Rectangles (MWISR) problem we are
given a set of n axis-parallel rectangles in the 2D-plane, and the goal is to
select a maximum weight subset of pairwise non-overlapping rectangles. Due to
many applications, e.g. in data mining, map labeling and admission control, the
problem has received a lot of attention by various research communities. We
present the first (1+epsilon)-approximation algorithm for the MWISR problem
with quasi-polynomial running time 2^{poly(log n/epsilon)}. In contrast, the
best known polynomial time approximation algorithms for the problem achieve
superconstant approximation ratios of O(log log n) (unweighted case) and O(log
n / log log n) (weighted case).
  Key to our results is a new geometric dynamic program which recursively
subdivides the plane into polygons of bounded complexity. We provide the
technical tools that are needed to analyze its performance. In particular, we
present a method of partitioning the plane into small and simple areas such
that the rectangles of an optimal solution are intersected in a very controlled
manner. Together with a novel application of the weighted planar graph
separator theorem due to Arora et al. this allows us to upper bound our
approximation ratio by (1+epsilon).
  Our dynamic program is very general and we believe that it will be useful for
other settings. In particular, we show that, when parametrized properly, it
provides a polynomial time (1+epsilon)-approximation for the special case of
the MWISR problem when each rectangle is relatively large in at least one
dimension. Key to this analysis is a method to tile the plane in order to
approximately describe the topology of these rectangles in an optimal solution.
This technique might be a useful insight to design better polynomial time
approximation algorithms or even a PTAS for the MWISR problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1786</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1786</id><created>2013-07-06</created><authors><author><keyname>Shi</keyname><forenames>Minjia</forenames></author></authors><title>MacWilliams type identities for some new $m$-spotty weight enumerators
  over finite commutative Frobenius rings</title><categories>cs.IT math.IT</categories><comments>Research article, under review since 30th March 2013. 18 pages,6
  Tables. arXiv admin note: text overlap with arXiv:1109.3800 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Past few years have seen an extensive use of RAM chips with wide I/O data
(e.g. 16, 32, 64 bits) in computer memory systems. These chips are highly
vulnerable to a special type of byte error, called an $m$-spotty byte error,
which can be effectively detected or corrected using byte error-control codes.
The MacWilliams identity provides the relationship between the weight
distribution of a code and that of its dual. This paper introduces $m$-spotty
Hamming weight enumerator, joint $m$-spotty Hamming weight enumerator and split
$m$-spotty Hamming weight enumerator for byte error-control codes over finite
commutative Frobenius rings as well as $m$-spotty Lee weight enumerator over an
infinite family of rings. In addition, MacWilliams type identities are also
derived for these enumerators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1790</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1790</id><created>2013-07-06</created><authors><author><keyname>Thorstensen</keyname><forenames>Evgenij</forenames></author></authors><title>Lifting Structural Tractability to CSP with Global Constraints</title><categories>cs.AI</categories><comments>To appear in proceedings of CP'13, LNCS 8124</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of problems can be modelled as constraint satisfaction problems
(CSPs), that is, a set of constraints that must be satisfied simultaneously.
Constraints can either be represented extensionally, by explicitly listing
allowed combinations of values, or implicitly, by special-purpose algorithms
provided by a solver. Such implicitly represented constraints, known as global
constraints, are widely used; indeed, they are one of the key reasons for the
success of constraint programming in solving real-world problems.
  In recent years, a variety of restrictions on the structure of CSP instances
that yield tractable classes have been identified. However, many such
restrictions fail to guarantee tractability for CSPs with global constraints.
In this paper, we investigate the properties of extensionally represented
constraints that these restrictions exploit to achieve tractability, and show
that there are large classes of global constraints that also possess these
properties. This allows us to lift these restrictions to the global case, and
identify new tractable classes of CSPs with global constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1805</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1805</id><created>2013-07-06</created><updated>2013-09-20</updated><authors><author><keyname>Scquizzato</keyname><forenames>Michele</forenames></author><author><keyname>Silvestri</keyname><forenames>Francesco</forenames></author></authors><title>Communication Lower Bounds for Distributed-Memory Computations</title><categories>cs.DS cs.DC</categories><comments>Minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give lower bounds on the communication complexity required to solve
several computational problems in a distributed-memory parallel machine, namely
standard matrix multiplication, stencil computations, comparison sorting, and
the Fast Fourier Transform. We revisit the assumptions under which preceding
results were derived and provide new lower bounds which use much weaker and
appropriate hypotheses. Our bounds rely on a mild assumption on work
distribution, and strengthen previous results which require either the
computation to be balanced among the processors, or specific initial
distributions of the input data, or an upper bound on the size of processors'
local memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1827</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1827</id><created>2013-07-06</created><updated>2014-07-08</updated><authors><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author></authors><title>Loss minimization and parameter estimation with heavy tails</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies applications and generalizations of a simple estimation
technique that provides exponential concentration under heavy-tailed
distributions, assuming only bounded low-order moments. We show that the
technique can be used for approximate minimization of smooth and strongly
convex losses, and specifically for least squares linear regression. For
instance, our $d$-dimensional estimator requires just
$\tilde{O}(d\log(1/\delta))$ random samples to obtain a constant factor
approximation to the optimal least squares loss with probability $1-\delta$,
without requiring the covariates or noise to be bounded or subgaussian. We
provide further applications to sparse linear regression and low-rank
covarinace matrix estimation with similar allowances on the noise and covariate
distributions. The core technique is a generalization of the median-of-means
estimator to arbitrary metric spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1829</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1829</id><created>2013-07-06</created><updated>2013-10-04</updated><authors><author><keyname>Zafeiris</keyname><forenames>Anna</forenames></author><author><keyname>Vicsek</keyname><forenames>Tamas</forenames></author></authors><title>Group performance is maximized by hierarchical competence distribution</title><categories>physics.soc-ph cs.SI</categories><comments>34 pages</comments><journal-ref>Nature Communications, Vol. 4, Article number 2484. Published 18
  September 2013</journal-ref><doi>10.1038/ncomms3484</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Groups of people or even robots often face problems they need to solve
together. Examples include collectively searching for resources, choosing when
and where to invest time and effort, and many more. Although a hierarchical
ordering of the relevance of the group members' inputs during collective
decision making is abundant, a quantitative demonstration of its origin and
advantages using a generic approach has not been described yet. Here we
introduce a family of models based on the most general features of group
decision making to show that the optimal distribution of competences is a
highly skewed function with a structured fat tail. Our results have been
obtained by optimizing the groups' compositions through identifying the best
performing distributions for both the competences and for the members'
flexibilities/pliancies. Potential applications include choosing the best
composition for a group intended to solve a given task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1834</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1834</id><created>2013-07-07</created><authors><author><keyname>Zhao</keyname><forenames>Dawei</forenames></author><author><keyname>Li</keyname><forenames>Lixiang</forenames></author><author><keyname>Peng</keyname><forenames>Haipeng</forenames></author><author><keyname>Luo</keyname><forenames>Qun</forenames></author><author><keyname>Yang</keyname><forenames>Yixian</forenames></author></authors><title>Multiple Vectors Propagation of Epidemics in Complex Networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter investigates the epidemic spreading in two-vectors propagation
network (TPN). We propose detailed theoretical analysis that allows us to
accurately calculate the epidemic threshold and outbreak size. It is found that
the epidemics can spread across the TPN even if two sub-single-vector
propagation networks (SPNs) of TPN are well below their respective epidemic
thresholds. Strong positive degree-degree correlation of nodes in TPN could
lead to a much lower epidemic threshold and a relatively smaller outbreak size.
However, the average similarity between the neighbors from different SPNs of
nodes has no effect on the epidemic threshold and outbreak size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1850</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1850</id><created>2013-07-07</created><updated>2014-06-02</updated><authors><author><keyname>Pauly</keyname><forenames>Arno</forenames></author><author><keyname>de Brecht</keyname><forenames>Matthew</forenames></author></authors><title>Towards Synthetic Descriptive Set Theory: An instantiation with
  represented spaces</title><categories>cs.LO math.CT math.LO</categories><msc-class>03E15, 54H05, 03D78, 18A99</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using ideas from synthetic topology, a new approach to descriptive set theory
is suggested. Synthetic descriptive set theory promises elegant explanations
for various phenomena in both classic and effective descriptive set theory.
Presently, we mainly focus on developing the ideas in the category of
represented spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1870</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1870</id><created>2013-07-07</created><authors><author><keyname>Mouret</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Koos</keyname><forenames>Sylvain</forenames></author><author><keyname>Doncieux</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Crossing the Reality Gap: a Short Introduction to the Transferability
  Approach</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In robotics, gradient-free optimization algorithms (e.g. evolutionary
algorithms) are often used only in simulation because they require the
evaluation of many candidate solutions. Nevertheless, solutions obtained in
simulation often do not work well on the real device. The transferability
approach aims at crossing this gap between simulation and reality by
\emph{making the optimization algorithm aware of the limits of the simulation}.
  In the present paper, we first describe the transferability function, that
maps solution descriptors to a score representing how well a simulator matches
the reality. We then show that this function can be learned using a regression
algorithm and a few experiments with the real devices. Our results are
supported by an extensive study of the reality gap for a simple quadruped robot
whose control parameters are optimized. In particular, we mapped the whole
search space in reality and in simulation to understand the differences between
the fitness landscapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1872</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1872</id><created>2013-07-07</created><authors><author><keyname>Sabek</keyname><forenames>Ibrahim</forenames></author><author><keyname>Yousri</keyname><forenames>Noha A.</forenames></author><author><keyname>Elmakky</keyname><forenames>Nagwa</forenames></author><author><keyname>Habib</keyname><forenames>Mona</forenames></author></authors><title>Intelligent Hybrid Man-Machine Translation Quality Estimation</title><categories>cs.CL</categories><comments>8 pages, 3 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferring evaluation scores based on human judgments is invaluable compared
to using current evaluation metrics which are not suitable for real-time
applications e.g. post-editing. However, these judgments are much more
expensive to collect especially from expert translators, compared to evaluation
based on indicators contrasting source and translation texts. This work
introduces a novel approach for quality estimation by combining learnt
confidence scores from a probabilistic inference model based on human
judgments, with selective linguistic features-based scores, where the proposed
inference model infers the credibility of given human ranks to solve the
scarcity and inconsistency issues of human judgments. Experimental results,
using challenging language-pairs, demonstrate improvement in correlation with
human judgments over traditional evaluation metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1879</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1879</id><created>2013-07-07</created><authors><author><keyname>Nedich</keyname><forenames>Angelia</forenames></author><author><keyname>Lee</keyname><forenames>Soomin</forenames></author></authors><title>On Stochastic Subgradient Mirror-Descent Algorithm with Weighted
  Averaging</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers stochastic subgradient mirror-descent method for solving
constrained convex minimization problems. In particular, a stochastic
subgradient mirror-descent method with weighted iterate-averaging is
investigated and its per-iterate convergence rate is analyzed. The novel part
of the approach is in the choice of weights that are used to construct the
averages. Through the use of these weighted averages, we show that the known
optimal rates can be obtained with simpler algorithms than those currently
existing in the literature. Specifically, by suitably choosing the stepsize
values, one can obtain the rate of the order $1/k$ for strongly convex
functions, and the rate $1/\sqrt{k}$ for general convex functions (not
necessarily differentiable). Furthermore, for the latter case, it is shown that
a stochastic subgradient mirror-descent with iterate averaging converges (along
a subsequence) to an optimal solution, almost surely, even with the stepsize of
the form $1/\sqrt{1+k}$, which was not previously known. The stepsize choices
that achieve the best rates are those proposed by Paul Tseng for acceleration
of proximal gradient methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1890</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1890</id><created>2013-07-07</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author></authors><title>Solution of Rectangular Fuzzy Games by Principle of Dominance Using
  LR-type Trapezoidal Fuzzy Numbers</title><categories>cs.AI</categories><comments>Proceedings of 2nd International Conference on Advanced Computing &amp;
  Communication Technologies, Asia Pacific Institute of Information Technology,
  Panipat, Haryana, India, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy Set Theory has been applied in many fields such as Operations Research,
Control Theory, and Management Sciences etc. In particular, an application of
this theory in Managerial Decision Making Problems has a remarkable
significance. In this Paper, we consider a solution of Rectangular Fuzzy game
with pay-off as imprecise numbers instead of crisp numbers viz., interval and
LR-type Trapezoidal Fuzzy Numbers. The solution of such Fuzzy games with pure
strategies by minimax-maximin principle is discussed. The Algebraic Method to
solve Fuzzy games without saddle point by using mixed strategies is also
illustrated. Here, pay-off matrix is reduced to pay-off matrix by Dominance
Method. This fact is illustrated by means of Numerical Example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1891</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1891</id><created>2013-07-07</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author><author><keyname>De</keyname><forenames>Kajal</forenames></author></authors><title>A Comparative study of Transportation Problem under Probabilistic and
  Fuzzy Uncertainties</title><categories>cs.AI</categories><comments>GANIT, Journal of Bangladesh Mathematical Society, Bangladesh
  Mathematical Society, Dhaka, Bangladesh, 2010 (In Press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transportation Problem is an important aspect which has been widely studied
in Operations Research domain. It has been studied to simulate different real
life problems. In particular, application of this Problem in NP- Hard Problems
has a remarkable significance. In this Paper, we present a comparative study of
Transportation Problem through Probabilistic and Fuzzy Uncertainties. Fuzzy
Logic is a computational paradigm that generalizes classical two-valued logic
for reasoning under uncertainty. In order to achieve this, the notation of
membership in a set needs to become a matter of degree. By doing this we
accomplish two things viz., (i) ease of describing human knowledge involving
vague concepts and (ii) enhanced ability to develop cost-effective solution to
real-world problem. The multi-valued nature of Fuzzy Sets allows handling
uncertain and vague information. It is a model-less approach and a clever
disguise of Probability Theory. We give comparative simulation results of both
approaches and discuss the Computational Complexity. To the best of our
knowledge, this is the first work on comparative study of Transportation
Problem using Probabilistic and Fuzzy Uncertainties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1893</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1893</id><created>2013-07-07</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author><author><keyname>De</keyname><forenames>Kajal</forenames></author><author><keyname>Chatterjee</keyname><forenames>Dipak</forenames></author><author><keyname>Mitra</keyname><forenames>Pabitra</forenames></author></authors><title>Trapezoidal Fuzzy Numbers for the Transportation Problem</title><categories>cs.AI</categories><comments>International Journal of Intelligent Computing and Applications,
  Volume 1, Number 2, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transportation Problem is an important problem which has been widely studied
in Operations Research domain. It has been often used to simulate different
real life problems. In particular, application of this Problem in NP Hard
Problems has a remarkable significance. In this Paper, we present the closed,
bounded and non empty feasible region of the transportation problem using fuzzy
trapezoidal numbers which ensures the existence of an optimal solution to the
balanced transportation problem. The multivalued nature of Fuzzy Sets allows
handling of uncertainty and vagueness involved in the cost values of each cells
in the transportation table. For finding the initial solution of the
transportation problem we use the Fuzzy Vogel Approximation Method and for
determining the optimality of the obtained solution Fuzzy Modified Distribution
Method is used. The fuzzification of the cost of the transportation problem is
discussed with the help of a numerical example. Finally, we discuss the
computational complexity involved in the problem. To the best of our knowledge,
this is the first work on obtaining the solution of the transportation problem
using fuzzy trapezoidal numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1895</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1895</id><created>2013-07-07</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author><author><keyname>De</keyname><forenames>Kajal</forenames></author><author><keyname>Chatterjee</keyname><forenames>Dipak</forenames></author></authors><title>Discovering Stock Price Prediction Rules of Bombay Stock Exchange Using
  Rough Fuzzy Multi Layer Perception Networks</title><categories>cs.AI</categories><comments>Book Chapter: Forecasting Financial Markets in India, Rudra P.
  Pradhan, Indian Institute of Technology Kharagpur, (Editor), Allied
  Publishers, India, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In India financial markets have existed for many years. A functionally
accented, diverse, efficient and flexible financial system is vital to the
national objective of creating a market driven, productive and competitive
economy. Today markets of varying maturity exist in equity, debt, commodities
and foreign exchange. In this work we attempt to generate prediction rules
scheme for stock price movement at Bombay Stock Exchange using an important
Soft Computing paradigm viz., Rough Fuzzy Multi Layer Perception. The use of
Computational Intelligence Systems such as Neural Networks, Fuzzy Sets, Genetic
Algorithms, etc. for Stock Market Predictions has been widely established. The
process is to extract knowledge in the form of rules from daily stock
movements. These rules can then be used to guide investors. To increase the
efficiency of the prediction process, Rough Sets is used to discretize the
data. The methodology uses a Genetic Algorithm to obtain a structured network
suitable for both classification and rule extraction. The modular concept,
based on divide and conquer strategy, provides accelerated training and a
compact network suitable for generating a minimum number of rules with high
certainty values. The concept of variable mutation operator is introduced for
preserving the localized structure of the constituting Knowledge Based
sub-networks, while they are integrated and evolved. Rough Set Dependency Rules
are generated directly from the real valued attribute table containing Fuzzy
membership values. The paradigm is thus used to develop a rule extraction
algorithm. The extracted rules are compared with some of the related rule
extraction techniques on the basis of some quantitative performance indices.
The proposed methodology extracts rules which are less in number, are accurate,
have high certainty factor and have low confusion with less computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1900</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1900</id><created>2013-07-07</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author><author><keyname>De</keyname><forenames>Kajal</forenames></author></authors><title>Fuzzy Integer Linear Programming Mathematical Models for Examination
  Timetable Problem</title><categories>cs.AI</categories><comments>International Journal of Innovative Computing, Information and
  Control (Special Issue), Volume 7, Number 5, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ETP is NP Hard combinatorial optimization problem. It has received tremendous
research attention during the past few years given its wide use in
universities. In this Paper, we develop three mathematical models for NSOU,
Kolkata, India using FILP technique. To deal with impreciseness and vagueness
we model various allocation variables through fuzzy numbers. The solution to
the problem is obtained using Fuzzy number ranking method. Each feasible
solution has fuzzy number obtained by Fuzzy objective function. The different
FILP technique performance are demonstrated by experimental data generated
through extensive simulation from NSOU, Kolkata, India in terms of its
execution times. The proposed FILP models are compared with commonly used
heuristic viz. ILP approach on experimental data which gives an idea about
quality of heuristic. The techniques are also compared with different
Artificial Intelligence based heuristics for ETP with respect to best and mean
cost as well as execution time measures on Carter benchmark datasets to
illustrate its effectiveness. FILP takes an appreciable amount of time to
generate satisfactory solution in comparison to other heuristics. The
formulation thus serves as good benchmark for other heuristics. The
experimental study presented here focuses on producing a methodology that
generalizes well over spectrum of techniques that generates significant results
for one or more datasets. The performance of FILP model is finally compared to
the best results cited in literature for Carter benchmarks to assess its
potential. The problem can be further reduced by formulating with lesser number
of allocation variables it without affecting optimality of solution obtained.
FLIP model for ETP can also be adapted to solve other ETP as well as
combinatorial optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1903</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1903</id><created>2013-07-07</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author><author><keyname>De</keyname><forenames>Kajal</forenames></author></authors><title>Achieving greater Explanatory Power and Forecasting Accuracy with
  Non-uniform spread Fuzzy Linear Regression</title><categories>cs.AI</categories><comments>Proceedings of 13th Conference of Society of Operations Management,
  Department of Management Studies, Indian Institute of Technology, Madras,
  Tamilnadu, India, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy regression models have been applied to several Operations Research
applications viz., forecasting and prediction. Earlier works on fuzzy
regression analysis obtain crisp regression coefficients for eliminating the
problem of increasing spreads for the estimated fuzzy responses as the
magnitude of the independent variable increases. But they cannot deal with the
problem of non-uniform spreads. In this work, a three-phase approach is
discussed to construct the fuzzy regression model with non-uniform spreads to
deal with this problem. The first phase constructs the membership functions of
the least-squares estimates of regression coefficients based on extension
principle to completely conserve the fuzziness of observations. They are then
defuzzified by the centre of area method to obtain crisp regression
coefficients in the second phase. Finally, the error terms of the method are
determined by setting each estimated spread equal to its corresponding observed
spread. The Tagaki-Sugeno inference system is used for improving the accuracy
of forecasts. The simulation example demonstrates the strength of fuzzy linear
regression model in terms of higher explanatory power and forecasting
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1905</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1905</id><created>2013-07-07</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author></authors><title>A Dynamic Algorithm for the Longest Common Subsequence Problem using Ant
  Colony Optimization Technique</title><categories>cs.AI</categories><comments>Proceedings of 2nd International Conference on Mathematics: Trends
  and Developments, Al Azhar University, Cairo, Egypt, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a dynamic algorithm for solving the Longest Common Subsequence
Problem using Ant Colony Optimization Technique. The Ant Colony Optimization
Technique has been applied to solve many problems in Optimization Theory,
Machine Learning and Telecommunication Networks etc. In particular, application
of this theory in NP-Hard Problems has a remarkable significance. Given two
strings, the traditional technique for finding Longest Common Subsequence is
based on Dynamic Programming which consists of creating a recurrence relation
and filling a table of size . The proposed algorithm draws analogy with
behavior of ant colonies function and this new computational paradigm is known
as Ant System. It is a viable new approach to Stochastic Combinatorial
Optimization. The main characteristics of this model are positive feedback,
distributed computation, and the use of constructive greedy heuristic. Positive
feedback accounts for rapid discovery of good solutions, distributed
computation avoids premature convergence and greedy heuristic helps find
acceptable solutions in minimum number of stages. We apply the proposed
methodology to Longest Common Subsequence Problem and give the simulation
results. The effectiveness of this approach is demonstrated by efficient
Computational Complexity. To the best of our knowledge, this is the first Ant
Colony Optimization Algorithm for Longest Common Subsequence Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1915</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1915</id><created>2013-07-07</created><updated>2015-10-15</updated><authors><author><keyname>Gurski</keyname><forenames>Frank</forenames></author><author><keyname>Rethmann</keyname><forenames>Jochen</forenames></author><author><keyname>Wanke</keyname><forenames>Egon</forenames></author></authors><title>Complexity of the FIFO Stack-Up Problem</title><categories>cs.DS cs.CC</categories><comments>18 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the combinatorial FIFO stack-up problem. In delivery industry, bins
have to be stacked-up from conveyor belts onto pallets with respect to customer
orders. Given k sequences q_1, ..., q_k of labeled bins and a positive integer
p, the aim is to stack-up the bins by iteratively removing the first bin of one
of the k sequences and put it onto an initially empty pallet of unbounded
capacity located at one of p stack-up places. Bins with different pallet labels
have to be placed on different pallets, bins with the same pallet label have to
be placed on the same pallet. After all bins for a pallet have been removed
from the given sequences, the corresponding stack-up place will be cleared and
becomes available for a further pallet. The FIFO stack-up problem is to find a
stack-up sequence such that all pallets can be build-up with the available p
stack-up places. In this paper, we introduce two digraph models for the FIFO
stack-up problem, namely the processing graph and the sequence graph. We show
that there is a processing of some list of sequences with at most p stack-up
places if and only if the sequence graph of this list has directed pathwidth at
most p-1. This connection implies that the FIFO stack-up problem is NP-complete
in general, even if there are at most 6 bins for every pallet and that the
problem can be solved in polynomial time, if the number p of stack-up places is
assumed to be fixed. Further the processing graph allows us to show that the
problem can be solved in polynomial time, if the number k of sequences is
assumed to be fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1926</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1926</id><created>2013-07-07</created><authors><author><keyname>Fayazbakhsh</keyname><forenames>Seyed Kaveh</forenames></author></authors><title>Modeling Human Mobility and its Applications in Routing in
  Delay-Tolerant Networks: a Short Survey</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human mobility patterns are complex and distinct from one person to another.
Nevertheless, motivated by tremendous potential benefits of modeling such
patterns in enabling new mobile services and technologies, researchers have
attempted to capture salient characteristics of human mobility. In this short
survey paper, we review some of the major techniques for modeling humans'
co-location, as well as predicting human location and trajectory. Further, we
review one of the most important application areas of such models, namely,
routing in delay-tolerant networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1927</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1927</id><created>2013-07-07</created><authors><author><keyname>Bayir</keyname><forenames>Murat Ali</forenames></author><author><keyname>Toroslu</keyname><forenames>Ismail Hakki</forenames></author></authors><title>Link Based Session Reconstruction: Finding All Maximal Paths</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new method for the session construction problem,
which is the first main step of the web usage mining process. Through
experiments, it is shown that when our new technique is used, it outperforms
previous approaches in web usage mining applications such as next-page
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1940</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1940</id><created>2013-07-07</created><authors><author><keyname>Frolov</keyname><forenames>Vladimir</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Chertkov</keyname><forenames>Misha</forenames></author></authors><title>Reinforcing Power Grid Transmission with FACTS Devices</title><categories>math.OC cs.SY physics.soc-ph</categories><comments>12 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore optimization methods for planning the placement, sizing and
operations of Flexible Alternating Current Transmission System (FACTS) devices
installed into the grid to relieve congestion created by load growth or
fluctuations of intermittent renewable generation. We limit our selection of
FACTS devices to those that can be represented by modification of the
inductance of the transmission lines. Our master optimization problem minimizes
the $l_1$ norm of the FACTS-associated inductance correction subject to
constraints enforcing that no line of the system exceeds its thermal limit. We
develop off-line heuristics that reduce this non-convex optimization to a
succession of Linear Programs (LP) where at each step the constraints are
linearized analytically around the current operating point. The algorithm is
accelerated further with a version of the cutting plane method greatly reducing
the number of active constraints during the optimization, while checking
feasibility of the non-active constraints post-factum. This hybrid algorithm
solves a typical single-contingency problem over the MathPower Polish Grid
model (3299 lines and 2746 nodes) in 40 seconds per iteration on a standard
laptop---a speed up that allows the sizing and placement of a family of FACTS
devices to correct a large set of anticipated contingencies. From testing of
multiple examples, we observe that our algorithm finds feasible solutions that
are always sparse, i.e., FACTS devices are placed on only a few lines. The
optimal FACTS are not always placed on the originally congested lines, however
typically the correction(s) is made at line(s) positioned in a relative
proximity of the overload line(s).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1942</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1942</id><created>2013-07-08</created><authors><author><keyname>Dunchev</keyname><forenames>Cvetan</forenames><affiliation>Institute of Computer Languages</affiliation></author><author><keyname>Leitsch</keyname><forenames>Alexander</forenames><affiliation>Institute of Computer Languages</affiliation></author><author><keyname>Libal</keyname><forenames>Tomer</forenames><affiliation>Institute of Computer Languages</affiliation></author><author><keyname>Riener</keyname><forenames>Martin</forenames><affiliation>Institute of Computer Languages</affiliation></author><author><keyname>Rukhaia</keyname><forenames>Mikheil</forenames><affiliation>Institute of Computer Languages</affiliation></author><author><keyname>Weller</keyname><forenames>Daniel</forenames><affiliation>Institute of Discrete Mathematics and Geometry</affiliation></author><author><keyname>Woltzenlogel-Paleo</keyname><forenames>Bruno</forenames><affiliation>Institute of Computer Languages</affiliation></author></authors><title>PROOFTOOL: a GUI for the GAPT Framework</title><categories>cs.LO cs.HC cs.MS</categories><comments>In Proceedings UITP 2012, arXiv:1307.1528</comments><proxy>EPTCS</proxy><acm-class>F.4.1; I.2.3</acm-class><journal-ref>EPTCS 118, 2013, pp. 1-14</journal-ref><doi>10.4204/EPTCS.118.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces PROOFTOOL, the graphical user interface for the General
Architecture for Proof Theory (GAPT) framework. Its features are described with
a focus not only on the visualization but also on the analysis and
transformation of proofs and related tree-like structures, and its
implementation is explained. Finally, PROOFTOOL is compared with three other
graphical interfaces for proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1943</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1943</id><created>2013-07-08</created><authors><author><keyname>Tankink</keyname><forenames>Carst</forenames><affiliation>Institute for Computing and Information Science, Radboud University Nijmegen</affiliation></author></authors><title>Proof in Context -- Web Editing with Rich, Modeless Contextual Feedback</title><categories>cs.HC cs.LO cs.SE</categories><comments>In Proceedings UITP 2012, arXiv:1307.1528</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 118, 2013, pp. 42-56</journal-ref><doi>10.4204/EPTCS.118.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Agora system is a prototypical Wiki for formal mathematics: a web-based
system for collaborating on formal mathematics, intended to support informal
documentation of formal developments. This system requires a reusable proof
editor component, both for collaborative editing of documents, and for
embedding in the resulting documents. This paper describes the design of
Agora's asynchronous editor, that is generic enough to support different tools
working on editor content and providing contextual information, with
interactive theorem proverss being a special, but important, case described in
detail for the Coq theorem prover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1944</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1944</id><created>2013-07-08</created><authors><author><keyname>Wenzel</keyname><forenames>Makarius</forenames></author></authors><title>READ-EVAL-PRINT in Parallel and Asynchronous Proof-checking</title><categories>cs.LO cs.AI cs.HC</categories><comments>In Proceedings UITP 2012, arXiv:1307.1528</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 118, 2013, pp. 57-71</journal-ref><doi>10.4204/EPTCS.118.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The LCF tradition of interactive theorem proving, which was started by Milner
in the 1970-ies, appears to be tied to the classic READ-EVAL-PRINT-LOOP of
sequential and synchronous evaluation of prover commands. We break up this loop
and retrofit the read-eval-print phases into a model of parallel and
asynchronous proof processing. Thus we explain some key concepts of the
Isabelle/Scala approach to prover interaction and integration, and the
Isabelle/jEdit Prover IDE as front-end technology. We hope to open up the
scientific discussion about non-trivial interaction models for ITP systems
again, and help getting other old-school proof assistants on a similar track.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1945</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1945</id><created>2013-07-08</created><authors><author><keyname>Windsteiger</keyname><forenames>Wolfgang</forenames><affiliation>RISC, JKU Linz, Austria</affiliation></author></authors><title>Theorema 2.0: A Graphical User Interface for a Mathematical Assistant
  System</title><categories>cs.MS cs.HC cs.SC</categories><comments>In Proceedings UITP 2012, arXiv:1307.1528</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 118, 2013, pp. 72-82</journal-ref><doi>10.4204/EPTCS.118.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theorema 2.0 stands for a re-design including a complete re-implementation of
the Theorema system, which was originally designed, developed, and implemented
by Bruno Buchberger and his Theorema group at RISC. In this paper, we present
the first prototype of a graphical user interface (GUI) for the new system. It
heavily relies on powerful interactive capabilities introduced in recent
releases of the underlying Mathematica system, most importantly the possibility
of having dynamic objects connected to interface elements like sliders, menus,
check-boxes, radio-buttons and the like. All these features are fully
integrated into the Mathematica programming environment and allow the
implementation of a modern user interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1949</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1949</id><created>2013-07-08</created><updated>2015-07-01</updated><authors><author><keyname>Yang</keyname><forenames>Mingrui</forenames></author><author><keyname>de Hoog</keyname><forenames>Frank</forenames></author></authors><title>Orthogonal Matching Pursuit with Thresholding and its Application in
  Compressive Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Greed is good. However, the tighter you squeeze, the less you have. In this
paper, a less greedy algorithm for sparse signal reconstruction in compressive
sensing, named orthogonal matching pursuit with thresholding is studied. Using
the global 2-coherence , which provides a &quot;bridge&quot; between the well known
mutual coherence and the restricted isometry constant, the performance of
orthogonal matching pursuit with thresholding is analyzed and more general
results for sparse signal reconstruction are obtained. It is also shown that
given the same assumption on the coherence index and the restricted isometry
constant as required for orthogonal matching pursuit, the thresholding
variation gives exactly the same reconstruction performance with significantly
less complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1954</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1954</id><created>2013-07-08</created><updated>2014-02-10</updated><authors><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames><affiliation>INRIA Saclay - Ile de France, CVN</affiliation></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames><affiliation>INRIA Saclay - Ile de France, CVN</affiliation></author><author><keyname>Blaschko</keyname><forenames>Matthew</forenames><affiliation>INRIA Saclay - Ile de France, CVN</affiliation></author></authors><title>B-tests: Low Variance Kernel Two-Sample Tests</title><categories>cs.LG stat.ML</categories><comments>Neural Information Processing Systems (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A family of maximum mean discrepancy (MMD) kernel two-sample tests is
introduced. Members of the test family are called Block-tests or B-tests, since
the test statistic is an average over MMDs computed on subsets of the samples.
The choice of block size allows control over the tradeoff between test power
and computation time. In this respect, the $B$-test family combines favorable
properties of previously proposed MMD two-sample tests: B-tests are more
powerful than a linear time test where blocks are just pairs of samples, yet
they are more computationally efficient than a quadratic time test where a
single large block incorporating all the samples is used to compute a
U-statistic. A further important advantage of the B-tests is their
asymptotically Normal null distribution: this is by contrast with the
U-statistic, which is degenerate under the null hypothesis, and for which
estimates of the null distribution are computationally demanding. Recent
results on kernel selection for hypothesis testing transfer seamlessly to the
B-tests, yielding a means to optimize test power via kernel choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1955</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1955</id><created>2013-07-08</created><authors><author><keyname>He</keyname><forenames>Jiong</forenames></author><author><keyname>Lu</keyname><forenames>Mian</forenames></author><author><keyname>He</keyname><forenames>Bingsheng</forenames></author></authors><title>Revisiting Co-Processing for Hash Joins on the Coupled CPU-GPU
  Architecture</title><categories>cs.DC</categories><comments>14 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query co-processing on graphics processors (GPUs) has become an effective
means to improve the performance of main memory databases. However, the
relatively low bandwidth and high latency of the PCI-e bus are usually
bottleneck issues for co-processing. Recently, coupled CPU-GPU architectures
have received a lot of attention, e.g. AMD APUs with the CPU and the GPU
integrated into a single chip. That opens up new opportunities for optimizing
query co-processing. In this paper, we experimentally revisit hash joins, one
of the most important join algorithms for main memory databases, on a coupled
CPU-GPU architecture. Particularly, we study the ?fine-grained co-processing
mechanisms on hash joins with and without partitioning. The co-processing
outlines an interesting design space. We extend existing cost models to
automatically guide decisions on the design space. Our experimental results on
a recent AMD APU show that (1) the coupled architecture enables ?fine-grained
co-processing and cache reuses, which are inefficient on discrete CPU-GPU
architectures; (2) the cost model can automatically guide the design and tuning
knobs in the design space; (3) fi?ne-grained co-processing achieves up to 53%,
35% and 28% performance improvement over CPU-only, GPU-only and conventional
CPU-GPU co-processing, respectively. We believe that the insights and
implications from this study are initial yet important for further research on
query co-processing on coupled CPU-GPU architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1960</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1960</id><created>2013-07-08</created><authors><author><keyname>Park</keyname><forenames>Jae Young</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author><author><keyname>Gilbert</keyname><forenames>Anna C.</forenames></author></authors><title>Modal Analysis with Compressive Measurements</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2014.2302736</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural Health Monitoring (SHM) systems are critical for monitoring aging
infrastructure (such as buildings or bridges) in a cost-effective manner. Such
systems typically involve collections of battery-operated wireless sensors that
sample vibration data over time. After the data is transmitted to a central
node, modal analysis can be used to detect damage in the structure. In this
paper, we propose and study three frameworks for Compressive Sensing (CS) in
SHM systems; these methods are intended to minimize power consumption by
allowing the data to be sampled and/or transmitted more efficiently. At the
central node, all of these frameworks involve a very simple technique for
estimating the structure's mode shapes without requiring a traditional CS
reconstruction of the vibration signals; all that is needed is to compute a
simple Singular Value Decomposition. We provide theoretical justification
(including measurement bounds) for each of these techniques based on the
equations of motion describing a simplified Multiple-Degree-Of-Freedom (MDOF)
system, and we support our proposed techniques using simulations based on
synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1961</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1961</id><created>2013-07-08</created><authors><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Li</keyname><forenames>Tiffany Jing</forenames></author></authors><title>Optimal Locally Repairable Linear Codes</title><categories>cs.IT math.IT</categories><comments>Under Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear erasure codes with local repairability are desirable for distributed
data storage systems. An [n, k, d] code having all-symbol (r,
\delta})-locality, denoted as (r, {\delta})a, is considered optimal if it also
meets the minimum Hamming distance bound. The existing results on the existence
and the construction of optimal (r, {\delta})a codes are limited to only the
special case of {\delta} = 2, and to only two small regions within this special
case, namely, m = 0 or m &gt;= (v+{\delta}-1) &gt; ({\delta}-1), where m = n mod
(r+{\delta}-1) and v = k mod r. This paper investigates the existence
conditions and presents deterministic constructive algorithms for optimal (r,
{\delta})a codes with general r and {\delta}. First, a structure theorem is
derived for general optimal (r, {\delta})a codes which helps illuminate some of
their structure properties. Next, the entire problem space with arbitrary n, k,
r and {\delta} is divided into eight different cases (regions) with regard to
the specific relations of these parameters. For two cases, it is rigorously
proved that no optimal (r, {\delta})a could exist. For four other cases the
optimal (r, {\delta})a codes are shown to exist, deterministic constructions
are proposed and the lower bound on the required field size for these
algorithms to work is provided. Our new constructive algorithms not only cover
more cases, but for the same cases where previous algorithms exist, the new
constructions require a considerably smaller field, which translates to
potentially lower computational complexity. Our findings substantially enriches
the knowledge on (r, {\delta})a codes, leaving only two cases in which the
existence of optimal codes are yet to be determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1994</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1994</id><created>2013-07-08</created><authors><author><keyname>Gau&#xdf;mann</keyname><forenames>Daniel</forenames></author><author><keyname>Hoffmann</keyname><forenames>Stefan</forenames></author><author><keyname>Wanke</keyname><forenames>Egon</forenames></author></authors><title>Hierarchical Bipartition Routing for delivery guarantee in sparse
  wireless ad hoc sensor networks with obstacles</title><categories>cs.NI</categories><comments>Presented on the ICWN 2012, Las Vegas</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and evaluate a very simple landmark-based network partition
technique called Hierarchical Bipartition Routing (HBR) to support routing with
delivery guarantee in wireless ad hoc sensor networks. It is a simple routing
protocol that can easily be combined with any other greedy routing algorithm to
obtain delivery guarantee. The efficiency of HBR increases if the network is
sparse and contains obstacles. The space necessary to store the additional
routing information at a node u is on average not larger than the size
necessary to store the IDs of the neighbors of u. The amount of work to setup
the complete data structure is on average proportional to flooding the entire
network log(n) times, where n is the total number of sensor nodes. We evaluate
the performance of HBR in combination with two simple energy-aware geographic
greedy routing algorithms based on physical coordinates and virtual
coordinates, respectively. Our simulations show that the difference between
using HBR and a weighted shortest path to escape a dead-end is only a few
percent in typical cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.1998</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.1998</id><created>2013-07-08</created><authors><author><keyname>Ladas</keyname><forenames>Alexandros</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jon</forenames></author><author><keyname>Ferguson</keyname><forenames>Eamonn</forenames></author></authors><title>Using Clustering to extract Personality Information from socio economic
  data</title><categories>cs.LG cs.CE</categories><comments>UKCI 2012, the 12th Annual Workshop on Computational Intelligence,
  Heriot-Watt University, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has become apparent that models that have been applied widely in
economics, including Machine Learning techniques and Data Mining methods,
should take into consideration principles that derive from the theories of
Personality Psychology in order to discover more comprehensive knowledge
regarding complicated economic behaviours. In this work, we present a method to
extract Behavioural Groups by using simple clustering techniques that can
potentially reveal aspects of the Personalities for their members. We believe
that this is very important because the psychological information regarding the
Personalities of individuals is limited in real world applications and because
it can become a useful tool in improving the traditional models of Knowledge
Economy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2001</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2001</id><created>2013-07-08</created><authors><author><keyname>Ahmed</keyname><forenames>Aslam</forenames></author><author><keyname>Greensmith</keyname><forenames>Julie</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Variance in System Dynamics and Agent Based Modelling Using the SIR
  Model of Infectious Disease</title><categories>cs.CE cs.MA</categories><comments>Proceedings of the 26th European Conference on Modelling and
  Simulation (ECMS), Koblenz, Germany, May 2012, pp 9-15, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical deterministic simulations of epidemiological processes, such as
those based on System Dynamics, produce a single result based on a fixed set of
input parameters with no variance between simulations. Input parameters are
subsequently modified on these simulations using Monte-Carlo methods, to
understand how changes in the input parameters affect the spread of results for
the simulation. Agent Based simulations are able to produce different output
results on each run based on knowledge of the local interactions of the
underlying agents and without making any changes to the input parameters. In
this paper we compare the influence and effect of variation within these two
distinct simulation paradigms and show that the Agent Based simulation of the
epidemiological SIR (Susceptible, Infectious, and Recovered) model is more
effective at capturing the natural variation within SIR compared to an
equivalent model using System Dynamics with Monte-Carlo simulation. To
demonstrate this effect, the SIR model is implemented using both System
Dynamics (with Monte-Carlo simulation) and Agent Based Modelling based on
previously published empirical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2004</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2004</id><created>2013-07-08</created><authors><author><keyname>Churchill</keyname><forenames>Martin</forenames></author><author><keyname>Laird</keyname><forenames>Jim</forenames></author><author><keyname>McCusker</keyname><forenames>Guy</forenames></author></authors><title>Imperative Programs as Proofs via Game Semantics</title><categories>cs.LO</categories><msc-class>68Q55, 03B70, 03F52, 18C50</msc-class><journal-ref>Annals of Pure and Applied Logic, Volume 164, Issue 11 (2013)</journal-ref><doi>10.1016/j.apal.2013.05.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game semantics extends the Curry-Howard isomorphism to a three-way
correspondence: proofs, programs, strategies. But the universe of strategies
goes beyond intuitionistic logics and lambda calculus, to capture stateful
programs. In this paper we describe a logical counterpart to this extension, in
which proofs denote such strategies. The system is expressive: it contains all
of the connectives of Intuitionistic Linear Logic, and first-order
quantification. Use of Laird's sequoid operator allows proofs with imperative
behaviour to be expressed. Thus, we can embed first-order Intuitionistic Linear
Logic into this system, Polarized Linear Logic, and an imperative total
programming language.
  The proof system has a tight connection with a simple game model, where games
are forests of plays. Formulas are modelled as games, and proofs as
history-sensitive winning strategies. We provide a strong full completeness
result with respect to this model: each finitary strategy is the denotation of
a unique analytic (cut-free) proof. Infinite strategies correspond to analytic
proofs that are infinitely deep. Thus, we can normalise proofs, via the
semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2006</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2006</id><created>2013-07-08</created><updated>2014-12-08</updated><authors><author><keyname>Bassino</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author><author><keyname>Bouvel</keyname><forenames>Mathilde</forenames></author><author><keyname>Pierrot</keyname><forenames>Adeline</forenames></author><author><keyname>Rossin</keyname><forenames>Dominique</forenames></author></authors><title>An algorithm for deciding the finiteness of the number of simple
  permutations in permutation classes</title><categories>math.CO cs.DM</categories><comments>Correction of a few minor typos</comments><msc-class>05A05, 68R05, 05-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we describe an algorithm to determine whether a permutation
class C given by a finite basis B of excluded patterns contains a finite number
of simple permutations. This is a continuation of the work initiated in
[Brignall, Ruskuc, Vatter, Simple permutations: decidability and unavoidable
substructures, 2008], and shares several aspects with it. Like in this article,
the main difficulty is to decide whether C contains a finite number of proper
pin-permutations, and this decision problem is solved using automata theory.
Moreover, we use an encoding of proper pin-permutations by words over a finite
alphabet, introduced by Brignall et al. However, unlike in their article, our
construction of automata is fully algorithmic and efficient. It is based on the
study of pin-permutations in [Bassino, Bouvel, Rossin, Enumeration of
pin-permutations, 2011]. The complexity of the overall algorithm is O(n log n +
s^{2k}) where n denotes the sum of the sizes of permutations in the basis B, s
is the maximal size of a pin-permutation in B and k is the number of
pin-permutations in B.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2015</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2015</id><created>2013-07-08</created><authors><author><keyname>Zervakis</keyname><forenames>Lefteris</forenames></author><author><keyname>Tryfonopoulos</keyname><forenames>Christos</forenames></author><author><keyname>Papadakis-Pesaresi</keyname><forenames>Antonios</forenames></author><author><keyname>Koubarakis</keyname><forenames>Manolis</forenames></author><author><keyname>Skiadopoulos</keyname><forenames>Spiros</forenames></author></authors><title>Full-text Support for Publish/Subscribe Ontology Systems</title><categories>cs.IR cs.DB</categories><comments>ESWC 2012 Demo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We envision a publish/subscribe ontology system that is able to index
millions of user subscriptions and filter them against ontology data that
arrive in a streaming fashion. In this work, we propose a SPARQL extension
appropriate for a publish/subscribe setting; our extension builds on the
natural semantic graph matching of the language and supports the creation of
full-text subscriptions. Subsequently, we propose a main-memory subscription
indexing algorithm which performs both semantic and full-text matching at low
complexity and minimal filtering time. Thus, when ontology data are published
matching subscriptions are identified and notifications are forwarded to users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2018</identifier>
 <datestamp>2013-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2018</id><created>2013-07-08</created><authors><author><keyname>Rompa</keyname><forenames>Jenny</forenames></author><author><keyname>Lepouras</keyname><forenames>Giorgos</forenames></author><author><keyname>Vassilakis</keyname><forenames>Costas</forenames></author><author><keyname>Tryfonopoulos</keyname><forenames>Christos</forenames></author></authors><title>OntoFM: A Personal Ontology-based File Manager for the Desktop</title><categories>cs.HC</categories><comments>ISWC 2011 Demo</comments><report-no>ISWC 2011 Demo</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personal ontologies have been proposed as a means to support the semantic
management of user information. Assuming that a personal ontology system is in
use, new tools have to be developed at user interface level to exploit the
enhanced capabilities offered by the system. In this work, we present an
ontology-based file manager that allows semantic searching on the user's
personal information space. The file manager exploits the ontology relations to
present files associated with specific concepts, proposes new related concepts
to users, and helps them explore the information space and locate the required
file.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2028</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2028</id><created>2013-07-08</created><updated>2014-04-15</updated><authors><author><keyname>Rollini</keyname><forenames>S. F.</forenames></author><author><keyname>Bruttomesso</keyname><forenames>R.</forenames></author><author><keyname>Sharygina</keyname><forenames>N.</forenames></author><author><keyname>Tsitovich</keyname><forenames>A.</forenames></author></authors><title>Resolution Proof Transformation for Compression and Interpolation</title><categories>cs.LO</categories><doi>10.1007/s10703-014-0208-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verification methods based on SAT, SMT, and Theorem Proving often rely on
proofs of unsatisfiability as a powerful tool to extract information in order
to reduce the overall effort. For example a proof may be traversed to identify
a minimal reason that led to unsatisfiability, for computing abstractions, or
for deriving Craig interpolants. In this paper we focus on two important
aspects that concern efficient handling of proofs of unsatisfiability:
compression and manipulation. First of all, since the proof size can be very
large in general (exponential in the size of the input problem), it is indeed
beneficial to adopt techniques to compress it for further processing. Secondly,
proofs can be manipulated as a flexible preprocessing step in preparation for
interpolant computation. Both these techniques are implemented in a framework
that makes use of local rewriting rules to transform the proofs. We show that a
careful use of the rules, combined with existing algorithms, can result in an
effective simplification of the original proofs. We have evaluated several
heuristics on a wide range of unsatisfiable problems deriving from SAT and SMT
test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2035</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2035</id><created>2013-07-08</created><updated>2014-11-23</updated><authors><author><keyname>Oikonomou</keyname><forenames>V. K.</forenames></author><author><keyname>Jost</keyname><forenames>J.</forenames></author></authors><title>Periodic Strategies a New Solution Concept-Algorithm for non-trivial
  Strategic Form Games</title><categories>cs.GT</categories><comments>63 pages, title changed, material added, substantial revision,
  working paper</comments><doi>10.1088/1742-6596/410/1/012070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new solution concept for selecting optimal strategies in
strategic form games which we call periodic strategies and the solution concept
periodicity. As we will explicitly demonstrate, the periodicity solution
concept has implications for non-trivial realistic games, which renders this
solution concept very valuable. The most striking application of periodicity is
that in mixed strategy strategic form games, we were able to find solutions
that result to values for the utility function of each player, that are equal
to the Nash equilibrium ones, with the difference that in the Nash strategies
playing, the payoffs strongly depend on what the opponent plays, while in the
periodic strategies case, the payoffs of each player are completely robust
against what the opponent plays. We formally define and study periodic
strategies in two player perfect information strategic form games, with pure
strategies and generalize the results to include multiplayer games with perfect
information. We prove that every non-trivial finite game has at least one
periodic strategy, with non-trivial meaning a game with non-degenerate payoffs.
In principle the algorithm we provide, holds true for every non-trivial game,
because in degenerate games, inconsistencies can occur. In addition, we also
address the incomplete information games in the context of Bayesian games, in
which case generalizations of Bernheim's rationalizability offers us the
possibility to embed the periodicity concept in the Bayesian games framework.
Applying the algorithm of periodic strategies in the case where mixed
strategies are used, we find some very interesting outcomes with useful
quantitative features for some classes of games. We support all our results
throughout the article by providing some illustrative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2036</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2036</id><created>2013-07-08</created><authors><author><keyname>Mueller</keyname><forenames>Eike H.</forenames></author><author><keyname>Scheichl</keyname><forenames>Robert</forenames></author></authors><title>Massively parallel solvers for elliptic PDEs in Numerical Weather- and
  Climate Prediction</title><categories>cs.DC math.NA</categories><comments>24 pages, 7 figures, 7 tables</comments><msc-class>35J25, 65F08, 65F10, 65N55, 68W10</msc-class><acm-class>D.1.3; G.1.3; G.1.8; J.2</acm-class><doi>10.1002/qj.2327</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand for substantial increases in the spatial resolution of global
weather- and climate- prediction models makes it necessary to use numerically
efficient and highly scalable algorithms to solve the equations of large scale
atmospheric fluid dynamics. For stability and efficiency reasons several of the
operational forecasting centres, in particular the Met Office and the ECMWF in
the UK, use semi-implicit semi-Lagrangian time stepping in the dynamical core
of the model. The additional burden with this approach is that a three
dimensional elliptic partial differential equation (PDE) for the pressure
correction has to be solved at every model time step and this often constitutes
a significant proportion of the time spent in the dynamical core. To run within
tight operational time scales the solver has to be parallelised and there seems
to be a (perceived) misconception that elliptic solvers do not scale to large
processor counts and hence implicit time stepping can not be used in very high
resolution global models. After reviewing several methods for solving the
elliptic PDE for the pressure correction and their application in atmospheric
models we demonstrate the performance and very good scalability of Krylov
subspace solvers and multigrid algorithms for a representative model equation
with more than $10^{10}$ unknowns on 65536 cores on HECToR, the UK's national
supercomputer. For this we tested and optimised solvers from two existing
numerical libraries (DUNE and hypre) and implemented both a Conjugate Gradient
solver and a geometric multigrid algorithm based on a tensor-product approach
which exploits the strong vertical anisotropy of the discretised equation. We
study both weak and strong scalability and compare the absolute solution times
for all methods; in contrast to one-level methods the multigrid solver is
robust with respect to parameter variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2042</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2042</id><created>2013-07-08</created><authors><author><keyname>Garc&#xed;a-Cerda&#xf1;a</keyname><forenames>&#xc0;ngel</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Ventura</forenames></author></authors><title>On Fragments without Implications of both the Full Lambek Logic and some
  of its Substructural Extensions</title><categories>math.LO cs.LO</categories><comments>75 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study some fragments without implications of the (Hilbert)
full Lambek logic $\mathbf{HFL}$ and also some fragments without implications
of some of the substructural extensions of that logic. To do this, we perform
an algebraic analysis of the Gentzen systems defined by the substructural
calculi $\FL_\sigma$. Such systems are extensions of the full Lambek calculus
$\FL$ with the rules codified by a subsequence, $\sigma$, of the sequence $e
w_l w_r c$; where $e$ stands for \emph{exchange}, $w_l$ for \emph{left
weakening}, $w_r$ for \emph{right weakening}, and $c$ for \emph{contraction}.
We prove that these Gentzen systems (in languages without implications) are
algebraizable by obtaining their equivalent algebraic semantics. All these
classes of algebras are varieties of pointed semilatticed monoids and they can
be embedded in their ideal completions. As a consequence of these results, we
reveal that the fragments of the Gentzen systems associated with the calculi
$\FL_\sigma$ are the restrictions of them to the sublanguages considered, and
we also reveal that in these languages, the fragments of the external systems
associated with $\FL[\sigma]$ are the external systems associated with the
restricted Gentzen systems (i.e., those obtained by restriction of
$\FL_\sigma]$ to the implication-less languages considered). We show that all
these external systems without implication have algebraic semantics but they
are not algebraizable (and are not even protoalgebraic). Results concerning
fragments without implication of intuitionistic logic without contraction were
already reported in Bou et al.(2006): On two fragments with negation and
without implication of the logic of residuated lattices. Archive for
Mathematical Logic 45(5) and in Adill\'on et al. (2007): On three
implication-less fragments of t-norm based fuzzy logics. Fuzzy Sets and Systems
158(23).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2043</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2043</id><created>2013-07-08</created><authors><author><keyname>Canales</keyname><forenames>Santiago</forenames></author><author><keyname>Hern&#xe1;ndez</keyname><forenames>Gregorio</forenames></author><author><keyname>Martins</keyname><forenames>Mafalda</forenames></author><author><keyname>Matos</keyname><forenames>In&#xea;s</forenames></author></authors><title>Distance domination, guarding and vertex cover for maximal outerplanar
  graph</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a distance guarding concept on triangulation graphs,
which can be associated with distance domination and distance vertex cover. We
show how these subjects are interconnected and provide tight bounds for any
n-vertex maximal outerplanar graph: the 2d-guarding number, g_{2d}(n) = n/5;
the 2d-distance domination number, gamma_{2d}(n) = n/5; and the 2d-distance
vertex cover number, beta_{2d}(n) = n/4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2051</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2051</id><created>2013-07-08</created><authors><author><keyname>Gupta</keyname><forenames>Anshul</forenames></author><author><keyname>Schewe</keyname><forenames>Sven</forenames></author></authors><title>The benefit of law-making power</title><categories>cs.GT</categories><comments>17 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study optimal equilibria in multi-player games. An equilibrium is optimal
for a player, if her payoff is maximal. A tempting approach to solving this
problem is to seek optimal Nash equilibria, the standard form of equilibria
where no player has an incentive to deviate from her strategy. We argue that a
player with the power to define an equilibrium is in a position, where she
should not be interested in the symmetry of a Nash equilibrium, and ignore the
question of whether or not her outcome can be improved if the other strategies
are fixed. That is, she would only have to make sure that the other players
have no incentive to deviate. This defines a greater class of equilibria, which
may have better (and cannot have worse) optimal equilibria for the designated
powerful player. We apply this strategy to concurrent bimatrix games and to
turn based multi-player mean-payoff games. For the latter, we show that such
political equilibria as well as Nash equilibria always exist, and provide
simple examples where the political equilibrium is superior. We show that
constructing political and Nash equilibria are NP-complete problems. We also
show that, for a fixed number of players, the hardest part is to solve the
underlying two-player mean-payoff games: using an MPG oracle, the problem is
solvable in polynomial time. It is therefore in UP and CoUP, and can be solved
in pseudo polynomial and expected subexponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2062</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2062</id><created>2013-07-08</created><authors><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames></author></authors><title>Comparing the Expressive Power of the Synchronous and the Asynchronous
  pi-calculi</title><categories>cs.LO cs.DC</categories><journal-ref>Mathematical Structures in Computer Science, 13(5) , 685-719, 2003</journal-ref><doi>10.1017/S0960129503004043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Asynchronous pi-calculus, proposed by Honda and Tokoro (1991) and,
independently, by Boudol (1992), is a subset of the pi-calculus (Milner, 1992)
which contains no explicit operators for choice and output-prefixing. The
communication mechanism of this calculus, however, is powerful enough to
simulate output-prefixing, as shown by Honda and Tokoro (1991) and by Boudol
(1992), and input-guarded choice, as shown by Nestmann and Pierce (2000). A
natural question arises, then, whether or not it is as expressive as the full
pi-calculus. We show that this is not the case. More precisely, we show that
there does not exist any uniform, fully distributed translation from the
pi-calculus into the asynchronous pi-calculus, up to any &quot;reasonable&quot; notion of
equivalence. This result is based on the incapability of the asynchronous
pi-calculus to break certain symmetries possibly present in the initial
communication graph. By similar arguments, we prove a separation result between
the pi-calculus and CCS, and between the pi-calculus and the pi-calculus with
internal mobility, a subset of the pi-calculus proposed by Sangiorgi where the
output actions can only transmit private names.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2064</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2064</id><created>2013-07-08</created><updated>2014-06-14</updated><authors><author><keyname>Xu</keyname><forenames>Bin</forenames></author></authors><title>Cycles of strategies and changes of distribution in public goods game:
  An experimental investigation</title><categories>physics.soc-ph cs.GT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this communication, a simple mechanism in the optional public goods game
is experimentally investigated using two experimental settings; and first time,
the cyclic strategy pattern in full state space is demonstrated by means of
velocity. It is, furthermore, elaborated that the strategies of cooperation,
defection and nonparticipant form a Rock-Paper-Scissors type cycle, and the
cycle of three strategies are persistent over 200 rounds. This cycle is very
similar to the cycle given by evolutionary dynamics e.g. replicator dynamics.
The mechanism that nonparticipant can sustain cooperation is driven by the
Rock-Paper-Scissors type of cyclic dominance in the three strategies. That is,
if the cycle is existent, the cooperation will always sustain. Meanwhile, the
distribution of social states changes in the state space and from cooperation
as the most frequent strategy to defection and, from defection to
nonparticipant, forms a clear rotation path in a long run. These results seem
to implicate that the evolutionary dynamics has ability to capture the real
dynamics applying not only on biosphere, but also on human society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2075</identifier>
 <datestamp>2013-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2075</id><created>2013-07-08</created><updated>2013-07-31</updated><authors><author><keyname>Graziotin</keyname><forenames>Daniel</forenames><affiliation>Free University of Bozen-Bolzano</affiliation></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames><affiliation>Free University of Bozen-Bolzano</affiliation></author></authors><title>A Web-based modeling tool for the SEMAT Essence theory of Software
  Engineering</title><categories>cs.SE</categories><comments>12 pages, 5 figures. Revised version (after-peer review) for the
  Journal of Open Research Software &lt;http://openresearchsoftware.metajnl.com/&gt;,
  July 2013</comments><acm-class>D.2.0; D.2.8; H.3.5; H.5.3</acm-class><journal-ref>Journal of Open Research Software 1(1):e4, 2013</journal-ref><doi>10.5334/jors.ad</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As opposed to more mature subjects, software engineering lacks general
theories to establish its foundations as a discipline. The Essence Theory of
software engineering (Essence) has been proposed by the Software Engineering
Methods and Theory (SEMAT) initiative. Essence goal is to develop a
theoretically sound basis for software engineering practice and its wide
adoption. Essence is yet far from reaching academic and industry adoption.
Reasons include a struggle to foresee its utilization potential and the lack of
tools implementing it. SEMAT Accelerator (SematAcc) is a Web-positioning tool
for a software engineering endeavor, which implements the SEMAT's Essence
kernel. SematAcc allows using Essence, thus helping to understand it. The tool
enables teaching, adopting, and researching Essence in controlled experiments
and case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2084</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2084</id><created>2013-07-08</created><authors><author><keyname>Kafsi</keyname><forenames>Mohamed</forenames></author><author><keyname>Kazemi</keyname><forenames>Ehsan</forenames></author><author><keyname>Maystre</keyname><forenames>Lucas</forenames></author><author><keyname>Yartseva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Grossglauser</keyname><forenames>Matthias</forenames></author><author><keyname>Thiran</keyname><forenames>Patrick</forenames></author></authors><title>Mitigating Epidemics through Mobile Micro-measures</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Presented at NetMob 2013, Boston</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemics of infectious diseases are among the largest threats to the quality
of life and the economic and social well-being of developing countries. The
arsenal of measures against such epidemics is well-established, but costly and
insufficient to mitigate their impact. In this paper, we argue that mobile
technology adds a powerful weapon to this arsenal, because (a) mobile devices
endow us with the unprecedented ability to measure and model the detailed
behavioral patterns of the affected population, and (b) they enable the
delivery of personalized behavioral recommendations to individuals in real
time. We combine these two ideas and propose several strategies to generate
such recommendations from mobility patterns. The goal of each strategy is a
large reduction in infections, with a small impact on the normal course of
daily life. We evaluate these strategies over the Orange D4D dataset and show
the benefit of mobile micro-measures, even if only a fraction of the population
participates. These preliminary results demonstrate the potential of mobile
technology to complement other measures like vaccination and quarantines
against disease epidemics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2087</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2087</id><created>2013-07-08</created><authors><author><keyname>Summers</keyname><forenames>Tyler H.</forenames></author><author><keyname>Goulart</keyname><forenames>Paul J.</forenames></author></authors><title>Performance Bounds for Constrained Linear Min-Max Control</title><categories>math.OC cs.SY</categories><comments>6 pages, in proceedings of the 2013 European Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a method to compute lower performance bounds for
discrete-time infinite-horizon min-max control problems with input constraints
and bounded disturbances. Such bounds can be used as a performance metric for
control policies synthesized via suboptimal design techniques. Our approach is
motivated by recent work on performance bounds for stochastic constrained
optimal control problems using relaxations of the Bellman equation. The central
idea of the paper is to find an unconstrained min-max control problem, with
negatively weighted disturbances as in H infinity control, that provides the
tightest possible lower performance bound on the original problem of interest
and whose value function is easily computed. The new method is demonstrated via
a numerical example for a system with box constrained input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2089</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2089</id><created>2013-07-08</created><authors><author><keyname>Summers</keyname><forenames>Tyler H.</forenames></author><author><keyname>Yu</keyname><forenames>Changbin</forenames></author><author><keyname>Dasgupta</keyname><forenames>Soura</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author></authors><title>Certifying non-existence of undesired locally stable equilibria in
  formation shape control problems</title><categories>math.OC cs.SY</categories><comments>6 pages; to appear in the 2013 IEEE Multiconference on Systems and
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental control problem for autonomous vehicle formations is formation
shape control, in which the agents must maintain a prescribed formation shape
using only information measured or communicated from neighboring agents. While
a large and growing literature has recently emerged on distance-based formation
shape control, global stability properties remain a significant open problem.
Even in four-agent formations, the basic question of whether or not there can
exist locally stable incorrect equilibrium shapes remains open. This paper
shows how this question can be answered for any size formation in principle
using semidefinite programming techniques for semialgebraic problems, involving
solutions sets of polynomial equations, inequations, and inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2090</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2090</id><created>2013-07-08</created><authors><author><keyname>Sole-Ribalta</keyname><forenames>Albert</forenames></author><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>Kouvaris</keyname><forenames>Nikos E.</forenames></author><author><keyname>Diaz-Guilera</keyname><forenames>Albert</forenames></author><author><keyname>Gomez</keyname><forenames>Sergio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Spectral properties of the Laplacian of multiplex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>8 pages, 11 figures</comments><journal-ref>Physical Review E 88 (2013) 032807</journal-ref><doi>10.1103/PhysRevE.88.032807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the more challenging tasks in the understanding of dynamical
properties of models on top of complex networks is to capture the precise role
of multiplex topologies. In a recent paper, Gomez et al. [Phys. Rev. Lett. 101,
028701 (2013)] proposed a framework for the study of diffusion processes in
such networks. Here, we extend the previous framework to deal with general
configurations in several layers of networks, and analyze the behavior of the
spectrum of the Laplacian of the full multiplex. We derive an interesting
decoupling of the problem that allow us to unravel the role played by the
interconnections of the multiplex in the dynamical processes on top of them.
Capitalizing on this decoupling we perform an asymptotic analysis that allow us
to derive analytical expressions for the full spectrum of eigenvalues. This
spectrum is used to gain insight into physical phenomena on top of multiplex,
specifically, diffusion processes and synchronizability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2097</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2097</id><created>2013-07-08</created><updated>2013-12-04</updated><authors><author><keyname>Buchin</keyname><forenames>Maike</forenames></author><author><keyname>Driemel</keyname><forenames>Anne</forenames></author><author><keyname>Speckmann</keyname><forenames>Bettina</forenames></author></authors><title>Computing the Fr\'{e}chet distance with shortcuts is NP-hard</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the shortcut Fr\'{e}chet distance, a natural variant of the
Fr\'{e}chet distance, that allows us to take shortcuts from and to any point
along one of the curves. The classic Fr\'echet distance is a bottle-neck
distance measure and hence quite sensitive to outliers. The shortcut
Fr\'{e}chet distance allows us to cut across outliers and hence produces
significantly more meaningful results when dealing with real world data.
Driemel and Har-Peled recently described approximation algorithms for the
restricted case where shortcuts have to start and end at input vertices. We
show that, in the general case, the problem of computing the shortcut
Fr\'{e}chet distance is NP-hard. This is the first hardness result for a
variant of the Fr\'{e}chet distance between two polygonal curves in the plane.
We also present two algorithms for the decision problem: a 3-approximation
algorithm for the general case and an exact algorithm for the vertex-restricted
case. Both algorithms run in O(n^3 log n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2100</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2100</id><created>2013-07-08</created><authors><author><keyname>Di Napoli</keyname><forenames>Edoardo</forenames><affiliation>J&#xfc;lich Supercomputing Centre, Forschungszentrum J&#xfc;lich</affiliation><affiliation>AICES, RWTH-Aachen University</affiliation></author><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames><affiliation>AICES, RWTH-Aachen University</affiliation></author><author><keyname>Quintana-Ort&#xec;</keyname><forenames>Gregorio</forenames><affiliation>Depto. de Ingenier&#xec;a y Ciencia de Computadores, Universidad Jaume I</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH-Aachen University</affiliation></author></authors><title>Towards an Efficient Use of the BLAS Library for Multilinear Tensor
  Contractions</title><categories>cs.MS cs.DM</categories><comments>27 Pages, 7 figures and additional tikz generated diagrams. Submitted
  to Applied Mathematics and Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical operators whose transformation rules constitute the building
blocks of a multi-linear algebra are widely used in physics and engineering
applications where they are very often represented as tensors. In the last
century, thanks to the advances in tensor calculus, it was possible to uncover
new research fields and make remarkable progress in the existing ones, from
electromagnetism to the dynamics of fluids and from the mechanics of rigid
bodies to quantum mechanics of many atoms. By now, the formal mathematical and
geometrical properties of tensors are well defined and understood; conversely,
in the context of scientific and high-performance computing, many tensor-
related problems are still open. In this paper, we address the problem of
efficiently computing contractions among two tensors of arbitrary dimension by
using kernels from the highly optimized BLAS library. In particular, we
establish precise conditions to determine if and when GEMM, the kernel for
matrix products, can be used. Such conditions take into consideration both the
nature of the operation and the storage scheme of the tensors, and induce a
classification of the contractions into three groups. For each group, we
provide a recipe to guide the users towards the most effective use of BLAS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2104</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2104</id><created>2013-07-08</created><updated>2014-03-05</updated><authors><author><keyname>Mastrandrea</keyname><forenames>Rossana</forenames></author><author><keyname>Squartini</keyname><forenames>Tiziano</forenames></author><author><keyname>Fagiolo</keyname><forenames>Giorgio</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author></authors><title>Enhanced reconstruction of weighted networks from strengths and degrees</title><categories>physics.data-an cs.SI physics.soc-ph</categories><journal-ref>New J. Phys. 16, 043022 (2014)</journal-ref><doi>10.1088/1367-2630/16/4/043022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network topology plays a key role in many phenomena, from the spreading of
diseases to that of financial crises. Whenever the whole structure of a network
is unknown, one must resort to reconstruction methods that identify the least
biased ensemble of networks consistent with the partial information available.
A challenging case, frequently encountered due to privacy issues in the
analysis of interbank flows and Big Data, is when there is only local
(node-specific) aggregate information available. For binary networks, the
relevant ensemble is one where the degree (number of links) of each node is
constrained to its observed value. However, for weighted networks the problem
is much more complicated. While the naive approach prescribes to constrain the
strengths (total link weights) of all nodes, recent counter-intuitive results
suggest that in weighted networks the degrees are often more informative than
the strengths. This implies that the reconstruction of weighted networks would
be significantly enhanced by the specification of both strengths and degrees, a
computationally hard and bias-prone procedure. Here we solve this problem by
introducing an analytical and unbiased maximum-entropy method that works in the
shortest possible time and does not require the explicit generation of
reconstructed samples. We consider several real-world examples and show that,
while the strengths alone give poor results, the additional knowledge of the
degrees yields accurately reconstructed networks. Information-theoretic
criteria rigorously confirm that the degree sequence, as soon as it is
non-trivial, is irreducible to the strength sequence. Our results have strong
implications for the analysis of motifs and communities and whenever the
reconstructed ensemble is required as a null model to detect higher-order
patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2105</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2105</id><created>2013-07-08</created><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author><author><keyname>Nazer</keyname><forenames>Bobak</forenames></author></authors><title>Successive Integer-Forcing and its Sum-Rate Optimality</title><categories>cs.IT math.IT</categories><comments>A shorter version was submitted to the 51st Allerton Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integer-forcing receivers generalize traditional linear receivers for the
multiple-input multiple-output channel by decoding integer-linear combinations
of the transmitted streams, rather then the streams themselves. Previous works
have shown that the additional degree of freedom in choosing the integer
coefficients enables this receiver to approach the performance of
maximum-likelihood decoding in various scenarios. Nonetheless, even for the
optimal choice of integer coefficients, the additive noise at the equalizer's
output is still correlated. In this work we study a variant of integer-forcing,
termed successive integer-forcing, that exploits these noise correlations to
improve performance. This scheme is the integer-forcing counterpart of
successive interference cancellation for traditional linear receivers.
Similarly to the latter, we show that successive integer-forcing is capacity
achieving when it is possible to optimize the rate allocation to the different
streams. In comparison to standard successive interference cancellation
receivers, the successive integer-forcing receiver offers more possibilities
for capacity achieving rate tuples, and in particular, ones that are more
balanced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2111</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2111</id><created>2013-07-08</created><authors><author><keyname>Dent</keyname><forenames>Ian</forenames></author><author><keyname>Craig</keyname><forenames>Tony</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Rodden</keyname><forenames>Tom</forenames></author></authors><title>Finding the creatures of habit; Clustering households based on their
  flexibility in using electricity</title><categories>cs.LG cs.CE</categories><comments>Digital Futures 2012, Aberdeen, UK, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Changes in the UK electricity market, particularly with the roll out of smart
meters, will provide greatly increased opportunities for initiatives intended
to change households' electricity usage patterns for the benefit of the overall
system. Users show differences in their regular behaviours and clustering
households into similar groupings based on this variability provides for
efficient targeting of initiatives. Those people who are stuck into a regular
pattern of activity may be the least receptive to an initiative to change
behaviour. A sample of 180 households from the UK are clustered into four
groups as an initial test of the concept and useful, actionable groupings are
found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2117</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2117</id><created>2013-07-08</created><authors><author><keyname>Fan</keyname><forenames>Yi-Zheng</forenames></author><author><keyname>Huang</keyname><forenames>Tao</forenames></author><author><keyname>Zhu</keyname><forenames>Ming</forenames></author></authors><title>Mixed Compressed Sensing Based on Random Graphs</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1212.3799</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a suitable measurement matrix is an important topic in compressed
sensing. Though the known random matrix, whose entries are drawn independently
from a certain probability distribution, can be used as a measurement matrix
and recover signal well, in most cases, we hope the measurement matrix imposed
with some special structure. In this paper, based on random graph models, we
show that the mixed symmetric random matrices, whose diagonal entries obey a
distribution and non-diagonal entries obey another distribution, can be used to
recover signal successfully with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2118</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2118</id><created>2013-07-08</created><authors><author><keyname>McAllester</keyname><forenames>David</forenames></author></authors><title>A PAC-Bayesian Tutorial with A Dropout Bound</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This tutorial gives a concise overview of existing PAC-Bayesian theory
focusing on three generalization bounds. The first is an Occam bound which
handles rules with finite precision parameters and which states that
generalization loss is near training loss when the number of bits needed to
write the rule is small compared to the sample size. The second is a
PAC-Bayesian bound providing a generalization guarantee for posterior
distributions rather than for individual rules. The PAC-Bayesian bound
naturally handles infinite precision rule parameters, $L_2$ regularization,
{\em provides a bound for dropout training}, and defines a natural notion of a
single distinguished PAC-Bayesian posterior distribution. The third bound is a
training-variance bound --- a kind of bias-variance analysis but with bias
replaced by expected training loss. The training-variance bound dominates the
other bounds but is more difficult to interpret. It seems to suggest variance
reduction methods such as bagging and may ultimately provide a more meaningful
analysis of dropouts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2136</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2136</id><created>2013-07-08</created><authors><author><keyname>Iwen</keyname><forenames>Mark</forenames></author><author><keyname>Saab</keyname><forenames>Rayan</forenames></author></authors><title>Near-Optimal Encoding for Sigma-Delta Quantization of Finite Frame
  Expansions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate encoding the bit-stream resulting from coarse
Sigma-Delta quantization of finite frame expansions (i.e., overdetermined
representations) of vectors. We show that for a wide range of finite-frames,
including random frames and piecewise smooth frames, there exists a simple
encoding algorithm ---acting only on the Sigma-Delta bit stream--- and an
associated decoding algorithm that together yield an approximation error which
decays exponentially in the number of bits used. The encoding strategy consists
of applying a discrete random operator to the Sigma-Delta bit stream and
assigning a binary codeword to the result. The reconstruction procedure is
essentially linear and equivalent to solving a least squares minimization
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2145</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2145</id><created>2013-07-08</created><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Desel</keyname><forenames>Joerg</forenames></author></authors><title>On Negotiation as Concurrency Primitive</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce negotiations, a model of concurrency close to Petri nets, with
multiparty negotiation as primitive. We study the problems of soundness of
negotiations and of, given a negotiation with possibly many steps, computing a
summary, i.e., an equivalent one-step negotiation. We provide a complete set of
reduction rules for sound, acyclic, weakly deterministic negotiations and show
that, for deterministic negotiations, the rules compute the summary in
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2150</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2150</id><created>2013-07-08</created><authors><author><keyname>Halchenko</keyname><forenames>Yaroslav O.</forenames></author><author><keyname>Hanke</keyname><forenames>Michael</forenames></author><author><keyname>Haxby</keyname><forenames>James V.</forenames></author><author><keyname>Hanson</keyname><forenames>Stephen Jose</forenames></author><author><keyname>Herrmann</keyname><forenames>Christoph S.</forenames></author></authors><title>Transmodal Analysis of Neural Signals</title><categories>q-bio.NC cs.LG q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localizing neuronal activity in the brain, both in time and in space, is a
central challenge to advance the understanding of brain function. Because of
the inability of any single neuroimaging techniques to cover all aspects at
once, there is a growing interest to combine signals from multiple modalities
in order to benefit from the advantages of each acquisition method. Due to the
complexity and unknown parameterization of any suggested complete model of BOLD
response in functional magnetic resonance imaging (fMRI), the development of a
reliable ultimate fusion approach remains difficult. But besides the primary
goal of superior temporal and spatial resolution, conjoint analysis of data
from multiple imaging modalities can alternatively be used to segregate neural
information from physiological and acquisition noise. In this paper we suggest
a novel methodology which relies on constructing a quantifiable mapping of data
from one modality (electroencephalography; EEG) into another (fMRI), called
transmodal analysis of neural signals (TRANSfusion). TRANSfusion attempts to
map neural data embedded within the EEG signal into its reflection in fMRI
data. Assessing the mapping performance on unseen data allows to localize brain
areas where a significant portion of the signal could be reliably
reconstructed, hence the areas neural activity of which is reflected in both
EEG and fMRI data. Consecutive analysis of the learnt model allows to localize
areas associated with specific frequency bands of EEG, or areas functionally
related (connected or coherent) to any given EEG sensor. We demonstrate the
performance of TRANSfusion on artificial and real data from an auditory
experiment. We further speculate on possible alternative uses: cross-modal data
filtering and EEG-driven interpolation of fMRI signals to obtain arbitrarily
high temporal sampling of BOLD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2164</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2164</id><created>2013-07-04</created><authors><author><keyname>Chermakani</keyname><forenames>Deepak Ponvel</forenames></author></authors><title>Efficiently determining Convergence in Polynomial Recurrence Sequences</title><categories>cs.DM math.CO</categories><comments>11 pages, 4 Theorems, 1 Example for Polynomial Recurrence Sequence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the necessary and sufficient condition, for a given Polynomial
Recurrence Sequence to converge to a given target rational K. By converge, we
mean that the Nth term of the sequence, is equal to K, as N tends to positive
infinity. The basic idea of our approach is to construct a univariate
polynomial equation in x, whose coefficients correspond to the terms of the
Sequence. The approach then obtains the condition by analyzing five cases that
cover all possible real values of x. The condition can be evaluated within time
that is a polynomial function of the size of the description of the Polynomial
Recurrence Sequence, hence convergence or non-convergence can be efficiently
determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2169</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2169</id><created>2013-07-08</created><updated>2014-07-24</updated><authors><author><keyname>Lopez-Ruiz</keyname><forenames>Ricardo</forenames></author><author><keyname>Shivanian</keyname><forenames>Elyas</forenames></author><author><keyname>Lopez</keyname><forenames>Jose-Luis</forenames></author></authors><title>Random Market Models with an H-Theorem</title><categories>q-fin.TR cs.GT nlin.AO</categories><comments>11 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1104.2187</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this communication, some economic models given by functional mappings are
addressed. These are models for random markets where agents trade by pairs and
exchange their money in a random and conservative way. They display the
exponential wealth distribution as asymptotic equilibrium, independently of the
effectiveness of the transactions and of the limitation of the total wealth.
The entropy increases with time in these models and the existence of an
H-theorem is computationally checked. Also, it is shown that any small
perturbation of the models equations make them to lose the exponential
distribution as an equilibrium solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2187</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2187</id><created>2013-07-08</created><updated>2013-08-25</updated><authors><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Everything you always wanted to know about the parameterized complexity
  of Subgraph Isomorphism (but were afraid to ask)</title><categories>cs.DS cs.CC</categories><comments>85 pages, 16 figures; program and input data file can be found as
  ancillary files. Version [v2]: revised conclusions, ancillary files added
  properly. Version [v3]: added a remark about fixed-parameter tractability of
  the Conjoining Matching problem following from Lemma 3.2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two graphs $H$ and $G$, the Subgraph Isomorphism problem asks if $H$ is
isomorphic to a subgraph of $G$. While NP-hard in general, algorithms exist for
various parameterized versions of the problem: for example, the problem can be
solved (1) in time $2^{O(|V(H)|)}\cdot n^{O(\tw(H))}$ using the color-coding
technique of Alon, Yuster, and Zwick; (2) in time $f(|V(H)|,\tw(G))\cdot n$
using Courcelle's Theorem; (3) in time $f(|V(H)|,\genus(G))\cdot n$ using a
result on first-order model checking by Frick and Grohe; or (4) in time
$f(\maxdeg(H))\cdot n^{O(\tw(G)})$ for connected $H$ using the algorithm of
Matou\v{s}ek and Thomas. Already this small sample of results shows that the
way an algorithm can depend on the parameters is highly nontrivial and subtle.
  We develop a framework involving 10 relevant parameters for each of $H$ and
$G$ (such as treewidth, pathwidth, genus, maximum degree, number of vertices,
number of components, etc.), and ask if an algorithm with running time \[
f_1(p_1,p_2,..., p_\ell)\cdot n^{f_2(p_{\ell+1},..., p_k)} \] exist, where each
of $p_1,..., p_k$ is one of the 10 parameters depending only on $H$ or $G$. We
show that {\em all} the questions arising in this framework are answered by a
set of 11 maximal positive results (algorithms) and a set of 17 maximal
negative results (hardness proofs); some of these results already appear in the
literature, while others are new in this paper.
  On the algorithmic side, our study reveals for example that an unexpected
combination of bounded degree, genus, and feedback vertex set number of $G$
gives rise to a highly nontrivial algorithm for Subgraph Isomorphism. On the
hardness side, we present W[1]-hardness proofs under extremely restricted
conditions, such as when $H$ is a bounded-degree tree of constant pathwidth and
$G$ is a planar graph of bounded pathwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2189</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2189</id><created>2013-07-08</created><authors><author><keyname>Slattery</keyname><forenames>R. E.</forenames></author><author><keyname>McHardy</keyname><forenames>R. R.</forenames></author><author><keyname>Bairathi</keyname><forenames>R.</forenames></author></authors><title>On the Topology of the Facebook Page Network</title><categories>cs.SI physics.soc-ph</categories><comments>3 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Facebook Page Network (FPN) is a platform for Businesses, Public Figures
and Organizations (BPOs) to connect with individuals and other BPOs in the
digital space. For over a decade scale-free networks have most appropriately
described a variety of seemingly disparate physical, biological and social
real-world systems unified by similar network properties such as
scale-invariance, growth via a preferential attachment mechanism, and a power
law degree distribution P(k) = ck^-{\lambda} where typically 2&lt;{\lambda}&lt;3. In
this paper we show that both the Facebook Page Network and its BPO-BPO
subnetwork suggest power law and scale-free characteristics. We argue that
social media analysts must consider the logarithmic and non-linear properties
of social media audiences of scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2191</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2191</id><created>2013-07-08</created><authors><author><keyname>Moses</keyname><forenames>Yoram</forenames><affiliation>Technion - Israel Institute of Technology</affiliation></author><author><keyname>Shamo</keyname><forenames>Marcia K.</forenames></author></authors><title>A Knowledge-based Treatment of Human-Automation Systems</title><categories>cs.HC cs.AI</categories><comments>39 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a supervisory control system the human agent knowledge of past, current,
and future system behavior is critical for system performance. Being able to
reason about that knowledge in a precise and structured manner is central to
effective system design. In this paper we introduce the application of a
well-established formal approach to reasoning about knowledge to the modeling
and analysis of complex human-automation systems. An intuitive notion of
knowledge in human-automation systems is sketched and then cast as a formal
model. We present a case study in which the approach is used to model and
reason about a classic problem from the human-automation systems literature;
the results of our analysis provide evidence for the validity and value of
reasoning about complex systems in terms of the knowledge of the system agents.
To conclude, we discuss research directions that will extend this approach, and
note several systems in the aviation and human-robot team domains that are of
particular interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2200</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2200</id><created>2013-07-08</created><authors><author><keyname>Dinh</keyname><forenames>Hang</forenames></author><author><keyname>Dinh</keyname><forenames>Hieu</forenames></author></authors><title>Inconsistency and Accuracy of Heuristics with A* Search</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many studies in heuristic search suggest that the accuracy of the heuristic
used has a positive impact on improving the performance of the search. In
another direction, historical research perceives that the performance of
heuristic search algorithms, such as A* and IDA*, can be improved by requiring
the heuristics to be consistent -- a property satisfied by any perfect
heuristic. However, a few recent studies show that inconsistent heuristics can
also be used to achieve a large improvement in these heuristic search
algorithms. These results leave us a natural question: which property of
heuristics, accuracy or consistency/inconsistency, should we focus on when
building heuristics? While there are studies on the heuristic accuracy with the
assumption of consistency, no studies on both the inconsistency and the
accuracy of heuristics are known to our knowledge.
  In this study, we investigate the relationship between the inconsistency and
the accuracy of heuristics with A* search. Our analytical result reveals a
correlation between these two properties. We then run experiments on the domain
for the Knapsack problem with a family of practical heuristics. Our empirical
results show that in many cases, the more accurate heuristics also have higher
level of inconsistency and result in fewer node expansions by A*.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2202</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2202</id><created>2013-07-08</created><authors><author><keyname>Gerok</keyname><forenames>Waldemar</forenames><affiliation>Leibniz Universit&#xe4;t Hannover</affiliation></author><author><keyname>Peissig</keyname><forenames>J&#xfc;rgen</forenames><affiliation>Leibniz Universit&#xe4;t Hannover</affiliation></author><author><keyname>Kaiser</keyname><forenames>Thomas</forenames><affiliation>Universit&#xe4;t Duisburg-Essen</affiliation></author></authors><title>TDOA assisted RSSD based localization using UWB and directional antennas</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the use of directional antennas for received signal
strength difference (RSSD) based localization using ultra-wideband and
demonstrates the achievable accuracy with this localization method applied to
UWB. As introduced in our previous work the RSSD localization is assisted with
one Time Difference of Arrival (TDOA) estimation. The use of directional
receiving antennas and an omni-directional transmitting antenna is assumed.
Localization is performed in 2D. Two localization approaches are considered:
RSSD using statistical channel model and fingerprinting approach. In the case
of statistical channel model simulations are performed using Matlab. In the
case of fingerprinting approach localization is done based on real
indoor-measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2203</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2203</id><created>2013-07-08</created><authors><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author><author><keyname>Bordin</keyname><forenames>Patricia</forenames></author><author><keyname>Berestycki</keyname><forenames>Henri</forenames></author><author><keyname>Gribaudi</keyname><forenames>Maurizio</forenames></author></authors><title>Self-organization versus top-down planning in the evolution of a city</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI nlin.AO</categories><comments>12 pages, 13 figures; published (online) in Nature Scientific Reports</comments><journal-ref>Nature Scientific Reports 3:2153 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interventions of central, top-down planning are serious limitations to the
possibility of modelling the dynamics of cities. An example is the city of
Paris (France), which during the 19th century experienced large modifications
supervised by a central authority, the `Haussmann period'. In this article, we
report an empirical analysis of more than 200 years (1789-2010) of the
evolution of the street network of Paris. We show that the usual network
measures display a smooth behavior and that the most important quantitative
signatures of central planning is the spatial reorganization of centrality and
the modification of the block shape distribution. Such effects can only be
obtained by structural modifications at a large-scale level, with the creation
of new roads not constrained by the existing geometry. The evolution of a city
thus seems to result from the superimposition of continuous, local growth
processes and punctual changes operating at large spatial scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2205</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2205</id><created>2013-07-08</created><updated>2013-10-24</updated><authors><author><keyname>Madry</keyname><forenames>Aleksander</forenames></author></authors><title>Navigating Central Path with Electrical Flows: from Flows to Matchings,
  and Back</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an $\tilde{O}(m^{10/7})=\tilde{O}(m^{1.43})$-time algorithm for
the maximum s-t flow and the minimum s-t cut problems in directed graphs with
unit capacities. This is the first improvement over the sparse-graph case of
the long-standing $O(m \min(\sqrt{m},n^{2/3}))$ time bound due to Even and
Tarjan [EvenT75]. By well-known reductions, this also establishes an
$\tilde{O}(m^{10/7})$-time algorithm for the maximum-cardinality bipartite
matching problem. That, in turn, gives an improvement over the celebrated
celebrated $O(m \sqrt{n})$ time bound of Hopcroft and Karp [HK73] whenever the
input graph is sufficiently sparse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2225</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2225</id><created>2013-07-08</created><authors><author><keyname>Br&#xe2;nzei</keyname><forenames>Simina</forenames></author><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Kurokawa</keyname><forenames>David</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel D.</forenames></author></authors><title>Equilibria of Generalized Cut and Choose Protocols</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classic cake cutting protocols -- which fairly allocate a divisible good
among agents with heterogeneous preferences -- are susceptible to manipulation.
Do their strategic outcomes still guarantee fairness? To answer this question
we adopt a novel algorithmic approach, proposing a concrete computational model
and reasoning about the game-theoretic properties of algorithms that operate in
this model. Specifically, we show that each protocol in the class of
generalized cut and choose (GCC) protocols -- which includes the most important
discrete cake cutting protocols -- is guaranteed to have approximate subgame
perfect Nash equilibria. Moreover, we observe that the (approximate) equilibria
of proportional protocols -- which guarantee each of the n agents a
1/n-fraction of the cake -- must be (approximately) proportional, and design a
GCC protocol where all Nash equilibrium outcomes satisfy the stronger fairness
notion of envy-freeness. Finally, we show that under an obliviousness
restriction, which still allows the computation of approximately envy-free
allocations, GCC protocols are guaranteed to have exact subgame perfect Nash
equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2228</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2228</id><created>2013-07-06</created><authors><author><keyname>Shi</keyname><forenames>Minjia</forenames></author></authors><title>The MacWilliams identity for $m$-spotty weight enumerator over
  $\mathbb{F}_2+u\mathbb{F}_2+\cdots+u^{m-1}\mathbb{F}_2$</title><categories>cs.IT math.IT</categories><comments>Research paper, under review since 18th October 2012. arXiv admin
  note: substantial text overlap with arXiv:1307.1786</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Past few years have seen an extensive use of RAM chips with wide I/O data
(e.g. 16, 32, 64 bits) in computer memory systems. These chips are highly
vulnerable to a special type of byte error, called an $m$-spotty byte error,
which can be effectively detected or corrected using byte error-control codes.
The MacWilliams identity provides the relationship between the weight
distribution of a code and that of its dual. The main purpose of this paper is
to present a version of the MacWilliams identity for $m$-spotty weight
enumerators over
$\mathbbm{F}_{2}+u\mathbbm{F}_{2}+\cdots+u^{m-1}\mathbbm{F}_{2}$ (shortly
$R_{u, m, 2}$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2262</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2262</id><created>2013-07-08</created><updated>2014-06-10</updated><authors><author><keyname>Furer</keyname><forenames>Martin</forenames></author><author><keyname>Yu</keyname><forenames>Huiwen</forenames></author></authors><title>Approximate the k-Set Packing Problem by Local Improvements</title><categories>cs.DS</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study algorithms based on local improvements for the $k$-Set Packing
problem. The well-known local improvement algorithm by Hurkens and Schrijver
has been improved by Sviridenko and Ward from $\frac{k}{2}+\epsilon$ to
$\frac{k+2}{3}$, and by Cygan to $\frac{k+1}{3}+\epsilon$ for any $\epsilon&gt;0$.
In this paper, we achieve the approximation ratio $\frac{k+1}{3}+\epsilon$ for
the $k$-Set Packing problem using a simple polynomial-time algorithm based on
the method by Sviridenko and Ward. With the same approximation guarantee, our
algorithm runs in time singly exponential in $\frac{1}{\epsilon^2}$, while the
running time of Cygan's algorithm is doubly exponential in
$\frac{1}{\epsilon}$. On the other hand, we construct an instance with locality
gap $\frac{k+1}{3}$ for any algorithm using local improvements of size
$O(n^{1/5})$, here $n$ is the total number of sets. Thus, our approximation
guarantee is optimal with respect to results achievable by algorithms based on
local improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2274</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2274</id><created>2013-07-08</created><authors><author><keyname>Harvey</keyname><forenames>Nicholas J. A.</forenames></author><author><keyname>Olver</keyname><forenames>Neil</forenames></author></authors><title>Pipage Rounding, Pessimistic Estimators and Matrix Concentration</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pipage rounding is a dependent random sampling technique that has several
interesting properties and diverse applications. One property that has been
particularly useful is negative correlation of the resulting vector.
Unfortunately negative correlation has its limitations, and there are some
further desirable properties that do not seem to follow from existing
techniques. In particular, recent concentration results for sums of independent
random matrices are not known to extend to a negatively dependent setting.
  We introduce a simple but useful technique called concavity of pessimistic
estimators. This technique allows us to show concentration of submodular
functions and concentration of matrix sums under pipage rounding. The former
result answers a question of Chekuri et al. (2009). To prove the latter result,
we derive a new variant of Lieb's celebrated concavity theorem in matrix
analysis.
  We provide numerous applications of these results. One is to spectrally-thin
trees, a spectral analog of the thin trees that played a crucial role in the
recent breakthrough on the asymmetric traveling salesman problem. We show a
polynomial time algorithm that, given a graph where every edge has effective
conductance at least $\kappa$, returns an $O(\kappa^{-1} \cdot \log n / \log
\log n)$-spectrally-thin tree. There are further applications to rounding of
semidefinite programs, to the column subset selection problem, and to a
geometric question of extracting a nearly-orthonormal basis from an isotropic
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2294</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2294</id><created>2013-07-08</created><updated>2014-07-25</updated><authors><author><keyname>Jung</keyname><forenames>Taeho</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Zhang</keyname><forenames>Lan</forenames></author></authors><title>A General Framework for Privacy-Preserving Distributed Greedy Algorithm</title><categories>cs.CR</categories><comments>This paper has been withdrawn due to personal reasons</comments><journal-ref>BigCom 2015, LNCS 9196, pp. 88-102, 2015</journal-ref><doi>10.1007/978-3-319-22047-5-8</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Increasingly more attention is paid to the privacy in online applications due
to the widespread data collection for various analysis purposes. Sensitive
information might be mined from the raw data during the analysis, and this led
to a great privacy concern among people (data providers) these days. To deal
with this privacy concerns, multitudes of privacy-preserving computation
schemes are proposed to address various computation problems, and we have found
many of them fall into a class of problems which can be solved by greedy
algorithms.
  In this paper, we propose a framework for distributed greedy algorithms in
which instances in the feasible set come from different parties. By our
framework, most generic distributed greedy algorithms can be converted to a
privacy preserving one which achieves the same result as the original greedy
algorithm while the private information associated with the instances is still
protected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2295</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2295</id><created>2013-07-08</created><updated>2013-08-22</updated><authors><author><keyname>Yu</keyname><forenames>Hao</forenames></author><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>Duality Codes and the Integrality Gap Bound for Index Coding</title><categories>cs.IT math.IT</categories><comments>20 pages, 6 figures. This version corrects some typos in the last
  one. This paper will be presented in part at the Allerton conference on
  communications, control, and computing, Monticello, IL, October, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a base station that delivers packets to multiple
receivers through a sequence of coded transmissions. All receivers overhear the
same transmissions. Each receiver may already have some of the packets as side
information, and requests another subset of the packets. This problem is known
as the index coding problem and can be represented by a bipartite digraph. An
integer linear program is developed that provides a lower bound on the minimum
number of transmissions required for any coding algorithm. Conversely, its
linear programming relaxation is shown to provide an upper bound that is
achievable by a simple form of vector linear coding. Thus, the information
theoretic optimum is bounded by the integrality gap between the integer program
and its linear relaxation. In the special case when the digraph has a planar
structure, the integrality gap is shown to be zero, so that exact optimality is
achieved. Finally, for non-planar problems, an enhanced integer program is
constructed that provides a smaller integrality gap. The dual of this problem
corresponds to a more sophisticated partial clique coding strategy that
time-shares between Reed-Solomon erasure codes. This work illuminates the
relationship between index coding, duality, and integrality gaps between
integer programs and their linear relaxations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2307</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2307</id><created>2013-07-08</created><authors><author><keyname>Zhang</keyname><forenames>Kun</forenames></author><author><keyname>Peng</keyname><forenames>Heng</forenames></author><author><keyname>Chan</keyname><forenames>Laiwan</forenames></author><author><keyname>Hyvarinen</keyname><forenames>Aapo</forenames></author></authors><title>Bridging Information Criteria and Parameter Shrinkage for Model
  Selection</title><categories>stat.ML cs.LG</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model selection based on classical information criteria, such as BIC, is
generally computationally demanding, but its properties are well studied. On
the other hand, model selection based on parameter shrinkage by $\ell_1$-type
penalties is computationally efficient. In this paper we make an attempt to
combine their strengths, and propose a simple approach that penalizes the
likelihood with data-dependent $\ell_1$ penalties as in adaptive Lasso and
exploits a fixed penalization parameter. Even for finite samples, its model
selection results approximately coincide with those based on information
criteria; in particular, we show that in some special cases, this approach and
the corresponding information criterion produce exactly the same model. One can
also consider this approach as a way to directly determine the penalization
parameter in adaptive Lasso to achieve information criteria-like model
selection. As extensions, we apply this idea to complex models including
Gaussian mixture model and mixture of factor analyzers, whose model selection
is traditionally difficult to do; by adopting suitable penalties, we provide
continuous approximators to the corresponding information criteria, which are
easy to optimize and enable efficient model selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2312</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2312</id><created>2013-07-08</created><authors><author><keyname>Oyen</keyname><forenames>Diane</forenames></author><author><keyname>Lane</keyname><forenames>Terran</forenames></author></authors><title>Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning</title><categories>stat.ML cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian network structure learning algorithms with limited data are being
used in domains such as systems biology and neuroscience to gain insight into
the underlying processes that produce observed data. Learning reliable networks
from limited data is difficult, therefore transfer learning can improve the
robustness of learned networks by leveraging data from related tasks. Existing
transfer learning algorithms for Bayesian network structure learning give a
single maximum a posteriori estimate of network models. Yet, many other models
may be equally likely, and so a more informative result is provided by Bayesian
structure discovery. Bayesian structure discovery algorithms estimate posterior
probabilities of structural features, such as edges. We present transfer
learning for Bayesian structure discovery which allows us to explore the shared
and unique structural features among related tasks. Efficient computation
requires that our transfer learning objective factors into local calculations,
which we prove is given by a broad class of transfer biases. Theoretically, we
show the efficiency of our approach. Empirically, we show that compared to
single task learning, transfer learning is better able to positively identify
true edges. We apply the method to whole-brain neuroimaging data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2313</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2313</id><created>2013-07-08</created><updated>2014-04-15</updated><authors><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author><author><keyname>Mozes</keyname><forenames>Shay</forenames></author><author><keyname>Weimann</keyname><forenames>Oren</forenames></author></authors><title>Improved Submatrix Maximum Queries in Monge Matrices</title><categories>cs.DS</categories><comments>To appear in ICALP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present efficient data structures for submatrix maximum queries in Monge
matrices and Monge partial matrices. For $n\times n$ Monge matrices, we give a
data structure that requires O(n) space and answers submatrix maximum queries
in $O(\log n)$ time. The best previous data structure [Kaplan et al., SODA`12]
required $O(n \log n)$ space and $O(\log^2 n)$ query time. We also give an
alternative data structure with constant query-time and $ O(n^{1+\varepsilon})$
construction time and space for any fixed $\varepsilon&lt;1$. For $n\times n$ {\em
partial} Monge matrices we obtain a data structure with O(n) space and $O(\log
n \cdot \alpha(n))$ query time. The data structure of Kaplan et al. required
$O(n \log n \cdot \alpha(n))$ space and $O(\log^2 n)$ query time.
  Our improvements are enabled by a technique for exploiting the structure of
the upper envelope of Monge matrices to efficiently report column maxima in
skewed rectangular Monge matrices. We hope this technique can be useful in
obtaining faster search algorithms in Monge partial matrices. In addition, we
give a linear upper bound on the number of breakpoints in the upper envelope of
a Monge partial matrix. This shows that the inverse Ackermann $\alpha(n)$ term
in the analysis of the data structure of Kaplan et. al is superfluous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2320</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2320</id><created>2013-07-08</created><authors><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Huang</keyname><forenames>Huang</forenames></author></authors><title>Dynamic Partial Cooperative MIMO System for Delay-Sensitive Applications
  with Limited Backhaul Capacity</title><categories>cs.IT cs.NI cs.PF math.IT</categories><comments>31 pages, 6 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering backhaul consumption in practical systems, it may not be the best
choice to engage all the time in full cooperative MIMO for interference
mitigation. In this paper, we propose a novel downlink partial cooperative MIMO
(Pco-MIMO) physical layer (PHY) scheme, which allows flexible tradeoff between
the partial data cooperation level and the backhaul consumption. Based on this
Pco-MIMO scheme, we consider dynamic transmit power and rate allocation
according to the imperfect channel state information at transmitters (CSIT) and
the queue state information (QSI) to minimize the average delay cost subject to
average backhaul consumption constraints and average power constraints. The
delay-optimal control problem is formulated as an infinite horizon average cost
constrained partially observed Markov decision process (CPOMDP). By exploiting
the special structure in our problem, we derive an equivalent Bellman Equation
to solve the CPOMDP. To reduce computational complexity and facilitate
distributed implementation, we propose a distributed online learning algorithm
to estimate the per-flow potential functions and Lagrange multipliers (LMs) and
a distributed online stochastic partial gradient algorithm to obtain the power
and rate control policy. The proposed low-complexity distributed solution is
based on local observations of the system states at the BSs and is very robust
against model variations. We also prove the convergence and the asymptotic
optimality of the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2328</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2328</id><created>2013-07-08</created><authors><author><keyname>Felgenhauer</keyname><forenames>Bertram</forenames></author><author><keyname>Avanzini</keyname><forenames>Martin</forenames></author><author><keyname>Sternagel</keyname><forenames>Christian</forenames></author></authors><title>A Haskell Library for Term Rewriting</title><categories>cs.PL</categories><comments>1st International Workshop on Haskell And Rewriting Techniques, HART
  2013, 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Haskell library for first-order term rewriting covering basic
operations on positions, terms, contexts, substitutions and rewrite rules. This
effort is motivated by the increasing number of term rewriting tools that are
written in Haskell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2342</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2342</id><created>2013-07-09</created><updated>2014-07-02</updated><authors><author><keyname>Vaiter</keyname><forenames>Samuel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Golbabaee</keyname><forenames>Mohammad</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Fadili</keyname><forenames>Jalal M.</forenames><affiliation>GREYC</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author></authors><title>Model Selection with Low Complexity Priors</title><categories>math.OC cs.IT math.IT math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regularization plays a pivotal role when facing the challenge of solving
ill-posed inverse problems, where the number of observations is smaller than
the ambient dimension of the object to be estimated. A line of recent work has
studied regularization models with various types of low-dimensional structures.
In such settings, the general approach is to solve a regularized optimization
problem, which combines a data fidelity term and some regularization penalty
that promotes the assumed low-dimensional/simple structure. This paper provides
a general framework to capture this low-dimensional structure through what we
coin partly smooth functions relative to a linear manifold. These are convex,
non-negative, closed and finite-valued functions that will promote objects
living on low-dimensional subspaces. This class of regularizers encompasses
many popular examples such as the L1 norm, L1-L2 norm (group sparsity), as well
as several others including the Linfty norm. We also show that the set of
partly smooth functions relative to a linear manifold is closed under addition
and pre-composition by a linear operator, which allows to cover mixed
regularization, and the so-called analysis-type priors (e.g. total variation,
fused Lasso, finite-valued polyhedral gauges). Our main result presents a
unified sharp analysis of exact and robust recovery of the low-dimensional
subspace model associated to the object to recover from partial measurements.
This analysis is illustrated on a number of special and previously studied
cases, and on an analysis of the performance of Linfty regularization in a
compressed sensing scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2347</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2347</id><created>2013-07-09</created><updated>2013-11-15</updated><authors><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author><author><keyname>Tomescu</keyname><forenames>Alexandru I.</forenames></author></authors><title>Combinatorial decomposition approaches for efficient counting and random
  generation FPTASes</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a combinatorial decomposition for a counting problem, we resort to the
simple scheme of approximating large numbers by floating-point representations
in order to obtain efficient Fully Polynomial Time Approximation Schemes
(FPTASes) for it. The number of bits employed for the exponent and the mantissa
will depend on the error parameter $0 &lt; \varepsilon \leq 1$ and on the
characteristics of the problem. Accordingly, we propose the first FPTASes with
$1 \pm \varepsilon$ relative error for counting and generating uniformly at
random a labeled DAG with a given number of vertices. This is accomplished
starting from a classical recurrence for counting DAGs, whose values we
approximate by floating-point numbers.
  After extending these results to other families of DAGs, we show how the same
approach works also with problems where we are given a compact representation
of a combinatorial ensemble and we are asked to count and sample elements from
it. We employ here the floating-point approximation method to transform the
classic pseudo-polynomial algorithm for counting 0/1 Knapsack solutions into a
very simple FPTAS with $1 - \varepsilon$ relative error. Its complexity
improves upon the recent result (\v{S}tefankovi\v{c} et al., SIAM J. Comput.,
2012), and, when $\varepsilon^{-1} = \Omega(n)$, also upon the best-known
randomized algorithm (Dyer, STOC, 2003). To show the versatility of this
technique, we also apply it to a recent generalization of the problem of
counting 0/1 Knapsack solutions in an arc-weighted DAG, obtaining a faster and
simpler FPTAS than the existing one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2348</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2348</id><created>2013-07-09</created><updated>2013-07-15</updated><authors><author><keyname>Davtyan</keyname><forenames>N. N.</forenames></author></authors><title>On the $\mu$-parameters of the Petersen graph</title><categories>cs.DM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.1389; and
  substantial text overlap with arXiv:1205.0125, arXiv:1307.1389 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an undirected, simple, finite, connected graph $G$, we denote by $V(G)$
and $E(G)$ the sets of its vertices and edges, respectively. A function
$\varphi:E(G)\rightarrow \{1,...,t\}$ is called a proper edge $t$-coloring of a
graph $G$, if adjacent edges are colored differently and each of $t$ colors is
used. The least value of $t$ for which there exists a proper edge $t$-coloring
of a graph $G$ is denoted by $\chi'(G)$. For any graph $G$, and for any integer
$t$ satisfying the inequality $\chi'(G)\leq t\leq |E(G)|$, we denote by
$\alpha(G,t)$ the set of all proper edge $t$-colorings of $G$. Let us also
define a set $\alpha(G)$ of all proper edge colorings of a graph $G$: $$
\alpha(G)\equiv\bigcup_{t=\chi'(G)}^{|E(G)|}\alpha(G,t). $$
  An arbitrary nonempty finite subset of consecutive integers is called an
interval. If $\varphi\in\alpha(G)$ and $x\in V(G)$, then the set of colors of
edges of $G$ which are incident with $x$ is denoted by $S_G(x,\varphi)$ and is
called a spectrum of the vertex $x$ of the graph $G$ at the proper edge
coloring $\varphi$. If $G$ is a graph and $\varphi\in\alpha(G)$, then define
$f_G(\varphi)\equiv|\{x\in V(G)/S_G(x,\varphi) \textrm{is an interval}\}|$.
  For a graph $G$ and any integer $t$, satisfying the inequality $\chi'(G)\leq
t\leq |E(G)|$, we define: $$
\mu_1(G,t)\equiv\min_{\varphi\in\alpha(G,t)}f_G(\varphi),\qquad
\mu_2(G,t)\equiv\max_{\varphi\in\alpha(G,t)}f_G(\varphi). $$
  For any graph $G$, we set: $$ \mu_{11}(G)\equiv\min_{\chi'(G)\leq
t\leq|E(G)|}\mu_1(G,t),\qquad \mu_{12}(G)\equiv\max_{\chi'(G)\leq
t\leq|E(G)|}\mu_1(G,t), $$ $$ \mu_{21}(G)\equiv\min_{\chi'(G)\leq
t\leq|E(G)|}\mu_2(G,t),\qquad \mu_{22}(G)\equiv\max_{\chi'(G)\leq
t\leq|E(G)|}\mu_2(G,t). $$
  For the Petersen graph, the exact values of the parameters $\mu_{11}$,
$\mu_{12}$, $\mu_{21}$ and $\mu_{22}$ are found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2350</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2350</id><created>2013-07-09</created><authors><author><keyname>Xiong</keyname><forenames>Junlin</forenames></author><author><keyname>Lam</keyname><forenames>James</forenames></author><author><keyname>Shu</keyname><forenames>Zhan</forenames></author><author><keyname>Mao</keyname><forenames>Xuerong</forenames></author></authors><title>Stability Analysis of Continuous-Time Switched Systems with a Random
  Switching Signal</title><categories>cs.SY</categories><comments>6 pages, 6 figures, accepted by IEEE-TAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the stability analysis of continuous-time
switched systems with a random switching signal. The switching signal manifests
its characteristics with that the dwell time in each subsystem consists of a
fixed part and a random part. The stochastic stability of such switched systems
is studied using a Lyapunov approach. A necessary and sufficient condition is
established in terms of linear matrix inequalities. The effect of the random
switching signal on system stability is illustrated by a numerical example and
the results coincide with our intuition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2352</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2352</id><created>2013-07-09</created><updated>2013-07-10</updated><authors><author><keyname>Trifonov</keyname><forenames>Peter</forenames></author><author><keyname>Miloslavskaya</keyname><forenames>Vera</forenames></author></authors><title>Polar Codes with Dynamic Frozen Symbols and Their Decoding by Directed
  Search</title><categories>cs.IT math.IT</categories><comments>Accepted to ITW2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel construction of polar codes with dynamic frozen symbols is proposed.
The proposed codes are subcodes of extended BCH codes, which ensure
sufficiently high minimum distance. Furthermore, a decoding algorithm is
proposed, which employs estimates of the not-yet-processed bit channel error
probabilities to perform directed search in code tree, reducing thus the total
number of iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2354</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2354</id><created>2013-07-09</created><authors><author><keyname>Motoyama</keyname><forenames>Kazutaka</forenames></author><author><keyname>Tanaka</keyname><forenames>Yoshikazu</forenames></author><author><keyname>Aida</keyname><forenames>Kento</forenames></author><author><keyname>Sakane</keyname><forenames>Eisaku</forenames></author><author><keyname>Miura</keyname><forenames>Kenichi</forenames></author></authors><title>Effective System for Simulating Dust Continuum Observations on
  Distributed Computing Resources</title><categories>astro-ph.IM cs.DC</categories><comments>7 pages, 9 figures, accepted for publication in Journal of Space
  Science Informatics Japan</comments><report-no>JAXA-RR-13-010</report-no><journal-ref>Journal of Space Science Informatics Japan, Volume 3, pp. 155-161,
  (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an effective system for simulating dust continuum observations by
radiative transfer simulations. By using workflow management system RENKEI-WFT,
we utilized distributed computing resources and automated a sequence of
computational tasks required for radiative transfer modeling, namely, main
radiative transfer simulations, pre-/post-processes, and data transfer between
computing resources. Our system simultaneously executes a lot of radiative
transfer simulations with different input parameters on distributed computing
resources. This capability of our system enables us to conduct effective
research by radiative transfer simulation. As a demonstration of our system, we
simulated dust continuum observations of protoplanetary disk. We performed
hydrodynamic simulation modeling photoevaporating protoplanetary disk
irradiated by ultra violet radiation from nearby massive stars. Results of this
hydrodynamic simulation were used as input data for radiative transfer
simulations. Expected spectral energy distributions and intensity maps were
obtained by our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2380</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2380</id><created>2013-07-09</created><updated>2013-07-12</updated><authors><author><keyname>Seneviratne</keyname><forenames>Sena</forenames></author><author><keyname>Levy</keyname><forenames>David C.</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>A Taxonomy of Performance Prediction Systems in the Parallel and
  Distributed Computing Grids</title><categories>cs.DC</categories><comments>35 pages,4 figures,2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As Grids are loosely-coupled congregations of geographically distributed
heterogeneous resources, the efficient utilization of the resources requires
the support of a sound Performance Prediction System (PPS). The performance
prediction of grid resources is helpful for both Resource Management Systems
and grid users to make optimized resource usage decisions. There have been many
PPS projects that span over several grid resources in several dimensions. In
this paper the taxonomy for describing the PPS architecture is discussed. The
taxonomy is used to categorize and identify approaches which are followed in
the implementation of the existing PPSs for Grids. The taxonomy and the survey
results are used to identify approaches and issues that have not been fully
explored in research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2381</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2381</id><created>2013-07-09</created><authors><author><keyname>Zhuansun</keyname><forenames>Guilin</forenames></author><author><keyname>Xiong</keyname><forenames>Junlin</forenames></author></authors><title>Local Mode Dependent Decentralized $H_{\infty}$ Control of Uncertain
  Markovian Jump Large-scale Systems</title><categories>cs.SY</categories><comments>19 pages, 4 figures, accepted by IET-CTA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of robust $H_{\infty}$ control using
decentralized state feedback controllers for a class of large-scale systems
with Markov jump parameters. A sufficient condition is developed to design
controllers using local system states and local system operation modes. The
sufficient condition is given in terms of rank constrained linear matrix
inequalities. An illustrative numerical example is given to demonstrate the
developed theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2397</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2397</id><created>2013-07-09</created><authors><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>From Technology-Driven Society to Socially Oriented Technology. The
  Future of Information Society -- Alternatives to Surveillance</title><categories>physics.soc-ph cs.CY</categories><comments>For related work see http://www.futurict.eu and the twitter,
  facebook, blog and vimeo channels of the FuturICT initiative</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our society is changing. Almost nothing these days works without a computer
chip. Computing power doubles every 18 months, and in ten years it will
probably exceed the capabilities of a human brain. Computers perform
approximately 70 percent of all financial transactions today and IBM's Watson
now seems to give better customer advise than some human telephone hotlines.
What does this imply for our future society?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2411</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2411</id><created>2013-07-09</created><updated>2014-03-14</updated><authors><author><keyname>Keszegh</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author></authors><title>Convex Polygons are Self-Coverable</title><categories>math.MG cs.CG cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new notion for geometric families called self-coverability and
show that homothets of convex polygons are self-coverable. As a corollary, we
obtain several results about coloring point sets such that any member of the
family with many points contains all colors. This is dual (and in some cases
equivalent) to the much investigated cover-decomposability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2415</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2415</id><created>2013-07-09</created><authors><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Keller</keyname><forenames>Orgad</forenames></author><author><keyname>Lewenstein</keyname><forenames>Moshe</forenames></author><author><keyname>Roditty</keyname><forenames>Liam</forenames></author></authors><title>Finding the Minimum-Weight k-Path</title><categories>cs.DS</categories><comments>To appear at WADS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a weighted $n$-vertex graph $G$ with integer edge-weights taken from a
range $[-M,M]$, we show that the minimum-weight simple path visiting $k$
vertices can be found in time $\tilde{O}(2^k \poly(k) M n^\omega) = O^*(2^k
M)$. If the weights are reals in $[1,M]$, we provide a
$(1+\varepsilon)$-approximation which has a running time of $\tilde{O}(2^k
\poly(k) n^\omega(\log\log M + 1/\varepsilon))$. For the more general problem
of $k$-tree, in which we wish to find a minimum-weight copy of a $k$-node tree
$T$ in a given weighted graph $G$, under the same restrictions on edge weights
respectively, we give an exact solution of running time $\tilde{O}(2^k \poly(k)
M n^3) $ and a $(1+\varepsilon)$-approximate solution of running time
$\tilde{O}(2^k \poly(k) n^3(\log\log M + 1/\varepsilon))$. All of the above
algorithms are randomized with a polynomially-small error probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2421</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2421</id><created>2013-07-09</created><authors><author><keyname>Huang</keyname><forenames>Yi</forenames></author><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Qiu</keyname><forenames>Ling</forenames></author></authors><title>Energy Efficient Coordinated Beamforming for Multi-cell MISO Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, to be presented in IEEE GLOBECOM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the optimal energy efficient coordinated
beamforming in multi-cell multiple-input single-output (MISO) systems with $K$
multiple-antenna base stations (BS) and $K$ single-antenna mobile stations
(MS), where each BS sends information to its own intended MS with cooperatively
designed transmit beamforming. We assume single user detection at the MS by
treating the interference as noise. By taking into account a realistic power
model at the BS, we characterize the Pareto boundary of the achievable energy
efficiency (EE) region of the $K$ links, where the EE of each link is defined
as the achievable data rate at the MS divided by the total power consumption at
the BS. Since the EE of each link is non-cancave (which is a non-concave
function over an affine function), characterizing this boundary is difficult.
To meet this challenge, we relate this multi-cell MISO system to cognitive
radio (CR) MISO channels by applying the concept of interference temperature
(IT), and accordingly transform the EE boundary characterization problem into a
set of fractional concave programming problems. Then, we apply the fractional
concave programming technique to solve these fractional concave problems, and
correspondingly give a parametrization for the EE boundary in terms of IT
levels. Based on this characterization, we further present a decentralized
algorithm to implement the multi-cell coordinated beamforming, which is shown
by simulations to achieve the EE Pareto boundary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2427</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2427</id><created>2013-07-09</created><authors><author><keyname>Pocci</keyname><forenames>M.</forenames></author><author><keyname>Demongodin</keyname><forenames>I.</forenames></author><author><keyname>Giambiasi</keyname><forenames>N.</forenames></author><author><keyname>Giua</keyname><forenames>A.</forenames></author></authors><title>Testing experiments on synchronized Petri nets</title><categories>cs.SY cs.FL</categories><comments>26 pages, 10 figures, 3 tables</comments><report-no>Tech. Rep., 01 2013</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synchronizing sequences have been proposed in the late 60's to solve testing
problems on systems modeled by finite state machines. Such sequences lead a
system, seen as a black box, from an unknown current state to a known final
one. This paper presents a first investigation of the computation of
synchronizing sequences for systems modeled by bounded synchronized Petri nets.
In the first part of the paper, existing techniques for automata are adapted to
this new setting. Later on, new approaches, that exploit the net structure to
efficiently compute synchronizing sequences without an exhaustive enumeration
of the state space, are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2430</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2430</id><created>2013-07-09</created><authors><author><keyname>Lin</keyname><forenames>Pin-Hsun</forenames></author><author><keyname>Su</keyname><forenames>Chien-Li</forenames></author><author><keyname>Su</keyname><forenames>Hsuan-Jung</forenames></author></authors><title>On The Fast Fading Multiple-Antenna Gaussian Broadcast Channel with
  Confidential Messages and Partial CSIT</title><categories>cs.IT math.IT</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wiretap channels the eavesdropper's channel state information (CSI) is
commonly assumed to be known at transmitter, fully or partially. However, under
perfect secrecy constraint the eavesdropper may not be motivated to feedback
any correct CSI. In this paper we consider a more feasible problem for the
transmitter to have eavesdropper's CSI. That is, the fast fading
multiple-antenna Gaussian broadcast channels (FMGBC-CM) with confidential
messages, where both receivers are legitimate users such that they both are
willing to feedback accurate CSI to maintain their secure transmission, and not
to be eavesdropped by the other. We assume that only the statistics of the
channel state information are known by the transmitter. We first show the
necessary condition for the FMGBC-CM not to be degraded to the common wiretap
channels. Then we derive the achievable rate region for the FMGBC-CM where the
channel input covariance matrices and the inflation factor are left unknown and
to be solved. After that we provide an analytical solution to the channel input
covariance matrices. We also propose an iterative algorithm to solve the
channel input covariance matrices and the inflation factor. Due to the
complicated rate region formulae in normal SNR, we resort to low SNR analysis
to investigate the characteristics of the channel. Finally, numerical examples
show that under perfect secrecy constraint both users can achieve positive
rates simultaneously, which verifies our derived necessary condition. Numerical
results also elucidate the effectiveness of the analytic solution and proposed
algorithm of solving the channel input covariance matrices and the inflation
factor under different conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2432</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2432</id><created>2013-07-09</created><authors><author><keyname>Olenko</keyname><forenames>Andriy</forenames></author><author><keyname>Pog&#xe1;ny</keyname><forenames>Tibor</forenames></author></authors><title>Average sampling restoration of harmonizable processes</title><categories>math.PR cs.IT math.IT</categories><comments>16 pages. This is an Author's Accepted Manuscript of an article
  published in the Communications in Statistics - Theory and Methods, Vol. 40,
  Issue 19-20, 2011, 3587-3598. [copyright Taylor \&amp; Francis], available online
  at: http://www.tandfonline.com/ [DOI: 10.1080/03610926.2011.581180]</comments><msc-class>60G12, 94A20, 42C15</msc-class><journal-ref>Communications in Statistics - Theory and Methods, Vol. 40, Issue
  19-20, 2011, 3587-3598</journal-ref><doi>10.1080/03610926.2011.581180</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The harmonizable Piranashvili-type stochastic processes are approximated by
finite time shifted average sampling sums. Explicit truncation error upper
bounds are established. Various corollaries and special cases are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2434</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2434</id><created>2013-07-09</created><authors><author><keyname>Al-Wassai</keyname><forenames>Firouz A.</forenames></author><author><keyname>Kalyankar</keyname><forenames>N. V.</forenames></author></authors><title>Major Limitations of Satellite images</title><categories>cs.CV</categories><journal-ref>Journal of Global Research in Computer Science, 4 (5), May 2013,
  51-59</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote sensing has proven to be a powerful tool for the monitoring of the
Earth surface to improve our perception of our surroundings has led to
unprecedented developments in sensor and information technologies. However,
technologies for effective use of the data and for extracting useful
information from the data of Remote sensing are still very limited since no
single sensor combines the optimal spectral, spatial and temporal resolution.
This paper briefly reviews the limitations of satellite remote sensing. Also,
reviews on the problems of image fusion techniques. The conclusion of this,
According to literature, the remote sensing is still the lack of software tools
for effective information extraction from remote sensing data. The trade-off in
spectral and spatial resolution will remain and new advanced data fusion
approaches are needed to make optimal use of remote sensors for extract the
most useful information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2438</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2438</id><created>2013-07-09</created><updated>2014-04-09</updated><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author></authors><title>Efficient Probabilistic Group Testing Based on Traitor Tracing</title><categories>cs.IT cs.CR math.IT</categories><comments>8 pages, 3 figures, 1 table</comments><journal-ref>51st Annual Allerton Conference on Communication, Control and
  Computing (Allerton), pp. 1458-1465, 2013</journal-ref><doi>10.1109/Allerton.2013.6736699</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by recent results from collusion-resistant traitor tracing, we
provide a framework for constructing efficient probabilistic group testing
schemes. In the traditional group testing model, our scheme asymptotically
requires T ~ 2 K ln N tests to find (with high probability) the correct set of
K defectives out of N items. The framework is also applied to several noisy
group testing and threshold group testing models, often leading to improvements
over previously known results, but we emphasize that this framework can be
applied to other variants of the classical model as well, both in adaptive and
in non-adaptive settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2440</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2440</id><created>2013-07-09</created><authors><author><keyname>Al-Wassai</keyname><forenames>Firouz Abdullah</forenames></author><author><keyname>Kalyankar</keyname><forenames>N. V.</forenames></author></authors><title>Image Fusion Technologies In Commercial Remote Sensing Packages</title><categories>cs.CV</categories><comments>Keywords: Commercial Processing Systems, Image Fusion, quality
  evaluation</comments><journal-ref>Journal of Global Research in Computer Science, 4 (5), May 2013,
  44-50</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several remote sensing software packages are used to the explicit purpose of
analyzing and visualizing remotely sensed data, with the developing of remote
sensing sensor technologies from last ten years. Accord-ing to literature, the
remote sensing is still the lack of software tools for effective information
extraction from remote sensing data. So, this paper provides a state-of-art of
multi-sensor image fusion technologies as well as review on the quality
evaluation of the single image or fused images in the commercial remote sensing
pack-ages. It also introduces program (ALwassaiProcess) developed for image
fusion and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2457</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2457</id><created>2013-06-10</created><authors><author><keyname>Bujack</keyname><forenames>Roxana</forenames></author><author><keyname>Scheuermann</keyname><forenames>Gerik</forenames></author><author><keyname>Hitzer</keyname><forenames>Eckhard</forenames></author></authors><title>Detection of Outer Rotations on 3D-Vector Fields with Iterative
  Geometric Correlation and its Efficiency</title><categories>cs.CV cs.GR</categories><comments>accepted for publication in Advances in Applied Clifford Algebras,
  (2013). arXiv admin note: substantial text overlap with arXiv:1306.2195</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation is a common technique for the detection of shifts. Its
generalization to the multidimensional geometric correlation in Clifford
algebras has been proven a useful tool for color image processing, because it
additionally contains information about a rotational misalignment. But so far
the exact correction of a three-dimensional outer rotation could only be
achieved in certain special cases. In this paper we prove that applying the
geometric correlation iteratively has the potential to detect the outer
rotational misalignment for arbitrary three-dimensional vector fields. We
further present the explicit iterative algorithm, analyze its efficiency
detecting the rotational misalignment in the color space of a color image. The
experiments suggest a method for the acceleration of the algorithm, which is
practically tested with great success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2467</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2467</id><created>2013-07-09</created><authors><author><keyname>Pfaltz</keyname><forenames>John L.</forenames></author></authors><title>The Irreducible Spine(s) of Undirected Networks</title><categories>cs.DM math.CO</categories><comments>Submitted to WISE 2013</comments><acm-class>G.2.2; G.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using closure concepts, we show that within every undirected network, or
graph, there is a unique irreducible subgraph which we call its &quot;spine&quot;. The
chordless cycles which comprise this irreducible core effectively characterize
the connectivity structure of the network as a whole. In particular, it is
shown that the center of the network, whether defined by distance or
betweenness centrality, is effectively contained in this spine. By counting the
number of cycles of length 3 &lt;= k &lt;= max_length, we can also create a kind of
signature that can be used to identify the network. Performance is analyzed,
and the concepts we develop are illurstrated by means of a relatively small
running sample network of about 400 nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2473</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2473</id><created>2013-07-09</created><authors><author><keyname>Ghica</keyname><forenames>Dan R.</forenames></author><author><keyname>Smith</keyname><forenames>Alex</forenames></author></authors><title>From bounded affine types to automatic timing analysis</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded linear types have proved to be useful for automated resource analysis
and control in functional programming languages. In this paper we introduce an
affine bounded linear typing discipline on a general notion of resource which
can be modeled in a semiring. For this type system we provide both a general
type-inference procedure, parameterized by the decision procedure of the
semiring equational theory, and a (coherent) categorical semantics. This is a
very useful type-theoretic and denotational framework for many applications to
resource-sensitive compilation, and it represents a generalization of several
existing type systems. As a non-trivial instance, motivated by our ongoing work
on hardware compilation, we present a complex new application to calculating
and controlling timing of execution in a (recursion-free) higher-order
functional programming language with local store.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2482</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2482</id><created>2013-07-09</created><updated>2014-04-13</updated><authors><author><keyname>Jakovetic</keyname><forenames>Dusan</forenames></author><author><keyname>Moura</keyname><forenames>Jose M. F.</forenames></author><author><keyname>Xavier</keyname><forenames>Joao</forenames></author></authors><title>Linear Convergence Rate of a Class of Distributed Augmented Lagrangian
  Algorithms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed optimization where nodes cooperatively minimize the sum
of their individual, locally known, convex costs $f_i(x)$'s, $x \in {\mathbb
R}^d$ is global. Distributed augmented Lagrangian (AL) methods have good
empirical performance on several signal processing and learning applications,
but there is limited understanding of their convergence rates and how it
depends on the underlying network. This paper establishes globally linear
(geometric) convergence rates of a class of deterministic and randomized
distributed AL methods, when the $f_i$'s are twice continuously differentiable
and have a bounded Hessian. We give explicit dependence of the convergence
rates on the underlying network parameters. Simulations illustrate our
analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2483</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2483</id><created>2013-07-09</created><authors><author><keyname>Vaidya</keyname><forenames>Nitin H.</forenames></author></authors><title>Iterative Byzantine Vector Consensus in Incomplete Graphs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses Byzantine vector consensus (BVC), wherein the input at
each process is a d-dimensional vector of reals, and each process is expected
to decide on a decision vector that is in the convex hull of the input vectors
at the fault-free processes [3, 8]. The input vector at each process may also
be viewed as a point in the d-dimensional Euclidean space R^d, where d &gt; 0 is a
?nite integer. Recent work [3, 8] has addressed Byzantine vector consensus in
systems that can be modeled by a complete graph. This paper considers Byzantine
vector consensus in incomplete graphs. In particular, we address a particular
class of iterative algorithms in incomplete graphs, and prove a necessary
condition, and a sufficient condition, for the graphs to be able to solve the
vector consensus problem iteratively. We present an iterative Byzantine vector
consensus algorithm, and prove it correct under the sufficient condition. The
necessary condition presented in this paper for vector consensus does not match
with the sufficient condition for d &gt; 1; thus, a weaker condition may
potentially suffice for Byzantine vector consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2499</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2499</id><created>2013-07-09</created><updated>2014-01-28</updated><authors><author><keyname>Zheng</keyname><forenames>Shenggen</forenames></author><author><keyname>Gruska</keyname><forenames>Jozef</forenames></author><author><keyname>Qiu</keyname><forenames>Daowen</forenames></author></authors><title>On the state complexity of semi-quantum finite automata</title><categories>cs.FL quant-ph</categories><comments>19 pages. We improve (make stronger) the results in section 3</comments><journal-ref>RAIRO-Inf. Theor. Appl. 48 (2014) 187-207. Earlier versions at
  LATA'14</journal-ref><doi>10.1051/ita/2014003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some of the most interesting and important results concerning quantum finite
automata are those showing that they can recognize certain languages with
(much) less resources than corresponding classical finite automata
\cite{Amb98,Amb09,AmYa11,Ber05,Fre09,Mer00,Mer01,Mer02,Yak10,ZhgQiu112,Zhg12}.
This paper shows three results of such a type that are stronger in some sense
than other ones because (a) they deal with models of quantum automata with very
little quantumness (so-called semi-quantum one- and two-way automata with one
qubit memory only); (b) differences, even comparing with probabilistic
classical automata, are bigger than expected; (c) a trade-off between the
number of classical and quantum basis states needed is demonstrated in one case
and (d) languages (or the promise problem) used to show main results are very
simple and often explored ones in automata theory or in communication
complexity, with seemingly little structure that could be utilized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2520</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2520</id><created>2013-07-09</created><authors><author><keyname>Kumar</keyname><forenames>Nirman</forenames></author><author><keyname>Raichel</keyname><forenames>Benjamin</forenames></author></authors><title>Fault Tolerant Clustering Revisited</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In discrete k-center and k-median clustering, we are given a set of points P
in a metric space M, and the task is to output a set C \subseteq ? P, |C| = k,
such that the cost of clustering P using C is as small as possible. For
k-center, the cost is the furthest a point has to travel to its nearest center,
whereas for k-median, the cost is the sum of all point to nearest center
distances. In the fault-tolerant versions of these problems, we are given an
additional parameter 1 ?\leq \ell \leq ? k, such that when computing the cost
of clustering, points are assigned to their \ell-th nearest-neighbor in C,
instead of their nearest neighbor. We provide constant factor approximation
algorithms for these problems that are both conceptually simple and highly
practical from an implementation stand-point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2521</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2521</id><created>2013-07-09</created><authors><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Philip</keyname><forenames>Geevarghese</forenames></author><author><keyname>Ray</keyname><forenames>Saurabh</forenames></author></authors><title>Point Line Cover: The Easy Kernel is Essentially Tight</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The input to the NP-hard Point Line Cover problem (PLC) consists of a set $P$
of $n$ points on the plane and a positive integer $k$, and the question is
whether there exists a set of at most $k$ lines which pass through all points
in $P$. A simple polynomial-time reduction reduces any input to one with at
most $k^2$ points. We show that this is essentially tight under standard
assumptions. More precisely, unless the polynomial hierarchy collapses to its
third level, there is no polynomial-time algorithm that reduces every instance
$(P,k)$ of PLC to an equivalent instance with $O(k^{2-\epsilon})$ points, for
any $\epsilon&gt;0$. This answers, in the negative, an open problem posed by
Lokshtanov (PhD Thesis, 2009).
  Our proof uses the machinery for deriving lower bounds on the size of kernels
developed by Dell and van Melkebeek (STOC 2010). It has two main ingredients:
We first show, by reduction from Vertex Cover, that PLC---conditionally---has
no kernel of total size $O(k^{2-\epsilon})$ bits. This does not directly imply
the claimed lower bound on the number of points, since the best known
polynomial-time encoding of a PLC instance with $n$ points requires
$\omega(n^{2})$ bits. To get around this we build on work of Goodman et al.
(STOC 1989) and devise an oracle communication protocol of cost $O(n\log n)$
for PLC; its main building block is a bound of $O(n^{O(n)})$ for the order
types of $n$ points that are not necessarily in general position, and an
explicit algorithm that enumerates all possible order types of n points. This
protocol and the lower bound on total size together yield the stated lower
bound on the number of points.
  While a number of essentially tight polynomial lower bounds on total sizes of
kernels are known, our result is---to the best of our knowledge---the first to
show a nontrivial lower bound for structural/secondary parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2531</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2531</id><created>2013-07-09</created><authors><author><keyname>Bienkowski</keyname><forenames>Marcin</forenames></author><author><keyname>Byrka</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Je&#x17c;</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Sgall</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Better Approximation Bounds for the Joint Replenishment Problem</title><categories>cs.DS</categories><msc-class>68W25, 68W27, 90C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Joint Replenishment Problem (JRP) deals with optimizing shipments of
goods from a supplier to retailers through a shared warehouse. Each shipment
involves transporting goods from the supplier to the warehouse, at a fixed cost
C, followed by a redistribution of these goods from the warehouse to the
retailers that ordered them, where transporting goods to a retailer $\rho$ has
a fixed cost $c_\rho$. In addition, retailers incur waiting costs for each
order. The objective is to minimize the overall cost of satisfying all orders,
namely the sum of all shipping and waiting costs.
  JRP has been well studied in Operations Research and, more recently, in the
area of approximation algorithms. For arbitrary waiting cost functions, the
best known approximation ratio is 1.8. This ratio can be reduced to 1.574 for
the JRP-D model, where there is no cost for waiting but orders have deadlines.
As for hardness results, it is known that the problem is APX-hard and that the
natural linear program for JRP has integrality gap at least 1.245. Both results
hold even for JRP-D. In the online scenario, the best lower and upper bounds on
the competitive ratio are 2.64 and 3, respectively. The lower bound of 2.64
applies even to the restricted version of JRP, denoted JRP-L, where the waiting
cost function is linear.
  We provide several new approximation results for JRP. In the offline case, we
give an algorithm with ratio 1.791, breaking the barrier of 1.8. In the online
case, we show a lower bound of 2.754 on the competitive ratio for JRP-L (and
thus JRP as well), improving the previous bound of 2.64. We also study the
online version of JRP-D, for which we prove that the optimal competitive ratio
is 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2536</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2536</id><created>2013-07-09</created><authors><author><keyname>Mastin</keyname><forenames>Andrew</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author></authors><title>Greedy Online Bipartite Matching on Random Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the average performance of online greedy matching algorithms on
$G(n,n,p)$, the random bipartite graph with $n$ vertices on each side and edges
occurring independently with probability $p=p(n)$. In the online model,
vertices on one side of the graph are given up front while vertices on the
other side arrive sequentially; when a vertex arrives its edges are revealed
and it must be immediately matched or dropped. We begin by analyzing the
\textsc{oblivious} algorithm, which tries to match each arriving vertex to a
random neighbor, even if the neighbor has already been matched. The algorithm
is shown to have a performance ratio of at least $1-1/e$ for all monotonic
functions $p(n)$, where the performance ratio is defined asymptotically as the
ratio of the expected matching size given by the algorithm to the expected
maximum matching size. Next we show that the conventional \textsc{greedy}
algorithm, which assigns each vertex to a random unmatched neighbor, has a
performance ratio of at least 0.837 for all monotonic functions $p(n)$. Under
the $G(n,n,p)$ model, the performance of \textsc{greedy} is equivalent to the
performance of the well known \textsc{ranking} algorithm, so our results show
that \textsc{ranking} has a performance ratio of at least 0.837. We finally
consider vertex-weighted bipartite matching. Our proofs are based on simple
differential equations that describe the evolution of the matching process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2537</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2537</id><created>2013-07-09</created><authors><author><keyname>Bachrach</keyname><forenames>Yoram</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Tardos</keyname><forenames>Eva</forenames></author><author><keyname>Vojnovic</keyname><forenames>Milan</forenames></author></authors><title>Strong Price of Anarchy and Coalitional Dynamics</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a framework for studying the effect of cooperation on the
quality of outcomes in utility games. Our framework is a coalitional analog of
the smoothness framework of non-cooperative games. Coalitional smoothness
implies bounds on the strong price of anarchy, the loss of quality of
coalitionally stable outcomes, as well as bounds on coalitional versions of
coarse correlated equilibria and sink equilibria, which we define as
out-of-equilibrium myopic behavior as determined by a natural coalitional
version of best-response dynamics.
  Our coalitional smoothness framework captures existing results bounding the
strong price of anarchy of network design games. We show that in any monotone
utility-maximization game, if each player's utility is at least his marginal
contribution to the welfare, then the strong price of anarchy is at most 2.
This captures a broad class of games, including games with a very high price of
anarchy. Additionally, we show that in potential games the strong price of
anarchy is close to the price of stability, the quality of the best Nash
equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2538</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2538</id><created>2013-07-09</created><updated>2013-09-27</updated><authors><author><keyname>Milius</keyname><forenames>Stefan</forenames><affiliation>Lehrstuhl f&#xfc;r Theoretische Informatik, Friedrich-Alexander Universit&#xe4;t Erlangen-</affiliation></author><author><keyname>Moss</keyname><forenames>Lawrence S</forenames><affiliation>Indiana University, Bloomington, IN, USA</affiliation></author><author><keyname>Schwencke</keyname><forenames>Daniel</forenames><affiliation>Institute of Transportation Systems, German Aerospace Center</affiliation></author></authors><title>Abstract GSOS Rules and a Modular Treatment of Recursive Definitions</title><categories>cs.LO math.CT</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (September
  30, 2013) lmcs:1180</journal-ref><doi>10.2168/LMCS-9(3:28)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Terminal coalgebras for a functor serve as semantic domains for state-based
systems of various types. For example, behaviors of CCS processes, streams,
infinite trees, formal languages and non-well-founded sets form terminal
coalgebras. We present a uniform account of the semantics of recursive
definitions in terminal coalgebras by combining two ideas: (1) abstract GSOS
rules l specify additional algebraic operations on a terminal coalgebra; (2)
terminal coalgebras are also initial completely iterative algebras (cias). We
also show that an abstract GSOS rule leads to new extended cia structures on
the terminal coalgebra. Then we formalize recursive function definitions
involving given operations specified by l as recursive program schemes for l,
and we prove that unique solutions exist in the extended cias. From our results
it follows that the solutions of recursive (function) definitions in terminal
coalgebras may be used in subsequent recursive definitions which still have
unique solutions. We call this principle modularity. We illustrate our results
by the five concrete terminal coalgebras mentioned above, e.\,g., a finite
stream circuit defines a unique stream function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2541</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2541</id><created>2013-07-09</created><updated>2013-12-16</updated><authors><author><keyname>Bhatt</keyname><forenames>Mehul</forenames></author><author><keyname>Wallgruen</keyname><forenames>Jan Oliver</forenames></author></authors><title>Geospatial Narratives and their Spatio-Temporal Dynamics: Commonsense
  Reasoning for High-level Analyses in Geographic Information Systems</title><categories>cs.AI cs.ET cs.HC</categories><comments>ISPRS International Journal of Geo-Information (ISSN 2220-9964);
  Special Issue on: Geospatial Monitoring and Modelling of Environmental
  Change}. IJGI. Editor: Duccio Rocchini. (pre-print of article in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modelling, analysis, and visualisation of dynamic geospatial phenomena
has been identified as a key developmental challenge for next-generation
Geographic Information Systems (GIS). In this context, the envisaged
paradigmatic extensions to contemporary foundational GIS technology raises
fundamental questions concerning the ontological, formal representational, and
(analytical) computational methods that would underlie their spatial
information theoretic underpinnings.
  We present the conceptual overview and architecture for the development of
high-level semantic and qualitative analytical capabilities for dynamic
geospatial domains. Building on formal methods in the areas of commonsense
reasoning, qualitative reasoning, spatial and temporal representation and
reasoning, reasoning about actions and change, and computational models of
narrative, we identify concrete theoretical and practical challenges that
accrue in the context of formal reasoning about `space, events, actions, and
change'. With this as a basis, and within the backdrop of an illustrated
scenario involving the spatio-temporal dynamics of urban narratives, we address
specific problems and solutions techniques chiefly involving `qualitative
abstraction', `data integration and spatial consistency', and `practical
geospatial abduction'. From a broad topical viewpoint, we propose that
next-generation dynamic GIS technology demands a transdisciplinary scientific
perspective that brings together Geography, Artificial Intelligence, and
Cognitive Science.
  Keywords: artificial intelligence; cognitive systems; human-computer
interaction; geographic information systems; spatio-temporal dynamics;
computational models of narrative; geospatial analysis; geospatial modelling;
ontology; qualitative spatial modelling and reasoning; spatial assistance
systems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2554</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2554</id><created>2013-07-09</created><authors><author><keyname>Abdelouarit</keyname><forenames>El Amin Aoulad</forenames><affiliation>LSIT</affiliation></author></authors><title>Les index pour les entrep\^ots de donn\'ees : comparaison entre index
  arbre-B et Bitmap</title><categories>cs.DB</categories><comments>14 pages, Proposition et d\'emonstration exp\'eriment\'ee</comments><proxy>ccsd</proxy><journal-ref>REVIST - Revue Ivoirienne des Sciences et Technologie - ISSN
  1813-3290 - No 20 - December 2012 - Pages:35-67</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of decision systems and specially data warehouses, the
visibility of the data warehouse design before its creation has become
essential, and that because of data warehouse importance as considered as the
unique data source giving meaning to the decision. In a decision system the
proper functioning of a data warehouse resides in the smooth running of the
middleware tools ETC step one hand, and the restitution step through the data
mining, reporting solutions, dashboards... etc other. The large volume of data
that passes through these stages require an optimal design for a highly
efficient decision system, without disregarding the choice of technologies that
are introduced for the data warehouse implementation such as: database
management system, the type of server operating systems, physical server
architecture (64-bit, for example) that can be a benefit performance of this
system. The designer of the data warehouse should consider the effectiveness of
data query, this depends on the selection of relevant indexes and their
combination with the materialized views, note that the index selection is a
NPcomplete problem, because the number of indexes is exponential in the total
number of attributes in the database, So, it is necessary to provide, while the
data warehouse design, the suitable type of index for this data warehouse. This
paper presents a comparative study between the index B-tree type and type
Bitmap, their advantages and disadvantages, with a real experiment showing that
its index of type Bitmap more advantageous than the index B-tree type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2555</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2555</id><created>2013-07-06</created><authors><author><keyname>Shi</keyname><forenames>Minjia</forenames></author></authors><title>MacWilliams Type identities for $m$-spotty Rosenbloom-Tsfasman weight
  enumerators over finite commutative Frobenius rings</title><categories>cs.IT math.IT</categories><comments>Research article, orignial manuscript under review since 2nd November
  2012. 9 pages, 4 Tables. arXiv admin note: substantial text overlap with
  arXiv:1307.1786, arXiv:1307.2228</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $m$-spotty byte error control codes provide a good source for detecting
and correcting errors in semiconductor memory systems using high density RAM
chips with wide I/O data (e.g. 8, 16, or 32 bits). $m$-spotty byte error
control codes are very suitable for burst correction. M. \&quot;{O}zen and V. Siap
[7] proved a MacWilliams identity for the $m$-spotty Rosenbloom-Tsfasman
(shortly RT) weight enumerators of binary codes. The main purpose of this paper
is to present the MacWilliams type identities for $m$-spotty RT weight
enumerators of linear codes over finite commutative Frobenius rings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2559</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2559</id><created>2013-07-09</created><authors><author><keyname>Lehre</keyname><forenames>Per Kristian</forenames></author><author><keyname>Witt</keyname><forenames>Carsten</forenames></author></authors><title>General Drift Analysis with Tail Bounds</title><categories>cs.NE</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drift analysis is one of the state-of-the-art techniques for the runtime
analysis of randomized search heuristics. In recent years, many different drift
theorems, including additive, multiplicative and variable drift, have been
developed, applied and partly generalized or adapted to particular processes. A
comprehensive overview article was missing.
  We provide not only such an overview but also present a universal drift
theorem that generalizes virtually all existing drift theorems found in the
literature. On the one hand, the new theorem bounds the expected first hitting
time of optimal states in the underlying stochastic process. On the other hand,
it also allows for general upper and lower tail bounds on the hitting time,
which were not known before except for the special case of upper bounds in
multiplicative drift scenarios. As a proof of concept, the new tail bounds are
applied to prove very precise sharp-concentration results on the running time
of the (1+1) EA on OneMax, general linear functions and LeadingOnes. Moreover,
user-friendly specializations of the general drift theorem are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2560</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2560</id><created>2013-06-23</created><authors><author><keyname>Jha</keyname><forenames>Saurabh</forenames></author><author><keyname>Agarwal</keyname><forenames>Tejaswi</forenames></author><author><keyname>Kanna</keyname><forenames>B. Rajesh</forenames></author></authors><title>Exploiting Data Parallelism in the yConvex Hypergraph Algorithm for
  Image Representation using GPGPUs</title><categories>cs.DC cs.CV</categories><comments>1 page, 1 figure published in Proceedings of the 27th ACM
  International Conference on Supercomputing, ICS 2013, Eugene, Oregon, USA</comments><acm-class>I.3</acm-class><journal-ref>ACM 978-1-4503-2130-3/13/06 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To define and identify a region-of-interest (ROI) in a digital image, the
shape descriptor of the ROI has to be described in terms of its boundary
characteristics. To address the generic issues of contour tracking, the yConvex
Hypergraph (yCHG) model was proposed by Kanna et al [1]. In this work, we
propose a parallel approach to implement the yCHG model by exploiting massively
parallel cores of NVIDIA's Compute Unified Device Architecture (CUDA). We
perform our experiments on the MODIS satellite image database by NASA, and
based on our analysis we observe that the performance of the serial
implementation is better on smaller images, but once the threshold is achieved
in terms of image resolution, the parallel implementation outperforms its
sequential counterpart by 2 to 10 times (2x-10x). We also conclude that an
increase in the number of hyperedges in the ROI of a given size does not impact
the performance of the overall algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2569</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2569</id><created>2013-07-09</created><updated>2014-09-27</updated><authors><author><keyname>Sinha</keyname><forenames>Abhinav</forenames></author><author><keyname>Anastasopoulos</keyname><forenames>Achilleas</forenames></author></authors><title>Generalized Proportional Allocation Mechanism Design for Multi-rate
  Multicast Service on the Internet</title><categories>cs.GT cs.NI</categories><comments>Technical report for Allerton 13' paper, 28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we construct two mechanisms that fully implement social welfare
maximising allocation in Nash equilibria for the case of single infinitely
divisible good being demanded by a separate groups of agents, whilst being
subject to multiple inequality constraints. The nature of the good demanded is
such that it can be duplicated locally at no cost. The first mechanism achieves
weak budget balance, while the second is an extension of the first, and
achieves strong budget balance at equilibrium. One important application of
these mechanisms is the multi-rate multicast service on the Internet where a
network operator wishes to allocate rates among strategic agents, who are
segregated in groups based on the content they demand (while their demanded
rates could be different), in such a way that maximises overall user
satisfaction while respecting capacity constraints on every link in the
network. The emphasis of this work is on full implementation, which means that
all Nash equilibria of the induced game result in the optimal allocations of
the centralized allocation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2570</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2570</id><created>2013-07-07</created><updated>2013-11-09</updated><authors><author><keyname>Mayr</keyname><forenames>Richard M.</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>Abdulla</keyname><forenames>Parosh Aziz</forenames><affiliation>Uppsala University</affiliation></author></authors><title>Priced Timed Petri Nets</title><categories>cs.LO cs.FL</categories><comments>51 pages. LMCS journal version of arXiv:1104.0617</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (November
  12, 2013) lmcs:874</journal-ref><doi>10.2168/LMCS-9(4:10)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider priced timed Petri nets, i.e., unbounded Petri nets where each
token carries a real-valued clock. Transition arcs are labeled with time
intervals, which specify constraints on the ages of tokens. Furthermore, our
cost model assigns token storage costs per time unit to places, and firing
costs to transitions. This general model strictly subsumes both priced timed
automata and unbounded priced Petri nets. We study the cost of computations
that reach a given control-state. In general, a computation with minimal cost
may not exist, due to strict inequalities in the time constraints. However, we
show that the infimum of the costs to reach a given control-state is computable
in the case where all place and transition costs are non-negative. On the other
hand, if negative costs are allowed, then the question whether a given
control-state is reachable with zero overall cost becomes undecidable. In fact,
this negative result holds even in the simpler case of discrete time (i.e.,
integer-valued clocks).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2579</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2579</id><created>2013-07-09</created><authors><author><keyname>Piech</keyname><forenames>Chris</forenames></author><author><keyname>Huang</keyname><forenames>Jonathan</forenames></author><author><keyname>Chen</keyname><forenames>Zhenghao</forenames></author><author><keyname>Do</keyname><forenames>Chuong</forenames></author><author><keyname>Ng</keyname><forenames>Andrew</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Tuned Models of Peer Assessment in MOOCs</title><categories>cs.LG cs.AI cs.HC stat.AP stat.ML</categories><comments>Proceedings of The 6th International Conference on Educational Data
  Mining (EDM 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In massive open online courses (MOOCs), peer grading serves as a critical
tool for scaling the grading of complex, open-ended assignments to courses with
tens or hundreds of thousands of students. But despite promising initial
trials, it does not always deliver accurate results compared to human experts.
In this paper, we develop algorithms for estimating and correcting for grader
biases and reliabilities, showing significant improvement in peer grading
accuracy on real data with 63,199 peer grades from Coursera's HCI course
offerings --- the largest peer grading networks analysed to date. We relate
grader biases and reliabilities to other student factors such as student
engagement, performance as well as commenting style. We also show that our
model can lead to more intelligent assignment of graders to gradees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2580</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2580</id><created>2013-07-09</created><updated>2013-12-30</updated><authors><author><keyname>Ellis-Braithwaite</keyname><forenames>Richard</forenames></author><author><keyname>Lock</keyname><forenames>Russell</forenames></author><author><keyname>Dawson</keyname><forenames>Ray</forenames></author><author><keyname>Haque</keyname><forenames>Badr</forenames></author></authors><title>Towards an Approach for Analysing the Strategic Alignment of Software
  Requirements using Quantified Goal Graphs</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1211.6258</comments><acm-class>D.2.1</acm-class><journal-ref>International Journal on Advances in Software, vol. 6, no. 2, 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Analysing the strategic alignment of software requirements primarily provides
assurance to stakeholders that the software-to-be will add value to the
organisation. Additionally, such analysis can improve a requirement by
disambiguating its purpose and value, thereby supporting validation and
value-oriented decisions in requirements engineering processes, such as
prioritisation, release planning, and trade-off analysis. We review current
approaches that could enable such an analysis. We focus on Goal Oriented
Requirements Engineering methodologies, since goal graphs are well suited for
relating software goals to business goals. However, we argue that unless the
extent of goal-goal contribution is quantified with verifiable metrics, goal
graphs are not sufficient for demonstrating the strategic alignment of software
requirements. Since the concept of goal contribution is predictive, what
results is a forecast of the benefits of implementing software requirements.
Thus, we explore how the description of the contribution relationship can be
enriched with concepts such as uncertainty and confidence, non-linear
causation, and utility. We introduce the approach using an example software
project from Rolls-Royce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2582</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2582</id><created>2013-07-09</created><authors><author><keyname>Cornelius</keyname><forenames>Sean P.</forenames></author><author><keyname>Motter</keyname><forenames>Adilson E.</forenames></author></authors><title>NECO - A scalable algorithm for NEtwork COntrol</title><categories>math.OC cond-mat.dis-nn cs.DS nlin.AO physics.soc-ph</categories><comments>Source codes available at
  http://www.nature.com/protocolexchange/system/uploads/2647/original/neco_source.zip</comments><journal-ref>Protocol Exchange (2013) doi:10.1038/protex.2013.063 Published
  online 27 June 2013</journal-ref><doi>10.1038/protex.2013.063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for the control of complex networks and other
nonlinear, high-dimensional dynamical systems. The computational approach is
based on the recently-introduced concept of compensatory perturbations --
intentional alterations to the state of a complex system that can drive it to a
desired target state even when there are constraints on the perturbations that
forbid reaching the target state directly. Included here is ready-to-use
software that can be applied to identify eligible control interventions in a
general system described by coupled ordinary differential equations, whose
specific form can be specified by the user. The algorithm is highly scalable,
with the computational cost scaling as the number of dynamical variables to the
power 2.5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2584</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2584</id><created>2013-07-09</created><updated>2014-09-02</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Hoydis</keyname><forenames>Jakob</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Massive MIMO Systems with Non-Ideal Hardware: Energy Efficiency,
  Estimation, and Capacity Limits</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory, 28 pages, 15
  figures. The results can be reproduced using the following Matlab code:
  https://github.com/emilbjornson/massive-MIMO-hardware-impairments</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 11, pp.
  7112-7139, November 2014</journal-ref><doi>10.1109/TIT.2014.2354403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of large-scale antenna arrays can bring substantial improvements in
energy and/or spectral efficiency to wireless systems due to the greatly
improved spatial resolution and array gain. Recent works in the field of
massive multiple-input multiple-output (MIMO) show that the user channels
decorrelate when the number of antennas at the base stations (BSs) increases,
thus strong signal gains are achievable with little inter-user interference.
Since these results rely on asymptotics, it is important to investigate whether
the conventional system models are reasonable in this asymptotic regime. This
paper considers a new system model that incorporates general transceiver
hardware impairments at both the BSs (equipped with large antenna arrays) and
the single-antenna user equipments (UEs). As opposed to the conventional case
of ideal hardware, we show that hardware impairments create finite ceilings on
the channel estimation accuracy and on the downlink/uplink capacity of each UE.
Surprisingly, the capacity is mainly limited by the hardware at the UE, while
the impact of impairments in the large-scale arrays vanishes asymptotically and
inter-user interference (in particular, pilot contamination) becomes
negligible. Furthermore, we prove that the huge degrees of freedom offered by
massive MIMO can be used to reduce the transmit power and/or to tolerate larger
hardware impairments, which allows for the use of inexpensive and
energy-efficient antenna elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2599</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2599</id><created>2013-07-09</created><authors><author><keyname>Han</keyname><forenames>Bin</forenames></author><author><keyname>Mo</keyname><forenames>Qun</forenames></author><author><keyname>Zhao</keyname><forenames>Zhenpeng</forenames></author></authors><title>Compactly Supported Tensor Product Complex Tight Framelets with
  Directionality</title><categories>cs.IT math.IT</categories><msc-class>42C40, 42C15, 65T60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although tensor product real-valued wavelets have been successfully applied
to many high-dimensional problems, they can only capture well edge
singularities along the coordinate axis directions. As an alternative and
improvement of tensor product real-valued wavelets and dual tree complex
wavelet transform, recently tensor product complex tight framelets with
increasing directionality have been introduced in [8] and applied to image
denoising in [13]. Despite several desirable properties, the directional tensor
product complex tight framelets constructed in [8,13] are bandlimited and do
not have compact support in the space/time domain. Since compactly supported
wavelets and framelets are of great interest and importance in both theory and
application, it remains as an unsolved problem whether there exist compactly
supported tensor product complex tight framelets with directionality. In this
paper, we shall satisfactorily answer this question by proving a theoretical
result on directionality of tight framelets and by introducing an algorithm to
construct compactly supported complex tight framelets with directionality. Our
examples show that compactly supported complex tight framelets with
directionality can be easily derived from any given eligible low-pass filters
and refinable functions. Several examples of compactly supported tensor product
complex tight framelets with directionality have been presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2603</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2603</id><created>2013-07-09</created><authors><author><keyname>Cur&#xe9;</keyname><forenames>Olivier</forenames></author><author><keyname>Lamolle</keyname><forenames>Myriam</forenames></author><author><keyname>Duc</keyname><forenames>Chan Le</forenames></author></authors><title>Ontology Based Data Integration Over Document and Column Family Oriented
  NOSQL</title><categories>cs.DB</categories><comments>16 pages, 6 figures, SSWS 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The World Wide Web infrastructure together with its more than 2 billion users
enables to store information at a rate that has never been achieved before.
This is mainly due to the will of storing almost all end-user interactions
performed on some web applications. In order to reply to scalability and
availability constraints, many web companies involved in this process recently
started to design their own data management systems. Many of them are referred
to as NOSQL databases, standing for 'Not only SQL'. With their wide adoption
emerges new needs and data integration is one of them. In this paper, we
consider that an ontology-based representation of the information stored in a
set of NOSQL sources is highly needed. The main motivation of this approach is
the ability to reason on elements of the ontology and to retrieve information
in an efficient and distributed manner. Our contributions are the following:
(1) we analyze a set of schemaless NOSQL databases to generate local
ontologies, (2) we generate a global ontology based on the discovery of
correspondences between the local ontologies and finally (3) we propose a query
translation solution from SPARQL to query languages of the sources. We are
currently implementing our data integration solution on two popular NOSQL
databases: MongoDB as a document database and Cassandra as a column family
store.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2611</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2611</id><created>2013-07-09</created><authors><author><keyname>Oyen</keyname><forenames>Diane</forenames></author><author><keyname>Niculescu-Mizil</keyname><forenames>Alexandru</forenames></author><author><keyname>Ostroff</keyname><forenames>Rachel</forenames></author><author><keyname>Stewart</keyname><forenames>Alex</forenames></author><author><keyname>Clark</keyname><forenames>Vincent P.</forenames></author></authors><title>Controlling the Precision-Recall Tradeoff in Differential Dependency
  Network Analysis</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical models have gained a lot of attention recently as a tool for
learning and representing dependencies among variables in multivariate data.
Often, domain scientists are looking specifically for differences among the
dependency networks of different conditions or populations (e.g. differences
between regulatory networks of different species, or differences between
dependency networks of diseased versus healthy populations). The standard
method for finding these differences is to learn the dependency networks for
each condition independently and compare them. We show that this approach is
prone to high false discovery rates (low precision) that can render the
analysis useless. We then show that by imposing a bias towards learning similar
dependency networks for each condition the false discovery rates can be reduced
to acceptable levels, at the cost of finding a reduced number of differences.
Algorithms developed in the transfer learning literature can be used to vary
the strength of the imposed similarity bias and provide a natural mechanism to
smoothly adjust this differential precision-recall tradeoff to cater to the
requirements of the analysis conducted. We present real case studies
(oncological and neurological) where domain experts use the proposed technique
to extract useful differential networks that shed light on the biological
processes involved in cancer and brain function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2641</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2641</id><created>2013-07-09</created><updated>2013-08-25</updated><authors><author><keyname>Wang</keyname><forenames>Timothy</forenames></author><author><keyname>Jobredeaux</keyname><forenames>Romain</forenames></author><author><keyname>Herencia</keyname><forenames>Heber</forenames></author><author><keyname>Garoche</keyname><forenames>Pierre-Loic</forenames></author><author><keyname>Dieumegard</keyname><forenames>Arnaud</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author><author><keyname>Pantel</keyname><forenames>Marc</forenames></author></authors><title>From Design to Implementation: an Automated, Credible Autocoding Chain
  for Control Systems</title><categories>cs.SY cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes a fully automated, credible autocoding chain for
control systems. The framework generates code, along with guarantees of high
level functional properties which can be independently verified. It relies on
domain specific knowledge and fomal methods of analysis to address a context of
heightened safety requirements for critical embedded systems and
ever-increasing costs of verification and validation. The platform strives to
bridge the semantic gap between domain expert and code verification expert.
First, a graphical dataflow language is extended with annotation symbols
enabling the control engineer to express high level properties of its control
law within the framework of a familiar language. An existing autocoder is
enhanced to both generate the code implementing the initial design, but also to
carry high level properties down to annotations at the level of the code.
Finally, using customized code analysis tools, certificates are generated which
guarantee the correctness of the annotations with respect to the code, and can
be verified using existing static analysis tools. Only a subset of properties
and controllers are handled at this point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2642</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2642</id><created>2013-07-09</created><updated>2014-11-20</updated><authors><author><keyname>Zhang</keyname><forenames>Xizhe</forenames></author><author><keyname>Lv</keyname><forenames>Tianyang</forenames></author><author><keyname>Yang</keyname><forenames>Xueying</forenames></author><author><keyname>Zhang</keyname><forenames>Bin</forenames></author></authors><title>Structure controllability of complex network based on preferential
  matching</title><categories>math-ph cs.SI math.MP physics.soc-ph</categories><doi>10.1371/journal.pone.0112039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimum driver node sets (MDSs) play an important role in studying the
structural controllability of complex networks. Recent research has shown that
MDSs tend to avoid high-degree nodes. However, this observation is based on the
analysis of a small number of MDSs, because enumerating all of the MDSs of a
network is a #P problem. Therefore, past research has not been sufficient to
arrive at a convincing conclusion. In this paper, first, we propose a
preferential matching algorithm to find MDSs that have a specific degree
property. Then, we show that the MDSs obtained by preferential matching can be
composed of high- and medium-degree nodes. Moreover, the experimental results
also show that the average degree of the MDSs of some networks tends to be
greater than that of the overall network, even when the MDSs are obtained using
previous research method. Further analysis shows that whether the driver nodes
tend to be high-degree nodes or not is closely related to the edge direction of
the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2661</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2661</id><created>2013-07-09</created><updated>2014-03-09</updated><authors><author><keyname>Banerji</keyname><forenames>Sourangsu</forenames></author><author><keyname>Chowdhury</keyname><forenames>Rahul Singha</forenames></author></authors><title>On IEEE 802.11: Wireless LAN Technology</title><categories>cs.NI</categories><comments>19 pages, 4 figures, 3 tables</comments><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics (IJMNCT),Volume 3,Issue 4, August 2013</journal-ref><doi>10.5121/ijmnct.2013.3405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network technologies are traditionally based on wireline solutions. But the
introduction of the IEEE 802.11 standards have made a huge impact on the market
such that laptops, PCs, printers, cellphones, and VoIP phones, MP3 players in
our homes, in offices and even in public areas have incorporated the wireless
LAN technology. Wireless broadband technologies nowadays provide unlimited
broadband access to users which were previously offered only to wireline users.
In this paper, we review and summarize one of the emerging wireless broadband
technology i.e. IEEE 802.11,which is a set of physical layer standard for
implementing wireless local area network computer communication in the
2.4,3.6,5 and 60GHz frequency band. They fix technology issues or add
functionality which is expected to be required by future applications. Though
some of the earlier versions of these technologies are obsolete (such as
HiperLAN) now but still we have included them in this review for the sake of
completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2669</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2669</id><created>2013-07-10</created><authors><author><keyname>Duan</keyname><forenames>Hubert Haoyang</forenames></author><author><keyname>Pestov</keyname><forenames>Vladimir</forenames></author><author><keyname>Singla</keyname><forenames>Varun</forenames></author></authors><title>Text Categorization via Similarity Search: An Efficient and Effective
  Novel Algorithm</title><categories>cs.IR</categories><comments>12 pages, 5 tables, accepted for the 6th International Conference on
  Similarity Search and Applications (SISAP 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a supervised learning algorithm for text categorization which has
brought the team of authors the 2nd place in the text categorization division
of the 2012 Cybersecurity Data Mining Competition (CDMC'2012) and a 3rd prize
overall. The algorithm is quite different from existing approaches in that it
is based on similarity search in the metric space of measure distributions on
the dictionary. At the preprocessing stage, given a labeled learning sample of
texts, we associate to every class label (document category) a point in the
space of question. Unlike it is usual in clustering, this point is not a
centroid of the category but rather an outlier, a uniform measure distribution
on a selection of domain-specific words. At the execution stage, an unlabeled
text is assigned a text category as defined by the closest labeled neighbour to
the point representing the frequency distribution of the words in the text. The
algorithm is both effective and efficient, as further confirmed by experiments
on the Reuters 21578 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2672</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2672</id><created>2013-07-10</created><updated>2013-11-15</updated><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Index Coding Problem with Side Information Repositories</title><categories>cs.IT math.IT</categories><comments>17 pages, 7 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To tackle the expected enormous increase in mobile video traffic in cellular
networks, an architecture involving a base station along with caching femto
stations (referred to as helpers), storing popular files near users, has been
proposed [1]. The primary benefit of caching is the enormous increase in
downloading rate when a popular file is available at helpers near a user
requesting that file. In this work, we explore a secondary benefit of caching
in this architecture through the lens of index coding. We assume a system with
n users and constant number of caching helpers. Only helpers store files, i.e.
have side information. We investigate the following scenario: Each user
requests a distinct file that is not found in the set of helpers nearby. Users
are served coded packets (through an index code) by an omniscient base station.
Every user decodes its desired packet from the coded packets and the side
information packets from helpers nearby. We assume that users can obtain any
file stored in their neighboring helpers without incurring transmission costs.
With respect to the index code employed, we investigate two achievable schemes:
1) XOR coloring based on coloring of the side information graph associated with
the problem and 2)Vector XOR coloring based on fractional coloring of the side
information graph. We show that the general problem reduces to a canonical
problem where every user is connected to exactly one helper under some
topological constraints. For the canonical problem, with constant number of
helpers (k), we show that the complexity of computing the best XOR/vector XOR
coloring schemes are polynomial in the number of users n. The result exploits a
special complete bi-partite structure that the side information graphs exhibit
for any finite k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2674</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2674</id><created>2013-07-10</created><authors><author><keyname>Li</keyname><forenames>Hongwei</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author><author><keyname>Zhou</keyname><forenames>Dengyong</forenames></author></authors><title>Error Rate Bounds in Crowdsourcing Models</title><categories>stat.ML cs.LG stat.AP</categories><comments>13 pages, 3 figures, downloadable supplementary files</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing is an effective tool for human-powered computation on many
tasks challenging for computers. In this paper, we provide finite-sample
exponential bounds on the error rate (in probability and in expectation) of
hyperplane binary labeling rules under the Dawid-Skene crowdsourcing model. The
bounds can be applied to analyze many common prediction methods, including the
majority voting and weighted majority voting. These bound results could be
useful for controlling the error rate and designing better algorithms. We show
that the oracle Maximum A Posterior (MAP) rule approximately optimizes our
upper bound on the mean error rate for any hyperplane binary labeling rule, and
propose a simple data-driven weighted majority voting (WMV) rule (called
one-step WMV) that attempts to approximate the oracle MAP and has a provable
theoretical guarantee on the error rate. Moreover, we use simulated and real
data to demonstrate that the data-driven EM-MAP rule is a good approximation to
the oracle MAP rule, and to demonstrate that the mean error rate of the
data-driven EM-MAP rule is also bounded by the mean error rate bound of the
oracle MAP rule with estimated parameters plugging into the bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2676</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2676</id><created>2013-07-10</created><updated>2013-08-24</updated><authors><author><keyname>Maimaiti</keyname><forenames>W.</forenames></author><author><keyname>Mancini</keyname><forenames>S.</forenames></author></authors><title>Efficiency of Entanglement Concentration by Photon Subtraction</title><categories>quant-ph cs.IT math.IT</categories><comments>6 pages, 3 figures, accepted by Physica Scripta</comments><journal-ref>Phys. Scr. T160 (2014) 014028</journal-ref><doi>10.1088/0031-8949/2014/T160/014028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a measure of efficiency for the photon subtraction protocol
aimed at entanglement concentration on a single copy of bipartite continuous
variable state. We then show that iterating the protocol does not lead to
higher efficiency than a single application. In order to overcome this limit we
present an adaptive version of the protocol able to greatly enhance its
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2690</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2690</id><created>2013-07-10</created><authors><author><keyname>Lychev</keyname><forenames>Robert</forenames></author><author><keyname>Goldberg</keyname><forenames>Sharon</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author></authors><title>BGP Security in Partial Deployment: Is the Juice Worth the Squeeze?</title><categories>cs.NI cs.CR cs.DC</categories><acm-class>C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the rollout of secure route origin authentication with the RPKI slowly
gains traction among network operators, there is a push to standardize secure
path validation for BGP (i.e., S*BGP: S-BGP, soBGP, BGPSEC, etc.). Origin
authentication already does much to improve routing security. Moreover, the
transition to S*BGP is expected to be long and slow, with S*BGP coexisting in
&quot;partial deployment&quot; alongside BGP for a long time. We therefore use
theoretical and experimental approach to study the security benefits provided
by partially-deployed S*BGP, vis-a-vis those already provided by origin
authentication. Because routing policies have a profound impact on routing
security, we use a survey of 100 network operators to find the policies that
are likely to be most popular during partial S*BGP deployment. We find that
S*BGP provides only meagre benefits over origin authentication when these
popular policies are used. We also study the security benefits of other routing
policies, provide prescriptive guidelines for partially-deployed S*BGP, and
show how interactions between S*BGP and BGP can introduce new vulnerabilities
into the routing system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2696</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2696</id><created>2013-07-10</created><updated>2013-07-11</updated><authors><author><keyname>Chan</keyname><forenames>T-H. Hubert</forenames></author><author><keyname>Chen</keyname><forenames>Fei</forenames></author><author><keyname>Wu</keyname><forenames>Xiaowei</forenames></author><author><keyname>Zhao</keyname><forenames>Zhichao</forenames></author></authors><title>Ranking on Arbitrary Graphs: Rematch via Continuous LP with Monotone and
  Boundary Condition Constraints</title><categories>cs.DS</categories><comments>Corrected references in abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by online advertisement and exchange settings, greedy randomized
algorithms for the maximum matching problem have been studied, in which the
algorithm makes (random) decisions that are essentially oblivious to the input
graph. Any greedy algorithm can achieve performance ratio 0.5, which is the
expected number of matched nodes to the number of nodes in a maximum matching.
  Since Aronson, Dyer, Frieze and Suen proved that the Modified Randomized
Greedy (MRG) algorithm achieves performance ratio 0.5 + \epsilon (where
\epsilon = frac{1}{400000}) on arbitrary graphs in the mid-nineties, no further
attempts in the literature have been made to improve this theoretical ratio for
arbitrary graphs until two papers were published in FOCS 2012. Poloczek and
Szegedy also analyzed the MRG algorithm to give ratio 0.5039, while Goel and
Tripathi used experimental techniques to analyze the Ranking algorithm to give
ratio 0.56. However, we could not reproduce the experimental results of Goel
and Tripathi.
  In this paper, we revisit the Ranking algorithm using the LP framework.
Special care is given to analyze the structural properties of the Ranking
algorithm in order to derive the LP constraints, of which one known as the
\emph{boundary} constraint requires totally new analysis and is crucial to the
success of our LP.
  We use continuous LP relaxation to analyze the limiting behavior as the
finite LP grows. Of particular interest are new duality and complementary
slackness characterizations that can handle the monotone and the boundary
constraints in continuous LP. We believe our work achieves the currently best
theoretical performance ratio of \frac{2(5-\sqrt{7})}{9} \approx 0.523 on
arbitrary graphs. Moreover, experiments suggest that Ranking cannot perform
better than 0.724 in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2700</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2700</id><created>2013-07-10</created><updated>2013-11-14</updated><authors><author><keyname>Rahmati</keyname><forenames>Zahed</forenames></author><author><keyname>Abam</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>King</keyname><forenames>Valerie</forenames></author><author><keyname>Whitesides</keyname><forenames>Sue</forenames></author></authors><title>Kinetic Data Structures for the Semi-Yao Graph and All Nearest Neighbors
  in R^d</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a simple kinetic data structure for maintaining all the
nearest neighbors of a set of $n$ moving points in $\mathbb{R}^d$, where the
trajectory of each point is an algebraic function of at most constant degree
$s$. The approach is based on maintaining the edges of the Semi-Yao graph, a
sparse graph whose edge set includes the pairs of nearest neighbors as a
subset.
  Our kinetic data structure (KDS) for maintaining all the nearest neighbors is
deterministic. It processes $O(n^2\beta_{2s+2}^2(n)\log n)$ events with a total
cost of $O(n^2\beta_{2s+2}(n)\log^{d+1} n)$. Here, $\beta_s(n)$ is an extremely
slow-growing function. The best previous KDS for all the nearest neighbors in $
\mathbb{R}^d$ is by Agarwal, Kaplan, and Sharir (TALG 2008). It is a randomized
result. Our structure and analysis are simpler than theirs. Also, we improve
their result by a factor of $\log^d n$ in the number of events and by a $\log
n$ factor in the total cost.
  This paper generalizes and improves the 2013 work of Rahmati, King and
Whitesides (SoCG 2013) on maintaining the Semi-Yao graph in $\mathbb{R}^2$; its
new technique provides the first KDS for the Semi-Yao graph in $\mathbb{R}^d$.
Our KDS is local in the worst case, meaning that only a constant number of
events is associated with any one point at any time.
  For maintaining all the nearest neighbors, neither our KDS nor the KDS by
Agarwal~\etal~is local, and furthermore, each event in our KDS and in their KDS
is handled in polylogarithmic time in an amortized sense.
  Finally, in this paper, we also give a KDS for maintenance of all the
$(1+\epsilon)$-nearest neighbors which is local and each event can be handled
in a polylogarithmic worst-case time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2704</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2704</id><created>2013-07-10</created><authors><author><keyname>Yao</keyname><forenames>Hua</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Applications of repeat degree on coverings of neighborhoods</title><categories>cs.AI</categories><comments>14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In covering based rough sets, the neighborhood of an element is the
intersection of all the covering blocks containing the element. All the
neighborhoods form a new covering called a covering of neighborhoods. In the
course of studying under what condition a covering of neighborhoods is a
partition, the concept of repeat degree is proposed, with the help of which the
issue is addressed. This paper studies further the application of repeat degree
on coverings of neighborhoods. First, we investigate under what condition a
covering of neighborhoods is the reduct of the covering inducing it. As a
preparation for addressing this issue, we give a necessary and sufficient
condition for a subset of a set family to be the reduct of the set family. Then
we study under what condition two coverings induce a same relation and a same
covering of neighborhoods. Finally, we give the method of calculating the
covering according to repeat degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2705</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2705</id><created>2013-07-10</created><updated>2014-05-29</updated><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Making Octants Colorful and Related Covering Decomposition Problems</title><categories>cs.CG math.CO</categories><comments>version after revision process; minor changes in the exposition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give new positive results on the long-standing open problem of geometric
covering decomposition for homothetic polygons. In particular, we prove that
for any positive integer k, every finite set of points in R^3 can be colored
with k colors so that every translate of the negative octant containing at
least k^6 points contains at least one of each color. The best previously known
bound was doubly exponential in k. This yields, among other corollaries, the
first polynomial bound for the decomposability of multiple coverings by
homothetic triangles. We also investigate related decomposition problems
involving intervals appearing on a line. We prove that no algorithm can
dynamically maintain a decomposition of a multiple covering by intervals under
insertion of new intervals, even in a semi-online model, in which some coloring
decisions can be delayed. This implies that a wide range of sweeping plane
algorithms cannot guarantee any bound even for special cases of the octant
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2708</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2708</id><created>2013-07-10</created><authors><author><keyname>Yao</keyname><forenames>Hua</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Unique expansion matroids and union minimal matroids</title><categories>cs.DM math.CO</categories><comments>16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expansion axiom of matroids requires only the existence of some kind of
independent sets, not the uniqueness of them. This causes that the base
families of some matroids can be reduced while the unions of the base families
of these matroids remain unchanged. In this paper, we define unique expansion
matroids in which the expansion axiom has some extent uniqueness; we define
union minimal matroids in which the base families have some extent minimality.
Some properties of them and the relationship between them are studied. First,
we propose the concepts of secondary base and forming base family. Secondly, we
propose the concept of unique expansion matroid, and prove that a matroid is a
unique expansion matroid if and only if its forming base family is a partition.
Thirdly, we propose the concept of union minimal matroid, and prove that unique
expansion matroids are union minimal matroids. Finally, we extend the concept
of unique expansion matroid to unique exchange matroid and prove that both
unique expansion matroids and their dual matroids are unique exchange matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2713</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2713</id><created>2013-07-10</created><authors><author><keyname>Obua</keyname><forenames>Steven</forenames></author><author><keyname>Adams</keyname><forenames>Mark</forenames></author><author><keyname>Aspinall</keyname><forenames>David</forenames></author></authors><title>Capturing Hiproofs in HOL Light</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical proof trees (hiproofs for short) add structure to ordinary proof
trees, by allowing portions of trees to be hierarchically nested. The
additional structure can be used to abstract away from details, or to label
particular portions to explain their purpose. In this paper we present two
complementary methods for capturing hiproofs in HOL Light, along with a tool to
produce web-based visualisations. The first method uses tactic recording, by
modifying tactics to record their arguments and construct a hierarchical tree;
this allows a tactic proof script to be modified. The second method uses proof
recording, which extends the HOL Light kernel to record hierachical proof trees
alongside theorems. This method is less invasive, but requires care to manage
the size of the recorded objects. We have implemented both methods, resulting
in two systems: Tactician and HipCam.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2718</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2718</id><created>2013-07-10</created><updated>2015-05-26</updated><authors><author><keyname>Konyagin</keyname><forenames>Sergei V.</forenames></author><author><keyname>Luca</keyname><forenames>Florian</forenames></author><author><keyname>Mans</keyname><forenames>Bernard</forenames></author><author><keyname>Mathieson</keyname><forenames>Luke</forenames></author><author><keyname>Sha</keyname><forenames>Min</forenames></author><author><keyname>Shparlinski</keyname><forenames>Igor E.</forenames></author></authors><title>Functional Graphs of Polynomials over Finite Fields</title><categories>math.NT cs.DM math.CO</categories><msc-class>05C20, 05C85, 11T06, 11T24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a function $f$ in a finite field ${\mathbb F}_q$ of $q$ elements, we
define the functional graph of $f$ as a directed graph on $q$ nodes labelled by
the elements of ${\mathbb F}_q$ where there is an edge from $u$ to $v$ if and
only if $f(u) = v$. We obtain some theoretic estimates on the number of
non-isomorphic graphs generated by all polynomials of a given degree. We then
develop a simple and practical algorithm to test the isomorphism of quadratic
polynomials that has linear memory and time complexities. Furthermore, we
extend this isomorphism testing algorithm to the general case of functional
graphs, and prove that, while its time complexity increases only slightly, its
memory complexity remains linear. We exploit this algorithm to provide an upper
bound on the number of functional graphs corresponding to polynomials of degree
$d$ over ${\mathbb F}_q$. Finally, we present some numerical results and
compare function graphs of quadratic polynomials with those generated by random
maps and pose interesting new problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2724</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2724</id><created>2013-07-10</created><authors><author><keyname>Cetin</keyname><forenames>A. Emre</forenames></author></authors><title>The technique of in-place associative sorting</title><categories>cs.DS</categories><comments>34 Pages. arXiv admin note: substantial text overlap with
  arXiv:1209.0572, arXiv:1210.1771, arXiv:1209.3668, arXiv:1209.1942,
  arXiv:1209.4714</comments><msc-class>68P05, 68P10</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the first place, a novel, yet straightforward in-place integer
value-sorting algorithm is presented. It sorts in linear time using constant
amount of additional memory for storing counters and indices beside the input
array. The technique is inspired from the principal idea behind one of the
ordinal theories of &quot;serial order in behavior&quot; and explained by the analogy
with the three main stages in the formation and retrieval of memory in
cognitive neuroscience: (i) practicing, (ii) storage and (iii) retrieval. It is
further improved in terms of time complexity as well as specialized for
distinct integers, though still improper for rank-sorting.
  Afterwards, another novel, yet straightforward technique is introduced which
makes this efficient value-sorting technique proper for rank-sorting. Hence,
given an array of n elements each have an integer key, the technique sorts the
elements according to their integer keys in linear time using only constant
amount of additional memory. The devised technique is very practical and
efficient outperforming bucket sort, distribution counting sort and address
calculation sort family of algorithms making it attractive in almost every case
even when space is not a critical resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2735</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2735</id><created>2013-07-10</created><authors><author><keyname>Dwivedi</keyname><forenames>Shri Prakash</forenames></author></authors><title>An Efficient Multiplication Algorithm Using Nikhilam Method</title><categories>cs.DS cs.SC</categories><comments>Extended version to appear in ITC 2013</comments><doi>10.1049/cp.2013.2209</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplication is one of the most important operation in computer arithmetic.
Many integer operations such as squaring, division and computing reciprocal
require same order of time as multiplication whereas some other operations such
as computing GCD and residue operation require at most a factor of $\log n$
time more than multiplication. We propose an integer multiplication algorithm
using Nikhilam method of Vedic mathematics which can be used to multiply two
binary numbers efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2747</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2747</id><created>2013-07-10</created><updated>2013-07-24</updated><authors><author><keyname>Delgosha</keyname><forenames>Payam</forenames></author><author><keyname>Beigi</keyname><forenames>Salman</forenames></author></authors><title>Impossibility of Local State Transformation via Hypercontractivity</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>27 pages</comments><doi>10.1007/s00220-014-2105-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local state transformation is the problem of transforming an arbitrary number
of copies of a bipartite resource state to a bipartite target state under local
operations. That is, given two bipartite states, is it possible to transform an
arbitrary number of copies of one of them to one copy of the other state under
local operations only? This problem is a hard one in general since we assume
that the number of copies of the resource state is arbitrarily large. In this
paper we prove some bounds on this problem using the hypercontractivity
properties of some super-operators corresponding to bipartite states. We
measure hypercontractivity in terms of both the usual super-operator norms as
well as completely bounded norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2748</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2748</id><created>2013-07-10</created><authors><author><keyname>Schmietendorf</keyname><forenames>Katrin</forenames><affiliation>Institut f&#xfc;r Physik, Carl-von-Ossietzky-Universit&#xe4;t Oldenburg, Germany</affiliation><affiliation>Institut f&#xfc;r Theoretische Physik, WWU M&#xfc;nster, Germany</affiliation></author><author><keyname>Peinke</keyname><forenames>Joachim</forenames><affiliation>Institut f&#xfc;r Physik, Carl-von-Ossietzky-Universit&#xe4;t Oldenburg, Germany</affiliation></author><author><keyname>Friedrich</keyname><forenames>Rudolf</forenames><affiliation>Institut f&#xfc;r Theoretische Physik, WWU M&#xfc;nster, Germany</affiliation></author><author><keyname>Kamps</keyname><forenames>Oliver</forenames><affiliation>Center for Nonlinear Science, M&#xfc;nster, Germany</affiliation></author></authors><title>Self-Organized Synchronization and Voltage Stability in Networks of
  Synchronous Machines</title><categories>nlin.AO cs.SY</categories><comments>9 pages, 9 figures</comments><doi>10.1140/epjst/e2014-02209-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of renewable energy sources in the course of the energy
transition is accompanied by grid decentralization and fluctuating power
feed-in characteristics. This raises new challenges for power system stability
and design. We intend to investigate power system stability from the viewpoint
of self-organized synchronization aspects. In this approach, the power grid is
represented by a network of synchronous machines. We supplement the classical
Kuramoto-like network model, which assumes constant voltages, with dynamical
voltage equations, and thus obtain an extended version, that incorporates the
coupled categories voltage stability and rotor angle synchronization. We
compare disturbance scenarios in small systems simulated on the basis of both
classical and extended model and we discuss resultant implications and possible
applications to complex modern power grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2756</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2756</id><created>2013-07-10</created><authors><author><keyname>Braghin</keyname><forenames>Stefano</forenames></author><author><keyname>Iovino</keyname><forenames>Vincenzo</forenames></author><author><keyname>Persiano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Trombetta</keyname><forenames>Alberto</forenames></author></authors><title>Secure and Policy-Private Resource Sharing in an Online Social Network</title><categories>cs.CR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing functionalities that allow online social network users to manage in
a secure and private way the publication of their information and/or resources
is a relevant and far from trivial topic that has been under scrutiny from
various research communities. In this work, we provide a framework that allows
users to define highly expressive access policies to their resources in a way
that the enforcement does not require the intervention of a (trusted or not)
third party. This is made possible by the deployment of a newly defined
cryptographic primitives that provides - among other things - efficient access
revocation and access policy privacy. Finally, we provide an implementation of
our framework as a Facebook application, proving the feasibility of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2763</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2763</id><created>2013-07-10</created><authors><author><keyname>Pantisano</keyname><forenames>Francesco</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Matching with Externalities for Context-Aware User-Cell Association in
  Small Cell Networks</title><categories>cs.GT cs.NI</categories><comments>6 pages, 3 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel user-cell association approach for wireless
small cell networks that exploits previously unexplored context information
extracted from users' devices, i.e., user equipments (UEs). Beyond
characterizing precise quality of service (QoS) requirements that accurately
reflect the UEs' application usage, our proposed cell association approach
accounts for the devices' hardware type (e.g., smartphone, tablet, laptop).
This approach has the practical benefit of enabling the small cells to make
better informed cell association decisions that handle practical
device-specific QoS characteristics. We formulate the problem as a matching
game between small cell base stations (SBSs) and UEs. In this game, the SBSs
and UEs rank one another based on well-designed utility functions that capture
composite QoS requirements, extracted from the context features (i.e.,
application in use, hardware type). We show that the preferences used by the
nodes to rank one another are interdependent and influenced by the existing
network-wide matching. Due to this unique feature of the preferences, we show
that the proposed game can be classified as a many-to-one matching game with
externalities. To solve this game, we propose a distributed algorithm that
enables the players (i.e., UEs and SBSs) to self-organize into a stable
matching that guarantees the required applications' QoS. Simulation results
show that the proposed context-aware cell association scheme yields significant
gains, reaching up to 52% improvement compared to baseline context-unaware
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2779</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2779</id><created>2013-07-10</created><updated>2013-10-09</updated><authors><author><keyname>Ouaknine</keyname><forenames>Joel</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Positivity Problems for Low-Order Linear Recurrence Sequences</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two decision problems for linear recurrence sequences (LRS) over
the integers, namely the Positivity Problem (are all terms of a given LRS
positive?) and the Ultimate Positivity Problem} (are all but finitely many
terms of a given LRS positive?). We show decidability of both problems for LRS
of order 5 or less, with complexity in the Counting Hierarchy for Positivity,
and in polynomial time for Ultimate Positivity. Moreover, we show by way of
hardness that extending the decidability of either problem to LRS of order 6
would entail major breakthroughs in analytic number theory, more precisely in
the field of Diophantine approximation of transcendental numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2783</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2783</id><created>2013-07-10</created><updated>2013-10-02</updated><authors><author><keyname>Christoforou</keyname><forenames>Evgenia</forenames><affiliation>Anxo</affiliation></author><author><keyname>Anta</keyname><forenames>Antonio Fernandez</forenames><affiliation>Anxo</affiliation></author><author><keyname>Georgiou</keyname><forenames>Chryssis</forenames><affiliation>Anxo</affiliation></author><author><keyname>Mosteiro</keyname><forenames>Miguel A.</forenames><affiliation>Anxo</affiliation></author><author><keyname>Angel</keyname><affiliation>Anxo</affiliation></author><author><keyname>Sanchez</keyname></author></authors><title>Reputation-based Mechanisms for Evolutionary Master-Worker Computing</title><categories>cs.DC cs.GT</categories><comments>33 pages, 19 figures</comments><msc-class>68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Internet-based Master-Worker task computing systems, such as
SETI@home, where a master sends tasks to potentially unreliable workers, and
the workers execute and report back the result. We model such computations
using evolutionary dynamics and consider three type of workers: altruistic,
malicious and rational. Altruistic workers always compute and return the
correct result, malicious workers always return an incorrect result, and
rational (selfish) workers decide to be truthful or to cheat, based on the
strategy that increases their benefit. The goal of the master is to reach
eventual correctness, that is, reach a state of the computation that always
receives the correct results. To this respect, we propose a mechanism that uses
reinforcement learning to induce a correct behavior to rational workers; to
cope with malice we employ reputation schemes. We analyze our reputation-based
mechanism modeling it as a Markov chain and we give provable guarantees under
which truthful behavior can be ensured. Simulation results, ob- tained using
parameter values that are likely to occur in practice, reveal interesting
trade-offs between various metrics, parameters and reputation types, affecting
cost, time of convergence to a truthful behavior and tolerance to cheaters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2785</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2785</id><created>2013-07-10</created><authors><author><keyname>Lin</keyname><forenames>Yu-Ru</forenames></author><author><keyname>Keegan</keyname><forenames>Brian</forenames></author><author><keyname>Margolin</keyname><forenames>Drew</forenames></author><author><keyname>Lazer</keyname><forenames>David</forenames></author></authors><title>Rising tides or rising stars?: Dynamics of shared attention on Twitter
  during media events</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages, 6 figures</comments><doi>10.1371/journal.pone.0094093</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Media events&quot; such as political debates generate conditions of shared
attention as many users simultaneously tune in with the dual screens of
broadcast and social media to view and participate. Are collective patterns of
user behavior under conditions of shared attention distinct from other &quot;bursts&quot;
of activity like breaking news events? Using data from a population of
approximately 200,000 politically-active Twitter users, we compare features of
their behavior during eight major events during the 2012 U.S. presidential
election to examine (1) the impact of &quot;media events&quot; have on patterns of social
media use compared to &quot;typical&quot; time and (2) whether changes during media
events are attributable to changes in behavior across the entire population or
an artifact of changes in elite users' behavior. Our findings suggest that
while this population became more active during media events, this additional
activity reflects concentrated attention to a handful of users, hashtags, and
tweets. Our work is the first study on distinguishing patterns of large-scale
social behavior under condition of uncertainty and shared attention, suggesting
new ways of mining information from social media to support collective
sensemaking following major events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2789</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2789</id><created>2013-07-10</created><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Moghaddam</keyname><forenames>Fereydoun Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Computer Simulation of 3-D Finite-Volume Liquid Transport in Fibrous
  Materials: a Physical Model for Ink Seepage into Paper</title><categories>cs.CE cond-mat.mes-hall</categories><comments>26 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A physical model for the simulation ink/paper interaction at the mesoscopic
scale is developed. It is based on the modified Ising model, and is generalized
to consider the restriction of the finite-volume of ink and also its dynamic
seepage. This allows the model to obtain the ink distribution within the paper
volume. At the mesoscopic scale, the paper is modeled using a discretized fiber
structure. The ink distribution is obtained by solving its equivalent
optimization problem, which is solved using a modified genetic algorithm, along
with a new boundary condition and the quasi-linear technique. The model is able
to simulate the finite-volume distribution of ink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2799</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2799</id><created>2013-07-10</created><authors><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Niu</keyname><forenames>Kai</forenames></author><author><keyname>Lin</keyname><forenames>Jiaru</forenames></author></authors><title>Polar Coded Modulation with Optimal Constellation Labeling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A practical $2^m$-ary polar coded modulation (PCM) scheme with optimal
constellation labeling is proposed. To efficiently find the optimal labeling
rule, the search space is reduced by exploiting the symmetry properties of the
channels. Simulation results show that the proposed PCM scheme can outperform
the bit-interleaved turbo coded modulation scheme used in the WCDMA (Wideband
Code Division Multiple Access) mobile communication systems by up to 1.5dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2800</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2800</id><created>2013-07-10</created><authors><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Niu</keyname><forenames>Kai</forenames></author><author><keyname>Lin</keyname><forenames>Jiaru</forenames></author></authors><title>A Hybrid ARQ Scheme Based on Polar Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hybrid automatic repeat request (HARQ) scheme based on a novel class of
rate-compatible polar (\mbox{RCP}) codes are proposed. The RCP codes are
constructed by performing punctures and repetitions on the conventional polar
codes. Simulation results over binary-input additive white Gaussian noise
channels (BAWGNCs) show that, using a low-complexity successive cancellation
(SC) decoder, the proposed HARQ scheme performs as well as the existing schemes
based on turbo codes and low-density parity-check (LDPC) codes. The proposed
transmission scheme is only about 1.0-1.5dB away from the channel capacity with
the information block length of 1024 bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2806</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2806</id><created>2013-07-10</created><authors><author><keyname>Disser</keyname><forenames>Yann</forenames></author><author><keyname>Klimm</keyname><forenames>Max</forenames></author><author><keyname>Megow</keyname><forenames>Nicole</forenames></author><author><keyname>Stiller</keyname><forenames>Sebastian</forenames></author></authors><title>Packing a Knapsack of Unknown Capacity</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of packing a knapsack without knowing its capacity.
Whenever we attempt to pack an item that does not fit, the item is discarded;
if the item fits, we have to include it in the packing. We show that there is
always a policy that packs a value within factor 2 of the optimum packing,
irrespective of the actual capacity. If all items have unit density, we achieve
a factor equal to the golden ratio. Both factors are shown to be best possible.
In fact, we obtain the above factors using packing policies that are universal
in the sense that they fix a particular order of the items and try to pack the
items in this order, independent of the observations made while packing. We
give efficient algorithms computing these policies. On the other hand, we show
that, for any alpha&gt;1, the problem of deciding whether a given universal policy
achieves a factor of alpha is coNP-complete. If alpha is part of the input, the
same problem is shown to be coNP-complete for items with unit densities.
Finally, we show that it is coNP-hard to decide, for given alpha, whether a set
of items admits a universal policy with factor alpha, even if all items have
unit densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2808</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2808</id><created>2013-07-10</created><authors><author><keyname>Hajiaghayi</keyname><forenames>Mohammadtaghi</forenames></author><author><keyname>Hu</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Li</keyname><forenames>Shi</forenames></author><author><keyname>Saha</keyname><forenames>Barna</forenames></author></authors><title>A Constant Factor Approximation Algorithm for Fault-Tolerant k-Median</title><categories>cs.DS</categories><comments>19 pages</comments><doi>10.1137/1.9781611973402.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the fault-tolerant $k$-median problem and give the
\emph{first} constant factor approximation algorithm for it. In the
fault-tolerant generalization of classical $k$-median problem, each client $j$
needs to be assigned to at least $r_j \ge 1$ distinct open facilities. The
service cost of $j$ is the sum of its distances to the $r_j$ facilities, and
the $k$-median constraint restricts the number of open facilities to at most
$k$. Previously, a constant factor was known only for the special case when all
$r_j$s are the same, and a logarithmic approximation ratio for the general
case. In addition, we present the first polynomial time algorithm for the
fault-tolerant $k$-median problem on a path or a HST by showing that the
corresponding LP always has an integral optimal solution.
  We also consider the fault-tolerant facility location problem, where the
service cost of $j$ can be a weighted sum of its distance to the $r_j$
facilities. We give a simple constant factor approximation algorithm,
generalizing several previous results which only work for nonincreasing weight
vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2811</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2811</id><created>2013-07-10</created><authors><author><keyname>Cai</keyname><forenames>Sheng</forenames></author><author><keyname>Jahangoshahi</keyname><forenames>Mohammad</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author></authors><title>GROTESQUE: Noisy Group Testing (Quick and Efficient)</title><categories>cs.IT math.IT</categories><comments>26 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group-testing refers to the problem of identifying (with high probability) a
(small) subset of $D$ defectives from a (large) set of $N$ items via a &quot;small&quot;
number of &quot;pooled&quot; tests. For ease of presentation in this work we focus on the
regime when $D = \cO{N^{1-\gap}}$ for some $\gap &gt; 0$. The tests may be
noiseless or noisy, and the testing procedure may be adaptive (the pool
defining a test may depend on the outcome of a previous test), or non-adaptive
(each test is performed independent of the outcome of other tests). A rich body
of literature demonstrates that $\Theta(D\log(N))$ tests are
information-theoretically necessary and sufficient for the group-testing
problem, and provides algorithms that achieve this performance. However, it is
only recently that reconstruction algorithms with computational complexity that
is sub-linear in $N$ have started being investigated (recent work by
\cite{GurI:04,IndN:10, NgoP:11} gave some of the first such algorithms). In the
scenario with adaptive tests with noisy outcomes, we present the first scheme
that is simultaneously order-optimal (up to small constant factors) in both the
number of tests and the decoding complexity ($\cO{D\log(N)}$ in both the
performance metrics). The total number of stages of our adaptive algorithm is
&quot;small&quot; ($\cO{\log(D)}$). Similarly, in the scenario with non-adaptive tests
with noisy outcomes, we present the first scheme that is simultaneously
near-optimal in both the number of tests and the decoding complexity (via an
algorithm that requires $\cO{D\log(D)\log(N)}$ tests and has a decoding
complexity of {${\cal O}(D(\log N+\log^{2}D))$}. Finally, we present an
adaptive algorithm that only requires 2 stages, and for which both the number
of tests and the decoding complexity scale as {${\cal O}(D(\log
N+\log^{2}D))$}. For all three settings the probability of error of our
algorithms scales as $\cO{1/(poly(D)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2817</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2817</id><created>2013-07-10</created><authors><author><keyname>Kumar</keyname><forenames>Vinay</forenames></author><author><keyname>Bhooshan</keyname><forenames>Sunil</forenames></author></authors><title>Design of One-Dimensional Linear Phase Digital IIR Filters Using
  Orthogonal Polynomials</title><categories>cs.OH</categories><comments>8 pages, 11 figures</comments><journal-ref>ISRN Signal Processing, vol. 2012, Article ID 870276, 7 pages,
  2012</journal-ref><doi>10.5402/2012/870276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, we discuss a method to design a linear phase
1-dimensional Infinite Impulse Response (IIR) filter using orthogonal
polynomials. The filter is designed using a set of object functions. These
object functions are realized using a set of orthogonal polynomials. The method
includes placement of zeros and poles in such a way that the amplitude
characteristics are not changed while we change the phase characteristics of
the resulting IIR filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2818</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2818</id><created>2013-07-10</created><authors><author><keyname>Singh</keyname><forenames>Harbinder</forenames></author><author><keyname>Kumar</keyname><forenames>Vinay</forenames></author><author><keyname>Bhooshan</keyname><forenames>Sunil</forenames></author></authors><title>Anisotropic Diffusion for Details Enhancement in Multi-Exposure Image
  Fusion</title><categories>cs.MM cs.CV</categories><comments>30 pages</comments><journal-ref>ISRN Signal Processing, vol. 2013, Article ID 928971, 18 pages,
  2013</journal-ref><doi>10.1155/2013/928971</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a multiexposure image fusion method based on texture features,
which exploits the edge preserving and intraregion smoothing property of
nonlinear diffusion filters based on partial differential equations (PDE). With
the captured multiexposure image series, we first decompose images into base
layers and detail layers to extract sharp details and fine details,
respectively. The magnitude of the gradient of the image intensity is utilized
to encourage smoothness at homogeneous regions in preference to inhomogeneous
regions. Then, we have considered texture features of the base layer to
generate a mask (i.e., decision mask) that guides the fusion of base layers in
multiresolution fashion. Finally, well-exposed fused image is obtained that
combines fused base layer and the detail layers at each scale across all the
input exposures. Proposed algorithm skipping complex High Dynamic Range Image
(HDRI) generation and tone mapping steps to produce detail preserving image for
display on standard dynamic range display devices. Moreover, our technique is
effective for blending flash/no-flash image pair and multifocus images, that
is, images focused on different targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2826</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2826</id><created>2013-07-10</created><authors><author><keyname>Han</keyname><forenames>Bin</forenames></author><author><keyname>Zhao</keyname><forenames>Zhenpeng</forenames></author></authors><title>Image Denoising Using Tensor Product Complex Tight Framelets with
  Increasing Directionality</title><categories>cs.IT math.IT</categories><msc-class>42C40, 42C15, 65T60, 94A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor product real-valued wavelets have been employed in many applications
such as image processing with impressive performance. Though edge singularities
are ubiquitous and play a fundamental role in two-dimensional problems, tensor
product real-valued wavelets are known to be only sub-optimal since they can
only capture edges well along the coordinate axis directions. The dual tree
complex wavelet transform (DTCWT), proposed by Kingsbury [16] and further
developed by Selesnick et al. [24], is one of the most popular and successful
enhancements of the classical tensor product real-valued wavelets. The
two-dimensional DTCWT is obtained via tensor product and offers improved
directionality with 6 directions. In this paper we shall further enhance the
performance of DTCWT for the problem of image denoising. Using framelet-based
approach and the notion of discrete affine systems, we shall propose a family
of tensor product complex tight framelets TPCTF_n for all integers n&gt;2 with
increasing directionality, where n refers to the number of filters in the
underlying one-dimensional complex tight framelet filter bank. For dimension
two, such tensor product complex tight framelet TPCTF_n offers (n-1)(n-3)/2+4
directions when n is odd, and (n-4)(n+2)/2+6 directions when n is even. In
particular, TPCTF_4, which is different to DTCWT in both nature and design,
provides an alternative to DTCWT. Indeed, TPCTF_4 behaves quite similar to
DTCWT by offering 6 directions in dimension two, employing the tensor product
structure, and enjoying slightly less redundancy than DTCWT. When TPCTF_4 is
applied to image denoising, its performance is comparable to DTCWT. Moreover,
better results on image denoising can be obtained by using TPCTF_6. Moreover,
TPCTF_n allows us to further improve DTCWT by using TPCTF_n as the first stage
filter bank in DTCWT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2827</identifier>
 <datestamp>2013-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2827</id><created>2013-07-10</created><updated>2013-08-28</updated><authors><author><keyname>Puljic</keyname><forenames>Marko</forenames></author></authors><title>Site Percolation on Multi-dimensional Lattice</title><categories>math-ph cs.DM math.CO math.MP</categories><comments>1 page, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sites in an infinite d-dimensional lattice, open with probability greater or
equal to 1/d, form an infinite open path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2828</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2828</id><created>2013-07-10</created><updated>2014-03-25</updated><authors><author><keyname>de Luca</keyname><forenames>Aldo</forenames></author><author><keyname>Pribavkina</keyname><forenames>Elena V.</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>A Coloring Problem for Infinite Words</title><categories>math.CO cs.DM</categories><comments>arXiv admin note: incorporates 1301.5263</comments><msc-class>68R15, 05D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the following question in the spirit of Ramsey
theory: Given $x\in A^\omega,$ where $A$ is a finite non-empty set, does there
exist a finite coloring of the non-empty factors of $x$ with the property that
no factorization of $x$ is monochromatic? We prove that this question has a
positive answer using two colors for almost all words relative to the standard
Bernoulli measure on $A^\omega.$ We also show that it has a positive answer for
various classes of uniformly recurrent words, including all aperiodic balanced
words, and all words $x\in A^\omega$ satisfying $\lambda_x(n+1)-\lambda_x(n)=1$
for all $n$ sufficiently large, where $ \lambda_x(n)$ denotes the number of
distinct factors of $x$ of length $n.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2839</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2839</id><created>2013-07-10</created><authors><author><keyname>Bauer</keyname><forenames>Ulrich</forenames></author><author><keyname>Ge</keyname><forenames>Xiaoyin</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Measuring Distance between Reeb Graphs</title><categories>cs.CG math.AT math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the prevailing ideas in geometric and topological data analysis is to
provide descriptors that encode useful information about hidden objects from
observed data. The Reeb graph is one such descriptor for a given scalar
function. The Reeb graph provides a simple yet meaningful abstraction of the
input domain, and can also be computed efficiently.
  Given the popularity of the Reeb graph in applications, it is important to
understand its stability and robustness with respect to changes in the input
function, as well as to be able to compare the Reeb graphs resulting from
different functions.
  In this paper, we propose a metric for Reeb graphs, called the functional
distortion distance. Under this distance measure, the Reeb graph is stable
against small changes of input functions. At the same time, it remains
discriminative at differentiating input functions. In particular, the main
result is that the functional distortion distance between two Reeb graphs is
bounded from below by (and thus more discriminative than) the bottleneck
distance between both the ordinary and extended persistence diagrams for
appropriate dimensions.
  As an application of our results, we analyze a natural simplification scheme
for Reeb graphs, and show that persistent features in Reeb graph remains
persistent under simplification. Understanding the stability of important
features of the Reeb graph under simplification is an interesting problem on
its own right, and critical to the practical usage of Reeb graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2852</identifier>
 <datestamp>2013-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2852</id><created>2013-07-10</created><updated>2013-09-03</updated><authors><author><keyname>M&#xfc;ller</keyname><forenames>Johannes C.</forenames></author></authors><title>Competitive Equilibrium Relaxations in General Auctions</title><categories>math.OC cs.GT</categories><comments>26 pages, 3 figures. (Minor revision: some reformulations, simplified
  proof of Theorem 13, one additional figure.)</comments><msc-class>90C11, 90C33, 91A46, 91B15, 91B26</msc-class><acm-class>G.1.6; K.6.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of an auction is to determine commodity prices such that all
participants are perfectly happy. Such a solution is called a competitive
equilibrium and does not exist in general. For this reason we are interested in
solutions which are similar to a competitive equilibrium. The article
introduces two relaxations of a competitive equilibrium for general auctions.
Both relaxations determine one price per commodity by solving a difficult
non-convex optimization problem. The first model is a mathematical program with
equilibrium constraints (MPEC), which ensures that each participant is either
perfectly happy or his bid is rejected. An exact algorithm and a heuristic are
provided for this model. The second model is a relaxation of the first one and
only ensures that no participant incurs a loss. In an optimal solution to the
second model, no participant can be made better off without making another one
worse off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2855</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2855</id><created>2013-07-10</created><updated>2013-10-13</updated><authors><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author><author><keyname>Zhu</keyname><forenames>Zeyuan Allen</forenames></author></authors><title>Flow-Based Algorithms for Local Graph Clustering</title><categories>cs.DS cs.LG stat.ML</categories><comments>A shorter version of this paper has appeared in the proceedings of
  the 25th ACM-SIAM Symposium on Discrete Algorithms (SODA) 2014</comments><doi>10.1137/1.9781611973402.94</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a subset S of vertices of an undirected graph G, the cut-improvement
problem asks us to find a subset S that is similar to A but has smaller
conductance. A very elegant algorithm for this problem has been given by
Andersen and Lang [AL08] and requires solving a small number of
single-commodity maximum flow computations over the whole graph G. In this
paper, we introduce LocalImprove, the first cut-improvement algorithm that is
local, i.e. that runs in time dependent on the size of the input set A rather
than on the size of the entire graph. Moreover, LocalImprove achieves this
local behaviour while essentially matching the same theoretical guarantee as
the global algorithm of Andersen and Lang.
  The main application of LocalImprove is to the design of better
local-graph-partitioning algorithms. All previously known local algorithms for
graph partitioning are random-walk based and can only guarantee an output
conductance of O(\sqrt{OPT}) when the target set has conductance OPT \in [0,1].
Very recently, Zhu, Lattanzi and Mirrokni [ZLM13] improved this to O(OPT /
\sqrt{CONN}) where the internal connectivity parameter CONN \in [0,1] is
defined as the reciprocal of the mixing time of the random walk over the graph
induced by the target set. In this work, we show how to use LocalImprove to
obtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). This
yields the first flow-based algorithm. Moreover, its performance strictly
outperforms the ones based on random walks and surprisingly matches that of the
best known global algorithm, which is SDP-based, in this parameter regime
[MMV12].
  Finally, our results show that spectral methods are not the only viable
approach to the construction of local graph partitioning algorithm and open
door to the study of algorithms with even better approximation and locality
guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2863</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2863</id><created>2013-07-10</created><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Kupec</keyname><forenames>Martin</forenames></author><author><keyname>Tuma</keyname><forenames>Vojtech</forenames></author></authors><title>Dynamic Data Structure for Tree-Depth Decomposition</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a dynamic data structure for representing a graph $G$ with
tree-depth at most $D$. Tree-depth is an important graph parameter which arose
in the study of sparse graph classes.
  The structure allows addition and removal of edges and vertices such that the
resulting graph still has tree-depth at most $D$, in time bounds depending only
on $D$. A tree-depth decomposition of the graph is maintained explicitly.
  This makes the data structure useful for dynamization of static algorithms
for graphs with bounded tree-depth. As an example application, we give a
dynamic data structure for MSO-property testing, with time bounds for removal
depending only on $D$ and constant-time testing of the property, while the time
for the initialization and insertion also depends on the size of the formula
expressing the property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2867</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2867</id><created>2013-07-10</created><authors><author><keyname>Cohen</keyname><forenames>David A.</forenames></author><author><keyname>Jeavons</keyname><forenames>Peter G.</forenames></author><author><keyname>Thorstensen</keyname><forenames>Evgenij</forenames></author><author><keyname>&#x17d;ivn&#xfd;</keyname><forenames>Stanislav</forenames></author></authors><title>Tractable Combinations of Global Constraints</title><categories>cs.AI cs.LO</categories><comments>To appear in proceedings of CP'13, LNCS 8124. arXiv admin note: text
  overlap with arXiv:1307.1790</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of constraint satisfaction problems involving global
constraints, i.e., special-purpose constraints provided by a solver and
represented implicitly by a parametrised algorithm. Such constraints are widely
used; indeed, they are one of the key reasons for the success of constraint
programming in solving real-world problems.
  Previous work has focused on the development of efficient propagators for
individual constraints. In this paper, we identify a new tractable class of
constraint problems involving global constraints of unbounded arity. To do so,
we combine structural restrictions with the observation that some important
types of global constraint do not distinguish between large classes of
equivalent solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2889</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2889</id><created>2013-07-10</created><authors><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Achieving the Uniform Rate Region of Multiple Access Channels Using
  Polar Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing polar codes for transmission over two-user multiple access channels
is considered. In the proposed scheme, both users encode their messages using a
polar encoder, while a joint successive cancellation decoder is deployed at the
receiver. The encoding is done separately, while the codes are constructed
jointly. This is done by treating the whole polar transformation on both users
as a single polar transformation, wherein the multiple access channel (MAC) is
regarded as one more level of polarization. We prove that our scheme achieves
the whole uniform rate region by changing the decoding order in the joint
successive cancellation decoder. Various simulation results over
binary-additive Gaussian noise MAC are provided. At the end, a comparison is
made with the existing results on polar codes for multiple access channels to
emphasize the differences and the main advantages of our scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2893</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2893</id><created>2013-07-10</created><updated>2015-11-25</updated><authors><author><keyname>Antunovi&#x107;</keyname><forenames>Ton&#x107;i</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Racz</keyname><forenames>Miklos Z.</forenames></author></authors><title>Coexistence in preferential attachment networks</title><categories>physics.soc-ph cs.SI math.PR</categories><comments>18 pages, 4 figures; v2 incorporates referee comments and suggestions</comments><doi>10.1017/S0963548315000383</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new model of competition on growing networks. This extends the
preferential attachment model, with the key property that node choices evolve
simultaneously with the network. When a new node joins the network, it chooses
neighbours by preferential attachment, and selects its type based on the number
of initial neighbours of each type. The model is analysed in detail, and in
particular, we determine the possible proportions of the various types in the
limit of large networks. An important qualitative feature we find is that, in
contrast to many current theoretical models, often several competitors will
coexist. This matches empirical observations in many real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2908</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2908</id><created>2013-07-10</created><updated>2013-10-28</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Ye</keyname><forenames>Chun</forenames></author></authors><title>Cake Cutting Algorithms for Piecewise Constant and Piecewise Uniform
  Valuations</title><categories>cs.GT cs.DS</categories><comments>39 pages</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cake cutting is one of the most fundamental settings in fair division and
mechanism design without money. In this paper, we consider different levels of
three fundamental goals in cake cutting: fairness, Pareto optimality, and
strategyproofness. In particular, we present robust versions of envy-freeness
and proportionality that are not only stronger than their standard
counter-parts but also have less information requirements. We then focus on
cake cutting with piecewise constant valuations and present three desirable
algorithms: CCEA (Controlled Cake Eating Algorithm), MEA (Market Equilibrium
Algorithm) and CSD (Constrained Serial Dictatorship). CCEA is polynomial-time,
robust envy-free, and non-wasteful. It relies on parametric network flows and
recent generalizations of the probabilistic serial algorithm. For the subdomain
of piecewise uniform valuations, we show that it is also group-strategyproof.
Then, we show that there exists an algorithm (MEA) that is polynomial-time,
envy-free, proportional, and Pareto optimal. MEA is based on computing a
market-based equilibrium via a convex program and relies on the results of
Reijnierse and Potters [24] and Devanur et al. [15]. Moreover, we show that MEA
and CCEA are equivalent to mechanism 1 of Chen et. al. [12] for piecewise
uniform valuations. We then present an algorithm CSD and a way to implement it
via randomization that satisfies strategyproofness in expectation, robust
proportionality, and unanimity for piecewise constant valuations. For the case
of two agents, it is robust envy-free, robust proportional, strategyproof, and
polynomial-time. Many of our results extend to more general settings in cake
cutting that allow for variable claims and initial endowments. We also show a
few impossibility results to complement our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2915</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2915</id><created>2013-07-10</created><authors><author><keyname>Kim</keyname><forenames>Woo-Cheol</forenames></author><author><keyname>Baek</keyname><forenames>Changryong</forenames></author><author><keyname>Lee</keyname><forenames>Dongwon</forenames></author></authors><title>Measuring the Optimality of Hadoop Optimization</title><categories>cs.DC cs.PF</categories><acm-class>H.3.4; B.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, much research has focused on how to optimize Hadoop jobs.
Their approaches are diverse, ranging from improving HDFS and Hadoop job
scheduler to optimizing parameters in Hadoop configurations. Despite their
success in improving the performance of Hadoop jobs, however, very little is
known about the limit of their optimization performance. That is, how optimal
is a given Hadoop optimization? When a Hadoop optimization method X improves
the performance of a job by Y %, how do we know if this improvement is as good
as it can be? To answer this question, in this paper, we first examine the
ideal best case, the lower bound, of running time for Hadoop jobs and develop a
measure to accurately estimate how optimal a given Hadoop optimization is with
respect to the lower bound. Then, we demonstrate how one may exploit the
proposed measure to improve the optimization of Hadoop jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2923</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2923</id><created>2013-07-10</created><updated>2014-05-01</updated><authors><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Papadogiannis</keyname><forenames>Agisilaos</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Two-Way Relaying under the Presence of Relay Transceiver Hardware
  Impairments</title><categories>cs.IT math.IT</categories><comments>Published in IEEE Communications Letters, 4 pages, 3 figures. The
  results can be reproduced using the following Matlab code:
  https://github.com/emilbjornson/twoway-relaying-hardware-impairments</comments><journal-ref>IEEE Communications Letters, vol. 17, no. 6, pp. 1136-1139, June
  2013</journal-ref><doi>10.1109/LCOMM.2013.042313.130191</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardware impairments in physical transceivers are known to have a deleterious
effect on communication systems; however, very few contributions have
investigated their impact on relaying. This paper quantifies the impact of
transceiver impairments in a two-way amplify-and-forward configuration. More
specifically, the effective signal-to-noise-and-distortion ratios at both
transmitter nodes are obtained. These are used to deduce exact and asymptotic
closed-form expressions for the outage probabilities (OPs), as well as
tractable formulations for the symbol error rates (SERs). It is explicitly
shown that non-zero lower bounds on the OP and SER exist in the high-power
regime---this stands in contrast to the special case of ideal hardware, where
the OP and SER go asymptotically to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2953</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2953</id><created>2013-07-10</created><authors><author><keyname>Khan</keyname><forenames>Atta ur Rehman</forenames></author><author><keyname>Othman</keyname><forenames>Mazliza</forenames></author><author><keyname>Khan</keyname><forenames>Abdul Nasir</forenames></author><author><keyname>khan</keyname><forenames>Imran Ali</forenames></author></authors><title>Framework for Ubiquitous Social Networks</title><categories>cs.CY</categories><comments>2012 1st International Conference on Future Trends in Computing and
  Communication Technologies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel framework for ubiquitous social networks (USNs).
Instead of making virtual connections, on the basis of human social networks,
an effort has been made to facilitate interactions among human social networks
with the help of virtual social networks. The imperative domains that support
ubiquitous social networks are highlighted and different scenarios are provided
to project real world applications of proposed framework. Our proposed
framework can provide preliminary foundations for creating ubiquitous social
networks in true essence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2958</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2958</id><created>2013-07-10</created><updated>2014-01-02</updated><authors><author><keyname>Siriteanu</keyname><forenames>Constantin</forenames></author><author><keyname>Blostein</keyname><forenames>Steven</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author><author><keyname>Shin</keyname><forenames>Hyundong</forenames></author><author><keyname>Yousefi</keyname><forenames>Shahram</forenames></author><author><keyname>Kuriki</keyname><forenames>Satoshi</forenames></author></authors><title>Exact MIMO Zero-Forcing Detection Analysis for Transmit-Correlated
  Rician Fading</title><categories>cs.IT math.IT</categories><comments>14 pages, two-colum, 1 table, 10 figures</comments><report-no>METR 2013-07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the performance of multiple input/multiple output (MIMO)
communications systems employing spatial multiplexing and zero-forcing
detection (ZF). The distribution of the ZF signal-to-noise ratio (SNR) is
characterized when either the intended stream or interfering streams experience
Rician fading, and when the fading may be correlated on the transmit side.
Previously, exact ZF analysis based on a well-known SNR expression has been
hindered by the noncentrality of the Wishart distribution involved. In
addition, approximation with a central-Wishart distribution has not proved
consistently accurate. In contrast, the following exact ZF study proceeds from
a lesser-known SNR expression that separates the intended and interfering
channel-gain vectors. By first conditioning on, and then averaging over the
interference, the ZF SNR distribution for Rician-Rayleigh fading is shown to be
an infinite linear combination of gamma distributions. On the other hand, for
Rayleigh-Rician fading, the ZF SNR is shown to be gamma-distributed. Based on
the SNR distribution, we derive new series expressions for the ZF average error
probability, outage probability, and ergodic capacity. Numerical results
confirm the accuracy of our new expressions, and reveal effects of interference
and channel statistics on performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2964</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2964</id><created>2013-07-10</created><updated>2013-10-12</updated><authors><author><keyname>Li</keyname><forenames>Xin</forenames></author><author><keyname>Thanh</keyname><forenames>Hua Vy Le</forenames></author></authors><title>Generating Stack-based Access Control Policies</title><categories>cs.CR cs.PL</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stack-based access control mechanism plays a fundamental role in the
security architecture of Java and Microsoft CLR (common language runtime). It
is enforced at runtime by inspecting methods in the current call stack for
granted permissions before the program performs safety-critical operations.
Although stack inspection is well studied, there is relatively little work on
automated generation of access control policies, and most existing work on
inferring security policies assume the permissions to be checked at stack
inspection points are known beforehand. Practiced approaches to generating
access control policies are still manually done by developers based on
domain-specific knowledges and trial-and-error testing. In this paper, we
present a systematic approach to automated generation of access control
policies for Java programs that necessarily ensure the program to pass stack
inspection. The techniques are abstract interpretation based context-sensitive
static program analyses. Our analysis models the program by combining a
context-sensitive call graph with a dependency graph. We are hereby able to
precisely identify permission requirements at stack inspection points, which
are usually ignored in previous study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2965</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2965</id><created>2013-07-10</created><updated>2014-04-22</updated><authors><author><keyname>Wang</keyname><forenames>Quan</forenames></author><author><keyname>Wu</keyname><forenames>Dijia</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Liu</keyname><forenames>Meizhu</forenames></author><author><keyname>Boyer</keyname><forenames>Kim L.</forenames></author><author><keyname>Zhou</keyname><forenames>Shaohua Kevin</forenames></author></authors><title>Semantic Context Forests for Learning-Based Knee Cartilage Segmentation
  in 3D MR Images</title><categories>cs.CV</categories><comments>MICCAI 2013: Workshop on Medical Computer Vision</comments><doi>10.1007/978-3-319-05530-5_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automatic segmentation of human knee cartilage from 3D MR images is a
useful yet challenging task due to the thin sheet structure of the cartilage
with diffuse boundaries and inhomogeneous intensities. In this paper, we
present an iterative multi-class learning method to segment the femoral, tibial
and patellar cartilage simultaneously, which effectively exploits the spatial
contextual constraints between bone and cartilage, and also between different
cartilages. First, based on the fact that the cartilage grows in only certain
area of the corresponding bone surface, we extract the distance features of not
only to the surface of the bone, but more informatively, to the densely
registered anatomical landmarks on the bone surface. Second, we introduce a set
of iterative discriminative classifiers that at each iteration, probability
comparison features are constructed from the class confidence maps derived by
previously learned classifiers. These features automatically embed the semantic
context information between different cartilages of interest. Validated on a
total of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the
proposed approach demonstrates high robustness and accuracy of segmentation in
comparison with existing state-of-the-art MR cartilage segmentation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2967</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2967</id><created>2013-07-10</created><updated>2016-02-24</updated><authors><author><keyname>Min</keyname><forenames>Byungjoon</forenames></author><author><keyname>Gwak</keyname><forenames>Sang-Hwan</forenames></author><author><keyname>Lee</keyname><forenames>Nanoom</forenames></author><author><keyname>Goh</keyname><forenames>K. -I.</forenames></author></authors><title>Layer-switching cost and optimality in information spreading on
  multiplex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>15 pages, 7 figures</comments><journal-ref>Scientific Reports 6, 21392 (2016)</journal-ref><doi>10.1038/srep21392</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a model of information spreading on multiplex networks, in which
agents interact through multiple interaction channels (layers), say online vs.\
offline communication layers, subject to layer-switching cost for transmissions
across different interaction layers. The model is characterized by the
layer-wise path-dependent transmissibility over a contact, that is dynamically
determined dependently on both incoming and outgoing transmission layers. We
formulate an analytical framework to deal with such path-dependent
transmissibility and demonstrate the nontrivial interplay between the
multiplexity and spreading dynamics, including optimality. It is shown that the
epidemic threshold and prevalence respond to the layer-switching cost
non-monotonically and that the optimal conditions can change in abrupt
non-analytic ways, depending also on the densities of network layers and the
type of seed infections. Our results elucidate the essential role of
multiplexity that its explicit consideration should be crucial for realistic
modeling and prediction of spreading phenomena on multiplex social networks in
an era of ever-diversifying social interaction layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2968</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2968</id><created>2013-07-10</created><updated>2016-01-30</updated><authors><author><keyname>Zukerman</keyname><forenames>Moshe</forenames></author></authors><title>Introduction to Queueing Theory and Stochastic Teletraffic Models</title><categories>math.PR cs.IT math.IT</categories><comments>260 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this textbook is to provide students with basic knowledge of
stochastic models that may apply to telecommunications research areas, such as
traffic modelling, resource provisioning and traffic management. These study
areas are often collectively called teletraffic. This book assumes prior
knowledge of a programming language, mathematics, probability and stochastic
processes normally taught in an electrical engineering course. For students who
have some but not sufficiently strong background in probability and stochastic
processes, we provide, in the first few chapters, background on the relevant
concepts in these areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2971</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2971</id><created>2013-07-11</created><authors><author><keyname>Flesia</keyname><forenames>Ana Georgina</forenames></author><author><keyname>Baumgartner</keyname><forenames>Josef</forenames></author><author><keyname>Gimenez</keyname><forenames>Javier</forenames></author><author><keyname>Martinez</keyname><forenames>Jorge</forenames></author></authors><title>Accuracy of MAP segmentation with hidden Potts and Markov mesh prior
  models via Path Constrained Viterbi Training, Iterated Conditional Modes and
  Graph Cut based algorithms</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study statistical classification accuracy of two different
Markov field environments for pixelwise image segmentation, considering the
labels of the image as hidden states and solving the estimation of such labels
as a solution of the MAP equation. The emission distribution is assumed the
same in all models, and the difference lays in the Markovian prior hypothesis
made over the labeling random field. The a priori labeling knowledge will be
modeled with a) a second order anisotropic Markov Mesh and b) a classical
isotropic Potts model. Under such models, we will consider three different
segmentation procedures, 2D Path Constrained Viterbi training for the Hidden
Markov Mesh, a Graph Cut based segmentation for the first order isotropic Potts
model, and ICM (Iterated Conditional Modes) for the second order isotropic
Potts model.
  We provide a unified view of all three methods, and investigate goodness of
fit for classification, studying the influence of parameter estimation,
computational gain, and extent of automation in the statistical measures
Overall Accuracy, Relative Improvement and Kappa coefficient, allowing robust
and accurate statistical analysis on synthetic and real-life experimental data
coming from the field of Dental Diagnostic Radiography. All algorithms, using
the learned parameters, generate good segmentations with little interaction
when the images have a clear multimodal histogram. Suboptimal learning proves
to be frail in the case of non-distinctive modes, which limits the complexity
of usable models, and hence the achievable error rate as well.
  All Matlab code written is provided in a toolbox available for download from
our website, following the Reproducible Research Paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2977</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2977</id><created>2013-07-11</created><authors><author><keyname>Lu</keyname><forenames>Qiwei</forenames></author><author><keyname>Huang</keyname><forenames>Wenchao</forenames></author><author><keyname>Gong</keyname><forenames>Xudong</forenames></author><author><keyname>Wang</keyname><forenames>Xingfu</forenames></author><author><keyname>Xiong</keyname><forenames>Yan</forenames></author><author><keyname>Miao</keyname><forenames>Fuyou</forenames></author></authors><title>A Secure Distributed Authentication scheme based on CRT-VSS and Trusted
  Computing in MANET</title><categories>cs.CR</categories><comments>under submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of MANET, secure and practical authentication is
becoming increasingly important. The existing works perform the research from
two aspects, i.e., (a)secure key division and distributed storage, (b)secure
distributed authentication. But there still exist several unsolved problems.
Specifically, it may suffer from cheating problems and fault authentication
attack, which can result in authentication failure and DoS attack towards
authentication service. Besides, most existing schemes are not with
satisfactory efficiency due to exponential arithmetic based on Shamir's scheme.
In this paper, we explore the property of verifiable secret sharing(VSS)
schemes with Chinese Remainder Theorem (CRT), then propose a secret key
distributed storage scheme based on CRT-VSS and trusted computing for MANET.
Specifically, we utilize trusted computing technology to solve two existing
cheating problems in secret sharing area before. After that, we do the analysis
of homomorphism property with CRT-VSS and design the corresponding
shares-product sharing scheme with better concision. On such basis, a secure
distributed Elliptic Curve-Digital Signature Standard signature (ECC-DSS)
authentication scheme based on CRT-VSS scheme and trusted computing is
proposed. Furthermore, as an important property of authentication scheme, we
discuss the refreshing property of CRT-VSS and do thorough comparisons with
Shamir's scheme. Finally, we provide formal guarantees towards our schemes
proposed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2982</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2982</id><created>2013-07-11</created><updated>2014-04-24</updated><authors><author><keyname>Norouzi</keyname><forenames>Mohammad</forenames></author><author><keyname>Punjani</keyname><forenames>Ali</forenames></author><author><keyname>Fleet</keyname><forenames>David J.</forenames></author></authors><title>Fast Exact Search in Hamming Space with Multi-Index Hashing</title><categories>cs.CV cs.AI cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is growing interest in representing image data and feature descriptors
using compact binary codes for fast near neighbor search. Although binary codes
are motivated by their use as direct indices (addresses) into a hash table,
codes longer than 32 bits are not being used as such, as it was thought to be
ineffective. We introduce a rigorous way to build multiple hash tables on
binary code substrings that enables exact k-nearest neighbor search in Hamming
space. The approach is storage efficient and straightforward to implement.
Theoretical analysis shows that the algorithm exhibits sub-linear run-time
behavior for uniformly distributed codes. Empirical results show dramatic
speedups over a linear scan baseline for datasets of up to one billion codes of
64, 128, or 256 bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2984</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2984</id><created>2013-07-11</created><authors><author><keyname>Zheng</keyname><forenames>Xiaoying</forenames></author><author><keyname>Cho</keyname><forenames>Chunglae</forenames></author><author><keyname>Xia</keyname><forenames>Ye</forenames></author></authors><title>Content Distribution by Multiple Multicast Trees and Intersession
  Cooperation: Optimal Algorithms and Approximations</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In traditional massive content distribution with multiple sessions, the
sessions form separate overlay networks and operate independently, where some
sessions may suffer from insufficient resources even though other sessions have
excessive resources. To cope with this problem, we consider the universal
swarming approach, which allows multiple sessions to cooperate with each other.
We formulate the problem of finding the optimal resource allocation to maximize
the sum of the session utilities and present a subgradient algorithm which
converges to the optimal solution in the time-average sense. The solution
involves an NP-hard subproblem of finding a minimum-cost Steiner tree. We cope
with this difficulty by using a column generation method, which reduces the
number of Steiner-tree computations. Furthermore, we allow the use of
approximate solutions to the Steiner-tree subproblem. We show that the
approximation ratio to the overall problem turns out to be no less than the
reciprocal of the approximation ratio to the Steiner-tree subproblem.
Simulation results demonstrate that universal swarming improves the performance
of resource-poor sessions with negligible impact to resource-rich sessions. The
proposed approach and algorithm are expected to be useful for
infrastructure-based content distribution networks with long-lasting sessions
and relatively stable network environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2987</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2987</id><created>2013-07-11</created><authors><author><keyname>Brazil</keyname><forenames>M.</forenames></author><author><keyname>Ras</keyname><forenames>C. J.</forenames></author><author><keyname>Thomas</keyname><forenames>D. A.</forenames></author></authors><title>Approximating Minimum Steiner Point Trees in Minkowski Planes</title><categories>math.OC cs.CG</categories><msc-class>90B18, 05C40, 90B85</msc-class><journal-ref>Networks. 56:244-254. 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of points, we define a minimum Steiner point tree to be a tree
interconnecting these points and possibly some additional points such that the
length of every edge is at most 1 and the number of additional points is
minimized. We propose using Steiner minimal trees to approximate minimum
Steiner point trees. It is shown that in arbitrary metric spaces this gives a
performance difference of at most $2n-4$, where $n$ is the number of terminals.
We show that this difference is best possible in the Euclidean plane, but not
in Minkowski planes with parallelogram unit balls. We also introduce a new
canonical form for minimum Steiner point trees in the Euclidean plane; this
demonstrates that minimum Steiner point trees are shortest total length trees
with a certain discrete-edge-length condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2991</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2991</id><created>2013-07-11</created><updated>2013-07-11</updated><authors><author><keyname>Lu</keyname><forenames>Qiwei</forenames></author><author><keyname>Huang</keyname><forenames>Wenchao</forenames></author><author><keyname>Xiong</keyname><forenames>Yan</forenames></author><author><keyname>Gong</keyname><forenames>Xudong</forenames></author></authors><title>Integrity Verification for Outsourcing Uncertain Frequent Itemset Mining</title><categories>cs.DB</categories><comments>under submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, due to the wide applications of uncertain data (e.g., noisy
data), uncertain frequent itemsets (UFI) mining over uncertain databases has
attracted much attention, which differs from the corresponding deterministic
problem from the generalized definition and resolutions. As the most costly
task in association rule mining process, it has been shown that outsourcing
this task to a service provider (e.g.,the third cloud party) brings several
benefits to the data owner such as cost relief and a less commitment to storage
and computational resources. However, the correctness integrity of mining
results can be corrupted if the service provider is with random fault or not
honest (e.g., lazy, malicious, etc). Therefore, in this paper, we focus on the
integrity and verification issue in UFI mining problem during outsourcing
process, i.e., how the data owner verifies the mining results. Specifically, we
explore and extend the existing work on deterministic FI outsourcing
verification to uncertain scenario. For this purpose, We extend the existing
outsourcing FI mining work to uncertain area w.r.t. the two popular UFI
definition criteria and the approximate UFI mining methods. Specifically, We
construct and improve the basic/enhanced verification scheme with such
different UFI definition respectively. After that, we further discuss the
scenario of existing approximation UFP mining, where we can see that our
technique can provide good probabilistic guarantees about the correctness of
the verification. Finally, we present the comparisons and analysis on the
schemes proposed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.2997</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.2997</id><created>2013-07-11</created><authors><author><keyname>Padmavathi</keyname><forenames>S.</forenames></author><author><keyname>S</keyname><forenames>Manojna K. S.</forenames></author><author><keyname>Reddy</keyname><forenames>S. Sphoorthy</forenames></author><author><keyname>Meenakshy</keyname><forenames>D.</forenames></author></authors><title>Conversion of Braille to Text in English, Hindi and Tamil Languages</title><categories>cs.CV</categories><comments>14 pages, 20 figures, 4 tables</comments><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA) Vol.3, No.3, June 2013</journal-ref><doi>10.5121/ijcsea.2013.3303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Braille system has been used by the visually impaired for reading and
writing. Due to limited availability of the Braille text books an efficient
usage of the books becomes a necessity. This paper proposes a method to convert
a scanned Braille document to text which can be read out to many through the
computer. The Braille documents are pre processed to enhance the dots and
reduce the noise. The Braille cells are segmented and the dots from each cell
is extracted and converted in to a number sequence. These are mapped to the
appropriate alphabets of the language. The converted text is spoken out through
a speech synthesizer. The paper also provides a mechanism to type the Braille
characters through the number pad of the keyboard. The typed Braille character
is mapped to the alphabet and spoken out. The Braille cell has a standard
representation but the mapping differs for each language. In this paper mapping
of English, Hindi and Tamil are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3003</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3003</id><created>2013-07-11</created><updated>2013-08-06</updated><authors><author><keyname>Massaro</keyname><forenames>Emanuele</forenames></author><author><keyname>Valerio</keyname><forenames>Lorenzo</forenames></author><author><keyname>Guazzini</keyname><forenames>Andrea</forenames></author><author><keyname>Passarella</keyname><forenames>Andrea</forenames></author><author><keyname>Bagnoli</keyname><forenames>Franco</forenames></author></authors><title>Application of a cognitive-inspired algorithm for detecting communities
  in mobility networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence and the global adaptation of mobile devices has influenced
human interactions at the individual, community, and social levels leading to
the so called Cyber-Physical World (CPW) convergence scenario [1]. One of the
most important features of CPW is the possibility of exploiting information
about the structure of the social communities of users, revealed by joint
movement patterns and frequency of physical co-location. Mobile devices of
users that belong to the same social community are likely to &quot;see&quot; each other
(and thus be able to communicate through ad-hoc networking techniques) more
frequently and regularly than devices outside the community. In mobile
opportunistic networks, this fact can be exploited, for example, to optimize
networking operations such as forwarding and dissemination of messages. In this
paper we present the application of a cognitive-inspired algorithm [2,3,4] for
revealing the structure of these dynamic social networks (simulated by the HCMM
model [5]) using information about physical encounters logged by the users'
mobile devices. The main features of our algorithm are: (i) the capacity of
detecting social communities induced by physical co-location of users through
distributed algorithms; (ii) the capacity to detect users belonging to more
communities (thus acting as bridges across them), and (iii) the capacity to
detect the time evolution of communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3004</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3004</id><created>2013-07-11</created><authors><author><keyname>Sharma</keyname><forenames>Sharad</forenames></author><author><keyname>Kumar</keyname><forenames>Shakti</forenames></author><author><keyname>Singh</keyname><forenames>Brahmjit</forenames></author></authors><title>Routing in Wireless Mesh Networks: Two Soft Computing Based Approaches</title><categories>cs.NI cs.AI</categories><comments>11 Pages, 7 Figures</comments><msc-class>94Axx</msc-class><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics ( IJMNCT) Vol. 3, No.3, June 2013</journal-ref><doi>10.5121/ijmnct.2013.3304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to dynamic network conditions, routing is the most critical part in WMNs
and needs to be optimised. The routing strategies developed for WMNs must be
efficient to make it an operationally self configurable network. Thus we need
to resort to near shortest path evaluation. This lays down the requirement of
some soft computing approaches such that a near shortest path is available in
an affordable computing time. This paper proposes a Fuzzy Logic based
integrated cost measure in terms of delay, throughput and jitter. Based upon
this distance (cost) between two adjacent nodes we evaluate minimal shortest
path that updates routing tables. We apply two recent soft computing approaches
namely Big Bang Big Crunch (BB-BC) and Biogeography Based Optimization (BBO)
approaches to enumerate shortest or near short paths. BB-BC theory is related
with the evolution of the universe whereas BBO is inspired by dynamical
equilibrium in the number of species on an island. Both the algorithms have low
computational time and high convergence speed. Simulation results show that the
proposed routing algorithms find the optimal shortest path taking into account
three most important parameters of network dynamics. It has been further
observed that for the shortest path problem BB-BC outperforms BBO in terms of
speed and percent error between the evaluated minimal path and the actual
shortest path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3005</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3005</id><created>2013-07-11</created><authors><author><keyname>Hoseini</keyname><forenames>Sayed Amir</forenames></author><author><keyname>Ashraf</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Computational Complexity Comparison Of Multi-Sensor Single Target Data
  Fusion Methods By Matlab</title><categories>cs.SY</categories><doi>10.5121/ijccms.2013.2201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Target tracking using observations from multiple sensors can achieve better
estimation performance than a single sensor. The most famous estimation tool in
target tracking is Kalman filter. There are several mathematical approaches to
combine the observations of multiple sensors by use of Kalman filter. An
important issue in applying a proper approach is computational complexity. In
this paper, four data fusion algorithms based on Kalman filter are considered
including three centralized and one decentralized methods. Using MATLAB,
computational loads of these methods are compared while number of sensors
increases. The results show that inverse covariance method has the best
computational performance if the number of sensors is above 20. For a smaller
number of sensors, other methods, especially group sensors, are more
appropriate..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3011</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3011</id><created>2013-07-11</created><authors><author><keyname>Kumar</keyname><forenames>Shakti</forenames></author><author><keyname>Singh</keyname><forenames>Brahmjit</forenames></author><author><keyname>Sharma</keyname><forenames>Sharad</forenames></author></authors><title>Soft Computing Framework for Routing in Wireless Mesh Networks: An
  Integrated Cost Function Approach</title><categories>cs.NI cs.AI</categories><comments>8 pages, 19 Figures</comments><msc-class>94Axx</msc-class><journal-ref>International Journal of Electronics, Computer and Communication
  Technologies (IJECCT), 2013, Vol.3(3),pp.25-32, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic behaviour of a WMN imposes stringent constraints on the routing
policy of the network. In the shortest path based routing the shortest paths
needs to be evaluated within a given time frame allowed by the WMN dynamics.
The exact reasoning based shortest path evaluation methods usually fail to meet
this rigid requirement. Thus, requiring some soft computing based approaches
which can replace &quot;best for sure&quot; solutions with &quot;good enough&quot; solutions. This
paper proposes a framework for optimal routing in the WMNs; where we
investigate the suitability of Big Bang-Big Crunch (BB-BC), a soft computing
based approach to evaluate shortest/near-shortest path. In order to make
routing optimal we first propose to replace distance between the adjacent nodes
with an integrated cost measure that takes into account throughput, delay,
jitter and residual energy of a node. A fuzzy logic based inference mechanism
evaluates this cost measure at each node. Using this distance measure we apply
BB-BC optimization algorithm to evaluate shortest/near shortest path to update
the routing tables periodically as dictated by network requirements. A large
number of simulations were conducted and it has been observed that BB-BC
algorithm appears to be a high potential candidate suitable for routing in
WMNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3013</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3013</id><created>2013-07-11</created><authors><author><keyname>Umezu</keyname><forenames>Keisuke</forenames></author><author><keyname>Kawamura</keyname><forenames>Takahiro</forenames></author><author><keyname>Ohsuga</keyname><forenames>Akihiko</forenames></author></authors><title>Context-based Barrier Notification Service Toward Outdoor Support for
  the Elderly</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aging society has been becoming a global problem not only in advanced
countries. Under such circumstances, it is said that participation of elderly
people in social activities is highly desirable from various perspectives
including decrease of social welfare costs. Thus, we propose a mobile service
that notifies barrier information nearby users outside to lowers the anxiety of
elderly people and promote their social activities. There are barrier free maps
in some areas, but those are static and updated annually at the earliest.
However, there exist temporary barriers like road repairing and parked
bicycles, and also every barrier is not for every elder person. That is, the
elder people are under several conditions and wills to go out, so that a
barrier for an elder person is not necessarily the one for the other.
Therefore, we first collect the barrier information in the user participatory
manner and select the ones the user need to know, then timely provide them via
a mobile phone equipped with GPS. This paper shows the public experiment that
we conducted in Tokyo, and confirms the usability and the accuracy of the
information filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3014</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3014</id><created>2013-07-11</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Senthilkumar</keyname><forenames>S.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>V. J.</forenames></author></authors><title>A New Approach to the Solution of Economic Dispatch Using Particle Swarm
  Optimization with Simulated Annealing</title><categories>cs.CE cs.NE</categories><comments>12 pages,3 figures. arXiv admin note: text overlap with
  arXiv:1206.0915, arXiv:0910.4116, arXiv:1306.1454 by other authors</comments><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA)Vol.3, No.3, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach to the solution of Economic Dispatch using Particle Swarm
Optimization is presented. It is the progression of allocating production
amongst the dedicated units such that the restriction forced are fulfilled and
the power needs are reduced. More just, the soft computing method has received
supplementary concentration and was used in a quantity of successful and
sensible applications. Here, an attempt has been made to find out the minimum
cost by using Particle Swarm Optimization Algorithm using the data of three
generating units. In this work, data has been taken such as the loss
coefficients with the max-min power limit and cost function. PSO and Simulated
Annealing are functional to put out the least amount for dissimilar energy
requirements. When the outputs are compared with the conventional method, PSO
seems to give an improved result with enhanced convergence feature. All the
methods are executed in MATLAB environment. The effectiveness and feasibility
of the proposed method were demonstrated by three generating units case study.
Output gives hopeful results, signifying that the projected method of
calculation is competent of economically formative advanced eminence solutions
addressing economic dispatch problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3015</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3015</id><created>2013-07-11</created><authors><author><keyname>James</keyname><forenames>Craig</forenames></author><author><keyname>Huang</keyname><forenames>Weidong</forenames></author><author><keyname>Stepanas</keyname><forenames>Kazys</forenames></author><author><keyname>Widzyk-Capehart</keyname><forenames>Eleonora</forenames></author><author><keyname>Alem</keyname><forenames>Leila</forenames></author><author><keyname>Gunn</keyname><forenames>Chris</forenames></author><author><keyname>Adcock</keyname><forenames>Matt</forenames></author><author><keyname>Haustein</keyname><forenames>Kerstin</forenames></author></authors><title>Designing a Network Based System for Delivery of Remote Mine Services</title><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There is a great body of work in the areas of
tele-assistance/tele-collaboration offering novel and effective ways to improve
collaboration between personnel located at a remote mine site and off-site
personnel located in major metropolitan areas. Much of this work involves the
use of high-bandwidth communications or targeted sensory experiences using
large format displays. There are also existing remote access technologies but
these suffer from limited functionality (providing text, voice, video or
one-way desktop sharing), are often poorly supported in the security-conscious
corporate environment and require complicated set up processes. There is
currently no singular piece of remote collaboration technology that is suitable
for the delivery of high-quality planning and scheduling services to clients at
a mining site from a remote operating centre. In response to this issue, as
part of a research and technology development effort between CSIRO and a mining
engineering firm, we have developed a concept of remote mining engineer (RME)
and conducted a functional requirements analysis for delivering mining
engineering services to mine sites remotely. Based on the obtained
requirements, a further study was performed to characterise existing
technologies and to identify the scope for future work in designing and
prototyping a network based system for RME. We report on the method and
findings of this study in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3017</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3017</id><created>2013-07-11</created><authors><author><keyname>Kaur</keyname><forenames>Kanika</forenames></author><author><keyname>Noor</keyname><forenames>Arti</forenames></author></authors><title>CMOS Low Power Cell Library For Digital Design</title><categories>cs.OH</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Historically, VLSI designers have focused on increasing the speed and
reducing the area of digital systems. However, the evolution of portable
systems and advanced Deep Sub-Micron fabrication technologies have brought
power dissipation as another critical design factor. Low power design reduces
cooling cost and increases reliability especially for high density systems.
Moreover, it reduces the weight and size of portable devices. The power
dissipation in CMOS circuits consists of static and dynamic components. Since
dynamic power is proportional to V2 dd and static power is proportional to Vdd,
lowering the supply voltage and device dimensions, the transistor threshold
voltage also has to be scaled down to achieve the required performance. In case
of static power, the power is consumed during the steady state condition i.e
when there are no input/output transitions. Static power has two sources: DC
power and Leakage power. Consecutively to facilitate voltage scaling without
disturbing the performance, threshold voltage has to be minimized. Furthermore
it leads to better noise margins and helps to avoid the hot carrier effects in
short channel devices. In this paper we have been proposed the new CMOS library
for the complex digital design using scaling the supply voltage and device
dimensions and also suggest the methods to control the leakage current to
obtain the minimum power dissipation at optimum value of supply voltage and
transistor threshold. In this paper CMOS Cell library has been implemented
using TSMC (0.18um) and TSMC (90nm) technology using HEP2 tool of IC designing
from Mentor Graphics for various analysis and simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3026</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3026</id><created>2013-07-11</created><authors><author><keyname>Hemalatha</keyname><forenames>S.</forenames></author><author><keyname>Acharya</keyname><forenames>U. Dinesh</forenames></author><author><keyname>Renuka</keyname><forenames>A.</forenames></author></authors><title>Comparison of secure and high capacity color image steganography
  techniques in RGB and YCbCr domains</title><categories>cs.MM cs.CR</categories><comments>Journal, 9 pages</comments><journal-ref>International Journal of Advanced Information Technology (IJAIT)
  Vol. 3, No. 3, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is one of the methods used for secret communication.
Steganography attempts to hide the existence of the information. The object
used to hide the secret information is called as cover object. Images are the
most popular cover objects used for steganography. Different techniques have to
be used for color image steganography and grey scale image steganography since
they are stored in different ways. Color image are normally stored with 24 bit
depth and grey scale images are stored with 8 bit depth. Color images can hold
large amount of secret information since they have three color components.
Different color spaces namely RGB (Red Green Blue), HSV (Hue, Saturation,
Value), YUV, YIQ, YCbCr (Luminance, Chrominance) etc. are used to represent
color images. Color image steganography can be done in any color space domain.
In this paper color image steganography in RGB and YCbCr domain are compared.
The secret information considered is grey scale image. Since RGB is the common
method of representation, hiding secret information in this format is not
secure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3033</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3033</id><created>2013-07-11</created><authors><author><keyname>Edelkamp</keyname><forenames>Stefan</forenames></author><author><keyname>Wei&#xdf;</keyname><forenames>Armin</forenames></author></authors><title>QuickXsort: Efficient Sorting with n log n - 1.399n +o(n) Comparisons on
  Average</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we generalize the idea of QuickHeapsort leading to the notion
of QuickXsort. Given some external sorting algorithm X, QuickXsort yields an
internal sorting algorithm if X satisfies certain natural conditions.
  With QuickWeakHeapsort and QuickMergesort we present two examples for the
QuickXsort-construction. Both are efficient algorithms that incur approximately
n log n - 1.26n +o(n) comparisons on the average. A worst case of n log n +
O(n) comparisons can be achieved without significantly affecting the average
case.
  Furthermore, we describe an implementation of MergeInsertion for small n.
Taking MergeInsertion as a base case for QuickMergesort, we establish a
worst-case efficient sorting algorithm calling for n log n - 1.3999n + o(n)
comparisons on average. QuickMergesort with constant size base cases shows the
best performance on practical inputs: when sorting integers it is slower by
only 15% to STL-Introsort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3040</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3040</id><created>2013-07-11</created><updated>2014-03-31</updated><authors><author><keyname>Bhatt</keyname><forenames>Mehul</forenames></author></authors><title>Between Sense and Sensibility: Declarative narrativisation of mental
  models as a basis and benchmark for visuo-spatial cognition and computation
  focussed collaborative cognitive systems</title><categories>cs.AI cs.CL cs.CV cs.HC cs.RO</categories><comments>5 pages, research statement summarising recent publications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What lies between `\emph{sensing}' and `\emph{sensibility}'? In other words,
what kind of cognitive processes mediate sensing capability, and the formation
of sensible impressions ---e.g., abstractions, analogies, hypotheses and theory
formation, beliefs and their revision, argument formation--- in domain-specific
problem solving, or in regular activities of everyday living, working and
simply going around in the environment? How can knowledge and reasoning about
such capabilities, as exhibited by humans in particular problem contexts, be
used as a model and benchmark for the development of collaborative cognitive
(interaction) systems concerned with human assistance, assurance, and
empowerment?
  We pose these questions in the context of a range of assistive technologies
concerned with \emph{visuo-spatial perception and cognition} tasks encompassing
aspects such as commonsense, creativity, and the application of specialist
domain knowledge and problem-solving thought processes. Assistive technologies
being considered include: (a) human activity interpretation; (b) high-level
cognitive rovotics; (c) people-centred creative design in domains such as
architecture &amp; digital media creation, and (d) qualitative analyses geographic
information systems. Computational narratives not only provide a rich cognitive
basis, but they also serve as a benchmark of functional performance in our
development of computational cognitive assistance systems. We posit that
computational narrativisation pertaining to space, actions, and change provides
a useful model of \emph{visual} and \emph{spatio-temporal thinking} within a
wide-range of problem-solving tasks and application areas where collaborative
cognitive systems could serve an assistive and empowering function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3043</identifier>
 <datestamp>2013-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3043</id><created>2013-07-11</created><updated>2013-09-13</updated><authors><author><keyname>Kosov</keyname><forenames>Sergey</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>Rottensteiner</keyname><forenames>Franz</forenames></author><author><keyname>Heipke</keyname><forenames>Christian</forenames></author></authors><title>A two-layer Conditional Random Field for the classification of partially
  occluded objects</title><categories>cs.CV</categories><comments>Conference Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional Random Fields (CRF) are among the most popular techniques for
image labelling because of their flexibility in modelling dependencies between
the labels and the image features. This paper proposes a novel CRF-framework
for image labeling problems which is capable to classify partially occluded
objects. Our approach is evaluated on aerial near-vertical images as well as on
urban street-view images and compared with another methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3046</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3046</id><created>2013-07-11</created><authors><author><keyname>Esheiba</keyname><forenames>Leila</forenames></author><author><keyname>Mokhtar</keyname><forenames>Hoda M. O.</forenames></author><author><keyname>El-Sharkawi</keyname><forenames>Mohamed</forenames></author></authors><title>Spatio-Temporal Queries for moving objects Data warehousing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, Moving Object Databases (MODs) have attracted a lot of
attention from researchers. Several research works were conducted to extend
traditional database techniques to accommodate the new requirements imposed by
the continuous change in location information of moving objects. Managing,
querying, storing, and mining moving objects were the key research directions.
This extensive interest in moving objects is a natural consequence of the
recent ubiquitous location-aware devices, such as PDAs, mobile phones, etc., as
well as the variety of information that can be extracted from such new
databases. In this paper we propose a Spatio-Temporal data warehousing (STDW)
for efficiently querying location information of moving objects. The proposed
schema introduces new measures like direction majority and other
direction-based measures that enhance the decision making based on location
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3047</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3047</id><created>2013-07-11</created><authors><author><keyname>Yildiz</keyname><forenames>Bahattin</forenames></author><author><keyname>Karadeniz</keyname><forenames>Suat</forenames></author></authors><title>Linear Codes over Z_4+uZ_4: MacWilliams identities, projections, and
  formally self-dual codes</title><categories>math.RA cs.IT math.IT</categories><comments>12 pages. Partially presented in the 13th International Workshop on
  Algebraic and combinatorial coding theory, Pomorie, Bulgaria, 2012</comments><msc-class>94B05, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes are considered over the ring Z_4+uZ_4, a non-chain extension of
Z_4. Lee weights, Gray maps for these codes are defined and MacWilliams
identities for the complete, symmetrized and Lee weight enumerators are proved.
Two projections from Z_4+uZ_4 to the rings Z_4 and F_2+uF_2 are considered and
self-dual codes over Z_4+uZ_4 are studied in connection with these projections.
Finally three constructions are given for formally self-dual codes over
Z_4+uZ_4 and their Z_4-images together with some good examples of formally
self-dual Z_4-codes obtained through these constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3051</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3051</id><created>2013-07-11</created><authors><author><keyname>Kaur</keyname><forenames>Ramneet</forenames></author><author><keyname>Singh</keyname><forenames>Balwinder</forenames></author></authors><title>Design and Implementation of Car Parking System on FPGA</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  As, the number of vehicles are increased day by day in rapid manner. It
causes the problem of traffic congestion, pollution (noise and air). To
overcome this problem A FPGA based parking system has been proposed. In this
paper, parking system is implemented using Finite State Machine modelling. The
system has two main modules i.e. identification module and slot checking
module. Identification module identifies the visitor. Slot checking module
checks the slot status. These modules are modeled in HDL and implemented on
FPGA. A prototype of parking system is designed with various interfaces like
sensor interfacing, stepper motor and LCD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3054</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3054</id><created>2013-07-11</created><authors><author><keyname>Nimkar</keyname><forenames>Sayali</forenames></author><author><keyname>Varghese</keyname><forenames>Sanal</forenames></author><author><keyname>Shrivastava</keyname><forenames>Sucheta</forenames></author></authors><title>Contrast Enhancement And Brightness Preservation Using Multi-
  Decomposition Histogram Equalization</title><categories>cs.CV</categories><comments>9 pages,13 figures</comments><journal-ref>SIPIJ, Vol.4, Issue.3, pp. 85-93</journal-ref><doi>10.5121/sipij.2013.4308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Histogram Equalization (HE) has been an essential addition to the Image
Enhancement world. Enhancement techniques like Classical Histogram Equalization
(CHE), Adaptive Histogram Equalization (ADHE), Bi-Histogram Equalization (BHE)
and Recursive Mean Separate Histogram Equalization (RMSHE) methods enhance
contrast, however, brightness is not well preserved with these methods, which
gives an unpleasant look to the final image obtained. Thus, we introduce a
novel technique Multi-Decomposition Histogram Equalization (MDHE) to eliminate
the drawbacks of the earlier methods. In MDHE, we have decomposed the input
sixty-four parts, applied CHE in each of the sub-images and then finally
interpolated them in correct order. The final image after MDHE results in
contrast enhanced and brightness preserved image compared to all other
techniques mentioned above. We have calculated the various parameters like
PSNR, SNR, RMSE, MSE, etc. for every technique. Our results are well supported
by bar graphs, histograms and the parameter calculations at the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3057</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3057</id><created>2013-07-11</created><authors><author><keyname>Pradhan</keyname><forenames>Chittaranjan</forenames></author><author><keyname>Bisoi</keyname><forenames>Ajay Kumar</forenames></author></authors><title>Chaotic Variations of AES Algorithm</title><categories>cs.CR</categories><comments>IJCCMS, 7 pages</comments><journal-ref>International Journal of Chaos, Control, Modelling and Simulation
  (IJCCMS) Vol.2, No.2, June 2013</journal-ref><doi>10.5121/ijccms.2013.2203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced Encryption Standard (AES) algorithm is considered as a secured
algorithm. Still, some security issues lie in the S-Box and the key used. In
this paper, we have tried to give focus on the security of the key used. Here,
the proposed modified algorithms for the AES have been simulated and tested
with different chaotic variations such as 1-D logistic chaos equation, cross
chaos equation as well as combination of both. For the evaluation purpose, the
CPU time has been taken as the parameter. Though the variations of AES
algorithms are taking some more time as compared to the standard AES algorithm,
still the variations can be taken into consideration in case of more sensitive
information. As we are giving more security to the key used for AES algorithm,
our proposed algorithms are very much secured from unauthorized people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3061</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3061</id><created>2013-07-11</created><authors><author><keyname>Sheta</keyname><forenames>Dr. Osama E.</forenames></author><author><keyname>Eldeen</keyname><forenames>Ahmed Nour</forenames></author></authors><title>The technology of using a data warehouse to support decision-making in
  health care</title><categories>cs.DB</categories><comments>12 pages</comments><doi>10.5121/ijdms.2013.5305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the technology of data warehouse in healthcare
decision-making and tools for support of these technologies, which is used to
cancer diseases. The healthcare executive managers and doctors needs
information about and insight into the existing health data, so as to make
decision more efficiently without interrupting the daily work of an On-Line
Transaction Processing (OLTP) system. This is a complex problem during the
healthcare decision-making process. To solve this problem, the building a
healthcare data warehouse seems to be efficient. First in this paper we explain
the concepts of the data warehouse, On-Line Analysis Processing (OLAP).
Changing the data in the data warehouse into a multidimensional data cube is
then shown. Finally, an application example is given to illustrate the use of
the healthcare data warehouse specific to cancer diseases developed in this
study. The executive managers and doctors can view data from more than one
perspective with reduced query time, thus making decisions faster and more
comprehensive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3073</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3073</id><created>2013-07-11</created><updated>2013-10-31</updated><authors><author><keyname>Guillemot</keyname><forenames>Sylvain</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Finding small patterns in permutations in linear time</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two permutations $\sigma$ and $\pi$, the \textsc{Permutation Pattern}
problem asks if $\sigma$ is a subpattern of $\pi$. We show that the problem can
be solved in time $2^{O(\ell^2\log \ell)}\cdot n$, where $\ell=|\sigma|$ and
$n=|\pi|$. In other words, the problem is fixed-parameter tractable
parameterized by the size of the subpattern to be found.
  We introduce a novel type of decompositions for permutations and a
corresponding width measure. We present a linear-time algorithm that either
finds $\sigma$ as a subpattern of $\pi$, or finds a decomposition of $\pi$
whose width is bounded by a function of $|\sigma|$. Then we show how to solve
the \textsc{Permutation Pattern} problem in linear time if a bounded-width
decomposition is given in the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3075</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3075</id><created>2013-07-11</created><updated>2014-12-08</updated><authors><author><keyname>Anurag</keyname></author><author><keyname>Singh</keyname><forenames>Gurmohan</forenames></author><author><keyname>Sulochana</keyname><forenames>V.</forenames></author></authors><title>Low Power Dual Edge-Triggered Static D Flip-Flop</title><categories>cs.OH</categories><comments>Dual-Edge Triggered, Flip-Flop, High Speed, Low Power, Static D
  Flip-Flop</comments><journal-ref>International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.4, No.3, June 2013</journal-ref><doi>10.5121/vlsic.2013.4303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper enumerates new architecture of low power dual-edge triggered
Flip-Flop (DETFF) designed at 180nm CMOS technology. In DETFF same data
throughput can be achieved with half of the clock frequency as compared to
single edge triggered Flip-Flop (SETFF). In this paper conventional and
proposed DETFF are presented and compared at same simulation conditions. The
post layout experimental results comparison shows that the average power
dissipation is improved by 48.17%, 41.29% and 36.84% when compared with SCDFF,
DEPFF and SEDNIFF respectively and improvement in PDP is 42.44%, 33.88% and
24.69% as compared to SCDFF, DEPFF and SEDNIFF respectively. Therefore the
proposed DETFF design is suitable for low power and small area applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3080</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3080</id><created>2013-07-11</created><authors><author><keyname>Kantor</keyname><forenames>Erez</forenames></author><author><keyname>Kutten</keyname><forenames>Shay</forenames></author></authors><title>Optimal competitiveness for Symmetric Rectilinear Steiner Arborescence
  and related problems</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present optimal competitive algorithms for two interrelated known problems
involving Steiner Arborescence. One is the continuous problem of the Symmetric
Rectilinear Steiner Arborescence (SRSA), studied by Berman and Coulston.
  A very related, but discrete problem (studied separately in the past) is the
online Multimedia Content Delivery (MCD) problem on line networks, presented
originally by Papadimitriu, Ramanathan, and Rangan. An efficient content
delivery was modeled as a low cost Steiner arborescence in a grid of
network*time they defined. We study here the version studied by Charikar,
Halperin, and Motwani (who used the same problem definitions, but removed some
constraints on the inputs).
  The bounds on the competitive ratios introduced separately in the above
papers are similar for the two problems: O(log N) for the continuous problem
and O(log n) for the network problem, where N was the number of terminals to
serve, and n was the size of the network. The lower bounds were Omega(sqrt{log
N}) and Omega(sqrt{log n}) correspondingly. Berman and Coulston conjectured
that both the upper bound and the lower bound could be improved.
  We disprove this conjecture and close these quadratic gaps for both problems.
We first present an O(sqrt{log n}) deterministic competitive algorithm for MCD
on the line, matching the lower bound. We then translate this algorithm to
become a competitive optimal algorithm O(sqrt{log N}) for SRSA. Finally, we
translate the latter back to solve MCD problem, this time competitive optimally
even in the case that the number of requests is small (that is, O(min{sqrt{log
n},sqrt{log N}})). We also present a Omega(sqrt[3]{log n}) lower bound on the
competitiveness of any randomized algorithm. Some of the techniques may be
useful in other contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3086</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3086</id><created>2013-07-11</created><authors><author><keyname>Ghenai</keyname><forenames>Afifa</forenames></author><author><keyname>Badaoui</keyname><forenames>Mohamed Youcef</forenames></author><author><keyname>Benmohammed</keyname><forenames>Mohamed</forenames></author></authors><title>Towards a Good ABS Design for more Reliable Vehicles on the Roads</title><categories>cs.SE</categories><comments>14 pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 5, No 3, June 2013</journal-ref><doi>10.5121/ijcsit.2013.5310</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, better driving also means better braking. To this end, vehicle
designers must find all failures during the design phase of antilock braking
systems which play an important role in automobiles safety. However,
mechatronic systems are so complex and failures can be badly identified. So it
is necessary to propose a design approach of an antilock braking system which
will be able to avoid wheels locking during braking and maintain vehicle
stability. This paper describes this approach, in which we model the functional
and the dysfunctional behavior of an antilock braking system using stopwatch
Petri nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3088</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3088</id><created>2013-07-11</created><authors><author><keyname>Murray-Rust</keyname><forenames>Dave</forenames></author><author><keyname>Murray-Rust</keyname><forenames>Peter</forenames></author></authors><title>The Declaratron, semantic specification for scientific computation using
  MathML</title><categories>cs.MS</categories><comments>Conference on Intelligent Computer Mathematics WiP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Declaratron, a system which takes a declarative approach to
specifying mathematically based scientific computation. This uses displayable
mathematical notation (Content MathML) and is both executable and semantically
well defined. We combine domain specific representations of physical science
(e.g. CML, Chemical Markup Language), MathML formulae and computational
specifications (DeXML) to create executable documents which include scientific
data and mathematical formulae. These documents preserve the provenance of the
data used, and build tight semantic links between components of mathematical
formulae and domain objects---in effect grounding the mathematical semantics in
the scientific domain. The Declaratron takes these specifications and i)
carries out entity resolution and decoration to prepare for computation ii)
uses a MathML execution engine to run calculations over the revised tree iii)
outputs domain objects and the complete document to give both results and an
encapsulated history of the computation. A short description of a case study is
given to illustrate how the system can be used. Many scientific problems
require frequent change of the mathematical functional form and the Declaratron
provides this without requiring changes to code. Additionally, it supports
reproducible science, machine indexing and semantic search of computations,
makes implicit assumptions visible, and separates domain knowledge from
computational techniques. We believe that the Declaratron could replace much
conventional procedural code in science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3091</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3091</id><created>2013-07-11</created><authors><author><keyname>Marietto</keyname><forenames>Maria das Gra&#xe7;as Bruno</forenames></author><author><keyname>de Aguiar</keyname><forenames>Rafael Varago</forenames></author><author><keyname>Barbosa</keyname><forenames>Gislene de Oliveira</forenames></author><author><keyname>Botelho</keyname><forenames>Wagner Tanaka</forenames></author><author><keyname>Pimentel</keyname><forenames>Edson</forenames></author><author><keyname>Fran&#xe7;a</keyname><forenames>Robson dos Santos</forenames></author><author><keyname>da Silva</keyname><forenames>Vera L&#xfa;cia</forenames></author></authors><title>Artificial Intelligence MArkup Language: A Brief Tutorial</title><categories>cs.AI cs.SE</categories><comments>International Journal of Computer science and engineering Survey
  (IJCSES) - 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to serve as a reference guide for the
development of chatterbots implemented with the AIML language. In order to
achieve this, the main concepts in Pattern Recognition area are described
because the AIML uses such theoretical framework in their syntactic and
semantic structures. After that, AIML language is described and each AIML
command/tag is followed by an application example. Also, the usage of AIML
embedded tags for the handling of sequence dialogue limitations between humans
and machines is shown. Finally, computer systems that assist in the design of
chatterbots with the AIML language are classified and described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3095</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3095</id><created>2013-07-11</created><authors><author><keyname>Holtkamp</keyname><forenames>Hauke</forenames></author><author><keyname>Auer</keyname><forenames>Gunther</forenames></author></authors><title>Fundamental Limits of Energy-Efficient Resource Sharing, Power Control
  and Discontinuous Transmission</title><categories>cs.IT math.IT</categories><comments>12 pages, ISBN 978-1-4577-0928-9. In Future Network &amp; Mobile Summit
  (FutureNetw), 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The achievable gains via power-optimal scheduling are investigated. Under the
QoS constraint of a guaranteed link rate, the overall power consumed by a
cellular BS is minimized. Available alternatives for the minimization of
transmit power consumption are presented. The transmit power is derived for the
two-user downlink situation. The analysis is extended to incorporate a BS power
model (which maps transmit power to supply power consumption) and the use of
DTX in a BS. Overall potential gains are evaluated by comparison of a
conventional SOTA BS with one that employs DTX exclusively, a power control
scheme and an optimal combined DTX and power control scheme. Fundamental limits
of the achievable savings are found to be at 5.5 dB under low load and 2 dB
under high load when comparing the SOTA consumption with optimal allocation
under the chosen power model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3099</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3099</id><created>2013-07-11</created><authors><author><keyname>Holtkamp</keyname><forenames>Hauke</forenames></author><author><keyname>Auer</keyname><forenames>Gunther</forenames></author><author><keyname>Haas</keyname><forenames>Harald</forenames></author></authors><title>Minimal average consumption downlink base station power control strategy</title><categories>cs.IT math.IT</categories><comments>13 pages. Personal Indoor and Mobile Radio Communications (PIMRC),
  2011 IEEE 22nd International Symposium on</comments><doi>10.1109/PIMRC.2011.6139957</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider single cell multi-user OFDMA downlink resource allocation on a
flat-fading channel such that average supply power is minimized while
fulfilling a set of target rates. Available degrees of freedom are transmission
power and duration. This paper extends our previous work on power optimal
resource allocation in the mobile downlink by detailing the optimal power
control strategy investigation and extracting fundamental characteristics of
power optimal operation in cellular downlink. We find that only a system wide
allocation of transmit powers is optimal rather than on link level. The
allocation strategy that minimizes overall power consumption requires the
transmission power on all links to be increased if only one link degrades.
Furthermore, we show that for mobile stations with equal channels but different
rate requirements, it is power optimal to assign equal transmit powers with
proportional transmit durations. To relate the effectiveness of power control
to live operation, we take the power model into consideration which maps
transmit power to supply power. We show that due to the affine mapping, the
solution is independent of the power model. However, the effectiveness of power
control measures is completely dependent on the underlying hardware and the
load dependence factor of a base station (instead of absolute consumption
values). Finally, we conclude that power control measures in base stations are
most relevant in macro stations which have load dependence factor of more than
50%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3102</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3102</id><created>2013-07-11</created><updated>2014-11-05</updated><authors><author><keyname>Balcan</keyname><forenames>Maria Florina</forenames></author><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author></authors><title>Statistical Active Learning Algorithms for Noise Tolerance and
  Differential Privacy</title><categories>cs.LG cs.DS stat.ML</categories><comments>Extended abstract appears in NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a framework for designing efficient active learning algorithms
that are tolerant to random classification noise and are
differentially-private. The framework is based on active learning algorithms
that are statistical in the sense that they rely on estimates of expectations
of functions of filtered random examples. It builds on the powerful statistical
query framework of Kearns (1993).
  We show that any efficient active statistical learning algorithm can be
automatically converted to an efficient active learning algorithm which is
tolerant to random classification noise as well as other forms of
&quot;uncorrelated&quot; noise. The complexity of the resulting algorithms has
information-theoretically optimal quadratic dependence on $1/(1-2\eta)$, where
$\eta$ is the noise rate.
  We show that commonly studied concept classes including thresholds,
rectangles, and linear separators can be efficiently actively learned in our
framework. These results combined with our generic conversion lead to the first
computationally-efficient algorithms for actively learning some of these
concept classes in the presence of random classification noise that provide
exponential improvement in the dependence on the error $\epsilon$ over their
passive counterparts. In addition, we show that our algorithms can be
automatically converted to efficient active differentially-private algorithms.
This leads to the first differentially-private active learning algorithms with
exponential label savings over the passive case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3103</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3103</id><created>2013-07-11</created><authors><author><keyname>Holtkamp</keyname><forenames>Hauke</forenames></author><author><keyname>Auer</keyname><forenames>Gunther</forenames></author><author><keyname>Haas</keyname><forenames>Harald</forenames></author></authors><title>On Minimizing Base Station Power Consumption</title><categories>cs.IT math.IT</categories><comments>13 pages. Vehicular Technology Conference (VTC Fall), 2011 IEEE</comments><doi>10.1109/VETECF.2011.6093054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider resource allocation over a wireless downlink where Base Station
(BS) power consumption is minimized while upholding a set of required link
rates. A Power and Resource Allocation Including Sleep (PRAIS) method is
proposed that combines resource sharing, Power Control (PC), and Discontinuous
Transmission (DTX), such that downlink power consumption is minimized, which
can be formed into a convex optimization problem. Unlike conventional
approaches that aim at minimizing transmit power, in this work the BS mains
supply power is chosen as the relevant metric. Based on a linear power model,
which maps a certain transmit power to the necessary mains supply power, we
quantify the fundamental limits of PRAIS in terms of achievable BS power
savings. The fundamental limits are numerically evaluated on link level for
four sets of BS power model parameters representative of envisaged future
hardware developments. We establish an expected lower limit for PRAIS of 27W to
68W depending on load per link for BSs installed in 2014, which provides a 61%
to 34% gain over conventional resource allocation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3107</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3107</id><created>2013-07-11</created><updated>2013-07-24</updated><authors><author><keyname>Geil</keyname><forenames>Olav</forenames></author><author><keyname>Martin</keyname><forenames>Stefano</forenames></author></authors><title>An improvement of the Feng-Rao bound for primary codes</title><categories>cs.IT math.AC math.IT</categories><comments>30 pages, 7 figures</comments><msc-class>94B65, 94B27, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new bound for the minimum distance of a general primary linear
code. For affine variety codes defined from generalised C_{ab} curves the new
bound often improves dramatically on the Feng-Rao bound for primary codes. The
method does not only work for the minimum distance but can be applied to any
generalised Hamming weight
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3110</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3110</id><created>2013-07-11</created><authors><author><keyname>Holtkamp</keyname><forenames>Hauke</forenames></author><author><keyname>Auer</keyname><forenames>Gunther</forenames></author><author><keyname>Bazzi</keyname><forenames>Samer</forenames></author><author><keyname>Haas</keyname><forenames>Harald</forenames></author></authors><title>Minimizing Base Station Power Consumption</title><categories>cs.IT math.IT</categories><comments>27 pages</comments><journal-ref>Selected Areas in Communications, IEEE Journal on (Volume:PP ,
  Issue: 99 ), 2014</journal-ref><doi>10.1109/JSAC.2014.141210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new radio resource management algorithm which aims at minimizing
the base station supply power consumption for multi-user MIMO-OFDM. Given a
base station power model that establishes a relation between the RF transmit
power and the supply power consumption, the algorithm optimizes the trade-off
between three basic power-saving mechanisms: antenna adaptation, power control
and discontinuous transmission. The algorithm comprises two steps: a) the first
step estimates sleep mode duration, resource shares and antenna configuration
based on average channel conditions and b) the second step exploits
instantaneous channel knowledge at the transmitter for frequency selective
time-variant channels. The proposed algorithm finds the number of transmit
antennas, the RF transmission power per resource unit and spatial channel, the
number of discontinuous transmission time slots, and the multi-user resource
allocation, such that supply power consumption is minimized. Simulation results
indicate that the proposed algorithm is capable of reducing the supply power
consumption by between 25% and 40%, dependend on the system load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3113</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3113</id><created>2013-07-11</created><authors><author><keyname>Graham</keyname><forenames>Ronald</forenames></author><author><keyname>Hamilton</keyname><forenames>Linus</forenames></author><author><keyname>Levavi</keyname><forenames>Ariel</forenames></author><author><keyname>Loh</keyname><forenames>Po-Shen</forenames></author></authors><title>Anarchy is free in network creation</title><categories>math.CO cs.GT</categories><comments>11 pages</comments><msc-class>05C35, 91A06, 68M10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet has emerged as perhaps the most important network in modern
computing, but rather miraculously, it was created through the individual
actions of a multitude of agents rather than by a central planning authority.
This motivates the game theoretic study of network formation, and our paper
considers one of the most-well studied models, originally proposed by Fabrikant
et al. In it, each of N agents corresponds to a vertex, which can create edges
to other vertices at a cost of alpha each, for some parameter alpha. Every edge
can be freely used by every vertex, regardless of who paid the creation cost.
To reflect the desire to be close to other vertices, each agent's cost function
is further augmented by the sum total of all (graph theoretic) distances to all
other vertices.
  Previous research proved that for many regimes of the (alpha, N) parameter
space, the total social cost (sum of all agents' costs) of every Nash
equilibrium is bounded by at most a constant multiple of the optimal social
cost. In algorithmic game theoretic nomenclature, this approximation ratio is
called the price of anarchy. In our paper, we significantly sharpen some of
those results, proving that for all constant non-integral alpha &gt; 2, the price
of anarchy is in fact 1+o(1), i.e., not only is it bounded by a constant, but
it tends to 1 as N tends to infinity. For constant integral alpha &gt;= 2, we show
that the price of anarchy is bounded away from 1. We provide quantitative
estimates on the rates of convergence for both results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3121</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3121</id><created>2013-07-11</created><updated>2014-02-12</updated><authors><author><keyname>Dartmann</keyname><forenames>Guido</forenames></author><author><keyname>Zandi</keyname><forenames>Ehsan</forenames></author><author><keyname>Ascheid</keyname><forenames>Gerd</forenames></author></authors><title>A Modified Levenberg-Marquardt Method for the Bidirectional Relay
  Channel</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Vehicular Technology We corrected
  small mistakes in the proof of Lemma 2 and Proposition 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an optimization approach for a system consisting of
multiple bidirectional links over a two-way amplify-and-forward relay. It is
desired to improve the fairness of the system. All user pairs exchange
information over one relay station with multiple antennas. Due to the joint
transmission to all users, the users are subject to mutual interference. A
mitigation of the interference can be achieved by max-min fair precoding
optimization where the relay is subject to a sum power constraint. The
resulting optimization problem is non-convex. This paper proposes a novel
iterative and low complexity approach based on a modified Levenberg-Marquardt
method to find near optimal solutions. The presented method finds solutions
close to the standard convex-solver based relaxation approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3125</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3125</id><created>2013-07-11</created><authors><author><keyname>Harrington</keyname><forenames>Patrick L</forenames><suffix>Jr</suffix></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Information Theoretic Adaptive Tracking of Epidemics in Complex Networks</title><categories>physics.soc-ph cs.SI</categories><comments>arXiv admin note: substantial text overlap with arXiv:0905.2236</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptively monitoring the states of nodes in a large complex network is of
interest in domains such as national security, public health, and energy grid
management. Here, we present an information theoretic adaptive tracking and
sampling framework that recursively selects measurements using the feedback
from performing inference on a dynamic Bayesian Network. We also present
conditions for the existence of a network specific, observation dependent,
phase transition in the updated posterior of hidden node states resulting from
actively monitoring the network. Since traditional epidemic thresholds are
derived using observation independent Markov chains, the threshold of the
posterior should more accurately model the true phase transition of a network.
The adaptive tracking framework and epidemic threshold should provide insight
into modeling the dynamic response of the updated posterior to active
intervention and control policies while monitoring modern complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3136</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3136</id><created>2013-07-11</created><authors><author><keyname>Elices</keyname><forenames>Juan A.</forenames></author><author><keyname>Perez-Gonzalez</keyname><forenames>Fernando</forenames></author></authors><title>Linking Correlated Network Flows through Packet Timing: a Game-Theoretic
  Approach</title><categories>cs.CR cs.GT</categories><comments>Global SIP 2013: Cyber-Security and Privacy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding that two network flows are essentially the same is an important
problem in intrusion detection or in tracing anonymous connections. A stepping
stone or an anonymity network may try to prevent flow correlation by delaying
the packets, introducing chaff traffic, or even splitting the flow in several
subflows.
  We introduce a game-theoretic framework for this problem. The framework is
used to derive the Nash equilibrium under two different adversary models: the
first one, when the adversary is limited to delaying packets, and the second,
when the adversary also adds dummy packets and removes packets from the flow.
As the optimal decoder is not computationally feasible, we restrict the
possible decoder to one that estimates and compensates the attack. Our analysis
can be used for understanding the limits of flow correlation based on packet
timings under an active attacker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3142</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3142</id><created>2013-07-11</created><updated>2013-11-05</updated><authors><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Mladen</forenames></author><author><keyname>Vukobratovi&#x107;</keyname><forenames>Dejan</forenames></author></authors><title>Perfect Codes in the Discrete Simplex</title><categories>cs.IT cs.DM math.IT</categories><comments>15 pages (single-column), 5 figures. Minor revisions made. Accepted
  for publication in Designs, Codes and Cryptography</comments><msc-class>94B25, 05B40, 52C17, 05C12, 68R99</msc-class><journal-ref>Des. Codes Cryptogr., vol. 75, no. 1, pp. 81-95, Apr. 2015</journal-ref><doi>10.1007/s10623-013-9893-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of existence of (nontrivial) perfect codes in the
discrete $ n $-simplex $ \Delta_{\ell}^n := \left\{ \begin{pmatrix} x_0,
\ldots, x_n \end{pmatrix} : x_i \in \mathbb{Z}_{+}, \sum_i x_i = \ell \right\}
$ under $ \ell_1 $ metric. The problem is motivated by the so-called multiset
codes, which have recently been introduced by the authors as appropriate
constructs for error correction in the permutation channels. It is shown that $
e $-perfect codes in the $ 1 $-simplex $ \Delta_{\ell}^1 $ exist for any $ \ell
\geq 2e + 1 $, the $ 2 $-simplex $ \Delta_{\ell}^2 $ admits an $ e $-perfect
code if and only if $ \ell = 3e + 1 $, while there are no perfect codes in
higher-dimensional simplices. In other words, perfect multiset codes exist only
over binary and ternary alphabets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3144</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3144</id><created>2013-07-11</created><authors><author><keyname>Sahoo</keyname><forenames>Biswapratapsingh</forenames></author></authors><title>Performance Comparison of Packet Scheduling Algorithms for Video Traffic
  in LTE Cellular Network</title><categories>cs.NI</categories><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics ( IJMNCT) Vol. 3, No.3, PP 09-18, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have studied downlink packet scheduling algorithms proposed
for LTE cellular networks. The study emphasize on three most promising
scheduling algorithms such as: FLS, EXP rule and LOG rule. The performance of
these three algorithms is conducted over video traffic in a vehicular
environment using LTE-Sim simulator. The simulation was setup with varying
number of users from 10 - 60 in fixed bounded regions of 1 km radius. The main
goal this study is to provide results that will help in the design process of
packet scheduler for LTE cellular networks, aiming to get better overall
performance users. Simulation results show that, the FLS scheme outperforms in
terms of average system throughput, average packet delay, PLR; and with a
satisfactory level of fairness index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3147</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3147</id><created>2013-07-11</created><updated>2014-07-08</updated><authors><author><keyname>Sahoo</keyname><forenames>B. P. S.</forenames></author><author><keyname>Rath</keyname><forenames>Satyajit</forenames></author></authors><title>Integrating GPS, GSM and Cellular Phone for Location Tracking and
  Monitoring</title><categories>cs.NI</categories><comments>Proceedings of International Conference on Geospatial Technologies
  and Applications, February 2012, IIT Bombay, Mumbai, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wide spread of mobiles as handheld devices leads to various innovative
applications that makes use of their ever increasing presence in our daily
life. One such application is location tracking and monitoring. This paper
proposes a prototype model for location tracking using Geographical Positioning
System (GPS) and Global System for Mobile Communication (GSM) technology. The
system displays the object moving path on the monitor and the same information
can also be communicated to the user cell phone, on demand of the user by
asking the specific information via SMS. This system is very useful for car
theft situations, for adolescent drivers being watched and monitored by
parents. The result shows that the object is being tracked with a minimal
tracking error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3156</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3156</id><created>2013-07-11</created><authors><author><keyname>Fedrizzi</keyname><forenames>Riccardo</forenames></author><author><keyname>Rasheed</keyname><forenames>Tinku</forenames></author></authors><title>Cooperative Short Range Routing for Energy Savings in Multi-Interface
  Wireless Networks</title><categories>cs.NI</categories><comments>Accepted at IEEE VTC-Spring Conference 2013, Dresden, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency in wireless networks has become an important field of
research due to ever increasing energy expenditure in battery supplied mobile
terminals. In this paper we present an energy efficient routing scheme for
multi-standard infrastructure wireless networks based on multi-hop cooperative
relaying. The aim of the proposed technique is to exploit short-range
cooperation to take benefit from mobile terminals having superior links thus
enable energy efficiency. Performance results show that higher data-rate
yielded by cooperation can compensate the expense of higher energy due to
multiple interfaces active on the same mobile terminal, making possible to
observe energy efficiency gain of the system. A maximum achievable energy
efficiency gain of up to 42 % was observed in our simulations when using the
cooperative short range routing technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3158</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3158</id><created>2013-07-11</created><authors><author><keyname>Valcarce</keyname><forenames>Alvaro</forenames></author><author><keyname>Rasheed</keyname><forenames>Tinku</forenames></author><author><keyname>Gomez</keyname><forenames>Karina</forenames></author><author><keyname>Kandeepan</keyname><forenames>Sithamparanathan</forenames></author><author><keyname>Reynaud</keyname><forenames>Laurent</forenames></author><author><keyname>Hermenier</keyname><forenames>Romain</forenames></author><author><keyname>Munari</keyname><forenames>Andrea</forenames></author><author><keyname>Mohorcic</keyname><forenames>Mihael</forenames></author><author><keyname>Smolnikar</keyname><forenames>Miha</forenames></author><author><keyname>Bucaille</keyname><forenames>Isabelle</forenames></author></authors><title>Airborne Base Stations for Emergency and Temporary Events</title><categories>cs.NI</categories><comments>Accepted at 5th EAI PSATS Conference, June 27-28, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a rapidly deployable wireless network based on Low
Altitude Platforms and portable land units to support disaster-relief
activities, and to extend capacity during temporary mass events. The system
integrates an amalgam of radio technologies such as LTE, WLAN and TETRA to
provide heterogeneous communications in the deployment location. Cognitive
radio is used for autonomous network con?guration. Sensor networks monitor the
environment in real-time during relief activities and provide distributed
spectrum sensing capacities. Finally, remote communications are supported via
S-band satellite links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3164</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3164</id><created>2013-07-11</created><authors><author><keyname>Coyle</keyname><forenames>David</forenames></author><author><keyname>Doherty</keyname><forenames>Gavin</forenames></author></authors><title>Supporting Therapeutic Relationships and Communication about Mental
  Health</title><categories>cs.HC</categories><comments>4 pages. Presented at the ACM CHI 2013 workshop on Patient-Clinician
  Communication</comments><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective communication and strong therapeutic relationships are critical to
successful mental health interventions. For example, in 1957 Carl Rogers, a
pioneer of person-centred therapy, proposed that an empowering relationship
could, in and of itself, create the necessary and sufficient conditions for
positive therapeutic outcomes [1]. Whilst modern psychological theories no
longer favour an exclusive focus on relationships, positive relationships and
the dynamics of client-therapist communication remain cornerstones of mental
health intervention theories. A more recent meta-review concluded that across
all interventions models, irrespective of the theoretical approach, the quality
of the relationship between therapists and clients is the second leading
determinant of successful clinical outcomes [2]. Over the past ten years we
(David Coyle and Gavin Doherty) have designed and evaluated a wide range to
systems that provide support for psychological (or talk- based) mental health
interventions [3]. Here we briefly consider two recent examples. In each case
our aim was to enhance communication and reshape clinical practice in a manner
that empowers patients. gNats Island is a computer game that supports
face-to-face interventions for adolescents [4]. MindBalance is an online
treatment programme for adults experiencing difficulties with depression [5].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3166</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3166</id><created>2013-07-11</created><authors><author><keyname>Coyle</keyname><forenames>David</forenames></author></authors><title>Replicating and Applying a Neuro-Cognitive Experimental Technique in HCI
  Research</title><categories>cs.HC</categories><comments>4 pages, presented at RepliCHI2013 at ACM CHI 2013</comments><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive neuroscience the sense of agency is defined as the as the
experience of controlling ones own actions and, through this control, affecting
the external world. At CHI 2012 I presented a paper entitled I did that!
Measuring Users Experience of Agency in their own Actions [1]. This extended
abstract draws heavily on that paper, which described an implicit measure
called intentional binding. This measure, developed by researchers in cognitive
neuroscience, has been shown to provide a robust implicit measure for the sense
of agency. My interest in intentional binding stemmed from prior HCI
literature, (e.g. the work of Shneiderman) which emphasises the importance of
the sense of control in human-computer interactions. The key question behind
the CHI 2012 paper was: can we apply intention binding to provide an implicit
measure for the experience of control in human-computer interactions? In
investigating this question, replication was a key element of the experimental
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3174</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3174</id><created>2013-07-11</created><authors><author><keyname>Coyle</keyname><forenames>David</forenames></author><author><keyname>Matthews</keyname><forenames>Mark</forenames></author><author><keyname>Doherty</keyname><forenames>Gavin</forenames></author><author><keyname>Sharry</keyname><forenames>John</forenames></author></authors><title>Engaging with mental health: a global challenge</title><categories>cs.HC cs.CY</categories><comments>4 pages, published at the Workshop on Interactive Systems in
  Healthcare 2010 at ACM CHI 2010</comments><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the metrics of the World Health Organisation, the Global Burden of
Disease Study has found that mental health difficulties are currently the
leading cause of disability in developed countries [1]. Projections also
indicate that the global burden of mental health difficulties will continue to
rise in the coming decades. The human and economic costs of this trend will be
substantial. In this paper we discuss how effectively designed interactive
systems, developed through collaborative, interdisciplinary efforts, can play a
significant role in helping to address this challenge. Our discussion is
grounded in a description of four exploratory systems, each of which has
undergone initial clinical evaluations. Directions for future research on
mental health technologies are also identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3176</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3176</id><created>2013-07-11</created><updated>2014-11-20</updated><authors><author><keyname>Korda</keyname><forenames>Nathaniel</forenames></author><author><keyname>A.</keyname><forenames>Prashanth L.</forenames></author><author><keyname>Munos</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Fast gradient descent for drifting least squares regression, with
  application to bandits</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning algorithms require to often recompute least squares
regression estimates of parameters. We study improving the computational
complexity of such algorithms by using stochastic gradient descent (SGD) type
schemes in place of classic regression solvers. We show that SGD schemes
efficiently track the true solutions of the regression problems, even in the
presence of a drift. This finding coupled with an $O(d)$ improvement in
complexity, where $d$ is the dimension of the data, make them attractive for
implementation in the big data settings. In the case when strong convexity in
the regression problem is guaranteed, we provide bounds on the error both in
expectation and high probability (the latter is often needed to provide
theoretical guarantees for higher level algorithms), despite the drifting least
squares solution. As an example of this case we prove that the regret
performance of an SGD version of the PEGE linear bandit algorithm
[Rusmevichientong and Tsitsiklis 2010] is worse that that of PEGE itself only
by a factor of $O(\log^4 n)$. When strong convexity of the regression problem
cannot be guaranteed, we investigate using an adaptive regularisation. We make
an empirical study of an adaptively regularised, SGD version of LinUCB [Li et
al. 2010] in a news article recommendation application, which uses the large
scale news recommendation dataset from Yahoo! front page. These experiments
show a large gain in computational complexity, with a consistently low tracking
error and click-through-rate (CTR) performance that is $75\%$ close.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3181</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3181</id><created>2013-07-11</created><authors><author><keyname>Zhong</keyname><forenames>Siyang</forenames></author><author><keyname>Huang</keyname><forenames>Xun</forenames></author></authors><title>Compressive sensing based beamforming for noisy measurements</title><categories>cs.IT math.IT</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing is the newly emerging method in information technology
that could impact array beamforming and the associated engineering
applications. However, practical measurements are inevitably polluted by noise
from external interference and internal acquisition process. Then, compressive
sensing based beamforming was studied in this work for those noisy measurements
with a signal-to-noise ratio. In this article, we firstly introduced the
fundamentals of compressive sensing theory. After that, we implemented two
algorithms (CSB-I and CSB-II). Both algorithms are proposed for those
presumably spatially sparse and incoherent signals. The two algorithms were
examined using a simple simulation case and a practical aeroacoustic test case.
The simulation case clearly shows that the CSB-I algorithm is quite sensitive
to the sensing noise. The CSB-II algorithm, on the other hand, is more robust
to noisy measurements. The results by CSB-II at $\mathrm{SNR}=-10\,$dB are
still reasonable with good resolution and sidelobe rejection. Therefore,
compressive sensing beamforming can be considered as a promising array signal
beamforming method for those measurements with inevitably noisy interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3184</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3184</id><created>2013-07-11</created><updated>2013-10-14</updated><authors><author><keyname>Epstein</keyname><forenames>Samuel</forenames></author></authors><title>Randomness Conservation over Algorithms</title><categories>cs.CC</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Current discrete randomness and information conservation inequalities are
over total recursive functions, i.e. restricted to deterministic processing.
This restriction implies that an algorithm can break algorithmic randomness
conservation inequalities. We address this issue by proving tight bounds of
randomness and information conservation with respect to recursively enumerable
transformations, i.e. processing by algorithms. We also show conservation of
randomness of finite strings with respect to enumerable distributions, i.e.
semicomputable semi-measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3185</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3185</id><created>2013-07-11</created><authors><author><keyname>Zhu</keyname><forenames>Yu-Xiao</forenames></author><author><keyname>Huang</keyname><forenames>Junming</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Zhang</keyname><forenames>Qian-Ming</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author></authors><title>Geography and similarity of regional cuisines in China</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>13 pages, 11 figures and 2 tables</comments><journal-ref>PLoS ONE 8(11): e79161, 2013</journal-ref><doi>10.1371/journal.pone.0079161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Food occupies a central position in every culture and it is therefore of
great interest to understand the evolution of food culture. The advent of the
World Wide Web and online recipe repositories has begun to provide
unprecedented opportunities for data-driven, quantitative study of food
culture. Here we harness an online database documenting recipes from various
Chinese regional cuisines and investigate the similarity of regional cuisines
in terms of geography and climate. We found that the geographical proximity,
rather than climate proximity is a crucial factor that determines the
similarity of regional cuisines. We develop a model of regional cuisine
evolution that provides helpful clues to understand the evolution of cuisines
and cultures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3189</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3189</id><created>2013-07-11</created><authors><author><keyname>Kogtenkov</keyname><forenames>Alexander</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author><author><keyname>Velder</keyname><forenames>Sergey</forenames></author></authors><title>Alias and Change Calculi, Applied to Frame Inference</title><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alias analysis, which determines whether two expressions in a program may
reference to the same object, has many potential applications in program
construction and verification. We have developed a theory for alias analysis,
the &quot;alias calculus&quot;, implemented its application to an object-oriented
language, and integrated the result into a modern IDE. The calculus has a
higher level of precision than many existing alias analysis techniques.
  One of the principal applications is to allow automatic change analysis,
which leads to inferring &quot;modifies clauses&quot;, providing a significant advance
towards addressing the Frame Problem. Experiments were able to infer the
&quot;modifies&quot; clauses of an existing formally specified library. Other
applications, in particular to concurrent programming, also appear possible.
  The article presents the calculus, the application to frame analysis
including ex-perimental results, and other projected applications. The ongoing
work includes building more efficient model capturing aliasing properties and
soundness proof for its essential elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3192</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3192</id><created>2013-07-11</created><authors><author><keyname>G&#xf6;bel</keyname><forenames>Oliver</forenames></author><author><keyname>Hoefer</keyname><forenames>Martin</forenames></author><author><keyname>Kesselheim</keyname><forenames>Thomas</forenames></author><author><keyname>Schleiden</keyname><forenames>Thomas</forenames></author><author><keyname>V&#xf6;cking</keyname><forenames>Berthold</forenames></author></authors><title>Online Independent Set Beyond the Worst-Case: Secretaries, Prophets, and
  Periods</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate online algorithms for maximum (weight) independent set on
graph classes with bounded inductive independence number like, e.g., interval
and disk graphs with applications to, e.g., task scheduling and spectrum
allocation. In the online setting, it is assumed that nodes of an unknown graph
arrive one by one over time. An online algorithm has to decide whether an
arriving node should be included into the independent set. Unfortunately, this
natural and practically relevant online problem cannot be studied in a
meaningful way within a classical competitive analysis as the competitive ratio
on worst-case input sequences is lower bounded by $\Omega(n)$.
  As a worst-case analysis is pointless, we study online independent set in a
stochastic analysis. Instead of focussing on a particular stochastic input
model, we present a generic sampling approach that enables us to devise online
algorithms achieving performance guarantees for a variety of input models. In
particular, our analysis covers stochastic input models like the secretary
model, in which an adversarial graph is presented in random order, and the
prophet-inequality model, in which a randomly generated graph is presented in
adversarial order. Our sampling approach bridges thus between stochastic input
models of quite different nature. In addition, we show that our approach can be
applied to a practically motivated admission control setting.
  Our sampling approach yields an online algorithm for maximum independent set
with competitive ratio $O(\rho^2)$ with respect to all of the mentioned
stochastic input models. for graph classes with inductive independence number
$\rho$. The approach can be extended towards maximum-weight independent set by
losing only a factor of $O(\log n)$ in the competitive ratio with $n$ denoting
the (expected) number of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3195</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3195</id><created>2013-07-11</created><authors><author><keyname>Aversa</keyname><forenames>Davide</forenames></author><author><keyname>Vassos</keyname><forenames>Stavros</forenames></author></authors><title>Action-based Character AI in Video-games with CogBots Architecture: A
  Preliminary Report</title><categories>cs.AI cs.SE</categories><comments>7 pages, for associated code repositories see
  https://github.com/THeK3nger/gridworld and
  https://github.com/THeK3nger/unity-cogbot</comments><acm-class>H.5.1; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an architecture for specifying the interaction of
non-player characters (NPCs) in the game-world in a way that abstracts common
tasks in four main conceptual components, namely perception, deliberation,
control, action. We argue that this architecture, inspired by AI research on
autonomous agents and robots, can offer a number of benefits in the form of
abstraction, modularity, re-usability and higher degrees of personalization for
the behavior of each NPC. We also show how this architecture can be used to
tackle a simple scenario related to the navigation of NPCs under incomplete
information about the obstacles that may obstruct the various way-points in the
game, in a simple and effective way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3203</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3203</id><created>2013-07-11</created><authors><author><keyname>Vicente</keyname><forenames>Renato</forenames></author><author><keyname>Susemihl</keyname><forenames>Alex</forenames></author><author><keyname>Jeric&#xf3;</keyname><forenames>Jo&#xe3;o Pedro</forenames></author><author><keyname>Caticha</keyname><forenames>Nestor</forenames></author></authors><title>Moral foundations in an interacting neural networks society</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>13 pags, 9 figures</comments><msc-class>91Dxx, 91Exx</msc-class><journal-ref>Physica A 400 124-138 (2014)</journal-ref><doi>10.1016/j.physa.2014.01.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The moral foundations theory supports that people, across cultures, tend to
consider a small number of dimensions when classifying issues on a moral basis.
The data also show that the statistics of weights attributed to each moral
dimension is related to self-declared political affiliation, which in turn has
been connected to cognitive learning styles by recent literature in
neuroscience and psychology. Inspired by these data, we propose a simple
statistical mechanics model with interacting neural networks classifying
vectors and learning from members of their social neighborhood about their
average opinion on a large set of issues. The purpose of learning is to reduce
dissension among agents even when disagreeing. We consider a family of learning
algorithms parametrized by \delta, that represents the importance given to
corroborating (same sign) opinions. We define an order parameter that
quantifies the diversity of opinions in a group with homogeneous learning
style. Using Monte Carlo simulations and a mean field approximation we find the
relation between the order parameter and the learning parameter \delta at a
temperature we associate with the importance of social influence in a given
group. In concordance with data, groups that rely more strongly on
corroborating evidence sustains less opinion diversity. We discuss predictions
of the model and propose possible experimental tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3207</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3207</id><created>2013-07-11</created><authors><author><keyname>Almeida</keyname><forenames>Paulo S&#xe9;rgio</forenames></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames></author></authors><title>Scalable Eventually Consistent Counters over Unreliable Networks</title><categories>cs.DC</categories><acm-class>C.2.4; E.1; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counters are an important abstraction in distributed computing, and play a
central role in large scale geo-replicated systems, counting events such as web
page impressions or social network &quot;likes&quot;. Classic distributed counters,
strongly consistent, cannot be made both available and partition-tolerant, due
to the CAP Theorem, being unsuitable to large scale scenarios. This paper
defines Eventually Consistent Distributed Counters (ECDC) and presents an
implementation of the concept, Handoff Counters, that is scalable and works
over unreliable networks. By giving up the sequencer aspect of classic
distributed counters, ECDC implementations can be made AP in the CAP design
space, while retaining the essence of counting. Handoff Counters are the first
CRDT (Conflict-free Replicated Data Type) based mechanism that overcomes the
identity explosion problem in naive CRDTs, such as G-Counters (where state size
is linear in the number of independent actors that ever incremented the
counter), by managing identities towards avoiding global propagation and
garbage collecting temporary entries. The approach used in Handoff Counters is
not restricted to counters, being more generally applicable to other data types
with associative and commutative operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3216</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3216</id><created>2013-07-11</created><authors><author><keyname>Baisakh</keyname></author><author><keyname>Mishra</keyname><forenames>Chinmayee</forenames></author><author><keyname>Pradhan</keyname><forenames>Abhilipsa</forenames></author></authors><title>A Novel Grid Based Dynamic Energy Efficient Routing Approach for Highly
  Dense Mobile Ad Hoc Networks</title><categories>cs.NI</categories><journal-ref>International Journal of Ad hoc, Sensor &amp; Ubiquitous Computing
  (IJASUC) Vol.4, No.3, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have proposed a novel Grid Based Dynamic Energy Efficient Routing (GBDEER)
approach which is aimed to construct an energy efficient path from source to
destination based on grid area, where each grid will have three deferent levels
of transmission power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3224</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3224</id><created>2013-07-11</created><authors><author><keyname>Cizelj</keyname><forenames>Igor</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Negotiating the Probabilistic Satisfaction of Temporal Logic Motion
  Specifications</title><categories>cs.RO</categories><comments>9 pages, 4 figures; The results in this paper were presented without
  proofs in IEEE/RSJ International Conference on Intelligent Robots and Systems
  November 3-7, 2013 at Tokyo Big Sight, Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a human-supervised control synthesis method for a stochastic
Dubins vehicle such that the probability of satisfying a specification given as
a formula in a fragment of Probabilistic Computational Tree Logic (PCTL) over a
set of environmental properties is maximized. Under some mild assumptions, we
construct a finite approximation for the motion of the vehicle in the form of a
tree-structured Markov Decision Process (MDP). We introduce an efficient
algorithm, which exploits the tree structure of the MDP, for synthesizing a
control policy that maximizes the probability of satisfaction. For the proposed
PCTL fragment, we define the specification update rules that guarantee the
increase (or decrease) of the satisfaction probability. We introduce an
incremental algorithm for synthesizing an updated MDP control policy that
reuses the initial solution. The initial specification can be updated, using
the rules, until the supervisor is satisfied with both the updated
specification and the corresponding satisfaction probability. We propose an
offline and an online application of this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3231</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3231</id><created>2013-07-10</created><authors><author><keyname>Allamigeon</keyname><forenames>Xavier</forenames></author><author><keyname>Gaubert</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Magron</keyname><forenames>Victor</forenames></author><author><keyname>Werner</keyname><forenames>Benjamin</forenames></author></authors><title>Certification of Bounds of Non-linear Functions: the Templates Method</title><categories>cs.SC cs.LO</categories><comments>16 pages, 3 figures, 2 tables</comments><journal-ref>Proceedings of CICM 2013 (Conferences on Intelligent Computer
  Mathematics, &quot;Calculemus'', Track A), Bath, July 2013, volume 7961 of Lecture
  Notes in Computer Science, pages 51--65, Springer</journal-ref><doi>10.1007/978-3-642-39320-4_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to certify lower bounds for real-valued multivariate
functions, defined by semialgebraic or transcendental expressions. The
certificate must be, eventually, formally provable in a proof system such as
Coq. The application range for such a tool is widespread; for instance Hales'
proof of Kepler's conjecture yields thousands of inequalities. We introduce an
approximation algorithm, which combines ideas of the max-plus basis method (in
optimal control) and of the linear templates method developed by Manna et al.
(in static analysis). This algorithm consists in bounding some of the
constituents of the function by suprema of quadratic forms with a well chosen
curvature. This leads to semialgebraic optimization problems, solved by
sum-of-squares relaxations. Templates limit the blow up of these relaxations at
the price of coarsening the approximation. We illustrate the efficiency of our
framework with various examples from the literature and discuss the interfacing
with Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3263</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3263</id><created>2013-07-11</created><updated>2013-09-20</updated><authors><author><keyname>Ghani</keyname><forenames>Neil</forenames><affiliation>University of Strathclyde</affiliation></author><author><keyname>Johann</keyname><forenames>Patricia</forenames><affiliation>Appalachian State University</affiliation></author><author><keyname>Fumex</keyname><forenames>Clement</forenames><affiliation>University of Strathclyde</affiliation></author></authors><title>Indexed Induction and Coinduction, Fibrationally</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (August 28,
  2013) lmcs:738</journal-ref><doi>10.2168/LMCS-9(3:6)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the fibrational approach to induction and coinduction
pioneered by Hermida and Jacobs, and developed by the current authors, in two
key directions. First, we present a dual to the sound induction rule for
inductive types that we developed previously. That is, we present a sound
coinduction rule for any data type arising as the carrier of the final
coalgebra of a functor, thus relaxing Hermida and Jacobs' restriction to
polynomial functors. To achieve this we introduce the notion of a quotient
category with equality (QCE) that i) abstracts the standard notion of a
fibration of relations constructed from a given fibration; and ii) plays a role
in the theory of coinduction dual to that played by a comprehension category
with unit (CCU) in the theory of induction. Secondly, we show that inductive
and coinductive indexed types also admit sound induction and coinduction rules.
Indexed data types often arise as carriers of initial algebras and final
coalgebras of functors on slice categories, so we give sufficient conditions
under which we can construct, from a CCU (QCE) U:E \rightarrow B, a fibration
with base B/I that models indexing by I and is also a CCU (resp., QCE). We
finish the paper by considering the more general case of sound induction and
coinduction rules for indexed data types when the indexing is itself given by a
fibration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3271</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3271</id><created>2013-07-11</created><authors><author><keyname>Schultz</keyname><forenames>Thomas</forenames></author><author><keyname>Vilanova</keyname><forenames>Anna</forenames></author><author><keyname>Brecheisen</keyname><forenames>Ralph</forenames></author><author><keyname>Kindlmann</keyname><forenames>Gordon</forenames></author></authors><title>Fuzzy Fibers: Uncertainty in dMRI Tractography</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fiber tracking based on diffusion weighted Magnetic Resonance Imaging (dMRI)
allows for noninvasive reconstruction of fiber bundles in the human brain. In
this chapter, we discuss sources of error and uncertainty in this technique,
and review strategies that afford a more reliable interpretation of the
results. This includes methods for computing and rendering probabilistic
tractograms, which estimate precision in the face of measurement noise and
artifacts. However, we also address aspects that have received less attention
so far, such as model selection, partial voluming, and the impact of
parameters, both in preprocessing and in fiber tracking itself. We conclude by
giving impulses for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3272</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3272</id><created>2013-07-11</created><authors><author><keyname>Kerber</keyname><forenames>Michael</forenames></author><author><keyname>Sharathkumar</keyname><forenames>R.</forenames></author></authors><title>Approximate Cech Complexes in Low and High Dimensions</title><categories>cs.CG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  \v{C}ech complexes reveal valuable topological information about point sets
at a certain scale in arbitrary dimensions, but the sheer size of these
complexes limits their practical impact. While recent work introduced
approximation techniques for filtrations of (Vietoris-)Rips complexes, a
coarser version of \v{C}ech complexes, we propose the approximation of \v{C}ech
filtrations directly.
  For fixed dimensional point set $S$, we present an approximation of the
\v{C}ech filtration of $S$ by a sequence of complexes of size linear in the
number of points. We generalize well-separated pair decompositions (WSPD) to
well-separated simplicial decomposition (WSSD) in which every simplex defined
on $S$ is covered by some element of WSSD. We give an efficient algorithm to
compute a linear-sized WSSD in fixed dimensional spaces. Using a WSSD, we then
present a linear-sized approximation of the filtration of \v{C}ech complex of
$S$.
  We also present a generalization of the known fact that the Rips complex
approximates the \v{C}ech complex by a factor of $\sqrt{2}$. We define a class
of complexes that interpolate between \v{C}ech and Rips complexes and that,
given any parameter $\eps &gt; 0$, approximate the \v{C}ech complex by a factor
$(1+\eps)$. Our complex can be represented by roughly $O(n^{\lceil
1/2\eps\rceil})$ simplices without any hidden dependence on the ambient
dimension of the point set. Our results are based on an interesting link
between \v{C}ech complex and coresets for minimum enclosing ball of
high-dimensional point sets. As a consequence of our analysis, we show improved
bounds on coresets that approximate the radius of the minimum enclosing ball.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3284</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3284</id><created>2013-07-11</created><authors><author><keyname>Yuan</keyname><forenames>Shuai</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Sequential Selection of Correlated Ads by POMDPs</title><categories>cs.IR</categories><acm-class>I.2.6; I.2.8</acm-class><journal-ref>Proceedings of the ACM CIKM '12. 515-524</journal-ref><doi>10.1145/2396761.2396828</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online advertising has become a key source of revenue for both web search
engines and online publishers. For them, the ability of allocating right ads to
right webpages is critical because any mismatched ads would not only harm web
users' satisfactions but also lower the ad income. In this paper, we study how
online publishers could optimally select ads to maximize their ad incomes over
time. The conventional offline, content-based matching between webpages and ads
is a fine start but cannot solve the problem completely because good matching
does not necessarily lead to good payoff. Moreover, with the limited display
impressions, we need to balance the need of selecting ads to learn true ad
payoffs (exploration) with that of allocating ads to generate high immediate
payoffs based on the current belief (exploitation). In this paper, we address
the problem by employing Partially observable Markov decision processes
(POMDPs) and discuss how to utilize the correlation of ads to improve the
efficiency of the exploration and increase ad incomes in a long run. Our
mathematical derivation shows that the belief states of correlated ads can be
naturally updated using a formula similar to collaborative filtering. To test
our model, a real world ad dataset from a major search engine is collected and
categorized. Experimenting over the data, we provide an analyse of the effect
of the underlying parameters, and demonstrate that our algorithms significantly
outperform other strong baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3290</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3290</id><created>2013-07-11</created><authors><author><keyname>Ahmad</keyname><forenames>Ziad</forenames></author><author><keyname>Chance</keyname><forenames>Zachary</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author></authors><title>Concatenated Coding Using Linear Schemes for Gaussian Broadcast Channels
  with Noisy Channel Output Feedback</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear coding schemes have been the main choice of coding for the additive
white Gaussian noise broadcast channel (AWGN-BC) with noiseless feedback in the
literature. The achievable rate regions of these schemes go well beyond the
capacity region of the AWGN-BC without feedback. In this paper, a concatenating
coding design for the $K$-user AWGN-BC with noisy feedback is proposed that
relies on linear feedback schemes to achieve rate tuples outside the
no-feedback capacity region. Specifically, a linear feedback code for the
AWGN-BC with noisy feedback is used as an inner code that creates an effective
single-user channel from the transmitter to each of the receivers, and then
open-loop coding is used for coding over these single-user channels. An
achievable rate region of linear feedback schemes for noiseless feedback is
shown to be achievable by the concatenated coding scheme for sufficiently small
feedback noise level. Then, a linear feedback coding scheme for the $K$-user
symmetric AWGN-BC with noisy feedback is presented and optimized for use in the
concatenated coding scheme. Lastly, we apply the concatenated coding design to
the two-user AWGN-BC with a single noisy feedback link from one of the
receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3294</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3294</id><created>2013-07-11</created><authors><author><keyname>Rahman</keyname><forenames>Md. Maklachur</forenames></author></authors><title>A dwt, dct and svd based watermarking technique to protect the image
  piracy</title><categories>cs.MM</categories><comments>12 pages, 12 figures and 1 table</comments><journal-ref>International Journal of Managing Public Sector Information and
  Communication Technologies (IJMPICT) Vol. 4, No. 2, June 2013</journal-ref><doi>10.5121/ijmpict.2013.4203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of information technology and multimedia, the use
of digital data is increasing day by day. So it becomes very essential to
protect multimedia information from piracy and also it is challenging. A great
deal of Copyright owners is worried about protecting any kind of illegal
repetition of their information. Hence, facing all these kinds of problems
development of the techniques is very important. Digital watermarking
considered as a solution to prevent the multimedia data. In this paper, an idea
of watermarking is proposed and implemented. In proposed watermarking method,
the original image is rearranged using zigzag sequence and DWT is applied on
rearranged image. Then DCT and SVD are applied on all high bands LH, HL and HH.
Watermark is then embedded by modifying the singular values of these bands.
Extraction of watermark is performed by the inversion of watermark embedding
process. For choosing of these three bands it gives facility of mid-band and
pure high band that ensures good imperceptibility and more robustness against
different kinds of attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3295</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3295</id><created>2013-07-11</created><authors><author><keyname>Alhmiedat</keyname><forenames>Tareq</forenames></author><author><keyname>Salem</keyname><forenames>Amer O. Abu</forenames></author><author><keyname>Taleb</keyname><forenames>Anas Abu</forenames></author></authors><title>An imporved decentralized approach for tracking multiple mobile targets
  through ZigBee WSNs</title><categories>cs.NI</categories><comments>16 pages</comments><doi>10.5121/ijwmn.2013.5305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Target localization and tracking problems in WSNs have received considerable
attention recently, driven by the requirement to achieve high localization
accuracy, with the minimum cost possible. In WSN based tracking applications,
it is critical to know the current location of any sensor node with the minimum
energy consumed. This paper focuses on the energy consumption issue in terms of
communication between nodes whenever the localization information is
transmitted to a sink node. Tracking through WSNs can be categorized into
centralized and decentralized systems. Decentralized systems offer low power
consumption when deployed to track a small number of mobile targets compared to
the centralized tracking systems. However, in several applications, it is
essential to position a large number of mobile targets. In such applications,
decentralized systems offer high power consumption, since the location of each
mobile target is required to be transmitted to a sink node, and this increases
the power consumption for the whole WSN. In this paper, we propose a power
efficient decentralized approach for tracking a large number of mobile targets
while offering reasonable localization accuracy through ZigBee network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3301</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3301</id><created>2013-07-11</created><updated>2015-03-30</updated><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author></authors><title>Optimal Bounds on Approximation of Submodular and XOS Functions by
  Juntas</title><categories>cs.DS cs.CC cs.LG</categories><comments>Extended abstract appears in proceedings of FOCS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the approximability of several classes of real-valued
functions by functions of a small number of variables ({\em juntas}). Our main
results are tight bounds on the number of variables required to approximate a
function $f:\{0,1\}^n \rightarrow [0,1]$ within $\ell_2$-error $\epsilon$ over
the uniform distribution: 1. If $f$ is submodular, then it is $\epsilon$-close
to a function of $O(\frac{1}{\epsilon^2} \log \frac{1}{\epsilon})$ variables.
This is an exponential improvement over previously known results. We note that
$\Omega(\frac{1}{\epsilon^2})$ variables are necessary even for linear
functions. 2. If $f$ is fractionally subadditive (XOS) it is $\epsilon$-close
to a function of $2^{O(1/\epsilon^2)}$ variables. This result holds for all
functions with low total $\ell_1$-influence and is a real-valued analogue of
Friedgut's theorem for boolean functions. We show that $2^{\Omega(1/\epsilon)}$
variables are necessary even for XOS functions.
  As applications of these results, we provide learning algorithms over the
uniform distribution. For XOS functions, we give a PAC learning algorithm that
runs in time $2^{poly(1/\epsilon)} poly(n)$. For submodular functions we give
an algorithm in the more demanding PMAC learning model (Balcan and Harvey,
2011) which requires a multiplicative $1+\gamma$ factor approximation with
probability at least $1-\epsilon$ over the target distribution. Our uniform
distribution algorithm runs in time $2^{poly(1/(\gamma\epsilon))} poly(n)$.
This is the first algorithm in the PMAC model that over the uniform
distribution can achieve a constant approximation factor arbitrarily close to 1
for all submodular functions. As follows from the lower bounds in (Feldman et
al., 2013) both of these algorithms are close to optimal. We also give
applications for proper learning, testing and agnostic learning with value
queries of these classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3306</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3306</id><created>2013-07-11</created><authors><author><keyname>Paya</keyname><forenames>Ashkan</forenames></author><author><keyname>Marinescu</keyname><forenames>Dan C.</forenames></author></authors><title>Energy-aware Application Scaling on a Cloud</title><categories>cs.DC</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud elasticity - the ability to use as much resources as needed at any
given time - and low cost - a user pays only for the resources it consumes -
represent solid incentives for many organizations to migrate some of their
computational activities to a public cloud. As the interest in cloud computing
grows, so does the size of the cloud computing centers and their energy
footprint. The realization that power consumption of cloud computing centers is
significant and it is expected to increase substantially in the future
motivates our interest in scheduling and scaling algorithms which minimize
power consumption. We propose energy-aware application scaling and resource
management algorithms. Though targeting primarily the Infrastructure as a
Service (IaaS), the system models and the algorithms we propose can be applied
to the other cloud delivery models and to private clouds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3310</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3310</id><created>2013-07-11</created><authors><author><keyname>Ameta</keyname><forenames>Juhi</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Improving the quality of Gujarati-Hindi Machine Translation through
  part-of-speech tagging and stemmer-assisted transliteration</title><categories>cs.CL</categories><comments>6 pages; June 2013,
  url-http://airccse.org/journal/ijnlc/papers/2313ijnlc05.pdf</comments><doi>10.5121/ijnlc.2013.2305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Translation for Indian languages is an emerging research area.
Transliteration is one such module that we design while designing a translation
system. Transliteration means mapping of source language text into the target
language. Simple mapping decreases the efficiency of overall translation
system. We propose the use of stemming and part-of-speech tagging for
transliteration. The effectiveness of translation can be improved if we use
part-of-speech tagging and stemming assisted transliteration.We have shown that
much of the content in Gujarati gets transliterated while being processed for
translation to Hindi language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3321</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3321</id><created>2013-07-12</created><authors><author><keyname>Kuppusamy</keyname><forenames>K. S.</forenames></author><author><keyname>Francis</keyname><forenames>Leena Mary</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author></authors><title>Report: A Model for Remote Parental Control System Using Smartphones</title><categories>cs.CY cs.HC</categories><comments>12 Pages, 4 Figures</comments><report-no>ISSN : 2277 - 548X</report-no><msc-class>68U35 Information systems</msc-class><journal-ref>International Journal on Cybernetics &amp; Informatics ( IJCI) Vol.2,
  No.3, June 2013</journal-ref><doi>10.5121/ijci.2013.2303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mammoth growth in the information technology sector in terms of both
quantity and pervasiveness has opened up a Pandora's Box of issues which are
relevant in technical and social aspects. This paper attempts to address one of
the critical issues among them which is concerned with the proper utilization
of computer systems and mobile devices by children and teens. This paper
proposes a model termed &quot;RePort (Remote Parental Control System using
Smartphones)&quot; which is aimed towards monitoring the access behaviour of
computers and smartphones in general and internet to be specific, by teens and
children in a private environment. The model is built such that the access
behaviour shall be monitored from a remote location with the help of smartphone
devices. The access- characteristics are modelled using various parameters in
four different layers of the model.The proposed model is validated with a
prototype implementation in the Android platform for smartphones and as a
background service for computer systems. Various experiments were conducted on
the prototype implementation and results of the experiments validate the
efficiency of the proposed model with respect to user's relevancy metric which
is computed as 93.46%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3324</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3324</id><created>2013-07-12</created><authors><author><keyname>Kumre</keyname><forenames>Laxmi</forenames></author><author><keyname>Somkuwar</keyname><forenames>Ajay</forenames></author><author><keyname>Agnihotri</keyname><forenames>Ganga</forenames></author></authors><title>Power efficient carry propagate adder</title><categories>cs.AR</categories><comments>10 Pages, 10 figures</comments><journal-ref>International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.4, No.3, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we describe the design details and performance of proposed Carry
Propagate Adder based on GDI technique. GDI technique is power efficient
technique for designing digital circuit that consumes less power as compare to
most commonly used CMOS technique. GDI also has an advantage of minimum
propagation delay, minimum area required and less complexity for designing any
digital circuit. We designed Carry Propagate Adder using GDI technique and
compared its performance with CMOS technique in terms of area, delay and power
dissipation. Circuit designed using CADENCE EDA tool and simulated using
SPECTRE VIRTUOSO tool at 0.18m technology. Comparative performance result shows
that Carry Propagate Adder using GDI technique dissipated 55.6% less power as
compare to Carry Propagate Adder using CMOS technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3325</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3325</id><created>2013-07-12</created><authors><author><keyname>Heusinger</keyname><forenames>J. Marcel</forenames></author></authors><title>Challenges of Critical and Emancipatory Design Science Research: The
  Design of 'Possible Worlds' as Response</title><categories>cs.CY cs.HC</categories><comments>Presented at the 15th International Conference on Enterprise
  Information Systems, 4-7 July, 2013 Angers FR. Website:
  http://www.iceis.org/?y=2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popper's (1967) 'piecemeal social change' is an approach manifesting itself
in science as critical and emancipatory (C&amp;E) research. It is concerned with
incrementally removing manifested inequalities to achieve a 'better' world.
Although design science research in information systems seems to be a prime
candidate for such endeavors, respective projects are clearly underrepresented.
This position paper argues that this is due to the demand of justifying
research ex post by an evaluation in practical settings. From the perspective
of C&amp;E research it is questionable if powerful actors grant access to their
organization and support projects which ultimately challenge their position. It
is suggested that theory development based on a synthesis of justificatory
knowledge is a complementary approach that allows designing realizable
responses to C&amp;E issues---the design of 'possible worlds' (Lewis, 1986) as
basis for C&amp;E design science research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3332</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3332</id><created>2013-07-12</created><authors><author><keyname>Olenko</keyname><forenames>Andriy</forenames></author><author><keyname>Pog&#xe1;ny</keyname><forenames>Tibor K.</forenames></author></authors><title>Universal truncation error upper bounds in irregular sampling
  restoration</title><categories>cs.IT math.IT</categories><comments>13 pages. This is an Author's Accepted Manuscript of an article
  published in the Applicable Analysis, Vol.90, No. 3-4. (2011), 595--608.
  [copyright Taylor &amp; Francis], available online at:
  http://www.tandfonline.com/ [DOI:10.1080/00036810903437754]</comments><msc-class>94A20, 26D15 (Primary), 30D15, 41A05(Secondary)</msc-class><journal-ref>Applicable Analysis, Vol.90, No. 3-4. (2011), 595-608</journal-ref><doi>10.1080/00036810903437754</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal (pointwise uniform and time shifted) truncation error upper bounds
are presented in Whittaker--Kotel'nikov--Shannon (WKS) sampling restoration sum
for Bernstein function class $B_{\pi,d}^q\,,\ q \ge 1,$ $d\in \mathbb N\,,$
when the sampled functions decay rate is unknown. The case of multidimensional
irregular sampling is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3336</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3336</id><created>2013-07-12</created><authors><author><keyname>Buche</keyname><forenames>Arti</forenames></author><author><keyname>Chandak</keyname><forenames>Dr. M. B.</forenames></author><author><keyname>Zadgaonkar</keyname><forenames>Akshay</forenames></author></authors><title>Opinion Mining and Analysis: A survey</title><categories>cs.CL cs.IR</categories><comments>10 pages</comments><journal-ref>IJNLC Vol. 2, No.3, June 2013</journal-ref><doi>10.5121/ijnlc.2013.2304</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The current research is focusing on the area of Opinion Mining also called as
sentiment analysis due to sheer volume of opinion rich web resources such as
discussion forums, review sites and blogs are available in digital form. One
important problem in sentiment analysis of product reviews is to produce
summary of opinions based on product features. We have surveyed and analyzed in
this paper, various techniques that have been developed for the key tasks of
opinion mining. We have provided an overall picture of what is involved in
developing a software system for opinion mining on the basis of our survey and
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3337</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3337</id><created>2013-07-12</created><authors><author><keyname>Chandrasekhar</keyname><forenames>T.</forenames></author><author><keyname>Thangavel</keyname><forenames>K.</forenames></author><author><keyname>Elayaraja</keyname><forenames>E.</forenames></author><author><keyname>Sathishkumar</keyname><forenames>E. N.</forenames></author></authors><title>Unsupervised Gene Expression Data using Enhanced Clustering Method</title><categories>cs.CE cs.LG</categories><comments>5 pages, 1 figures, conference</comments><journal-ref>International Conference on Emerging Trends in Computing,
  Communication and Nanotechnology (ICE-CCN), 25-26 March 2013, Page(s): 518 -
  522</journal-ref><doi>10.1109/ICE-CCN.2013.6528554</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microarrays are made it possible to simultaneously monitor the expression
profiles of thousands of genes under various experimental conditions.
Identification of co-expressed genes and coherent patterns is the central goal
in microarray or gene expression data analysis and is an important task in
bioinformatics research. Feature selection is a process to select features
which are more informative. It is one of the important steps in knowledge
discovery. The problem is that not all features are important. Some of the
features may be redundant, and others may be irrelevant and noisy. In this work
the unsupervised Gene selection method and Enhanced Center Initialization
Algorithm (ECIA) with K-Means algorithms have been applied for clustering of
Gene Expression Data. This proposed clustering algorithm overcomes the
drawbacks in terms of specifying the optimal number of clusters and
initialization of good cluster centroids. Gene Expression Data show that could
identify compact clusters with performs well in terms of the Silhouette
Coefficients cluster measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3341</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3341</id><created>2013-07-12</created><authors><author><keyname>Elices</keyname><forenames>Juan A.</forenames></author><author><keyname>Perez-Gonzalez</keyname><forenames>Fernando</forenames></author></authors><title>The Flow Fingerprinting Game</title><categories>cs.CR cs.GT</categories><comments>Workshop on Information Forensics and Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linking two network flows that have the same source is essential in intrusion
detection or in tracing anonymous connections. To improve the performance of
this process, the flow can be modified (fingerprinted) to make it more
distinguishable. However, an adversary located in the middle can modify the
flow to impair the correlation by delaying the packets or introducing dummy
traffic.
  We introduce a game-theoretic framework for this problem, that is used to
derive the Nash Equilibrium. As obtaining the optimal adversary delays
distribution is intractable, some approximations are done. We study the
concrete example where these delays follow a truncated Gaussian distribution.
We also compare the optimal strategies with other fingerprinting schemes. The
results are useful for understanding the limits of flow correlation based on
packet timings under an active attacker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3346</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3346</id><created>2013-07-12</created><authors><author><keyname>Olenko</keyname><forenames>Andriy</forenames></author><author><keyname>Pog&#xe1;ny</keyname><forenames>Tibor K.</forenames></author></authors><title>Universal truncation error upper bounds in sampling restoration</title><categories>cs.IT math.IT</categories><comments>18 pages, 2 figures. This is an Author's Accepted Manuscript of an
  article published in the Georgian Mathematical Journal. Vol.17, No. 4.
  (2010), 765-786. The final publication is available at De Gruyter. DOI:
  10.1515/gmj.2010.033</comments><msc-class>94A20, 41A25(Primary), 41A05, 41A17, 41A80, 26D15(Secondary)</msc-class><journal-ref>Georgian Mathematical Journal. Vol.17, No. 4. (2010), 765--786</journal-ref><doi>10.1515/gmj.2010.033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal (pointwise uniform and time shifted) truncation error upper bounds
are presented for the Whittaker--Kotel'nikov--Shannon (WKS) sampling
restoration sum for Bernstein function classes $B_{\pi,d}^q,\, q&gt;1,\, d\in
\mathbb N$, when the decay rate of the sampled functions is unknown. The case
of regular sampling is discussed. Extremal properties of related series of sinc
functions are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3348</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3348</id><created>2013-07-12</created><authors><author><keyname>Yan</keyname><forenames>Shihao</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author></authors><title>Location Verification Systems in Emerging Wireless Networks</title><categories>cs.NI cs.CR</categories><doi>10.3939/j.issn.1673-5188.2013.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As location-based techniques and applications become ubiquitous in emerging
wireless networks, the verification of location information will become of
growing importance. This has led in recent years to an explosion of activity
related to location verification techniques in wireless networks, with a
specific focus on Intelligent Transport Systems (ITS) being evident. Such focus
is largely due to the mission-critical nature of vehicle location verification
within the ITS scenario. In this work we review recent research in wireless
location verification related to the vehicular network scenario. We
particularly focus on location verification systems that rely on formal
mathematical classification frameworks, showing how many systems are either
partially or fully encompassed by such frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3356</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3356</id><created>2013-07-12</created><authors><author><keyname>Moniruzzaman</keyname><forenames>A B M</forenames></author><author><keyname>Hossain</keyname><forenames>Dr Syed Akhter</forenames></author></authors><title>Comparative Study on Agile software development methodologies</title><categories>cs.SE</categories><comments>25 pages, 25 images, 86 references used, with authors biographies</comments><journal-ref>Global Journal of Computer Science and Technology (c) Volume 13
  Issue 7 Version I</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Today-s business environment is very much dynamic, and organisations are
constantly changing their software requirements to adjust with new environment.
They also demand for fast delivery of software products as well as for
accepting changing requirements. In this aspect, traditional plan-driven
developments fail to meet up these requirements. Though traditional software
development methodologies, such as life cycle-based structured and object
oriented approaches, continue to dominate the systems development few decades
and much research has done in traditional methodologies, Agile software
development brings its own set of novel challenges that must be addressed to
satisfy the customer through early and continuous delivery of the valuable
software. It is a set of software development methods based on iterative and
incremental development process, where requirements and development evolve
through collaboration between self-organizing, cross-functional teams that
allows rapid delivery of high quality software to meet customer needs and also
accommodate changes in the requirements. In this paper, we significantly
identify and describe the major factors, that Agile development approach
improves software development process to meet the rapid changing business
environments. We also provide a brief comparison of agile development
methodologies with traditional systems development methodologies, and discuss
current state of adopting agile methodologies. We speculate that from the need
to satisfy the customer through early and continuous delivery of the valuable
software, Agile software development is emerged as an alternative to
traditional plan-based software development methods. The purpose of this paper,
is to provide an in-depth understanding, the major benefits of agile
development approach to software development industry, as well as provide a
comparison study report of ASDM over TSDM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3360</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3360</id><created>2013-07-12</created><updated>2015-02-17</updated><authors><author><keyname>Cambareri</keyname><forenames>Valerio</forenames></author><author><keyname>Mangia</keyname><forenames>Mauro</forenames></author><author><keyname>Pareschi</keyname><forenames>Fabio</forenames></author><author><keyname>Rovatti</keyname><forenames>Riccardo</forenames></author><author><keyname>Setti</keyname><forenames>Gianluca</forenames></author></authors><title>Low-complexity Multiclass Encryption by Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Signal Processing, accepted for publication.
  Article in press</comments><doi>10.1109/TSP.2015.2407315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea that compressed sensing may be used to encrypt information from
unauthorised receivers has already been envisioned, but never explored in depth
since its security may seem compromised by the linearity of its encoding
process. In this paper we apply this simple encoding to define a general
private-key encryption scheme in which a transmitter distributes the same
encoded measurements to receivers of different classes, which are provided
partially corrupted encoding matrices and are thus allowed to decode the
acquired signal at provably different levels of recovery quality.
  The security properties of this scheme are thoroughly analysed: firstly, the
properties of our multiclass encryption are theoretically investigated by
deriving performance bounds on the recovery quality attained by lower-class
receivers with respect to high-class ones. Then we perform a statistical
analysis of the measurements to show that, although not perfectly secure,
compressed sensing grants some level of security that comes at almost-zero cost
and thus may benefit resource-limited applications.
  In addition to this we report some exemplary applications of multiclass
encryption by compressed sensing of speech signals, electrocardiographic tracks
and images, in which quality degradation is quantified as the impossibility of
some feature extraction algorithms to obtain sensitive information from
suitably degraded signal recoveries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3388</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3388</id><created>2013-07-12</created><authors><author><keyname>Faisal</keyname><forenames>Fazle Elahi</forenames></author><author><keyname>Milenkovic</keyname><forenames>Tijana</forenames></author></authors><title>Dynamic networks reveal key players in aging</title><categories>cs.CE q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Since susceptibility to diseases increases with age, studying
aging gains importance. Analyses of gene expression or sequence data, which
have been indispensable for investigating aging, have been limited to studying
genes and their protein products in isolation, ignoring their connectivities.
However, proteins function by interacting with other proteins, and this is
exactly what biological networks (BNs) model. Thus, analyzing the proteins' BN
topologies could contribute to understanding of aging. Current methods for
analyzing systems-level BNs deal with their static representations, even though
cells are dynamic. For this reason, and because different data types can give
complementary biological insights, we integrate current static BNs with
aging-related gene expression data to construct dynamic, age-specific BNs.
Then, we apply sensitive measures of topology to the dynamic BNs to study
cellular changes with age.
  Results: While global BN topologies do not significantly change with age,
local topologies of a number of genes do. We predict such genes as
aging-related. We demonstrate credibility of our predictions by: 1) observing
significant overlap between our predicted aging-related genes and &quot;ground
truth&quot; aging-related genes; 2) showing that our aging-related predictions group
by functions and diseases that are different than functions and diseases of
genes that are not predicted as aging-related; 3) observing significant overlap
between functions and diseases that are enriched in our aging-related
predictions and those that are enriched in &quot;ground truth&quot; aging-related data;
4) providing evidence that diseases which are enriched in our aging-related
predictions are linked to human aging; and 5) validating all of our
high-scoring novel predictions via manual literature search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3396</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3396</id><created>2013-07-12</created><authors><author><keyname>Swaminathan</keyname><forenames>R.</forenames></author><author><keyname>Karnavel</keyname><forenames>K.</forenames></author></authors><title>Software as a Service - Common Service Bus (SAAS-CSB)</title><categories>cs.DC cs.SE</categories><doi>10.5121/cseij.2013.3301</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Software-as-a-Service (SaaS) is a form of cloud computing that relieves the
user from the concern of hardware, software installation and management. It is
an emerging business model that delivers software applications to the users
through Web-based technology. Software vendors have varying requirements and
SaaS applications most typically support such requirements. The various
applications used by unique customers in a single instance are known as
Multi-Tenancy. There would be a delay in service when the user sends the data
from multiple applications to multiple destinations and from multiple
applications to single destination due to the use of single CSB. This problem
can be overcome by using multiple CSB concepts and hence multiple senders can
efficiently send their data to multiple receivers at the same time. The
multiple clouds are monitored and managed by the SaaS-CSB portal. The idea of
SaaS-CSB Portal is to provide a single pane of glass for the user to consume
and govern any service from any cloud. Thus, SaaS-CSB application allows
companies to save their IT cost and valuable time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3398</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3398</id><created>2013-07-12</created><authors><author><keyname>Karnavel</keyname><forenames>K.</forenames></author><author><keyname>Divya</keyname><forenames>V.</forenames></author><author><keyname>Gnanakeerthika</keyname></author><author><keyname>Karthika</keyname><forenames>P.</forenames></author></authors><title>Agent Based Software Testing Framework (ABSTF) for Application
  Maintenance</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Software testing framework can be stated as the process of verifying and
validating that a computer program/application works as expected and meets the
requirements of the user. Usually testing can be done manually or using tools.
Manual testing can be time consuming and tedious, also it requires heavy
investment of human resources. Testing tools in fact have many advantages and
are widely used nowadays, but also has several disadvantages. One particular
problem is that human intervention is no longer needed. Testing tools are of
high cost and so, it cannot be used for small companies. Hence in this paper we
propose Agent based testing, which is a fast growing approach in testing field.
The proposed system (ABSTF) has to reduce the application testing moment,
easily find out bug and solve the bug by Regression Testing. Here by we are
going to use a safe efficient regression selection technique algorithm to
selectively retest the modified program. We also use Traceability relation of
completeness checking for agent oriented system, which identifies missing
elements in entire application and classifies defect in the testing moment.
With the ability of agents to act autonomously, monitoring code changes and
generating test cases for the changed version of code can be done dynamically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3399</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3399</id><created>2013-07-12</created><authors><author><keyname>Kumar</keyname><forenames>N. Sampath</forenames></author><author><keyname>KarthikChandran</keyname><forenames>U.</forenames></author><author><keyname>ArunKumar</keyname><forenames>N.</forenames></author><author><keyname>Karnavel</keyname><forenames>K.</forenames></author></authors><title>Social Networking Site For Self Portfolio</title><categories>cs.SI cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Online social networking concept is a global phenomenon and there are
millions of sites which help in being connected with friends and family. This
project focuses on creating self-portfolios for the users which makes the users
engaging with their skills. The users follow the other users to interact and
communicate with them. Users can encourage the other users blogs and videos by
clicking the hit button. The functionality of this site is designed to focus on
both professional as well as academics. Each user is given a dashboard for
uploading videos and writing blogs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3402</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3402</id><created>2013-07-12</created><authors><author><keyname>Karnavel</keyname><forenames>K.</forenames></author><author><keyname>Sakthivel</keyname><forenames>M.</forenames></author><author><keyname>Karuppasamy</keyname><forenames>L.</forenames></author></authors><title>Improving Data Security in Infrastructure Networks Based on Unipath
  Routing</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An infrastructure network is a self-organizing network which uses Access
Point (AP) of wireless links that connecting one node with another. These nodes
can communicate without using ad hoc, instead these nodes form an arbitrary
topology (BSS/ESS) in which these nodes play the role of routers. Though the
efficiency of Infrastructure networks is high, they are highly vulnerable to
security attacks. Detecting/Preventing these attacks over the network is highly
challenging task. Many solutions are proposed to provide authentication,
confidentiality, availability, secure routing and intrusion avoidance in
infrastructure networks. Providing security in such dynamically changing
networks is a hard task. Characteristic of infrastructure network should also
be taken into consideration in order to design efficient solutions. In this
study, we focus on efficiently increasing the flow transmission confidentiality
in infrastructure networks based on multi-path routing. In order to increase
confidentiality of transmitted data, we take advantage of the existence of
multiple paths between nodes in an infrastructure network with the help of
Access Point. In this approach the original data is split into package and are
forwarded through access point. The encrypted packets are then forwarded in
different disjoint paths that exist between sender and receiver. Even if an
attacker succeeds to obtain one or more transmitted packets, the probability of
reconstructing the original message is very low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3411</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3411</id><created>2013-07-12</created><authors><author><keyname>Montazerolghaem</keyname><forenames>Ahmad Reza</forenames></author><author><keyname>Yaghmaee</keyname><forenames>Mohammad Hossein</forenames></author></authors><title>Sip Overload Control Testbed: Design, Building and Evaluation</title><categories>cs.NI</categories><comments>10 pages, 9 figures, 2 table, 37 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Having facilities such as being in text form, end-to-end connection
establishment, and being independence from the type of transmitted data, SIP
protocol is a good choice for signaling protocol in order to set up a
connection between two users of an IP network. Although utilization of SIP
protocol in a wide range of applications has made various vulnerabilities in
this protocol, amongst which overload could make serious problems in SIP
servers. A SIP is overloaded when it does not have sufficient resources
(majorly CPU processing power and memory) to process all messages. In this
paper the window-based overload control mechanism which does not require
explicit feedback is developed and implemented on Asterisk open source proxy
and evaluated. The results of implementation show that this method could
practically maintain throughput in case of overload. As we know this is the
only overload control method which is implemented on a real platform without
using explicit feedback. The results show that the under load server maintains
its throughput at the maximum capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3412</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3412</id><created>2013-07-12</created><updated>2013-07-15</updated><authors><author><keyname>Criado</keyname><forenames>Regino</forenames></author><author><keyname>Garcia</keyname><forenames>Esther</forenames></author><author><keyname>Pedroche</keyname><forenames>Francisco</forenames></author><author><keyname>Romance</keyname><forenames>Miguel</forenames></author></authors><title>A new method for comparing rankings through complex networks: Model and
  analysis of competitiveness of major European soccer leagues</title><categories>physics.soc-ph cs.DM cs.SI</categories><doi>10.1063/1.4826446</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show a new technique to analyze families of rankings. In
particular we focus on sports rankings and, more precisely, on soccer leagues.
We consider that two teams compete when they change their relative positions in
consecutive rankings. This allows to define a graph by linking teams that
compete. We show how to use some structural properties of this competitivity
graph to measure to what extend the teams in a league compete. These structural
properties are the mean degree, the mean strength and the clustering
coefficient. We give a generalization of the Kendall's correlation coefficient
to more than two rankings. We also show how to make a dynamic analysis of a
league and how to compare different leagues. We apply this technique to analyze
the four major European soccer leagues: Bundesliga, Italian Lega, Spanish Liga,
and Premier League. We compare our results with the classical analysis of sport
ranking based on measures of competitive balance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3419</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3419</id><created>2013-07-12</created><updated>2013-12-06</updated><authors><author><keyname>Schmidt</keyname><forenames>Michael</forenames></author><author><keyname>Lausen</keyname><forenames>Georg</forenames></author></authors><title>Pleasantly Consuming Linked Data with RDF Data Descriptions</title><categories>cs.DB</categories><comments>12 pages + Appendix</comments><acm-class>H.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the intention of RDF is to provide an open, minimally constraining
way for representing information, there exists an increasing number of
applications for which guarantees on the structure and values of an RDF data
set become desirable if not essential. What is missing in this respect are
mechanisms to tie RDF data to quality guarantees akin to schemata of relational
databases, or DTDs in XML, in particular when translating legacy data coming
with a rich set of integrity constraints - like keys or cardinality
restrictions - into RDF. Addressing this shortcoming, we present the RDF Data
Description language (RDD), which makes it possible to specify instance-level
data constraints over RDF. Making such constraints explicit does not only help
in asserting and maintaining data quality, but also opens up new optimization
opportunities for query engines and, most importantly, makes query formulation
a lot easier for users and system developers. We present design goals, syntax,
and a formal, First-order logics based semantics of RDDs and discuss the impact
on consuming Linked Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3430</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3430</id><created>2013-07-12</created><updated>2013-11-19</updated><authors><author><keyname>Bonaventura</keyname><forenames>Moreno</forenames></author><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author></authors><title>Characteristic times of biased random walks on complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 14 figures, 1 table</comments><journal-ref>Phys. Rev. E 89, 012803 (2014)</journal-ref><doi>10.1103/PhysRevE.89.012803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider degree-biased random walkers whose probability to move from a
node to one of its neighbors of degree $k$ is proportional to $k^{\alpha}$,
where $\alpha$ is a tuning parameter. We study both numerically and
analytically three types of characteristic times, namely: i) the time the
walker needs to come back to the starting node, ii) the time it takes to visit
a given node for the first time, and iii) the time it takes to visit all the
nodes of the network. We consider a large data set of real-world networks and
we show that the value of $\alpha$ which minimizes the three characteristic
times is different from the value $\alpha_{\rm min}=-1$ analytically found for
uncorrelated networks in the mean-field approximation. In addition to this, we
found that assortative networks have preferentially a value of $\alpha_{\rm
min}$ in the range $[-1,-0.5]$, while disassortative networks have $\alpha_{\rm
min}$ in the range $[-0.5, 0]$. We derive an analytical relation between the
degree correlation exponent $\nu$ and the optimal bias value $\alpha_{\rm
min}$, which works well for real-world assortative networks. When only local
information is available, degree-biased random walks can guarantee smaller
characteristic times than the classical unbiased random walks, by means of an
appropriate tuning of the motion bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3435</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3435</id><created>2013-07-12</created><updated>2013-07-15</updated><authors><author><keyname>Afshar</keyname><forenames>Hadi Mohasel</forenames></author><author><keyname>Sunehag</keyname><forenames>Peter</forenames></author></authors><title>On Nicod's Condition, Rules of Induction and the Raven Paradox</title><categories>cs.AI</categories><comments>On raven paradox, Nicod's condition, projectability, induction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Philosophers writing about the ravens paradox often note that Nicod's
Condition (NC) holds given some set of background information, and fails to
hold against others, but rarely go any further. That is, it is usually not
explored which background information makes NC true or false. The present paper
aims to fill this gap. For us, &quot;(objective) background knowledge&quot; is restricted
to information that can be expressed as probability events. Any other
configuration is regarded as being subjective and a property of the a priori
probability distribution. We study NC in two specific settings. In the first
case, a complete description of some individuals is known, e.g. one knows of
each of a group of individuals whether they are black and whether they are
ravens. In the second case, the number of individuals having a particular
property is given, e.g. one knows how many ravens or how many black things
there are (in the relevant population). While some of the most famous answers
to the paradox are measure-dependent, our discussion is not restricted to any
particular probability measure. Our most interesting result is that in the
second setting, NC violates a simple kind of inductive inference (namely
projectability). Since relative to NC, this latter rule is more closely related
to, and more directly justified by our intuitive notion of inductive reasoning,
this tension makes a case against the plausibility of NC. In the end, we
suggest that the informal representation of NC may seem to be intuitively
plausible because it can easily be mistaken for reasoning by analogy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3439</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3439</id><created>2013-07-12</created><authors><author><keyname>Singh</keyname><forenames>Y. Jayanta</forenames></author><author><keyname>Gupta</keyname><forenames>Shalu</forenames></author></authors><title>Speedy Object Detection based on Shape</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1210.7038 by other authors</comments><journal-ref>The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.5, No.3, June 2013</journal-ref><doi>10.5121/ijma.2013.5302</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This study is a part of design of an audio system for in-house object
detection system for visually impaired, low vision personnel by birth or by an
accident or due to old age. The input of the system will be scene and output as
audio. Alert facility is provided based on severity levels of the objects
(snake, broke glass etc) and also during difficulties. The study proposed
techniques to provide speedy detection of objects based on shapes and its
scale. Features are extraction to have minimum spaces using dynamic scaling.
From a scene, clusters of objects are formed based on the scale and shape.
Searching is performed among the clusters initially based on the shape, scale,
mean cluster value and index of object(s). The minimum operation to detect the
possible shape of the object is performed. In case the object does not have a
likely matching shape, scale etc, then the several operations required for an
object detection will not perform; instead, it will declared as a new object.
In such way, this study finds a speedy way of detecting objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3448</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3448</id><created>2013-07-12</created><authors><author><keyname>Sheta</keyname><forenames>Dr. Osama E.</forenames></author><author><keyname>Eldeen</keyname><forenames>Ahmed Nour</forenames></author></authors><title>Evaluating a healthcare data warehouse for cancer diseases</title><categories>cs.DB</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the evaluation of the architecture of healthcare data
warehouse specific to cancer diseases. This data warehouse containing relevant
cancer medical information and patient data. The data warehouse provides the
source for all current and historical health data to help executive manager and
doctors to improve the decision making process for cancer patients. The
evaluation model based on Bill Inmon's definition of data warehouse is proposed
to evaluate the Cancer data warehouse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3457</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3457</id><created>2013-07-12</created><authors><author><keyname>Bah</keyname><forenames>Bubacarr</forenames></author><author><keyname>Sadeghian</keyname><forenames>Ali</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Energy-aware adaptive bi-Lipschitz embeddings</title><categories>cs.LG cs.IT math.IT</categories><comments>4 pages, 2 figures, conference</comments><msc-class>68Q99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a dimensionality reducing matrix design based on training data
with constraints on its Frobenius norm and number of rows. Our design criteria
is aimed at preserving the distances between the data points in the
dimensionality reduced space as much as possible relative to their distances in
original data space. This approach can be considered as a deterministic
Bi-Lipschitz embedding of the data points. We introduce a scalable learning
algorithm, dubbed AMUSE, and provide a rigorous estimation guarantee by
leveraging game theoretic tools. We also provide a generalization
characterization of our matrix based on our sample data. We use compressive
sensing problems as an example application of our problem, where the Frobenius
norm design constraint translates into the sensing energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3463</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3463</id><created>2013-07-12</created><updated>2014-03-27</updated><authors><author><keyname>Eremeev</keyname><forenames>Anton</forenames></author></authors><title>Non-Elitist Genetic Algorithm as a Local Search Method</title><categories>cs.NE</categories><comments>Extended abstract of the talk presented at Dagstuhl Seminar &quot;Theory
  of Evolutionary Algorithms&quot; (Dagstuhl, Germany, 30 June - 5 July 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sufficient conditions are found under which the iterated non-elitist genetic
algorithm with tournament selection first visits a local optimum in
polynomially bounded time on average. It is shown that these conditions are
satisfied on a class of problems with guaranteed local optima (GLO) if
appropriate parameters of the algorithm are chosen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3489</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3489</id><created>2013-07-11</created><authors><author><keyname>Ali</keyname><forenames>Bilel Ben</forenames></author><author><keyname>Jarray</keyname><forenames>Fethi</forenames></author></authors><title>Genetic approach for arabic part of speech tagging</title><categories>cs.CL cs.NE</categories><comments>12 pages, 8 figures</comments><msc-class>68T50</msc-class><journal-ref>International Journal on Natural Language Computing (IJNLC) Vol.
  2, No.3, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing number of textual resources available, the ability to
understand them becomes critical. An essential first step in understanding
these sources is the ability to identify the part of speech in each sentence.
Arabic is a morphologically rich language, wich presents a challenge for part
of speech tagging. In this paper, our goal is to propose, improve and implement
a part of speech tagger based on a genetic alorithm. The accuracy obtained with
this method is comparable to that of other probabilistic approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3522</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3522</id><created>2013-07-12</created><authors><author><keyname>Lera</keyname><forenames>Daniela</forenames></author><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author></authors><title>Acceleration of univariate global optimization algorithms working with
  Lipschitz functions and Lipschitz first derivatives</title><categories>math.OC cs.MS cs.NA math.NA</categories><comments>21 pages,5 figures, 6 tables</comments><msc-class>90C26, 65K05</msc-class><journal-ref>SIAM Journal on Optimization, (2013), 23(1), 508-529</journal-ref><doi>10.1137/110859129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with two kinds of the one-dimensional global optimization
problems over a closed finite interval: (i) the objective function $f(x)$
satisfies the Lipschitz condition with a constant $L$; (ii) the first
derivative of $f(x)$ satisfies the Lipschitz condition with a constant $M$. In
the paper, six algorithms are presented for the case (i) and six algorithms for
the case (ii). In both cases, auxiliary functions are constructed and
adaptively improved during the search. In the case (i), piece-wise linear
functions are constructed and in the case (ii) smooth piece-wise quadratic
functions are used. The constants $L$ and $M$ either are taken as values known
a priori or are dynamically estimated during the search. A recent technique
that adaptively estimates the local Lipschitz constants over different zones of
the search region is used to accelerate the search. A new technique called the
\emph{local improvement} is introduced in order to accelerate the search in
both cases (i) and (ii). The algorithms are described in a unique framework,
their properties are studied from a general viewpoint, and convergence
conditions of the proposed algorithms are given. Numerical experiments executed
on 120 test problems taken from the literature show quite a promising
performance of the new accelerating techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3529</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3529</id><created>2013-07-12</created><authors><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author></authors><title>Solving ordinary differential equations on the Infinity Computer by
  working with infinitesimals numerically</title><categories>math.NA cs.MS cs.NA</categories><comments>25 pages, 1 figure, 3 tables</comments><msc-class>65L05, 65D25, 65G50</msc-class><journal-ref>Applied Mathematics and Computation, (2013), 219(22), 10668-10681</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exists a huge number of numerical methods that iteratively construct
approximations to the solution $y(x)$ of an ordinary differential equation
(ODE) $y'(x)=f(x,y)$ starting from an initial value $y_0=y(x_0)$ and using a
finite approximation step $h$ that influences the accuracy of the obtained
approximation. In this paper, a new framework for solving ODEs is presented for
a new kind of a computer -- the Infinity Computer (it has been patented and its
working prototype exists). The new computer is able to work numerically with
finite, infinite, and infinitesimal numbers giving so the possibility to use
different infinitesimals numerically and, in particular, to take advantage of
infinitesimal values of $h$. To show the potential of the new framework a
number of results is established. It is proved that the Infinity Computer is
able to calculate derivatives of the solution $y(x)$ and to reconstruct its
Taylor expansion of a desired order numerically without finding the respective
derivatives analytically (or symbolically) by the successive derivation of the
ODE as it is usually done when the Taylor method is applied. Methods using
approximations of derivatives obtained thanks to infinitesimals are discussed
and a technique for an automatic control of rounding errors is introduced.
Numerical examples are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3544</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3544</id><created>2013-07-12</created><updated>2014-09-03</updated><authors><author><keyname>Kailkhura</keyname><forenames>Bhavya</forenames></author><author><keyname>Han</keyname><forenames>Yunghsiang S.</forenames></author><author><keyname>Brahma</keyname><forenames>Swastik</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Distributed Bayesian Detection with Byzantine Data</title><categories>cs.IT cs.CR cs.DC cs.GT math.IT stat.AP</categories><comments>32 pages, 4 figures, Submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of distributed Bayesian detection in
the presence of Byzantines in the network. It is assumed that a fraction of the
nodes in the network are compromised and reprogrammed by an adversary to
transmit false information to the fusion center (FC) to degrade detection
performance. The problem of distributed detection is formulated as a binary
hypothesis test at the FC based on 1-bit data sent by the sensors. The
expression for minimum attacking power required by the Byzantines to blind the
FC is obtained. More specifically, we show that above a certain fraction of
Byzantine attackers in the network, the detection scheme becomes completely
incapable of utilizing the sensor data for detection. We analyze the problem
under different attacking scenarios and derive results for different
non-asymptotic cases. It is found that existing asymptotics-based results do
not hold under several non-asymptotic scenarios. When the fraction of
Byzantines is not sufficient to blind the FC, we also provide closed form
expressions for the optimal attacking strategies for the Byzantines that most
degrade the detection performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3548</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3548</id><created>2013-07-12</created><authors><author><keyname>Baisakh</keyname></author></authors><title>A Review of Energy Efficient Dynamic Source Routing Protocol for Mobile
  Ad Hoc Networks</title><categories>cs.NI</categories><comments>International Journal of Computer Applications April 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comprehensive summery of different energy efficient
protocols that are based on the basic Mechanism of DSR and enlightens the
effort and commitment that has been made since last 10 year to turn the
traditional DSR as energy efficient routing protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3549</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3549</id><created>2013-07-12</created><authors><author><keyname>Chandrasekhar</keyname><forenames>T.</forenames></author><author><keyname>Thangavel</keyname><forenames>K.</forenames></author><author><keyname>Elayaraja</keyname><forenames>E.</forenames></author></authors><title>Performance Analysis of Clustering Algorithms for Gene Expression Data</title><categories>cs.CE cs.LG</categories><comments>4 pages,4 figures. arXiv admin note: substantial text overlap with
  arXiv:1112.4261, arXiv:1201.4914, arXiv:1307.3337</comments><journal-ref>International Journal of Scientific &amp; Engineering Research Volume
  3, Issue 12, December-2012, page 1-4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microarray technology is a process that allows thousands of genes
simultaneously monitor to various experimental conditions. It is used to
identify the co-expressed genes in specific cells or tissues that are actively
used to make proteins, This method is used to analysis the gene expression, an
important task in bioinformatics research. Cluster analysis of gene expression
data has proved to be a useful tool for identifying co-expressed genes,
biologically relevant groupings of genes and samples. In this paper we analysed
K-Means with Automatic Generations of Merge Factor for ISODATA- AGMFI, to group
the microarray data sets on the basic of ISODATA. AGMFI is to generate initial
values for merge and Spilt factor, maximum merge times instead of selecting
efficient values as in ISODATA. The initial seeds for each cluster were
normally chosen either sequentially or randomly. The quality of the final
clusters was found to be influenced by these initial seeds. For the real life
problems, the suitable number of clusters cannot be predicted. To overcome the
above drawback the current research focused on developing the clustering
algorithms without giving the initial number of clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3550</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3550</id><created>2013-07-12</created><authors><author><keyname>Karnavel</keyname><forenames>K.</forenames></author><author><keyname>shalini</keyname><forenames>L.</forenames></author><author><keyname>Ramananthini</keyname><forenames>M.</forenames></author></authors><title>Refining Data Security in Infrastructure Networks Support of Multipath
  Routing</title><categories>cs.DC cs.CR cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.3402</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An infrastructure network is a self-organizing network with help of Access
Point (AP) of wireless links connecting nodes to another. The nodes can
communicate without an ad hoc. They form an uninformed topology (BSS/ESS),
where the nodes play the role of routers and are free to move randomly.
Infrastructure networks proved their efficiency being used in different fields
but they are highly vulnerable to security attacks and dealing with this is one
of the main challenges of these networks at present. In recent times some
clarification are proposed to provide authentication, confidentiality,
availability, secure routing and intrusion avoidance in infrastructure
networks. Implementing security in such dynamically changing networks is a hard
task. Infrastructure network characteristics should be taken into consideration
to be clever to design efficient solutions. Here we spotlight on civilizing the
flow transmission privacy in infrastructure networks based on multipath
routing. Certainly, we take benefit of the being of multiple paths between
nodes in an infrastructure network to increase the confidentiality robustness
of transmitted data with the help of Access Point. In our approach the original
message to secure is split into shares through access point that are encrypted
and combined then transmitted along different disjointed existing paths between
sender and receiver. Even if an intruder achieve something to get one or more
transmitted distribute the likelihood that the unique message will be
reconstituted is very squat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3573</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3573</id><created>2013-07-12</created><authors><author><keyname>Yuan</keyname><forenames>Shuai</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>van der Meer</keyname><forenames>Maurice</forenames></author></authors><title>Adaptive Keywords Extraction with Contextual Bandits for Advertising on
  Parked Domains</title><categories>cs.IR</categories><comments>To appear in the proceedings of the IATP '13 workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain name registrars and URL shortener service providers place
advertisements on the parked domains (Internet domain names which are not in
service) in order to generate profits. As the web contents have been removed,
it is critical to make sure the displayed ads are directly related to the
intents of the visitors who have been directed to the parked domains. Because
of the missing contents in these domains, it is non-trivial to generate the
keywords to describe the previous contents and therefore the users intents. In
this paper we discuss the adaptive keywords extraction problem and introduce an
algorithm based on the BM25F term weighting and linear multi-armed bandits. We
built a prototype over a production domain registration system and evaluated it
using crowdsourcing in multiple iterations. The prototype is compared with
other popular methods and is shown to be more effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3581</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3581</id><created>2013-07-12</created><updated>2014-11-02</updated><authors><author><keyname>He</keyname><forenames>Li</forenames></author><author><keyname>Qi</keyname><forenames>Hairong</forenames></author><author><keyname>Zaretzki</keyname><forenames>Russell</forenames></author></authors><title>Image color transfer to evoke different emotions based on color
  combinations</title><categories>cs.CV cs.GR</categories><comments>Signal, Image and Video Processing, September 2014</comments><doi>10.1007/s11760-014-0691-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a color transfer framework to evoke different emotions for
images based on color combinations is proposed. The purpose of this color
transfer is to change the &quot;look and feel&quot; of images, i.e., evoking different
emotions. Colors are confirmed as the most attractive factor in images. In
addition, various studies in both art and science areas have concluded that
other than single color, color combinations are necessary to evoke specific
emotions. Therefore, we propose a novel framework to transfer color of images
based on color combinations, using a predefined color emotion model. The
contribution of this new framework is three-fold. First, users do not need to
provide reference images as used in traditional color transfer algorithms. In
most situations, users may not have enough aesthetic knowledge or path to
choose desired reference images. Second, because of the usage of color
combinations instead of single color for emotions, a new color transfer
algorithm that does not require an image library is proposed. Third, again
because of the usage of color combinations, artifacts that are normally seen in
traditional frameworks using single color are avoided. We present encouraging
results generated from this new framework and its potential in several possible
applications including color transfer of photos and paintings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3585</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3585</id><created>2013-07-12</created><authors><author><keyname>Gr&#xe9;goire</keyname><forenames>&#xc9;ric</forenames></author><author><keyname>Lagniez</keyname><forenames>Jean-Marie</forenames></author><author><keyname>Mazure</keyname><forenames>Bertrand</forenames></author></authors><title>Improving MUC extraction thanks to local search</title><categories>cs.AI</categories><comments>17 pages, 5 figures, 1 table, 3 algorithms, 33 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ExtractingMUCs(MinimalUnsatisfiableCores)fromanunsatisfiable constraint
network is a useful process when causes of unsatisfiability must be understood
so that the network can be re-engineered and relaxed to become sat- isfiable.
Despite bad worst-case computational complexity results, various MUC- finding
approaches that appear tractable for many real-life instances have been
proposed. Many of them are based on the successive identification of so-called
transition constraints. In this respect, we show how local search can be used
to possibly extract additional transition constraints at each main iteration
step. The approach is shown to outperform a technique based on a form of model
rotation imported from the SAT-related technology and that also exhibits
additional transi- tion constraints. Our extensive computational
experimentations show that this en- hancement also boosts the performance of
state-of-the-art DC(WCORE)-like MUC extractors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3586</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3586</id><created>2013-07-12</created><updated>2014-01-20</updated><authors><author><keyname>Stein</keyname><forenames>Noah D.</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author></authors><title>Exchangeable Equilibria, Part I: Symmetric Bimatrix Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of exchangeable equilibria of a symmetric bimatrix
game, defined as those correlated equilibria in which players' strategy choices
are conditionally independently and identically distributed given some hidden
variable. We give several game-theoretic interpretations and a version of the
&quot;revelation principle&quot;. Geometrically, the set of exchangeable equilibria is
convex and lies between the symmetric Nash equilibria and the symmetric
correlated equilibria. Exchangeable equilibria can achieve higher expected
utility than symmetric Nash equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3608</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3608</id><created>2013-07-13</created><authors><author><keyname>Budhiraja</keyname><forenames>Rohit</forenames></author><author><keyname>KS</keyname><forenames>Karthik</forenames></author><author><keyname>Ramamurthi</keyname><forenames>Bhaskar</forenames></author></authors><title>Linear Precoders for Non-Regenerative Asymmetric Two-way Relaying in
  Cellular Systems</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-way relaying (TWR) reduces the spectral-efficiency loss caused in
conventional half-duplex relaying. TWR is possible when two nodes exchange data
simultaneously through a relay. In cellular systems, data exchange between base
station (BS) and users is usually not simultaneous e.g., a user (TUE) has
uplink data to transmit during multiple access (MAC) phase, but does not have
downlink data to receive during broadcast (BC) phase. This non-simultaneous
data exchange will reduce TWR to spectrally-inefficient conventional
half-duplex relaying. With infrastructure relays, where multiple users
communicate through a relay, a new transmission protocol is proposed to recover
the spectral loss. The BC phase following the MAC phase of TUE is now used by
the relay to transmit downlink data to another user (RUE). RUE will not be able
to cancel the back-propagating interference. A structured precoder is designed
at the multi-antenna relay to cancel this interference. With multiple-input
multiple-output (MIMO) nodes, the proposed precoder also triangulates the
compound MAC and BC phase MIMO channels. The channel triangulation reduces the
weighted sum-rate optimization to power allocation problem, which is then cast
as a geometric program. Simulation results illustrate the effectiveness of the
proposed protocol over conventional solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3616</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3616</id><created>2013-07-13</created><authors><author><keyname>Ye</keyname><forenames>Fred Y.</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>The &quot;Academic Trace&quot; of the Performance Matrix: A Mathematical Synthesis
  of the h-Index and the Integrated Impact Indicator (I3)</title><categories>cs.DL</categories><comments>Journal of the American Society for Information Science and
  Technology (forthcoming)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The h-index provides us with nine natural classes which can be written as a
matrix of three vectors. The three vectors are: X=(X1, X2, X3) indicate
publication distribution in the h-core, the h-tail, and the uncited ones,
respectively; Y=(Y1, Y2, Y3) denote the citation distribution of the h-core,
the h-tail and the so-called &quot;excess&quot; citations (above the h-threshold),
respectively; and Z=(Z1, Z2, Z3)= (Y1-X1, Y2-X2, Y3-X3). The matrix V=(X,Y,Z)T
constructs a measure of academic performance, in which the nine numbers can all
be provided with meanings in different dimensions. The &quot;academic trace&quot; tr(V)
of this matrix follows naturally, and contributes a unique indicator for total
academic achievements by summarizing and weighting the accumulation of
publications and citations. This measure can also be used to combine the
advantages of the h-index and the Integrated Impact Indicator (I3) into a
single number with a meaningful interpretation of the values. We illustrate the
use of tr(V) for the cases of two journal sets, two universities, and ourselves
as two individual authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3617</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3617</id><created>2013-07-13</created><updated>2015-06-12</updated><authors><author><keyname>Kanade</keyname><forenames>Varun</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author></authors><title>MCMC Learning</title><categories>cs.LG stat.ML</categories><comments>28 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of learning under the uniform distribution is rich and deep, with
connections to cryptography, computational complexity, and the analysis of
boolean functions to name a few areas. This theory however is very limited due
to the fact that the uniform distribution and the corresponding Fourier basis
are rarely encountered as a statistical model.
  A family of distributions that vastly generalizes the uniform distribution on
the Boolean cube is that of distributions represented by Markov Random Fields
(MRF). Markov Random Fields are one of the main tools for modeling high
dimensional data in many areas of statistics and machine learning.
  In this paper we initiate the investigation of extending central ideas,
methods and algorithms from the theory of learning under the uniform
distribution to the setup of learning concepts given examples from MRF
distributions. In particular, our results establish a novel connection between
properties of MCMC sampling of MRFs and learning under the MRF distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3621</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3621</id><created>2013-07-13</created><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>A Polynomial-time Approximation Scheme for Fault-tolerant Distributed
  Storage</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a problem which has received considerable attention in systems
literature because of its applications to routing in delay tolerant networks
and replica placement in distributed storage systems. In abstract terms the
problem can be stated as follows: Given a random variable $X$ generated by a
known product distribution over $\{0,1\}^n$ and a target value $0 \leq \theta
\leq 1$, output a non-negative vector $w$, with $\|w\|_1 \le 1$, which
maximizes the probability of the event $w \cdot X \ge \theta$. This is a
challenging non-convex optimization problem for which even computing the value
$\Pr[w \cdot X \ge \theta]$ of a proposed solution vector $w$ is #P-hard.
  We provide an additive EPTAS for this problem which, for constant-bounded
product distributions, runs in $ \poly(n) \cdot 2^{\poly(1/\eps)}$ time and
outputs an $\eps$-approximately optimal solution vector $w$ for this problem.
Our approach is inspired by, and extends, recent structural results from the
complexity-theoretic study of linear threshold functions. Furthermore, in spite
of the objective function being non-smooth, we give a \emph{unicriterion} PTAS
while previous work for such objective functions has typically led to a
\emph{bicriterion} PTAS. We believe our techniques may be applicable to get
unicriterion PTAS for other non-smooth objective functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3625</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3625</id><created>2013-07-13</created><updated>2013-12-22</updated><authors><author><keyname>Aliakbary</keyname><forenames>Sadegh</forenames></author><author><keyname>Habibi</keyname><forenames>Jafar</forenames></author><author><keyname>Movaghar</keyname><forenames>Ali</forenames></author></authors><title>Quantification and Comparison of Degree Distributions in Complex
  Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degree distribution is an important characteristic of complex networks.
In many applications, quantification of degree distribution in the form of a
fixed-length feature vector is a necessary step. On the other hand, we often
need to compare the degree distribution of two given networks and extract the
amount of similarity between the two distributions. In this paper, we propose a
novel method for quantification of the degree distributions in complex
networks. Based on this quantification method,a new distance function is also
proposed for degree distributions, which captures the differences in the
overall structure of the two given distributions. The proposed method is able
to effectively compare networks even with different scales, and outperforms the
state of the art methods considerably, with respect to the accuracy of the
distance function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3626</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3626</id><created>2013-07-13</created><authors><author><keyname>Aliakbary</keyname><forenames>Sadegh</forenames></author><author><keyname>Motallebi</keyname><forenames>Sadegh</forenames></author><author><keyname>Habibi</keyname><forenames>Jafar</forenames></author><author><keyname>Movaghar</keyname><forenames>Ali</forenames></author></authors><title>Learning an Integrated Distance Metric for Comparing Structure of
  Complex Networks</title><categories>cs.SI cs.AI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph comparison plays a major role in many network applications. We often
need a similarity metric for comparing networks according to their structural
properties. Various network features - such as degree distribution and
clustering coefficient - provide measurements for comparing networks from
different points of view, but a global and integrated distance metric is still
missing. In this paper, we employ distance metric learning algorithms in order
to construct an integrated distance metric for comparing structural properties
of complex networks. According to natural witnesses of network similarities
(such as network categories) the distance metric is learned by the means of a
dataset of some labeled real networks. For evaluating our proposed method which
is called NetDistance, we applied it as the distance metric in
K-nearest-neighbors classification. Empirical results show that NetDistance
outperforms previous methods, at least 20 percent, with respect to precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3630</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3630</id><created>2013-07-13</created><updated>2014-01-24</updated><authors><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Bian</keyname><forenames>Kaigui</forenames></author></authors><title>Mc-Dis: A Heterogeneous Neighbor Discovery Protocol for Multi-channel
  Wireless Networks</title><categories>cs.NI</categories><comments>There is a critical technical error in the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed wireless networks, neighbor discovery is one of the
bootstrapping primitives in supporting many important network functionalities.
Existing neighbor discovery protocols mostly assume a single-channel network
model and can only support a subset of duty cycles, thus limiting the energy
conservation levels of wireless devices. In this paper, we study the neighbor
discovery problem in multi-channel networks where the wireless nodes have
heterogeneous duty cycles, asynchronous clocks and asymmetrical channel
perceptions, which we formulate as heterogeneous neighbor discovery problem. We
first establish a performance bound for any neighbor discovery protocol by
relating the two performance metrics, discovery delay and diversity. We then
present the design, analysis and evaluation of Mc-Dis, a multi-channel neighbor
discovery protocol that can support can practically support almost all duty
cycles and guarantee discovery on every channel in multichannel networks even
when nodes have asynchronous clocks and asymmetrical channel perceptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3638</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3638</id><created>2013-07-13</created><authors><author><keyname>Soni</keyname><forenames>Gaurav</forenames></author><author><keyname>Chandravanshi</keyname><forenames>Kamlesh</forenames></author></authors><title>A Nobel Defence Scheme Against Selfish Node Attack in MANET</title><categories>cs.NI</categories><comments>5 tables, 4 figures, 13 references</comments><journal-ref>International Journal on Computational Science &amp;
  Applications(IJCSA) 2013 ISSN: 2200-0011 is a AIRCC journel</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security is one of the major issue in wired and wireless network but due to
the presence of centralized administration not difficult to find out
misbehavior in network other than in Mobile Ad hoc Network due to the absence
of centralized management and frequently changes in topology security is one of
a major issue in MANET. Only prevention methods for attack are not enough. In
this paper a new Intrusion Detection System (IDS) algorithm has proposed
against selfish node attack to secure MANET. Here the behavior of selfish node
is unnecessary flooding the information in network and block all types of
packets transferring between the reliable nodes. Proposed IDS Algorithm
identifies the behavior of selfish node and also blocked their misbehavior
activities. In case of selfish node attack network performance is almost
negligible but after applying IDS on attack network performance is enhanced up
to 92% and provides 0% Infection rate from attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3645</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3645</id><created>2013-07-13</created><authors><author><keyname>Molkaraie</keyname><forenames>Mehdi</forenames></author><author><keyname>Loeliger</keyname><forenames>Hans-Andrea</forenames></author></authors><title>Partition Function of the Ising Model via Factor Graph Duality</title><categories>cs.IT cond-mat.stat-mech math.IT physics.comp-ph stat.CO</categories><comments>Proc. IEEE Int. Symp. on Information Theory (ISIT), Istanbul, Turkey,
  July 7-12, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The partition function of a factor graph and the partition function of the
dual factor graph are related to each other by the normal factor graph duality
theorem. We apply this result to the classical problem of computing the
partition function of the Ising model. In the one-dimensional case, we thus
obtain an alternative derivation of the (well-known) analytical solution. In
the two-dimensional case, we find that Monte Carlo methods are much more
efficient on the dual graph than on the original graph, especially at low
temperature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3648</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3648</id><created>2013-07-13</created><updated>2014-05-19</updated><authors><author><keyname>Gajser</keyname><forenames>David</forenames></author></authors><title>Verifying Time Complexity of Deterministic Turing Machines</title><categories>cs.LO cs.CC cs.FL</categories><comments>18 pages, 1 figure</comments><doi>10.1016/j.tcs.2015.07.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, for all reasonable functions $T(n)=o(n\log n)$, we can
algorithmically verify whether a given one-tape Turing machine runs in time at
most $T(n)$. This is a tight bound on the order of growth for the function $T$
because we prove that, for $T(n)\geq(n+1)$ and $T(n)=\Omega(n\log n)$, there
exists no algorithm that would verify whether a given one-tape Turing machine
runs in time at most $T(n)$.
  We give results also for the case of multi-tape Turing machines. We show that
we can verify whether a given multi-tape Turing machine runs in time at most
$T(n)$ iff $T(n_0)&lt; (n_0+1)$ for some $n_0\in\mathbb{N}$.
  We prove a very general undecidability result stating that, for any class of
functions $\mathcal{F}$ that contains arbitrary large constants, we cannot
verify whether a given Turing machine runs in time $T(n)$ for some
$T\in\mathcal{F}$. In particular, we cannot verify whether a Turing machine
runs in constant, polynomial or exponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3650</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3650</id><created>2013-07-13</created><updated>2015-01-19</updated><authors><author><keyname>Boland</keyname><forenames>Natashia</forenames></author><author><keyname>Kalinowski</keyname><forenames>Thomas</forenames></author><author><keyname>Kaur</keyname><forenames>Simranjit</forenames></author></authors><title>Scheduling arc shut downs in a network to maximize flow over time with a
  bounded number of jobs per time period</title><categories>cs.DS cs.DM</categories><msc-class>90C10, 90B10, 68Q25</msc-class><doi>10.1007/s10878-015-9910-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of scheduling maintenance on arcs of a capacitated
network so as to maximize the total flow from a source node to a sink node over
a set of time periods. Maintenance on an arc shuts down the arc for the
duration of the period in which its maintenance is scheduled, making its
capacity zero for that period. A set of arcs is designated to have maintenance
during the planning period, which will require each to be shut down for exactly
one time period. In general this problem is known to be NP-hard, and several
special instance classes have been studied. Here we propose an additional
constraint which limits the number of maintenance jobs per time period, and we
study the impact of this on the complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3664</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3664</id><created>2013-07-13</created><updated>2014-04-15</updated><authors><author><keyname>Strohmeier</keyname><forenames>Martin</forenames></author><author><keyname>Lenders</keyname><forenames>Vincent</forenames></author><author><keyname>Martinovic</keyname><forenames>Ivan</forenames></author></authors><title>On the Security of the Automatic Dependent Surveillance-Broadcast
  Protocol</title><categories>cs.CR cs.NI</categories><comments>Survey, 22 Pages, 21 Figures</comments><report-no>RR-13-10</report-no><doi>10.1109/COMST.2014.2365951</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic dependent surveillance-broadcast (ADS-B) is the communications
protocol currently being rolled out as part of next generation air
transportation systems. As the heart of modern air traffic control, it will
play an essential role in the protection of two billion passengers per year,
besides being crucial to many other interest groups in aviation. The inherent
lack of security measures in the ADS-B protocol has long been a topic in both
the aviation circles and in the academic community. Due to recently published
proof-of-concept attacks, the topic is becoming ever more pressing, especially
with the deadline for mandatory implementation in most airspaces fast
approaching.
  This survey first summarizes the attacks and problems that have been reported
in relation to ADS-B security. Thereafter, it surveys both the theoretical and
practical efforts which have been previously conducted concerning these issues,
including possible countermeasures. In addition, the survey seeks to go beyond
the current state of the art and gives a detailed assessment of security
measures which have been developed more generally for related wireless networks
such as sensor networks and vehicular ad hoc networks, including a taxonomy of
all considered approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3667</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3667</id><created>2013-07-13</created><updated>2014-03-08</updated><authors><author><keyname>Coniglio</keyname><forenames>Marcelo</forenames></author><author><keyname>Esteva</keyname><forenames>Francesc</forenames></author><author><keyname>Godo</keyname><forenames>Llu&#xed;s</forenames></author></authors><title>Logics of formal inconsistency arising from systems of fuzzy logic</title><categories>math.LO cs.AI cs.LO</categories><comments>Revised and improved final version. 33 pages, 3 figures</comments><msc-class>03B52, 03B53</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the meeting of fuzzy logic with paraconsistency in a very
precise and foundational way. Specifically, in this paper we introduce
expansions of the fuzzy logic MTL by means of primitive operators for
consistency and inconsistency in the style of the so-called Logics of Formal
Inconsistency (LFIs). The main novelty of the present approach is the
definition of postulates for this type of operators over MTL-algebras, leading
to the definition and axiomatization of a family of logics, expansions of MTL,
whose degree-preserving counterpart are paraconsistent and moreover LFIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3673</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3673</id><created>2013-07-13</created><authors><author><keyname>Ntoulas</keyname><forenames>Alexandros</forenames></author><author><keyname>Alonso</keyname><forenames>Omar</forenames></author><author><keyname>Kandylas</keyname><forenames>Vasilis</forenames></author></authors><title>A Data Management Approach for Dataset Selection Using Human Computation</title><categories>cs.LG cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the number of applications that use machine learning algorithms increases,
the need for labeled data useful for training such algorithms intensifies.
  Getting labels typically involves employing humans to do the annotation,
which directly translates to training and working costs. Crowdsourcing
platforms have made labeling cheaper and faster, but they still involve
significant costs, especially for the cases where the potential set of
candidate data to be labeled is large. In this paper we describe a methodology
and a prototype system aiming at addressing this challenge for Web-scale
problems in an industrial setting. We discuss ideas on how to efficiently
select the data to use for training of machine learning algorithms in an
attempt to reduce cost. We show results achieving good performance with reduced
cost by carefully selecting which instances to label. Our proposed algorithm is
presented as part of a framework for managing and generating training datasets,
which includes, among other components, a human computation element.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3675</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3675</id><created>2013-07-13</created><authors><author><keyname>Dyer</keyname><forenames>Chris</forenames></author></authors><title>Minimum Error Rate Training and the Convex Hull Semiring</title><categories>cs.LG</categories><acm-class>I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the line search used in the minimum error rate training algorithm
MERT as the &quot;inside score&quot; of a weighted proof forest under a semiring defined
in terms of well-understood operations from computational geometry. This
conception leads to a straightforward complexity analysis of the dynamic
programming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009)
and practical approaches to implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3681</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3681</id><created>2013-07-13</created><updated>2016-02-15</updated><authors><author><keyname>Avendano</keyname><forenames>Martin</forenames></author><author><keyname>Kogan</keyname><forenames>Roman</forenames></author><author><keyname>Nisse</keyname><forenames>Mounir</forenames></author><author><keyname>Rojas</keyname><forenames>J. Maurice</forenames></author></authors><title>Metric Estimates and Membership Complexity for Archimedean Amoebae and
  Tropical Hypersurfaces</title><categories>math.AG cs.CC</categories><comments>Major revision: 21 pages, 5 figures. This version dramatically
  improves the metric bounds in the univariate case, and proves optimality for
  the first multivariate metric bound. Also, the second multivariate bound is
  corrected. Additional commentary on Maslov dequantization, and new proofs
  added to make the whole paper self-contained</comments><msc-class>Primary 14T99, 52B70, Secondary 14Q20, 52C07, 65Y20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given any complex Laurent polynomial f, Amoeba(f) is the image of the complex
zero set of f under the coordinate-wise log absolute value map. We give an
efficiently constructible polyhedral approximation, ArchTrop(f), of Amoeba(f),
and derive explicit upper and lower bounds, solely as a function of the
sparsity of f, for the Hausdorff distance between these two sets. We thus
obtain an Archimedean analogue of Kapranov's Non-Archimedean Amoeba Theorem and
a higher-dimensional extension of earlier estimates of Mikhalkin and Ostrowski.
We also show that deciding whether a given point is in ArchTrop(f) is doable in
polynomial-time, for any fixed dimension, unlike the corresponding question for
Amoeba(f), which is NP-hard already in one variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3682</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3682</id><created>2013-07-13</created><authors><author><keyname>Bakhova</keyname><forenames>Maiia</forenames></author></authors><title>A reduction of 3-SAT problem to Buchberger algorithm</title><categories>cs.CC</categories><msc-class>14Q20, 68Q15, 13P10</msc-class><acm-class>F.2.1; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a number of known NP class problems, and majority of them have been
shown to be equivalent to others. In particular now it is clear that
construction of a Gr\&quot;{o}bner basis (or Buchberger algorithm) must be one of
equivalent problems, but there was no example. In the following paper the
reduction is constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3687</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3687</id><created>2013-07-13</created><authors><author><keyname>Zhao</keyname><forenames>Junzhou</forenames></author></authors><title>On Analyzing Estimation Errors due to Constrained Connections in Online
  Review Systems</title><categories>cs.SI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constrained connection is the phenomenon that a reviewer can only review a
subset of products/services due to narrow range of interests or limited
attention capacity. In this work, we study how constrained connections can
affect estimation performance in online review systems (ORS). We find that
reviewers' constrained connections will cause poor estimation performance, both
from the measurements of estimation accuracy and Bayesian Cramer Rao lower
bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3690</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3690</id><created>2013-07-13</created><authors><author><keyname>Saligram</keyname><forenames>Rakshith</forenames></author><author><keyname>Hegde</keyname><forenames>Shrihari Shridhar</forenames></author><author><keyname>Kulkarni</keyname><forenames>Shashidhar A</forenames></author><author><keyname>Bhagyalakshmi</keyname><forenames>H. R.</forenames></author><author><keyname>Venkatesha</keyname><forenames>M. K.</forenames></author></authors><title>Design of Parity Preserving Logic Based Fault Tolerant Reversible
  Arithmetic Logic Unit</title><categories>cs.AR cs.ET</categories><comments>15 pages, 20 Figures, 8 Tables, Iinternational Journal of VLSI Design
  and Communication Systems, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible Logic is gaining significant consideration as the potential logic
design style for implementation in modern nanotechnology and quantum computing
with minimal impact on physical entropy .Fault Tolerant reversible logic is one
class of reversible logic that maintain the parity of the input and the
outputs. Significant contributions have been made in the literature towards the
design of fault tolerant reversible logic gate structures and arithmetic units,
however, there are not many efforts directed towards the design of fault
tolerant reversible ALUs. Arithmetic Logic Unit (ALU) is the prime performing
unit in any computing device and it has to be made fault tolerant. In this
paper we aim to design one such fault tolerant reversible ALU that is
constructed using parity preserving reversible logic gates. The designed ALU
can generate up to seven Arithmetic operations and four logical operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3692</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3692</id><created>2013-07-13</created><authors><author><keyname>Miller</keyname><forenames>Gary L.</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Xu</keyname><forenames>Shen Chen</forenames></author></authors><title>Parallel Graph Decompositions Using Random Shifts</title><categories>cs.DS</categories><acm-class>F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show an improved parallel algorithm for decomposing an undirected
unweighted graph into small diameter pieces with a small fraction of the edges
in between. These decompositions form critical subroutines in a number of graph
algorithms. Our algorithm builds upon the shifted shortest path approach
introduced in [Blelloch, Gupta, Koutis, Miller, Peng, Tangwongsan, SPAA 2011].
By combining various stages of the previous algorithm, we obtain a
significantly simpler algorithm with the same asymptotic guarantees as the best
sequential algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3696</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3696</id><created>2013-07-14</created><updated>2013-07-25</updated><authors><author><keyname>Genin</keyname><forenames>Daniel</forenames></author><author><keyname>Splett</keyname><forenames>Jolene</forenames></author></authors><title>Where in the Internet is congestion?</title><categories>cs.NI cs.SI</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Understanding the distribution of congestion in the Internet is a
long-standing problem. Using data from the SamKnows US broadband access network
measurement study, commissioned by the FCC, we explore patterns of congestion
distribution in DSL and cable Internet service provider (ISP) networks. Using
correlation-based analysis we estimate prevalence of congestion in the
periphery versus the core of ISP networks. We show that there are significant
differences in congestion levels and its distribution between DSL and cable ISP
networks and identify bottleneck sections in each type of network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3699</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3699</id><created>2013-07-14</created><authors><author><keyname>Chung</keyname><forenames>Kai-Min</forenames></author><author><keyname>Liu</keyname><forenames>Zhenming</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author></authors><title>Statistically-secure ORAM with $\tilde{O}(\log^2 n)$ Overhead</title><categories>cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate a simple, statistically secure, ORAM with computational
overhead $\tilde{O}(\log^2 n)$; previous ORAM protocols achieve only
computational security (under computational assumptions) or require
$\tilde{\Omega}(\log^3 n)$ overheard. An additional benefit of our ORAM is its
conceptual simplicity, which makes it easy to implement in both software and
(commercially available) hardware.
  Our construction is based on recent ORAM constructions due to Shi, Chan,
Stefanov, and Li (Asiacrypt 2011) and Stefanov and Shi (ArXiv 2012), but with
some crucial modifications in the algorithm that simplifies the ORAM and enable
our analysis. A central component in our analysis is reducing the analysis of
our algorithm to a &quot;supermarket&quot; problem; of independent interest (and of
importance to our analysis,) we provide an upper bound on the rate of &quot;upset&quot;
customers in the &quot;supermarket&quot; problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3701</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3701</id><created>2013-07-14</created><authors><author><keyname>Kuchi</keyname><forenames>Kiran</forenames></author></authors><title>Exploiting Spatial Interference Alignment and Opportunistic Scheduling
  in the Downlink of Interference Limited Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the performance of single stream and multi-stream
spatial multiplexing (SM) systems employing opportunistic scheduling in the
presence of interference. In the proposed downlink framework, every active user
reports the post-processing signal-to-interference-plus-noise-power-ratio
(post-SINR) or the receiver specific mutual information (MI) to its own
transmitter using a feedback channel. The combination of scheduling and
multi-antenna receiver processing leads to substantial interference suppression
gain. Specifically, we show that opportunistic scheduling exploits spatial
interference alignment (SIA) property inherent to a multi-user system for
effective interference mitigation. We obtain bounds for the outage probability
and the sum outage capacity for single stream and multi stream SM employing
real or complex encoding for a symmetric interference channel model.
  The techniques considered in this paper are optimal in different operating
regimes. We show that the sum outage capacity can be maximized by reducing the
SM rate to a value less than the maximum allowed value. The optimum SM rate
depends on the number of interferers and the number of available active users.
In particular, we show that the generalized multi-user SM (MU SM) method
employing real-valued encoding provides a performance that is either
comparable, or significantly higher than that of MU SM employing complex
encoding. A combination of analysis and simulation is used to describe the
trade-off between the multiplexing rate and sum outage capacity for different
antenna configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3712</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3712</id><created>2013-07-14</created><authors><author><keyname>Raza</keyname><forenames>Khalid</forenames></author><author><keyname>Parveen</keyname><forenames>Rafat</forenames></author></authors><title>Reconstruction of gene regulatory network of colon cancer using
  information theoretic approach</title><categories>cs.CE cs.ET cs.SY q-bio.MN</categories><comments>5 pages, 4 figures, 1 table</comments><journal-ref>Confluence 2013: The Next Generation Information Technology Summit
  (4th International Conference), pp. 461 - 466</journal-ref><doi>10.1049/cp.2013.2357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstruction of gene regulatory networks or 'reverse-engineering' is a
process of identifying gene interaction networks from experimental microarray
gene expression profile through computation techniques. In this paper, we tried
to reconstruct cancer-specific gene regulatory network using information
theoretic approach - mutual information. The considered microarray data
consists of large number of genes with 20 samples - 12 samples from colon
cancer patient and 8 from normal cell. The data has been preprocessed and
normalized. A t-test statistics has been applied to filter differentially
expressed genes. The interaction between filtered genes has been computed using
mutual information and ten different networks has been constructed with varying
number of interactions ranging from 30 to 500. We performed the topological
analysis of the reconstructed network, revealing a large number of interactions
in colon cancer. Finally, validation of the inferred results has been done with
available biological databases and literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3715</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3715</id><created>2013-07-14</created><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author></authors><title>Large System Analysis of Cooperative Multi-cell Downlink Transmission
  via Regularized Channel Inversion with Imperfect CSIT</title><categories>cs.IT math.IT</categories><comments>13 pages, 9 figures, IEEE Transactions on Wireless Communications</comments><report-no>TW-Mar-12-</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the ergodic sum-rate of a multi-cell downlink
system with base station (BS) cooperation using regularized zero-forcing (RZF)
precoding. Our model assumes that the channels between BSs and users have
independent spatial correlations and imperfect channel state information at the
transmitter (CSIT) is available. Our derivations are based on large dimensional
random matrix theory (RMT) under the assumption that the numbers of antennas at
the BS and users approach to infinity with some fixed ratios. In particular, a
deterministic equivalent expression of the ergodic sum-rate is obtained and is
instrumental in getting insight about the joint operations of BSs, which leads
to an efficient method to find the asymptotic-optimal regularization parameter
for the RZF. In another application, we use the deterministic channel rate to
study the optimal feedback bit allocation among the BSs for maximizing the
ergodic sum-rate, subject to a total number of feedback bits constraint. By
inspecting the properties of the allocation, we further propose a scheme to
greatly reduce the search space for optimization. Simulation results
demonstrate that the ergodic sum-rates achievable by a subspace search provides
comparable results to those by an exhaustive search under various typical
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3722</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3722</id><created>2013-07-14</created><authors><author><keyname>Cheng</keyname><forenames>Chih-Hong</forenames></author><author><keyname>Lee</keyname><forenames>Edward A.</forenames></author></authors><title>Numerical LTL Synthesis for Cyber-Physical Systems</title><categories>cs.SE cs.LO cs.SY</categories><comments>10 pages; work-in-progress report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-physical systems (CPS) are systems that interact with the physical
world via sensors and actuators. In such a system, the reading of a sensor
represents measures of a physical quantity, and sensor values are often reals
ranged over bounded intervals. The implementation of control laws is based on
nonlinear numerical computations over the received sensor values. Synthesizing
controllers fulfilling features within CPS brings a huge challenge to the
research community in formal methods, as most of the works in automatic
controller synthesis (LTL synthesis) are restricted to specifications having a
few discrete inputs within the Boolean domain.
  In this report, we present a novel approach that addresses the above
challenge to synthesize controllers for CPS. Our core methodology, called
numerical LTL synthesis, extends LTL synthesis by using inputs or outputs in
real numbers and by allowing predicates of polynomial constraints to be defined
within an LTL formula as specification. The synthesis algorithm is based on an
interplay between an LTL synthesis engine which handles the pseudo-Boolean
structure, together with a nonlinear constraint validity checker which tests
the (in)feasibility of a (counter-)strategy. The methodology is integrated
within the CPS research framework Ptolemy II via the development of an LTL
synthesis module G4LTL and a validity checker JBernstein. Although we only
target the theory of nonlinear real arithmetic, the use of pseudo-Boolean
synthesis framework also allows an easy extension to embed a richer set of
theories, making the technique applicable to a much broader audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3724</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3724</id><created>2013-07-14</created><authors><author><keyname>Kuchi</keyname><forenames>Kiran</forenames></author></authors><title>Limiting Performance of Conventional and Widely Linear DFT-precoded-OFDM
  Receivers in Wideband Frequency Selective Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the limiting behavior of linear and decision feedback
equalizers (DFEs) in single/multiple antenna systems employing
real/complex-valued modulation alphabets. The wideband frequency selective
channel is modeled using a Rayleigh fading channel model with infinite number
of time domain channel taps. Using this model, we show that the considered
equalizers offer a fixed post signal-to-noise-ratio (post-SNR) at the equalizer
output that is close to the matched filter bound (MFB). General expressions for
the post-SNR are obtained for zero-forcing (ZF) based conventional receivers as
well as for the case of receivers employing widely linear (WL) processing.
Simulation is used to study the bit error rate (BER) performance of both MMSE
and ZF based receivers. Results show that the considered receivers
advantageously exploit the rich frequency selective channel to mitigate both
fading and inter-symbol-interference (ISI) while offering a performance
comparable to the MFB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3736</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3736</id><created>2013-07-14</created><authors><author><keyname>Azar</keyname><forenames>Pablo D.</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author><author><keyname>Weinberg</keyname><forenames>S. Matthew</forenames></author></authors><title>Prophet Inequalities with Limited Information</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical prophet inequality, a gambler observes a sequence of
stochastic rewards $V_1,...,V_n$ and must decide, for each reward $V_i$,
whether to keep it and stop the game or to forfeit the reward forever and
reveal the next value $V_i$. The gambler's goal is to obtain a constant
fraction of the expected reward that the optimal offline algorithm would get.
Recently, prophet inequalities have been generalized to settings where the
gambler can choose $k$ items, and, more generally, where he can choose any
independent set in a matroid. However, all the existing algorithms require the
gambler to know the distribution from which the rewards $V_1,...,V_n$ are
drawn.
  The assumption that the gambler knows the distribution from which
$V_1,...,V_n$ are drawn is very strong. Instead, we work with the much simpler
assumption that the gambler only knows a few samples from this distribution. We
construct the first single-sample prophet inequalities for many settings of
interest, whose guarantees all match the best possible asymptotically,
\emph{even with full knowledge of the distribution}. Specifically, we provide a
novel single-sample algorithm when the gambler can choose any $k$ elements
whose analysis is based on random walks with limited correlation. In addition,
we provide a black-box method for converting specific types of solutions to the
related \emph{secretary problem} to single-sample prophet inequalities, and
apply it to several existing algorithms. Finally, we provide a constant-sample
prophet inequality for constant-degree bipartite matchings.
  We apply these results to design the first posted-price and multi-dimensional
auction mechanisms with limited information in settings with asymmetric
bidders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3741</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3741</id><created>2013-07-14</created><authors><author><keyname>Xia</keyname><forenames>Jing</forenames></author><author><keyname>Xiong</keyname><forenames>Maosheng</forenames></author></authors><title>On a question of Babadi and Tarokh</title><categories>math.NT cs.IT math.IT</categories><msc-class>11L99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent remarkable paper, Babadi and Tarokh proved the &quot;randomness&quot; of
sequences arising from binary linear block codes in the sense of spectral
distribution, provided that their dual distances are sufficiently large.
However, numerical experiments conducted by the authors revealed that Gold
sequences which have dual distance 5 also satisfy such randomness property.
Hence the interesting question was raised as to whether or not the stringent
requirement of large dual distances can be relaxed in the theorem in order to
explain the randomness of Gold sequences. This paper improves their result on
several fronts and provides an affirmative answer to this question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3753</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3753</id><created>2013-07-14</created><authors><author><keyname>Velema</keyname><forenames>Maria</forenames></author></authors><title>Classical Encryption and Authentication under Quantum Attacks</title><categories>cs.CR</categories><comments>MSc thesis Master in Logic, University of Amsterdam</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Post-quantum cryptography studies the security of classical, i.e. non-quantum
cryptographic protocols against quantum attacks. Until recently, the considered
adversaries were assumed to use quantum computers and behave like classical
adversaries otherwise. A more conservative approach is to assume that also the
communication between the honest parties and the adversary is (partly) quantum.
We discuss several options to define secure encryption and authentication
against these stronger adversaries who can carry out 'superposition attacks'.
We re-prove a recent result of Boneh and Zhandry, stating that a uniformly
random function (and hence also a quantum-secure pseudorandom function) can
serve as a message-authentication code which is secure, even if the adversary
can evaluate this function in superposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3755</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3755</id><created>2013-07-14</created><authors><author><keyname>Kari</keyname><forenames>Lila</forenames><affiliation>Department of Computer Science, University of Western Ontario, Canada</affiliation></author><author><keyname>Hill</keyname><forenames>Kathleen A.</forenames><affiliation>Department of Biology, University of Western Ontario, Canada</affiliation></author><author><keyname>Sayem</keyname><forenames>Abu Sadat</forenames><affiliation>Department of Computer Science, University of Western Ontario, Canada</affiliation></author><author><keyname>Bryans</keyname><forenames>Nathaniel</forenames><affiliation>Microsoft Corporation</affiliation></author><author><keyname>Davis</keyname><forenames>Katelyn</forenames><affiliation>Department of Biology, University of Western Ontario, Canada</affiliation></author><author><keyname>Dattani</keyname><forenames>Nikesh S.</forenames><affiliation>Department of Chemistry, Oxford University, UK</affiliation></author></authors><title>Map of Life: Measuring and Visualizing Species' Relatedness with
  &quot;Molecular Distance Maps&quot;</title><categories>q-bio.GN cs.CV q-bio.PE q-bio.QM</categories><comments>13 pages, 8 figures. Funded by: NSERC/CRSNG (Natural Science &amp;
  Engineering Research Council of Canada / Conseil de recherches en sciences
  naturelles et en g\'enie du Canada), and the Oxford University Press.
  Acknowledgements: Ronghai Tu, Tao Tao, Steffen Kopecki, Andre Lachance,
  Jeremy McNeil, Greg Thorn, Oxford University Mathematical Institute</comments><msc-class>92, 68</msc-class><acm-class>J.3; J.2; I.4; I.5; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel combination of methods that (i) portrays quantitative
characteristics of a DNA sequence as an image, (ii) computes distances between
these images, and (iii) uses these distances to output a map wherein each
sequence is a point in a common Euclidean space. In the resulting &quot;Molecular
Distance Map&quot; each point signifies a DNA sequence, and the geometric distance
between any two points reflects the degree of relatedness between the
corresponding sequences and species.
  Molecular Distance Maps present compelling visual representations of
relationships between species and could be used for taxonomic clarifications,
for species identification, and for studies of evolutionary history. One of the
advantages of this method is its general applicability since, as sequence
alignment is not required, the DNA sequences chosen for comparison can be
completely different regions in different genomes. In fact, this method can be
used to compare any two DNA sequences. For example, in our dataset of 3,176
mitochondrial DNA sequences, it correctly finds the mtDNA sequences most
closely related to that of the anatomically modern human (the Neanderthal, the
Denisovan, and the chimp), and it finds that the sequence most different from
it belongs to a cucumber. Furthermore, our method can be used to compare real
sequences to artificial, computer-generated, DNA sequences. For example, it is
used to determine that the distances between a Homo sapiens sapiens mtDNA and
artificial sequences of the same length and same trinucleotide frequencies can
be larger than the distance between the same human mtDNA and the mtDNA of a
fruit-fly.
  We demonstrate this method's promising potential for taxonomical
clarifications by applying it to a diverse variety of cases that have been
historically controversial, such as the genus Polypterus, the family Tarsiidae,
and the vast (super)kingdom Protista.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3757</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3757</id><created>2013-07-14</created><updated>2013-10-21</updated><authors><author><keyname>Gu</keyname><forenames>Albert</forenames></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Kumar</keyname><forenames>Amit</forenames></author></authors><title>The Power of Deferral: Maintaining a Constant-Competitive Steiner Tree
  Online</title><categories>cs.DS</categories><comments>An extended abstract appears in the 45th ACM Symposium on the Theory
  of Computing (STOC), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the online Steiner tree problem, a sequence of points is revealed
one-by-one: when a point arrives, we only have time to add a single edge
connecting this point to the previous ones, and we want to minimize the total
length of edges added. For two decades, we know that the greedy algorithm
maintains a tree whose cost is O(log n) times the Steiner tree cost, and this
is best possible. But suppose, in addition to the new edge we add, we can
change a single edge from the previous set of edges: can we do much better? Can
we maintain a tree that is constant-competitive?
  We answer this question in the affirmative. We give a primal-dual algorithm,
and a novel dual-based analysis, that makes only a single swap per step (in
addition to adding the edge connecting the new point to the previous ones), and
such that the tree's cost is only a constant times the optimal cost.
  Previous results for this problem gave an algorithm that performed an
amortized constant number of swaps: for each n, the number of swaps in the
first n steps was O(n). We also give a simpler tight analysis for this
amortized case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3759</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3759</id><created>2013-07-14</created><authors><author><keyname>Martyushev</keyname><forenames>Evgeniy</forenames></author></authors><title>A Minimal Six-Point Auto-Calibration Algorithm</title><categories>cs.CV</categories><comments>7 pages, 4 figures</comments><journal-ref>Proceedings of the 23rd International Conference on Computer
  Graphics and Vision, September 16-20, 2013 Vladivostok, Russia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-iterative auto-calibration algorithm is presented. It deals with a
minimal set of six scene points in three views taken by a camera with fixed but
unknown intrinsic parameters. Calibration is based on the image correspondences
only. The algorithm is implemented and validated on synthetic image data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3760</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3760</id><created>2013-07-14</created><authors><author><keyname>Sasvari</keyname><forenames>Peter</forenames></author></authors><title>The Impacts of Using Business Information Systems on Operational
  Effectiveness in Hungary</title><categories>cs.CY</categories><comments>5 pages</comments><msc-class>94A15</msc-class><acm-class>H.1.1</acm-class><journal-ref>International Journal of Emerging Research in Management &amp;
  Technology, ISSN: 2278-9359 (Volume-2, Issue-4), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business expectations regarding the introduction of business information
systems were investigated according to company size categories. The results
clearly showed that according to the majority of the respondents the
information supply for decision-makers improved. In contrast, business
information systems as a means of improving competitiveness were only regarded
by corporations, this aspect was only around the average in the other company
size categories. The respondents evaluated to what extent the usage of business
information system provided assistance for their economic analyses. The
obtained results show that business information systems can be utilized well in
controlling and reporting. There are differences in their judgement by size
categories. Particularly corporations can take advantage of the support of
business information systems mainly in the field of planning, plan-actual
analysis and the exploration of cost reducing possibilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3766</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3766</id><created>2013-07-14</created><authors><author><keyname>Priscakova</keyname><forenames>Zuzana</forenames></author><author><keyname>Rabova</keyname><forenames>Ivana</forenames></author></authors><title>Model of solutions for data security in Cloud Computing</title><categories>cs.CR</categories><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology. 2013. Volume 3, p. 11-21. ISSN 2231-3605</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to develop a model to ensure data stored in the
cloud. Model based on situations that arise in a business environment. The
model also includes individual participants and their data operations.
Implementation of the model is transferred using UML. The model is divided into
7 modules. Each module is apparent from the terms of data security and
described specific situations when working with data. Based on this model it is
possible to convert the implementation of cloud into enterprise environments
with respect to data security in the firm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3780</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3780</id><created>2013-07-14</created><authors><author><keyname>Aref</keyname><forenames>Vahid</forenames></author><author><keyname>Schmalen</keyname><forenames>Laurent</forenames></author><author><keyname>Brink</keyname><forenames>Stephan ten</forenames></author></authors><title>On the Convergence Speed of Spatially Coupled LDPC Ensembles</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatially coupled low-density parity-check codes show an outstanding
performance under the low-complexity belief propagation (BP) decoding
algorithm. They exhibit a peculiar convergence phenomenon above the BP
threshold of the underlying non-coupled ensemble, with a wave-like convergence
propagating through the spatial dimension of the graph, allowing to approach
the MAP threshold. We focus on this particularly interesting regime in between
the BP and MAP thresholds.
  On the binary erasure channel, it has been proved that the information
propagates with a constant speed toward the successful decoding solution. We
derive an upper bound on the propagation speed, only depending on the basic
parameters of the spatially coupled code ensemble such as degree distribution
and the coupling factor $w$. We illustrate the convergence speed of different
code ensembles by simulation results, and show how optimizing degree profiles
helps to speed up the convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3782</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3782</id><created>2013-07-14</created><authors><author><keyname>Mahmoud</keyname><forenames>Karim M.</forenames></author></authors><title>Handwritten Digits Recognition using Deep Convolutional Neural Network:
  An Experimental Study using EBlearn</title><categories>cs.NE cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, results of an experimental study of a deep convolution neural
network architecture which can classify different handwritten digits using
EBLearn library are reported. The purpose of this neural network is to classify
input images into 10 different classes or digits (0-9) and to explore new
findings. The input dataset used consists of digits images of size 32X32 in
grayscale (MNIST dataset).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3785</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3785</id><created>2013-07-14</created><authors><author><keyname>Tossou</keyname><forenames>Aristide C. Y.</forenames></author><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author></authors><title>Probabilistic inverse reinforcement learning in unknown environments</title><categories>stat.ML cs.LG</categories><comments>UAI 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning by demonstration from agents acting in
unknown stochastic Markov environments or games. Our aim is to estimate agent
preferences in order to construct improved policies for the same task that the
agents are trying to solve. To do so, we extend previous probabilistic
approaches for inverse reinforcement learning in known MDPs to the case of
unknown dynamics or opponents. We do this by deriving two simplified
probabilistic models of the demonstrator's policy and utility. For
tractability, we use maximum a posteriori estimation rather than full Bayesian
inference. Under a flat prior, this results in a convex optimisation problem.
We find that the resulting algorithms are highly competitive against a variety
of other methods for inverse reinforcement learning that do have knowledge of
the dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3791</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3791</id><created>2013-07-14</created><authors><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Valaee</keyname><forenames>Shahrokh</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Partially Blind Instantly Decodable Network Codes for Lossy Feedback
  Environment</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the multicast completion and decoding delay
minimization problems of instantly decodable network coding (IDNC) in the case
of lossy feedback. In such environments, the sender falls into uncertainties
about packet reception at the different receivers, which forces it to perform
partially blind selections of packet combinations in subsequent transmissions.
To determine efficient partially blind policies that handle the completion and
decoding delays of IDNC in such environment, we first extend the perfect
feedback formulation in [2], [3] to the lossy feedback environment, by
incorporating the uncertainties resulting from unheard feedback events in these
formulations. For the completion delay problem, we use this formulation to
identify the maximum likelihood state of the network in events of unheard
feedback, and employ it to design a partially blind graph update extension to
the multicast IDNC algorithm in [3]. For the decoding delay problem, we derive
an expression for the expected decoding delay increment for any arbitrary
transmission. This expression is then used to derive the optimal policy to
reduce the decoding delay in such lossy feedback environment. Results show that
our proposed solution both outperforms other approaches and achieves a
tolerable degradation even at relatively high feedback loss rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3794</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3794</id><created>2013-07-14</created><updated>2013-11-10</updated><authors><author><keyname>Bhaskar</keyname><forenames>Umang</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author><author><keyname>Schulman</keyname><forenames>Leonard J.</forenames></author></authors><title>The Network Improvement Problem for Equilibrium Routing</title><categories>cs.GT cs.DS</categories><comments>27 pages (including abstract), 3 figures</comments><acm-class>G.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In routing games, agents pick their routes through a network to minimize
their own delay. A primary concern for the network designer in routing games is
the average agent delay at equilibrium. A number of methods to control this
average delay have received substantial attention, including network tolls,
Stackelberg routing, and edge removal.
  A related approach with arguably greater practical relevance is that of
making investments in improvements to the edges of the network, so that, for a
given investment budget, the average delay at equilibrium in the improved
network is minimized. This problem has received considerable attention in the
literature on transportation research and a number of different algorithms have
been studied. To our knowledge, none of this work gives guarantees on the
output quality of any polynomial-time algorithm. We study a model for this
problem introduced in transportation research literature, and present both
hardness results and algorithms that obtain nearly optimal performance
guarantees.
  - We first show that a simple algorithm obtains good approximation guarantees
for the problem. Despite its simplicity, we show that for affine delays the
approximation ratio of 4/3 obtained by the algorithm cannot be improved.
  - To obtain better results, we then consider restricted topologies. For
graphs consisting of parallel paths with affine delay functions we give an
optimal algorithm. However, for graphs that consist of a series of parallel
links, we show the problem is weakly NP-hard.
  - Finally, we consider the problem in series-parallel graphs, and give an
FPTAS for this case.
  Our work thus formalizes the intuition held by transportation researchers
that the network improvement problem is hard, and presents topology-dependent
algorithms that have provably tight approximation guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3796</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3796</id><created>2013-07-14</created><updated>2013-09-23</updated><authors><author><keyname>Ahmed</keyname><forenames>Elsayed</forenames></author><author><keyname>Eltawil</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Self-Interference Cancellation with Nonlinear Distortion Suppression for
  Full-Duplex Systems</title><categories>cs.IT math.IT</categories><comments>To be presented in Asilomar Conference on Signals, Systems &amp;
  Computers (November 2013)</comments><doi>10.1109/ACSSC.2013.6810483</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In full-duplex systems, due to the strong self-interference signal, system
nonlinearities become a significant limiting factor that bounds the possible
cancellable self-interference power. In this paper, a self-interference
cancellation scheme for full-duplex orthogonal frequency division multiplexing
systems is proposed. The proposed scheme increases the amount of cancellable
self-interference power by suppressing the distortion caused by the transmitter
and receiver nonlinearities. An iterative technique is used to jointly estimate
the self-interference channel and the nonlinearity coefficients required to
suppress the distortion signal. The performance is numerically investigated
showing that the proposed scheme achieves a performance that is less than 0.5dB
off the performance of a linear full-duplex system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3797</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3797</id><created>2013-07-14</created><authors><author><keyname>Wang</keyname><forenames>X. Y.</forenames></author><author><keyname>Vilathgamuwa</keyname><forenames>D. M.</forenames></author><author><keyname>Choi</keyname><forenames>S. S.</forenames></author></authors><title>Energy Storage System Design for a Power Buffer System to Provide Load
  Ride-through</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of a power buffer to mitigate the negative impact of constant
power loads on voltage stability as well as enhancing ride-through capability
for the loads during upstream voltage disturbances is examined. The power
buffer adjusts its front-end converter control so that the buffer-load
combination would appear as a constant impedance load to the upstream supply
system when depressed voltage occurs. A battery energy-storage back-up source
within the buffer is activated to maintain the load power demand. It is shown
that the buffer performance is affected by the battery state of discharge and
discharge current. Analytical expressions are also derived to relate the
buffer-load ride-through capability with the battery state-of-discharge. The
most onerous buffer-battery condition under which the load-ride through can be
achieved has been identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3799</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3799</id><created>2013-07-14</created><authors><author><keyname>Vilathgamuwa</keyname><forenames>D. Mahinda</forenames></author><author><keyname>Wang</keyname><forenames>X. Y.</forenames></author><author><keyname>Tseng</keyname><forenames>King Jet</forenames></author><author><keyname>Gajanayake</keyname><forenames>C. J.</forenames></author></authors><title>Z-source Inverter Based Grid-interface For Variable-speed Permanent
  Magnet Wind Turbine Generators</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Z-source inverter based grid-interface for a variable-speed wind turbine
connected to a permanent magnet synchronous generator is proposed. A control
system is designed to harvest maximum wind energy under varied wind conditions
with the use of a permanent magnet synchronous generator, a diode-rectifier and
a Z-source inverter. Control systems for speed regulation of the generator and
for DC- and AC- sides of the Z-source inverter are implemented. Laboratory
experiments are used to verify the efficacy of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3802</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3802</id><created>2013-07-14</created><updated>2014-09-25</updated><authors><author><keyname>Norman</keyname><forenames>Joseph W.</forenames></author></authors><title>Probability Distinguishes Different Types of Conditional Statements</title><categories>math.LO cs.AI math.PR</categories><comments>Fixed a few typographical errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The language of probability is used to define several different types of
conditional statements. There are four principal types: subjunctive, material,
existential, and feasibility. Two further types of conditionals are defined
using the propositional calculus and Boole's mathematical logic:
truth-functional and Boolean feasibility (which turn out to be special cases of
probabilistic conditionals). Each probabilistic conditional is quantified by a
fractional parameter between zero and one that says whether it is purely
affirmative, purely negative, or intermediate in its sense. Conditionals can be
specialized further by their content to express factuality and
counterfactuality, and revised or reformulated to account for exceptions and
confounding factors. The various conditionals have distinct mathematical
representations: through intermediate probability expressions and logical
formulas, each conditional is eventually translated into a set of polynomial
equations and inequalities (with real coefficients). The polynomial systems
from different types of conditionals exhibit different patterns of behavior,
concerning for example opposing conditionals or false antecedents. Interesting
results can be computed from the relevant polynomial systems using well-known
methods from algebra and computer science. Among other benefits, the proposed
framework of analysis offers paraconsistent procedures for logical deduction
that produce such familiar results as modus ponens, transitivity, disjunction
introduction, and disjunctive syllogism; all while avoiding any explosion of
consequences from inconsistent premises. Several example problems from Goodman
and Adams are analyzed. A new perspective called polylogicism is presented:
mathematical logic that respects the diversity among conditionals in particular
and logic problems in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3809</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3809</id><created>2013-07-14</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>The Euler characteristic of an even-dimensional graph</title><categories>math.GT cs.DM</categories><comments>16 pages, 4 figures</comments><msc-class>05C50, 81Q10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We write the Euler characteristic X(G) of a four dimensional finite simple
geometric graph G=(V,E) in terms of the Euler characteristic X(G(w)) of
two-dimensional geometric subgraphs G(w). The Euler curvature K(x) of a four
dimensional graph satisfying the Gauss-Bonnet relation sum_x K(x) = X(G) can so
be rewritten as an average 1-E[K(x,f)]/2 over a collection two dimensional
&quot;sectional graph curvatures&quot; K(x,f) through x. Since scalar curvature, the
average of all these two dimensional curvatures through a point, is the
integrand of the Hilbert action, the integer 2-2 X(G) becomes an
integral-geometrically defined Hilbert action functional. The result has an
interpretation in the continuum for compact 4-manifolds M: the Euler curvature
K(x), the integrand in the classical Gauss-Bonnet-Chern theorem, can be seen as
an average over a probability space W of 1-K(x,w)/2 with curvatures K(x,w) of
compact 2-manifolds M(w). Also here, the Euler characteristic has an
interpretation of an exotic Hilbert action, in which sectional curvatures are
replaced by surface curvatures of integral geometrically defined random
two-dimensional sub-manifolds M(w) of M.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3810</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3810</id><created>2013-07-14</created><updated>2013-07-18</updated><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>Counting rooted forests in a network</title><categories>math.SP cs.DM cs.SI math-ph math.MP</categories><comments>13 pages, 6 figures, Since submitting the first version, we have
  learned that the forest theorem has already been proven by Chebotarev-Shamis.
  We prove a generalization of their theorem. The proof relies on a general new
  result in linear algebra and is different from the one given by Chebotarev
  and Shamis</comments><msc-class>05C50 05C30 05C05 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use a recently found generalization of the Cauchy-Binet theorem to give a
new proof of the Chebotarev-Shamis forest theorem telling that det(1+L) is the
number of rooted spanning forests in a finite simple graph G with Laplacian L.
More generally, we show that det(1+k L) is the number of rooted edge-k-colored
spanning forests in G. If a forest with an even number of edges is called even,
then det(1-L) is the difference between even and odd rooted spanning forests in
G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3811</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3811</id><created>2013-07-14</created><authors><author><keyname>Liu</keyname><forenames>Weifeng</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Cheng</keyname><forenames>Jun</forenames></author><author><keyname>Tang</keyname><forenames>Yuanyan</forenames></author></authors><title>Multiview Hessian Discriminative Sparse Coding for Image Annotation</title><categories>cs.MM cs.CV cs.IT math.IT</categories><comments>35 pages</comments><journal-ref>Computer vision and image understanding,118(2014) 50-60</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding represents a signal sparsely by using an overcomplete
dictionary, and obtains promising performance in practical computer vision
applications, especially for signal restoration tasks such as image denoising
and image inpainting. In recent years, many discriminative sparse coding
algorithms have been developed for classification problems, but they cannot
naturally handle visual data represented by multiview features. In addition,
existing sparse coding algorithms use graph Laplacian to model the local
geometry of the data distribution. It has been identified that Laplacian
regularization biases the solution towards a constant function which possibly
leads to poor extrapolating power. In this paper, we present multiview Hessian
discriminative sparse coding (mHDSC) which seamlessly integrates Hessian
regularization with discriminative sparse coding for multiview learning
problems. In particular, mHDSC exploits Hessian regularization to steer the
solution which varies smoothly along geodesics in the manifold, and treats the
label information as an additional view of feature for incorporating the
discriminative power for image annotation. We conduct extensive experiments on
PASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for image
annotation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3818</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3818</id><created>2013-07-15</created><authors><author><keyname>Dai</keyname><forenames>Xiongping</forenames></author><author><keyname>Huang</keyname><forenames>Tingwen</forenames></author><author><keyname>Huang</keyname><forenames>Yu</forenames></author><author><keyname>Xiao</keyname><forenames>Mingqing</forenames></author></authors><title>Chaotic Characteristics of Discrete-time Linear Inclusion Dynamical
  Systems</title><categories>cs.SY math.OC</categories><comments>7 pages; submitted</comments><msc-class>93C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the chaotic behavior of a discrete-time linear
inclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3822</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3822</id><created>2013-07-15</created><updated>2013-07-17</updated><authors><author><keyname>Wu</keyname><forenames>Bang Ye</forenames></author></authors><title>A simple approximation algorithm for the internal Steiner minimum tree</title><categories>cs.DS</categories><msc-class>68W05, 68W25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a metric graph $G=(V,E)$ and $R\subset V$, the internal Steiner minimum
tree problem asks for a minimum weight Steiner tree spanning $R$ such that
every vertex in $R$ is not a leaf. This note shows a simple polynomial-time
$2\rho$-approximation algorithm, in which $\rho$ is the approximation ratio for
the Steiner minimum tree problem. The result improves the previous best
approximation ratio $2\rho+1$ for the problem. The ratio is not currently best
but the algorithm is very simple.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3824</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3824</id><created>2013-07-15</created><authors><author><keyname>Burjorjee</keyname><forenames>Keki M.</forenames></author></authors><title>The Fundamental Learning Problem that Genetic Algorithms with Uniform
  Crossover Solve Efficiently and Repeatedly As Evolution Proceeds</title><categories>cs.NE cs.AI cs.CC cs.DM cs.LG</categories><comments>For an easy introduction to implicit concurrency (with animations),
  visit
  http://blog.hackingevolution.net/2013/03/24/implicit-concurrency-in-genetic-algorithms/</comments><acm-class>I.2.8; I.2.6; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes theoretical bonafides for implicit concurrent
multivariate effect evaluation--implicit concurrency for short---a broad and
versatile computational learning efficiency thought to underlie
general-purpose, non-local, noise-tolerant optimization in genetic algorithms
with uniform crossover (UGAs). We demonstrate that implicit concurrency is
indeed a form of efficient learning by showing that it can be used to obtain
close-to-optimal bounds on the time and queries required to approximately
correctly solve a constrained version (k=7, \eta=1/5) of a recognizable
computational learning problem: learning parities with noisy membership
queries. We argue that a UGA that treats the noisy membership query oracle as a
fitness function can be straightforwardly used to approximately correctly learn
the essential attributes in O(log^1.585 n) queries and O(n log^1.585 n) time,
where n is the total number of attributes. Our proof relies on an accessible
symmetry argument and the use of statistical hypothesis testing to reject a
global null hypothesis at the 10^-100 level of significance. It is, to the best
of our knowledge, the first relatively rigorous identification of efficient
computational learning in an evolutionary algorithm on a non-trivial learning
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3832</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3832</id><created>2013-07-15</created><authors><author><keyname>Bernadet</keyname><forenames>Alexis</forenames><affiliation>LIX</affiliation></author><author><keyname>Graham-Lengrand</keyname><forenames>St&#xe9;phane</forenames><affiliation>LIX</affiliation></author></authors><title>A simple presentation of the effective topos</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose for the Effective Topos an alternative construction: a
realisability framework composed of two levels of abstraction. This
construction simplifies the proof that the Effective Topos is a topos (equipped
with natural numbers), which is the main issue that this paper addresses. In
this our work can be compared to Frey's monadic tripos-to-topos construction.
However, no topos theory or even category theory is here required for the
construction of the framework itself, which provides a semantics for
higher-order type theories, supporting extensional equalities and the axiom of
unique choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3835</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3835</id><created>2013-07-15</created><updated>2016-02-03</updated><authors><author><keyname>Di Lorenzo</keyname><forenames>Paolo</forenames></author><author><keyname>Barbarossa</keyname><forenames>Sergio</forenames></author><author><keyname>Sardellitti</keyname><forenames>Stefania</forenames></author></authors><title>Joint Optimization of Radio Resources and Code Partitioning in Mobile
  Edge Computing</title><categories>cs.NI</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to propose a computation offloading strategy for
mobile edge computing. We exploit the concept of call graph, which models a
generic computer program as a set of procedures related to each other through a
weighted directed graph. Our goal is to derive the optimal partition of the
call graph establishing which procedures are to be executed locally or
remotely. The main novelty of our work is that the optimal partition is
obtained jointly with the selection of radio parameters, e.g., transmit power
and constellation size, in order to minimize the energy consumption at the
mobile handset, under a latency constraint taking into account transmit time
and execution time. We consider both single and multi-channel transmission
strategies and we prove that a globally optimal solution can be achieved in
both cases. Finally, we propose a suboptimal strategy aimed at solving a
relaxed version of the original problem in order to tradeoff complexity and
performance of the proposed framework. Finally, several numerical results
illustrate under what conditions in terms of call graph topology, communication
strategy, and computation parameters, the proposed offloading strategy provides
large performance gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3846</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3846</id><created>2013-07-15</created><authors><author><keyname>Bratieres</keyname><forenames>Sebastien</forenames></author><author><keyname>Quadrianto</keyname><forenames>Novi</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Bayesian Structured Prediction Using Gaussian Processes</title><categories>stat.ML cs.LG</categories><comments>8 pages with figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a conceptually novel structured prediction model, GPstruct,
which is kernelized, non-parametric and Bayesian, by design. We motivate the
model with respect to existing approaches, among others, conditional random
fields (CRFs), maximum margin Markov networks (M3N), and structured support
vector machines (SVMstruct), which embody only a subset of its properties. We
present an inference procedure based on Markov Chain Monte Carlo. The framework
can be instantiated for a wide range of structured objects such as linear
chains, trees, grids, and other general graphs. As a proof of concept, the
model is benchmarked on several natural language processing tasks and a video
gesture segmentation task involving a linear chain structure. We show
prediction accuracies for GPstruct which are comparable to or exceeding those
of CRFs and SVMstruct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3853</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3853</id><created>2013-07-15</created><authors><author><keyname>Yavits</keyname><forenames>Leonid</forenames></author><author><keyname>Morad</keyname><forenames>Amir</forenames></author><author><keyname>Ginosar</keyname><forenames>Ran</forenames></author></authors><title>Thermal analysis of 3D associative processor</title><categories>cs.AR</categories><comments>arXiv admin note: text overlap with arXiv:1306.3109</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thermal density and hot spots limit three-dimensional (3D) implementation of
massively-parallel SIMD processors and prohibit stacking DRAM dies above them.
This study proposes replacing SIMD by an Associative Processor (AP). AP
exhibits close to uniform thermal distribution with reduced hot spots.
Additionally, AP may outperform SIMD processor when the data set size is
sufficiently large, while dissipating less power. Comparative performance and
thermal analysis supported by simulation confirm that AP might be preferable
over SIMD for 3D implementation of large scale massively parallel processing
engines combined with 3D DRAM integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3855</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3855</id><created>2013-07-15</created><authors><author><keyname>Shi</keyname><forenames>Yue</forenames></author><author><keyname>Karatzoglou</keyname><forenames>Alexandros</forenames></author><author><keyname>Baltrunas</keyname><forenames>Linas</forenames></author><author><keyname>Larson</keyname><forenames>Martha</forenames></author><author><keyname>Hanjalic</keyname><forenames>Alan</forenames></author></authors><title>GAPfm: Optimal Top-N Recommendations for Graded Relevance Domains</title><categories>cs.IR</categories><comments>Manuscript under review. A short version of this manuscript has been
  accepted at CIKM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems are frequently used in domains in which users express
their preferences in the form of graded judgments, such as ratings. If accurate
top-N recommendation lists are to be produced for such graded relevance
domains, it is critical to generate a ranked list of recommended items directly
rather than predicting ratings. Current techniques choose one of two
sub-optimal approaches: either they optimize for a binary metric such as
Average Precision, which discards information on relevance grades, or they
optimize for Normalized Discounted Cumulative Gain (NDCG), which ignores the
dependence of an item's contribution on the relevance of more highly ranked
items.
  In this paper, we address the shortcomings of existing approaches by
proposing the Graded Average Precision factor model (GAPfm), a latent factor
model that is particularly suited to the problem of top-N recommendation in
domains with graded relevance data. The model optimizes for Graded Average
Precision, a metric that has been proposed recently for assessing the quality
of ranked results list for graded relevance. GAPfm learns a latent factor model
by directly optimizing a smoothed approximation of GAP. GAPfm's advantages are
twofold: it maintains full information about graded relevance and also
addresses the limitations of models that optimize NDCG. Experimental results
show that GAPfm achieves substantial improvements on the top-N recommendation
task, compared to several state-of-the-art approaches. In order to ensure that
GAPfm is able to scale to very large data sets, we propose a fast learning
algorithm that uses an adaptive item selection strategy. A final experiment
shows that GAPfm is useful not only for generating recommendation lists, but
also for ranking a given list of rated items.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3863</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3863</id><created>2013-07-15</created><updated>2013-12-09</updated><authors><author><keyname>Mahajan</keyname><forenames>Meena</forenames></author></authors><title>Algebraic Complexity Classes</title><categories>cs.CC</categories><comments>Corrected some typos, added some references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey describes, at an introductory level, the algebraic complexity
framework originally proposed by Leslie Valiant in 1979, and some of the
insights that have been obtained more recently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3872</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3872</id><created>2013-07-15</created><authors><author><keyname>Farruggia</keyname><forenames>Andrea</forenames></author><author><keyname>Ferragina</keyname><forenames>Paolo</forenames></author><author><keyname>Frangioni</keyname><forenames>Antonio</forenames></author><author><keyname>Venturini</keyname><forenames>Rossano</forenames></author></authors><title>Bicriteria data compression</title><categories>cs.IT cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of massive datasets (and the consequent design of high-performing
distributed storage systems) have reignited the interest of the scientific and
engineering community towards the design of lossless data compressors which
achieve effective compression ratio and very efficient decompression speed.
Lempel-Ziv's LZ77 algorithm is the de facto choice in this scenario because of
its decompression speed and its flexibility in trading decompression speed
versus compressed-space efficiency. Each of the existing implementations offers
a trade-off between space occupancy and decompression speed, so software
engineers have to content themselves by picking the one which comes closer to
the requirements of the application in their hands. Starting from these
premises, and for the first time in the literature, we address in this paper
the problem of trading optimally, and in a principled way, the consumption of
these two resources by introducing the Bicriteria LZ77-Parsing problem, which
formalizes in a principled way what data-compressors have traditionally
approached by means of heuristics. The goal is to determine an LZ77 parsing
which minimizes the space occupancy in bits of the compressed file, provided
that the decompression time is bounded by a fixed amount (or vice-versa). This
way, the software engineer can set its space (or time) requirements and then
derive the LZ77 parsing which optimizes the decompression speed (or the space
occupancy, respectively). We solve this problem efficiently in O(n log^2 n)
time and optimal linear space within a small, additive approximation, by
proving and deploying some specific structural properties of the weighted graph
derived from the possible LZ77-parsings of the input file. The preliminary set
of experiments shows that our novel proposal dominates all the highly
engineered competitors, hence offering a win-win situation in theory&amp;practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3877</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3877</id><created>2013-07-15</created><authors><author><keyname>Cetin</keyname><forenames>A. Emre</forenames></author></authors><title>Idempotent permutations</title><categories>cs.DS</categories><comments>32 pages</comments><msc-class>68P05, 68P10</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Together with a characteristic function, idempotent permutations uniquely
determine idempotent maps, as well as their linearly ordered arrangement
simultaneously. Furthermore, in-place linear time transformations are possible
between them. Hence, they may be important for succinct data structures,
information storing, sorting and searching.
  In this study, their combinatorial interpretation is given and their
application on sorting is examined. Given an array of n integer keys each in
[1,n], if it is allowed to modify the keys in the range [-n,n], idempotent
permutations make it possible to obtain linearly ordered arrangement of the
keys in O(n) time using only 4log(n) bits, setting the theoretical lower bound
of time and space complexity of sorting. If it is not allowed to modify the
keys out of the range [1,n], then n+4log(n) bits are required where n of them
is used to tag some of the keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1307.3888</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1307.3888</id><created>2013-07-15</created><authors><author><keyname>Ninagawa</keyname><forenames>Shigeru</forenames></author></authors><title>Solving the Parity Problem with Rule 60 in Array Size of the Power of
  Two</title><categories>cs.OH nlin.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the parity problem, a given cellular automaton has to classify any initial
configuration into two classes according to its parity. Elementary cellular
automaton rule 60 can solve the parity problem in periodic boundary conditions
with array size of the power of two. The spectral analysis of the
configurations of rule 60 at each time step in the evolution reveals that
spatial periodicity emerges as the evolution proceeds and the patterns with
longer period split into the ones with shorter period. This phenomenon is
analogous to the cascade process in which large scale eddies split into smaller
ones in turbulence. By measuring the Lempel-Ziv complexity of configuration, we
found the stepping decrease of the complexity during the evolution. This result
might imply that a decision problem solving process is accompanied with the
decline of complexity of configuration.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="47000" completeListSize="102538">1122234|48001</resumptionToken>
</ListRecords>
</OAI-PMH>
