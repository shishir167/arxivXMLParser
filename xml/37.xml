<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:04:35Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|36001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2738</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2738</id><created>2012-09-12</created><authors><author><keyname>Opoku</keyname><forenames>Samuel King</forenames><affiliation>Kumasi Polytechnic</affiliation></author></authors><title>A Robust Cryptographic System using Neighborhood-Generated Keys</title><categories>cs.CR</categories><comments>9 pages, 2 tables and 4 figures</comments><journal-ref>Samuel King Opoku, &quot;A Robust Cryptographic System using
  Neighborhood-Generated Keys&quot;. International Journal of Research in Computer
  Science, 2 (5): pp. 1-9, September 2012</journal-ref><doi>10.7815/ijorcs.25.2012.041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to hide information from unauthorized individuals has been a
prevalent issue over the years. Countless algorithms such as DES, AES and SHA
have been developed. These algorithms depend on varying key length and key
management strategies to encrypt and decrypt messages. The size of the
encrypted message is so large that it therefore consumes and wastes valuable
storage space when implemented in organizations that store and handle large
volumes of small data. The ability to share the generated keys securely also
poses a problem. This paper proposes a robust cryptographic algorithm which
generates its keys from the surroundings and already-designed coding schemes.
The proposed system conserves storage space and processing power.The algorithm
is implemented and tested using PHP and MySQL DBMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2744</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2744</id><created>2012-09-12</created><authors><author><keyname>Lee</keyname><forenames>James R.</forenames></author><author><keyname>Mendel</keyname><forenames>Manor</forenames></author><author><keyname>Moharrami</keyname><forenames>Mohammad</forenames></author></authors><title>A node-capacitated Okamura-Seymour theorem</title><categories>math.CO cs.DM math.MG</categories><comments>30 pages, 5 figures</comments><doi>10.1007/s10107-014-0810-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical Okamura-Seymour theorem states that for an edge-capacitated,
multi-commodity flow instance in which all terminals lie on a single face of a
planar graph, there exists a feasible concurrent flow if and only if the cut
conditions are satisfied. Simple examples show that a similar theorem is
impossible in the node-capacitated setting. Nevertheless, we prove that an
approximate flow/cut theorem does hold: For some universal c &gt; 0, if the node
cut conditions are satisfied, then one can simultaneously route a c-fraction of
all the demands. This answers an open question of Chekuri and Kawarabayashi.
More generally, we show that this holds in the setting of multi-commodity
polymatroid networks introduced by Chekuri, et. al. Our approach employs a new
type of random metric embedding in order to round the convex programs
corresponding to these more general flow problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2755</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2755</id><created>2012-09-12</created><authors><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Relaxing the Gaussian AVC</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The arbitrarily varying channel (AVC) is a conservative way of modeling an
unknown interference, and the corresponding capacity results are pessimistic.
We reconsider the Gaussian AVC by relaxing the classical model and thereby
weakening the adversarial nature of the interference. We examine three
different relaxations. First, we show how a very small amount of common
randomness between transmitter and receiver is sufficient to achieve the rates
of fully randomized codes. Second, akin to the dirty paper coding problem, we
study the impact of an additional interference known to the transmitter. We
provide partial capacity results that differ significantly from the standard
AVC. Third, we revisit a Gaussian MIMO AVC in which the interference is
arbitrary but of limited dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2759</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2759</id><created>2012-09-12</created><authors><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Haridasan</keyname><forenames>Maya</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author></authors><title>Multi-track Map Matching</title><categories>cs.LG cs.DS stat.AP</categories><comments>11 pages, 8 figures, short version appears in 20th International
  Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL GIS
  2012). Extended Abstract in Proceedings of the 10th international conference
  on Mobile systems, applications, and services (MobiSys 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study algorithms for matching user tracks, consisting of time-ordered
location points, to paths in the road network. Previous work has focused on the
scenario where the location data is linearly ordered and consists of fairly
dense and regular samples. In this work, we consider the \emph{multi-track map
matching}, where the location data comes from different trips on the same
route, each with very sparse samples. This captures the realistic scenario
where users repeatedly travel on regular routes and samples are sparsely
collected, either due to energy consumption constraints or because samples are
only collected when the user actively uses a service. In the multi-track
problem, the total set of combined locations is only partially ordered, rather
than globally ordered as required by previous map-matching algorithms. We
propose two methods, the iterative projection scheme and the graph Laplacian
scheme, to solve the multi-track problem by using a single-track map-matching
subroutine. We also propose a boosting technique which may be applied to either
approach to improve the accuracy of the estimated paths. In addition, in order
to deal with variable sampling rates in single-track map matching, we propose a
method based on a particular regularized cost function that can be adapted for
different sampling rates and measurement errors. We evaluate the effectiveness
of our techniques for reconstructing tracks under several different
configurations of sampling error and sampling rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2784</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2784</id><created>2012-09-13</created><authors><author><keyname>Mehta</keyname><forenames>Nishant A.</forenames></author><author><keyname>Lee</keyname><forenames>Dongryeol</forenames></author><author><keyname>Gray</keyname><forenames>Alexander G.</forenames></author></authors><title>Minimax Multi-Task Learning and a Generalized Loss-Compositional
  Paradigm for MTL</title><categories>cs.LG stat.ML</categories><comments>appearing at NIPS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since its inception, the modus operandi of multi-task learning (MTL) has been
to minimize the task-wise mean of the empirical risks. We introduce a
generalized loss-compositional paradigm for MTL that includes a spectrum of
formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a
new MTL formulation that minimizes the maximum of the tasks' empirical risks.
Via a certain relaxation of minimax MTL, we obtain a continuum of MTL
formulations spanning minimax MTL and classical MTL. The full paradigm itself
is loss-compositional, operating on the vector of empirical risks. It
incorporates minimax MTL, its relaxations, and many new MTL formulations as
special cases. We show theoretically that minimax MTL tends to avoid worst case
outcomes on newly drawn test tasks in the learning to learn (LTL) test setting.
The results of several MTL formulations on synthetic and real problems in the
MTL and LTL test settings are encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2790</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2790</id><created>2012-09-13</created><authors><author><keyname>Chen</keyname><forenames>Xianfu</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author><author><keyname>Chen</keyname><forenames>Tao</forenames></author><author><keyname>Lasanen</keyname><forenames>Mika</forenames></author></authors><title>Improving Energy Efficiency in Femtocell Networks: A Hierarchical
  Reinforcement Learning Framework</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates energy efficiency for two-tier femtocell networks
through combining game theory and stochastic learning. With the Stackelberg
game formulation, a hierarchical reinforcement learning framework is applied to
study the joint average utility maximization of macrocells and femtocells
subject to the minimum signal-to-interference-plus-noise-ratio requirements.
The macrocells behave as the leaders and the femtocells are followers during
the learning procedure. At each time step, the leaders commit to dynamic
strategies based on the best responses of the followers, while the followers
compete against each other with no further information but the leaders'
strategy information. In this paper, we propose two learning algorithms to
schedule each cell's stochastic power levels, leading by the macrocells.
Numerical experiments are presented to validate the proposed studies and show
that the two learning algorithms substantially improve the energy efficiency of
the femtocell networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2794</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2794</id><created>2012-09-13</created><authors><author><keyname>Paci</keyname><forenames>Hakik</forenames></author><author><keyname>Mece</keyname><forenames>Elinda Kajo</forenames></author><author><keyname>Xhuvani</keyname><forenames>Aleksander</forenames></author></authors><title>Protecting oracle pl/sql source code from a dba user</title><categories>cs.DB</categories><journal-ref>International Journal of Database Management Systems, 4, (2012)
  43-52</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we are presenting a new way to disable DDL statements on some
specific PL/SQL procedures to a dba user in the Oracle database. Nowadays dba
users have access to a lot of data and source code even if they do not have
legal permissions to see or modify them. With this method we can disable the
ability to execute DDL and DML statements on some specific pl/sql procedures
from every Oracle database user even if it has a dba role. Oracle gives to
developer the possibility to wrap the pl/sql procedures, functions and packages
but those wrapped scripts can be unwrapped by using third party tools. The
scripts that we have developed analyzes all database sessions, and if they
detect a DML or a DDL statement from an unauthorized user to procedure,
function or package which should be protected then the execution of the
statement is denied. Furthermore, these scripts do not allow a dba user to drop
or disable the scripts themselves. In other words by managing sessions prior to
the execution of an eventual statement from a dba user, we can prevent the
execution of eventual statements which target our scripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2806</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2806</id><created>2012-09-13</created><authors><author><keyname>Amdouni</keyname><forenames>Ichrak</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Minet</keyname><forenames>Pascale</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>TRASA: TRaffic Aware Slot Assignment Algorithm in Wireless Sensor
  Networks</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>The 2nd International Conference On Communications and Information
  Technology: ICCIT 2012 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In data gathering applications which is a typical application paradigm in
wireless sensor networks, sensor nodes may have different traffic demands.
Assigning equal channel access to each node may lead to congestion, inefficient
use of the bandwidth and decrease of the application performance. In this
paper, we prove that the time slot assignment problem is NP-complete when p-hop
nodes are not assigned the same slot, with 1 &lt;= p &lt;= h for any strictly
positive integer h. We propose TRASA, a TRaffic Aware time Slot Assignment
algorithm able to allocate slots to sensors proportionally to their demand. We
evaluate the performance of TRASA for different heuristics and prove that it
provides an optimized spatial reuse and a minimized cycle length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2807</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2807</id><created>2012-09-13</created><authors><author><keyname>Chopra</keyname><forenames>Amit K.</forenames></author></authors><title>True Peer Review</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer science, conferences and journals conduct peer review in order to
decide what to publish. Many have pointed out the inherent weaknesses in peer
review, including those of bias, quality, and accountability. Many have
suggested and adopted refinements of peer review, for instance, double blind
peer review with author rebuttals.
  In this essay, I argue that peer review as currently practiced conflates the
sensible idea of getting comments on a paper with the irrevocably-flawed one
that we either accept or reject the paper, which I term gatekeeping. If we look
at the two separately, then it is clear that the ills associated with current
peer review systems are not due to the practice of getting comments, but due to
the practice of gatekeeping.
  True peer review constitutes my proposal for replacing existing peer review
systems. It embraces the idea of open debate on the merits of a paper; however,
it rejects unequivocally the exercise of gatekeeping. True peer review offers
all the benefits of current peer review systems but has none of its weaknesses.
True peer review will lead to a truly engaged community of researchers and
therefore better science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2810</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2810</id><created>2012-09-13</created><authors><author><keyname>Amdouni</keyname><forenames>Ichrak</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Minet</keyname><forenames>Pascale</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Adjih</keyname><forenames>C&#xe9;dric</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Node Coloring in Wireless Networks: Complexity Results and Grid Coloring</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1104.1859</comments><proxy>ccsd</proxy><journal-ref>IFIP Wireless and Mobile Networking Conference WMNC 2011 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coloring is used in wireless networks to improve communication efficiency,
mainly in terms of bandwidth, energy and possibly end-to-end delays. In this
paper, we define the h-hop node coloring problem, with h any positive integer,
adapted to two types of applications in wireless networks. We specify both
general mode for general applications and strategic mode for data gathering
applications.We prove that the associated decision problem is NP-complete. We
then focus on grid topologies that constitute regular topologies for large or
dense wireless networks. We consider various transmission ranges and identify a
color pattern that can be reproduced to color the whole grid with the optimal
number of colors. We obtain an optimal periodic coloring of the grid for the
considered transmission range. We then present a 3-hop distributed coloring
algorithm, called SERENA. Through simulation results, we highlight the impact
of node priority assignment on the number of colors obtained for any network
and grids in particular. We then compare these optimal results on grids with
those obtained by SERENA and identify directions to improve SERENA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2816</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2816</id><created>2012-09-13</created><authors><author><keyname>Padmavathi</keyname><forenames>S.</forenames></author><author><keyname>Soman</keyname><forenames>B. Priyalakshmi. Dr. K. P.</forenames></author></authors><title>Hirarchical Digital Image Inpainting Using Wavelets</title><categories>cs.CV</categories><comments>8 pages, 9 figures</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.3, No.4, August 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inpainting is the technique of reconstructing unknown or damaged portions of
an image in a visually plausible way. Inpainting algorithm automatically fills
the damaged region in an image using the information available in undamaged
region. Propagation of structure and texture information becomes a challenge as
the size of damaged area increases. In this paper, a hierarchical inpainting
algorithm using wavelets is proposed. The hierarchical method tries to keep the
mask size smaller while wavelets help in handling the high pass structure
information and low pass texture information separately. The performance of the
proposed algorithm is tested using different factors. The results of our
algorithm are compared with existing methods such as interpolation, diffusion
and exemplar techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2817</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2817</id><created>2012-09-13</created><authors><author><keyname>Podobnik</keyname><forenames>Boris</forenames></author><author><keyname>Horvatic</keyname><forenames>Davor</forenames></author><author><keyname>Dickison</keyname><forenames>Mark</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author></authors><title>Preferential Attachment in the Interaction between Dynamically Generated
  Interdependent Networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-fin.RM</categories><comments>8 pages, 4 figures</comments><doi>10.1209/0295-5075/100/50004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the scale-free network model of Barab\`asi and Albert [Science
286, 509 (1999)] by proposing a class of stochastic models for scale-free
interdependent networks in which interdependent nodes are not randomly
connected but rather are connected via preferential attachment (PA). Each
network grows through the continuous addition of new nodes, and new nodes in
each network attach preferentially and simultaneously to (a) well-connected
nodes within the same network and (b) well-connected nodes in other networks.
We present analytic solutions for the power-law exponents as functions of the
number of links both between networks and within networks. We show that a
cross-clustering coefficient vs. size of network $N$ follows a power law. We
illustrate the models using selected examples from the Internet and finance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2820</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2820</id><created>2012-09-13</created><updated>2014-09-27</updated><authors><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>Conditions for a Monotonic Channel Capacity</title><categories>cs.IT math.IT</categories><comments>This is an updated and expanded version of arXiv:1108.0391</comments><journal-ref>IEEE Trans. Commun., vol. 63, no. 3, pp. 738-748, Mar. 2015</journal-ref><doi>10.1109/TCOMM.2014.2381247</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by results in optical communications, where the performance can
degrade dramatically if the transmit power is sufficiently increased, the
channel capacity is characterized for various kinds of memoryless vector
channels. It is proved that for all static point-to-point channels, the channel
capacity is a nondecreasing function of power. As a consequence, maximizing the
mutual information over all input distributions with a certain power is for
such channels equivalent to maximizing it over the larger set of input
distributions with upperbounded power. For interference channels such as
optical wavelength-division multiplexing systems, the primary channel capacity
is always nondecreasing with power if all interferers transmit with identical
distributions as the primary user. Also, if all input distributions in an
interference channel are optimized jointly, then the achievable sum-rate
capacity is again nondecreasing. The results generalizes to the channel
capacity as a function of a wide class of costs, not only power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2848</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2848</id><created>2012-09-13</created><authors><author><keyname>Kling</keyname><forenames>Peter</forenames></author><author><keyname>Cord-Landwehr</keyname><forenames>Andreas</forenames></author><author><keyname>Mallmann-Trenn</keyname><forenames>Frederik</forenames></author></authors><title>Slow Down &amp; Sleep for Profit in Online Deadline Scheduling</title><categories>cs.DS</categories><comments>An extended abstract of this paper has been accepted for publication
  in the proceedings of the 1st Mediterranean Conference on Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and study a new model for energy-aware and profit-oriented
scheduling on a single processor. The processor features dynamic speed scaling
as well as suspension to a sleep mode. Jobs arrive over time, are preemptable,
and have different sizes, values, and deadlines. On the arrival of a new job,
the scheduler may either accept or reject the job. Accepted jobs need a certain
energy investment to be finished in time, while rejected jobs cause costs equal
to their values. Here, power consumption at speed $s$ is given by
$P(s)=s^{\alpha}+\beta$ and the energy investment is power integrated over
time. Additionally, the scheduler may decide to suspend the processor to a
sleep mode in which no energy is consumed, though awaking entails fixed
transition costs $\gamma$. The objective is to minimize the total value of
rejected jobs plus the total energy.
  Our model combines aspects from advanced energy conservation techniques
(namely speed scaling and sleep states) and profit-oriented scheduling models.
We show that \emph{rejection-oblivious} schedulers (whose rejection decisions
are not based on former decisions) have -- in contrast to the model without
sleep states -- an unbounded competitive ratio w.r.t\text{.} the processor
parameters $\alpha$ and $\beta$. It turns out that the worst-case performance
of such schedulers depends linearly on the jobs' value densities (the ratio
between a job's value and its work). We give an algorithm whose competitiveness
nearly matches this lower bound. If the maximum value density is not too large,
the competitiveness becomes $\alpha^{\alpha}+2e\alpha$. Also, we show that it
suffices to restrict the value density of low-value jobs only. Using a
technique from \cite{Chan:2010} we transfer our results to processors with a
fixed maximum speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2855</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2855</id><created>2012-09-13</created><authors><author><keyname>Hoque</keyname><forenames>Mohammad Ashraful</forenames></author><author><keyname>Siekkinen</keyname><forenames>Matti</forenames></author><author><keyname>Nurminen</keyname><forenames>Jukka K.</forenames></author><author><keyname>Aalto</keyname><forenames>Mika</forenames></author></authors><title>Investigating Streaming Techniques and Energy Efficiency of Mobile Video
  Services</title><categories>cs.MM cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We report results from a measurement study of three video streaming services,
YouTube, Dailymotion and Vimeo on six different smartphones. We measure and
analyze the traffic and energy consumption when streaming different quality
videos over Wi-Fi and 3G. We identify five different techniques to deliver the
video and show that the use of a particular technique depends on the device,
player, quality, and service. The energy consumption varies dramatically
between devices, services, and video qualities depending on the streaming
technique used. As a consequence, we come up with suggestions on how to improve
the energy efficiency of mobile video streaming services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2859</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2859</id><created>2012-09-13</created><authors><author><keyname>Zocca</keyname><forenames>Alessandro</forenames></author><author><keyname>Borst</keyname><forenames>Sem C.</forenames></author><author><keyname>van Leeuwaarden</keyname><forenames>Johan S. H.</forenames></author></authors><title>Mixing Properties of CSMA Networks on Partite Graphs</title><categories>math.PR cs.NI</categories><comments>Valuetools, 6th International Conference on Performance Evaluation
  Methodologies and Tools, October 9-12, 2012, Carg\`ese, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a stylized stochastic model for a wireless CSMA network.
Experimental results in prior studies indicate that the model provides
remarkably accurate throughput estimates for IEEE 802.11 systems. In
particular, the model offers an explanation for the severe spatial unfairness
in throughputs observed in such networks with asymmetric interference
conditions. Even in symmetric scenarios, however, it may take a long time for
the activity process to move between dominant states, giving rise to potential
starvation issues. In order to gain insight in the transient throughput
characteristics and associated starvation effects, we examine in the present
paper the behavior of the transition time between dominant activity states. We
focus on partite interference graphs, and establish how the magnitude of the
transition time scales with the activation rate and the sizes of the various
network components. We also prove that in several cases the scaled transition
time has an asymptotically exponential distribution as the activation rate
grows large, and point out interesting connections with related exponentiality
results for rare events and meta-stability phenomena in statistical physics. In
addition, we investigate the convergence rate to equilibrium of the activity
process in terms of mixing times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2868</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2868</id><created>2012-09-13</created><authors><author><keyname>Groh</keyname><forenames>Georg</forenames></author><author><keyname>Straub</keyname><forenames>Florian</forenames></author><author><keyname>Koster</keyname><forenames>Benjamin</forenames></author></authors><title>Spatio-Temporal Small Worlds for Decentralized Information Retrieval in
  Social Networking</title><categories>cs.SI cs.IR physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss foundations and options for alternative, agent-based information
retrieval (IR) approaches in Social Networking, especially Decentralized and
Mobile Social Networking scenarios. In addition to usual semantic contexts,
these approaches make use of long-term social and spatio-temporal contexts in
order to satisfy conscious as well as unconscious information needs according
to Human IR heuristics. Using a large Twitter dataset, we investigate these
approaches and especially investigate the question in how far spatio-temporal
contexts can act as a conceptual bracket implicating social and semantic
cohesion, giving rise to the concept of Spatio-Temporal Small Worlds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2873</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2873</id><created>2012-09-13</created><authors><author><keyname>Lee</keyname><forenames>Juyong</forenames></author><author><keyname>Gross</keyname><forenames>Steven P.</forenames></author><author><keyname>Lee</keyname><forenames>Jooyoung</forenames></author></authors><title>Extraction of hidden information by efficient community detection in
  networks</title><categories>physics.data-an cs.SI physics.bio-ph physics.soc-ph q-bio.MN</categories><comments>17 pages, 2 figures and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, we are overwhelmed by a deluge of experimental data, and network
physics has the potential to become an invaluable method to increase our
understanding of large interacting datasets. However, this potential is often
unrealized for two reasons: uncovering the hidden community structure of a
network, known as community detection, is difficult, and further, even if one
has an idea of this community structure, it is not a priori obvious how to
efficiently use this information. Here, to address both of these issues, we,
first, identify optimal community structure of given networks in terms of
modularity by utilizing a recently introduced community detection method.
Second, we develop an approach to use this community information to extract
hidden information from a network. When applied to a protein-protein
interaction network, the proposed method outperforms current state-of-the-art
methods that use only the local information of a network. The method is
generally applicable to networks from many areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2875</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2875</id><created>2012-09-13</created><authors><author><keyname>Osherson</keyname><forenames>Daniel</forenames></author><author><keyname>Weinstein</keyname><forenames>Scott</forenames></author></authors><title>Notes on random reals</title><categories>cs.CC</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of random real numbers is exceedingly well-developed, and
fascinating from many points of view. It is also quite challenging
mathematically. The present notes are intended as no more than a gateway to the
larger theory. They review just the most elementary part of the theory (bearing
on Kolmogorov- and ML-randomness). We hope that the simple arguments presented
here will encourage the enterprising student to examine richer treatments of
the subject available elsewhere, notably, in Downey and Hirschfeldt (2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2881</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2881</id><created>2012-09-13</created><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames></author><author><keyname>Gaurav</keyname><forenames>Kumar</forenames></author></authors><title>Far-out Vertices In Weighted Repeated Configuration Model</title><categories>math.PR cs.NI</categories><journal-ref>Proceedings of the MAMA workshop, held in conjunction with ACM
  Sigmetrics/Performance, London 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an edge-weighted uniform random graph with a given degree
sequence (Repeated Configuration Model) which is a useful approximation for
many real-world networks. It has been observed that the vertices which are
separated from the rest of the graph by a distance exceeding certain threshold
play an important role in determining some global properties of the graph like
diameter, flooding time etc., in spite of being statistically rare. We give a
convergence result for the distribution of the number of such far-out vertices.
We also make a conjecture about how this relates to the longest edge of the
minimal spanning tree on the graph under consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2883</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2883</id><created>2012-09-13</created><updated>2012-11-08</updated><authors><author><keyname>Arvelo</keyname><forenames>Eduardo</forenames></author><author><keyname>Martins</keyname><forenames>Nuno C.</forenames></author></authors><title>Control Design for Markov Chains under Safety Constraints: A Convex
  Approach</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the design of time-invariant memoryless control
policies for fully observed controlled Markov chains, with a finite state
space. Safety constraints are imposed through a pre-selected set of forbidden
states. A state is qualified as safe if it is not a forbidden state and the
probability of it transitioning to a forbidden state is zero. The main
objective is to obtain control policies whose closed loop generates the maximal
set of safe recurrent states, which may include multiple recurrent classes. A
design method is proposed that relies on a finitely parametrized convex program
inspired on entropy maximization principles. A numerical example is provided
and the adoption of additional constraints is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2887</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2887</id><created>2012-09-13</created><authors><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>Decoding of Subspace Codes, a Problem of Schubert Calculus over Finite
  Fields</title><categories>cs.IT math.IT</categories><comments>To appear in Mathematical System Theory - Festschrift in Honor of Uwe
  Helmke on the Occasion of his Sixtieth Birthday, CreateSpace, 2012, ISBN
  978-1470044008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schubert calculus provides algebraic tools to solve enumerative problems.
There have been several applied problems in systems theory, linear algebra and
physics which were studied by means of Schubert calculus. The method is most
powerful when the base field is algebraically closed. In this article we first
review some of the successes Schubert calculus had in the past. Then we show
how the problem of decoding of subspace codes used in random network coding can
be formulated as a problem in Schubert calculus. Since for this application the
base field has to be assumed to be a finite field new techniques will have to
be developed in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2890</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2890</id><created>2012-09-13</created><updated>2012-10-08</updated><authors><author><keyname>Ehrhard</keyname><forenames>Thomas</forenames><affiliation>CNRS</affiliation></author><author><keyname>Bucciarelli</keyname><forenames>Antonio</forenames><affiliation>Universite Paris Diderot</affiliation></author><author><keyname>Carraro</keyname><forenames>Alberto</forenames><affiliation>Universita Ca' Foscari</affiliation></author><author><keyname>Manzonetto</keyname><forenames>Giulio</forenames><affiliation>Universite Paris Nord</affiliation></author></authors><title>Full Abstraction for the Resource Lambda Calculus with Tests, through
  Taylor Expansion</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (October
  10, 2012) lmcs:1047</journal-ref><doi>10.2168/LMCS-8(4:3)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the semantics of a resource-sensitive extension of the lambda
calculus in a canonical reflexive object of a category of sets and relations, a
relational version of Scott's original model of the pure lambda calculus. This
calculus is related to Boudol's resource calculus and is derived from Ehrhard
and Regnier's differential extension of Linear Logic and of the lambda
calculus. We extend it with new constructions, to be understood as implementing
a very simple exception mechanism, and with a &quot;must&quot; parallel composition.
These new operations allow to associate a context of this calculus with any
point of the model and to prove full abstraction for the finite sub-calculus
where ordinary lambda calculus application is not allowed. The result is then
extended to the full calculus by means of a Taylor Expansion formula. As an
intermediate result we prove that the exception mechanism is not essential in
the finite sub-calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2894</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2894</id><created>2012-09-13</created><authors><author><keyname>Chen</keyname><forenames>Chao</forenames></author><author><keyname>Xie</keyname><forenames>Hongmei</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author></authors><title>Layered Subspace Codes for Network Coding</title><categories>cs.IT math.IT</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace codes were introduced by K\&quot;otter and Kschischang for error control
in random linear network coding. In this paper, a layered type of subspace
codes is considered, which can be viewed as a superposition of multiple
component subspace codes. Exploiting the layered structure, we develop two
decoding algorithms for these codes. The first algorithm operates by separately
decoding each component code. The second algorithm is similar to the successive
interference cancellation (SIC) algorithm for conventional superposition
coding, and further permits an iterative version. We show that both algorithms
decode not only deterministically up to but also probabilistically beyond the
error-correction capability of the overall code. Finally we present possible
applications of layered subspace codes in several network coding scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2903</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2903</id><created>2012-09-13</created><authors><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author><author><keyname>Nandi</keyname><forenames>Pradipti</forenames></author><author><keyname>Barman</keyname><forenames>Nilanjana</forenames></author></authors><title>A Novel Approach of Harris Corner Detection of Noisy Images using
  Adaptive Wavelet Thresholding Technique</title><categories>cs.CV</categories><comments>5 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:1209.1558</comments><journal-ref>International Journal of Computer Science &amp; Technology(IJCST) Vol.
  2, ISSUE 4, OCT. - DEC. 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a method of corner detection for obtaining features
which is required to track and recognize objects within a noisy image. Corner
detection of noisy images is a challenging task in image processing. Natural
images often get corrupted by noise during acquisition and transmission. Though
Corner detection of these noisy images does not provide desired results, hence
de-noising is required. Adaptive wavelet thresholding approach is applied for
the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2905</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2905</id><created>2012-09-13</created><authors><author><keyname>Montpetit</keyname><forenames>Marie-Jose</forenames></author><author><keyname>Cesar</keyname><forenames>Pablo</forenames></author><author><keyname>Matijasevic</keyname><forenames>Maja</forenames></author><author><keyname>Liu</keyname><forenames>Zhu</forenames></author><author><keyname>Crowcroft</keyname><forenames>John</forenames></author><author><keyname>Martinez-Bonastre</keyname><forenames>Oscar</forenames></author></authors><title>Surveying the Social, Smart and Converged TV Landscape: Where is
  Television Research Headed?</title><categories>cs.MM</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The TV is dead motto of just a few years ago has been replaced by the
prospect of Internet Protocol (IP) television experiences over converged
networks to become one of the great technology opportunities in the next few
years. As an introduction to the Special Issue on Smart, Social and Converged
Television, this extended editorial intends to review the current IP television
landscape in its many realizations: operator-based, over-the-top, and user
generated. We will address new services like social TV and recommendation
engines, dissemination including new paradigms built on peer to peer and
content centric networks, as well as the all important quality of experience
that challenges services and networks alike. But we intend to go further than
just review the existing work by proposing areas for the future of television
research. These include strategies to provide services that are more efficient
in network and energy usage while being socially engaging, novel services that
will provide consumers with a broader choice of content and devices, and
metrics that will enable operators and users alike to define the level of
service they require or that they are ready to provide. These topics are
addressed in this survey paper that attempts to create a unifying framework to
link them all together. Not only is television not dead, it is well alive,
thriving and fostering innovation and this paper will hopefully prove it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2910</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2910</id><created>2012-09-13</created><authors><author><keyname>Heimlicher</keyname><forenames>Simon</forenames></author><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author><author><keyname>Massouli&#xe9;</keyname><forenames>Laurent</forenames></author></authors><title>Community Detection in the Labelled Stochastic Block Model</title><categories>cs.SI cs.LG math.PR physics.soc-ph</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of community detection from observed interactions
between individuals, in the context where multiple types of interaction are
possible. We use labelled stochastic block models to represent the observed
data, where labels correspond to interaction types. Focusing on a two-community
scenario, we conjecture a threshold for the problem of reconstructing the
hidden communities in a way that is correlated with the true partition. To
substantiate the conjecture, we prove that the given threshold correctly
identifies a transition on the behaviour of belief propagation from insensitive
to sensitive. We further prove that the same threshold corresponds to the
transition in a related inference problem on a tree model from infeasible to
feasible. Finally, numerical results using belief propagation for community
detection give further support to the conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2918</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2918</id><created>2012-09-13</created><updated>2014-01-17</updated><authors><author><keyname>Rusu</keyname><forenames>C&#x103;t&#x103;lin V.</forenames></author><author><keyname>Florian</keyname><forenames>R&#x103;zvan V.</forenames></author></authors><title>A new class of metrics for spike trains</title><categories>cs.IT cs.NE math.IT q-bio.NC</categories><journal-ref>Rusu, C. V., &amp; Florian, R. V. (2014). A new class of metrics for
  spike trains. Neural Computation, 26(2), 306-348</journal-ref><doi>10.1162/NECO_a_00545</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distance between a pair of spike trains, quantifying the differences
between them, can be measured using various metrics. Here we introduce a new
class of spike train metrics, inspired by the Pompeiu-Hausdorff distance, and
compare them with existing metrics. Some of our new metrics (the modulus-metric
and the max-metric) have characteristics that are qualitatively different than
those of classical metrics like the van Rossum distance or the Victor &amp; Purpura
distance. The modulus-metric and the max-metric are particularly suitable for
measuring distances between spike trains where information is encoded in
bursts, but the number and the timing of spikes inside a burst does not carry
information. The modulus-metric does not depend on any parameters and can be
computed using a fast algorithm, in a time that depends linearly on the number
of spikes in the two spike trains. We also introduce localized versions of the
new metrics, which could have the biologically-relevant interpretation of
measuring the differences between spike trains as they are perceived at a
particular moment in time by a neuron receiving these spike trains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2946</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2946</id><created>2012-09-11</created><authors><author><keyname>Rodriguez</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>SPCMIB</affiliation></author></authors><title>Technical Report: CSVM Ecosystem</title><categories>cs.CE cs.DS q-bio.QM</categories><comments>31 pages including 2p of Annex</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CSVM format is derived from CSV format and allows the storage of tabular
like data with a limited but extensible amount of metadata. This approach could
help computer scientists because all information needed to uses subsequently
the data is included in the CSVM file and is particularly well suited for
handling RAW data in a lot of scientific fields and to be used as a canonical
format. The use of CSVM has shown that it greatly facilitates: the data
management independently of using databases; the data exchange; the integration
of RAW data in dataflows or calculation pipes; the search for best practices in
RAW data management. The efficiency of this format is closely related to its
plasticity: a generic frame is given for all kind of data and the CSVM parsers
don't make any interpretation of data types. This task is done by the
application layer, so it is possible to use same format and same parser codes
for a lot of purposes. In this document some implementation of CSVM format for
ten years and in different laboratories are presented. Some programming
examples are also shown: a Python toolkit for using the format, manipulating
and querying is available. A first specification of this format (CSVM-1) is now
defined, as well as some derivatives such as CSVM dictionaries used for data
interchange. CSVM is an Open Format and could be used as a support for Open
Data and long term conservation of RAW or unpublished data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2948</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2948</id><created>2012-09-12</created><authors><author><keyname>Srinivasan</keyname><forenames>Sujatha</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Sivakumar</forenames></author></authors><title>Cultural Algorithm Toolkit for Multi-objective Rule Mining</title><categories>cs.NE cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1207.2630</comments><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vo2, No.4, 2012, 9-23</journal-ref><doi>10.5121/ijcsa.2012.2402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cultural algorithm is a kind of evolutionary algorithm inspired from societal
evolution and is composed of a belief space, a population space and a protocol
that enables exchange of knowledge between these sources. Knowledge created in
the population space is accepted into the belief space while this collective
knowledge from these sources is combined to influence the decisions of the
individual agents in solving problems. Classification rules comes under
descriptive knowledge discovery in data mining and are the most sought out by
users since they represent highly comprehensible form of knowledge. The rules
have certain properties which make them useful forms of actionable knowledge to
users. The rules are evaluated using these properties namely the rule metrics.
In the current study a Cultural Algorithm Toolkit for Classification Rule
Mining (CAT-CRM) is proposed which allows the user to control three different
set of parameters namely the evolutionary parameters, the rule parameters as
well as agent parameters and hence can be used for experimenting with an
evolutionary system, a rule mining system or an agent based social system.
Results of experiments conducted to observe the effect of different number and
type of metrics on the performance of the algorithm on bench mark data sets is
reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.2951</identifier>
 <datestamp>2012-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.2951</id><created>2012-09-13</created><authors><author><keyname>Acharyya</keyname><forenames>Rashmisnata</forenames></author><author><keyname>B.</keyname><forenames>Manjanna</forenames></author><author><keyname>Das</keyname><forenames>Gautam K.</forenames></author></authors><title>Unit Disk Cover Problem</title><categories>cs.CG</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set ${\cal D}$ of unit disks in the Euclidean plane, we consider (i)
the {\it discrete unit disk cover} (DUDC) problem and (ii) the {\it rectangular
region cover} (RRC) problem. In the DUDC problem, for a given set ${\cal P}$ of
points the objective is to select minimum cardinality subset ${\cal D}^*
\subseteq {\cal D}$ such that each point in ${\cal P}$ is covered by at least
one disk in ${\cal D}^*$. On the other hand, in the RRC problem the objective
is to select minimum cardinality subset ${\cal D}^{**} \subseteq {\cal D}$ such
that each point of a given rectangular region ${\cal R}$ is covered by a disk
in ${\cal D}^{**}$. For the DUDC problem, we propose an $(9+\epsilon)$-factor
($0 &lt; \epsilon \leq 6$) approximation algorithm. The previous best known
approximation factor was 15 \cite{FL12}. For the RRC problem, we propose (i) an
$(9 + \epsilon)$-factor ($0 &lt; \epsilon \leq 6$) approximation algorithm, (ii)
an 2.25-factor approximation algorithm in reduce radius setup, improving
previous 4-factor approximation result in the same setup \cite{FKKLS07}.
  The solution of DUDC problem is based on a PTAS for the subproblem LSDUDC,
where all the points in ${\cal P}$ are on one side of a line and covered by the
disks centered on the other side of that line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3026</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3026</id><created>2012-09-13</created><authors><author><keyname>SalahEldeen</keyname><forenames>Hany M.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Losing My Revolution: How Many Resources Shared on Social Media Have
  Been Lost?</title><categories>cs.DL cs.IR</categories><comments>12 pages, Theory and Practice of Digital Libraries (TPDL) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media content has grown exponentially in the recent years and the role
of social media has evolved from just narrating life events to actually shaping
them. In this paper we explore how many resources shared in social media are
still available on the live web or in public web archives. By analyzing six
different event-centric datasets of resources shared in social media in the
period from June 2009 to March 2012, we found about 11% lost and 20% archived
after just a year and an average of 27% lost and 41% archived after two and a
half years. Furthermore, we found a nearly linear relationship between time of
sharing of the resource and the percentage lost, with a slightly less linear
relationship between time of sharing and archiving coverage of the resource.
From this model we conclude that after the first year of publishing, nearly 11%
of shared resources will be lost and after that we will continue to lose 0.02%
per day.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3047</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3047</id><created>2012-09-13</created><authors><author><keyname>Moustakas</keyname><forenames>Aris L.</forenames></author><author><keyname>Kazakopoulos</keyname><forenames>Pavlos</forenames></author></authors><title>SINR Statistics of Correlated MIMO Linear Receivers</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures. Submitted for publication to IEEE Transactions
  of Information Theory, May 2011. The work in this paper was presented in part
  in IEEE International Symposium on Information Theory, St. Petersburg, Aug.
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear receivers offer a low complexity option for multi-antenna
communication systems. Therefore, understanding the outage behavior of the
corresponding SINR is important in a fading mobile environment. In this paper
we introduce a large deviations method, valid nominally for a large number M of
antennas, which provides the probability density of the SINR of Gaussian
channel MIMO Minimum Mean Square Error (MMSE) and zero-forcing (ZF) receivers,
with arbitrary transmission power profiles and in the presence of receiver
antenna correlations. This approach extends the Gaussian approximation of the
SINR, valid for large M asymptotically close to the center of the distribution,
obtaining the non-Gaussian tails of the distribution. Our methodology allows us
to calculate the SINR distribution to next-to-leading order (O(1/M)) and
showcase the deviations from approximations that have appeared in the
literature (e.g. the Gaussian or the generalized Gamma distribution). We also
analytically evaluate the outage probability, as well as the uncoded
bit-error-rate. We find that our approximation is quite accurate even for the
smallest antenna arrays (2x2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3050</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3050</id><created>2012-09-13</created><authors><author><keyname>Opoku</keyname><forenames>Samuel King</forenames><affiliation>Kumasi Polytechnic</affiliation></author></authors><title>Parallel Sorting System for Objects</title><categories>cs.DC cs.DS</categories><comments>8 pages, 13 figures</comments><journal-ref>Cyber Journals: Multidisciplinary Journals in Science and
  Technology, Journal of Selected Areas in Software Engineering (JSSE), Vol. 2,
  No. 12, pages 1-8, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional sorting algorithms make use of such data structures as array,
file and list which define access methods of the items to be sorted. Such
traditional methods as exchange sort, divide and conquer sort, selection sort
and insertion sort require supervisory control program. The supervisory control
program has access to the items and is responsible for arranging them in the
proper order. This paper presents a different sorting algorithm that does not
require supervisory control program. The objects sort themselves and they are
able to terminate when sorting is completed. The algorithm also employs
parallel processing mechanisms to increase its efficiency and effectiveness.
The paper makes a review of the traditional sorting methods, identifying their
pros and cons and proposes a different design based on conceptual combination
of these algorithms. Algorithms designed were implemented and tested in Java
desktop application
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3052</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3052</id><created>2012-09-13</created><authors><author><keyname>Opoku</keyname><forenames>Samuel King</forenames><affiliation>Kumasi Polytechnic</affiliation></author></authors><title>A Simultaneous-Movement Mobile Multiplayer Game Design based on Adaptive
  Background Partitioning Technique</title><categories>cs.GT cs.HC cs.MM</categories><comments>8 pages</comments><journal-ref>Cyber Journals: Multidisciplinary Journals in Science and
  Technology, JSAT, Vol. 3, No. 4, pg 1-8, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementations of mobile games have become prevalent industrial technology
due to the ubiquitous nature of mobile devices. However, simultaneous-movement
multiplayer games, games that a player competes simultaneously with other
players, are usually affected by such parameters as latency, type of game
architecture and type of communication technology. This paper makes a review of
the above parameters, considering the pros and cons of the various techniques
used in addressing each parameter. It then goes ahead to propose an enhanced
mechanism for dealing with packet delays based on partitioning the game
background into grids. The proposed design is implemented and tested using
Bluetooth and Wi-Fi communication technologies. The efficiency and
effectiveness of the design are also analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3053</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3053</id><created>2012-09-13</created><authors><author><keyname>Opoku</keyname><forenames>Samuel King</forenames><affiliation>Kumasi Polytechnic</affiliation></author></authors><title>An Indoor Tracking System Based on Bluetooth Technology</title><categories>cs.NI</categories><comments>8 pages</comments><journal-ref>Cyber Journals: Multidisciplinary Journals in Science and
  Technology, Journal of Selected Areas in Telecommunications (JSAT), Vol. 2,
  No. 12, pages 1-8, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementations of tracking systems have become prevalent issues in modern
technology due to its advantage of location detection of objects. Objects are
usually tracked using trackers based on GPS, GSM, RFID and Bluetooth signal
strength implementation. These mechanisms usually require line of sight
operations, limited coverage and low level programming language for accessing
Bluetooth signal strength. This paper presents an alternative technique for
tracking the movement of indoor objects based on Bluetooth communication
technology, principles of motion and least square statistical method.
Algorithms are designed and implemented using Java
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3054</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3054</id><created>2012-09-13</created><authors><author><keyname>Kent</keyname><forenames>Robert E.</forenames></author></authors><title>Database Semantics</title><categories>cs.DB math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper, the first step to connect relational databases with systems
consequence (Kent: &quot;System Consequence&quot; 2009), is concerned with the semantics
of relational databases. It aims to to study system consequence in the
logical/semantic system of relational databases. The paper, which was inspired
by and which extends a recent set of papers on the theory of relational
database systems (Spivak: &quot;Functorial Data Migration&quot; 2012), is linked with
work on the Information Flow Framework (IFF) [http://suo.ieee.org/IFF/]
connected with the ontology standards effort (SUO), since relational databases
naturally embed into first order logic. The database semantics discussed here
is concerned with the conceptual level of database architecture. We offer both
an intuitive and technical discussion. Corresponding to the notions of primary
and foreign keys, relational database semantics takes two forms: a
distinguished form where entities are distinguished from relations, and a
unified form where relations and entities coincide. The distinguished form
corresponds to the theory presented in (Spivak: &quot;Simplicial databases&quot;
2009)[arXiv:0904.2012]. The unified form, a special case of the distinguished
form, corresponds to the theory presented in (Spivak: &quot;Functorial Data
Migration&quot; 2012). A later paper will discuss various formalisms of relational
databases, such as relational algebra and first order logic, and will complete
the description of the relational database logical environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3056</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3056</id><created>2012-09-13</created><authors><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Woznica</keyname><forenames>Adam</forenames></author><author><keyname>Kalousis</keyname><forenames>Alexandros</forenames></author></authors><title>Parametric Local Metric Learning for Nearest Neighbor Classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning local metrics for nearest neighbor
classification. Most previous works on local metric learning learn a number of
local unrelated metrics. While this &quot;independence&quot; approach delivers an
increased flexibility its downside is the considerable risk of overfitting. We
present a new parametric local metric learning method in which we learn a
smooth metric matrix function over the data manifold. Using an approximation
error bound of the metric matrix function we learn local metrics as linear
combinations of basis metrics defined on anchor points over different regions
of the instance space. We constrain the metric matrix function by imposing on
the linear combinations manifold regularization which makes the learned metric
matrix function vary smoothly along the geodesics of the data manifold. Our
metric learning method has excellent performance both in terms of predictive
power and scalability. We experimented with several large-scale classification
problems, tens of thousands of instances, and compared it with several state of
the art metric learning methods, both global and local, as well as to SVM with
automatic kernel selection, all of which it outperforms in a significant
manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3061</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3061</id><created>2012-09-13</created><authors><author><keyname>Arrag</keyname><forenames>Sliman</forenames></author><author><keyname>Hamdoun</keyname><forenames>Abdellatif</forenames></author><author><keyname>Tragha</keyname><forenames>Abderrahim</forenames></author><author><keyname>Khamlich</keyname><forenames>Salah eddine</forenames></author></authors><title>Design and Implementation A different Architectures of mixcolumn in FPGA</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper details Implementation of the Encryption algorithm AES under VHDL
language In FPGA by using different architecture of mixcolumn. We then review
this research investigates the AES algorithm in FPGA and the Very High Speed
Integrated Circuit Hardware Description language (VHDL). Altera Quartus II
software is used for simulation and optimization of the synthesizable VHDL
code. The set of transformations of both Encryptions and decryption are
simulated using an iterative design approach in order to optimize the hardware
consumption. Altera Cyclone III Family devices are utilized for hardware
evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3080</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3080</id><created>2012-09-13</created><updated>2012-09-17</updated><authors><author><keyname>Yao</keyname><forenames>Yong</forenames></author><author><keyname>Xu</keyname><forenames>Jia</forenames></author><author><keyname>Zhang</keyname><forenames>Jingzhong</forenames></author></authors><title>The expansion of real forms on the simplex and applications</title><categories>math.AG cs.SC</categories><comments>10 pages, 1 figures</comments><msc-class>13J25, 13J30, 16Y60, 68T15, 26D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If n points B_1,---,B_n$ in the standard simplex \Delta_n are affinely
independent, then they can span an (n-1)-simplex denoted by
\Lambda=Con(B_1,---,B_n). Here \Lambda corresponds to an n*n matrix [\Lambda]
whose columns are B_1,---,B_n. In this paper, we firstly proved that if \Lambda
of diameter sufficiently small contains a point $P$, and f(P)&gt;0 (&lt;0) for a form
f in R[X], then the coefficients of f([\Lambda] X) are all positive (negative).
Next, as an application of this result, a necessary and sufficient condition
for determining the real zeros on \Delta_n of a system of homogeneous algebraic
equations with integral coefficients is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3089</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3089</id><created>2012-09-14</created><authors><author><keyname>Adda</keyname><forenames>Mehdi</forenames></author><author><keyname>Wu</keyname><forenames>Lei</forenames></author><author><keyname>White</keyname><forenames>Sharon</forenames></author><author><keyname>Feng</keyname><forenames>Yi</forenames></author></authors><title>Pattern Detection with Rare Item-set Mining</title><categories>cs.SE cs.DB</categories><comments>17 pages, 5 figures, International Journal on Soft Computing,
  Artificial Intelligence and Applications (IJSCAI), Vol.1, No.1, August 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discovery of new and interesting patterns in large datasets, known as
data mining, draws more and more interest as the quantities of available data
are exploding. Data mining techniques may be applied to different domains and
fields such as computer science, health sector, insurances, homeland security,
banking and finance, etc. In this paper we are interested by the discovery of a
specific category of patterns, known as rare and non-present patterns. We
present a novel approach towards the discovery of non-present patterns using
rare item-set mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3093</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3093</id><created>2012-09-14</created><authors><author><keyname>Kale</keyname><forenames>Hema</forenames><affiliation>ETC Department, Jhulelal Institute of Technology Nagpur, India</affiliation></author><author><keyname>Dethe</keyname><forenames>C. G.</forenames><affiliation>ECE Department, Priyadarshni Institute of Engineering and Technology, Nagpur, India</affiliation></author><author><keyname>Mushrif</keyname><forenames>M. M.</forenames><affiliation>ETC Department, Yashwantrao Chavan College of Engineering Nagpur, India</affiliation></author></authors><title>Improved Algorithm for Throughput Maximization in MC-CDMA</title><categories>cs.NI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Multi-Carrier Code Division Multiple Access (MC-CDMA) is becoming a very
significant downlink multiple access technique for high-rate data transmission
in the fourth generation wireless communication systems. By means of efficient
resource allocation higher data rate i.e. throughput can be achieved. This
paper evaluates the performance of criteria used for group (subchannel)
allocation employed in downlink transmission, which results in throughput
maximization. Proposed algorithm gives the modified technique of sub channel
allocation in the downlink transmission of MC-CDMA systems. Simulation are
carried out for all the three combining schemes, results shows that for the
given power and BER proposed algorithm comparatively gives far better results
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3099</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3099</id><created>2012-09-14</created><authors><author><keyname>Boukhobza</keyname><forenames>Jalil</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Olivier</keyname><forenames>Pierre</forenames><affiliation>Lab-STICC</affiliation></author><author><keyname>Rubini</keyname><forenames>St&#xe9;phane</forenames><affiliation>LISYC</affiliation></author></authors><title>A Cache Management Strategy to Replace Wear Leveling Techniques for
  Embedded Flash Memory</title><categories>cs.AR</categories><comments>Ce papier a obtenu le &quot;Best Paper Award&quot; dans le &quot;Computer System
  track&quot; nombre de page: 8; International Symposium on Performance Evaluation
  of Computer &amp; Telecommunication Systems, La Haye : Netherlands (2011)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prices of NAND flash memories are falling drastically due to market growth
and fabrication process mastering while research efforts from a technological
point of view in terms of endurance and density are very active. NAND flash
memories are becoming the most important storage media in mobile computing and
tend to be less confined to this area. The major constraint of such a
technology is the limited number of possible erase operations per block which
tend to quickly provoke memory wear out. To cope with this issue,
state-of-the-art solutions implement wear leveling policies to level the wear
out of the memory and so increase its lifetime. These policies are integrated
into the Flash Translation Layer (FTL) and greatly contribute in decreasing the
write performance. In this paper, we propose to reduce the flash memory wear
out problem and improve its performance by absorbing the erase operations
throughout a dual cache system replacing FTL wear leveling and garbage
collection services. We justify this idea by proposing a first performance
evaluation of an exclusively cache based system for embedded flash memories.
Unlike wear leveling schemes, the proposed cache solution reduces the total
number of erase operations reported on the media by absorbing them in the cache
for workloads expressing a minimal global sequential rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3102</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3102</id><created>2012-09-14</created><updated>2013-10-15</updated><authors><author><keyname>Estrada</keyname><forenames>Octavio Andr&#xe9;s Gonz&#xe1;lez</forenames><affiliation>IMAM</affiliation></author><author><keyname>Nadal</keyname><forenames>E.</forenames><affiliation>CITV</affiliation></author><author><keyname>R&#xf3;denas</keyname><forenames>J. J.</forenames><affiliation>CITV</affiliation></author><author><keyname>Kerfriden</keyname><forenames>Pierre</forenames><affiliation>IMAM</affiliation></author><author><keyname>Bordas</keyname><forenames>St&#xe9;phane Pierre-Alain</forenames><affiliation>IMAM</affiliation></author><author><keyname>Fuenmayor</keyname><forenames>F. J.</forenames><affiliation>CITV</affiliation></author></authors><title>Mesh adaptivity driven by goal-oriented locally equilibrated
  superconvergent patch recovery</title><categories>math.NA cs.NA physics.class-ph</categories><comments>in Computational Mechanics</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last decade there has been an increase on the use of goal-oriented
error estimates which help to quantify and control the local error on a
quantity of interest (QoI) that might result relevant for design purposes (e.g.
the mean stress or mean displacement in a particular area, the stress intensity
factor for fracture problems,...). Residual-based error estimators have been
used to estimate the error in quantities of interest for finite element
approximations. This work presents a recovery-based error estimation technique
for QoI whose main characteristic is the use of an enhanced version of the
Superconvergent Patch Recovery (SPR) technique developed by Zienkiewicz and
Zhu. This enhanced version of the SPR technique, used to recover the primal and
dual solutions, provides a nearly statically admissible stress field that
results in accurate estimations of the error in the QoI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3105</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3105</id><created>2012-09-14</created><authors><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author></authors><title>Spectrum Leasing and Cooperative Resource Allocation in Cognitive OFDMA
  Networks</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE/KICS Journal of Communications and Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a cooperative OFDMA-based cognitive radio network where
the primary system leases some of its subchannels to the secondary system for a
fraction of time in exchange for the secondary users (SUs) assisting the
transmission of primary users (PUs) as relays. Our aim is to determine the
cooperation strategies among the primary and secondary systems so as to
maximize the sum-rate of SUs while maintaining quality-of-service (QoS)
requirements of PUs. We formulate a joint optimization problem of PU
transmission mode selection, SU (or relay) selection, subcarrier assignment,
power control, and time allocation. By applying dual method, this mixed integer
programming problem is decomposed into parallel per-subcarrier subproblems,
with each determining the cooperation strategy between one PU and one SU. We
show that, on each leased subcarrier, the optimal strategy is to let a SU
exclusively act as a relay or transmit for itself. This result is fundamentally
different from the conventional spectrum leasing in single-channel systems
where a SU must transmit a fraction of time for itself if it helps the PU's
transmission. We then propose a subgradient-based algorithm to find the
asymptotically optimal solution to the primal problem in polynomial time.
Simulation results demonstrate that the proposed algorithm can significantly
enhance the network performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3113</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3113</id><created>2012-09-14</created><authors><author><keyname>Tander</keyname><forenames>Baran</forenames></author><author><keyname>&#xd6;zmen</keyname><forenames>Atilla</forenames></author><author><keyname>Ba&#x15f;kan</keyname><forenames>Murat</forenames></author></authors><title>Detection and Classification of Viewer Age Range Smart Signs at TV
  Broadcast</title><categories>cs.CV</categories><comments>11 pages, 13 figures, 2 tables</comments><doi>10.5121/sipij.2012.3410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the identification and classification of Viewer Age Range
Smart Signs, designed by the Radio and Television Supreme Council of Turkey, to
give age range information for the TV viewers, are realized. Therefore, the
automatic detection at the broadcast will be possible, enabling the
manufacturing of TV receivers which are sensible to these signs. The most
important step at this process is the pattern recognition. Since the symbols
that must be identified are circular, various circle detection techniques can
be employed. In our study, first, two different circle segmentation methods for
still images are analyzed, their advantages and drawbacks are discussed. A
popular neural network structure called Multilayer Perceptron is employed for
the classification. Afterwards, the same procedures are carried out for
streaming video. All of the steps depicted above are realized on a standard PC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3117</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3117</id><created>2012-09-14</created><authors><author><keyname>Shamsi</keyname><forenames>Khurram Naim</forenames></author><author><keyname>Khan</keyname><forenames>Zafar Iqbal</forenames></author></authors><title>Development of an e-learning system incorporating semantic web</title><categories>cs.CY cs.IR</categories><comments>4 pages, 2 figures, published at International Journal of Research in
  Computer Science</comments><journal-ref>International Journal of Research in Computer Science, 2 (5): pp.
  11-14, 2012</journal-ref><doi>10.7815/ijorcs.25.2012.042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-Learning is efficient, task relevant and just-in-time learning grown from
the learning requirements of the new and dynamically changing world. The term
Semantic Web covers the steps to create a new WWW architecture that augments
the content with formal semantics enabling better possibilities of navigation
through the cyberspace and its contents. In this paper, we present the Semantic
Web-Based model for our e-learning system taking into account the learning
environment at Saudi Arabian universities. The proposed system is mainly based
on ontology-based descriptions of content, context and structure of the
learning materials. It further provides flexible and personalized access to
these learning materials. The framework has been validated by an interview
based qualitative method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3126</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3126</id><created>2012-09-14</created><authors><author><keyname>Torres-Moreno</keyname><forenames>Juan-Manuel</forenames></author></authors><title>Beyond Stemming and Lemmatization: Ultra-stemming to Improve Automatic
  Text Summarization</title><categories>cs.IR cs.CL</categories><comments>22 pages, 12 figures, 9 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In Automatic Text Summarization, preprocessing is an important phase to
reduce the space of textual representation. Classically, stemming and
lemmatization have been widely used for normalizing words. However, even using
normalization on large texts, the curse of dimensionality can disturb the
performance of summarizers. This paper describes a new method for normalization
of words to further reduce the space of representation. We propose to reduce
each word to its initial letters, as a form of Ultra-stemming. The results show
that Ultra-stemming not only preserve the content of summaries produced by this
representation, but often the performances of the systems can be dramatically
improved. Summaries on trilingual corpora were evaluated automatically with
Fresa. Results confirm an increase in the performance, regardless of summarizer
system used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3129</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3129</id><created>2012-09-14</created><authors><author><keyname>Smerieri</keyname><forenames>Anteo</forenames></author><author><keyname>Duport</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Paquot</keyname><forenames>Yvan</forenames></author><author><keyname>Schrauwen</keyname><forenames>Benjamin</forenames></author><author><keyname>Haelterman</keyname><forenames>Marc</forenames></author><author><keyname>Massar</keyname><forenames>Serge</forenames></author></authors><title>Analog readout for optical reservoir computers</title><categories>cs.ET cs.LG cs.NE physics.optics</categories><comments>to appear in NIPS 2012</comments><journal-ref>Advances in Neural Information Processing Systems (NIPS) 25,
  953-961 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reservoir computing is a new, powerful and flexible machine learning
technique that is easily implemented in hardware. Recently, by using a
time-multiplexed architecture, hardware reservoir computers have reached
performance comparable to digital implementations. Operating speeds allowing
for real time information operation have been reached using optoelectronic
systems. At present the main performance bottleneck is the readout layer which
uses slow, digital postprocessing. We have designed an analog readout suitable
for time-multiplexed optoelectronic reservoir computers, capable of working in
real time. The readout has been built and tested experimentally on a standard
benchmark task. Its performance is better than non-reservoir methods, with
ample room for further improvement. The present work thereby overcomes one of
the major limitations for the future development of hardware reservoir
computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3137</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3137</id><created>2012-09-14</created><updated>2013-03-28</updated><authors><author><keyname>Zhou</keyname><forenames>Qing F.</forenames></author><author><keyname>Zhang</keyname><forenames>Q. T.</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author></authors><title>Diophantine Approach to Blind Interference Alignment of Homogeneous
  K-user 2x1 MISO Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the sufficient condition for a blindly interference-aligned (BIA)
2-user 2x1 broadcast channel (BC) in homogeneous fading to achieve its maximal
4/3 DoF is well understood, its counterpart for the general K-user 2x1 MISO BC
in homogeneous block fading to achieve the corresponding 2k/(2+K-1) (DoF)
remains unsolved and is, thus, the focus of this paper. An interference channel
is said BIA-feasible if it achieves its maximal DoF only via BIA. In this
paper, we cast this general feasibility problem in the framework of finding
integer solutions for a system of linear Diophantine equations. By assuming
independent user links each of the same coherence time and by studying the
solvability of the Diophantine system, we derive the sufficient and necessary
conditions on the K users' fading block offsets to ensure the BIA feasibility
of the K-user BC. If the K offsets are independent and uniformly distributed
over a coherence block, we can further prove that 11 users are enough for one
to find, with certainty of 95%, 3 users among them to form a BIA-feasible
3-user 2x1 BC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3150</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3150</id><created>2012-09-14</created><authors><author><keyname>G&#xfc;rcan</keyname><forenames>&#xd6;nder</forenames></author><author><keyname>Dikenelli</keyname><forenames>O&#x11f;uz</forenames></author><author><keyname>T&#xfc;rker</keyname><forenames>Kemal S.</forenames></author></authors><title>Agent-based Exploration of Wirings of Biological Neural Networks:
  Position Paper</title><categories>cs.NE q-bio.NC</categories><comments>6 pages, 1 figure, ABModSim 2010 Workshop</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The understanding of human central nervous system depends on knowledge of its
wiring. However, there are still gaps in our understanding of its wiring due to
technical difficulties. While some information is coming out from human
experiments, medical research is lacking of simulation models to put current
findings together to obtain the global picture and to predict hypotheses to
lead future experiments. Agent-based modeling and simulation (ABMS) is a strong
candidate for the simulation model. In this position paper, we discuss the
current status of &quot;neural wiring&quot; and &quot;ABMS in biological systems&quot;. In
particular, we discuss that the ABMS context provides features required for
exploration of biological neural wiring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3155</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3155</id><created>2012-09-14</created><authors><author><keyname>Alves</keyname><forenames>Rui</forenames></author><author><keyname>Lim</keyname><forenames>Veranika</forenames></author><author><keyname>Niforatos</keyname><forenames>Evangelos</forenames></author><author><keyname>Chen</keyname><forenames>Monchu</forenames></author><author><keyname>Karapanos</keyname><forenames>Evangelos</forenames></author><author><keyname>Nunes</keyname><forenames>Nuno Jardim</forenames></author></authors><title>Augmenting Customer Journey Maps with quantitative empirical data: a
  case on EEG and eye tracking</title><categories>cs.HC</categories><comments>2 pages, 3 figures. Published in DIS2012, in Newcastle, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the use of electroencephalography (EEG) and eye
tracking in exploring customer experiences in service design. These tools are
expected to allow designers to generate customer journeys from empirical data
leading to new visualization methods and therefore improvements in service
design deliverables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3190</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3190</id><created>2012-09-14</created><authors><author><keyname>Wocjan</keyname><forenames>Pawel</forenames></author><author><keyname>Elphick</keyname><forenames>Clive</forenames></author></authors><title>New spectral bounds on the chromatic number encompassing all eigenvalues
  of the adjacency matrix</title><categories>math.CO cs.DM quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this article is to improve existing lower bounds on the
chromatic number chi. Let mu_1,...,mu_n be the eigenvalues of the adjacency
matrix sorted in non-increasing order.
  First, we prove the lower bound chi &gt;= 1 + max_m {sum_{i=1}^m mu_i / -
sum_{i=1}^m mu_{n-i+1}} for m=1,...,n-1. This generalizes the Hoffman lower
bound which only involves the maximum and minimum eigenvalues, i.e., the case
$m=1$. We provide several examples for which the new bound exceeds the {\sc
Hoffman} lower bound.
  Second, we conjecture the lower bound chi &gt;= 1 + S^+ / S^-, where S^+ and S^-
are the sums of the squares of positive and negative eigenvalues, respectively.
To corroborate this conjecture, we prove the weaker bound chi &gt;= S^+/S^-. We
show that the conjectured lower bound is tight for several families of graphs.
We also performed various searches for a counter-example, but none was found.
  Our proofs rely on a new technique of converting the adjacency matrix into
the zero matrix by conjugating with unitary matrices and use majorization of
spectra of self-adjoint matrices.
  We also show that the above bounds are actually lower bounds on the
normalized orthogonal rank of a graph, which is always less than or equal to
the chromatic number. The normalized orthogonal rank is the minimum dimension
making it possible to assign vectors with entries of modulus one to the
vertices such that two such vectors are orthogonal if the corresponding
vertices are connected.
  All these bounds are also valid when we replace the adjacency matrix A by W *
A where W is an arbitrary self-adjoint matrix and * denotes the Schur product,
that is, entrywise product of W and A.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3201</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3201</id><created>2012-09-14</created><authors><author><keyname>Billera</keyname><forenames>Louis J.</forenames></author><author><keyname>Blanco</keyname><forenames>Sa&#xfa;l A.</forenames></author></authors><title>Bandwidth of the product of paths of the same length</title><categories>math.CO cs.DM</categories><comments>10 pages, 1 figure</comments><msc-class>05C30 (Primary) 05A99, 97N70 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we give a numerical expression for the bandwidth $bw(P_{n}^{d})$
of the $d$-product of a path with $n$ edges, $P_{n}^{d}$. We prove that this
bandwidth is given by the sum of certain multinomial coefficients. We also show
that $bw(P_{n}^{d})$ is bounded above and below by the largest coefficient in
the expansion of $(1+x+...+x^{n})^{k}$, with $k\in{d,d+1}$. Moreover, we
compare the asymptotic behavior of $bw(P_{n}^{d})$ with the bandwidth of the
labeling obtained by ordering the vertices of $P_{n}^{d}$ in lexicographic
order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3203</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3203</id><created>2012-09-14</created><updated>2013-07-24</updated><authors><author><keyname>Di Marco</keyname><forenames>Piergiuseppe</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Santucci</keyname><forenames>Fortunato</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Modeling IEEE 802.15.4 Networks over Fading Channels</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the performance of the medium access control (MAC) of the IEEE
802.15.4 has been investigated under the assumption of ideal wireless channel,
the understanding of the cross-layer dynamics between MAC and physical layer is
an open problem when the wireless channel exhibits path loss, multi-path
fading, and shadowing. The analysis of MAC and wireless channel interaction is
essential for consistent performance prediction, correct design and
optimization of the protocols. In this paper, a novel approach to analytical
modeling of these interactions is proposed. The analysis considers
simultaneously a composite channel fading, interference generated by multiple
terminals, the effects induced by hidden terminals, and the MAC reduced carrier
sensing capabilities. Depending on the MAC parameters and physical layer
thresholds, it is shown that the MAC performance indicators over fading
channels can be far from those derived under ideal channel assumptions. As
novel results, we show to what extent the presence of fading may be beneficial
for the overall network performance by reducing the multiple access
interference, and how this can be used to drive joint selection of MAC and
physical layer parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3234</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3234</id><created>2012-09-14</created><authors><author><keyname>Velner</keyname><forenames>Yaron</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Rabinovich</keyname><forenames>Alexander</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Francois</forenames></author></authors><title>The Complexity of Multi-Mean-Payoff and Multi-Energy Games</title><categories>cs.GT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1007.1669</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mean-payoff games, the objective of the protagonist is to ensure that the
limit average of an infinite sequence of numeric weights is nonnegative. In
energy games, the objective is to ensure that the running sum of weights is
always nonnegative. Multi-mean-payoff and multi-energy games replace individual
weights by tuples, and the limit average (resp. running sum) of each coordinate
must be (resp. remain) nonnegative. These games have applications in the
synthesis of resource-bounded processes with multiple resources.
  We prove the finite-memory determinacy of multi-energy games and show the
inter-reducibility of multimean-payoff and multi-energy games for finite-memory
strategies. We also improve the computational complexity for solving both
classes of games with finite-memory strategies: while the previously best known
upper bound was EXPSPACE, and no lower bound was known, we give an optimal
coNP-complete bound. For memoryless strategies, we show that the problem of
deciding the existence of a winning strategy for the protagonist is
NP-complete. Finally we present the first solution of multi-meanpayoff games
with infinite-memory strategies. We show that multi-mean-payoff games with
mean-payoff-sup objectives can be decided in NP and coNP, whereas
multi-mean-payoff games with mean-payoff-inf objectives are coNP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3236</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3236</id><created>2012-08-29</created><authors><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>A note on &quot;Folding wheels and fans.&quot;</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In S.Gervacio, R.Guerrero and H.Rara, Folding wheels and fans, Graphs and
Combinatorics 18 (2002) 731-737, the authors obtain formulas for the clique
numbers onto which wheels and fans fold. We present an interpolation theorem
which generalizes their theorems 4.2 and 5.2. We show that their formula for
wheels is wrong. We show that for threshold graphs, the achromatic number and
folding number coincides with the chromatic number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3253</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3253</id><created>2012-09-14</created><authors><author><keyname>Roemer</keyname><forenames>Florian</forenames></author><author><keyname>Haardt</keyname><forenames>Martin</forenames></author></authors><title>A framework for the analytical performance assessment of matrix and
  tensor-based ESPRIT-type algorithms</title><categories>cs.PF cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a generic framework for the asymptotic performance
analysis of subspace-based parameter estimation schemes. It is based on earlier
results on an explicit first-order expansion of the estimation error in the
signal subspace obtained via an SVD of the noisy observation matrix. We extend
these results in a number of aspects. Firstly, we derive an explicit
first-order expansion of the Higher- Order SVD (HOSVD)-based subspace estimate.
Secondly, we show how to obtain explicit first-order expansions of the
estimation error of ESPRIT-type algorithms and provide the expressions for
matrix-based and tensor-based Standard ESPRIT and Unitary ESPRIT. Thirdly, we
derive closed-form expressions for the mean square error (MSE) and show that
they only depend on the second-order moments of the noise. Hence, we only need
the noise to be zero mean and possess finite second order moments. Fourthly, we
investigate the effect of using Structured Least Squares (SLS) to solve the
overdetermined shift invariance equations in ESPRIT and provide an explicit
first-order expansion as well as a closed-form MSE expression. Finally, we
simplify the MSE for the special case of a single source and compute the
asymptotic efficiency of the investigated ESPRIT-type algorithms in compact
closed-form expressions which only depend on the array size and the effective
SNR. Our results are more general than existing results on the performance
analysis of ESPRIT-type algorithms since (a) we do not need any assumptions
about the noise except for the mean to be zero and the second-order moments to
be finite (in contrast to earlier results that require Gaussianity or
second-order circular symmetry); (b) our results are asymptotic in the
effective SNR, i.e., we do not require the number of samples to be large; (c)
we present a framework that incorporates various ESPRIT-type algorithms in one
unified manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3275</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3275</id><created>2012-09-14</created><authors><author><keyname>Ribeiro</keyname><forenames>A. C.</forenames></author><author><keyname>de Figueiredo</keyname><forenames>C. M. H.</forenames></author><author><keyname>Marquezino</keyname><forenames>F. L.</forenames></author><author><keyname>Kowada</keyname><forenames>L. A. B.</forenames></author></authors><title>Cayley graphs and analysis of quantum cost for reversible circuit
  synthesis</title><categories>cs.DM quant-ph</categories><comments>IV Workshop-School on Quantum Computation and Information (WECIQ
  2012), Fortaleza, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the theory of Cayley graphs as a framework to analyse gate counts
and quantum costs resulting from reversible circuit synthesis. Several methods
have been proposed in the reversible logic synthesis literature by considering
different libraries whose gates are associated to the generating sets of
certain Cayley graphs. In a Cayley graph, the distance between two vertices
corresponds to the optimal circuit size. The lower bound for the diameter of
Cayley graphs is also a lower bound for the worst case for any algorithm that
uses the corresponding gate library. In this paper, we study two Cayley graphs
on the Symmetric Group $S_{2^n}$: the first, denoted by $I_n$, is defined by a
generating set associated to generalized Toffoli gates; and the second, the
hypercube Cayley graph $H_n$, is defined by a generating set associated to
multiple-control Toffoli gates. Those two Cayley graphs have degree $n2^{n-1}$
and order $2^n!$. Maslov, Dueck and Miller proposed a reversible circuit
synthesis that we model by the Cayley graph $I_n$. We propose a synthesis
algorithm based on the Cayley graph $H_n$ with upper bound of $(n-1)2^{n}+1$
multiple-control Toffoli gates. In addition, the diameter of the Cayley graph
$H_n$ gives a lower bound of $n2^{n-1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3286</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3286</id><created>2012-09-14</created><updated>2012-09-17</updated><authors><author><keyname>Glazyrin</keyname><forenames>Nikolay</forenames></author></authors><title>Music Recommendation System for Million Song Dataset Challenge</title><categories>cs.IR cs.SI</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a system that took 8th place in Million Song Dataset challenge
is described. Given full listening history for 1 million of users and half of
listening history for 110000 users participatints should predict the missing
half. The system proposed here uses memory-based collaborative filtering
approach and user-based similarity. MAP@500 score of 0.15037 was achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3300</identifier>
 <datestamp>2012-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3300</id><created>2012-09-14</created><authors><author><keyname>Al-Bashabsheh</keyname><forenames>Ali</forenames></author><author><keyname>Mao</keyname><forenames>Yongyi</forenames></author></authors><title>Normal Factor Graphs as Probabilistic Models</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new probabilistic modelling framework based on the recent notion
of normal factor graph (NFG). We show that the proposed NFG models and their
transformations unify some existing models such as factor graphs, convolutional
factor graphs, and cumulative distribution networks. The two subclasses of the
NFG models, namely the constrained and generative models, exhibit a duality in
their dependence structure. Transformation of NFG models further extends the
power of this modelling framework. We point out the well-known NFG
representations of parity and generator realizations of a linear code as
generative and constrained models, and comment on a more prevailing duality in
this context. Finally, we address the algorithmic aspect of computing the
exterior function of NFGs and the inference problem on NFGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3307</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3307</id><created>2012-09-14</created><updated>2013-06-25</updated><authors><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Brockmann</keyname><forenames>Dirk</forenames></author></authors><title>Natural emergence of clusters and bursts in network evolution</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO</categories><comments>6 pages, 4 figures</comments><journal-ref>Phys. Rev. X 3, 021016 (2013)</journal-ref><doi>10.1103/PhysRevX.3.021016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network models with preferential attachment, where new nodes are injected
into the network and form links with existing nodes proportional to their
current connectivity, have been well studied for some time. Extensions have
been introduced where nodes attach proportionally to arbitrary fitness
functions. However, in these models, attaching to a node always increases the
ability of that node to gain more links in the future. We study network growth
where nodes attach proportionally to the clustering coefficients, or local
densities of triangles, of existing nodes. Attaching to a node typically lowers
its clustering coefficient, in contrast to preferential attachment or
rich-get-richer models. This simple modification naturally leads to a variety
of rich phenomena, including aging, non-Poissonian bursty dynamics, and
community formation. This theoretical model shows that complex network
structure can be generated without artificially imposing multiple dynamical
mechanisms and may reveal potentially overlooked mechanisms present in complex
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3312</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3312</id><created>2012-09-14</created><updated>2013-05-15</updated><authors><author><keyname>Yap</keyname><forenames>Han Lun</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author><author><keyname>Rozell</keyname><forenames>Christopher J.</forenames></author></authors><title>Stable Manifold Embeddings with Structured Random Matrices</title><categories>cs.IT math.DG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fields of compressed sensing (CS) and matrix completion have shown that
high-dimensional signals with sparse or low-rank structure can be effectively
projected into a low-dimensional space (for efficient acquisition or
processing) when the projection operator achieves a stable embedding of the
data by satisfying the Restricted Isometry Property (RIP). It has also been
shown that such stable embeddings can be achieved for general Riemannian
submanifolds when random orthoprojectors are used for dimensionality reduction.
Due to computational costs and system constraints, the CS community has
recently explored the RIP for structured random matrices (e.g., random
convolutions, localized measurements, deterministic constructions). The main
contribution of this paper is to show that any matrix satisfying the RIP (i.e.,
providing a stable embedding for sparse signals) can be used to construct a
stable embedding for manifold-modeled signals by randomizing the column signs
and paying reasonable additional factors in the number of measurements. We
demonstrate this result with several new constructions for stable manifold
embeddings using structured matrices. This result allows advances in efficient
projection schemes for sparse signals to be immediately applied to manifold
signal models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3314</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3314</id><created>2012-09-14</created><authors><author><keyname>Teodoro</keyname><forenames>George</forenames></author><author><keyname>Pan</keyname><forenames>Tony</forenames></author><author><keyname>Kurc</keyname><forenames>Tahsin</forenames></author><author><keyname>Kong</keyname><forenames>Jun</forenames></author><author><keyname>Cooper</keyname><forenames>Lee</forenames></author><author><keyname>Saltz</keyname><forenames>Joel</forenames></author></authors><title>Efficient Irregular Wavefront Propagation Algorithms on Hybrid CPU-GPU
  Machines</title><categories>cs.DC cs.DS</categories><comments>37 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of efficient execution of a computation
pattern, referred to here as the irregular wavefront propagation pattern
(IWPP), on hybrid systems with multiple CPUs and GPUs. The IWPP is common in
several image processing operations. In the IWPP, data elements in the
wavefront propagate waves to their neighboring elements on a grid if a
propagation condition is satisfied. Elements receiving the propagated waves
become part of the wavefront. This pattern results in irregular data accesses
and computations. We develop and evaluate strategies for efficient computation
and propagation of wavefronts using a multi-level queue structure. This queue
structure improves the utilization of fast memories in a GPU and reduces
synchronization overheads. We also develop a tile-based parallelization
strategy to support execution on multiple CPUs and GPUs. We evaluate our
approaches on a state-of-the-art GPU accelerated machine (equipped with 3 GPUs
and 2 multicore CPUs) using the IWPP implementations of two widely used image
processing operations: morphological reconstruction and euclidean distance
transform. Our results show significant performance improvements on GPUs. The
use of multiple CPUs and GPUs cooperatively attains speedups of 50x and 85x
with respect to single core CPU executions for morphological reconstruction and
euclidean distance transform, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3315</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3315</id><created>2012-09-14</created><authors><author><keyname>Harrison</keyname><forenames>P. G.</forenames></author><author><keyname>Harrison</keyname><forenames>S. K.</forenames></author><author><keyname>Patel</keyname><forenames>N. M.</forenames></author><author><keyname>Zertal</keyname><forenames>S.</forenames></author></authors><title>Storage Workload Modelling by Hidden Markov Models: Application to FLASH
  Memory</title><categories>cs.PF</categories><comments>29 pages, 18 figures</comments><journal-ref>Performance Evaluation 69, 1 (2012), 17-40</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A workload analysis technique is presented that processes data from operation
type traces and creates a Hidden Markov Model (HMM) to represent the workload
that generated those traces. The HMM can be used to create representative
traces for performance models, such as simulators, avoiding the need to
repeatedly acquire suitable traces. It can also be used to estimate directly
the transition probabilities and rates of a Markov modulated arrival process,
for use as input to an analytical performance model of Flash memory. The HMMs
obtained from industrial workloads are validated by comparing their
autocorrelation functions and other statistics with those of the corresponding
monitored time series. Further, the performance model applications are
illustrated by numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3318</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3318</id><created>2012-09-14</created><updated>2013-02-02</updated><authors><author><keyname>Lefkimmiatis</keyname><forenames>Stamatios</forenames></author><author><keyname>Ward</keyname><forenames>John Paul</forenames></author><author><keyname>Unser</keyname><forenames>Michael</forenames></author></authors><title>Hessian Schatten-Norm Regularization for Linear Inverse Problems</title><categories>math.OC cs.CV cs.NA</categories><comments>15 pages double-column format. This manuscript will appear in IEEE
  Transactions on Image Processing</comments><journal-ref>IEEE Trans. Image Process. 22 (2013), no. 5, 1873--1888</journal-ref><doi>10.1109/TIP.2013.2237919</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel family of invariant, convex, and non-quadratic
functionals that we employ to derive regularized solutions of ill-posed linear
inverse imaging problems. The proposed regularizers involve the Schatten norms
of the Hessian matrix, computed at every pixel of the image. They can be viewed
as second-order extensions of the popular total-variation (TV) semi-norm since
they satisfy the same invariance properties. Meanwhile, by taking advantage of
second-order derivatives, they avoid the staircase effect, a common artifact of
TV-based reconstructions, and perform well for a wide range of applications. To
solve the corresponding optimization problems, we propose an algorithm that is
based on a primal-dual formulation. A fundamental ingredient of this algorithm
is the projection of matrices onto Schatten norm balls of arbitrary radius.
This operation is performed efficiently based on a direct link we provide
between vector projections onto $\ell_q$ norm balls and matrix projections onto
Schatten norm balls. Finally, we demonstrate the effectiveness of the proposed
methods through experimental results on several inverse imaging problems with
real and simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3330</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3330</id><created>2012-09-14</created><updated>2013-04-03</updated><authors><author><keyname>Olson</keyname><forenames>Randal S.</forenames></author><author><keyname>Hintze</keyname><forenames>Arend</forenames></author><author><keyname>Dyer</keyname><forenames>Fred C.</forenames></author><author><keyname>Knoester</keyname><forenames>David B.</forenames></author><author><keyname>Adami</keyname><forenames>Christoph</forenames></author></authors><title>Predator confusion is sufficient to evolve swarming behavior</title><categories>q-bio.PE cs.NE nlin.AO q-bio.NC</categories><comments>11 pages, 6 figures. Supplementary information (including video files
  S1 and S5) in ancillary material. Videos S2-S4 are available from the authors
  upon request</comments><journal-ref>J. Royal Society Interface 10 (2013) 2010305</journal-ref><doi>10.1098/rsif.2013.0305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarming behaviors in animals have been extensively studied due to their
implications for the evolution of cooperation, social cognition, and
predator-prey dynamics. An important goal of these studies is discerning which
evolutionary pressures favor the formation of swarms. One hypothesis is that
swarms arise because the presence of multiple moving prey in swarms causes
confusion for attacking predators, but it remains unclear how important this
selective force is. Using an evolutionary model of a predator-prey system, we
show that predator confusion provides a sufficient selection pressure to evolve
swarming behavior in prey. Furthermore, we demonstrate that the evolutionary
effect of predator confusion on prey could in turn exert pressure on the
structure of the predator's visual field, favoring the frontally oriented,
high-resolution visual systems commonly observed in predators that feed on
swarming animals. Finally, we provide evidence that when prey evolve swarming
in response to predator confusion, there is a change in the shape of the
functional response curve describing the predator's consumption rate as prey
density increases. Thus, we show that a relatively simple perceptual
constraint--predator confusion--could have pervasive evolutionary effects on
prey behavior, predator sensory mechanisms, and the ecological interactions
between predators and prey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3331</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3331</id><created>2012-09-14</created><updated>2012-09-27</updated><authors><author><keyname>Bae</keyname><forenames>Jung Hyun</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Outage-based ergodic link adaptation for fading channels with delayed
  CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link adaptation in which the transmission data rate is dynamically adjusted
according to channel variation is often used to deal with time-varying nature
of wireless channel. When channel state information at the transmitter (CSIT)
is delayed by more than channel coherence time due to feedback delay, however,
the effect of link adaptation can possibly be taken away if this delay is not
taken into account. One way to deal with such delay is to predict current
channel quality given available observation, but this would inevitably result
in prediction error. In this paper, an algorithm with different view point is
proposed. By using conditional cdf of current channel given observation, outage
probability can be computed for each value of transmission rate $R$. By
assuming that the transmission block error rate (BLER) is dominated by outage
probability, the expected throughput can also be computed, and $R$ can be
determined to maximize it. The proposed scheme is designed to be optimal if
channel has ergodicity, and it is shown to considerably outperform conventional
schemes in certain Rayleigh fading channel model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3332</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3332</id><created>2012-09-14</created><authors><author><keyname>Teodoro</keyname><forenames>George</forenames></author><author><keyname>Pan</keyname><forenames>Tony</forenames></author><author><keyname>Kurc</keyname><forenames>Tahsin M.</forenames></author><author><keyname>Kong</keyname><forenames>Jun</forenames></author><author><keyname>Cooper</keyname><forenames>Lee A. D.</forenames></author><author><keyname>Saltz</keyname><forenames>Joel H.</forenames></author></authors><title>High-throughput Execution of Hierarchical Analysis Pipelines on Hybrid
  Cluster Platforms</title><categories>cs.DC cs.SY</categories><comments>12 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose, implement, and experimentally evaluate a runtime middleware to
support high-throughput execution on hybrid cluster machines of large-scale
analysis applications. A hybrid cluster machine consists of computation nodes
which have multiple CPUs and general purpose graphics processing units (GPUs).
Our work targets scientific analysis applications in which datasets are
processed in application-specific data chunks, and the processing of a data
chunk is expressed as a hierarchical pipeline of operations. The proposed
middleware system combines a bag-of-tasks style execution with coarse-grain
dataflow execution. Data chunks and associated data processing pipelines are
scheduled across cluster nodes using a demand driven approach, while within a
node operations in a given pipeline instance are scheduled across CPUs and
GPUs. The runtime system implements several optimizations, including
performance aware task scheduling, architecture aware process placement, data
locality conscious task assignment, and data prefetching and asynchronous data
copy, to maximize utilization of the aggregate computing power of CPUs and GPUs
and minimize data copy overheads. The application and performance benefits of
the runtime middleware are demonstrated using an image analysis application,
which is employed in a brain cancer study, on a state-of-the-art hybrid cluster
in which each node has two 6-core CPUs and three GPUs. Our results show that
implementing and scheduling application data processing as a set of fine-grain
operations provide more opportunities for runtime optimizations and attain
better performance than a coarser-grain, monolithic implementation. The
proposed runtime system can achieve high-throughput processing of large
datasets - we were able to process an image dataset consisting of 36,848
4Kx4K-pixel image tiles at about 150 tiles/second rate on 100 nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3342</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3342</id><created>2012-09-14</created><authors><author><keyname>Charron-Bost</keyname><forenames>Bernadette</forenames></author><author><keyname>F&#xfc;gger</keyname><forenames>Matthias</forenames></author><author><keyname>Nowak</keyname><forenames>Thomas</forenames></author></authors><title>New Transience Bounds for Long Walks</title><categories>cs.DM</categories><comments>20 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1111.4600</comments><msc-class>15A80, 05C38, 90B10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear max-plus systems describe the behavior of a large variety of complex
systems. It is known that these systems show a periodic behavior after an
initial transient phase. Assessment of the length of this transient phase
provides important information on complexity measures of such systems, and so
is crucial in system design. We identify relevant parameters in a graph
representation of these systems and propose a modular strategy to derive new
upper bounds on the length of the transient phase. By that we are the first to
give asymptotically tight and potentially subquadratic transience bounds. We
use our bounds to derive new complexity results, in particular in distributed
computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3344</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3344</id><created>2012-09-14</created><authors><author><keyname>Kwon</keyname><forenames>Hyukjoon</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Combining Schemes for Hybrid ARQ with Interference-Aware Successive
  Decoding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For decades, cellular networks have greatly evolved to support high data
rates over reliable communication. Hybrid automatic-repeat-request (ARQ) is one
of the techniques to make such improvement possible. However, this advancement
is reduced at the cell edge where interference is not negligible. In order to
overcome the challenge at the cell edge, the concept of interference-aware
receiver has been recently proposed in which both desired and interference
signals are successively decoded, called interference-aware successive decoding
(IASD). Although IASD is the advanced receiver technology, interference signals
are out of the mobile station's control so that they cannot be requested by the
mobile station. For this reason, this paper proposes new combining schemes for
the IASD receiver, which operate with hybrid ARQ in a bit level or in a symbol
level. In addition, this paper compares the memory requirement among the
proposed combining schemes and analyzes the impact of discrete modulation on
the proposed scheme. Simulation results presents the superiority of the
proposed combining schemes and shows the improvement in terms of the number of
transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3348</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3348</id><created>2012-09-14</created><authors><author><keyname>Li</keyname><forenames>Rong-Hua</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author><author><keyname>Lin</keyname><forenames>Jiyuan</forenames></author></authors><title>Evolution of cooperation in spatial traveler's dilemma game</title><categories>physics.soc-ph cs.GT</categories><doi>10.1371/journal.pone.0058597</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traveler's dilemma (TD) is one of social dilemmas which has been well studied
in the economics community, but it is attracted little attention in the physics
community. The TD game is a two-person game. Each player can select an integer
value between $R$ and $M$ ($R &lt; M$) as a pure strategy. If both of them select
the same value, the payoff to them will be that value. If the players select
different values, say $i$ and $j$ ($R \le i &lt; j \le M$), then the payoff to the
player who chooses the small value will be $i+R$ and the payoff to the other
player will be $i-R$. We term the player who selects a large value as the
cooperator, and the one who chooses a small value as the defector. The reason
is that if both of them select large values, it will result in a large total
payoff. The Nash equilibrium of the TD game is to choose the smallest value
$R$. However, in previous behavioral studies, players in TD game typically
select values that are much larger than $R$, and the average selected value
exhibits an inverse relationship with $R$. To explain such anomalous behavior,
in this paper, we study the evolution of cooperation in spatial traveler's
dilemma game where the players are located on a square lattice and each player
plays TD games with his neighbors. Players in our model can adopt their
neighbors' strategies following two standard models of spatial game dynamics.
Monte-Carlo simulation is applied to our model, and the results show that the
cooperation level of the system, which is proportional to the average value of
the strategies, decreases with increasing $R$ until $R$ is greater than the
threshold where cooperation vanishes. Our findings indicate that spatial
reciprocity promotes the evolution of cooperation in TD game and the spatial TD
game model can interpret the anomalous behavior observed in previous behavioral
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3352</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3352</id><created>2012-09-14</created><updated>2014-02-03</updated><authors><author><keyname>Agrawal</keyname><forenames>Shipra</forenames></author><author><keyname>Goyal</keyname><forenames>Navin</forenames></author></authors><title>Thompson Sampling for Contextual Bandits with Linear Payoffs</title><categories>cs.LG cs.DS stat.ML</categories><comments>Improvements from previous version: (1) dependence on d improved from
  d^2 to d^{3/2} (2) Simpler and more modular proof techniques (3) bounds in
  terms of log(N) added</comments><msc-class>68W40, 68Q25</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thompson Sampling is one of the oldest heuristics for multi-armed bandit
problems. It is a randomized algorithm based on Bayesian ideas, and has
recently generated significant interest after several studies demonstrated it
to have better empirical performance compared to the state-of-the-art methods.
However, many questions regarding its theoretical performance remained open. In
this paper, we design and analyze a generalization of Thompson Sampling
algorithm for the stochastic contextual multi-armed bandit problem with linear
payoff functions, when the contexts are provided by an adaptive adversary. This
is among the most important and widely studied versions of the contextual
bandits problem. We provide the first theoretical guarantees for the contextual
version of Thompson Sampling. We prove a high probability regret bound of
$\tilde{O}(d^{3/2}\sqrt{T})$ (or $\tilde{O}(d\sqrt{T \log(N)})$), which is the
best regret bound achieved by any computationally efficient algorithm available
for this problem in the current literature, and is within a factor of
$\sqrt{d}$ (or $\sqrt{\log(N)}$) of the information-theoretic lower bound for
this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3353</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3353</id><created>2012-09-14</created><authors><author><keyname>Agrawal</keyname><forenames>Shipra</forenames></author><author><keyname>Goyal</keyname><forenames>Navin</forenames></author></authors><title>Further Optimal Regret Bounds for Thompson Sampling</title><categories>cs.LG cs.DS stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:1111.1797</comments><msc-class>68W40, 68Q25</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thompson Sampling is one of the oldest heuristics for multi-armed bandit
problems. It is a randomized algorithm based on Bayesian ideas, and has
recently generated significant interest after several studies demonstrated it
to have better empirical performance compared to the state of the art methods.
In this paper, we provide a novel regret analysis for Thompson Sampling that
simultaneously proves both the optimal problem-dependent bound of
$(1+\epsilon)\sum_i \frac{\ln T}{\Delta_i}+O(\frac{N}{\epsilon^2})$ and the
first near-optimal problem-independent bound of $O(\sqrt{NT\ln T})$ on the
expected regret of this algorithm. Our near-optimal problem-independent bound
solves a COLT 2012 open problem of Chapelle and Li. The optimal
problem-dependent regret bound for this problem was first proven recently by
Kaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are
conceptually simple, easily extend to distributions other than the Beta
distribution, and also extend to the more general contextual bandits setting
[Manuscript, Agrawal and Goyal, 2012].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3356</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3356</id><created>2012-09-15</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Calheiros</keyname><forenames>Rodrigo N.</forenames></author><author><keyname>Li</keyname><forenames>Xiaorong</forenames></author></authors><title>Autonomic Cloud Computing: Open Challenges and Architectural Elements</title><categories>cs.DC</categories><comments>8 pages, 6 figures, conference keynote paper</comments><acm-class>C.2.4</acm-class><journal-ref>Proceedings of the Third International Conference of Emerging
  Applications of Information Technology (EAIT 2012, IEEE Press, USA), Kolkata,
  India, November 29-December 01, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As Clouds are complex, large-scale, and heterogeneous distributed systems,
management of their resources is a challenging task. They need automated and
integrated intelligent strategies for provisioning of resources to offer
services that are secure, reliable, and cost-efficient. Hence, effective
management of services becomes fundamental in software platforms that
constitute the fabric of computing Clouds. In this direction, this paper
identifies open issues in autonomic resource provisioning and presents
innovative management techniques for supporting SaaS applications hosted on
Clouds. We present a conceptual architecture and early results evidencing the
benefits of autonomic management of Clouds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3358</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3358</id><created>2012-09-15</created><updated>2016-02-16</updated><authors><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Goela</keyname><forenames>Naveen</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Computation in Multicast Networks: Function Alignment and Converse
  Theorems</title><categories>cs.IT math.IT</categories><comments>to appear in the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical problem in network coding theory considers communication over
multicast networks. Multiple transmitters send independent messages to multiple
receivers which decode the same set of messages. In this work, computation over
multicast networks is considered: each receiver decodes an identical function
of the original messages. For a countably infinite class of two-transmitter
two-receiver single-hop linear deterministic networks, the computing capacity
is characterized for a linear function (modulo-2 sum) of Bernoulli sources.
Inspired by the geometric concept of interference alignment in networks, a new
achievable coding scheme called function alignment is introduced. A new
converse theorem is established that is tighter than cut-set based and
genie-aided bounds. Computation (vs. communication) over multicast networks
requires additional analysis to account for multiple receivers sharing a
network's computational resources. We also develop a network decomposition
theorem which identifies elementary parallel subnetworks that can constitute an
original network without loss of optimality. The decomposition theorem provides
a conceptually-simpler algebraic proof of achievability that generalizes to
$L$-transmitter $L$-receiver networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3366</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3366</id><created>2012-09-15</created><updated>2012-10-11</updated><authors><author><keyname>Zhou</keyname><forenames>Qing F.</forenames></author><author><keyname>Zhang</keyname><forenames>Q. T.</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author></authors><title>Implement Blind Interference Alignment over Homogeneous 3-user 2x1
  Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>The proof of Theorem 3 is revised. arXiv admin note: text overlap
  with arXiv:1209.3137</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper first studies the homogeneous 3-user 2x1 broadcast channel (BC)
with no CSIT. We show a sufficient condition for it to achieve the optimal 3/2
degrees of freedom (DoF) by using Blind Interference Alignment (BIA). BIA
refers to the interference alignment method without the need of CSIT. It
further studies the 2x1 broadcast network in which there are K&gt;=3 homogeneous
single-antenna users, and their coherence time offsets are independently and
uniformly distributed. We show that, if K&gt;=11, the two-antenna transmitter can
find, with more than 95% certainty, three users to form a BIA-feasible 3-user
BC and achieve the optimal 3/2 DoF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3387</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3387</id><created>2012-09-15</created><authors><author><keyname>Murthy</keyname><forenames>Garimella Rama</forenames><affiliation>International Institute of Information Technology</affiliation></author></authors><title>Graphs: Associated Markov Chains</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research paper, weighted / unweighted, directed / undirected graphs
are associated with interesting Discrete Time Markov Chains (DTMCs) as well as
Continuous Time Markov Chains (CTMCs). The equilibrium / transient behaviour of
such Markov chains is studied. Also entropy dynamics (Shannon entropy) of
certain structured Markov chains is investigated. Finally certain structured
graphs and the associated Markov chains are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3394</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3394</id><created>2012-09-15</created><updated>2014-04-22</updated><authors><author><keyname>Chiani</keyname><forenames>Marco</forenames></author></authors><title>Distribution of the largest eigenvalue for real Wishart and Gaussian
  random matrices and a simple approximation for the Tracy-Widom distribution</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Journal of Multivariate Analysis (2014)</comments><journal-ref>Journal of Multivariate Analysis, Vol. 129, p. 69-81, 2014</journal-ref><doi>10.1016/j.jmva.2014.04.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive efficient recursive formulas giving the exact distribution of the
largest eigenvalue for finite dimensional real Wishart matrices and for the
Gaussian Orthogonal Ensemble (GOE). In comparing the exact distribution with
the limiting distribution of large random matrices, we also found that the
Tracy-Widom law can be approximated by a properly scaled and shifted Gamma
distribution, with great accuracy for the values of common interest in
statistical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3406</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3406</id><created>2012-09-15</created><updated>2012-09-23</updated><authors><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Information Metrics (iMetrics): A Research Specialty with a
  Socio-Cognitive Identity?</title><categories>cs.DL</categories><comments>Accepted for publication in Scientometrics</comments><doi>10.1007/s11192-012-0861-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Bibliometrics&quot;, &quot;scientometrics&quot;, &quot;informetrics&quot;, and &quot;webometrics&quot; can all
be considered as manifestations of a single research area with similar
objectives and methods, which we call &quot;information metrics&quot; or iMetrics. This
study explores the cognitive and social distinctness of iMetrics with respect
to the general information science (IS), focusing on a core of researchers,
shared vocabulary and literature/knowledge base. Our analysis investigates the
similarities and differences between four document sets. The document sets are
drawn from three core journals for iMetrics research (Scientometrics, Journal
of the American Society for Information Science and Technology, and Journal of
Informetrics). We split JASIST into document sets containing iMetrics and
general IS articles. The volume of publications in this representation of the
specialty has increased rapidly during the last decade. A core of researchers
that predominantly focus on iMetrics topics can thus be identified. This core
group has developed a shared vocabulary as exhibited in high similarity of
title words and one that shares a knowledge base. The research front of this
field moves faster than the research front of information science in general,
bringing it closer to Price's dream.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3411</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3411</id><created>2012-09-15</created><authors><author><keyname>Chary</keyname><forenames>Michael</forenames></author></authors><title>A Computational Model of the Effects of Drug Addiction on Neural
  Population Dynamics</title><categories>q-bio.NC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reward processing and derangements thereof, such as drug addiction, involve
the coordinated activity of many brain areas. Prior work has identified many
behavioral, molecular biological and single neuron changes throughout the
mesocorticolimbic system that reflect and drive addictive behavior.
Subpopulations in the ventral tegemental area (VTA) encode positive reward
prediction error, negative reward prediction error, and the magnitude of the
reward. Phasic activity in VTA dopaminergic neurons correlates with hedonic
value. Tonic activity of groups in the dorsomedial prefrontal cortex (dmPFC)
can encode antidepressant states. However, little is known about how drug
addiction might affect population encoding across larger brain regions. Here,
we compare the information content associated with network patterns in naive,
acutely intoxicated and chronically addicted states in a plastic attractor
network. We found that addiction decreases the network's ability to store and
discriminate among patterns of activity. Altered dopaminergic tone flattens the
energy landscape and decreases the entropy associated with each network
pattern. Altered dmPFC activity produces signal-to-noise deficits similar to
computational models of schizophrenia. Our results provide a conceptual
framework for interpreting altered neural population dynamics in
psychopathological states based on information theory. They also suggest a view
of the subtypes of depression as on a continuum of combinations of cortical and
subcortical dysfunction. This suggests that patients who suffer from depression
with psychotic features will have more cortical than mesolimbic dysfunction.
Furthermore, our framework can be applied to other psychiatric illnesses and so
may help us, in general, quantitatively understand psychiatric illnesses as
disorders in the representation and processing of information by distributed
brain networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3416</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3416</id><created>2012-09-15</created><authors><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Li</keyname><forenames>Shuo</forenames></author><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Zhou</keyname><forenames>Yiqing</forenames></author><author><keyname>Kuang</keyname><forenames>Jingming</forenames></author></authors><title>Distributed Resource Allocation Algorithm Design for Multi-Cell Networks
  Based on Advanced Decomposition Theory</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, Submitted to Communications Letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we investigate the resource allocation for downlink
multi-cell coordinated OFDMA wireless networks, in which power allocation and
subcarrier scheduling are jointly optimized. Aiming at maximizing the weighted
sum of the minimal user rates (WSMR) of coordinated cells under individual
power constraints at each base station, an effective distributed resource
allocation algorithm using a modified decomposition method is proposed, which
is suitable by practical implementation due to its low complexity and fast
convergence speed. Simulation results demonstrate that the proposed
decentralized algorithm provides substantial throughput gains with lower
computational cost compared to existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3418</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3418</id><created>2012-09-15</created><authors><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author><author><keyname>Scarcello</keyname><forenames>Francesco</forenames></author></authors><title>Mechanisms for Fair Allocation Problems: No-Punishment Payment Rules in
  Fully Verifiable Settings</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mechanism design is addressed in the context of fair allocations of
indivisible goods with monetary compensation. Motivated by a real-world social
choice problem, mechanisms with verification are considered in a setting where
(i) agents' declarations on allocated goods can be fully verified before
payments are performed, and where (ii) verification is not used to punish
agents whose declarations resulted in incorrect ones. Within this setting, a
mechanism is designed that is shown to be truthful, efficient, and
budget-balanced, and where agents' utilities are fairly determined by the
Shapley value of suitable coalitional games. The proposed mechanism is however
shown to be #P-complete. Thus, to deal with applications with many agents
involved, two polynomial-time randomized variants are also proposed: one that
is still truthful and efficient, and which is approximately budget-balanced
with high probability, and another one that is truthful in expectation, while
still budget-balanced and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3419</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3419</id><created>2012-09-15</created><authors><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author><author><keyname>Scarcello</keyname><forenames>Francesco</forenames></author></authors><title>Tractable Optimization Problems through Hypergraph-Based Structural
  Restrictions</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Several variants of the Constraint Satisfaction Problem have been proposed
and investigated in the literature for modelling those scenarios where
solutions are associated with some given costs. Within these frameworks
computing an optimal solution is an NP-hard problem in general; yet, when
restricted over classes of instances whose constraint interactions can be
modelled via (nearly-)acyclic graphs, this problem is known to be solvable in
polynomial time. In this paper, larger classes of tractable instances are
singled out, by discussing solution approaches based on exploiting hypergraph
acyclicity and, more generally, structural decomposition methods, such as
(hyper)tree decompositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3422</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3422</id><created>2012-09-15</created><updated>2015-01-22</updated><authors><author><keyname>Aubert</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Seiller</keyname><forenames>Thomas</forenames></author></authors><title>Characterizing co-NL by a group action</title><categories>cs.LO cs.CC</categories><comments>To appear in Mathematical Structures in Computer Science</comments><doi>10.1017/S0960129514000267</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In a recent paper, Girard proposes to use his recent construction of a
geometry of interaction in the hyperfinite factor in an innovative way to
characterize complexity classes. We begin by giving a detailed explanation of
both the choices and the motivations of Girard's definitions. We then provide a
complete proof that the complexity class co-NL can be characterized using this
new approach. We introduce as a technical tool the non-deterministic pointer
machine, a concrete model to computes algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3433</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3433</id><created>2012-09-15</created><authors><author><keyname>Zawbaa</keyname><forenames>Hossam M.</forenames></author><author><keyname>Aly</keyname><forenames>Salah A.</forenames></author><author><keyname>Gutub</keyname><forenames>Adnan A.</forenames></author></authors><title>A Hajj And Umrah Location Classification System For Video Crowded Scenes</title><categories>cs.CV cs.CY cs.LG</categories><comments>9 pages, 10 figures, 2 tables, 3 algirthms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new automatic system for classifying ritual locations in
diverse Hajj and Umrah video scenes is investigated. This challenging subject
has mostly been ignored in the past due to several problems one of which is the
lack of realistic annotated video datasets. HUER Dataset is defined to model
six different Hajj and Umrah ritual locations[26].
  The proposed Hajj and Umrah ritual location classifying system consists of
four main phases: Preprocessing, segmentation, feature extraction, and location
classification phases. The shot boundary detection and background/foregroud
segmentation algorithms are applied to prepare the input video scenes into the
KNN, ANN, and SVM classifiers. The system improves the state of art results on
Hajj and Umrah location classifications, and successfully recognizes the six
Hajj rituals with more than 90% accuracy. The various demonstrated experiments
show the promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3436</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3436</id><created>2012-09-15</created><updated>2012-12-04</updated><authors><author><keyname>Costa</keyname><forenames>Edgar</forenames></author><author><keyname>Gerbicz</keyname><forenames>Robert</forenames></author><author><keyname>Harvey</keyname><forenames>David</forenames></author></authors><title>A search for Wilson primes</title><categories>math.NT cs.DS</categories><comments>24 pages, simplified notation and space analysis</comments><msc-class>11A07 (Primary) 11Y16 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Wilson prime is a prime p such that (p-1)! = -1 mod p^2. We report on a
search for Wilson primes up to 2 * 10^13, and describe several new algorithms
that were used in the search. In particular we give the first known algorithm
that computes (p-1)! mod p^2 in average polynomial time per prime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3458</identifier>
 <datestamp>2012-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3458</id><created>2012-09-16</created><updated>2012-10-23</updated><authors><author><keyname>Ariffin</keyname><forenames>M. R. K.</forenames></author></authors><title>A New Efficient Asymmetric Cryptosystem Based on the Integer
  Factorization Problem</title><categories>cs.CR math.NT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1207.1157</comments><msc-class>94A60, 68P25, 11D45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new asymmetric cryptosystem based on the Integer Factorization Problem is
proposed. It posses an encryption and decryption speed of $O(n^2)$, thus making
it the fastest asymmetric encryption scheme available. It has a simple
mathematical structure. Thus, it would have low computational requirements and
would enable communication devices with low computing power to deploy secure
communication procedures efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3460</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3460</id><created>2012-09-16</created><authors><author><keyname>Choudhary</keyname><forenames>Swadesh</forenames></author><author><keyname>Sharma</keyname><forenames>Hrishikesh</forenames></author><author><keyname>Adiga</keyname><forenames>B. S.</forenames></author><author><keyname>Patkar</keyname><forenames>Sachin</forenames></author></authors><title>Expander-like Codes based on Finite Projective Geometry</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel error correcting code and decoding algorithm which have
construction similar to expander codes. The code is based on a bipartite graph
derived from the subsumption relations of finite projective geometry, and
Reed-Solomon codes as component codes. We use a modified version of well-known
Zemor's decoding algorithm for expander codes, for decoding our codes. By
derivation of geometric bounds rather than eigenvalue bounds, it has been
proved that for practical values of the code rate, the random error correction
capability of our codes is much better than those derived for previously
studied graph codes, including Zemor's bound. MATLAB simulations further reveal
that the average case performance of this code is 10 times better than these
geometric bounds obtained, in almost 99% of the test cases. By exploiting the
symmetry of projective space lattices, we have designed a corresponding decoder
that has optimal throughput. The decoder design has been prototyped on Xilinx
Virtex 5 FPGA. The codes are designed for potential applications in secondary
storage media. As an application, we also discuss usage of these codes to
improve the burst error correction capability of CD-ROM decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3487</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3487</id><created>2012-09-16</created><authors><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author><author><keyname>Kelsey</keyname><forenames>Tom</forenames></author><author><keyname>McCaffery</keyname><forenames>Martin</forenames></author></authors><title>A framework for large-scale distributed AI search across disconnected
  heterogeneous infrastructures</title><categories>cs.DC cs.AI math.CO</categories><comments>18 pages plus references. arXiv admin note: substantial text overlap
  with arXiv:1008.4328</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for a large-scale distributed eScience Artificial
Intelligence search. Our approach is generic and can be used for many different
problems. Unlike many other approaches, we do not require dedicated machines,
homogeneous infrastructure or the ability to communicate between nodes. We give
special consideration to the robustness of the framework, minimising the loss
of effort even after total loss of infrastructure, and allowing easy
verification of every step of the distribution process. In contrast to most
eScience applications, the input data and specification of the problem is very
small, being easily given in a paragraph of text. The unique challenges our
framework tackles are related to the combinatorial explosion of the space that
contains the possible solutions and the robustness of long-running
computations. Not only is the time required to finish the computations unknown,
but also the resource requirements may change during the course of the
computation. We demonstrate the applicability of our framework by using it to
solve a challenging and hitherto open problem in computational mathematics. The
results demonstrate that our approach easily scales to computations of a size
that would have been impossible to tackle in practice just a decade ago.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3495</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3495</id><created>2012-09-16</created><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author><author><keyname>de Weger</keyname><forenames>Benne</forenames></author></authors><title>The Collatz conjecture and De Bruijn graphs</title><categories>math.NT cs.DM math.DS</categories><comments>9 pages, 8 figures</comments><journal-ref>Indagationes Mathematicae, vol. 24, no. 4, pp. 971-983, 2013</journal-ref><doi>10.1016/j.indag.2013.03.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study variants of the well-known Collatz graph, by considering the action
of the 3n+1 function on congruence classes. For moduli equal to powers of 2,
these graphs are shown to be isomorphic to binary De Bruijn graphs. Unlike the
Collatz graph, these graphs are very structured, and have several interesting
properties. We then look at a natural generalization of these finite graphs to
the 2-adic integers, and show that the isomorphism between these infinite
graphs is exactly the conjugacy map previously studied by Bernstein and
Lagarias. Finally, we show that for generalizations of the 3n+1 function, we
get similar relations with 2-adic and p-adic De Bruijn graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3505</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3505</id><created>2012-09-16</created><authors><author><keyname>Lee</keyname><forenames>Seunghyun</forenames></author><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Cognitive Energy Harvesting and Transmission from a Network Perspective</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless networks can be self-sustaining by harvesting energy from
radio-frequency (RF) signals. Building on classic cognitive radio networks, we
propose a novel method for network coexisting where mobiles from a secondary
network, called secondary transmitters (STs), either harvest energy from
transmissions by nearby transmitters from a primary network, called primary
transmitters (PTs), or transmit information if PTs are sufficiently far away;
STs store harvested energy in rechargeable batteries with finite capacity and
use all available energy for subsequent transmission when batteries are fully
charged. In this model, each PT is centered at a guard zone and a harvesting
zone that are disks with given radiuses; a ST harvests energy if it lies in
some harvesting zone, transmits fixed-power signals if it is outside all guard
zones or else idles. Based on this model, the spatial throughput of the
secondary network is maximized using a stochastic-geometry model where PTs and
STs are modeled as independent homogeneous Poisson point processes (HPPPs),
under the outage constraints for coexisting networks and obtained in a simple
closed-form. It is observed from the result that the maximum secondary
throughput decreases linearly with the growing PT density, and the optimal ST
density is inversely proportional to the derived transmission probability for
STs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3516</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3516</id><created>2012-09-16</created><updated>2012-09-19</updated><authors><author><keyname>Yokota</keyname><forenames>Rio</forenames></author></authors><title>An FMM Based on Dual Tree Traversal for Many-core Architectures</title><categories>cs.NA</categories><msc-class>70F10</msc-class><acm-class>D.1.2; D.1.3; G.1.0; G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work attempts to integrate the independent efforts in the fast
N-body community to create the fastest N-body library for many-core and
heterogenous architectures. Focus is placed on low accuracy optimizations, in
response to the recent interest to use FMM as a preconditioner for sparse
linear solvers. A direct comparison with other state-of-the-art fast N-body
codes demonstrates that orders of magnitude increase in performance can be
achieved by careful selection of the optimal algorithm and low-level
optimization of the code. The current N-body solver uses a fast multipole
method with an efficient strategy for finding the list of cell-cell
interactions by a dual tree traversal. A task-based threading model is used to
maximize thread-level parallelism and intra-node load-balancing. In order to
extract the full potential of the SIMD units on the latest CPUs, the inner
kernels are optimized using AVX instructions. Our code -- exaFMM -- is an order
of magnitude faster than the current state-of-the-art FMM codes, which are
themselves an order of magnitude faster than the average FMM code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3517</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3517</id><created>2012-09-16</created><authors><author><keyname>Hermans</keyname><forenames>Felienne</forenames></author><author><keyname>Pinzger</keyname><forenames>Martin</forenames></author><author><keyname>van Deursen</keyname><forenames>Arie</forenames></author></authors><title>Measuring Spreadsheet Formula Understandability</title><categories>cs.SE</categories><comments>12 Pages, 1 Colour Figure, 4 Tables; ISBN: 978-0-9569258-6-2</comments><proxy>Grenville Croll</proxy><journal-ref>Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2012 pp77-96</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreadsheets are widely used in industry, because they are flexible and easy
to use. Sometimes they are even used for business-critical applications. It is
however difficult for spreadsheet users to correctly assess the quality of
spreadsheets, especially with respect to their understandability.
Understandability of spreadsheets is important, since spreadsheets often have a
long lifespan, during which they are used by several users.
  In this paper, we establish a set of spreadsheet understandability metrics.
We start by studying related work and interviewing 40 spreadsheet professionals
to obtain a set of characteristics that might contribute to understandability
problems in spreadsheets. Based on those characteristics we subsequently
determine a number of understandability metrics. To evaluate the usefulness of
our metrics, we conducted a series of experiments in which professional
spreadsheet users performed a number of small maintenance tasks on a set of
spreadsheets from the EUSES spreadsheet corpus. We subsequently calculate the
correlation between the metrics and the performance of subjects on these tasks.
  The results clearly indicate that the number of ranges, the nesting depth and
the presence of conditional operations in formulas significantly increase the
difficulty of understanding a spreadsheet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3518</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3518</id><created>2012-09-16</created><authors><author><keyname>Allen</keyname><forenames>Stephen</forenames></author></authors><title>On the Interpretation of Spreadsheets within their Environment</title><categories>cs.SE</categories><comments>10 Pages, 1 Table, 6 Figures, ISBN: 978-0-9569258-6-2</comments><proxy>Grenville Croll</proxy><journal-ref>Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2012 pp97-104</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A demonstration in MS Excel to show how users can connect their spreadsheet
models to the external environment that the model represents. We employ indexes
to generate a list of relevant evidence that is hyperlinked to the context in
which the evidence is discussed. The hyperlinks between the index and the
contextual discussion have their own specific presentational identity. We
contend that these presentational differences aid the integrity and
understanding of complex models. Where models are complex, separate individual
results can lead to contradictory conclusions. The demonstration includes a
methodology for interpreting the analyses within a workbook and presenting them
in the form of a standard written report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3523</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3523</id><created>2012-09-16</created><updated>2012-10-29</updated><authors><author><keyname>Seb&#xf6;</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>Eight-Fifth Approximation for TSP Paths</title><categories>cs.DM cs.DS math.CO</categories><comments>15 pages, corrected typos in citations, minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the approximation ratio 8/5 for the metric $\{s,t\}$-path-TSP
problem, and more generally for shortest connected $T$-joins.
  The algorithm that achieves this ratio is the simple &quot;Best of Many&quot; version
of Christofides' algorithm (1976), suggested by An, Kleinberg and Shmoys
(2012), which consists in determining the best Christofides $\{s,t\}$-tour out
of those constructed from a family $\Fscr_{&gt;0}$ of trees having a convex
combination dominated by an optimal solution $x^*$ of the fractional
relaxation. They give the approximation guarantee $\frac{\sqrt{5}+1}{2}$ for
such an $\{s,t\}$-tour, which is the first improvement after the 5/3 guarantee
of Hoogeveen's Christofides type algorithm (1991). Cheriyan, Friggstad and Gao
(2012) extended this result to a 13/8-approximation of shortest connected
$T$-joins, for $|T|\ge 4$.
  The ratio 8/5 is proved by simplifying and improving the approach of An,
Kleinberg and Shmoys that consists in completing $x^*/2$ in order to dominate
the cost of &quot;parity correction&quot; for spanning trees. We partition the edge-set
of each spanning tree in $\Fscr_{&gt;0}$ into an $\{s,t\}$-path (or more
generally, into a $T$-join) and its complement, which induces a decomposition
of $x^*$. This decomposition can be refined and then efficiently used to
complete $x^*/2$ without using linear programming or particular properties of
$T$, but by adding to each cut deficient for $x^*/2$ an individually tailored
explicitly given vector, inherent in $x^*$.
  A simple example shows that the Best of Many Christofides algorithm may not
find a shorter $\{s,t\}$-tour than 3/2 times the incidentally common optima of
the problem and of its fractional relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3525</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3525</id><created>2012-09-16</created><authors><author><keyname>Delavar</keyname><forenames>Arash Ghorbannia</forenames></author><author><keyname>Javiz</keyname><forenames>Elham</forenames></author></authors><title>EBCD: a routing algorithm based on bee colony for energy consumption
  reduction in wireless relay networks</title><categories>cs.NI</categories><comments>International Journal of Ad hoc, Sensor &amp; Ubiquitous Computing
  (IJASUC) Vol.3, No.4, 2012</comments><doi>10.5121/ijasuc.2012.3401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the important issues in wireless networks is the Routing problem that
is effective on system performance, in this article the attempt is made to
propose a routing algorithm using the bee colony in order to reduce energy
consumption in wireless relay networks. In EBCD algorithm, through combined of
energy, distance and traffic parameters a routing algorithm for wireless
networks is presented with more efficiency than its predecessor. Applying the
bee colony method would allow the placement of the parameters under
conventional conditions and to get closer to a mechanism with a better
adaptability than that of the existing algorithm. According to the parameters
considered, the proposed algorithm provides a fitness function that can be
applied as a multi-hop. Unlike other algorithms of its kind this can increase
service quality based on environmental conditions through its multiple
services. This new method can store the energy accumulated in the nodes and
reduce the hop restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3549</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3549</id><created>2012-09-17</created><authors><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author><author><keyname>Gupta</keyname><forenames>Abhishek</forenames></author><author><keyname>Langbort</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Ba&#x15f;ar</keyname><forenames>Tamer</forenames></author></authors><title>Nash Equilibria for Stochastic Games with Asymmetric Information-Part 1:
  Finite Games</title><categories>cs.GT cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model of stochastic games where multiple controllers jointly control the
evolution of the state of a dynamic system but have access to different
information about the state and action processes is considered. The asymmetry
of information among the controllers makes it difficult to compute or
characterize Nash equilibria. Using common information among the controllers,
the game with asymmetric information is shown to be equivalent to another game
with symmetric information. Further, under certain conditions, a Markov state
is identified for the equivalent symmetric information game and its Markov
perfect equilibria are characterized. This characterization provides a backward
induction algorithm to find Nash equilibria of the original game with
asymmetric information in pure or behavioral strategies. Each step of this
algorithm involves finding Bayesian Nash equilibria of a one-stage Bayesian
game. The class of Nash equilibria of the original game that can be
characterized in this backward manner are named common information based Markov
perfect equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3555</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3555</id><created>2012-09-17</created><authors><author><keyname>Dai</keyname><forenames>Liyun</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>logcf: An Efficient Tool for Real Root Isolation</title><categories>cs.SC math.AG</categories><comments>18 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper revisits an algorithm for isolating real roots of univariate
polynomials based on continued fractions. It follows the work of Vincent,
Uspen- sky, Collins and Akritas, Johnson and Krandick. We use some tricks,
especially a new algorithm for computing an upper bound of positive roots. In
this way, the algorithm of isolating real roots is improved. The complexity of
our method for computing an upper bound of positive roots is O(n log(u+1))
where u is the optimal upper bound satisfying Theorem 3 and n is the degree of
the polynomial. Our method has been implemented as a software package logcf
using C++ language. For many benchmarks logcf is two or three times faster than
the function RootIntervals of Mathematica. And it is much faster than another
continued fractions based software CF, which seems to be one of the fastest
available open software for exact real root isolation. For those benchmarks
which have only real roots, logcf is much faster than Sleeve and eigensolve
which are based on numerical computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3557</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3557</id><created>2012-09-17</created><authors><author><keyname>Kavathia</keyname><forenames>Fenil</forenames></author><author><keyname>Modi</keyname><forenames>Ajay</forenames></author></authors><title>SSL Enhancement</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of e-commerce, ssl protocol is more and more widely
applied to various network services. It is one of key technologies to keep
user's data in secure transmission via internet. This document majorly focuses
on &quot;SSLStrip&quot; which generates the most recent attack in the secure network
connections. It strips out all the secure connections to unsecure plain
connection. In this article we depict this attack and to nullify it, we have
proposed a technique cum practical solution to strengthen data security by
developing mozilla-firefox add-on and servlet code which will strengthen our
defense against the https hijacking attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3564</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3564</id><created>2012-09-17</created><authors><author><keyname>Nia</keyname><forenames>Saeid Sharifian</forenames></author><author><keyname>Vafaei</keyname><forenames>Abbas</forenames></author><author><keyname>Shahimohamadi</keyname><forenames>Hamid</forenames></author></authors><title>Deadlock Recovery Technique in Bus Enhanced NoC Architecture</title><categories>cs.AR</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increase in the speed of processors has led to crucial role of communication
in the performance of systems. As a result, routing is taken into consideration
as one of the most important subjects of the Network on Chip architecture.
Routing algorithms to deadlock avoidance prevent packets route completely based
on network traffic condition by means of restricting the route of packets. This
action leads to less performance especially in non-uniform traffic patterns. On
the other hand True Fully Adoptive Routing algorithm provides routing of
packets completely based on traffic condition. However, deadlock detection and
recovery mechanisms are needed to handle deadlocks. Use of global bus beside
NoC as a parallel supportive environment, provide platform to offer advantages
of both features of bus and NoC. This bus is useful for broadcast and multicast
operations, sending delay sensitive signals, system management and other
services. In this research, we use this bus as an escaping path for deadlock
recovery technique. According to simulation results, this bus is suitable
platform for deadlock recovery technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3573</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3573</id><created>2012-09-17</created><authors><author><keyname>Ernvall-Hyt&#xf6;nen</keyname><forenames>Anne-Maria</forenames></author></authors><title>A short note on the kissing number of the lattice in Gaussian wiretap
  coding</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that on an $n=24m+8k$-dimensional even unimodular lattice, if the
shortest vector length is $\geq 2m$, then as the number of vectors of length
$2m$ decreases, the secrecy gain increases. We will also prove a similar result
on general unimodular lattices. Furthermore, assuming the conjecture by
Belfiore and Sol\'e, we will calculate the difference between inverses of
secrecy gains as the number of vectors varies. Finally, we will show by an
example that there exist two lattices in the same dimension with the same
shortest vector length and the same kissing number, but different secrecy
gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3590</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3590</id><created>2012-09-17</created><authors><author><keyname>Mohanty</keyname><forenames>Ipsita</forenames></author><author><keyname>Velusamy</keyname><forenames>R. Leela</forenames></author></authors><title>Information Retrieval From Internet Applications For Digital Forensic</title><categories>cs.CR cs.IR cs.SI</categories><comments>15 pages, 9 figures; International Journal of Security, Privacy and
  Trust Management (IJSPTM), Vol. 1, No 3/4, August 2012</comments><doi>10.5121/ijsptm.2012.1302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced internet technologies providing services like e-mail, social
networking, online banking, online shopping etc., have made day-to-day
activities simple and convenient. Increasing dependency on the internet,
convenience, and decreasing cost of electronic devices have resulted in
frequent use of online services. However, increased indulgence over the
internet has also accelerated the pace of digital crimes. The increase in
number and complexity of digital crimes has caught the attention of forensic
investigators. The Digital Investigators are faced with the challenge of
gathering accurate digital evidence from as many sources as possible. In this
paper, an attempt was made to recover digital evidence from a system's RAM in
the form of information about the most recent browsing session of the user.
Four different applications were chosen and the experiment was conducted across
two browsers. It was found that crucial information about the target user such
as, user name, passwords, etc., was recoverable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3594</identifier>
 <datestamp>2013-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3594</id><created>2012-09-17</created><updated>2013-08-27</updated><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Hoffmann</keyname><forenames>Michael</forenames></author><author><keyname>Kusters</keyname><forenames>Vincent</forenames></author></authors><title>On Universal Point Sets for Planar Graphs</title><categories>cs.CG</categories><comments>Fixed incorrect numbers of universal point sets in the last part</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set P of points in R^2 is n-universal, if every planar graph on n vertices
admits a plane straight-line embedding on P. Answering a question by Kobourov,
we show that there is no n-universal point set of size n, for any n&gt;=15.
Conversely, we use a computer program to show that there exist universal point
sets for all n&lt;=10 and to enumerate all corresponding order types. Finally, we
describe a collection G of 7'393 planar graphs on 35 vertices that do not admit
a simultaneous geometric embedding without mapping, that is, no set of 35
points in the plane supports a plane straight-line embedding of all graphs in
G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3600</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3600</id><created>2012-09-17</created><authors><author><keyname>Lamperski</keyname><forenames>Andrew</forenames></author><author><keyname>Doyle</keyname><forenames>John C.</forenames></author></authors><title>Output Feedback H_2 Model Matching for Decentralized Systems with Delays</title><categories>cs.SY math.OC</categories><comments>Preliminary draft of conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a new solution to the output feedback H_2 model matching
problem for a large class of delayed information sharing patterns. Existing
methods for such problems typically reduce the decentralized problem to a
centralized problem of higher state dimension. In contrast, the controller
given in this paper is constructed from the solutions to the centralized
control and estimation Riccati equations for the original system. The problem
is solved by decomposing the controller into two components. One is
centralized, but delayed, while the other is decentralized with finite impulse
response (FIR). It is then shown that the optimal controller can be constructed
through a combination of centralized spectral factorization and quadratic
programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3607</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3607</id><created>2012-09-17</created><authors><author><keyname>Sampo</keyname><forenames>Jouni</forenames></author></authors><title>Some refined results on convergence of curvelet transform</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article presents proof that M-term non-linear approximation of functions that
are C^3 apart from C^3 edges in curvelet frame have squared L^2 approximation
bounded by M^(-2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3608</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3608</id><created>2012-09-17</created><authors><author><keyname>Soos</keyname><forenames>Sandor</forenames><affiliation>Dept. Science Policy and Scientometrics, Library of the Hungarian Academy of Sciences</affiliation></author></authors><title>Age-sensitive bibliographic coupling with an application in the history
  of science</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In science mapping, bibliographic coupling (BC) has been a standard tool for
discovering the cognitive structure of research areas, such as constituent
subareas, directions, schools of thought, or paradigms. Modelled as a set of
documents, research areas are often sorted into document clusters via BC
representing a thematic unit each. In this paper we propose an alternative
method called age-sensitive bibliographic coupling: the aim is to enable the
standard method to produce historically valid thematic units, that is, to yield
document clusters that represent the historical development of the thematic
structure of the subject as well. As such, the method is expected to be
especially beneficial for investigations on science dynamics and the history of
science. We apply the method within a bibliometric study in the modern history
of bioscience, addressing the development of a complex, interdisciplinary
discourse called the Species Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3617</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3617</id><created>2012-09-17</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author></authors><title>Strategy complexity of finite-horizon Markov decision processes and
  simple stochastic games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes (MDPs) and simple stochastic games (SSGs) provide a
rich mathematical framework to study many important problems related to
probabilistic systems. MDPs and SSGs with finite-horizon objectives, where the
goal is to maximize the probability to reach a target state in a given finite
time, is a classical and well-studied problem. In this work we consider the
strategy complexity of finite-horizon MDPs and SSGs. We show that for all
$\epsilon&gt;0$, the natural class of counter-based strategies require at most
$\log \log (\frac{1}{\epsilon}) + n+1$ memory states, and memory of size
$\Omega(\log \log (\frac{1}{\epsilon}) + n)$ is required. Thus our bounds are
asymptotically optimal. We then study the periodic property of optimal
strategies, and show a sub-exponential lower bound on the period for optimal
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3638</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3638</id><created>2012-09-17</created><updated>2013-08-05</updated><authors><author><keyname>Doncel</keyname><forenames>Josu</forenames></author><author><keyname>Ayesta</keyname><forenames>Urtzi</forenames></author><author><keyname>Avrachenkov</keyname><forenames>Konstantine</forenames></author><author><keyname>Jacko</keyname><forenames>Peter</forenames></author></authors><title>Congestion Control of TCP Flows in Internet Routers by Means of Index
  Policy</title><categories>cs.NI cs.PF</categories><comments>To appear in Computer Networks</comments><doi>10.1016/j.comnet.2013.08.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of fast and fair transmission of flows
in a router, which is a fundamental issue in networks like the Internet. We
model the interaction between a TCP source and a bottleneck queue with the
objective of designing optimal packet admission controls in the bottleneck
queue. We focus on the relaxed version of the problem obtained by relaxing the
fixed buffer capacity constraint that must be satisfied at all time epoch. The
relaxation allows us to reduce the multi-flow problem into a family of
single-flow problems, for which we can analyze both theoretically and
numerically the existence of optimal control policies of special structure. In
particular, we show that for a variety of parameters, TCP flows can be
optimally controlled in routers by so-called index policies, but not always by
threshold policies. We have also implemented index policies in Network
Simulator-3 and tested in a simple topology their applicability in real
networks. The simulation results show that the index policy covers a big range
of desirable properties with respect to fairness between different versions of
TCP models, across users with different round-trip-time and minimum buffer
required to achieve full utility of the queue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3644</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3644</id><created>2012-09-17</created><authors><author><keyname>Potgieter</keyname><forenames>Petrus H.</forenames></author></authors><title>Availability of titles on peer-to-peer file sharing networks</title><categories>cs.NI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  File sharing, typically involving video or audio material in which copyright
may persist and using peer-to-peer (P2P) networks like BitTorrent, has been
reported to make up the bulk of Internet traffic. The free-riding problem
appears in this &quot;digital gift economy&quot; but its users exhibit rational
behaviour, subject to the characteristics of the particular network. The high
demand for the Internet as a delivery channel for entertainment underlines the
importance of understanding the dynamics of this market, especially when
considering possible business models for future pricing or licensing regimes
and for the provisioning of network capacity to support future services. The
availability of specific titles on file sharing networks is the focus of this
paper, with a special emphasis on the P2P protocol BitTorrent. The paper
compares the incentives provided in BitTorrent to those in other file-sharing
communities, including file hosting, and discusses the number of titles
available in the community at any given time, with an emphasis on popular video
items with ambiguous legal status.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3650</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3650</id><created>2012-09-17</created><authors><author><keyname>Tapiador</keyname><forenames>Antonio</forenames></author><author><keyname>Carrera</keyname><forenames>Diego</forenames></author></authors><title>A survey on social network sites' functional features</title><categories>cs.HC cs.SI</categories><comments>Preprint of article published in Proceedings of IADIS WWW/Internet
  2012. Madrid, Spain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through social network sites (SNS) are between the most popular sites in the
Web, there is not a formal study on their functional features. This paper
introduces a comprehensive list of them. Then, it shows how these features are
supported by top 16 social network platforms. Results show some universal
features, such as comments support, public sharing of contents, system
notifications and profile pages with avatars. A strong tendency in using
external services for authentication and contact recognition has been found,
which is quite significant in top SNS. Most popular content types include text,
pictures and video. The home page is the site for publishing content and
following activities, whilst profile pages mainly include owner's contacts and
content lists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3668</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3668</id><created>2012-09-17</created><authors><author><keyname>Cetin</keyname><forenames>A. Emre</forenames></author></authors><title>Improved in-place associative integer sorting</title><categories>cs.DS</categories><comments>16 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.0572, arXiv:1209.1942</comments><msc-class>68P05, 68P10</msc-class><acm-class>E.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A novel integer sorting technique was proposed replacing bucket sort,
distribution counting sort and address calculation sort family of algorithms
which requires only constant amount of additional memory. The technique was
inspired from one of the ordinal theories of &quot;serial order in behavior&quot; and
explained by the analogy with the three main stages in the formation and
retrieval of memory in cognitive neuroscience namely (i) practicing, (ii)
storing and (iii) retrieval.
  In this study, the technique is improved both theoretically and practically
and an algorithm is obtained which is faster than the former making it more
competitive. With the improved version, n integers S[0...n-1] each in the range
[0, n-1] are sorted exactly in O(n) time while the complexity of the former
technique was the recursion T(n) = T(n/2) + O(n) yielding T(n) = O(n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3672</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3672</id><created>2012-09-17</created><updated>2014-07-01</updated><authors><author><keyname>Davenport</keyname><forenames>Mark A.</forenames></author><author><keyname>Plan</keyname><forenames>Yaniv</forenames></author><author><keyname>Berg</keyname><forenames>Ewout van den</forenames></author><author><keyname>Wootters</keyname><forenames>Mary</forenames></author></authors><title>1-Bit Matrix Completion</title><categories>math.ST cs.IT math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a theory of matrix completion for the extreme case
of noisy 1-bit observations. Instead of observing a subset of the real-valued
entries of a matrix M, we obtain a small number of binary (1-bit) measurements
generated according to a probability distribution determined by the real-valued
entries of M. The central question we ask is whether or not it is possible to
obtain an accurate estimate of M from this data. In general this would seem
impossible, but we show that the maximum likelihood estimate under a suitable
constraint returns an accurate estimate of M when ||M||_{\infty} &lt;= \alpha, and
rank(M) &lt;= r. If the log-likelihood is a concave function (e.g., the logistic
or probit observation models), then we can obtain this maximum likelihood
estimate by optimizing a convex program. In addition, we also show that if
instead of recovering M we simply wish to obtain an estimate of the
distribution generating the 1-bit measurements, then we can eliminate the
requirement that ||M||_{\infty} &lt;= \alpha. For both cases, we provide lower
bounds showing that these estimates are near-optimal. We conclude with a suite
of experiments that both verify the implications of our theorems as well as
illustrate some of the practical applications of 1-bit matrix completion. In
particular, we compare our program to standard matrix completion methods on
movie rating data in which users submit ratings from 1 to 5. In order to use
our program, we quantize this data to a single bit, but we allow the standard
matrix completion program to have access to the original ratings (from 1 to 5).
Surprisingly, the approach based on binary data performs significantly better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3686</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3686</id><created>2012-09-17</created><updated>2014-12-20</updated><authors><author><keyname>Mozafari</keyname><forenames>Barzan</forenames></author><author><keyname>Sarkar</keyname><forenames>Purnamrita</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author></authors><title>Active Learning for Crowd-Sourced Databases</title><categories>cs.LG cs.DB</categories><comments>A shorter version of this manuscript has been published in
  Proceedings of Very Large Data Bases 2015, entitled &quot;Scaling Up
  Crowd-Sourcing to Very Large Datasets: A Case for Active Learning&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd-sourcing has become a popular means of acquiring labeled data for a
wide variety of tasks where humans are more accurate than computers, e.g.,
labeling images, matching objects, or analyzing sentiment. However, relying
solely on the crowd is often impractical even for data sets with thousands of
items, due to time and cost constraints of acquiring human input (which cost
pennies and minutes per label). In this paper, we propose algorithms for
integrating machine learning into crowd-sourced databases, with the goal of
allowing crowd-sourcing applications to scale, i.e., to handle larger datasets
at lower costs. The key observation is that, in many of the above tasks, humans
and machine learning algorithms can be complementary, as humans are often more
accurate but slow and expensive, while algorithms are usually less accurate,
but faster and cheaper.
  Based on this observation, we present two new active learning algorithms to
combine humans and algorithms together in a crowd-sourced database. Our
algorithms are based on the theory of non-parametric bootstrap, which makes our
results applicable to a broad class of machine learning models. Our results, on
three real-life datasets collected with Amazon's Mechanical Turk, and on 15
well-known UCI data sets, show that our methods on average ask humans to label
one to two orders of magnitude fewer items to achieve the same accuracy as a
baseline that labels random images, and two to eight times fewer questions than
previous active learning schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3694</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3694</id><created>2012-09-17</created><authors><author><keyname>Ma</keyname><forenames>Yifei</forenames></author><author><keyname>Garnett</keyname><forenames>Roman</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author></authors><title>Submodularity in Batch Active Learning and Survey Problems on Gaussian
  Random Fields</title><categories>cs.LG cs.AI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world datasets can be represented in the form of a graph whose edge
weights designate similarities between instances. A discrete Gaussian random
field (GRF) model is a finite-dimensional Gaussian process (GP) whose prior
covariance is the inverse of a graph Laplacian. Minimizing the trace of the
predictive covariance Sigma (V-optimality) on GRFs has proven successful in
batch active learning classification problems with budget constraints. However,
its worst-case bound has been missing. We show that the V-optimality on GRFs as
a function of the batch query set is submodular and hence its greedy selection
algorithm guarantees an (1-1/e) approximation ratio. Moreover, GRF models have
the absence-of-suppressor (AofS) condition. For active survey problems, we
propose a similar survey criterion which minimizes 1'(Sigma)1. In practice,
V-optimality criterion performs better than GPs with mutual information gain
criteria and allows nonuniform costs for different nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3702</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3702</id><created>2012-09-17</created><updated>2014-05-29</updated><authors><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author><author><keyname>Yang</keyname><forenames>Tao</forenames></author><author><keyname>Collings</keyname><forenames>Iain B.</forenames></author></authors><title>Multiple-Input Multiple-Output Two-Way Relaying: A Space-Division
  Approach</title><categories>cs.IT math.IT</categories><comments>38 pages, submitted to IEEE Trans. Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel space-division based network-coding scheme for
multiple-input multiple-output (MIMO) two-way relay channels (TWRCs), in which
two multi-antenna users exchange information via a multi-antenna relay. In the
proposed scheme, the overall signal space at the relay is divided into two
subspaces. In one subspace, the spatial streams of the two users have nearly
orthogonal directions, and are completely decoded at the relay. In the other
subspace, the signal directions of the two users are nearly parallel, and
linear functions of the spatial streams are computed at the relay, following
the principle of physical-layer network coding (PNC). Based on the recovered
messages and message-functions, the relay generates and forwards network-coded
messages to the two users. We show that, at high signal-to-noise ratio (SNR),
the proposed scheme achieves the asymptotic sum rate capacity of MIMO TWRCs
within 1/2log(5/4) = 0.161 bits per user-antenna for any antenna configuration
and channel realization. We perform large-system analysis to derive the average
sum-rate of the proposed scheme over Rayleigh-fading MIMO TWRCs. We show that
the average asymptotic sum rate gap to the capacity upper bound is at most
0.053 bits per relay-antenna. It is demonstrated that the proposed scheme
significantly outperforms the existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3721</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3721</id><created>2012-09-17</created><authors><author><keyname>Charalambous</keyname><forenames>Marios C.</forenames></author><author><keyname>Mavromoustakis</keyname><forenames>Constandinos X.</forenames></author><author><keyname>Yassein</keyname><forenames>Muneer Bani</forenames></author></authors><title>A Resource Intensive Traffic-Aware Scheme for Cluster-based Energy
  Conservation in Wireless Devices</title><categories>cs.DC cs.PF</categories><comments>6 pages, 8 figures, To appear in the proceedings of IEEE 14th
  International Conference on High Performance Computing and Communications
  (HPCC-2012) of the Third International Workshop on Wireless Networks and
  Multimedia (WNM-2012), 25-27 June 2012, Liverpool, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless traffic that is destined for a certain device in a network, can be
exploited in order to minimize the availability and delay trade-offs, and
mitigate the Energy consumption. The Energy Conservation (EC) mechanism can be
node-centric by considering the traversed nodal traffic in order to prolong the
network lifetime. This work describes a quantitative traffic-based approach
where a clustered Sleep-Proxy mechanism takes place in order to enable each
node to sleep according to the time duration of the active traffic that each
node expects and experiences. Sleep-proxies within the clusters are created
according to pairwise active-time comparison, where each node expects during
the active periods, a requested traffic. For resource availability and recovery
purposes, the caching mechanism takes place in case where the node for which
the traffic is destined is not available. The proposed scheme uses Role-based
nodes which are assigned to manipulate the traffic in a cluster, through the
time-oriented backward difference traffic evaluation scheme. Simulation study
is carried out for the proposed backward estimation scheme and the
effectiveness of the end-to-end EC mechanism taking into account a number of
metrics and measures for the effects while incrementing the sleep time duration
under the proposed framework. Comparative simulation results show that the
proposed scheme could be applied to infrastructure-less systems, providing
energy-efficient resource exchange with significant minimization in the power
consumption of each device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3728</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3728</id><created>2012-09-17</created><authors><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Huang</keyname><forenames>Yongwei</forenames></author></authors><title>Linear Precoding Designs for Amplify-and-Forward Multiuser Two-Way Relay
  Systems</title><categories>cs.IT math.IT</categories><comments>13 pages, 12 figures, Accepted by IEEE TWC</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Two-way relaying can improve spectral efficiency in two-user cooperative
communications. It also has great potential in multiuser systems. A major
problem of designing a multiuser two-way relay system (MU-TWRS) is transceiver
or precoding design to suppress co-channel interference. This paper aims to
study linear precoding designs for a cellular MU-TWRS where a multi-antenna
base station (BS) conducts bi-directional communications with multiple mobile
stations (MSs) via a multi-antenna relay station (RS) with amplify-and-forward
relay strategy. The design goal is to optimize uplink performance, including
total mean-square error (Total-MSE) and sum rate, while maintaining individual
signal-to-interference-plus-noise ratio (SINR) requirement for downlink
signals. We show that the BS precoding design with the RS precoder fixed can be
converted to a standard second order cone programming (SOCP) and the optimal
solution is obtained efficiently. The RS precoding design with the BS precoder
fixed, on the other hand, is non-convex and we present an iterative algorithm
to find a local optimal solution. Then, the joint BS-RS precoding is obtained
by solving the BS precoding and the RS precoding alternately. Comprehensive
simulation is conducted to demonstrate the effectiveness of the proposed
precoding designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3733</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3733</id><created>2012-09-14</created><updated>2013-05-15</updated><authors><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Pahwa</keyname><forenames>Sakshi</forenames></author><author><keyname>Scoglio</keyname><forenames>Caterina</forenames></author></authors><title>Cascade Failures from Distributed Generation in Power Grids</title><categories>physics.soc-ph cs.SY</categories><comments>10 pages, 3 figures, Critis2012 satellite &amp; Sesame Workshop
  &quot;Topological Approach to the Vulnerability Assessment and Cascading Failure
  Analysis of Electricity Networks&quot; to be published in International Journal of
  Critical Infrastructures</comments><journal-ref>International Journal of Critical Infrastructures 11, pp 27-35,
  2015</journal-ref><doi>10.1504/IJCIS.2015.067395</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power grids are nowadays experiencing a transformation due to the
introduction of Distributed Generation based on Renewable Sources. At
difference with classical Distributed Generation, where local power sources
mitigate anomalous user consumption peaks, Renewable Sources introduce in the
grid intrinsically erratic power inputs. By introducing a simple schematic (but
realistic) model for power grids with stochastic distributed generation, we
study the effects of erratic sources on the robustness of several IEEE power
grid test networks with up to 2000 buses. We find that increasing the
penetration of erratic sources causes the grid to fail with a sharp transition.
We compare such results with the case of failures caused by the natural
increasing power demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3734</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3734</id><created>2012-09-17</created><authors><author><keyname>Rodler</keyname><forenames>Patrick</forenames></author><author><keyname>Shchekotykhin</keyname><forenames>Kostyantyn</forenames></author><author><keyname>Fleiss</keyname><forenames>Philipp</forenames></author><author><keyname>Friedrich</keyname><forenames>Gerhard</forenames></author></authors><title>RIO: Minimizing User Interaction in Ontology Debugging</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Efficient ontology debugging is a cornerstone for many activities in the
context of the Semantic Web, especially when automatic tools produce (parts of)
ontologies such as in the field of ontology matching. The best currently known
interactive debugging systems rely upon some meta information in terms of fault
probabilities, which can speed up the debugging procedure in the good case, but
can also have negative impact on the performance in the bad case. The problem
is that assessment of the meta information is only possible a-posteriori.
Consequently, as long as the actual fault is unknown, there is always some risk
of suboptimal interactive diagnoses discrimination. As an alternative, one
might prefer to rely on a tool which pursues a no-risk strategy. In this case,
however, possibly well-chosen meta information cannot be exploited, resulting
again in inefficient debugging actions. In this work we present a reinforcement
learning strategy that continuously adapts its behavior depending on the
performance achieved and minimizes the risk of using low-quality meta
information. Therefore, this method is suitable for application scenarios where
reliable a-priori fault estimates are difficult to obtain. Using problematic
ontologies in the field of ontology matching, we show that the proposed
risk-aware query strategy outperforms both active learning approaches and
no-risk strategies on average in terms of required amount of user interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3736</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3736</id><created>2012-09-17</created><updated>2012-10-17</updated><authors><author><keyname>Khan</keyname><forenames>Zaryab</forenames></author></authors><title>Security and Efficiency analysis of Fully Homomorphic Encryption (FHE)
  algorithm ZK111</title><categories>cs.CR</categories><comments>this paper has been withdrawn because of an incomplete description of
  the concept which needs further detailed description</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a summarized follow up of the unconditional security and quadratic
time O(n^2) efficiency of ZK111.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3737</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3737</id><created>2012-09-17</created><authors><author><keyname>Banerjee</keyname><forenames>Soumya Jyoti</forenames></author><author><keyname>Roy</keyname><forenames>Soumen</forenames></author></authors><title>Key to Network Controllability</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-bio.MN</categories><comments>Arising from YY Liu, JJ Slotine and AL Barabasi, Nature 473, 167 -
  173 (2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Liu et al recently proposed a minimum number of driver nodes needed to obtain
full structural controllability over a directed network. Driver nodes are
unmatched nodes, from which there are directed paths to all matched nodes.
Their most important assertion is that a system's controllability is to a great
extent encoded by the underlying network's degree distribution, $P(k_{in},
k_{out})$. Is the controllability of a network decided almost completely by the
immediate neighbourhood of a node, while, even slightly distant nodes play no
role at all? Motivated by the above question, in this communication, we argue
that an effective understanding of controllability in directed networks can be
reached using distance based measures of closeness centrality and betweenness
centrality and may not require the knowledge of local connectivity measures
like in-degree and out-degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3756</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3756</id><created>2012-09-17</created><updated>2012-12-18</updated><authors><author><keyname>Nikolaou</keyname><forenames>Charalampos</forenames></author><author><keyname>Koubarakis</keyname><forenames>Manolis</forenames></author></authors><title>Incomplete Information in RDF</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend RDF with the ability to represent property values that exist, but
are unknown or partially known, using constraints. Following ideas from the
incomplete information literature, we develop a semantics for this extension of
RDF, called RDFi, and study SPARQL query evaluation in this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3759</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3759</id><created>2012-09-17</created><updated>2012-09-24</updated><authors><author><keyname>Jawaid</keyname><forenames>Syed Talha</forenames></author><author><keyname>Smith</keyname><forenames>Stephen L.</forenames></author></authors><title>The Maximum Traveling Salesman Problem with Submodular Rewards</title><categories>math.OC cs.DM math.CO</categories><comments>Extended version of ACC 2013 submission (including p-system greedy
  bound with curvature)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we look at the problem of finding the tour of maximum reward
on an undirected graph where the reward is a submodular function, that has a
curvature of $\kappa$, of the edges in the tour. This problem is known to be
NP-hard. We analyze two simple algorithms for finding an approximate solution.
Both algorithms require $O(|V|^3)$ oracle calls to the submodular function. The
approximation factors are shown to be $\frac{1}{2+\kappa}$ and
$\max\set{\frac{2}{3(2+\kappa)},2/3(1-\kappa)}$, respectively; so the second
method has better bounds for low values of $\kappa$. We also look at how these
algorithms perform for a directed graph and investigate a method to consider
edge costs in addition to rewards. The problem has direct applications in
monitoring an environment using autonomous mobile sensors where the sensing
reward depends on the path taken. We provide simulation results to empirically
evaluate the performance of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3761</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3761</id><created>2012-09-17</created><authors><author><keyname>Sun</keyname><forenames>Ming</forenames></author><author><keyname>Priebe</keyname><forenames>Carey E.</forenames></author><author><keyname>Tang</keyname><forenames>Minh</forenames></author></authors><title>Generalized Canonical Correlation Analysis for Disparate Data Fusion</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manifold matching works to identify embeddings of multiple disparate data
spaces into the same low-dimensional space, where joint inference can be
pursued. It is an enabling methodology for fusion and inference from multiple
and massive disparate data sources. In this paper we focus on a method called
Canonical Correlation Analysis (CCA) and its generalization Generalized
Canonical Correlation Analysis (GCCA), which belong to the more general Reduced
Rank Regression (RRR) framework. We present an efficiency investigation of CCA
and GCCA under different training conditions for a particular text document
classification task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3793</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3793</id><created>2012-09-17</created><authors><author><keyname>Avanzini</keyname><forenames>Martin</forenames></author><author><keyname>Moser</keyname><forenames>Georg</forenames></author></authors><title>Polynomial Path Orders: A Maximal Model</title><categories>cs.CC</categories><acm-class>F.2.2; F.4.1; F.4.2; D.2.4; D.2.8</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper is concerned with the automated complexity analysis of term
rewrite systems (TRSs for short) and the ramification of these in implicit
computational complexity theory (ICC for short). We introduce a novel path
order with multiset status, the polynomial path order POP*. Essentially relying
on the principle of predicative recursion as proposed by Bellantoni and Cook,
its distinct feature is the tight control of resources on compatible TRSs: The
(innermost) runtime complexity of compatible TRSs is polynomially bounded. We
have implemented the technique, as underpinned by our experimental evidence our
approach to the automated runtime complexity analysis is not only feasible, but
compared to existing methods incredibly fast. As an application in the context
of ICC we provide an order-theoretic characterisation of the polytime
computable functions. To be precise, the polytime computable functions are
exactly the functions computable by an orthogonal constructor TRS compatible
with POP*.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3794</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3794</id><created>2012-09-17</created><authors><author><keyname>Alzantot</keyname><forenames>Moustafa</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>CrowdInside: Automatic Construction of Indoor Floorplans</title><categories>cs.OH</categories><journal-ref>20th ACM SIGSPATIAL International Conference on Advances in
  Geographic Information Systems (ACM SIGSPATIAL GIS 2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence of a worldwide indoor floorplans database can lead to
significant growth in location-based applications, especially for indoor
environments. In this paper, we present CrowdInside: a crowdsourcing-based
system for the automatic construction of buildings floorplans. CrowdInside
leverages the smart phones sensors that are ubiquitously available with humans
who use a building to automatically and transparently construct accurate motion
traces. These accurate traces are generated based on a novel technique for
reducing the errors in the inertial motion traces by using the points of
interest in the indoor environment, such as elevators and stairs, for error
resetting. The collected traces are then processed to detect the overall
floorplan shape as well as higher level semantics such as detecting rooms and
corridors shapes along with a variety of points of interest in the environment.
Implementation of the system in two testbeds, using different Android phones,
shows that CrowdInside can detect the points of interest accurately with 0.2%
false positive rate and 1.3% false negative rate. In addition, the proposed
error resetting technique leads to more than 12 times enhancement in the median
distance error compared to the state-of-the-art. Moreover, the detailed
floorplan can be accurately estimated with a a relatively small number of
traces. This number is amortized over the number of users of the building. We
also discuss possible extensions to CrowdInside for inferring even higher level
semantics about the discovered floorplans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3804</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3804</id><created>2012-09-17</created><updated>2013-01-25</updated><authors><author><keyname>Li</keyname><forenames>Xiao</forenames></author><author><keyname>Rueetschi</keyname><forenames>Andrea</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Compressive Link Acquisition in Multiuser Communications</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2258014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important receiver operation is to detect the presence specific preamble
signals with unknown delays in the presence of scattering, Doppler effects and
carrier offsets. This task, referred to as &quot;link acquisition&quot;, is typically a
sequential search over the transmitted signal space. Recently, many authors
have suggested applying sparse recovery algorithms in the context of similar
estimation or detection problems. These works typically focus on the benefits
of sparse recovery, but not generally on the cost brought by compressive
sensing. Thus, our goal is to examine the trade-off in complexity and
performance that is possible when using sparse recovery. To do so, we propose a
sequential sparsity-aware compressive sampling (C-SA) acquisition scheme, where
a compressive multi-channel sampling (CMS) front-end is followed by a sparsity
regularized likelihood ratio test (SR-LRT) module.
  The proposed C-SA acquisition scheme borrows insights from the models studied
in the context of sub-Nyquist sampling, where a minimal amount of samples is
captured to reconstruct signals with Finite Rate of Innovation (FRI). In
particular, we propose an A/D conversion front-end that maximizes a well-known
probability divergence measure, the average Kullback-Leibler distance, of all
the hypotheses of the SR-LRT performed on the samples. We compare the proposed
acquisition scheme vis-\`{a}-vis conventional alternatives with relatively low
computational cost, such as the Matched Filter (MF), in terms of performance
and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3808</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3808</id><created>2012-09-17</created><authors><author><keyname>Yuan</keyname><forenames>Ye</forenames></author><author><keyname>Stan</keyname><forenames>Guy-Bart</forenames></author><author><keyname>Warnick</keyname><forenames>Sean</forenames></author><author><keyname>Goncalves</keyname><forenames>Jorge</forenames></author></authors><title>Minimal realization of the dynamical structure function and its
  application to network reconstruction</title><categories>cs.SY q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconstruction, i.e., obtaining network structure from data, is a
central theme in systems biology, economics and engineering. In some previous
work, we introduced dynamical structure functions as a tool for posing and
solving the problem of network reconstruction between measured states. While
recovering the network structure between hidden states is not possible since
they are not measured, in many situations it is important to estimate the
minimal number of hidden states in order to understand the complexity of the
network under investigation and help identify potential targets for
measurements. Estimating the minimal number of hidden states is also crucial to
obtain the simplest state-space model that captures the network structure and
is coherent with the measured data. This paper characterizes minimal order
state-space realizations that are consistent with a given dynamical structure
function by exploring properties of dynamical structure functions and
developing an algorithm to explicitly obtain such a minimal realization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3811</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3811</id><created>2012-09-17</created><authors><author><keyname>Menon</keyname><forenames>Aditya Krishna</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author><author><keyname>Gulwani</keyname><forenames>Sumit</forenames></author><author><keyname>Lampson</keyname><forenames>Butler</forenames></author><author><keyname>Kalai</keyname><forenames>Adam Tauman</forenames></author></authors><title>Textual Features for Programming by Example</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Programming by Example, a system attempts to infer a program from input
and output examples, generally by searching for a composition of certain base
functions. Performing a naive brute force search is infeasible for even mildly
involved tasks. We note that the examples themselves often present clues as to
which functions to compose, and how to rank the resulting programs. In text
processing, which is our domain of interest, clues arise from simple textual
features: for example, if parts of the input and output strings are
permutations of one another, this suggests that sorting may be useful. We
describe a system that learns the reliability of such clues, allowing for
faster search and a principled ranking over programs. Experiments on a
prototype of this system show that this learning scheme facilitates efficient
inference on a range of text processing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3818</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3818</id><created>2012-09-17</created><updated>2013-04-01</updated><authors><author><keyname>Raj</keyname><forenames>Alok</forenames></author></authors><title>Evolution and the structure of learning agents</title><categories>cs.AI cs.LG</categories><comments>total 4 pages. Submitted to IEEE Congress on Evolutionary Computation
  2013</comments><acm-class>I.2; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the thesis that all learning agents of finite information
size are limited by their informational structure in what goals they can
efficiently learn to achieve in a complex environment. Evolutionary change is
critical for creating the required structure for all learning agents in any
complex environment. The thesis implies that there is no efficient universal
learning algorithm. An agent can go past the learning limits imposed by its
structure only by slow evolutionary change or blind search which in a very
complex environment can only give an agent an inefficient universal learning
capability that can work only in evolutionary timescales or improbable luck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3824</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3824</id><created>2012-09-17</created><authors><author><keyname>Kwon</keyname><forenames>Hyukjoon</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Interference Mitigation via Interference-Aware Successive Decoding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern wireless networks, interference is no longer negligible since each
cell becomes smaller to support high throughput. The reduced size of each cell
forces to install many cells, and consequently causes to increase inter-cell
interference at many cell edge areas. This paper considers a practical way of
mitigating interference at the receiver equipped with multiple antennas in
interference channels. Recently, it is shown that the capacity region of
interference channels over point-to-point codes could be established with a
combination of two schemes: treating interference as noise and jointly decoding
both desired and interference signals. In practice, the first scheme is
straightforwardly implementable, but the second scheme needs impractically huge
computational burden at the receiver. Within a practical range of complexity,
this paper proposes the interference-aware successive decoding (IASD) algorithm
which successively decodes desired and interference signals while updating a
priori information of both signals. When multiple decoders are allowed to be
used, the proposed IASD can be extended to interference-aware parallel decoding
(IAPD). The proposed algorithm is analyzed with extrinsic information transfer
(EXIT) chart so as to show that the interference decoding is advantageous to
improve the performance. Simulation results demonstrate that the proposed
algorithm significantly outperforms interference non-decoding algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3827</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3827</id><created>2012-09-17</created><authors><author><keyname>Wu</keyname><forenames>Fei</forenames></author><author><keyname>Hua</keyname><forenames>Cunqing</forenames></author><author><keyname>Shan</keyname><forenames>Hangguan</forenames></author><author><keyname>Huang</keyname><forenames>Aiping</forenames></author></authors><title>Moving Window Network Coding in Cooperative Multicast (v1)</title><categories>cs.NI</categories><comments>submitted to IEEE Trans. Mobile Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative multicast is an effective solution to address the bottleneck
problem of single-hop broadcast in wireless networks. By incorporating with the
random linear network coding technique, the existing schemes can reduce the
retransmission overhead significantly. However, the receivers may incur large
decoding delay and complexity due to the batch decoding scheme. In addition,
the dependency on the explicit feedback leads to scalability problem in larger
networks. In this paper, a cooperative multicast protocol named MWNCast is
proposed based on a novel moving window network coding technique. We prove
three properties of the proposed scheme. Firstly, without explicit feedback,
MWNCast can approach the cooperative capacity with the packet loss probability
dropping almost exponentially with the increase of window size. Secondly, the
average decoding delay of a receiver is on the order of
$O(\frac{1}{(1-\rho)^2})$ with respect to its traffic intensity $\rho$.
Thirdly, MWNCast can achieve the linear decoding complexity of $O(W)$ with
respect to the window size $W$. Simulation results show that MWNCast
outperforms the existing schemes by achieving better tradeoff between the
throughput and decoding delay, meanwhile keeping the packet loss probability
and decoding complexity at a very low levelwithout explicit feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3849</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3849</id><created>2012-09-18</created><updated>2013-07-03</updated><authors><author><keyname>Buhrman</keyname><forenames>Harry</forenames></author><author><keyname>Garcia-Soriano</keyname><forenames>David</forenames></author><author><keyname>Matsliah</keyname><forenames>Arie</forenames></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames></author></authors><title>The non-adaptive query complexity of testing k-parities</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove tight bounds of Theta(k log k) queries for non-adaptively testing
whether a function f:{0,1}^n -&gt; {0,1} is a k-parity or far from any k-parity.
The lower bound combines a recent method of Blais, Brody and Matulef [BBM11] to
get lower bounds for testing from communication complexity with an Omega(k \log
k) lower bound for the one-way communication complexity of k-disjointness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3865</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3865</id><created>2012-09-18</created><authors><author><keyname>Hartmann</keyname><forenames>Tanja</forenames></author><author><keyname>Rollin</keyname><forenames>Jonathan</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Cubic Augmentation of Planar Graphs</title><categories>math.CO cs.CC cs.DM</categories><comments>accepted at ISAAC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of augmenting a planar graph such that it
becomes 3-regular and remains planar. We show that it is NP-hard to decide
whether such an augmentation exists. On the other hand, we give an efficient
algorithm for the variant of the problem where the input graph has a fixed
planar (topological) embedding that has to be preserved by the augmentation. We
further generalize this algorithm to test efficiently whether a 3-regular
planar augmentation exists that additionally makes the input graph connected or
biconnected. If the input graph should become even triconnected, we show that
the existence of a 3-regular planar augmentation is again NP-hard to decide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3868</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3868</id><created>2012-09-18</created><authors><author><keyname>Kling</keyname><forenames>Peter</forenames></author><author><keyname>Pietrzyk</keyname><forenames>Peter</forenames></author></authors><title>Profitable Scheduling on Multiple Speed-Scalable Processors</title><categories>cs.DS</categories><comments>Extended abstract submitted to STACS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new online algorithm for profit-oriented scheduling on multiple
speed-scalable processors. Moreover, we provide a tight analysis of the
algorithm's competitiveness. Our results generalize and improve upon work by
\textcite{Chan:2010}, which considers a single speed-scalable processor. Using
significantly different techniques, we can not only extend their model to
multiprocessors but also prove an enhanced and tight competitive ratio for our
algorithm.
  In our scheduling problem, jobs arrive over time and are preemptable. They
have different workloads, values, and deadlines. The scheduler may decide not
to finish a job but instead to suffer a loss equaling the job's value. However,
to process a job's workload until its deadline the scheduler must invest a
certain amount of energy. The cost of a schedule is the sum of lost values and
invested energy. In order to finish a job the scheduler has to determine which
processors to use and set their speeds accordingly. A processor's energy
consumption is power $\Power{s}$ integrated over time, where
$\Power{s}=s^{\alpha}$ is the power consumption when running at speed $s$.
Since we consider the online variant of the problem, the scheduler has no
knowledge about future jobs. This problem was introduced by
\textcite{Chan:2010} for the case of a single processor. They presented an
online algorithm which is $\alpha^{\alpha}+2e\alpha$-competitive. We provide an
online algorithm for the case of multiple processors with an improved
competitive ratio of $\alpha^{\alpha}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3869</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3869</id><created>2012-09-18</created><authors><author><keyname>Tanwar</keyname><forenames>Poonam</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author><author><keyname>Datta</keyname><forenames>Dr. Kamlesh</forenames></author></authors><title>Hybrid technique for effective knowledge representation &amp; a comparative
  study</title><categories>cs.AI</categories><comments>15 pages,9 figures, 1 table, Pablished in IJCSES,International
  Journal of Computer Science &amp; Engineering Survey Vol.3, No.4, August 2012</comments><journal-ref>Pablished in IJCSES,International Journal of Computer Science &amp;
  Engineering Survey Vol.3, No.4, August 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge representation (KR) and inference mechanism are most desirable
thing to make the system intelligent. System is known to an intelligent if its
intelligence is equivalent to the intelligence of human being for a particular
domain or general. Because of incomplete ambiguous and uncertain information
the task of making intelligent system is very difficult. The objective of this
paper is to present the hybrid KR technique for making the system effective &amp;
Optimistic. The requirement for (effective &amp; optimistic) is because the system
must be able to reply the answer with a confidence of some factor. This paper
also presents the comparison between various hybrid KR techniques with the
proposed one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3878</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3878</id><created>2012-09-18</created><authors><author><keyname>Tapiador</keyname><forenames>Antonio</forenames></author><author><keyname>Salvach&#xfa;a</keyname><forenames>Joaqu&#xed;n</forenames></author></authors><title>Content Management in Ruby on Rails</title><categories>cs.SE</categories><comments>Proceedings of the IADIS International Conference on Collaborative
  Technologies 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web development is currently driven by model-view-controller (MVC)
frameworks. How has content management adapted to this scenario? This paper
reviews content management features in Ruby on Rails framework and its most
popular plug-ins. These features are distributed among the different layers of
the MVC architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3902</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3902</id><created>2012-09-18</created><updated>2015-03-24</updated><authors><author><keyname>Banisch</keyname><forenames>Sven</forenames></author><author><keyname>Lima</keyname><forenames>Ricardo</forenames></author></authors><title>Markov Chain Aggregation for Simple Agent-Based Models on Symmetric
  Networks: The Voter Model</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>The previous short version of this paper had been entitled: Markov
  Projections of the Voter Model</comments><msc-class>60J20, 92B05, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Agent Based Models, in particular the Voter Model (VM), a general
framework of aggregation is developed which exploits the symmetries of the
agent network $G$. Depending on the symmetry group $Aut_{\omega} (N)$ of the
weighted agent network, certain ensembles of agent configurations can be
interchanged without affecting the dynamical properties of the VM. These
configurations can be aggregated into the same macro state and the dynamical
process projected onto these states is, contrary to the general case, still a
Markov chain. The method facilitates the analysis of the relation between
microscopic processes and a their aggregation to a macroscopic level of
description and informs about the complexity of a system introduced by
heterogeneous interaction relations. In some cases the macro chain is solvable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3904</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3904</id><created>2012-09-18</created><authors><author><keyname>Agathangelou</keyname><forenames>Chrysovalandis</forenames></author><author><keyname>Georgiou</keyname><forenames>Chryssis</forenames></author><author><keyname>Mavronicolas</keyname><forenames>Marios</forenames></author></authors><title>A Distributed Algorithm for Gathering Many Fat Mobile Robots in the
  Plane</title><categories>cs.DC</categories><comments>39 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the problem of gathering autonomous robots in the
plane. In particular, we consider non-transparent unit-disc robots (i.e., fat)
in an asynchronous setting. Vision is the only mean of coordination. Using a
state-machine representation we formulate the gathering problem and develop a
distributed algorithm that solves the problem for any number of robots.
  The main idea behind our algorithm is for the robots to reach a configuration
in which all the following hold: (a) The robots' centers form a convex hull in
which all robots are on the convex, (b) Each robot can see all other robots,
and (c) The configuration is connected, that is, every robot touches another
robot and all robots together form a connected formation. We show that starting
from any initial configuration, the robots, making only local decisions and
coordinate by vision, eventually reach such a configuration and terminate,
yielding a solution to the gathering problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3909</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3909</id><created>2012-09-18</created><updated>2015-08-16</updated><authors><author><keyname>Galil</keyname><forenames>Mohamed A. El</forenames></author></authors><title>Network Routing Optimization Using Swarm Intelligence</title><categories>cs.NE cs.DM</categories><comments>10 Pages</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The aim of this paper is to highlight and explore a traditional problem,
which is the minimum spanning tree, and finding the shortest-path in network
routing, by using Swarm Intelligence. This work to be considered as an
investigation topic with combination between operations research, discrete
mathematics, and evolutionary computing aiming to solve one of networking
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3913</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3913</id><created>2012-09-18</created><authors><author><keyname>Trencs&#xe9;ni</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Gazs&#xf3;</keyname><forenames>Attila</forenames></author></authors><title>Keyspace: A Consistently Replicated, Highly-Available Key-Value Store</title><categories>cs.DB cs.DC</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the design and architecture of Keyspace, a distributed
key-value store offering strong consistency, fault-tolerance and high
availability. The source code is available under the open-source AGPL license
for Linux, Windows and BSD-like platforms. As of 2012, Keyspace is no longer
undergoing active development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3914</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3914</id><created>2012-09-18</created><updated>2012-12-16</updated><authors><author><keyname>Urban</keyname><forenames>Josef</forenames></author><author><keyname>Vyskocil</keyname><forenames>Jiri</forenames></author></authors><title>Theorem Proving in Large Formal Mathematics as an Emerging AI Field</title><categories>cs.AI cs.DL</categories><report-no>DPA-12271</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, we have linked a large corpus of formal mathematics with
automated theorem proving (ATP) tools, and started to develop combined AI/ATP
systems working in this setting. In this paper we first relate this project to
the earlier large-scale automated developments done by Quaife with McCune's
Otter system, and to the discussions of the QED project about formalizing a
significant part of mathematics. Then we summarize our adventure so far, argue
that the QED dreams were right in anticipating the creation of a very
interesting semantic AI field, and discuss its further research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3916</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3916</id><created>2012-09-18</created><authors><author><keyname>Kelsey</keyname><forenames>Thomas W.</forenames></author><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author><author><keyname>Jefferson</keyname><forenames>Christoffer A.</forenames></author><author><keyname>Linton</keyname><forenames>Stephen A.</forenames></author><author><keyname>Miguel</keyname><forenames>Ian</forenames></author><author><keyname>Nightingale</keyname><forenames>Peter</forenames></author><author><keyname>Gent</keyname><forenames>Ian P.</forenames></author></authors><title>Qualitative Modelling via Constraint Programming: Past, Present and
  Future</title><categories>cs.CE cs.AI math.DS q-bio.CB</categories><comments>15 pages plus references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qualitative modelling is a technique integrating the fields of theoretical
computer science, artificial intelligence and the physical and biological
sciences. The aim is to be able to model the behaviour of systems without
estimating parameter values and fixing the exact quantitative dynamics.
Traditional applications are the study of the dynamics of physical and
biological systems at a higher level of abstraction than that obtained by
estimation of numerical parameter values for a fixed quantitative model.
Qualitative modelling has been studied and implemented to varying degrees of
sophistication in Petri nets, process calculi and constraint programming. In
this paper we reflect on the strengths and weaknesses of existing frameworks,
we demonstrate how recent advances in constraint programming can be leveraged
to produce high quality qualitative models, and we describe the advances in
theory and technology that would be needed to make constraint programming the
best option for scientific investigation in the broadest sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3920</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3920</id><created>2012-09-18</created><updated>2012-09-22</updated><authors><author><keyname>Kurganskyy</keyname><forenames>Oleksiy</forenames></author></authors><title>A proof of the lonely runner conjecture</title><categories>cs.DM math.CO</categories><comments>This paper has been withdrawn by the author due to an error</comments><msc-class>11J99, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we prove the lonely runner conjecture also known as the view
obstruction problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3925</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3925</id><created>2012-09-18</created><authors><author><keyname>Soille</keyname><forenames>Pierre</forenames><affiliation>IPSC</affiliation></author><author><keyname>Najman</keyname><forenames>Laurent</forenames><affiliation>LIGM</affiliation></author></authors><title>On morphological hierarchical representations for image processing and
  spatial data clustering</title><categories>cs.DM cs.DS</categories><proxy>ccsd</proxy><journal-ref>Workshop on APPLICATIONS OF DISCRETE GEOMETRY AND MATHEMATICAL
  MORPHOLOGY, Istanbul : Turkey (2010)</journal-ref><doi>10.1007/978-3-642-32313-3_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical data representations in the context of classi cation and data
clustering were put forward during the fties. Recently, hierarchical image
representations have gained renewed interest for segmentation purposes. In this
paper, we briefly survey fundamental results on hierarchical clustering and
then detail recent paradigms developed for the hierarchical representation of
images in the framework of mathematical morphology: constrained connectivity
and ultrametric watersheds. Constrained connectivity can be viewed as a way to
constrain an initial hierarchy in such a way that a set of desired constraints
are satis ed. The framework of ultrametric watersheds provides a generic scheme
for computing any hierarchical connected clustering, in particular when such a
hierarchy is constrained. The suitability of this framework for solving
practical problems is illustrated with applications in remote sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3927</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3927</id><created>2012-09-18</created><authors><author><keyname>de Luca</keyname><forenames>Aldo</forenames></author></authors><title>Some extremal properties of the Fibonacci word</title><categories>cs.DM math.CO</categories><comments>25 pages, to appear</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the Fibonacci word $f$ satisfies among all characteristic
Sturmian words, three interesting extremal properties. The first concerns the
length and the second the minimal period of its palindromic prefixes. Each of
these two properties characterizes $f$ up to a renaming of its letters. A third
property concerns the number of occurrences of the letter $b$ in its
palindromic prefixes. It characterizes uniquely $f$ among all characteristic
Sturmian words having the prefix $abaa$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3943</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3943</id><created>2012-09-18</created><authors><author><keyname>Saidi</keyname><forenames>Wafa Tebourski Ourida Ben Boubaker</forenames></author></authors><title>Formal Concept Analysis Based Association Rules Extraction</title><categories>cs.DB</categories><comments>8</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No. 2, July 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating a huge number of association rules reduces their utility in the
decision making process, done by domain experts. In this context, based on the
theory of Formal Concept Analysis, we propose to extend the notion of Formal
Concept through the generalization of the notion of itemset in order to
consider the itemset as an intent, its support as the cardinality of the extent
and its relevance which is related to the confidence of rule. Accordingly, we
propose a new approach to extract interesting itemsets through the concept
coverage. This approach uses a new quality-criteria of a rule: the relevance
bringing a semantic added value to formal concept analysis approach to discover
association rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3944</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3944</id><created>2012-09-18</created><authors><author><keyname>Karaa</keyname><forenames>Wafa Tebourski Wahiba Ben Abdessalem</forenames></author></authors><title>Cyclic Association Rules Mining under Constraints</title><categories>cs.DB</categories><comments>8</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several researchers have explored the temporal aspect of association rules
mining. In this paper, we focus on the cyclic association rules, in order to
discover correlations among items characterized by regular cyclic variation
overtime. The overview of the state of the art has revealed the drawbacks of
proposed algorithm literatures, namely the excessive number of generated rules
which are not meeting the expert's expectations. To overcome these
restrictions, we have introduced our approach dedicated to generate the cyclic
association rules under constraints through a new method called
Constraint-Based Cyclic Association Rules CBCAR. The carried out experiments
underline the usefulness and the performance of our new approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3954</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3954</id><created>2012-09-18</created><authors><author><keyname>van Kreveld</keyname><forenames>Marc</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Pach</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>How Many Potatoes are in a Mesh?</title><categories>cs.DM cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the combinatorial question of how many convex polygons can be
made by using the edges taken from a fixed triangulation of n vertices. For
general triangulations, there can be exponentially many: we show a construction
that has Omega(1.5028^n) convex polygons, and prove an O(1.62^n) upper bound in
the worst case. If the triangulation is fat (every triangle has its angles
lower-bounded by a constant delta&gt;0), then there can be only polynomially many.
  We also consider the problem of counting convex outerplanar polygons (i.e.,
they contain no vertices of the triangulation in their interiors) in the same
triangulations. In this setting, we get the same exponential bounds in general
triangulations, and lower polynomial bounds in fat triangulations. If the
triangulation is furthermore compact (the ratio between the longest and
shortest distance between any two vertices is bounded), the bounds drop further
to Theta (n^2) for general convex outerplanar polygons, and Theta (n) for fat
convex outerplanar polygons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3977</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3977</id><created>2012-09-18</created><updated>2013-05-16</updated><authors><author><keyname>Gast&#xf3;n</keyname><forenames>Bernat</forenames></author><author><keyname>Pujol</keyname><forenames>Jaume</forenames></author><author><keyname>Villanueva</keyname><forenames>Merc&#xe8;</forenames></author></authors><title>Quasi-cyclic Flexible Regenerating Codes</title><categories>cs.IT cs.DC math.IT</categories><comments>17 pages, 1 figure, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a distributed storage environment, where the data is placed in nodes
connected through a network, it is likely that one of these nodes fails. It is
known that the use of erasure coding improves the fault tolerance and minimizes
the redundancy added in distributed storage environments. The use of
regenerating codes not only make the most of the erasure coding improvements,
but also minimizes the amount of data needed to regenerate a failed node.
  In this paper, a new family of regenerating codes based on quasi-cyclic codes
is presented. Quasi-cyclic flexible minimum storage regenerating (QCFMSR) codes
are constructed and their existence is proved. Quasi-cyclic flexible
regenerating codes with minimum bandwidth constructed from a base QCFMSR code
are also provided. These codes not only achieve optimal MBR parameters in terms
of stored data and repair bandwidth, but also for an specific choice of the
parameters involved, they can be decreased under the optimal MBR point.
  Quasi-cyclic flexible regenerating codes are very interesting because of
their simplicity and low complexity. They allow exact repair-by-transfer in the
minimum bandwidth case and an exact pseudo repair-by-transfer in the MSR case,
where operations are needed only when a new node enters into the system
replacing a lost one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3982</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3982</id><created>2012-09-18</created><authors><author><keyname>Li</keyname><forenames>Zhang</forenames></author><author><keyname>Pollak</keyname><forenames>Ilya</forenames></author></authors><title>Sparsifying Defaults: Optimal Bailout Policies for Financial Networks in
  Distress</title><categories>q-fin.CP cs.SI math.OC q-fin.RM</categories><report-no>TR-ECE-12-08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The events of the last few years revealed an acute need for tools to
systematically model and analyze large financial networks. Many applications of
such tools include the forecasting of systemic failures and analyzing probable
effects of economic policy decisions. We consider optimizing the amount and
structure of a bailout in a borrower-lender network: Given a fixed amount of
cash to be injected into the system, how should it be distributed among the
nodes in order to achieve the smallest overall amount of unpaid liabilities or
the smallest number of nodes in default? We develop an exact algorithm for the
problem of minimizing the amount of unpaid liabilities, by showing that it is
equivalent to a linear program. For the problem of minimizing the number of
defaults, we develop an approximate algorithm using a reweighted l1
minimization approach. We illustrate this algorithm using an example with
synthetic data for which the optimal solution can be calculated exactly, and
show through numerical simulation that the solutions calculated by our
algorithm are close to optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.3995</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.3995</id><created>2012-09-18</created><authors><author><keyname>Fliege</keyname><forenames>Joerg</forenames></author></authors><title>A Randomized Parallel Algorithm with Run Time $O(n^2)$ for Solving an $n
  \times n$ System of Linear Equations</title><categories>math.NA cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, following suggestions by Tao, we extend the randomized
algorithm for linear equations over prime fields by Raghavendra to a randomized
algorithm for linear equations over the reals. We also show that the algorithm
can be parallelized to solve a system of linear equations $A x = b$ with a
regular $n \times n$ matrix $A$ in time $O(n^2)$, with probability one. Note
that we do not assume that $A$ is symmetric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4015</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4015</id><created>2012-09-15</created><authors><author><keyname>Reddy</keyname><forenames>B. Roja</forenames></author><author><keyname>M</keyname><forenames>Uttara Kumari .</forenames></author></authors><title>Performance Analysis of MIMO Radar Waveform using Accelerated Particle
  Swarm Optimization Algorithm</title><categories>cs.OH</categories><comments>10 pages, 10 figures, Signal &amp; Image Processing : An International
  Journal (SIPIJ), August 2012</comments><doi>10.5121/sipij.2012.3416</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Accelerated Particle Swarm Optimization Algorithm is promoted to
numerically design orthogonal Discrete Frequency Waveforms and Modified
Discrete Frequency Waveforms (DFCWs) with good correlation properties for MIMO
radar. We employ Accelerated Particle Swarm Optimization algorithm (ACC_PSO),
Particles of a swarm communicate good positions, velocity and accelerations to
each other as well as dynamically adjust their own position, velocity and
acceleration derived from the best of all particles. The simulation results
show that the proposed algorithm is effective for the design of DFCWs signal
used in MIMO radar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4022</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4022</id><created>2012-09-18</created><authors><author><keyname>Tatko</keyname><forenames>Ryan</forenames></author><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author></authors><title>Game Theoretic Formation of a Centrality Based Network</title><categories>cs.GT cs.SI physics.soc-ph</categories><comments>Submitted to 2012 ASE Social Informatics Conference</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We model the formation of networks as a game where players aspire to maximize
their own centrality by increasing the number of other players to which they
are path-wise connected, while simultaneously incurring a cost for each added
adjacent edge. We simulate the interactions between players using an algorithm
that factors in rational strategic behavior based on a common objective
function. The resulting networks exhibit pairwise stability, from which we
derive necessary stable conditions for specific graph topologies. We then
expand the model to simulate non-trivial games with large numbers of players.
We show that using conditions necessary for the stability of star topologies we
can induce the formation of hub players that positively impact the total
welfare of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4056</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4056</id><created>2012-09-18</created><authors><author><keyname>Dixit</keyname><forenames>Kashyap</forenames></author><author><keyname>Jha</keyname><forenames>Madhav</forenames></author><author><keyname>Thakurta</keyname><forenames>Abhradeep</forenames></author></authors><title>Testing Lipschitz Property over Product Distribution and its
  Applications to Statistical Data Privacy</title><categories>cs.CR</categories><comments>17 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this work, we present a connection between Lipschitz property testing and
a relaxed notion of differential privacy, where we assume that the datasets are
being sampled from a domain according to some distribution defined on it.
Specifically, we show that testing whether an algorithm is private can be
reduced to testing Lipschitz property in the distributional setting.
  We also initiate the study of distribution Lipschitz testing. We present an
efficient Lipschitz tester for the hypercube domain when the &quot;distance to
property&quot; is measured with respect to product distribution. Most previous works
in property testing of functions (including prior works on Lipschitz testing)
work with uniform distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4061</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4061</id><created>2012-09-18</created><authors><author><keyname>Holub</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author></authors><title>Binary equality sets are generated by two words</title><categories>cs.FL math.CO</categories><comments>Thoroughly revised version of a journal publication</comments><msc-class>68R15</msc-class><journal-ref>Journal of Algebra 259 (2003), 1-42</journal-ref><doi>10.1016/S0021-8693(02)00534-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the equality language of two non-periodic binary morphisms is
generated by at most two words. If its rank is two, then the generators start
(and end) with different letters.
  This in particular implies that any binary language has a test set of
cardinality at most two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4065</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4065</id><created>2012-09-18</created><updated>2012-09-19</updated><authors><author><keyname>Yilmaz</keyname><forenames>Ahmet</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ferkan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Kucur</keyname><forenames>O&#x11f;uz</forenames></author></authors><title>On the Performance of Transmit Antenna Selection Based on Shadowing Side
  Information</title><categories>cs.IT math.IT math.ST stat.OT stat.TH</categories><comments>7 pages, 5 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a transmit antenna selection scheme, which is based on
shadowing side information, is investigated. In this scheme, the selected
single transmit antenna provides the highest shadowing coefficient between
transmitter and receiver. By the proposed technique, the frequency of the usage
of the feedback channel from the receiver to the transmitter and also channel
estimation complexity at the receiver can be reduced. We study the performance
of our proposed technique and in the analysis, we consider an independent but
not identically distributed Generalized-K composite fading model. More
specifically exact and closed-form expressions for the outage probability, the
moment generating function, the moments of signal-to-noise ratio, and the
average symbol error probability are derived. In addition, asymptotic outage
probability and symbol error probability expressions are also presented in
order to investigate the diversity order and the array gain. Finally, our
theoretical performance results are validated by Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4066</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4066</id><created>2012-09-14</created><authors><author><keyname>Kokalj-Filipovic</keyname><forenames>Silvija</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author><author><keyname>Spasojevic</keyname><forenames>Predrag</forenames></author></authors><title>Low Complexity Differentiating Adaptive Erasure Codes for Multimedia
  Wireless Broadcast</title><categories>cs.IT math.IT</categories><comments>broadcast, erasure codes, fountain, decoding complexity, repair</comments><journal-ref>Low complexity doped wireless broadcast for multimedia
  applications, IEEE Trans. on Comm., vol. 61, no. 8, pp. 3462-3471, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the erasure channel FEC model as defined in multimedia wireless
broadcast standards, we illustrate how doping mechanisms included in the design
of erasure coding and decoding may improve the scalability of the packet
throughput, decrease overall latency and potentially differentiate among
classes of multimedia subscribers regardless of their signal quality. We
describe decoding mechanisms that allow for linear complexity and give
complexity bounds when feedback is available. We show that elaborate coding
schemes which include pre-coding stages are inferior to simple Ideal Soliton
based rateless codes, combined with the proposed two-phase decoder. The
simplicity of this scheme and the availability of tight bounds on latency given
pre-allocated radio resources makes it a practical and efficient design
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4093</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4093</id><created>2012-09-18</created><updated>2014-05-01</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Zetterberg</keyname><forenames>Per</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Capacity Limits and Multiplexing Gains of MIMO Channels with Transceiver
  Impairments</title><categories>cs.IT math.IT</categories><comments>Published in IEEE Communications Letters, 5 pages, 5 figures. The
  results can be reproduced using the following Matlab code:
  https://github.com/emilbjornson/capacity-limits-transceiver-impairments</comments><journal-ref>IEEE Communications Letters, vol. 17, no. 1, pp. 91-94, January
  2013</journal-ref><doi>10.1109/LCOMM.2012.112012.122003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of ideal MIMO channels has a high-SNR slope that equals the
minimum of the number of transmit and receive antennas. This letter analyzes if
this result holds when there are distortions from physical transceiver
impairments. We prove analytically that such physical MIMO channels have a
finite upper capacity limit, for any channel distribution and SNR. The high-SNR
slope thus collapses to zero. This appears discouraging, but we prove the
encouraging result that the relative capacity gain of employing MIMO is at
least as large as with ideal transceivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4115</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4115</id><created>2012-09-18</created><updated>2013-04-03</updated><authors><author><keyname>Samek</keyname><forenames>Wojciech</forenames></author><author><keyname>Meinecke</keyname><forenames>Frank C.</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author></authors><title>Transferring Subspaces Between Subjects in Brain-Computer Interfacing</title><categories>stat.ML cs.HC cs.LG</categories><journal-ref>W. Samek, F. C. Meinecke and K-R. M\&quot;uller, Transferring Subspaces
  Between Subjects in Brain-Computer Interfacing, IEEE Transactions on
  Biomedical Engineering, 2013</journal-ref><doi>10.1109/TBME.2013.2253608</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compensating changes between a subjects' training and testing session in
Brain Computer Interfacing (BCI) is challenging but of great importance for a
robust BCI operation. We show that such changes are very similar between
subjects, thus can be reliably estimated using data from other users and
utilized to construct an invariant feature space. This novel approach to
learning from other subjects aims to reduce the adverse effects of common
non-stationarities, but does not transfer discriminative information. This is
an important conceptual difference to standard multi-subject methods that e.g.
improve the covariance matrix estimation by shrinking it towards the average of
other users or construct a global feature space. These methods do not reduces
the shift between training and test data and may produce poor results when
subjects have very different signal characteristics. In this paper we compare
our approach to two state-of-the-art multi-subject methods on toy data and two
data sets of EEG recordings from subjects performing motor imagery. We show
that it can not only achieve a significant increase in performance, but also
that the extracted change patterns allow for a neurophysiologically meaningful
interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4129</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4129</id><created>2012-09-18</created><updated>2013-10-11</updated><authors><author><keyname>Zhang</keyname><forenames>Yuchen</forenames></author><author><keyname>Duchi</keyname><forenames>John C.</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin</forenames></author></authors><title>Comunication-Efficient Algorithms for Statistical Optimization</title><categories>stat.ML cs.LG stat.CO</categories><comments>44 pages, to appear in Journal of Machine Learning Research (JMLR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze two communication-efficient algorithms for distributed statistical
optimization on large-scale data sets. The first algorithm is a standard
averaging method that distributes the $N$ data samples evenly to $\nummac$
machines, performs separate minimization on each subset, and then averages the
estimates. We provide a sharp analysis of this average mixture algorithm,
showing that under a reasonable set of conditions, the combined parameter
achieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$.
Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rate
achievable by a centralized algorithm having access to all $\totalnumobs$
samples. The second algorithm is a novel method, based on an appropriate form
of bootstrap subsampling. Requiring only a single round of communication, it
has mean-squared error that decays as $\order(N^{-1} + (N/m)^{-3})$, and so is
more robust to the amount of parallelization. In addition, we show that a
stochastic gradient-based method attains mean-squared error decaying as
$O(N^{-1} + (N/ m)^{-3/2})$, easing computation at the expense of penalties in
the rate of convergence. We also provide experimental evaluation of our
methods, investigating their performance both on simulated data and on a
large-scale regression problem from the internet search domain. In particular,
we show that our methods can be used to efficiently solve an advertisement
prediction problem from the Chinese SoSo Search Engine, which involves logistic
regression with $N \approx 2.4 \times 10^8$ samples and $d \approx 740,000$
covariates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4133</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4133</id><created>2012-09-18</created><authors><author><keyname>Elleithy</keyname><forenames>Abdelrahman</forenames></author><author><keyname>Liu</keyname><forenames>Gonhsin</forenames></author><author><keyname>Elrashidi</keyname><forenames>Ali</forenames></author></authors><title>A New Model of the Lifetime of Wireless Sensor Networks in Sea Water
  Communications</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1201.2237</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new model for the lifetime of wireless sensor
networks used for sea water communications. The new model for power
communications takes into consideration parameters such as power consumption
for the active mode, power consumption for the sleep mode, power consumption
for the transient mode, transmission period, transient mode duration, sleep
mode duration, and active mode duration. The power communications model is
incorporated in the life time model of wireless sensor networks. The life time
model takes into consideration several parameters such as the total number of
sensors, network size, percentage of sink nodes, location of sensors, the
mobility of sensors, power consumption when nodes move and the power
consumption of communications. The new model for power consumption in
communications shows more accurate results about the lifetime of the sensor
network in comparison with previously published results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4145</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4145</id><created>2012-09-18</created><authors><author><keyname>Lee</keyname><forenames>Changwoo</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Kim</keyname><forenames>Taehyung</forenames></author><author><keyname>Choi</keyname><forenames>Sooyong</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author></authors><title>Network Massive MIMO for Cell-Boundary Users: From a Precoding
  Normalization Perspective</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose network massive multiple- input multiple-output
(MIMO) systems, where three radio units (RUs) connected via one digital unit
(DU) support multiple user equipments (UEs) at a cell-boundary through the same
radio resource, i.e., the same frequency/time band. For precoding designs,
zero-forcing (ZF) and matched filter (MF) with vector or matrix normalization
are considered. We also derive the formulae of the lower and upper bounds of
the achievable sum rate for each precoding. Based on our analytical results, we
observe that vector normalization is better for ZF while matrix normalization
is better for MF. Given antenna configurations, we also derive the optimal
switching point as a function of the number of active users in a network.
Numerical simulations confirm our analytical
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4157</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4157</id><created>2012-09-19</created><authors><author><keyname>Patri</keyname><forenames>Om Prasad</forenames></author><author><keyname>Rao</keyname><forenames>K. Sanmukh</forenames></author></authors><title>AutoAmp : An Open-Source Analog Amplifier Design Tool - For Classroom
  and Lab Purposes</title><categories>cs.OH</categories><comments>presented at the Indian Conference for Academic Research by
  Undergraduate Students (ICARUS), 2010, IIT Kanpur; AutoAmp : An Open-Source
  Analog Amplifier Design Tool - For Classroom and Lab Purposes, Proceedings of
  the Indian Conference for Academic Research by Undergraduate Students
  (ICARUS), 2010</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This correspondence presents an open-source tool AutoAmp developed at the
Indian Institute of Technology, Guwahati. It is available at
http://sourceforge.net/projects/autoamp-iitg/ This tool helps the user to
design different types of electronic amplifiers, using solid state devices, for
a given specification. It can handle several types of designs namely
common-emitter BJT amplifier (single and two-stage), operational amplifiers
(inverting and non-inverting) and power amplifier. Not only does it design the
amplifier, it also simulates the designed amplifier using SPICE simulator and
displays the performance curves. This tool is deemed to prove invaluable in
undergraduate teaching and labs. Especially in electronics-design related
laboratories, the student need not design the amplifiers which are mostly the
heart of many electronic designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4169</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4169</id><created>2012-09-19</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>S</keyname><forenames>Hemanth K.</forenames></author></authors><title>Hybrid Data Mining Technique for Knowledge Discovery from Engineering
  Materials' Data sets</title><categories>cs.DB</categories><comments>12 pages, 8 figures; International Journal of Database Management
  Systems (IJDMS), Vol.3, No.1, February 2011. arXiv admin note: text overlap
  with arXiv:1206.3078 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studying materials informatics from a data mining perspective can be
beneficial for manufacturing and other industrial engineering applications.
Predictive data mining technique and machine learning algorithm are combined to
design a knowledge discovery system for the selection of engineering materials
that meet the design specifications. Predictive method-Naive Bayesian
classifier and Machine learning Algorithm - Pearson correlation coefficient
method were implemented respectively for materials classification and
selection. The knowledge extracted from the engineering materials data sets is
proposed for effective decision making in advanced engineering materials design
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4187</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4187</id><created>2012-09-19</created><authors><author><keyname>Trencs&#xe9;ni</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Gazs&#xf3;</keyname><forenames>Attila</forenames></author><author><keyname>Reinhardt</keyname><forenames>Holger</forenames></author></authors><title>PaxosLease: Diskless Paxos for Leases</title><categories>cs.DC cs.DB</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes PaxosLease, a distributed algorithm for lease
negotiation. PaxosLease is based on Paxos, but does not require disk writes or
clock synchrony. PaxosLease is used for master lease negotation in the
open-source Keyspace and ScalienDB replicated key-value stores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4199</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4199</id><created>2012-09-19</created><updated>2015-11-16</updated><authors><author><keyname>Zhou</keyname><forenames>Xiaojun</forenames></author></authors><title>Discrete State Transition Algorithm for Unconstrained Integer
  Optimization Problems</title><categories>math.OC cs.IT math.IT math.PR math.RT</categories><comments>14 pages, 13 figures</comments><doi>10.1016/j.neucom.2015.08.041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recently new intelligent optimization algorithm called discrete state
transition algorithm is considered in this study, for solving unconstrained
integer optimization problems. Firstly, some key elements for discrete state
transition algorithm are summarized to guide its well development. Several
intelligent operators are designed for local exploitation and global
exploration. Then, a dynamic adjustment strategy ``risk and restoration in
probability&quot; is proposed to capture global solutions with high probability.
Finally, numerical experiments are carried out to test the performance of the
proposed algorithm compared with other heuristics, and they show that the
similar intelligent operators can be applied to ranging from traveling salesman
problem, boolean integer programming, to discrete value selection problem,
which indicates the adaptability and flexibility of the proposed intelligent
elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4206</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4206</id><created>2012-09-19</created><authors><author><keyname>Biswas</keyname><forenames>Barun</forenames></author><author><keyname>Basuli</keyname><forenames>Krishnendu</forenames></author><author><keyname>Naskar</keyname><forenames>Saptarshi</forenames></author><author><keyname>Chakraborti</keyname><forenames>Saomya</forenames></author><author><keyname>Sarma</keyname><forenames>Samar Sen</forenames></author></authors><title>A combinatorial algorithm to generate all spanning trees of a weighted
  graph in order of increasing cost</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The most popular algorithms for generation of minimal spanning tree are
Kruskal and Prim algorithm. Many algorithms have been proposed for generation
of all spanning tree. This paper deals with generation of all possible spanning
trees in increasing cost of a weighted graph. This approach uses one matrix
called Difference Weighted Circuit Matrix; it is little bit modification of
FCM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4207</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4207</id><created>2012-09-19</created><authors><author><keyname>Li</keyname><forenames>Yen-Huan</forenames></author><author><keyname>Su</keyname><forenames>Borching</forenames></author><author><keyname>Yeh</keyname><forenames>Ping-Cheng</forenames></author></authors><title>A Cramer-Rao Bound for Semi-Blind Channel Estimation in Redundant Block
  Transmission Systems</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Cramer-Rao bound (CRB) for semi-blind channel estimators in redundant block
transmission systems is derived. The derived CRB is valid for any system
adopting a full-rank linear redundant precoder, including the popular
cyclic-prefixed orthogonal frequency-division multiplexing system. Simple forms
of CRBs for multiple complex parameters, either unconstrained or constrained by
a holomorphic function, are also derived, which facilitate the CRB derivation
of the problem of interest. The derived CRB is a lower bound on the variance of
any unbiased semi-blind channel estimator, and can serve as a tractable
performance metric for system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4209</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4209</id><created>2012-09-19</created><updated>2014-06-25</updated><authors><author><keyname>Aliabadi</keyname><forenames>Behrooz Kamary</forenames></author><author><keyname>Ba</keyname><forenames>Sil&#xe8;ye</forenames></author></authors><title>Tight Sufficient Conditions on Exact Sparsity Pattern Recovery</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A noisy underdetermined system of linear equations is considered in which a
sparse vector (a vector with a few nonzero elements) is subject to measurement.
The measurement matrix elements are drawn from a Gaussian distribution. We
study the information-theoretic constraints on exact support recovery of a
sparse vector from the measurement vector and matrix. We compute a tight,
sufficient condition that is applied to ergodic wide-sense stationary sparse
vectors. We compare our results with the existing bounds and recovery
conditions. Finally, we extend our results to approximately sparse signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4214</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4214</id><created>2012-09-19</created><updated>2013-03-06</updated><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Weiss</keyname><forenames>Armin</forenames></author></authors><title>QuickHeapsort: Modifications and improved analysis</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><doi>10.1007/978-3-642-38536-0_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new analysis for QuickHeapsort splitting it into the analysis of
the partition-phases and the analysis of the heap-phases. This enables us to
consider samples of non-constant size for the pivot selection and leads to
better theoretical bounds for the algorithm. Furthermore we introduce some
modifications of QuickHeapsort, both in-place and using n extra bits. We show
that on every input the expected number of comparisons is n lg n - 0.03n + o(n)
(in-place) respectively n lg n -0.997 n+ o (n). Both estimates improve the
previously known best results. (It is conjectured in Wegener93 that the
in-place algorithm Bottom-Up-Heapsort uses at most n lg n + 0.4 n on average
and for Weak-Heapsort which uses n extra-bits the average number of comparisons
is at most n lg n -0.42n in EdelkampS02.) Moreover, our non-in-place variant
can even compete with index based Heapsort variants (e.g. Rank-Heapsort in
WangW07) and Relaxed-Weak-Heapsort (n lg n -0.9 n+ o (n) comparisons in the
worst case) for which no O(n)-bound on the number of extra bits is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4218</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4218</id><created>2012-09-19</created><authors><author><keyname>Fran&#xe7;ois</keyname><forenames>M&#xe9;riaux</forenames></author><author><keyname>Stefan</keyname><forenames>Valentin</forenames></author><author><keyname>Samson</keyname><forenames>Lasaulce</forenames></author><author><keyname>Michel</keyname><forenames>Kieffer</forenames></author></authors><title>An Energy-Efficient Power Allocation Game with Selfish Channel State
  Reporting in Cellular Networks</title><categories>cs.NI cs.GT</categories><comments>In Proceedings of the 6th International Conference on Performance
  Evaluation Methodologies and Tools (Valuetools), Oct. 2012, Carg\`ese, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With energy-efficient resource allocation, mobile users and base station have
different objectives. While the base station strives for an energy-efficient
operation of the complete cell, each user aims to maximize its own data rate.
To obtain this individual benefit, users may selfishly adjust their Channel
State Information (CSI) reports, reducing the cell's energy efficiency. To
analyze this conflict of interest, we formalize energy-efficient power
allocation as a utility maximization problem and present a simple algorithm
that performs close to the optimum. By formulating selfish CSI reporting as a
game, we prove the existence of an unique equilibrium and characterize energy
efficiency with true and selfish CSI in closed form. Our numerical results show
that, surprisingly, energy-efficient power allocation in small cells is more
robust against selfish CSI than cells with large transmit powers. This and
further design rules show that our paper provides valuable theoretical insight
to energy-efficient networks when CSI reports cannot be trusted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4227</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4227</id><created>2012-09-19</created><authors><author><keyname>Bereg</keyname><forenames>Sergey</forenames></author><author><keyname>Holroyd</keyname><forenames>Alexander E.</forenames></author><author><keyname>Nachmanson</keyname><forenames>Lev</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author></authors><title>Edge Routing with Ordered Bundles</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge bundling reduces the visual clutter in a drawing of a graph by uniting
the edges into bundles. We propose a method of edge bundling drawing each edge
of a bundle separately as in metro-maps and call our method ordered bundles. To
produce aesthetically looking edge routes it minimizes a cost function on the
edges. The cost function depends on the ink, required to draw the edges, the
edge lengths, widths and separations. The cost also penalizes for too many
edges passing through narrow channels by using the constrained Delaunay
triangulation. The method avoids unnecessary edge-node and edge-edge crossings.
To draw edges with the minimal number of crossings and separately within the
same bundle we develop an efficient algorithm solving a variant of the
metro-line crossing minimization problem. In general, the method creates clear
and smooth edge routes giving an overview of the global graph structure, while
still drawing each edge separately and thus enabling local analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4233</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4233</id><created>2012-09-18</created><authors><author><keyname>Levillain</keyname><forenames>Roland</forenames><affiliation>LIGM, LRDE</affiliation></author><author><keyname>G&#xe9;raud</keyname><forenames>Thierry</forenames><affiliation>LRDE</affiliation></author><author><keyname>Najman</keyname><forenames>Laurent</forenames><affiliation>LIGM</affiliation></author></authors><title>Writing Reusable Digital Geometry Algorithms in a Generic Image
  Processing Framework</title><categories>cs.MS cs.CV</categories><comments>Workshop on Applications of Discrete Geometry and Mathematical
  Morphology, Istanb : France (2010)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-32313-3_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital Geometry software should reflect the generality of the underlying
mathe- matics: mapping the latter to the former requires genericity. By
designing generic solutions, one can effectively reuse digital geometry data
structures and algorithms. We propose an image processing framework focused on
the Generic Programming paradigm in which an algorithm on the paper can be
turned into a single code, written once and usable with various input types.
This approach enables users to design and implement new methods at a lower
cost, try cross-domain experiments and help generalize results
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4236</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4236</id><created>2012-09-19</created><authors><author><keyname>Yatawatta</keyname><forenames>Sarod</forenames></author></authors><title>Estimation of Radio Interferometer Beam Shapes Using Riemannian
  Optimization</title><categories>astro-ph.IM cs.CE</categories><comments>Accepted on 18-09-2012. Draft version. The final publication is
  available at springerlink.com ; Experimental Astronomy, 2012</comments><journal-ref>Experimental Astronomy, Volume 35, Issue 3, pp.469-487, 2013</journal-ref><doi>10.1007/s10686-012-9318-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The knowledge of receiver beam shapes is essential for accurate radio
interferometric imaging. Traditionally, this information is obtained by
holographic techniques or by numerical simulation. However, such methods are
not feasible for an observation with time varying beams, such as the beams
produced by a phased array radio interferometer. We propose the use of the
observed data itself for the estimation of the beam shapes. We use the
directional gains obtained along multiple sources across the sky for the
construction of a time varying beam model. The construction of this model is an
ill posed non linear optimization problem. Therefore, we propose to use
Riemannian optimization, where we consider the constraints imposed as a
manifold. We compare the performance of the proposed approach with traditional
unconstrained optimization and give results to show the superiority of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4238</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4238</id><created>2012-09-19</created><authors><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author></authors><title>The Capacity of the Gaussian Cooperative Two-user Multiple Access
  Channel to within a Constant Gap</title><categories>cs.IT math.IT</categories><comments>Submitted to the 2013 IEEE International Conference on Communications
  (ICC 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity region of the cooperative two-user Multiple Access Channel (MAC)
in Gaussian noise is determined to within a constant gap for both the
Full-Duplex (FD) and Half-Duplex (HD) case. The main contributions are: (a) for
both FD and HD: unilateral cooperation suffices to achieve capacity to within a
constant gap where only the user with the strongest link to the destination
needs to engage in cooperation, (b) for both FD and HD: backward joint decoding
is not necessary to achieve capacity to within a constant gap, and (c) for HD:
time sharing between the case where the two users do not cooperate and the case
where the user with the strongest link to the destination acts as pure relay
for the other user suffices to achieve capacity to within a constant gap. These
findings show that simple achievable strategies are approximately optimal for
all channel parameters with interesting implications for practical cooperative
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4240</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4240</id><created>2012-09-19</created><updated>2012-11-30</updated><authors><author><keyname>Hunt</keyname><forenames>D.</forenames></author><author><keyname>Szymanski</keyname><forenames>B. K.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Network Coordination and Synchronization in a Noisy Environment with
  Time Delays</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.MA nlin.CD</categories><comments>final version, reflecting changes in response to referees' comments</comments><journal-ref>Phys. Rev. E 86, 056114 (2012)</journal-ref><doi>10.1103/PhysRevE.86.056114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effects of nonzero time delays in stochastic synchronization
problems with linear couplings in complex networks. We consider two types of
time delays: transmission delays between interacting nodes and local delays at
each node (due to processing, cognitive, or execution delays). By investigating
the underlying fluctuations for several delay schemes, we obtain the
synchronizability threshold (phase boundary) and the scaling behavior of the
width of the synchronization landscape, in some cases for arbitrary networks
and in others for specific weighted networks. Numerical computations allow the
behavior of these networks to be explored when direct analytical results are
not available. We comment on the implications of these findings for simple
locally or globally weighted network couplings and possible trade-offs present
in such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4246</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4246</id><created>2012-09-19</created><authors><author><keyname>Shen</keyname><forenames>Xiaojing</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author><author><keyname>Zhu</keyname><forenames>Yunmin</forenames></author></authors><title>Distributed Bayesian Detection Under Unknown Observation Statistics</title><categories>cs.IT math.IT</categories><comments>17 pages, 6 figures, submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, distributed Bayesian detection problems with unknown prior
probabilities of hypotheses are considered. The sensors obtain observations
which are conditionally dependent across sensors and their probability density
functions (pdf) are not exactly known. The observations are quantized and are
sent to the fusion center. The fusion center fuses the current quantized
observations and makes a final decision. It also designs (updated) quantizers
to be used at the sensors and the fusion rule based on all previous quantized
observations. Information regarding updated quantizers is sent back to the
sensors for use at the next time. In this paper, the conditional joint pdf is
represented in a parametric form by using the copula framework. The unknown
parameters include dependence parameters and marginal parameters. Maximum
likelihood estimation (MLE) with feedback based on quantized data is proposed
to estimate the unknown parameters. These estimates are iteratively used to
refine the quantizers and the fusion rule to improve distributed detection
performance by using feedback. Numerical examples show that the new detection
method based on MLE with feedback is much better than the usual detection
method based on the assumption of conditionally independent observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4255</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4255</id><created>2012-09-19</created><updated>2012-10-09</updated><authors><author><keyname>Lamberger</keyname><forenames>Mario</forenames></author><author><keyname>Teufl</keyname><forenames>Elmar</forenames></author></authors><title>Memoryless Near-Collisions, Revisited</title><categories>cs.CR</categories><comments>13 pages, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss the problem of generically finding near-collisions
for cryptographic hash functions in a memoryless way. A common approach is to
truncate several output bits of the hash function and to look for collisions of
this modified function. In two recent papers, an enhancement to this approach
was introduced which is based on classical cycle-finding techniques and
covering codes. This paper investigates two aspects of the problem of
memoryless near-collisions. Firstly, we give a full treatment of the trade-off
between the number of truncated bits and the success-probability of the
truncation based approach. Secondly, we demonstrate the limits of cycle-finding
methods for finding near-collisions by showing that, opposed to the collision
case, a memoryless variant cannot match the query-complexity of the
&quot;memory-full&quot; birthday-like near-collision finding method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4257</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4257</id><created>2012-09-19</created><authors><author><keyname>Tran</keyname><forenames>Dang-Hoan</forenames></author></authors><title>Communication-Efficient and Exact Clustering Distributed Streaming Data</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A widely used approach to clustering a single data stream is the two-phased
approach in which the online phase creates and maintains micro-clusters while
the off-line phase generates the macro-clustering from the micro-clusters. We
use this approach to propose a distributed framework for clustering streaming
data. Our proposed framework consists of fundamen- tal processes: one
coordinator-site process and many remote-site processes. Remote-site processes
can directly communicate with the coordinator-process but cannot communicate
the other remote site processes. Every remote-site process generates and
maintains micro- clusters that represent cluster information summary, from its
local data stream. Remote sites send the local micro-clusterings to the
coordinator by the serialization technique, or the coordinator invokes the
remote methods in order to get the local micro-clusterings from the remote
sites. After the coordinator receives all the local micro-clusterings from the
remote sites, it generates the global clustering by the macro-clustering
method. Our theoretical and empirical results show that, the global clustering
generated by our distributed framework is similar to the clustering generated
by the underlying centralized algorithm on the same data set. By using the
local micro-clustering approach, our framework achieves high scalability, and
communication-efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4268</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4268</id><created>2012-09-19</created><updated>2012-10-04</updated><authors><author><keyname>Egger</keyname><forenames>Jeff</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>M&#xf8;gelberg</keyname><forenames>Rasmus Ejlers</forenames><affiliation>IT University, Copehagen</affiliation></author><author><keyname>Simpson</keyname><forenames>Alex</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Linear-use CPS translations in the Enriched Effect Calculus</title><categories>cs.LO</categories><comments>27 pages; LMCS 2012</comments><proxy>LMCS</proxy><acm-class>D.3.1; F.3.3; F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (October 5,
  2012) lmcs:923</journal-ref><doi>10.2168/LMCS-8(4:2)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enriched effect calculus (EEC) is an extension of Moggi's computational
metalanguage with a selection of primitives from linear logic. This paper
explores the enriched effect calculus as a target language for
continuation-passing-style (CPS) translations in which the typing of the
translations enforces the linear usage of continuations. We first observe that
established call-by-value and call-by name linear-use CPS translations of
simply-typed lambda-calculus into intuitionistic linear logic (ILL) land in the
fragment of ILL given by EEC. These two translations are uniformly generalised
by a single generic translation of the enriched effect calculus into itself. As
our main theorem, we prove that the generic self-translation of EEC is
involutive up to isomorphism. As corollaries, we obtain full completeness
results, both for the generic translation, and for the original call-by-value
and call-by-name translations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4275</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4275</id><created>2012-09-19</created><updated>2012-10-02</updated><authors><author><keyname>Natarajan</keyname><forenames>Prabhu</forenames></author><author><keyname>Hoang</keyname><forenames>Trong Nghia</forenames></author><author><keyname>Low</keyname><forenames>Kian Hsiang</forenames></author><author><keyname>Kankanhalli</keyname><forenames>Mohan</forenames></author></authors><title>Decision-Theoretic Coordination and Control for Active Multi-Camera
  Surveillance in Uncertain, Partially Observable Environments</title><categories>cs.AI cs.MA cs.MM cs.RO</categories><comments>6th ACM/IEEE International Conference on Distributed Smart Cameras
  (ICDSC 2012), Extended version with proofs, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem of surveillance is to monitor multiple targets moving in a
large-scale, obstacle-ridden environment with occlusions. This paper presents a
novel principled Partially Observable Markov Decision Process-based approach to
coordinating and controlling a network of active cameras for tracking and
observing multiple mobile targets at high resolution in such surveillance
environments. Our proposed approach is capable of (a) maintaining a belief over
the targets' states (i.e., locations, directions, and velocities) to track
them, even when they may not be observed directly by the cameras at all times,
(b) coordinating the cameras' actions to simultaneously improve the belief over
the targets' states and maximize the expected number of targets observed with a
guaranteed resolution, and (c) exploiting the inherent structure of our
surveillance problem to improve its scalability (i.e., linear time) in the
number of targets to be observed. Quantitative comparisons with
state-of-the-art multi-camera coordination and control techniques show that our
approach can achieve higher surveillance quality in real time. The practical
feasibility of our approach is also demonstrated using real AXIS 214 PTZ
cameras
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4277</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4277</id><created>2012-09-19</created><updated>2013-01-04</updated><authors><author><keyname>Omodei</keyname><forenames>Elisa</forenames></author><author><keyname>Poibeau</keyname><forenames>Thierry</forenames></author><author><keyname>Cointet</keyname><forenames>Jean-Philippe</forenames></author></authors><title>Multi-Level Modeling of Quotation Families Morphogenesis</title><categories>cs.CY cs.CL cs.SI physics.soc-ph</categories><comments>Published in the Proceedings of the ASE/IEEE 4th Intl. Conf. on
  Social Computing &quot;SocialCom 2012&quot;, Sep. 3-5, 2012, Amsterdam, NL</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates cultural dynamics in social media by examining the
proliferation and diversification of clearly-cut pieces of content: quoted
texts. In line with the pioneering work of Leskovec et al. and Simmons et al.
on memes dynamics we investigate in deep the transformations that quotations
published online undergo during their diffusion. We deliberately put aside the
structure of the social network as well as the dynamical patterns pertaining to
the diffusion process to focus on the way quotations are changed, how often
they are modified and how these changes shape more or less diverse families and
sub-families of quotations. Following a biological metaphor, we try to
understand in which way mutations can transform quotations at different scales
and how mutation rates depend on various properties of the quotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4280</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4280</id><created>2012-09-19</created><authors><author><keyname>Yilmaz</keyname><forenames>Y. Kenan</forenames></author><author><keyname>Cemgil</keyname><forenames>A. Taylan</forenames></author></authors><title>Alpha/Beta Divergences and Tweedie Models</title><categories>stat.ML cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the underlying probabilistic interpretation of alpha and beta
divergences. We first show that beta divergences are inherently tied to Tweedie
distributions, a particular type of exponential family, known as exponential
dispersion models. Starting from the variance function of a Tweedie model, we
outline how to get alpha and beta divergences as special cases of Csisz\'ar's
$f$ and Bregman divergences. This result directly generalizes the well-known
relationship between the Gaussian distribution and least squares estimation to
Tweedie models and beta divergence minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4290</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4290</id><created>2012-09-19</created><authors><author><keyname>Potapov</keyname><forenames>Alexey</forenames></author><author><keyname>Rodionov</keyname><forenames>Sergey</forenames></author><author><keyname>Myasnikov</keyname><forenames>Andrew</forenames></author><author><keyname>Begimov</keyname><forenames>Galymzhan</forenames></author></authors><title>Cognitive Bias for Universal Algorithmic Intelligence</title><categories>cs.AI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing theoretical universal algorithmic intelligence models are not
practically realizable. More pragmatic approach to artificial general
intelligence is based on cognitive architectures, which are, however,
non-universal in sense that they can construct and use models of the
environment only from Turing-incomplete model spaces. We believe that the way
to the real AGI consists in bridging the gap between these two approaches. This
is possible if one considers cognitive functions as a &quot;cognitive bias&quot; (priors
and search heuristics) that should be incorporated into the models of universal
algorithmic intelligence without violating their universality. Earlier reported
results suiting this approach and its overall feasibility are discussed on the
example of perception, planning, knowledge representation, attention, theory of
mind, language, and some others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4297</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4297</id><created>2012-09-19</created><authors><author><keyname>Ong</keyname><forenames>Benjamin</forenames></author><author><keyname>Melfi</keyname><forenames>Andrew</forenames></author><author><keyname>Christlieb</keyname><forenames>Andrew</forenames></author></authors><title>Parallel Semi-Implicit Time Integrators</title><categories>cs.DC math.NA</categories><msc-class>65L05, 65Y05, 65L20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we further develop a family of parallel time integrators known
as Revisionist Integral Deferred Correction methods (RIDC) to allow for the
semi-implicit solution of time dependent PDEs. Additionally, we show that our
semi-implicit RIDC algorithm can harness the computational potential of
multiple general purpose graphical processing units (GPUs) in a single node by
utilizing existing CUBLAS libraries for matrix linear algebra routines in our
implementation. In the numerical experiments, we show that our implementation
computes a fourth order solution using four GPUs and four CPUs in approximately
the same wall clock time as a first order solution computed using a single GPU
and a single CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4316</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4316</id><created>2012-09-19</created><authors><author><keyname>Petra</keyname><forenames>Stefania</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Andreas</forenames></author></authors><title>Critical Parameter Values and Reconstruction Properties of Discrete
  Tomography: Application to Experimental Fluid Dynamics</title><categories>math.NA cs.IT math.IT</categories><comments>22 pages, submitted to Fundamenta Informaticae. arXiv admin note:
  text overlap with arXiv:1208.5894</comments><msc-class>65F22, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze representative ill-posed scenarios of tomographic PIV with a focus
on conditions for unique volume reconstruction. Based on sparse random seedings
of a region of interest with small particles, the corresponding systems of
linear projection equations are probabilistically analyzed in order to
determine (i) the ability of unique reconstruction in terms of the imaging
geometry and the critical sparsity parameter, and (ii) sharpness of the
transition to non-unique reconstruction with ghost particles when choosing the
sparsity parameter improperly. The sparsity parameter directly relates to the
seeding density used for PIV in experimental fluids dynamics that is chosen
empirically to date. Our results provide a basic mathematical characterization
of the PIV volume reconstruction problem that is an essential prerequisite for
any algorithm used to actually compute the reconstruction. Moreover, we connect
the sparse volume function reconstruction problem from few tomographic
projections to major developments in compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4317</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4317</id><created>2012-09-19</created><authors><author><keyname>Zhang</keyname><forenames>Haichao</forenames></author><author><keyname>Wipf</keyname><forenames>David</forenames></author><author><keyname>Zhang</keyname><forenames>Yanning</forenames></author></authors><title>Image Super-Resolution via Sparse Bayesian Modeling of Natural Images</title><categories>cs.CV</categories><comments>8 figures, 29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image super-resolution (SR) is one of the long-standing and active topics in
image processing community. A large body of works for image super resolution
formulate the problem with Bayesian modeling techniques and then obtain its
Maximum-A-Posteriori (MAP) solution, which actually boils down to a regularized
regression task over separable regularization term. Although straightforward,
this approach cannot exploit the full potential offered by the probabilistic
modeling, as only the posterior mode is sought. Also, the separable property of
the regularization term can not capture any correlations between the sparse
coefficients, which sacrifices much on its modeling accuracy. We propose a
Bayesian image SR algorithm via sparse modeling of natural images. The sparsity
property of the latent high resolution image is exploited by introducing latent
variables into the high-order Markov Random Field (MRF) which capture the
content adaptive variance by pixel-wise adaptation. The high-resolution image
is estimated via Empirical Bayesian estimation scheme, which is substantially
faster than our previous approach based on Markov Chain Monte Carlo sampling
[1]. It is shown that the actual cost function for the proposed approach
actually incorporates a non-factorial regularization term over the sparse
coefficients. Experimental results indicate that the proposed method can
generate competitive or better results than \emph{state-of-the-art} SR
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4330</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4330</id><created>2012-09-19</created><authors><author><keyname>D'Souza</keyname><forenames>Supriya</forenames></author><author><keyname>Rao</keyname><forenames>Abhishek</forenames></author><author><keyname>Sharma</keyname><forenames>Amit</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>Modeling and Verification of a Multi-Agent Argumentation System using
  NuSMV</title><categories>cs.AI cs.MA</categories><comments>8 pages, 4 figures; 20092012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous intelligent agent research is a domain situated at the forefront
of artificial intelligence. Interest-based negotiation (IBN) is a form of
negotiation in which agents exchange information about their underlying goals,
with a view to improve the likelihood and quality of a offer. In this paper we
model and verify a multi-agent argumentation scenario of resource sharing
mechanism to enable resource sharing in a distributed system. We use IBN in our
model wherein agents express their interests to the others in the society to
gain certain resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4340</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4340</id><created>2012-09-19</created><updated>2014-07-15</updated><authors><author><keyname>Winkelbauer</keyname><forenames>Andreas</forenames></author></authors><title>Moments and Absolute Moments of the Normal Distribution</title><categories>math.ST cs.IT math.IT math.PR stat.OT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present formulas for the (raw and central) moments and absolute moments of
the normal distribution. We note that these results are not new, yet many
textbooks miss out on at least some of them. Hence, we believe that it is
worthwhile to collect these formulas and their derivations in these notes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4365</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4365</id><created>2012-09-19</created><authors><author><keyname>Johnston</keyname><forenames>Andrew P.</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author></authors><title>Stochastic Stabilization of Partially Observed and Multi-Sensor Systems
  Driven by Gaussian Noise under Fixed-Rate Information Constraints</title><categories>math.OC cs.IT math.IT</categories><comments>31 pages, 2 figures. This paper is to appear in part at the IEEE
  Conference on Decision and Control, Hawaii, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the stabilization of unstable multidimensional partially
observed single-sensor and multi-sensor linear systems driven by unbounded
noise and controlled over discrete noiseless channels under fixed-rate
information constraints. Stability is achieved under fixed-rate communication
requirements that are asymptotically tight in the limit of large sampling
periods. Through the use of similarity transforms, sampling and random-time
drift conditions we obtain a coding and control policy leading to the existence
of a unique invariant distribution and finite second moment for the sampled
state. We use a vector stabilization scheme in which all modes of the linear
system visit a compact set together infinitely often. We prove tight necessary
and sufficient conditions for the general multi-sensor case under an assumption
related to the Jordan form structure of such systems. In the absence of this
assumption, we give sufficient conditions for stabilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4372</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4372</id><created>2012-09-19</created><updated>2015-04-22</updated><authors><author><keyname>Seferoglu</keyname><forenames>Hulya</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Separation of Routing and Scheduling in Backpressure-Based Wireless
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backpressure routing and scheduling, with its throughput-optimal operation
guarantee, is a promising technique to improve throughput in wireless multi-hop
networks. Although backpressure is conceptually viewed as layered, the
decisions of routing and scheduling are made jointly, which imposes several
challenges in practice. In this work, we present Diff-Max, an approach that
separates routing and scheduling and has three strengths: (i) Diff-Max improves
throughput significantly, (ii) the separation of routing and scheduling makes
practical implementation easier by minimizing cross-layer operations; i.e.,
routing is implemented in the network layer and scheduling is implemented in
the link layer, and (iii) the separation of routing and scheduling leads to
modularity; i.e., routing and scheduling are independent modules in Diff-Max,
and one can continue to operate even if the other does not. Our approach is
grounded in a network utility maximization (NUM) formulation and its solution.
Based on the structure of Diff-Max, we propose two practical schemes:
Diff-subMax and wDiff-subMax. We demonstrate the benefits of our schemes
through simulation in ns-2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4379</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4379</id><created>2012-09-19</created><authors><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author><author><keyname>Yu</keyname><forenames>Nengkun</forenames></author><author><keyname>Feng</keyname><forenames>Yuan</forenames></author></authors><title>Defining Quantum Control Flow</title><categories>quant-ph cs.LO cs.PL</categories><acm-class>D.3.1; F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A remarkable difference between quantum and classical programs is that the
control flow of the former can be either classical or quantum. One of the key
issues in the theory of quantum programming languages is defining and
understanding quantum control flow. A functional language with quantum control
flow was defined by Altenkirch and Grattage [\textit{Proc. LICS'05}, pp.
249-258]. This paper extends their work, and we introduce a general quantum
control structure by defining three new quantum program constructs, namely
quantum guarded command, quantum choice and quantum recursion. We clarify the
relation between quantum choices and probabilistic choices. An interesting
difference between quantum recursions with classical control flows and with
quantum control flows is revealed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4383</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4383</id><created>2012-09-19</created><authors><author><keyname>Viswanatha</keyname><forenames>Kumar</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>Minimum Communication Cost for Joint Distributed Source Coding and
  Dispersive Information Routing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of minimum cost communication of correlated
sources over a network with multiple sinks, which consists of distributed
source coding followed by routing. We introduce a new routing paradigm called
dispersive information routing, wherein the intermediate nodes are allowed to
`split' a packet and forward subsets of the received bits on each of the
forward paths. This paradigm opens up a rich class of research problems which
focus on the interplay between encoding and routing in a network. Unlike
conventional routing methods such as in [1], dispersive information routing
ensures that each sink receives just the information needed to reconstruct the
sources it is required to reproduce. We demonstrate using simple examples that
our approach offers better asymptotic performance than conventional routing
techniques. This paradigm leads to a new information theoretic setup, which has
not been studied earlier. We propose a new coding scheme, using principles from
multiple descriptions encoding [2] and Han and Kobayashi decoding [3]. We show
that this coding scheme achieves the complete rate region for certain special
cases of the general setup and thereby achieves the minimum communication cost
under this routing paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4403</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4403</id><created>2012-09-19</created><updated>2012-11-06</updated><authors><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author><author><keyname>Buchin</keyname><forenames>Maike</forenames></author><author><keyname>Meulemans</keyname><forenames>Wouter</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author></authors><title>Four Soviets Walk the Dog - with an Application to Alt's Conjecture</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two polygonal curves in the plane, there are several ways to define a
measure of similarity between them. One measure that has been extremely popular
in the past is the Frechet distance. Since it has been proposed by Alt and
Godau in 1992, many variants and extensions have been described. However, even
20 years later, the original O(n^2 log n) algorithm by Alt and Godau for
computing the Frechet distance remains the state of the art (here n denotes the
number of vertices on each curve). This has led Helmut Alt to conjecture that
the associated decision problem is 3SUM-hard.
  In recent work, Agarwal et al. show how to break the quadratic barrier for
the discrete version of the Frechet distance, where we consider sequences of
points instead of polygonal curves. Building on their work, we give an
algorithm to compute the Frechet distance between two polygonal curves in time
O(n^2 (log n)^(1/2) (\log\log n)^(3/2)) on a pointer machine and in time O(n^2
(loglog n)^2) on a word RAM. Furthermore, we show that there exists an
algebraic decision tree for the Frechet problem of depth O(n^(2-epsilon)), for
some epsilon &gt; 0. This provides evidence that computing the Frechet distance
may not be 3SUM-hard after all and reveals an intriguing new aspect of this
well-studied problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4405</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4405</id><created>2012-09-19</created><authors><author><keyname>You</keyname><forenames>Qingshan</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author></authors><title>Strongly Convex Programming for Principal Component Pursuit</title><categories>cs.IT math.IT math.NA</categories><comments>10 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we address strongly convex programming for princi- pal
component pursuit with reduced linear measurements, which decomposes a
superposition of a low-rank matrix and a sparse matrix from a small set of
linear measurements. We first provide sufficient conditions under which the
strongly convex models lead to the exact low-rank and sparse matrix recov- ery;
Second, we also give suggestions on how to choose suitable parameters in
practical algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4408</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4408</id><created>2012-09-19</created><authors><author><keyname>Ma</keyname><forenames>Longfei</forenames></author><author><keyname>Chen</keyname><forenames>Xue</forenames></author><author><keyname>Meng</keyname><forenames>Zhouxiang</forenames></author></authors><title>A performance Analysis of the Game of Life based on parallel algorithm</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, Conway's Game of Life using OpenMP parallel processing to
simulate several different parallel methods, experimental performance results
and compare to find the optimal solution of the parallelization of the Game of
Life. Finally pointed out the importance of the design of parallel algorithms
in solving the parallel problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4414</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4414</id><created>2012-09-19</created><updated>2012-12-20</updated><authors><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>On Cyclic DNA Codes</title><categories>cs.IT math.IT q-bio.OT</categories><comments>there is an error in Lemma 3.4</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper considers cyclic DNA codes of arbitrary length over the ring
$R=\F_2[u]/u^4-1$. A mapping is given between the elements of $R$ and the
alphabet $\{A,C,G,T\}$ which allows the additive stem distance to be extended
to this ring. Cyclic codes over $R$ are designed such that their images under
the mapping are also cyclic or quasi-cyclic of index 2. The additive distance
and hybridization energy are functions of the neighborhood energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4419</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4419</id><created>2012-09-20</created><authors><author><keyname>Wang</keyname><forenames>Chao</forenames></author></authors><title>Head Frontal-View Identification Using Extended LLE</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic head frontal-view identification is challenging due to appearance
variations caused by pose changes, especially without any training samples. In
this paper, we present an unsupervised algorithm for identifying frontal view
among multiple facial images under various yaw poses (derived from the same
person). Our approach is based on Locally Linear Embedding (LLE), with the
assumption that with yaw pose being the only variable, the facial images should
lie in a smooth and low dimensional manifold. We horizontally flip the facial
images and present two K-nearest neighbor protocols for the original images and
the flipped images, respectively. In the proposed extended LLE, for any facial
image (original or flipped one), we search (1) the Ko nearest neighbors among
the original facial images and (2) the Kf nearest neighbors among the flipped
facial images to construct the same neighborhood graph. The extended LLE
eliminates the differences (because of background, face position and scale in
the whole image and some asymmetry of left-right face) between the original
facial image and the flipped facial image at the same yaw pose so that the
flipped facial images can be used effectively. Our approach does not need any
training samples as prior information. The experimental results show that the
frontal view of head can be identified reliably around the lowest point of the
pose manifold for multiple facial images, especially the cropped facial images
(little background and centered face).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4420</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4420</id><created>2012-09-20</created><authors><author><keyname>LI</keyname><forenames>Lan-Ting</forenames></author></authors><title>An Efficient Color Face Verification Based on 2-Directional
  2-Dimensional Feature Extraction</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel and uniform framework for face verification is presented in this
paper. First of all, a 2-directional 2-dimensional feature extraction method is
adopted to extract client-specific template - 2D discrimant projection matrix.
Then the face skin color information is utilized as an additive feature to
enhance decision making strategy that makes use of not only 2D grey feature but
also 2D skin color feature. A fusion decision of both is applied to experiment
the performance on the XM2VTS database according to Lausanne protocol.
Experimental results show that the framework achieves high verification
accuracy and verification speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4425</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4425</id><created>2012-09-20</created><authors><author><keyname>Schmid</keyname><forenames>Natalia A.</forenames></author><author><keyname>Alkhweldi</keyname><forenames>Marwan</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>Distributed Estimation of a Parametric Field Using Sparse Noisy Data</title><categories>cs.IT math.IT</categories><comments>to appear at Milcom-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of distributed estimation of a parametric physical field is
stated as a maximum likelihood estimation problem. Sensor observations are
distorted by additive white Gaussian noise. Prior to data transmission, each
sensor quantizes its observation to $M$ levels. The quantized data are then
communicated over parallel additive white Gaussian channels to a fusion center
for a joint estimation. An iterative expectation-maximization (EM) algorithm to
estimate the unknown parameter is formulated, and its linearized version is
adopted for numerical analysis. The numerical examples are provided for the
case of the field modeled as a Gaussian bell. The dependence of the integrated
mean-square error on the number of quantization levels, the number of sensors
in the network and the SNR in observation and transmission channels is
analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4433</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4433</id><created>2012-09-20</created><updated>2013-03-18</updated><authors><author><keyname>Manchester</keyname><forenames>Ian R.</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques E.</forenames></author></authors><title>Transverse Contraction Criteria for Existence, Stability, and Robustness
  of a Limit Cycle</title><categories>math.OC cs.RO cs.SY</categories><comments>6 pages, 1 figure. Conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives a differential contraction condition for the existence of
an orbitally-stable limit cycle in an autonomous system. This transverse
contraction condition can be represented as a pointwise linear matrix
inequality (LMI), thus allowing convex optimization tools such as
sum-of-squares programming to be used to search for certificates of the
existence of a stable limit cycle. Many desirable properties of contracting
dynamics are extended to this context, including preservation of contraction
under a broad class of interconnections. In addition, by introducing the
concepts of differential dissipativity and transverse differential
dissipativity, contraction and transverse contraction can be established for
large scale systems via LMI conditions on component subsystems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4444</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4444</id><created>2012-09-20</created><authors><author><keyname>Pedarsani</keyname><forenames>Ramtin</forenames></author><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Tal</keyname><forenames>Ido</forenames></author><author><keyname>Telatar</keyname><forenames>Emre</forenames></author></authors><title>On the Construction of Polar Codes</title><categories>cs.IT math.IT</categories><comments>In ISIT 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of efficiently constructing polar codes over binary
memoryless symmetric (BMS) channels. The complexity of designing polar codes
via an exact evaluation of the polarized channels to find which ones are &quot;good&quot;
appears to be exponential in the block length. In \cite{TV11}, Tal and Vardy
show that if instead the evaluation if performed approximately, the
construction has only linear complexity. In this paper, we follow this approach
and present a framework where the algorithms of \cite{TV11} and new related
algorithms can be analyzed for complexity and accuracy. We provide numerical
and analytical results on the efficiency of such algorithms, in particular we
show that one can find all the &quot;good&quot; channels (except a vanishing fraction)
with almost linear complexity in block-length (except a polylogarithmic
factor).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4445</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4445</id><created>2012-09-20</created><authors><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author><author><keyname>Ramakrishna</keyname><forenames>G.</forenames></author></authors><title>Speech Signal Filters based on Soft Computing Techniques: A Comparison</title><categories>cs.AI</categories><comments>5 pages</comments><journal-ref>The 2010 International Congress on Computer Applications and
  Computational Science (CACS 2010), 4-6 December, 2010, Singapore</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a comparison of various soft computing techniques used for
filtering and enhancing speech signals. The three major techniques that fall
under soft computing are neural networks, fuzzy systems and genetic algorithms.
Other hybrid techniques such as neuro-fuzzy systems are also available. In
general, soft computing techniques have been experimentally observed to give
far superior performance as compared to non-soft computing techniques in terms
of robustness and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4452</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4452</id><created>2012-09-20</created><authors><author><keyname>Feng</keyname><forenames>Xiao</forenames></author><author><keyname>Yuan</keyname><forenames>Liping</forenames></author></authors><title>Acute Triangulations of the Cuboctahedral Surface</title><categories>math.CO cs.DM</categories><comments>16 pages, 8 figures, presented on CGGA2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove that the surface of the cuboctahedron can be
triangulated into 8 non-obtuse triangles and 12 acute triangles. Furthermore,
we show that both bounds are the best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4453</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4453</id><created>2012-09-20</created><authors><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author><author><keyname>Atrey</keyname><forenames>Shree Harsh</forenames></author><author><keyname>Sharma</keyname><forenames>Deepak Kumar</forenames></author></authors><title>A Metric for the Activeness of a Distributed Object Oriented Component
  Library</title><categories>cs.SE</categories><comments>9 pages</comments><journal-ref>MR International Journal of Engineering &amp; Technology, Vol. 2,
  No.1, pp. 19-26; June, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper makes an attempt to analyze the Activeness of a Distributed Object
Oriented Component Library and develops a software metric called Distributed
Component Activeness Quotient which is defined as the degree of readiness of a
DOOCL. The advantages of the DCAQ include a possible comparison between various
DOOCLs leading to selection of the best DOOCL for use during the development
task, and providing a measure for gauging the usefulness of the DOOCL as
indicated by the value of the DCAQ. The disadvantage of the DCAQ is that it may
have some error because of its subjective and random nature. The Stability of a
DOOCL is another characteristic which is indicated by the DCAQ. The greater the
value of the DCAQ, greater will be the stability of the corresponding DOOCL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4463</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4463</id><created>2012-09-20</created><authors><author><keyname>Shaharabani</keyname><forenames>Doron</forenames></author><author><keyname>Salzman</keyname><forenames>Oren</forenames></author><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>Sparsification of Motion-Planning Roadmaps by Edge Contraction</title><categories>cs.RO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Roadmap Sparsification by Edge Contraction (RSEC), a simple and
effective algorithm for reducing the size of a motion-planning roadmap. The
algorithm exhibits minimal effect on the quality of paths that can be extracted
from the new roadmap. The primitive operation used by RSEC is edge contraction
- the contraction of a roadmap edge to a single vertex and the connection of
the new vertex to the neighboring vertices of the contracted edge. For certain
scenarios, we compress more than 98% of the edges and vertices at the cost of
degradation of average shortest path length by at most 2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4471</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4471</id><created>2012-09-20</created><authors><author><keyname>Milo&#x161;evi&#x107;</keyname><forenames>Nikola</forenames></author></authors><title>Stemmer for Serbian language</title><categories>cs.CL cs.IR</categories><comments>16 pages, 8 figures, code included</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In linguistic morphology and information retrieval, stemming is the process
for reducing inflected (or sometimes derived) words to their stem, base or root
form; generally a written word form. In this work is presented suffix stripping
stemmer for Serbian language, one of the highly inflectional languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4479</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4479</id><created>2012-09-20</created><authors><author><keyname>Piwowarski</keyname><forenames>Benjamin</forenames></author><author><keyname>Dupret</keyname><forenames>Georges</forenames></author><author><keyname>Lalmas</keyname><forenames>Mounia</forenames></author></authors><title>Beyond Cumulated Gain and Average Precision: Including Willingness and
  Expectation in the User Model</title><categories>cs.IR</categories><acm-class>H.3.3; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define a new metric family based on two concepts: The
definition of the stopping criterion and the notion of satisfaction, where the
former depends on the willingness and expectation of a user exploring search
results. Both concepts have been discussed so far in the IR literature, but we
argue in this paper that defining a proper single valued metric depends on
merging them into a single conceptual framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4483</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4483</id><created>2012-09-20</created><updated>2013-06-08</updated><authors><author><keyname>Soussi</keyname><forenames>Mohieddine El</forenames></author><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Compute-and-Forward on a Multiaccess Relay Channel: Coding and
  Symmetric-Rate Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a system in which two users communicate with a destination with
the help of a half-duplex relay. Based on the compute-and-forward scheme, we
develop and evaluate the performance of coding strategies that are of network
coding spirit. In this framework, instead of decoding the users' information
messages, the destination decodes two integer-valued linear combinations that
relate the transmitted codewords. Two decoding schemes are considered. In the
first one, the relay computes one of the linear combinations and then forwards
it to the destination. The destination computes the other linear combination
based on the direct transmissions. In the second one, accounting for the side
information available at the destination through the direct links, the relay
compresses what it gets using Wyner-Ziv compression and conveys it to the
destination. The destination then computes the two linear combinations,
locally. For both coding schemes, we discuss the design criteria, and derive
the allowed symmetric-rate. Next, we address the power allocation and the
selection of the integer-valued coefficients to maximize the offered
symmetric-rate; an iterative coordinate descent method is proposed. The
analysis shows that the first scheme can outperform standard relaying
techniques in certain regimes, and the second scheme, while relying on feasible
structured lattice codes, can at best achieve the same performance as regular
compress-and-forward for the multiaccess relay network model that we study. The
results are illustrated through some numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4485</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4485</id><created>2012-09-20</created><authors><author><keyname>Zuzak</keyname><forenames>Ivan</forenames><affiliation>University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia</affiliation></author><author><keyname>Benc</keyname><forenames>Ivan</forenames><affiliation>Croatian Telecom, Croatia</affiliation></author></authors><title>Performance Evaluation of Hierarchical Publish-Subscribe Monitoring
  Architecture for Service-Oriented Applications</title><categories>cs.DC</categories><comments>7 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Contemporary high-performance service-oriented applications demand a
performance efficient run-time monitoring. In this paper, we analyze a
hierarchical publish-subscribe architecture for monitoring service-oriented
applications. The analyzed architecture is based on a tree topology and
publish-subscribe communication model for aggregation of distributed monitoring
data. In order to satisfy interoperability and platform independence of
service-orientation, monitoring reports are represented as XML documents. Since
XML formatting introduces a significant processing and network load, we analyze
the performance of monitoring architecture with respect to the number of
monitored nodes, the load of system machines, and the overall latency of the
monitoring system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4493</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4493</id><created>2012-09-20</created><authors><author><keyname>Melchert</keyname><forenames>O.</forenames></author></authors><title>Minimum weight spanning trees of weighted scale free networks</title><categories>cs.DS physics.comp-ph</categories><comments>10 pages, 5 figures, Lecture notes related to the summer school
  &quot;Modern Computational Science 2012 - Optimization&quot;, the example programs can
  be downloaded at http://www.mcs.uni-oldenburg.de/58424.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this lecture we will consider the minimum weight spanning tree (MST)
problem, i.e., one of the simplest and most vital combinatorial optimization
problems. We will discuss a particular greedy algorithm that allows to compute
a MST for undirected weighted graphs, namely Kruskal's algorithm, and we will
study the structure of MSTs obtained for weighted scale free random graphs.
This is meant to clarify whether the structure of MSTs is sensitive to
correlations between edge weights and topology of the underlying scale free
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4499</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4499</id><created>2012-09-20</created><updated>2012-09-21</updated><authors><author><keyname>Chmel&#xed;k</keyname><forenames>Martin</forenames></author><author><keyname>&#x158;eh&#xe1;k</keyname><forenames>Vojt&#x11b;ch</forenames></author></authors><title>Controllable-choice Message Sequence Graphs</title><categories>cs.LO</categories><comments>The full version of paper accepted to LNCS proceedings of MEMICS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the realizability problem of Message Sequence Graphs (MSG), i.e.
the problem whether a given MSG specification is correctly distributable among
parallel components communicating via messages. This fundamental problem of MSG
is known to be undecidable. We introduce a well motivated restricted class of
MSG, so called controllable-choice MSG, and show that all its models are
realizable and moreover it is decidable whether a given MSG model is a member
of this class. In more detail, this class of MSG specifications admits a
deadlock-free realization by overloading existing messages with additional
bounded control data. We also show that the presented class is the largest
known subclass of MSG that allows for deadlock-free realization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4504</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4504</id><created>2012-09-20</created><updated>2012-12-05</updated><authors><author><keyname>Chen</keyname><forenames>Xiaomin</forenames></author><author><keyname>Leith</keyname><forenames>Douglas</forenames></author></authors><title>Frames in Outdoor 802.11 WLANs Provide a Hybrid
  Binary-Symmetric/Packet-Erasure Channel</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Corrupted frames with CRC errors potentially provide a useful channel through
which we can transmit information. Using measurements taken in an outdoor
environment, we demonstrate that for 802.11 wireless links the channel provided
by corrupted frames alone (i.e. ignoring frames with PHY errors and frames
received correctly) can be accurately modelled as a binary symmetric channel
(BSC) provided appropriate pre- and post- processing is carried out. Also, the
channel provided by corrupted frames and other frames combined can be
accurately modelled as a hybrid binary symmetric/packet erasure channel.
Importantly, we find that this hybrid channel offers capacity increases of more
than 100% compared to a conventional packet erasure channel over a wide range
of RSSIs. This is a striking observation as it indicates that the potential
exists for significant network throughput gains if the information contained in
802.11 corrupted packets is exploited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4506</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4506</id><created>2012-09-20</created><authors><author><keyname>Duy</keyname><forenames>Truong Vinh Truong</forenames></author><author><keyname>Ozaki</keyname><forenames>Taisuke</forenames></author></authors><title>A three-dimensional domain decomposition method for large-scale DFT
  electronic structure calculations</title><categories>cond-mat.mtrl-sci cs.CE cs.DC physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With tens of petaflops supercomputers already in operation and exaflops
machines expected to appear within the next 10 years, efficient parallel
computational methods are required to take advantage of such extreme-scale
machines. In this paper, we present a three-dimensional domain decomposition
scheme for enabling large-scale electronic calculations based on density
functional theory (DFT) on massively parallel computers. It is composed of two
methods: (i) atom decomposition method and (ii) grid decomposition method. In
the former, we develop a modified recursive bisection method based on inertia
tensor moment to reorder the atoms along a principal axis so that atoms that
are close in real space are also close on the axis to ensure data locality. The
atoms are then divided into sub-domains depending on their projections onto the
principal axis in a balanced way among the processes. In the latter, we define
four data structures for the partitioning of grids that are carefully
constructed to make data locality consistent with that of the clustered atoms
for minimizing data communications between the processes. We also propose a
decomposition method for solving the Poisson equation using three-dimensional
FFT in Hartree potential calculation, which is shown to be better than a
previously proposed parallelization method based on a two-dimensional
decomposition in terms of communication efficiency. For evaluation, we perform
benchmark calculations with our open-source DFT code, OpenMX, paying particular
attention to the O(N) Krylov subspace method. The results show that our scheme
exhibits good strong and weak scaling properties, with the parallel efficiency
at 131,072 cores being 67.7% compared to the baseline of 16,384 cores with
131,072 diamond atoms on the K computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4508</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4508</id><created>2012-09-20</created><authors><author><keyname>Kutzkov</keyname><forenames>Konstantin</forenames></author></authors><title>Deterministic algorithms for skewed matrix products</title><categories>cs.DS cs.NA</categories><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Pagh presented a randomized approximation algorithm for the
multiplication of real-valued matrices building upon work for detecting the
most frequent items in data streams. We continue this line of research and
present new {\em deterministic} matrix multiplication algorithms.
  Motivated by applications in data mining, we first consider the case of
real-valued, nonnegative $n$-by-$n$ input matrices $A$ and $B$, and show how to
obtain a deterministic approximation of the weights of individual entries, as
well as the entrywise $p$-norm, of the product $AB$. The algorithm is simple,
space efficient and runs in one pass over the input matrices. For a user
defined $b \in (0, n^2)$ the algorithm runs in time $O(nb +
n\cdot\text{Sort}(n))$ and space $O(n + b)$ and returns an approximation of the
entries of $AB$ within an additive factor of $\|AB\|_{E1}/b$, where $\|C\|_{E1}
= \sum_{i, j} |C_{ij}|$ is the entrywise 1-norm of a matrix $C$ and
$\text{Sort}(n)$ is the time required to sort $n$ real numbers in linear space.
Building upon a result by Berinde et al. we show that for skewed matrix
products (a common situation in many real-life applications) the algorithm is
more efficient and achieves better approximation guarantees than previously
known randomized algorithms.
  When the input matrices are not restricted to nonnegative entries, we present
a new deterministic group testing algorithm detecting nonzero entries in the
matrix product with large absolute value. The algorithm is clearly outperformed
by randomized matrix multiplication algorithms, but as a byproduct we obtain
the first $O(n^{2 + \varepsilon})$-time deterministic algorithm for matrix
products with $O(\sqrt{n})$ nonzero entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4523</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4523</id><created>2012-09-20</created><updated>2013-08-01</updated><authors><author><keyname>Lefortier</keyname><forenames>Damien</forenames></author><author><keyname>Ostroumova</keyname><forenames>Liudmila</forenames></author><author><keyname>Samosvat</keyname><forenames>Egor</forenames></author></authors><title>Evolution of the Media Web</title><categories>cs.IR cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a detailed study of the part of the Web related to media content,
i.e., the Media Web. Using publicly available data, we analyze the evolution of
incoming and outgoing links from and to media pages. Based on our observations,
we propose a new class of models for the appearance of new media content on the
Web where different \textit{attractiveness} functions of nodes are possible
including ones taken from well-known preferential attachment and fitness
models. We analyze these models theoretically and empirically and show which
ones realistically predict both the incoming degree distribution and the
so-called \textit{recency property} of the Media Web, something that existing
models did not do well. Finally we compare these models by estimating the
likelihood of the real-world link graph from our data set given each model and
obtain that models we introduce are significantly more likely than previously
proposed ones. One of the most surprising results is that in the Media Web the
probability for a post to be cited is determined, most likely, by its quality
rather than by its current popularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4527</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4527</id><created>2012-09-20</created><authors><author><keyname>Choi</keyname><forenames>Okyoung</forenames></author><author><keyname>Kim</keyname><forenames>Seokhyun</forenames></author><author><keyname>Jeong</keyname><forenames>Jaeseong</forenames></author><author><keyname>Lee</keyname><forenames>Hyang-Won</forenames></author><author><keyname>Chong</keyname><forenames>Song</forenames></author></authors><title>Delay-Optimal Data Forwarding in Vehicular Sensor Networks</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Sensor Network (VSN) is emerging as a new solution for monitoring
urban environments such as Intelligent Transportation Systems and air
pollution. One of the crucial factors that determine the service quality of
urban monitoring applications is the delivery delay of sensing data packets in
the VSN. In this paper, we study the problem of routing data packets with
minimum delay in the VSN, by exploiting i) vehicle traffic statistics, ii)
anycast routing and iii) knowledge of future trajectories of vehicles such as
buses. We first introduce a novel road network graph model that incorporates
the three factors into the routing metric. We then characterize the packet
delay on each edge as a function of the vehicle density, speed and the length
of the edge. Based on the network model and delay function, we formulate the
packet routing problem as a Markov Decision Process (MDP) and develop an
optimal routing policy by solving the MDP. Evaluations using real vehicle
traces in a city show that our routing policy significantly improves the delay
performance compared to existing routing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4532</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4532</id><created>2012-09-20</created><authors><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Ramakrishna</keyname><forenames>G.</forenames></author></authors><title>Applicability of Crisp and Fuzzy Logic in Intelligent Response
  Generation</title><categories>cs.AI</categories><comments>4 pages, 1 table</comments><journal-ref>Published in proceedings of National Conference on Information,
  Computational Technologies and e-Governance 2010, Alwar, Rajasthan, India,
  19-20 November, 2010, pp. 137-139</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the merits and demerits of crisp logic and fuzzy logic
with respect to their applicability in intelligent response generation by a
human being and by a robot. Intelligent systems must have the capability of
taking decisions that are wise and handle situations intelligently. A direct
relationship exists between the level of perfection in handling a situation and
the level of completeness of the available knowledge or information or data
required to handle the situation. The paper concludes that the use of crisp
logic with complete knowledge leads to perfection in handling situations
whereas fuzzy logic can handle situations imperfectly only. However, in the
light of availability of incomplete knowledge fuzzy theory is more effective
but may be disadvantageous as compared to crisp logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4535</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4535</id><created>2012-09-20</created><authors><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author><author><keyname>Sharma</keyname><forenames>Deepak Kumar</forenames></author><author><keyname>Atrey</keyname><forenames>Shree Harsh</forenames></author><author><keyname>Sharma</keyname><forenames>Anubhav Kumar</forenames></author></authors><title>Application of Fuzzy Mathematics to Speech-to-Text Conversion by
  Elimination of Paralinguistic Content</title><categories>cs.AI</categories><comments>6 pages, 3 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1001.2267 by other authors</comments><journal-ref>Published in proceedings of National Conference on Soft Computing
  and Artificial Intelligence 2009, Faridabad, Haryana, India, Jan 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the past few decades, man has been trying to create an intelligent
computer which can talk and respond like he can. The task of creating a system
that can talk like a human being is the primary objective of Automatic Speech
Recognition. Various Speech Recognition techniques have been developed in
theory and have been applied in practice. This paper discusses the problems
that have been encountered in developing Speech Recognition, the techniques
that have been applied to automate the task, and a representation of the core
problems of present day Speech Recognition by using Fuzzy Mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4554</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4554</id><created>2012-09-20</created><authors><author><keyname>Buchnik</keyname><forenames>Erez M.</forenames></author></authors><title>Bouma2 - A Quasi-Stateless, Tunable Multiple String-Match Algorithm</title><categories>cs.DS</categories><comments>33 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bouma2 algorithm attempts to challenge the prevalent &quot;stateful&quot; exact
string-match paradigms by suggesting a &quot;quasi-stateless&quot; approach. We claim
that using state-machines to solve the multiple exact string-match problem
introduces a hidden artificial constraint, namely the Consume-Order Dependency,
which results in unnecessary overhead. Bouma2 is not restricted in this sense;
we postulate that this allows memory-efficiency and improved performance versus
its state-machine equivalents. The heart of the Bouma2 preprocessing problem is
formulated as a weighted Integer Linear Programming problem, that can be tuned
for memory footprint and performance optimization. Specifically, this allows
Bouma2 to be input-sensitive, as tuning can be based on input characteristics.
Evaluating Bouma2 against the Aho-Corasick variant of the popular Snort
Intrusion Prevention System, we demonstrate double the throughput while using
about 10% of the memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4557</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4557</id><created>2012-09-20</created><authors><author><keyname>Wiese</keyname><forenames>Moritz</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>Strong Secrecy for Multiple Access Channels</title><categories>cs.IT math.IT</categories><comments>55 pages</comments><journal-ref>Ahlswede Festschrift, LNCS 7777, pp. 71-122, Springer-Verlag
  Berlin Heidelberg, 2013</journal-ref><doi>10.1007/978-3-642-36899-8_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show strongly secret achievable rate regions for two different wiretap
multiple-access channel coding problems. In the first problem, each encoder has
a private message and both together have a common message to transmit. The
encoders have entropy-limited access to common randomness. If no common
randomness is available, then the achievable region derived here does not allow
for the secret transmission of a common message. The second coding problem
assumes that the encoders do not have a common message nor access to common
randomness. However, they may have a conferencing link over which they may
iteratively exchange rate-limited information. This can be used to form a
common message and common randomness to reduce the second coding problem to the
first one. We give the example of a channel where the achievable region equals
zero without conferencing or common randomness and where conferencing
establishes the possibility of secret message transmission. Both coding
problems describe practically relevant networks which need to be secured
against eavesdropping attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4560</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4560</id><created>2012-09-20</created><authors><author><keyname>McCreesh</keyname><forenames>Ciaran</forenames></author><author><keyname>Prosser</keyname><forenames>Patrick</forenames></author></authors><title>Distributing an Exact Algorithm for Maximum Clique: maximising the
  costup</title><categories>cs.DS cs.DC cs.DM cs.PF</categories><comments>13 pages, 2 Algorithms, 2 figures, 2 tables</comments><report-no>TR-2012-334</report-no><journal-ref>Algorithms 2012, 5(4), 545-587</journal-ref><doi>10.3390/a5040545</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take an existing implementation of an algorithm for the maximum clique
problem and modify it so that we can distribute it over an ad-hoc cluster of
machines. Our goal was to achieve a significant speedup in performance with
minimal development effort, i.e. a maximum costup. We present a simple
modification to a state-of-the-art exact algorithm for maximum clique that
allows us to distribute it across many machines. An empirical study over large
hard benchmarks shows that speedups of an order of magnitude are routine for 25
or more machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4576</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4576</id><created>2012-09-20</created><updated>2013-02-09</updated><authors><author><keyname>Girard</keyname><forenames>Antoine</forenames></author></authors><title>Low-Complexity Quantized Switching Controllers using Approximate
  Bisimulation</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of synthesizing low-complexity
controllers for incrementally stable switched systems. For that purpose, we
establish a new approximation result for the computation of symbolic models
that are approximately bisimilar to a given switched system. The main advantage
over existing results is that it allows us to design naturally quantized
switching controllers for safety or reachability specifications; these can be
pre-computed offline and therefore the online execution time is reduced. Then,
we present a technique to reduce the memory needed to store the control law by
borrowing ideas from algebraic decision diagrams for compact function
representation and by exploiting the non-determinism of the synthesized
controllers. We show the merits of our approach by applying it to a simple
model of temperature regulation in a building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4600</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4600</id><created>2012-09-18</created><authors><author><keyname>Sharma</keyname><forenames>Kamlesh</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author></authors><title>Classification Of Heterogeneous Operating System</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operating system is a bridge between system and user. An operating system
(OS) is a software program that manages the hardware and software resources of
a computer. The OS performs basic tasks, such as controlling and allocating
memory, prioritizing the processing of instructions, controlling input and
output devices, facilitating networking, and managing files. It is difficult to
present a complete as well as deep account of operating systems developed till
date. So, this paper tries to overview only a subset of the available operating
systems and its different categories. OS are being developed by a large number
of academic and commercial organizations for the last several decades. This
paper, therefore, concentrates on the different categories of OS with special
emphasis to those that had deep impact on the evolution process. The aim of
this paper is to provide a brief timely commentary on the different categories
important operating systems available today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4605</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4605</id><created>2012-09-20</created><authors><author><keyname>Kik</keyname><forenames>Marcin</forenames></author><author><keyname>G&#x229;bala</keyname><forenames>Maciej</forenames></author><author><keyname>Kuty&#x142;owski</keyname><forenames>Miros&#x142;aw</forenames></author></authors><title>One-side Energy costs of the RBO receiver</title><categories>cs.DS cs.DC cs.DM cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $n = 2^k$ be the length of the broadcast cycle of the RBO broadcast
scheduling protocol (see [arXiv:1108.5095] and [arXiv:1201.3318]). Let $lb$ and
$ub$ be the variables of the RBO receiver as defined in [ arXiv:1201.3318 ]. We
show that the number of changes of $lb$ (the &quot;left-side energy&quot;) is not greater
than $k + 1$. We also show that the number of changes of $rb$ (the &quot;right-side
energy&quot;) is not greater than $k + 2$. Thus the &quot;extra energy&quot; (defined in
[arXiv:1201.3318]) is bounded by $2 k + 3$. This updates the previous bound
from [arXiv:1201.3318], which was $4 k + 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4608</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4608</id><created>2012-09-20</created><updated>2013-05-15</updated><authors><author><keyname>Khadka</keyname><forenames>Mahesh S.</forenames></author><author><keyname>George</keyname><forenames>K. M.</forenames></author><author><keyname>Park</keyname><forenames>N.</forenames></author><author><keyname>Kim</keyname><forenames>J. B.</forenames></author></authors><title>Performance Analysis of Hybrid Forecasting Model In Stock Market
  Forecasting</title><categories>q-fin.ST cs.CE</categories><journal-ref>International Journal of Managing Information Technology (IJMIT),
  Vol. 4, No. 3, August 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents performance analysis of hybrid model comprise of
concordance and Genetic Programming (GP) to forecast financial market with some
existing models. This scheme can be used for in depth analysis of stock market.
Different measures of concordances such as Kendalls Tau, Ginis Mean Difference,
Spearmans Rho, and weak interpretation of concordance are used to search for
the pattern in past that look similar to present. Genetic Programming is then
used to match the past trend to present trend as close as possible. Then
Genetic Program estimates what will happen next based on what had happened
next. The concept is validated using financial time series data (S&amp;P 500 and
NASDAQ indices) as sample data sets. The forecasted result is then compared
with standard ARIMA model and other model to analyse its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4612</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4612</id><created>2012-09-20</created><authors><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author></authors><title>Polar Codes: Robustness of the Successive Cancellation Decoder with
  Respect to Quantization</title><categories>cs.IT math.IT</categories><comments>In ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes provably achieve the capacity of a wide array of channels under
successive decoding. This assumes infinite precision arithmetic. Given the
successive nature of the decoding algorithm, one might worry about the
sensitivity of the performance to the precision of the computation.
  We show that even very coarsely quantized decoding algorithms lead to
excellent performance. More concretely, we show that under successive decoding
with an alphabet of cardinality only three, the decoder still has a threshold
and this threshold is a sizable fraction of capacity. More generally, we show
that if we are willing to transmit at a rate $\delta$ below capacity, then we
need only $c \log(1/\delta)$ bits of precision, where $c$ is a universal
constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4616</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4616</id><created>2012-09-20</created><updated>2014-03-27</updated><authors><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Rethinking Centrality: The Role of Dynamical Processes in Social Network
  Analysis</title><categories>cs.SI physics.soc-ph</categories><comments>Discrete and Continuous Dynamical Systems (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many popular measures used in social network analysis, including centrality,
are based on the random walk. The random walk is a model of a stochastic
process where a node interacts with one other node at a time. However, the
random walk may not be appropriate for modeling social phenomena, including
epidemics and information diffusion, in which one node may interact with many
others at the same time, for example, by broadcasting the virus or information
to its neighbors. To produce meaningful results, social network analysis
algorithms have to take into account the nature of interactions between the
nodes. In this paper we classify dynamical processes as conservative and
non-conservative and relate them to well-known measures of centrality used in
network analysis: PageRank and Alpha-Centrality. We demonstrate, by ranking
users in online social networks used for broadcasting information, that
non-conservative Alpha-Centrality generally leads to a better agreement with an
empirical ranking scheme than the conservative PageRank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4620</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4620</id><created>2012-09-20</created><updated>2013-10-23</updated><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author><author><keyname>Bhandari</keyname><forenames>Vartika</forenames></author></authors><title>Broadcast Using Certified Propagation Algorithm in Presence of Byzantine
  Faults</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the correctness of the Certified Propagation Algorithm (CPA) [6,
1, 8, 5] in solving broadcast with locally bounded Byzantine faults. CPA allows
the nodes to use only local information regarding the network topology. We
provide a tight necessary and sufficient condition on the network topology for
the correctness of CPA. To the best of our knowledge, this work is the first to
solve the open problem in [8]. We also present some simple extensions of this
result
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4621</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4621</id><created>2012-09-20</created><authors><author><keyname>Kanchu</keyname><forenames>Krishnama Raju</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Goldbach Circles and Balloons and Their Cross Correlation</title><categories>cs.CR</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goldbach partitions can be used in creation of ellipses and circles on the
number line. We extend this work and determine the count and other properties
of concentric Goldbach circles for different values of n. The autocorrelation
function of this sequence with respect to even and odd values suggests that it
has excellent randomness properties. Cross correlation properties of ellipse
and circle sequences are provided that indicate that these sequences have
minimal dependencies and, therefore, they can be used in spread spectrum and
other cryptographic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4623</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4623</id><created>2012-09-20</created><authors><author><keyname>Stephen</keyname><forenames>Tamon</forenames></author><author><keyname>Yusun</keyname><forenames>Timothy</forenames></author></authors><title>Counting inequivalent monotone Boolean functions</title><categories>cs.DS math.CO</categories><msc-class>Primary: 68W05, Secondary: 06E30, 05A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monotone Boolean functions (MBFs) are Boolean functions $f: {0,1}^n
\rightarrow {0,1}$ satisfying the monotonicity condition $x \leq y \Rightarrow
f(x) \leq f(y)$ for any $x,y \in {0,1}^n$. The number of MBFs in n variables is
known as the $n$th Dedekind number. It is a longstanding computational
challenge to determine these numbers exactly - these values are only known for
$n$ at most 8. Two monotone Boolean functions are inequivalent if one can be
obtained from the other by renaming the variables. The number of inequivalent
MBFs in $n$ variables was known only for up to $n = 6$. In this paper we
propose a strategy to count inequivalent MBF's by breaking the calculation into
parts based on the profiles of these functions. As a result we are able to
compute the number of inequivalent MBFs in 7 variables. The number obtained is
490013148.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4633</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4633</id><created>2012-09-20</created><authors><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Kumar</keyname><forenames>Nand</forenames></author><author><keyname>Hooda</keyname><forenames>Sugandha</forenames></author><author><keyname>Bhardwaj</keyname><forenames>Nitin</forenames></author></authors><title>A Metric For The Activeness Of An Object-Oriented Component Library</title><categories>cs.SE</categories><comments>6 pages. arXiv admin note: text overlap with arXiv:1209.4453</comments><journal-ref>Software Engineering Research and Practice, WORLDCOMP 2007 - The
  2007 World Congress in Computer Science, Computer Engineering, and Applied
  Computing, Las Vegas, Nevada, USA, 25th - 28th June 2007, pp. 704-709</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an attempt has been made to analyze the Activeness of an
Object Oriented Component Library and develop a special type of software metric
called Component Activeness Quotient which is defined as the degree of
readiness of an OOCL. The advantages of the CAQ include a possible comparison
between various OOCLs leading to selection of the best OOCL for use during the
development task, and Stability of the software can be gauged as indicated by
the value of the CAQ. The disadvantage of the CAQ is that it may have some
error because of its subjective and random nature. The paper also tries to
improvise the calculation of the Activeness Quotient. The extreme case of a
software organization having an RQ greater than 1 and MQ equal to 0 was not
handled by the method of taking an average of RQ and MQ to calculate the AQ.
The improvisation is that the AQ must be equal to a product of MQ and RQ and
this is mentioned in the Appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4634</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4634</id><created>2012-09-20</created><authors><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Sharma</keyname><forenames>Deepak Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>Jugnesh</forenames></author><author><keyname>Verma</keyname><forenames>Ramesh Chandra</forenames></author><author><keyname>Prasad</keyname><forenames>T V</forenames></author></authors><title>A Metric for the Activeness of a Class</title><categories>cs.SE</categories><comments>7 pages, 1 table</comments><journal-ref>International Journal of Information Technology &amp; Knowledge
  Management, Vol II, Issue I, 109-113, June 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the authors propose a software metric called Class Activeness
Metric which helps to determine the level of accessibility of the members of a
class when it is instantiated as objects. Object interactions need to be
straight forward as far as possible as complexity in these interactions can
lead to time delays in accessing members not just confusing inheritance
hierarchies. For object interactions to be non-complex, the classes must be
designed well so that they are easily accessible. This necessitates the
development of a metric for gauging the quality of design of a class. This
metric is the Class Activeness Metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4635</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4635</id><created>2012-09-20</created><authors><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Sharma</keyname><forenames>Deepak Kumar</forenames></author></authors><title>DolNet: A Division Of Labour Based Distributed Object Oriented Software
  Process Model</title><categories>cs.SE</categories><comments>9 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1209.4453</comments><journal-ref>Published in proceedings of the First International Conference on
  Data Management 2008, Ghaziabad, Uttar Pradesh, India, Feb 2008, pp.1284-1290</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Software Development today is in its childhood and not too
widespread as a method of developing software in the global IT Industry. In
this context, Petrinets are a mathematical model for describing distributed
systems theoretically, whereas AttNets are one of their offshoots. But
development of true distributed software is limited to network operating
systems majorly. Software that runs on many machines with separate programs for
each machine, are very few. This paper introduces and defines Distributed
Object Oriented Software Engineering DOOSE as a new field in software
engineering. The paper further gives a Distributed Object Oriented Software
Process Model DOOSPM, called the DolNet, which describes how work may be done
by a software development organization while working on Distributed Object
Oriented DOO Projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4679</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4679</id><created>2012-09-20</created><authors><author><keyname>Nagpal</keyname><forenames>Vinayak</forenames></author><author><keyname>Wang</keyname><forenames>I-Hsiang</forenames></author><author><keyname>Jorgovanovic</keyname><forenames>Milos</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author><author><keyname>Nikolic</keyname><forenames>Borivoje</forenames></author></authors><title>Coding and System Design for Quantize-Map-and-Forward Relaying</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Journal of Selected Areas in Communication,
  Theories and Methods for Advanced Wireless Relays Part 2, 2012</comments><journal-ref>IEEE Journal of Selected Areas in Communication, 2013</journal-ref><doi>10.1109/JSAC.2013.130807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a low-complexity coding scheme and system design
framework for the half duplex relay channel based on the
Quantize-Map-and-Forward (QMF) relay- ing scheme. The proposed framework allows
linear complexity operations at all network terminals. We propose the use of
binary LDPC codes for encoding at the source and LDGM codes for mapping at the
relay. We express joint decoding at the destination as a belief propagation
algorithm over a factor graph. This graph has the LDPC and LDGM codes as
subgraphs connected via probabilistic constraints that model the QMF relay
operations. We show that this coding framework extends naturally to the high
SNR regime using bit interleaved coded modulation (BICM). We develop density
evolution analysis tools for this factor graph and demonstrate the design of
practical codes for the half-duplex relay channel that perform within 1dB of
information theoretic QMF threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4683</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4683</id><created>2012-09-20</created><updated>2013-10-03</updated><authors><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Xu</keyname><forenames>Zi</forenames></author><author><keyname>Razaviyayn</keyname><forenames>Meisam</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Joint User Grouping and Linear Virtual Beamforming: Complexity,
  Algorithms and Approximation Bounds</title><categories>cs.IT math.IT</categories><comments>To appear, JSAC special issue on virtual antenna systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a wireless system with a large number of distributed nodes, the quality of
communication can be greatly improved by pooling the nodes to perform joint
transmission/reception. In this paper, we consider the problem of optimally
selecting a subset of nodes from potentially a large number of candidates to
form a virtual multi-antenna system, while at the same time designing their
joint linear transmission strategies. We focus on two specific application
scenarios: 1) multiple single antenna transmitters cooperatively transmit to a
receiver; 2) a single transmitter transmits to a receiver with the help of a
number of cooperative relays. We formulate the joint node selection and
beamforming problems as cardinality constrained optimization problems with both
discrete variables (used for selecting cooperative nodes) and continuous
variables (used for designing beamformers). For each application scenario, we
first characterize the computational complexity of the joint optimization
problem, and then propose novel semi-definite relaxation (SDR) techniques to
obtain approximate solutions. We show that the new SDR algorithms have a
guaranteed approximation performance in terms of the gap to global optimality,
regardless of channel realizations. The effectiveness of the proposed
algorithms is demonstrated via numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4687</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4687</id><created>2012-09-20</created><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author></authors><title>Capacity of Gaussian Channels with Duty Cycle and Power Constraints</title><categories>cs.IT math.IT</categories><comments>36 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many wireless communication systems, radios are subject to a duty cycle
constraint, that is, a radio only actively transmits signals over a fraction of
the time. For example, it is desirable to have a small duty cycle in some low
power systems; a half-duplex radio cannot keep transmitting if it wishes to
receive useful signals; and a cognitive radio needs to listen and detect
primary users frequently. This work studies the capacity of scalar
discrete-time Gaussian channels subject to duty cycle constraint as well as
average transmit power constraint. An idealized duty cycle constraint is first
studied, which can be regarded as a requirement on the minimum fraction of
nontransmissions or zero symbols in each codeword. A unique discrete input
distribution is shown to achieve the channel capacity. In many situations,
numerically optimized on-off signaling can achieve much higher rate than
Gaussian signaling over a deterministic transmission schedule. This is in part
because the positions of nontransmissions in a codeword can convey information.
Furthermore, a more realistic duty cycle constraint is studied, where the extra
cost of transitions between transmissions and nontransmissions due to pulse
shaping is accounted for. The capacity-achieving input is no longer independent
over time and is hard to compute. A lower bound of the achievable rate as a
function of the input distribution is shown to be maximized by a first-order
Markov input process, the distribution of which is also discrete and can be
computed efficiently. The results in this paper suggest that, under various
duty cycle constraints, departing from the usual paradigm of intermittent
packet transmissions may yield substantial gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4700</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4700</id><created>2012-09-21</created><authors><author><keyname>Merekin</keyname><forenames>Yuri V.</forenames></author></authors><title>Fast Computation of the Arnold Complexity of Length $2^{n}$ Binary Words</title><categories>math.CO cs.IT math.IT</categories><comments>7 pages</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For fast computation of the Arnold complexity of length $2^{n}$ binary words
we obtain an upper bound for the Shannon function $Sh(n)$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4703</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4703</id><created>2012-09-21</created><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Fu</keyname><forenames>Hu</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>Simultaneous Auctions are (almost) Efficient</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous item auctions are simple procedures for allocating items to
bidders with potentially complex preferences over different item sets. In a
simultaneous auction, every bidder submits bids on all items simultaneously.
The allocation and prices are then resolved for each item separately, based
solely on the bids submitted on that item. Such procedures occur in practice
(e.g. eBay) but are not truthful. We study the efficiency of Bayesian Nash
equilibrium (BNE) outcomes of simultaneous first- and second-price auctions
when bidders have complement-free (a.k.a. subadditive) valuations. We show that
the expected social welfare of any BNE is at least 1/2 of the optimal social
welfare in the case of first-price auctions, and at least 1/4 in the case of
second-price auctions. These results improve upon the previously-known
logarithmic bounds, which were established by [Hassidim, Kaplan, Mansour and
Nisan '11] for first-price auctions and by [Bhawalkar and Roughgarden '11] for
second-price auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4714</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4714</id><created>2012-09-21</created><authors><author><keyname>Cetin</keyname><forenames>A. Emre</forenames></author></authors><title>Sorting distinct integers using improved in-place associative sort</title><categories>cs.DS</categories><comments>16 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.3668, arXiv:1209.1942, arXiv:1209.0572</comments><msc-class>68P05, 68P10</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In-place associative integer sorting technique was proposed for integer lists
which requires only constant amount of additional memory replacing bucket sort,
distribution counting sort and address calculation sort family of algorithms.
Afterwards, the technique was further improved and an in-place sorting
algorithm is proposed where n integers S[0...n-1] each in the range [0, n-1]
are sorted exactly in O(n) time while the complexity of the former technique
was the recursion T(n) = T(n/2) + O(n) yielding T(n) = O(n).
  The technique was specialized with two variants one for read-only distinct
integer keys and the other for modifiable distinct integers, as well. Assuming
w is the fixed word length, the variant for modifiable distinct integers was
capable of sorting n distinct integers S[0...n-1] each in the range [0, m-1] in
exactly O(n) time if m &lt; (w-logn)n. Otherwise, it sort in O(n + m/(w-logn))
time for the worst, O(m/(w-logn)) time for the average (uniformly distributed
keys) and O(n) time for the best case using only O(1) extra space.
  In this study, the variant for modifiable distinct integers is improved and
an algorithm is obtained that sorts n distinct integers S[0...n-1] each in the
range [0, m-1] in exactly O(n) time if m &lt; (w-1)n. Otherwise, it sort in O(n +
m/(w-1)) time for the worst, O(m/(w-1)) time for the average (uniformly
distributed keys) and O(n) time for the best case using only O(1) extra space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4751</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4751</id><created>2012-09-21</created><authors><author><keyname>Chatterjee</keyname><forenames>Priyanka</forenames></author><author><keyname>Agarwal</keyname><forenames>Nikhil</forenames></author></authors><title>Energy Aware, Scalable, K-Hop Based Cluster Formation In MANET</title><categories>cs.DC cs.DS cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of Mobile Ad-hoc Network remains attractive due to the desire to
achieve better performance and scalability. MANETs are distributed systems
consisting of mobile hosts that are connected by multi-hop wireless links. Such
systems are self organized and facilitate communication in the network without
any centralized administration. MANETs exhibit battery power constraint and
suffer scalability issues therefore cluster formation is expensive. This is due
to the large number of messages passed during the process of cluster formation.
Clustering has evolved as an imperative research domain that enhances system
performance such as throughput and delay in Mobile Ad hoc Networks (MANETs) in
the presence of both mobility and a large number of mobile terminals.In this
thesis, we present a clustering scheme that minimizes message overhead and
congestion for cluster formation and maintenance. The algorithm is devised to
be independent of the MANET Routing algorithm. Depending upon the context, the
clustering algorithm may be implemented in the routing or in higher layers. The
dynamic formation of clusters helps reduce data packet overhead, node
complexity and power consumption, The simulation has been performed in ns-2.
The simulation shows that the number of clusters formed is in proportion with
the number of nodes in MANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4760</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4760</id><created>2012-09-21</created><authors><author><keyname>Gligorijevic</keyname><forenames>Vladimir</forenames></author><author><keyname>Skowron</keyname><forenames>Marcin</forenames></author><author><keyname>Tadic</keyname><forenames>Bosiljka</forenames></author></authors><title>Structure and stability of online chat networks built on
  emotion-carrying links</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 5 figures</comments><doi>10.1016/j.physa.2012.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-resolution data of online chats are studied as a physical system in
laboratory in order to quantify collective behavior of users. Our analysis
reveals strong regularities characteristic to natural systems with additional
features. In particular, we find self-organized dynamics with long-range
correlations in user actions and persistent associations among users that have
the properties of a social network. Furthermore, the evolution of the graph and
its architecture with specific k-core structure are shown to be related with
the type and the emotion arousal of exchanged messages. Partitioning of the
graph by deletion of the links which carry high arousal messages exhibits
critical fluctuations at the percolation threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4761</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4761</id><created>2012-09-21</created><authors><author><keyname>Urakov</keyname><forenames>Airat</forenames></author><author><keyname>Timeryaev</keyname><forenames>Timofey</forenames></author></authors><title>Algorithms of Fast Search of Center, Radius and Diameter on Weighted
  Graphs</title><categories>cs.DS cs.DM</categories><comments>16 pages, 3 figures, 3 tables</comments><msc-class>05C22 (Primary) 05C12 (Secondary)</msc-class><acm-class>G.2.2; F.2.2; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two problems in the search of metric characteristics on weighted undirected
graphs with non-negative edge weights are being considered. The first problem:
a weighted undirected graph with non-negative edge weight is given. The radius,
diameter and at least one center and one pair of peripheral vertices of the
graph are to be found. In the second problem we have additionally calculated
the distances matrix. For the problems being considered, we proposed fast
search algorithms which use only small fraction of graph's vertices for the
search of the metric characteristics. The proposed algorithms have been
compared to other popular methods of solving problems considered on various
inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4763</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4763</id><created>2012-09-21</created><authors><author><keyname>Ricciato</keyname><forenames>Fabio</forenames></author><author><keyname>Castiglione</keyname><forenames>Paolo</forenames></author></authors><title>Pseudo-random Aloha for Enhanced Collision-recovery in RFID</title><categories>cs.NI</categories><comments>This manuscript has been submitted to IEEE on the 19th September 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we motivate the need to revisit the MAC protocol used in Gen2
RFID system in order to leverage receiver structures with Collision Recovery
capabilities at the PHY layer. To this end we propose to consider a simple
variant of the Framed Slotted Aloha with pseudo-random (deterministic) slot
selection as opposite to the classical random selection. Pseudo-random access
allows naturally to implement Inter-frame Successive Interference Cancellation
(ISIC) without changing the PHY modulation and coding format of legacy RFID
standard. By means of simulations we show that ISIC can bring 20-25% gain in
throughput with respect to traditional intra-frame SIC. Besides that, we
elaborate on the potential of leveraging pseudo-random access protocols in
combination with advanced PHY techniques in the context of RFID applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4771</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4771</id><created>2012-09-21</created><authors><author><keyname>Goldstein</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Beling</keyname><forenames>Piotr</forenames></author></authors><title>Counting common substrings effectively</title><categories>cs.DS</categories><comments>6 pages, in Polish</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents effective (dynamic) algorithm for solving a problem of
counting the number of substrings of given string which are also substrings of
second string. Presented algorithm can be used for example for quick
calculation of strings similarity measure using generalized $n$-gram method
(Niewiadomski measure), which are shown. Correctness and complexity analyses
are included.
  -----
  W artykule przedstawiono efektywny (dynamiczny) algorytm wyznaczaj\k{a}cy
miar\k{e} podobie\'nstwa wyraz\'ow za pomoc\k{a} uog\'olnionej metody
$n$-gram\'ow (miary Niewiadomskiego). Uzasadniono tak\.ze poprawno\'s\'c
dzia{\l}ania algorytmu i oszacowano jego z{\l}o\.zono\'s\'c obliczeniow\k{a}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4772</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4772</id><created>2012-09-21</created><updated>2012-09-25</updated><authors><author><keyname>Kawamura</keyname><forenames>Masaki</forenames></author><author><keyname>Uezu</keyname><forenames>Tatsuya</forenames></author><author><keyname>Okada</keyname><forenames>Masato</forenames></author></authors><title>Statistical mechanical evaluation of spread spectrum watermarking model
  with image restoration</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cases in which an original image is blind, a decoding method where both
the image and the messages can be estimated simultaneously is desirable. We
propose a spread spectrum watermarking model with image restoration based on
Bayes estimation. We therefore need to assume some prior probabilities. The
probability for estimating the messages is given by the uniform distribution,
and the ones for the image are given by the infinite range model and 2D Ising
model. Any attacks from unauthorized users can be represented by channel
models. We can obtain the estimated messages and image by maximizing the
posterior probability.
  We analyzed the performance of the proposed method by the replica method in
the case of the infinite range model. We first calculated the theoretical
values of the bit error rate from obtained saddle point equations and then
verified them by computer simulations. For this purpose, we assumed that the
image is binary and is generated from a given prior probability. We also assume
that attacks can be represented by the Gaussian channel. The computer
simulation retults agreed with the theoretical values.
  In the case of prior probability given by the 2D Ising model, we evaluated
the decoding performance by computer simulations since the replica theory could
not be applied. Results using the 2D Ising model showed that the proposed
method with image restoration is as effective as the infinite range model for
decoding messages.
  We compared the performances in a case in which the image was blind and one
in which it was informed. The difference between these cases was small as long
as the embedding and attack rates were small. This demonstrates that the
proposed method with simultaneous estimation is effective as a watermarking
decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4777</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4777</id><created>2012-09-21</created><authors><author><keyname>Abugharsa</keyname><forenames>Ahmed Bashir</forenames></author><author><keyname>Basari</keyname><forenames>Abd Samad Bin Hasan</forenames></author><author><keyname>Almangush</keyname><forenames>Hamida</forenames></author></authors><title>A Novel Image Encryption Using an Integration Technique of Blocks
  Rotation Based on the Magic Cube and the AES Algorithm</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, several encryption algorithms have been proposed to protect
digital images from cryptographic attacks. These encryption algorithms
typically use a relatively small key space and therefore, provide safe,
especially if they are of a dimension. In this paper proposes an encryption
algorithm for a new image protection scheme based on the rotation of the faces
of a Magic Cube. The original image is divided into six sub-images and these
sub-images are divided amongst a number of blocks and attached to the faces of
a Magic Cube. The faces are then scrambled using rotation of the Magic Cube.
Then the rotated image is fed to the AES algorithm which is applied to the
pixels of the image to encrypt the scrambled image. Finally, experimental
results and security analysis show that the proposed image encryption scheme
not only encrypts the picture to achieve perfect hiding, but the algorithm can
also withstand exhaustive, statistical and differential attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4778</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4778</id><created>2012-09-21</created><authors><author><keyname>Rehenasulthana</keyname><forenames>M.</forenames></author><author><keyname>Bhuvaneswari</keyname><forenames>P. T. V.</forenames></author><author><keyname>Rama</keyname><forenames>N.</forenames></author></authors><title>Enhanced Location Based Routing Protocol for 6LoWPAN</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.4, No.3, 2012, 93-108</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  6LoWPAN (IPv6 over IEEE 802.15.4) standardized by IEEE 802.15.4 provides IP
communication capability for nodes in WSN. An adaptation layer is introduced
above the MAC layer to achieve header compression, fragmentation and reassembly
of IP packets. The location-based information is used to simplify the routing
policy. This paper proposes an efficient location-based routing protocol,
considering link quality and distance between nodes as the routing metric. The
proposed Enhanced Location-based routing protocol (ELBRP) was simulated in NS2
version 2.32 and performance were analysed in terms of packet delivery ratio,
throughput and average end-to-end delay. From the results obtained, it is found
that the proposed ELBRP outperforms existing LOAD protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4784</identifier>
 <datestamp>2013-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4784</id><created>2012-09-21</created><updated>2013-08-15</updated><authors><author><keyname>Sen</keyname><forenames>Nilanjan</forenames></author></authors><title>An Identity based Node Authentication and Session Key Management
  Algorithm using Elliptic Curve Cryptography</title><categories>cs.CR</categories><comments>The paper has been withdrawn for modification and the newer version
  will be uploaded soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn for modification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4785</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4785</id><created>2012-09-21</created><authors><author><keyname>Li</keyname><forenames>Xiaodong</forenames></author><author><keyname>Voroninski</keyname><forenames>Vladislav</forenames></author></authors><title>Sparse Signal Recovery from Quadratic Measurements via Convex
  Programming</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a system of quadratic equations |&lt;z_j, x&gt;|^2 = b_j,
j = 1, ..., m, where x in R^n is unknown while normal random vectors z_j in R_n
and quadratic measurements b_j in R are known. The system is assumed to be
underdetermined, i.e., m &lt; n. We prove that if there exists a sparse solution
x, i.e., at most k components of x are non-zero, then by solving a convex
optimization program, we can solve for x up to a multiplicative constant with
high probability, provided that k &lt;= O((m/log n)^(1/2)). On the other hand, we
prove that k &lt;= O(log n (m)^(1/2)) is necessary for a class of naive convex
relaxations to be exact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4789</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4789</id><created>2012-09-21</created><authors><author><keyname>Biernacki</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Krieger</keyname><forenames>Udo</forenames></author></authors><title>Session level analysis of P2P television traces</title><categories>cs.NI</categories><journal-ref>Lecture Notes in Computer Science, 2010, Volume 6157/2010, 157-166</journal-ref><doi>10.1007/978-3-642-13789-1_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we examine statistical properties of traffic generated by the
popular P2P IPTV application SopCast. The analysis aims at a better
understanding of the mechanisms used by such applications and their impact on
the network. Since the most popular P2P IPTV applications use proprietary
unpublished protocols, we look directly at the generated traffic focusing on a
single session analysis, which is the major contribution of our work. We
present a basic characterisation of the traffic profile generated by SopCast
during every separate session in terms of the intensity, the burstiness, the
distribution of the packet sizes and the correlation. We show that some of
these statistical properties of the analysed traffic may be quite different
depending on the particular session.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4806</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4806</id><created>2012-09-21</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Paulo</forenames><affiliation>LIP</affiliation></author><author><keyname>Roy</keyname><forenames>Shubhabrata</forenames><affiliation>LIP</affiliation></author><author><keyname>Begin</keyname><forenames>Thomas</forenames><affiliation>LIP</affiliation></author><author><keyname>Loiseau</keyname><forenames>Patrick</forenames><affiliation>EURECOM</affiliation></author></authors><title>Dynamic Resource Management in Clouds: A Probabilistic Approach</title><categories>cs.NI</categories><comments>IEICE Transactions on Communications (2012). arXiv admin note:
  substantial text overlap with arXiv:1209.5158</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic resource management has become an active area of research in the
Cloud Computing paradigm. Cost of resources varies significantly depending on
configuration for using them. Hence efficient management of resources is of
prime interest to both Cloud Providers and Cloud Users. In this work we suggest
a probabilistic resource provisioning approach that can be exploited as the
input of a dynamic resource management scheme. Using a Video on Demand use case
to justify our claims, we propose an analytical model inspired from standard
models developed for epidemiology spreading, to represent sudden and intense
workload variations. We show that the resulting model verifies a Large
Deviation Principle that statistically characterizes extreme rare events, such
as the ones produced by &quot;buzz/flash crowd effects&quot; that may cause workload
overflow in the VoD context. This analysis provides valuable insight on
expectable abnormal behaviors of systems. We exploit the information obtained
using the Large Deviation Principle for the proposed Video on Demand use-case
for defining policies (Service Level Agreements). We believe these policies for
elastic resource provisioning and usage may be of some interest to all
stakeholders in the emerging context of cloud networking
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4811</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4811</id><created>2012-09-21</created><authors><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author><author><keyname>Kumar</keyname><forenames>M. Sathish</forenames></author><author><keyname>Mruthyunjaya</keyname><forenames>H. S.</forenames></author></authors><title>Performance Analysis of Error Control Coding Techniques for
  Peak-to-Average Power Ratio Reduction of Multicarrier Signals</title><categories>cs.IT math.IT</categories><comments>26 pages, 12 figures</comments><journal-ref>International Journal of Information Processing, vol.4, no.1,
  pp.22-35, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing demands on high data rate mobile communications services will
inevitably drive future broadband mobile communication systems toward achieving
data transmission rates in excess of 100 Mbps. One of the promising
technologies which can satisfy this demand on high data rate mobile
communications services is the Orthogonal Frequency Division Multiplexing
(OFDM) transmission technology which falls under the general category of
multicarrier modulation systems. OFDM is a spectrally efficient modulation
technique that can achieve high speed data transmission over multipath fading
channels without the need for powerful equalization techniques. However the
price paid for this high spectral efficiency and less intensive equalization is
low power efficiency. OFDM signals are very sensitive to non-linear effects due
to the high peak-to-average power ratio (PAPR), which leads to the power
inefficiency in the RF section of the transmitter. This paper analyzes the
relation between aperiodic autocorrelation of OFDM symbols and PAPR. The paper
also gives a comparative study of PAPR reduction performance of various channel
coding techniques for the OFDM signals. For our study we have considered
Hamming codes, cyclic codes, convolution codes, Golay and Reed-Muller codes.
The results show that each of the channel coding technique has a different PAPR
reduction performance. Coding technique with the highest value of PAPR
reduction has been identified along with an illustration on PAPR reduction
performances with respect to each code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4818</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4818</id><created>2012-09-21</created><updated>2015-06-18</updated><authors><author><keyname>Presman</keyname><forenames>Noam</forenames></author><author><keyname>Litsyn</keyname><forenames>Simon</forenames></author></authors><title>Recursive Descriptions of Polar Codes</title><categories>cs.IT cs.AR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are recursive general concatenated codes. This property motivates
a recursive formalization of the known decoding algorithms: Successive
Cancellation, Successive Cancellation with Lists and Belief Propagation. Using
such description allows an easy development of these algorithms for arbitrary
polarizing kernels. Hardware architectures for these decoding algorithms are
also described in a recursive way, both for Arikan's standard polar codes and
for arbitrary polarizing kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4819</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4819</id><created>2012-09-21</created><updated>2013-06-26</updated><authors><author><keyname>Corrigan-Gibbs</keyname><forenames>Henry</forenames></author><author><keyname>Wolinsky</keyname><forenames>David Isaac</forenames></author><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Proactively Accountable Anonymous Messaging in Verdict</title><categories>cs.CR cs.NI</categories><comments>22 pages, 9 figures</comments><report-no>YALEU/DCS/TR1478</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The DC-nets approach to anonymity has long held attraction for its strength
against traffic analysis, but practical implementations remain vulnerable to
internal disruption or &quot;jamming&quot; attacks requiring time-consuming tracing
procedures to address. We present Verdict, the first practical anonymous group
communication system built using proactively verifiable DC-nets: participants
use public key cryptography to construct DC-net ciphertexts, and knowledge
proofs to detect and detect and exclude misbehavior before disruption. We
compare three alternative constructions for verifiable DC-nets, one using
bilinear maps and two based on simpler ElGamal encryption. While verifiable
DC-nets incurs higher computation overheads due to the public-key cryptography
involved, our experiments suggest Verdict is practical for anonymous group
messaging or microblogging applications, supporting groups of 100 clients at 1
second per round or 1000 clients at 10 seconds per round. Furthermore, we show
how existing symmetric-key DC-nets can &quot;fall back&quot; to a verifiable DC-net to
quickly identify mis- behavior improving previous detections schemes by two
orders of magnitude than previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4820</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4820</id><created>2012-09-21</created><authors><author><keyname>Andrychowicz</keyname><forenames>Marcin</forenames></author></authors><title>Efficient Refreshing Protocol for Leakage-Resilient Storage Based on the
  Inner-Product Extractor</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent trend in cryptography is to protect data and computation against
various side-channel attacks. Dziembowski and Faust (TCC 2012) have proposed a
general way to protect arbitrary circuits against any continual leakage
assuming that: (i) the memory is divided into the parts, which leaks
independently (ii) the leakage in each observation is bounded (iii) the circuit
has an access to a leak-free component, which samples random orthogonal
vectors. The pivotal element of their construction is a protocol for refreshing
the so-called Leakage-Resilient Storage (LRS).
  In this note, we present a more efficient and simpler protocol for refreshing
LRS under the same assumptions. Our solution needs O(n) operations to fully
refresh the secret (in comparison to {\Omega}(n^2) for a protocol of
Dziembowski and Faust), where n is a security parameter that describes the
maximal amount of leakage in each invocation of the refreshing procedure
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4825</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4825</id><created>2012-09-21</created><updated>2013-06-08</updated><authors><author><keyname>Pahikkala</keyname><forenames>Tapio</forenames></author><author><keyname>Airola</keyname><forenames>Antti</forenames></author><author><keyname>Stock</keyname><forenames>Michiel</forenames></author><author><keyname>De Baets</keyname><forenames>Bernard</forenames></author><author><keyname>Waegeman</keyname><forenames>Willem</forenames></author></authors><title>Efficient Regularized Least-Squares Algorithms for Conditional Ranking
  on Relational Data</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In domains like bioinformatics, information retrieval and social network
analysis, one can find learning tasks where the goal consists of inferring a
ranking of objects, conditioned on a particular target object. We present a
general kernel framework for learning conditional rankings from various types
of relational data, where rankings can be conditioned on unseen data objects.
We propose efficient algorithms for conditional ranking by optimizing squared
regression and ranking loss functions. We show theoretically, that learning
with the ranking loss is likely to generalize better than with the regression
loss. Further, we prove that symmetry or reciprocity properties of relations
can be efficiently enforced in the learned models. Experiments on synthetic and
real-world data illustrate that the proposed methods deliver state-of-the-art
performance in terms of predictive power and computational efficiency.
Moreover, we also show empirically that incorporating symmetry or reciprocity
properties can improve the generalization performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4829</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4829</id><created>2012-09-21</created><authors><author><keyname>Molloy</keyname><forenames>Michael</forenames></author><author><keyname>Restrepo</keyname><forenames>Ricardo</forenames></author></authors><title>Frozen variables in random boolean constraint satisfaction problems</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the exact freezing threshold, r^f, for a family of models of
random boolean constraint satisfaction problems, including NAE-SAT and
hypergraph 2-colouring, when the constraint size is sufficiently large. If the
constraint-density of a random CSP, F, in our family is greater than r^f then
for almost every solution of F, a linear number of variables are frozen,
meaning that their colours cannot be changed by a sequence of alterations in
which we change o(n) variables at a time, always switching to another solution.
If the constraint-density is less than r^f, then almost every solution has o(n)
frozen variables.
  Freezing is a key part of the clustering phenomenon that is hypothesized by
non-rigorous techniques from statistical physics. The understanding of
clustering has led to the development of advanced heuristics such as Survey
Propogation. It has been suggested that the freezing threshold is a precise
algorithmic barrier: that for densities below r^f the random CSPs can be solved
using very simple algorithms, while for densities above r^f one requires more
sophisticated techniques in order to deal with frozen clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4831</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4831</id><created>2012-09-21</created><updated>2012-12-18</updated><authors><author><keyname>Fern&#xe1;ndez-Gracia</keyname><forenames>J.</forenames></author><author><keyname>Castell&#xf3;</keyname><forenames>X.</forenames></author><author><keyname>Egu&#xed;luz</keyname><forenames>V. M.</forenames></author><author><keyname>Miguel</keyname><forenames>M. San</forenames></author></authors><title>Dynamics of link states in complex networks: The case of a majority rule</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 13 figures</comments><journal-ref>Phys. Rev. E 86, 066113 (2012)</journal-ref><doi>10.1103/PhysRevE.86.066113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the idea that some characteristics are specific to the relations
between individuals and not of the individuals themselves, we study a prototype
model for the dynamics of the states of the links in a fixed network of
interacting units. Each link in the network can be in one of two equivalent
states. A majority link-dynamics rule is implemented, so that in each dynamical
step the state of a randomly chosen link is updated to the state of the
majority of neighboring links. Nodes can be characterized by a link
heterogeneity index, giving a measure of the likelihood of a node to have a
link in one of the two states. We consider this link-dynamics model on fully
connected networks, square lattices and Erd \&quot;os-Renyi random networks. In each
case we find and characterize a number of nontrivial asymptotic configurations,
as well as some of the mechanisms leading to them and the time evolution of the
link heterogeneity index distribution. For a fully connected network and random
networks there is a broad distribution of possible asymptotic configurations.
Most asymptotic configurations that result from link-dynamics have no
counterpart under traditional node dynamics in the same topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4838</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4838</id><created>2012-09-21</created><authors><author><keyname>Dobrev</keyname><forenames>Dimiter</forenames></author></authors><title>Formal Definition of AI</title><categories>cs.AI</categories><journal-ref>International Journal &quot;Information Theories &amp; Applications&quot;,
  vol.12, Number 3, 2005, pp.277-285</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A definition of Artificial Intelligence was proposed in [1] but this
definition was not absolutely formal at least because the word &quot;Human&quot; was
used. In this paper we will formalize the definition from [1]. The biggest
problem in this definition was that the level of intelligence of AI is compared
to the intelligence of a human being. In order to change this we will introduce
some parameters to which AI will depend. One of this parameters will be the
level of intelligence and we will define one AI to each level of intelligence.
We assume that for some level of intelligence the respective AI will be more
intelligent than a human being. Nevertheless, we cannot say which is this level
because we cannot calculate its exact value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4850</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4850</id><created>2012-09-21</created><updated>2013-04-11</updated><authors><author><keyname>Boutin</keyname><forenames>Mireille</forenames></author><author><keyname>Huang</keyname><forenames>Shanshan</forenames></author></authors><title>The Pascal Triangle of a Discrete Image: Definition, Properties and
  Application to Shape Analysis</title><categories>math-ph cs.CV math.MP</categories><proxy>Sigma</proxy><journal-ref>SIGMA 9 (2013), 031, 25 pages</journal-ref><doi>10.3842/SIGMA.2013.031</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We define the Pascal triangle of a discrete (gray scale) image as a pyramidal
arrangement of complex-valued moments and we explore its geometric
significance. In particular, we show that the entries of row k of this triangle
correspond to the Fourier series coefficients of the moment of order k of the
Radon transform of the image. Group actions on the plane can be naturally
prolonged onto the entries of the Pascal triangle. We study the prolongation of
some common group actions, such as rotations and reflections, and we propose
simple tests for detecting equivalences and self-equivalences under these group
actions. The motivating application of this work is the problem of
characterizing the geometry of objects on images, for example by detecting
approximate symmetries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4854</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4854</id><created>2012-09-20</created><updated>2013-01-09</updated><authors><author><keyname>Szilv&#xe1;si-Nagy</keyname><forenames>M&#xe1;rta</forenames></author><author><keyname>M&#xe1;ty&#xe1;si</keyname><forenames>Gyula</forenames></author><author><keyname>B&#xe9;la</keyname><forenames>Szilvia</forenames></author></authors><title>Geometric simulation of locally optimal tool paths in three-axis milling</title><categories>cs.CG cs.CE math.NA</categories><comments>Submitted to Journal for Geometry and Graphics (2012)</comments><msc-class>65D18, 62P30</msc-class><acm-class>F.2.2; J.2</acm-class><journal-ref>Journal for Geometry and Graphics Vol. 17.(2) pp. 189-201. (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most important aim in tool path generation methods is to increase the
machining efficiency by minimizing the total length of tool paths while the
error is kept under a prescribed tolerance. This can be achieved by determining
the moving direction of the cutting tool such that the machined stripe is the
widest. From a technical point of view it is recommended that the angle between
the tool axis and the surface normal does not change too much along the tool
path in order to ensure even abrasion of the tool. In this paper a mathematical
method for tool path generation in 3-axis milling is presented, which considers
these requirements by combining the features of isophotic curves and principal
curvatures. It calculates the proposed moving direction of the tool at each
point of the surface. The proposed direction depends on the measurement of the
tool and on the curvature values of the surface. For triangulated surfaces a
new local offset computation method is presented, which is suitable also for
detecting tool collision with the target surface and self intersection in the
offset mesh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4855</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4855</id><created>2012-09-20</created><authors><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author><author><keyname>Ramakrishna</keyname><forenames>G.</forenames></author></authors><title>The Future of Neural Networks</title><categories>cs.NE</categories><comments>6 pages, 2 figures</comments><journal-ref>Published in proceedings of 6th National Conference on Computing
  for Nation Development, INDIACom 2012, New Delhi, India, 23-24 February,
  2012, pp. 481-486</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes some recent developments in neural networks and discusses
the applicability of neural networks in the development of a machine that
mimics the human brain. The paper mentions a new architecture, the pulsed
neural network that is being considered as the next generation of neural
networks. The paper also explores the use of memristors in the development of a
brain-like computer called the MoNETA. A new model, multi/infinite dimensional
neural networks, are a recent development in the area of advanced neural
networks. The paper concludes that the need of neural networks in the
development of human-like technology is essential and may be non-expendable for
it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4865</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4865</id><created>2012-09-21</created><authors><author><keyname>Capelli</keyname><forenames>Florent</forenames></author><author><keyname>Durand</keyname><forenames>Arnaud</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author></authors><title>The arithmetic complexity of tensor contractions</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the algebraic complexity of tensor calulus. We consider a
generalization of iterated matrix product to tensors and show that the
resulting formulas exactly capture VP, the class of polynomial families
efficiently computable by arithmetic circuits. This gives a natural and robust
characterization of this complexity class that despite its naturalness is not
very well understood so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4867</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4867</id><created>2012-09-21</created><authors><author><keyname>Akavipat</keyname><forenames>Ruj</forenames></author><author><keyname>Al-Ameen</keyname><forenames>Mahdi N.</forenames></author><author><keyname>Kapadia</keyname><forenames>Apu</forenames></author><author><keyname>Rahman</keyname><forenames>Zahid</forenames></author><author><keyname>Schlegel</keyname><forenames>Roman</forenames></author><author><keyname>Wright</keyname><forenames>Matthew</forenames></author></authors><title>ReDS: A Framework for Reputation-Enhanced DHTs</title><categories>cs.NI cs.CR</categories><journal-ref>IEEE TPDS Special Issue on Trust, Security, and Privacy, Feb.
  2014, Vol. 5, Issue 2</journal-ref><doi>10.1109/TPDS.2013.231</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Hash Tables (DHTs) such as Chord and Kademlia offer an efficient
solution for locating resources in peer-to-peer networks. Unfortunately,
malicious nodes along a lookup path can easily subvert such queries. Several
systems, including Halo (based on Chord) and Kad (based on Kademlia), mitigate
such attacks by using a combination of redundancy and diversity in the paths
taken by redundant lookup queries. Much greater assurance can be provided,
however. We describe Reputation for Directory Services (ReDS), a framework for
enhancing lookups in redundant DHTs by tracking how well other nodes service
lookup requests. We describe how the ReDS technique can be applied to virtually
any redundant DHT including Halo and Kad. We also study the collaborative
identification and removal of bad lookup paths in a way that does not rely on
the sharing of reputation scores --- we show that such sharing is vulnerable to
attacks that make it unsuitable for most applications of ReDS. Through
extensive simulations we demonstrate that ReDS improves lookup success rates
for Halo and Kad by 80% or more over a wide range of conditions, even against
strategic attackers attempting to game their reputation scores and in the
presence of node churn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4887</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4887</id><created>2012-09-21</created><authors><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Katselis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author></authors><title>A Note on the SPICE Method</title><categories>stat.ML cs.SY</categories><comments>5 pages, 1 figure. Submitted to the IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2013.2272291</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we analyze the SPICE method developed in [1], and establish
its connections with other standard sparse estimation methods such as the Lasso
and the LAD-Lasso. This result positions SPICE as a computationally efficient
technique for the calculation of Lasso-type estimators. Conversely, this
connection is very useful for establishing the asymptotic properties of SPICE
under several problem scenarios and for suggesting suitable modifications in
cases where the naive version of SPICE would not work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4889</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4889</id><created>2012-09-21</created><authors><author><keyname>Wu</keyname><forenames>Xiugang</forenames></author><author><keyname>Xie</keyname><forenames>Liang-Liang</forenames></author></authors><title>A Unified Relay Framework with both D-F and C-F Relay Nodes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decode-and-forward (D-F) and compress-and-forward (C-F) are two fundamentally
different relay strategies proposed by (Cover and El Gamal, 1979).
Individually, either of them has been successfully generalized to multi-relay
channels. In this paper, to allow each relay node the freedom of choosing
either of the two strategies, we propose a unified framework, where both the
D-F and C-F strategies can be employed simultaneously in the network. It turns
out that, to fully incorporate the advantages of both the best known D-F and
C-F strategies into a unified framework, the major challenge arises as follows:
For the D-F relay nodes to fully utilize the help of the C-F relay nodes,
decoding at the D-F relay nodes should not be conducted until all the blocks
have been finished; However, in the multi-level D-F strategy, the upstream
nodes have to decode prior to the downstream nodes in order to help, which
makes simultaneous decoding at all the D-F relay nodes after all the blocks
have been finished inapplicable. To tackle this problem, nested blocks combined
with backward decoding are used in our framework, so that the D-F relay nodes
at different levels can perform backward decoding at different frequencies. As
such, the upstream D-F relay nodes can decode before the downstream D-F relay
nodes, and the use of backward decoding at each D-F relay node ensures the full
exploitation of the help of both the other D-F relay nodes and the C-F relay
nodes. The achievable rates under our unified relay framework are found to
combine both the best known D-F and C-F achievable rates and include them as
special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4893</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4893</id><created>2012-09-21</created><updated>2012-10-10</updated><authors><author><keyname>Varadarajan</keyname><forenames>Kasturi</forenames></author><author><keyname>Xiao</keyname><forenames>Xin</forenames></author></authors><title>On the Sensitivity of Shape Fitting Problems</title><categories>cs.CG cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this article, we study shape fitting problems, $\epsilon$-coresets, and
total sensitivity. We focus on the $(j,k)$-projective clustering problems,
including $k$-median/$k$-means, $k$-line clustering, $j$-subspace
approximation, and the integer $(j,k)$-projective clustering problem. We derive
upper bounds of total sensitivities for these problems, and obtain
$\epsilon$-coresets using these upper bounds. Using a dimension-reduction type
argument, we are able to greatly simplify earlier results on total sensitivity
for the $k$-median/$k$-means clustering problems, and obtain
positively-weighted $\epsilon$-coresets for several variants of the
$(j,k)$-projective clustering problem. We also extend an earlier result on
$\epsilon$-coresets for the integer $(j,k)$-projective clustering problem in
fixed dimension to the case of high dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4894</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4894</id><created>2012-09-17</created><authors><author><keyname>Maan</keyname><forenames>Jitendra</forenames></author></authors><title>A Connected Enterprise - Transformation through Mobility and Social
  Networks</title><categories>cs.OH</categories><comments>8 pages, 3 figures</comments><doi>10.5121/ijmit.2012.4308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to rapid changes in business dynamics, there is a growing demand to
encourage social conversations/exchanges and the ability to connect and
communicate with peers, partners, customers and other stakeholders anytime,
anywhere which drives the need of mobile-enable, the existing enterprise
applications. This paper highlights a distinct set of needs and key customer
challenges that must be considered and addressed for deployment of Social
Collaboration applications and Mobility services in enterprises. It not only
addresses the Critical Success Factors for enterprise mobility enablement but
also outlines the unique business requirements to rapidly create social
collaboration culture and the discipline of turning social data into meaningful
insights to drive business decisions in real-time. Moreover, the paper
emphasizes on developing composite offerings on social enterprise and Mobile
networks that not only offer the value proposition in terms of financially
oriented results, but also help customer to maximize return on investment
(ROI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4895</identifier>
 <datestamp>2012-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4895</id><created>2012-09-20</created><authors><author><keyname>Lakra</keyname><forenames>Sachin</forenames></author><author><keyname>Prasad</keyname><forenames>T. V.</forenames></author><author><keyname>Sharma</keyname><forenames>Deepak</forenames></author><author><keyname>Atrey</keyname><forenames>Shree Harsh</forenames></author><author><keyname>Sharma</keyname><forenames>Anubhav</forenames></author></authors><title>A Neuro-Fuzzy Technique for Implementing the Half-Adder Circuit Using
  the CANFIS Model</title><categories>cs.NE</categories><comments>7 pages, 9 figures, 7 tables</comments><journal-ref>Published in proceedings titled Advances in Computer Science and
  Engineering - of the Second International Conference on Data Management 2009,
  Ghaziabad, Uttar Pradesh, India, Feb 2009, pp. 99-107</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Neural Network, in general, is not considered to be a good solver of
mathematical and binary arithmetic problems. However, networks have been
developed for such problems as the XOR circuit. This paper presents a technique
for the implementation of the Half-adder circuit using the CoActive Neuro-Fuzzy
Inference System (CANFIS) Model and attempts to solve the problem using the
NeuroSolutions 5 Simulator. The paper gives the experimental results along with
the interpretations and possible applications of the technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4896</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4896</id><created>2012-09-19</created><updated>2012-09-25</updated><authors><author><keyname>Alkutkar</keyname><forenames>Harsh</forenames></author><author><keyname>Abhyankar</keyname><forenames>Ajinkya</forenames></author><author><keyname>Gawali</keyname><forenames>Rushikesh</forenames></author><author><keyname>Gandhele</keyname><forenames>Saurabh</forenames></author></authors><title>Using Microsoft PowerPoint Presentations To Create HTML5 Based
  E-learning Courses</title><categories>cs.CY</categories><comments>Paper has been withdrawn due to inconsistencies in images</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper introduces how to use Microsoft PowerPoint presentation files to
generate HTML5 based interactive e-learning courses packaged in SCORM format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4897</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4897</id><created>2012-09-21</created><authors><author><keyname>Tanizawa</keyname><forenames>Toshihiro</forenames></author></authors><title>Structural robustness and transport efficiency of complex networks with
  degree correlation</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine two properties of complex networks, the robustness against
targeted node removal (attack) and the transport efficiency in terms of degree
correlation in node connection by numerical evaluation of exact analytic
expressions. We find that, while the assortative correlation enhances the
structural robustness against attack, the disassortative correlation
significantly improves the transport efficiency of the network under
consideration. This finding might shed light on the reason why some networks in
the real world prefer assortative correlation and others prefer disassortative
one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4922</identifier>
 <datestamp>2013-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4922</id><created>2012-09-21</created><authors><author><keyname>Alamir</keyname><forenames>Mazen</forenames></author></authors><title>Monitoring Control Updating Period In Fast Gradient Based NMPC</title><categories>cs.SY cs.SE</categories><comments>6 pages, 8 Figures</comments><journal-ref>Proceedings of the European Control Conference, Zurich, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a method is proposed for on-line monitoring of the control
updating period in fast-gradient-based Model Predictive Control (MPC) schemes.
Such schemes are currently under intense investigation as a way to accommodate
for real-time requirements when dealing with systems showing fast dynamics. The
method needs cheap computations that use the algorithm on-line behavior in
order to recover the optimal updating period in terms of cost function
decrease. A simple example of constrained triple integrator is used to
illustrate the proposed method and to assess its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4927</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4927</id><created>2012-09-21</created><updated>2013-04-15</updated><authors><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author></authors><title>History-Preserving Bisimilarity for Higher-Dimensional Automata via Open
  Maps</title><categories>cs.LO math.CT</categories><comments>Minor updates in accordance with reviewer comments. Submitted to MFPS
  2013</comments><acm-class>F.1.1; F.1.2; F.3.2; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that history-preserving bisimilarity for higher-dimensional automata
has a simple characterization directly in terms of higher-dimensional
transitions. This implies that it is decidable for finite higher-dimensional
automata. To arrive at our characterization, we apply the open-maps framework
of Joyal, Nielsen and Winskel in the category of unfoldings of precubical sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4935</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4935</id><created>2012-09-21</created><authors><author><keyname>Wright</keyname><forenames>Melvyn</forenames></author></authors><title>Adaptive Real Time Imaging Synthesis Telescopes</title><categories>astro-ph.IM cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The digital revolution is transforming astronomy from a data-starved to a
data-submerged science. Instruments such as the Atacama Large Millimeter Array
(ALMA), the Large Synoptic Survey Telescope (LSST), and the Square Kilometer
Array (SKA) will measure their accumulated data in petabytes. The capacity to
produce enormous volumes of data must be matched with the computing power to
process that data and produce meaningful results. In addition to handling huge
data rates, we need adaptive calibration and beamforming to handle atmospheric
fluctuations and radio frequency interference, and to provide a user
environment which makes the full power of large telescope arrays accessible to
both expert and non-expert users. Delayed calibration and analysis limit the
science which can be done. To make the best use of both telescope and human
resources we must reduce the burden of data reduction.
  Our instrumentation comprises of a flexible correlator, beam former and
imager with digital signal processing closely coupled with a computing cluster.
This instrumentation will be highly accessible to scientists, engineers, and
students for research and development of real-time processing algorithms, and
will tap into the pool of talented and innovative students and visiting
scientists from engineering, computing, and astronomy backgrounds.
  Adaptive real-time imaging will transform radio astronomy by providing
real-time feedback to observers. Calibration of the data is made in close to
real time using a model of the sky brightness distribution. The derived
calibration parameters are fed back into the imagers and beam formers. The
regions imaged are used to update and improve the a-priori model, which becomes
the final calibrated image by the time the observations are complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4936</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4936</id><created>2012-09-21</created><updated>2012-12-08</updated><authors><author><keyname>Mertens</keyname><forenames>Stephan</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Continuum Percolation Thresholds in Two Dimensions</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CG</categories><comments>6 pages, 8 figures</comments><journal-ref>Phys. Rev. E 86, 061109 (2012)</journal-ref><doi>10.1103/PhysRevE.86.061109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide variety of methods have been used to compute percolation thresholds.
In lattice percolation, the most powerful of these methods consists of
microcanonical simulations using the union-find algorithm to efficiently
determine the connected clusters, and (in two dimensions) using exact values
from conformal field theory for the probability, at the phase transition, that
various kinds of wrapping clusters exist on the torus. We apply this approach
to percolation in continuum models, finding overlaps between objects with
real-valued positions and orientations. In particular, we find precise values
of the percolation transition for disks, squares, rotated squares, and rotated
sticks in two dimensions, and confirm that these transitions behave as
conformal field theory predicts. The running time and memory use of our
algorithm are essentially linear as a function of the number of objects at
criticality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4950</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4950</id><created>2012-09-21</created><authors><author><keyname>Sun</keyname><forenames>Xiaoling</forenames></author><author><keyname>Kaur</keyname><forenames>Jasleen</forenames></author><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Social Dynamics of Science</title><categories>physics.soc-ph cs.DL cs.SI</categories><journal-ref>Sci. Rep. 3:1069, 2013</journal-ref><doi>10.1038/srep01069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The birth and decline of disciplines are critical to science and society.
However, no quantitative model to date allows us to validate competing theories
of whether the emergence of scientific disciplines drives or follows the
formation of social communities of scholars. Here we propose an agent-based
model based on a \emph{social dynamics of science,} in which the evolution of
disciplines is guided mainly by the social interactions among scientists. We
find that such a social theory can account for a number of stylized facts about
the relationships between disciplines, authors, and publications. These results
provide strong quantitative support for the key role of social interactions in
shaping the dynamics of science. A &quot;science of science&quot; must gauge the role of
exogenous events, such as scientific discoveries and technological advances,
against this purely social baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4951</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4951</id><created>2012-09-21</created><updated>2013-08-01</updated><authors><author><keyname>Xu</keyname><forenames>Tu</forenames></author><author><keyname>Wang</keyname><forenames>Junhui</forenames></author></authors><title>An efficient model-free estimation of multiclass conditional probability</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional multiclass conditional probability estimation methods, such as
Fisher's discriminate analysis and logistic regression, often require
restrictive distributional model assumption. In this paper, a model-free
estimation method is proposed to estimate multiclass conditional probability
through a series of conditional quantile regression functions. Specifically,
the conditional class probability is formulated as difference of corresponding
cumulative distribution functions, where the cumulative distribution functions
can be converted from the estimated conditional quantile regression functions.
The proposed estimation method is also efficient as its computation cost does
not increase exponentially with the number of classes. The theoretical and
numerical studies demonstrate that the proposed estimation method is highly
competitive against the existing competitors, especially when the number of
classes is relatively large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4965</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4965</id><created>2012-09-22</created><updated>2013-03-16</updated><authors><author><keyname>Li</keyname><forenames>Yuan</forenames></author></authors><title>Structure theorem of square complex orthogonal design</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Square COD (complex orthogonal design) with size $[n, n, k]$ is an $n \times
n$ matrix $\mathcal{O}_z$, where each entry is a complex linear combination of
$z_i$ and their conjugations $z_i^*$, $i=1,\ldots, k$, such that
$\mathcal{O}_z^H \mathcal{O}_z = (|z_1|^2 + \ldots + |z_k|^2)I_n$. Closely
following the work of Hottinen and Tirkkonen, which proved an upper bound of
$k/n$ by making a crucial observation between square COD and group
representation, we prove the structure theorem of square COD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4968</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4968</id><created>2012-09-22</created><authors><author><keyname>Paikaray</keyname><forenames>Bijay</forenames></author></authors><title>Vertical Handoff Decision Based On Genetic Algorithm in 4G Network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid improvement of the mobile generations was for the purpose of
supporting as many mobile devices as possible that could benefit the users at
anytime and anywhere in terms of common practical applications such as internet
access, video-on-demand, video conferencing system and many more applications.
In this paper, a review for the mobile generations in the wireless
communications is pre-sented in order to highlight and compare the issues and
challenges that are involved in each generation starting from the earlier
generations along to the following generations and finally till the 4th
Generation (4G). The 4G wireless network is intended to complement and replace
the current generations. Accessing information anywhere, anytime, with a
seamless connection to a wide range of information and services, and receiving
a large volume of information, data, pictures, video, and so on, are the keys
features of 4G. Based on the developing trends of mobile communication, 4G will
have broader bandwidth, higher data rate, and smoother and quicker handoff to
provide seamless service across a multitude of wireless systems and networks.
One of the major issues of seamless mobility is handoff management. It is a
major challenge to design intelligent handoff management schemes for
4G-systems. In this paper we have presented the design of an adaptive
multi-attribute vertical handoff decision algorithm based on genetic algorithm
which is both cost effective and useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4970</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4970</id><created>2012-09-22</created><updated>2013-01-24</updated><authors><author><keyname>Mauroy</keyname><forenames>Alexandre</forenames></author><author><keyname>Sacr&#xe9;</keyname><forenames>Pierre</forenames></author><author><keyname>Sepulchre</keyname><forenames>Rodolphe</forenames></author></authors><title>Kick synchronization versus diffusive synchronization</title><categories>cs.SY math.DS nlin.AO</categories><comments>Correction of a mistake in Definition 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper provides an introductory discussion about two fundamental models of
oscillator synchronization: the (continuous-time) diffusive model, that
dominates the mathematical literature on synchronization, and the (hybrid) kick
model, that accounts for most popular examples of synchronization, but for
which only few theoretical results exist. The paper stresses fundamental
differences between the two models, such as the different contraction measures
underlying the analysis, as well as important analogies that can be drawn in
the limit of weak coupling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4971</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4971</id><created>2012-09-22</created><authors><author><keyname>Fran&#xe7;ois</keyname><forenames>Nathana&#xeb;l</forenames></author><author><keyname>Magniez</keyname><forenames>Frederic</forenames></author></authors><title>Streaming Complexity of Checking Priority Queues</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is in the line of designing efficient checkers for testing the
reliability of some massive data structures. Given a sequential access to the
insert/extract operations on such a structure, one would like to decide, a
posteriori only, if it corresponds to the evolution of a reliable structure. In
a context of massive data, one would like to minimize both the amount of
reliable memory of the checker and the number of passes on the sequence of
operations. Chu, Kannan and McGregor initiated the study of checking priority
queues in this setting. They showed that use of timestamps allows to check a
priority queue with a single pass and memory space O(N^(1/2)), up to a
polylogarithmic factor. Later, Chakrabarti, Cormode, Kondapally and McGregor
removed the use of timestamps, and proved that more passes do not help. We show
that, even in the presence of timestamps, more passes do not help, solving a
previously open problem. On the other hand, we show that a second pass, but in
reverse direction, shrinks the memory space to O((log N)^2), extending a
phenomenon the first time observed by Magniez, Mathieu and Nayak for checking
well-parenthesized expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4975</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4975</id><created>2012-09-22</created><authors><author><keyname>Liu</keyname><forenames>Yanfang</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Parametric matroid of rough set</title><categories>cs.AI cs.DM</categories><comments>15 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rough set is mainly concerned with the approximations of objects through an
equivalence relation on a universe. Matroid is a combinatorial generalization
of linear independence in vector spaces. In this paper, we define a parametric
set family, with any subset of a universe as its parameter, to connect rough
sets and matroids. On the one hand, for a universe and an equivalence relation
on the universe, a parametric set family is defined through the lower
approximation operator. This parametric set family is proved to satisfy the
independent set axiom of matroids, therefore it can generate a matroid, called
a parametric matroid of the rough set. Three equivalent representations of the
parametric set family are obtained. Moreover, the parametric matroid of the
rough set is proved to be the direct sum of a partition-circuit matroid and a
free matroid. On the other hand, since partition-circuit matroids were well
studied through the lower approximation number, we use it to investigate the
parametric matroid of the rough set. Several characteristics of the parametric
matroid of the rough set, such as independent sets, bases, circuits, the rank
function and the closure operator, are expressed by the lower approximation
number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4976</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4976</id><created>2012-09-22</created><updated>2012-11-29</updated><authors><author><keyname>Liu</keyname><forenames>Yanfang</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Matroidal structure of rough sets based on serial and transitive
  relations</title><categories>cs.AI</categories><comments>16 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of rough sets is concerned with the lower and upper approximations
of objects through a binary relation on a universe. It has been applied to
machine learning, knowledge discovery and data mining. The theory of matroids
is a generalization of linear independence in vector spaces. It has been used
in combinatorial optimization and algorithm design. In order to take advantages
of both rough sets and matroids, in this paper we propose a matroidal structure
of rough sets based on a serial and transitive relation on a universe. We
define the family of all minimal neighborhoods of a relation on a universe, and
prove it satisfy the circuit axioms of matroids when the relation is serial and
transitive. In order to further study this matroidal structure, we investigate
the inverse of this construction: inducing a relation by a matroid. The
relationships between the upper approximation operators of rough sets based on
relations and the closure operators of matroids in the above two constructions
are studied. Moreover, we investigate the connections between the above two
constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4978</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4978</id><created>2012-09-22</created><updated>2012-11-29</updated><authors><author><keyname>Liu</keyname><forenames>Yanfang</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Covering matroid</title><categories>cs.AI</categories><comments>15 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new type of matroids, namely covering matroids,
and investigate the connections with the second type of covering-based rough
sets and some existing special matroids. Firstly, as an extension of
partitions, coverings are more natural combinatorial objects and can sometimes
be more efficient to deal with problems in the real world. Through extending
partitions to coverings, we propose a new type of matroids called covering
matroids and prove them to be an extension of partition matroids. Secondly,
since some researchers have successfully applied partition matroids to
classical rough sets, we study the relationships between covering matroids and
covering-based rough sets which are an extension of classical rough sets.
Thirdly, in matroid theory, there are many special matroids, such as
transversal matroids, partition matroids, 2-circuit matroid and
partition-circuit matroids. The relationships among several special matroids
and covering matroids are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4982</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4982</id><created>2012-09-22</created><authors><author><keyname>Steiner</keyname><forenames>Ingmar</forenames><affiliation>INRIA Lorraine - LORIA, Trinity College Dublin</affiliation></author><author><keyname>Richmond</keyname><forenames>Korin</forenames><affiliation>CSTR</affiliation></author><author><keyname>Ouni</keyname><forenames>Slim</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Using multimodal speech production data to evaluate articulatory
  animation for audiovisual speech synthesis</title><categories>cs.HC cs.GR</categories><proxy>ccsd</proxy><journal-ref>3rd International Symposium on Facial Analysis and Animation
  (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of modeling speech articulation for high-quality audiovisual
(AV) speech synthesis is widely acknowledged. Nevertheless, while
state-of-the-art, data-driven approaches to facial animation can make use of
sophisticated motion capture techniques, the animation of the intraoral
articulators (viz. the tongue, jaw, and velum) typically makes use of simple
rules or viseme morphing, in stark contrast to the otherwise high quality of
facial modeling. Using appropriate speech production data could significantly
improve the quality of articulatory animation for AV synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4992</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4992</id><created>2012-09-22</created><authors><author><keyname>Chandrashekar</keyname><forenames>Praveen</forenames></author></authors><title>Discontinuous Galerkin method for Navier-Stokes equations using kinetic
  flux vector splitting</title><categories>cs.NA cs.CE math.NA</categories><doi>10.1016/j.jcp.2012.09.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kinetic schemes for compressible flow of gases are constructed by exploiting
the connection between Boltzmann equation and the Navier-Stokes equations. This
connection allows us to construct a flux splitting for the Navier-Stokes
equations based on the direction of molecular motion from which a numerical
flux can be obtained. The naive use of such a numerical flux function in a
discontinuous Galerkin (DG) discretization leads to an unstable scheme in the
viscous dominated case. Stable schemes are constructed by adding additional
terms either in a symmetric or non-symmetric manner which are motivated by the
DG schemes for elliptic equations. The novelty of the present scheme is the use
of kinetic fluxes to construct the stabilization terms. In the symmetric case,
interior penalty terms have to be added for stability and the resulting schemes
give optimal convergence rates in numerical experiments. The non-symmetric
schemes lead to a cell energy/entropy inequality but exhibit sub-optimal
convergence rates. These properties are studied by applying the schemes to a
scalar convection-diffusion equation and the 1-D compressible Navier-Stokes
equations. In the case of Navier-Stokes equations, entropy variables are used
to construct stable schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4994</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4994</id><created>2012-09-22</created><authors><author><keyname>Chandrashekar</keyname><forenames>Praveen</forenames></author></authors><title>Kinetic energy preserving and entropy stable finite volume schemes for
  compressible Euler and Navier-Stokes equations</title><categories>cs.NA cs.CE math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centered numerical fluxes can be constructed for compressible Euler equations
which preserve kinetic energy in the semi-discrete finite volume scheme. The
essential feature is that the momentum flux should be of the form $f^m_\jph =
\tp_\jph + \avg{u}_\jph f^\rho_\jph$ where $\avg{u}_\jph = (u_j + u_{j+1})/2$
and $\tp_\jph, f^\rho_\jph$ are {\em any} consistent approximations to the
pressure and the mass flux. This scheme thus leaves most terms in the numerical
flux unspecified and various authors have used simple averaging. Here we
enforce approximate or exact entropy consistency which leads to a unique choice
of all the terms in the numerical fluxes. As a consequence novel entropy
conservative flux that also preserves kinetic energy for the semi-discrete
finite volume scheme has been proposed. These fluxes are centered and some
dissipation has to be added if shocks are present or if the mesh is coarse. We
construct scalar artificial dissipation terms which are kinetic energy stable
and satisfy approximate/exact entropy condition. Secondly, we use entropy-
variable based matrix dissipation flux which leads to kinetic energy and
entropy stable schemes. These schemes are shown to be free of entropy violating
solutions unlike the original Roe scheme. For hypersonic flows a blended scheme
is proposed which gives carbuncle free solutions for blunt body flows.
Numerical results for Euler and Navier-Stokes equations are presented to
demonstrate the performance of the different schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.4997</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.4997</id><created>2012-09-22</created><updated>2012-11-08</updated><authors><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author></authors><title>In science &quot;there is no bad publicity&quot;: Papers criticized in comments
  have high scientific impact</title><categories>physics.soc-ph cs.DL</categories><comments>5 pages, 4 figures + supplementary information</comments><journal-ref>Sci. Rep. 2, 815 (2012)</journal-ref><doi>10.1038/srep00815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comments are special types of publications whose aim is to correct or
criticize previously published papers. For this reason, comments are believed
to make commented papers less worthy or trusty to the eyes of the scientific
community, and thus predestined to have low scientific impact. Here, we show
that such belief is not supported by empirical evidence. We consider thirteen
major publication outlets in science, and perform systematic comparisons
between the citations accumulated by commented and non commented articles. We
find that (i) commented papers are, on average, much more cited than non
commented papers, and (ii) commented papers are more likely to be among the
most cited papers of a journal. Since comments are published soon after
criticized papers, comments should be viewed as early indicators of the future
impact of criticized papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5004</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5004</id><created>2012-09-22</created><updated>2014-02-01</updated><authors><author><keyname>Joe-Wong</keyname><forenames>Carlee</forenames></author><author><keyname>Sen</keyname><forenames>Soumya</forenames></author><author><keyname>Ha</keyname><forenames>Sangtae</forenames></author></authors><title>Offering Supplementary Network Technologies: Adoption Behavior and
  Offloading Benefits</title><categories>cs.NI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To alleviate the congestion caused by rapid growth in demand for mobile data,
wireless service providers (WSPs) have begun encouraging users to offload some
of their traffic onto supplementary network technologies, e.g., offloading from
3G or 4G to WiFi or femtocells. With the growing popularity of such offerings,
a deeper understanding of the underlying economic principles and their impact
on technology adoption is necessary. To this end, we develop a model for user
adoption of a base technology (e.g., 3G) and a bundle of the base plus a
supplementary technology (e.g., 3G + WiFi). Users individually make their
adoption decisions based on several factors, including the technologies'
intrinsic qualities, negative congestion externalities from other subscribers,
and the flat access rates that a WSP charges. We then show how these user-level
decisions translate into aggregate adoption dynamics and prove that these
converge to a unique equilibrium for a given set of exogenously determined
system parameters. We fully characterize these equilibria and study adoption
behaviors of interest to a WSP. We then derive analytical expressions for the
revenue-maximizing prices and optimal coverage factor for the supplementary
technology and examine some resulting non-intuitive user adoption behaviors.
Finally, we develop a mobile app to collect empirical 3G/WiFi usage data and
numerically investigate the profit-maximizing adoption levels when a WSP
accounts for its cost of deploying the supplemental technology and savings from
offloading traffic onto this technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5013</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5013</id><created>2012-09-22</created><updated>2012-10-18</updated><authors><author><keyname>Cremene</keyname><forenames>Ligia</forenames></author><author><keyname>Dumitrescu</keyname><forenames>Dumitru</forenames></author></authors><title>Analysis of Cognitive Radio Scenes Based on Non-cooperative Game
  Theoretical Modelling</title><categories>nlin.AO cs.ET cs.GT cs.NI</categories><comments>8 double-column pages, 10 figures. arXiv admin note: text overlap
  with arXiv:1209.5387</comments><doi>10.1049/iet-com.2011.0712</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A noncooperative game theoretical approach for analysing opportunistic
spectrum access (OSA) in cognitive radio (CR) environments is proposed. New
concepts from game theory are applied to spectrum access analysis in order to
extract rules of behaviour for an emerging environment. In order to assess OSA
scenarios of CRs, two oligopoly game models are reformulated in terms of
resource access: Cournot and Stackelberg games. Five CR scenes are analysed:
simultaneous access of unlicensed users (commons regime) with symmetric and
asymmetric costs, with and without bandwidth constraints and sequential access
(licensed against unlicensed). Several equilibrium concepts are studied as game
solutions: Nash, Pareto and the joint NashPareto equilibrium. The latter
captures a game situation where players are non-homogeneous users, exhibiting
different types of rationality, Nash and Pareto. This enables a more realistic
modelling of interactions on a CR scene. An evolutionary game equilibrium
detection method is used. The Nash equilibrium indicates the maximum number of
channels a CR may access without decreasing its payoff. The Pareto equilibrium
describes a larger range of payoffs, capturing unbalanced as well as equitable
solutions. The analysis of the Stackelberg modelling shows that payoffs are
maximised for all users if the incumbents are Nash oriented and the new
entrants are Pareto driven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5019</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5019</id><created>2012-09-22</created><authors><author><keyname>Polatkan</keyname><forenames>Gungor</forenames></author><author><keyname>Zhou</keyname><forenames>Mingyuan</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author><author><keyname>Blei</keyname><forenames>David</forenames></author><author><keyname>Daubechies</keyname><forenames>Ingrid</forenames></author></authors><title>A Bayesian Nonparametric Approach to Image Super-resolution</title><categories>cs.LG stat.ML</categories><comments>30 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Super-resolution methods form high-resolution images from low-resolution
images. In this paper, we develop a new Bayesian nonparametric model for
super-resolution. Our method uses a beta-Bernoulli process to learn a set of
recurring visual patterns, called dictionary elements, from the data. Because
it is nonparametric, the number of elements found is also determined from the
data. We test the results on both benchmark and natural images, comparing with
several other models from the research literature. We perform large-scale human
evaluation experiments to assess the visual quality of the results. In a first
implementation, we use Gibbs sampling to approximate the posterior. However,
this algorithm is not feasible for large-scale data. To circumvent this, we
then develop an online variational Bayes (VB) algorithm. This algorithm finds
high quality dictionaries in a fraction of the time needed by the Gibbs
sampler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5025</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5025</id><created>2012-09-22</created><updated>2013-02-27</updated><authors><author><keyname>Abdullah</keyname><forenames>Mohammed Amin</forenames></author><author><keyname>Draief</keyname><forenames>Moez</forenames></author></authors><title>Consensus on the Initial Global Majority by Local Majority Polling for a
  Class of Sparse Graphs</title><categories>cs.DC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the local majority protocol on simple graphs of a given degree
sequence, for a certain class of degree sequences. We show that for almost all
such graphs, subject to a sufficiently large bias, within time $A \log_d \log_d
n$ the local majority protocol achieves consensus on the initial global
majority with probability $1-n^{-\Omega((\log n)^{\varepsilon})}$, where
$\varepsilon&gt;0$ is a constant. $A$ is bounded by a universal constant and $d$
is a parameter of the graph; the smallest integer which is the degree of
$\Theta(n)$ vertices in the graph. We further show that under the assumption
that a vertex $v$ does not change its colour if it and all of its neighbours
are the same colour, \emph{any} local protocol $\mathcal{P}$ takes time at
least $(1-o(1))\log_d\log_d n$, with probability $1-e^{-\Omega(n^{1-o(1)})}$ on
such graphs. We further show that for almost all $d$-regular simple graphs with
$d$ constant, we can get a stronger probability to convergence of initial
majority at the expense of time. Specifically, with probability
$1-O\brac{c^{-n^{\varepsilon}}}$, the local majority protocol achieves
consensus on the initial majority by time $O(\log n)$. Finally, we show how the
technique for the above sparse graphs can be applied in a straightforward
manner to get bounds for the Erd\H{o}s--Renyi random graphs in the connected
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5036</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5036</id><created>2012-09-23</created><updated>2015-10-07</updated><authors><author><keyname>Casares</keyname><forenames>Ram&#xf3;n</forenames></author></authors><title>Proof of Church's Thesis</title><categories>cs.LO</categories><comments>6 pages. Added &quot;Epilogue on Post (1936) Program&quot;</comments><msc-class>68Q05, 03D10</msc-class><acm-class>F.1.1; F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We prove that if our calculating capability is limited to that of a universal
Turing machine with a finite tape, then Church's thesis is true. This way we
accomplish Post (1936) program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5037</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5037</id><created>2012-09-23</created><authors><author><keyname>Chen</keyname><forenames>Junting</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Delay Analysis of Max-Weight Queue Algorithm for Time-varying Wireless
  Adhoc Networks - Control Theoretical Approach</title><categories>cs.SY cs.IT math.IT</categories><comments>This paper has been accepted by the IEEE Transactions on Signal
  Processing</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 61, no. 1, pp.
  99-108, 2013</journal-ref><doi>10.1109/TSP.2012.2222380</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max weighted queue (MWQ) control policy is a widely used cross-layer control
policy that achieves queue stability and a reasonable delay performance. In
most of the existing literature, it is assumed that optimal MWQ policy can be
obtained instantaneously at every time slot. However, this assumption may be
unrealistic in time varying wireless systems, especially when there is no
closed-form MWQ solution and iterative algorithms have to be applied to obtain
the optimal solution. This paper investigates the convergence behavior and the
queue delay performance of the conventional MWQ iterations in which the channel
state information (CSI) and queue state information (QSI) are changing in a
similar timescale as the algorithm iterations. Our results are established by
studying the stochastic stability of an equivalent virtual stochastic dynamic
system (VSDS), and an extended Foster-Lyapunov criteria is applied for the
stability analysis. We derive a closed form delay bound of the wireless network
in terms of the CSI fading rate and the sensitivity of MWQ policy over CSI and
QSI. Based on the equivalent VSDS, we propose a novel MWQ iterative algorithm
with compensation to improve the tracking performance. We demonstrate that
under some mild conditions, the proposed modified MWQ algorithm converges to
the optimal MWQ control despite the time-varying CSI and QSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5038</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5038</id><created>2012-09-23</created><authors><author><keyname>Gordon</keyname><forenames>Daniel</forenames></author><author><keyname>Hendler</keyname><forenames>Danny</forenames></author><author><keyname>Rokach</keyname><forenames>Lior</forenames></author></authors><title>Fast Randomized Model Generation for Shapelet-Based Time Series
  Classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series classification is a field which has drawn much attention over the
past decade. A new approach for classification of time series uses
classification trees based on shapelets. A shapelet is a subsequence extracted
from one of the time series in the dataset. A disadvantage of this approach is
the time required for building the shapelet-based classification tree. The
search for the best shapelet requires examining all subsequences of all lengths
from all time series in the training set.
  A key goal of this work was to find an evaluation order of the shapelets
space which enables fast convergence to an accurate model. The comparative
analysis we conducted clearly indicates that a random evaluation order yields
the best results. Our empirical analysis of the distribution of high-quality
shapelets within the shapelets space provides insights into why randomized
shapelets sampling is superior to alternative evaluation orders.
  We present an algorithm for randomized model generation for shapelet-based
classification that converges extremely quickly to a model with surprisingly
high accuracy after evaluating only an exceedingly small fraction of the
shapelets space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5039</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5039</id><created>2012-09-23</created><authors><author><keyname>Dilawari</keyname><forenames>Jaswinder Singh</forenames></author><author><keyname>Khanna</keyname><forenames>Ravinder</forenames></author></authors><title>Creation of Digital Test Form for Prepress Department</title><categories>cs.CV</categories><comments>5 Pages,4 Figures</comments><journal-ref>(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 10, No. 9, September 2012 (IJCSIS) International Journal of
  Computer Science and Information Security, Vol. 10, No. 9, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main problem in colour management in prepress department is lack of
availability of literature on colour management and knowledge gap between
prepress department and press department. So a digital test from has been
created by Adobe Photoshop to analyse the ICC profile and to create a new
profile and this analysed data is used to study about various grey scale of RGB
and CMYK images. That helps in conversion of image from RGB to CMYK in prepress
department.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5040</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5040</id><created>2012-09-23</created><authors><author><keyname>Dilawari</keyname><forenames>Jaswinder Singh</forenames></author><author><keyname>Khanna</keyname><forenames>Ravinder</forenames></author></authors><title>Image Classification and Optimized Image Reproduction</title><categories>cs.CV</categories><comments>5 Pages, 9 Figures</comments><journal-ref>International Journal of Computer Science Issues,volume 9,Issue 5
  ,Septmeber 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By taking into account the properties and limitations of the human visual
system, images can be more efficiently compressed, colors more accurately
reproduced, prints better rendered. To show all these advantages in this paper
new adapted color charts have been created based on technical and visual image
category analysis. A number of tests have been carried out using extreme images
with their key information strictly in dark and light areas. It was shown that
the image categorization using the adapted color charts improves the analysis
of relevant image information with regard to both the image gradation and the
detail reproduction. The images with key information in hi-key areas were also
test printed using the adapted color charts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5041</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5041</id><created>2012-09-23</created><authors><author><keyname>Dilawari</keyname><forenames>Jaswinder Singh</forenames></author><author><keyname>Khanna</keyname><forenames>Ravinder</forenames></author></authors><title>An Implementation of Computer Graphics as Prepress Image Enhancement
  Process</title><categories>cs.CV</categories><comments>4 Pages,8 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The production of a printed product involves three stages: prepress, the
printing process (press) itself, and finishing (post press). There are various
types of equipments (printers, scanners) and various qualities image are
present in the market. These give different color rendering each time during
reproduction. So, a color key tool has been developed keeping Color Management
Scheme (CMS) in mind so that during reproduction no color rendering takes place
irrespective of use of any device and resolution level has also been improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5045</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5045</id><created>2012-09-23</created><updated>2013-09-18</updated><authors><author><keyname>Li</keyname><forenames>Angsheng</forenames></author><author><keyname>Peng</keyname><forenames>Pan</forenames></author></authors><title>Detecting and Characterizing Small Dense Bipartite-like Subgraphs by the
  Bipartiteness Ratio Measure</title><categories>cs.DS</categories><comments>17 pages; ISAAC 2013</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding and characterizing subgraphs with small
\textit{bipartiteness ratio}. We give a bicriteria approximation algorithm
\verb|SwpDB| such that if there exists a subset $S$ of volume at most $k$ and
bipartiteness ratio $\theta$, then for any $0&lt;\epsilon&lt;1/2$, it finds a set
$S'$ of volume at most $2k^{1+\epsilon}$ and bipartiteness ratio at most
$4\sqrt{\theta/\epsilon}$. By combining a truncation operation, we give a local
algorithm \verb|LocDB|, which has asymptotically the same approximation
guarantee as the algorithm \verb|SwpDB| on both the volume and bipartiteness
ratio of the output set, and runs in time
$O(\epsilon^2\theta^{-2}k^{1+\epsilon}\ln^3k)$, independent of the size of the
graph. Finally, we give a spectral characterization of the small dense
bipartite-like subgraphs by using the $k$th \textit{largest} eigenvalue of the
Laplacian of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5052</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5052</id><created>2012-09-23</created><updated>2015-01-05</updated><authors><author><keyname>Li</keyname><forenames>Angsheng</forenames></author><author><keyname>Peng</keyname><forenames>Pan</forenames></author></authors><title>Testing Small Set Expansion in General Graphs</title><categories>cs.DS</categories><comments>23 pages; STACS 2015</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of testing small set expansion for general graphs. A
graph $G$ is a $(k,\phi)$-expander if every subset of volume at most $k$ has
conductance at least $\phi$. Small set expansion has recently received
significant attention due to its close connection to the unique games
conjecture, the local graph partitioning algorithms and locally testable codes.
  We give testers with two-sided error and one-sided error in the adjacency
list model that allows degree and neighbor queries to the oracle of the input
graph. The testers take as input an $n$-vertex graph $G$, a volume bound $k$,
an expansion bound $\phi$ and a distance parameter $\varepsilon&gt;0$. For the
two-sided error tester, with probability at least $2/3$, it accepts the graph
if it is a $(k,\phi)$-expander and rejects the graph if it is $\varepsilon$-far
from any $(k^*,\phi^*)$-expander, where $k^*=\Theta(k\varepsilon)$ and
$\phi^*=\Theta(\frac{\phi^4}{\min\{\log(4m/k),\log n\}\cdot(\ln k)})$. The
query complexity and running time of the tester are
$\widetilde{O}(\sqrt{m}\phi^{-4}\varepsilon^{-2})$, where $m$ is the number of
edges of the graph. For the one-sided error tester, it accepts every
$(k,\phi)$-expander, and with probability at least $2/3$, rejects every graph
that is $\varepsilon$-far from $(k^*,\phi^*)$-expander, where
$k^*=O(k^{1-\xi})$ and $\phi^*=O(\xi\phi^2)$ for any $0&lt;\xi&lt;1$. The query
complexity and running time of this tester are
$\widetilde{O}(\sqrt{\frac{n}{\varepsilon^3}}+\frac{k}{\varepsilon \phi^4})$.
We also give a two-sided error tester with smaller gap between $\phi^*$ and
$\phi$ in the rotation map model that allows (neighbor, index) queries and
degree queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5066</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5066</id><created>2012-09-23</created><authors><author><keyname>Miyaji</keyname><forenames>Atsuko</forenames></author><author><keyname>Rahman</keyname><forenames>Mohammad Shahriar</forenames></author></authors><title>APRAP: Another Privacy Preserving RFID Authentication Protocol</title><categories>cs.CR</categories><comments>6 pages, 1 figure, The 6th workshop on Secure Network Protocols
  (NPSec 2010), IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy preserving RFID (Radio Frequency Identification) authentication has
been an active research area in recent years. Both forward security and
backward security are required to maintain the privacy of a tag, i.e., exposure
of a tag's secret key should not reveal the past or future secret keys of the
tag. We envisage the need for a formal model for backward security for RFID
protocol designs in shared key settings, since the RFID tags are too
resource-constrained to support public key settings. However, there has not
been much research on backward security for shared key environment since Serge
Vaudenay in his Asiacrypt 2007 paper showed that perfect backward security is
impossible to achieve without public key settings. We propose a Privacy
Preserving RFID Authentication Protocol for shared key environment, APRAP,
which minimizes the damage caused by secret key exposure using insulated keys.
Even if a tag's secret key is exposed during an authentication session, forward
security and 'restricted' backward security of the tag are preserved under our
assumptions. The notion of 'restricted' backward security is that the adversary
misses the protocol transcripts which are needed to update the compromised
secret key. Although our definition does not capture perfect backward security,
it is still suitable for effective implementation as the tags are highly mobile
in practice. We also provide a formal security model of APRAP. Our scheme is
more efficient than previous proposals from the viewpoint of computational
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5071</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5071</id><created>2012-09-23</created><updated>2013-01-27</updated><authors><author><keyname>Borello</keyname><forenames>Martino</forenames></author><author><keyname>Willems</keyname><forenames>Wolfgang</forenames></author></authors><title>Automorphism of order 2p in binary self-dual extremal codes of length a
  multiple of 24</title><categories>cs.IT math.IT math.RT</categories><comments>13 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let C be a binary self-dual code with an automorphism g of order 2p, where p
is an odd prime, such that g^p is a fixed point free involution. If C is
extremal of length a multiple of 24 all the involutions are fixed point free,
except the Golay Code and eventually putative codes of length 120. Connecting
module theoretical properties of a self-dual code C with coding theoretical
ones of the subcode C(g^p) which consists of the set of fixed points of g^p, we
prove that C is a projective F_2&lt;g&gt;-module if and only if a natural projection
of C(g^p) is a self-dual code. We then discuss easy to handle criteria to
decide if C is projective or not. As an application we consider in the last
part extremal self-dual codes of length 120, proving that their automorphism
group does not contain elements of order 38 and 58.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5074</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5074</id><created>2012-09-23</created><authors><author><keyname>Abdelkefi</keyname><forenames>Atef</forenames></author><author><keyname>Efthekhari</keyname><forenames>Yaser</forenames></author><author><keyname>Jiang</keyname><forenames>Yuming</forenames></author></authors><title>Locating Disruptions on Internet Paths through End-to-End Measurements</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In backbone networks carrying heavy traffic loads, unwanted and unusual
end-to-end delay changes can happen, though possibly rarely. In order to
understand and manage the network to potentially avoid such abrupt changes, it
is crucial and challenging to locate where in the network lies the cause of
such delays so that some corresponding actions may be taken. To tackle this
challenge, the present paper proposes a simple and novel approach. The proposed
approach relies only on end-to-end measurements, unlike literature approaches
that often require a distributed and possibly complicated monitoring /
measurement infrastructure. Here, the key idea of the proposed approach is to
make use of compressed sensing theory to estimate delays on each hop between
the two nodes where end-to-end delay measurement is conducted, and infer
critical hops that contribute to the abrupt delay increases. To demonstrate its
effectiveness, the proposed approach is applied to a real network. The results
are encouraging, showing that the proposed approach is able to locate the hops
that have the most significant impact on or contribute the most to abrupt
increases on the end-to-end delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5076</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5076</id><created>2012-09-23</created><authors><author><keyname>Miyaji</keyname><forenames>Atsuko</forenames></author><author><keyname>Rahman</keyname><forenames>Mohammad Shahriar</forenames></author><author><keyname>Soshi</keyname><forenames>Masakazu</forenames></author></authors><title>Efficient and Low-Cost RFID Authentication Schemes</title><categories>cs.CR</categories><comments>21 pages, Journal of Wireless Mobile Networks, Ubiquitous Computing,
  and Dependable Applications (JoWUA), Vol 2, No 3, pp. 4-25, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security in passive resource-constrained Radio Frequency Identification
(RFID) tags is of much interest nowadays. Resistance against illegal tracking,
cloning, timing, and replay attacks are necessary for a secure RFID
authentication scheme. Reader authentication is also necessary to thwart any
illegal attempt to read the tags. With an objective to design a secure and
low-cost RFID authentication protocol, Gene Tsudik proposed a timestamp-based
protocol using symmetric keys, named YA-TRAP*. Although YA-TRAP* achieves its
target security properties, it is susceptible to timing attacks, where the
timestamp to be sent by the reader to the tag can be freely selected by an
adversary. Moreover, in YA-TRAP*, reader authentication is not provided, and a
tag can become inoperative after exceeding its pre-stored threshold timestamp
value. In this paper, we propose two mutual RFID authentication protocols that
aim to improve YA-TRAP* by preventing timing attack, and by providing reader
authentication. Also, a tag is allowed to refresh its pre-stored threshold
value in our protocols, so that it does not become inoperative after exceeding
the threshold. Our protocols also achieve other security properties like
forward security, resistance against cloning, replay, and tracking attacks.
Moreover, the computation and communication costs are kept as low as possible
for the tags. It is important to keep the communication cost as low as possible
when many tags are authenticated in batch-mode. By introducing aggregate
function for the reader-to-server communication, the communication cost is
reduced. We also discuss different possible applications of our protocols. Our
protocols thus capture more security properties and more efficiency than
YA-TRAP*. Finally, we show that our protocols can be implemented using the
current standard low-cost RFID infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5077</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5077</id><created>2012-09-23</created><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Complexity Reduction for Parameter-Dependent Linear Systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a complexity reduction algorithm for a family of
parameter-dependent linear systems when the system parameters belong to a
compact semi-algebraic set. This algorithm potentially describes the underlying
dynamical system with fewer parameters or state variables. To do so, it
minimizes the distance (i.e., H-infinity-norm of the difference) between the
original system and its reduced version. We present a sub-optimal solution to
this problem using sum-of-squares optimization methods. We present the results
for both continuous-time and discrete-time systems. Lastly, we illustrate the
applicability of our proposed algorithm on numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5083</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5083</id><created>2012-09-23</created><updated>2015-08-07</updated><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author></authors><title>A Simple Proof for the Existence of &quot;Good&quot; Pairs of Nested Lattices</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a simplified proof for the existence of nested lattice
codebooks allowing to achieve the capacity of the additive white Gaussian noise
channel, as well as the optimal rate-distortion trade-off for a Gaussian
source. The proof is self-contained and relies only on basic probabilistic and
geometrical arguments. An ensemble of nested lattices that is different, and
more elementary, than the one used in previous proofs is introduced. This
ensemble is based on lifting different subcodes of a linear code to the
Euclidean space using Construction A. In addition to being simpler, our
analysis is less sensitive to the assumption that the additive noise is
Gaussian. In particular, for additive ergodic noise channels it is shown that
the achievable rates of the nested lattice coding scheme depend on the noise
distribution only via its power. Similarly, the nested lattice source coding
scheme attains the same rate-distortion trade-off for all ergodic sources with
the same second moment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5091</identifier>
 <datestamp>2012-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5091</id><created>2012-09-23</created><updated>2012-10-26</updated><authors><author><keyname>Steenbergen</keyname><forenames>John</forenames></author><author><keyname>Klivans</keyname><forenames>Caroline</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sayan</forenames></author></authors><title>A Cheeger-Type Inequality on Simplicial Complexes</title><categories>math.CO cs.CG</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper, we consider a variation on Cheeger numbers related to the
coboundary expanders recently defined by Dotterer and Kahle. A Cheeger-type
inequality is proved, which is similar to a result on graphs due to Fan Chung.
This inequality is then used to study the relationship between coboundary
expanders on simplicial complexes and their corresponding eigenvalues,
complementing and extending results found by Gundert and Wagner. In particular,
we find these coboundary expanders do not satisfy natural Buser or Cheeger
inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5097</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5097</id><created>2012-09-23</created><authors><author><keyname>Mezzarobba</keyname><forenames>Marc</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author></authors><title>A Note on the Space Complexity of Fast D-Finite Function Evaluation</title><categories>cs.SC</categories><proxy>ccsd</proxy><journal-ref>CASC - Computer Algebra in Scientific Computing 7442 (2012)
  212-223</journal-ref><doi>10.1007/978-3-642-32973-9_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We state and analyze a generalization of the &quot;truncation trick&quot; suggested by
Gourdon and Sebah to improve the performance of power series evaluation by
binary splitting. It follows from our analysis that the values of D-finite
functions (i.e., functions described as solutions of linear differential
equations with polynomial coefficients) may be computed with error bounded by
2^(-p) in time O(p*(lg p)^(3+o(1))) and space O(p). The standard fast algorithm
for this task, due to Chudnovsky and Chudnovsky, achieves the same time
complexity bound but requires \Theta(p*lg p) bits of memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5108</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5108</id><created>2012-09-23</created><authors><author><keyname>Knockaert</keyname><forenames>Luc</forenames></author></authors><title>Global passive system approximation</title><categories>cs.SY</categories><comments>15 pages, 11 figures</comments><msc-class>93D09, 41A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new approach towards global passive approximation
in order to find a passive transfer function G(s) that is nearest in some
well-defined matrix norm sense to a non-passive transfer function H(s). It is
based on existing solutions to pertinent matrix nearness problems. It is shown
that the key point in constructing the nearest passive transfer function, is to
find a good rational approximation of the well-known ramp function over an
interval defined by the minimum and maximum dissipation of H(s). The proposed
algorithms rely on the stable anti-stable projection of a given transfer
function. Pertinent examples are given to show the scope and accuracy of the
proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5111</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5111</id><created>2012-09-23</created><authors><author><keyname>Bergstra</keyname><forenames>J.</forenames></author><author><keyname>Yamins</keyname><forenames>D.</forenames></author><author><keyname>Cox</keyname><forenames>D. D.</forenames></author></authors><title>Making a Science of Model Search</title><categories>cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer vision algorithms depend on a variety of parameter choices and
settings that are typically hand-tuned in the course of evaluating the
algorithm. While such parameter tuning is often presented as being incidental
to the algorithm, correctly setting these parameter choices is frequently
critical to evaluating a method's full potential. Compounding matters, these
parameters often must be re-tuned when the algorithm is applied to a new
problem domain, and the tuning process itself often depends on personal
experience and intuition in ways that are hard to describe. Since the
performance of a given technique depends on both the fundamental quality of the
algorithm and the details of its tuning, it can be difficult to determine
whether a given technique is genuinely better, or simply better tuned.
  In this work, we propose a meta-modeling approach to support automated hyper
parameter optimization, with the goal of providing practical tools to replace
hand-tuning with a reproducible and unbiased optimization process. Our approach
is to expose the underlying expression graph of how a performance metric (e.g.
classification accuracy on validation examples) is computed from parameters
that govern not only how individual processing steps are applied, but even
which processing steps are included. A hyper parameter optimization algorithm
transforms this graph into a program for optimizing that performance metric.
Our approach yields state of the art results on three disparate computer vision
problems: a face-matching verification task (LFW), a face identification task
(PubFig83) and an object recognition task (CIFAR-10), using a single algorithm.
More broadly, we argue that the formalization of a meta-model supports more
objective, reproducible, and quantitative evaluation of computer vision
algorithms, and that it can serve as a valuable tool for guiding algorithm
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5123</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5123</id><created>2012-09-23</created><authors><author><keyname>Erde</keyname><forenames>Joshua</forenames></author></authors><title>An n-in-a-row Game</title><categories>math.CO cs.DM</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The usual $n$-in-a-row game is a positional game in which two player
alternately claim points in $\bb{Z}^2$ with the winner being the first player
to claim $n$ consecutive points in a line. We consider a variant of the game,
suggested by Croft, where the number of points claimed increases by 1 each
turn, and so on turn $t$ a player claims $t$ points. Croft asked how long it
takes to win this game. We show that, perhaps surprisingly, the time needed to
win this game is $(1-o(1))n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5127</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5127</id><created>2012-09-23</created><updated>2013-02-24</updated><authors><author><keyname>Kikuchi</keyname><forenames>Akihito</forenames></author></authors><title>A proposal to first principles electronic structure calculation:
  Symbolic-Numeric method</title><categories>cs.SC cond-mat.mtrl-sci physics.comp-ph</categories><comments>49 pages. This paper is originally written in Japanese and published
  in a Japanese journal &quot;bussei-kenkyu&quot;. The author submits here the original
  Japanese text, accompanied with a brief (almost full) English translation. In
  this revised version, the author has corrected some typos. If the reader
  would like to get the complete full translation, contact the author [
  kikuchi.akihito@canon.co.jp ]</comments><journal-ref>Bussei Kenkyu, Vol 1. No.3, 013101, 2012, (
  http://bussei-kenkyu.jp/pdf/01/3/0030-013101.pdf )</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study proposes an approach toward the first principles electronic
structure calculation with the aid of symbolic-numeric solving. The symbolic
computation enables us to express the Hartree-Fock-Roothaan equation and the
molecular integrals in analytic forms and approximate them as a set of
polynomial equations. By use of the Grobner bases technique, the polynomial
equations are transformed into other ones which have identical roots. The
converted equations take more convenient forms which will simplify numerical
procedures, from which we can derive necessary physical properties in order, in
an a la carte way. This method enables us to solve the electronic structure
calculation, the optimization of any kind, or the inverse problem as a forward
problem in a unified way, in which there is no need for iterative
self-consistent procedures with trials and errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5130</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5130</id><created>2012-09-23</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Distributed Spectrum Access with Spatial Reuse</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient distributed spectrum sharing mechanism is crucial for improving the
spectrum utilization. The spatial aspect of spectrum sharing, however, is less
understood than many other aspects. In this paper, we generalize a recently
proposed spatial congestion game framework to design efficient distributed
spectrum access mechanisms with spatial reuse. We first propose a spatial
channel selection game to model the distributed channel selection problem with
fixed user locations. We show that the game is a potential game, and develop a
distributed learning mechanism that converges to a Nash equilibrium only based
on users' local observations. We then formulate the joint channel and location
selection problem as a spatial channel selection and mobility game, and show
that it is also a potential game. We next propose a distributed strategic
mobility algorithm, jointly with the distributed learning mechanism, that can
converge to a Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5145</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5145</id><created>2012-09-23</created><authors><author><keyname>Bezanson</keyname><forenames>Jeff</forenames></author><author><keyname>Karpinski</keyname><forenames>Stefan</forenames></author><author><keyname>Shah</keyname><forenames>Viral B.</forenames></author><author><keyname>Edelman</keyname><forenames>Alan</forenames></author></authors><title>Julia: A Fast Dynamic Language for Technical Computing</title><categories>cs.PL cs.CE</categories><acm-class>D.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic languages have become popular for scientific computing. They are
generally considered highly productive, but lacking in performance. This paper
presents Julia, a new dynamic language for technical computing, designed for
performance from the beginning by adapting and extending modern programming
language techniques. A design based on generic functions and a rich type system
simultaneously enables an expressive programming model and successful type
inference, leading to good performance for a wide range of programs. This makes
it possible for much of the Julia library to be written in Julia itself, while
also incorporating best-of-breed C and Fortran libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5152</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5152</id><created>2012-09-24</created><authors><author><keyname>Kanade</keyname><forenames>Aditya</forenames></author><author><keyname>Sanyal</keyname><forenames>Amitabha</forenames></author><author><keyname>Khedker</keyname><forenames>Uday P.</forenames></author></authors><title>A Logic for Correlating Temporal Properties across Program
  Transformations</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program transformations are widely used in synthesis, optimization, and
maintenance of software. Correctness of program transformations depends on
preservation of some important properties of the input program. By regarding
programs as Kripke structures, many interesting properties of programs can be
expressed in temporal logics. In temporal logic, a formula is interpreted on a
single program. However, to prove correctness of transformations, we encounter
formulae which contain some subformulae interpreted on the input program and
some on the transformed program. An example where such a situation arises is
verification of optimizing program transformations applied by compilers.
  In this paper, we present a logic called Temporal Transformation Logic (TTL)
to reason about such formulae. We consider different types of primitive
transformations and present TTL inference rules for them. Our definitions of
program transformations and temporal logic operators are novel in their use of
the boolean matrix algebra. This results in specifications that are succinct
and constructive. Further, we use the boolean matrix algebra in a uniform
manner to prove soundness of the TTL inference rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5158</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5158</id><created>2012-09-24</created><updated>2012-10-02</updated><authors><author><keyname>Roy</keyname><forenames>Shubhabrata</forenames><affiliation>LIP</affiliation></author><author><keyname>Begin</keyname><forenames>Thomas</forenames><affiliation>LIP</affiliation></author><author><keyname>Loiseau</keyname><forenames>Patrick</forenames><affiliation>EURECOM</affiliation></author><author><keyname>Goncalves</keyname><forenames>Paulo</forenames><affiliation>LIP</affiliation></author></authors><title>Un mod\`ele de trafic adapt\'e \`a la volatilit\'e de charge d'un
  service de vid\'eo \`a la demande: Identification, validation et application
  \`a la gestion dynamique de ressources</title><categories>cs.NI cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1209.4806</comments><proxy>ccsd</proxy><report-no>RR-8072</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic resource management has become an active area of research in the
Cloud Computing paradigm. Cost of resources varies significantly depending on
configuration for using them. Hence efficient management of resources is of
prime interest to both Cloud Providers and Cloud Users. In this report we
suggest a probabilistic resource provisioning approach that can be exploited as
the input of a dynamic resource management scheme. Using a Video on Demand use
case to justify our claims, we propose an analytical model inspired from
standard models developed for epidemiology spreading, to represent sudden and
intense workload variations. As an essential step we also derive a heuristic
identification procedure to calibrate all the model parameters and evaluate the
performance of our estimator on synthetic time series. We show how good can our
model fit to real workload traces with respect to the stationary case in terms
of steady-state probability and autocorrelation structure. We find that the
resulting model verifies a Large Deviation Principle that statistically
characterizes extreme rare events, such as the ones produced by &quot;buzz effects&quot;
that may cause workload overflow in the VoD context. This analysis provides
valuable insight on expectable abnormal behaviors of systems. We exploit the
information obtained using the Large Deviation Principle for the proposed Video
on Demand use-case for defining policies (Service Level Agreements). We believe
these policies for elastic resource provisioning and usage may be of some
interest to all stakeholders in the emerging context of cloud networking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5160</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5160</id><created>2012-09-24</created><authors><author><keyname>Monagan</keyname><forenames>Michael</forenames></author></authors><title>A new edge selection heuristic for computing the Tutte polynomial of an
  undirected graph</title><categories>cs.DM cs.SC</categories><comments>11 pages, 7 figures, 9 tables, to appear in the proceedings of FPSAC
  2012</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new edge selection heuristic and vertex ordering heuristic that
together enable one to compute the Tutte polynomial of much larger sparse
graphs than was previously doable. As a specific example, we are able to
compute the Tutte polynomial of the truncated icosahedron graph using our Maple
implementation in under 4 minutes on a single CPU. This compares with a recent
result of Haggard, Pearce and Royle whose special purpose C++ software took one
week on 150 computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5173</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5173</id><created>2012-09-24</created><authors><author><keyname>Pich&#xe9;</keyname><forenames>Robert</forenames></author></authors><title>On the Parametric Instability Caused by Step Size Variation in
  Runge-Kutta-Nystr\&quot;om Methods</title><categories>math.NA cs.NA</categories><comments>11 pages</comments><report-no>Tampere University of Technology Department of Information
  Technology Mathematics Report 73</report-no><msc-class>65L06, 65L20, 65P40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parametric instability arising when ordinary differential equations
(ODEs) are numerically integrated with Runge-Kutta-Nystr\&quot;om (RKN) methods with
varying step sizes is investigated. It is shown that when linear constant
coefficient ODEs are integrated with RKN methods that are based on A-stable
Runge-Kutta methods, the solution is nonincreasing in some norm for all
positive step sizes, constant or varying. Perturbation methods are used to
quantify the critical step sizes associated with parametric instability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5180</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5180</id><created>2012-09-24</created><updated>2013-10-15</updated><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Stochastic Sensor Scheduling for Networked Control Systems</title><categories>math.OC cs.SY math.PR</categories><comments>Corrected Typos</comments><journal-ref>IEEE Transactions on Automatic Control, 59 (5), pp. 1147-1162, May
  2014</journal-ref><doi>10.1109/TAC.2014.2298733</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal sensor scheduling with applications to networked estimation and
control systems is considered. We model sensor measurement and transmission
instances using jumps between states of a continuous-time Markov chain. We
introduce a cost function for this Markov chain as the summation of terms
depending on the average sampling frequencies of the subsystems and the effort
needed for changing the parameters of the underlying Markov chain. By
minimizing this cost function through extending Brockett's recent approach to
optimal control of Markov chains, we extract an optimal scheduling policy to
fairly allocate the network resources among the control loops. We study the
statistical properties of this scheduling policy in order to compute upper
bounds for the closed-loop performance of the networked system, where several
decoupled scalar subsystems are connected to their corresponding estimator or
controller through a shared communication medium. We generalize the estimation
results to observable subsystems of arbitrary order. Finally, we illustrate the
developed results numerically on a networked system composed of several
decoupled water tanks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5187</identifier>
 <datestamp>2013-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5187</id><created>2012-09-24</created><updated>2013-08-28</updated><authors><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Identification of Sparse Linear Operators</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of identifying a linear deterministic operator from
its response to a given probing signal. For a large class of linear operators,
we show that stable identifiability is possible if the total support area of
the operator's spreading function satisfies D&lt;=1/2. This result holds for an
arbitrary (possibly fragmented) support region of the spreading function, does
not impose limitations on the total extent of the support region, and, most
importantly, does not require the support region to be known prior to
identification. Furthermore, we prove that stable identifiability of almost all
operators is possible if D&lt;1. This result is surprising as it says that there
is no penalty for not knowing the support region of the spreading function
prior to identification. Algorithms that provably recover all operators with
D&lt;=1/2, and almost all operators with D&lt;1 are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5192</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5192</id><created>2012-09-24</created><authors><author><keyname>Demirci</keyname><forenames>H. G&#xf6;kalp</forenames></author><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author><author><keyname>Yakary&#x131;lmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Probabilistic verifiers for asymmetric debates</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the power of silent constant-space probabilistic verifiers that
watch asymmetric debates (where one side is unable to see some of the messages
of the other) between two deterministic provers, and try to determine who is
right. We prove that probabilistic verifiers outperform their deterministic
counterparts as asymmetric debate checkers. It is shown that the membership
problem for every language in NSPACE(s(n)) has a 2^{s(n)}-time debate where one
prover is completely blind to the other one, for polynomially bounded space
constructible s(n). When partial information is allowed to be seen by the
handicapped prover, the class of languages debatable in 2^{s(n)} time contains
TIME(2^{s(n)}), so a probabilistic finite automaton can solve any decision
problem in P with small error in polynomial time with the aid of such a debate.
We also compare our systems with those with a single prover, and with
competing-prover interactive proof systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5198</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5198</id><created>2012-09-24</created><updated>2012-12-09</updated><authors><author><keyname>bertolazzi</keyname><forenames>Enrico</forenames></author><author><keyname>Rimoldi</keyname><forenames>Anna</forenames></author></authors><title>Fast matrix decomposition in F2</title><categories>math.NA cs.NA</categories><comments>20 pages, 4 figure, 6 tables, 1 algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work an efficient algorithm to perform a block decomposition (and so
to compute the rank) of large dense rectangular matrices with entries in
$\FF_2$ is presented. Depending on the way the matrix is stored, the operations
acting on rows or block of consecutive columns (stored as one integer) should
be preferred. In this paper, an algorithm that completely avoids the column
permutations is given. In particular, a block decomposition is presented and
its running times are compared with the ones adopted into SAGE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5199</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5199</id><created>2012-09-24</created><authors><author><keyname>Aschner</keyname><forenames>Rom</forenames></author><author><keyname>Katz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Morgenstern</keyname><forenames>Gila</forenames></author><author><keyname>Yuditsky</keyname><forenames>Yelena</forenames></author></authors><title>Approximation Schemes for Covering and Packing</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The local search framework for obtaining PTASs for NP-hard geometric
optimization problems was introduced, independently, by Chan and Har-Peled
(2009) and Mustafa and Ray (2010). In this paper, we generalize the framework
by extending its analysis to additional families of graphs, beyond the family
of planar graphs. We then present several applications of the generalized
framework, some of which are very different from those presented to date (using
the original framework). These applications include PTASs for finding a maximum
l-shallow set of a set of fat objects, for finding a maximum triangle matching
in an l-shallow unit disk graph, and for vertex-guarding a
(not-necessarily-simple) polygon under an appropriate shallowness assumption.
  We also present a PTAS (using the original framework) for the important
problem where one has to find a minimum-cardinality subset of a given set of
disks (of varying radii) that covers a given set of points, and apply it to a
class cover problem (studied in Bereg et al., 2012) to obtain an improved
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5208</identifier>
 <datestamp>2013-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5208</id><created>2012-09-24</created><updated>2013-02-12</updated><authors><author><keyname>Beck</keyname><forenames>Martin</forenames></author><author><keyname>Kerschbaum</keyname><forenames>Florian</forenames></author></authors><title>Approximate Two-Party Privacy-Preserving String Matching with Linear
  Complexity</title><categories>cs.CR</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider two parties who want to compare their strings, e.g., genomes, but do
not want to reveal them to each other. We present a system for
privacy-preserving matching of strings, which differs from existing systems by
providing a deterministic approximation instead of an exact distance. It is
efficient (linear complexity), non-interactive and does not involve a third
party which makes it particularly suitable for cloud computing. We extend our
protocol, such that it mitigates iterated differential attacks proposed by
Goodrich. Further an implementation of the system is evaluated and compared
against current privacy-preserving string matching algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5210</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5210</id><created>2012-09-24</created><authors><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author></authors><title>APS Implementation Over Vehicular Ad Hoc Networks</title><categories>cs.NI</categories><comments>7 pages, 11 figures. arXiv admin note: substantial text overlap with
  arXiv:1111.4819</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The real world scenario has changed from the wired connection to wireless
connection.Over the years software, development has responded to the increasing
growth of wireless connectivity in developing network enabled software.The
problem arises in the wireless domain due to random packet loss in transport
layer and as well as in data link layer for the end to end connection. The
basic problem has been considered in this work is to convert the real world
scenario of Vehicular ad hoc network into a lab oriented problem by used the
APS-system and study the result to achieve better performance in the wireless
domain. The real world physical problems map into analytical problem and
simulate that analytic problem with respect to real world scenario by Automated
Position System (APS) for antenna mounted over the mobile node in 2 Dimension
space. Here the methodology quantifies the performance and the impact of the
packet loss, delay, by the bit error rate and throughput with respect to the
real- world scenario of VANET in the MAC layer, data link layer and transport
layer. The result presents the Directional Antenna which is mounted over the
vehicle gives less bit error in comparison to Isotropic and Discone antenna.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5212</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5212</id><created>2012-09-24</created><authors><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Wang</keyname><forenames>Xiumin</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Li</keyname><forenames>Tiffany Jing</forenames></author><author><keyname>Feng</keyname><forenames>Rongquan</forenames></author></authors><title>Error Correction for Cooperative Data Exchange</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of error correction for a cooperative data
exchange (CDE) system, where some clients are compromised or failed and send
false messages. Assuming each client possesses a subset of the total messages,
we analyze the error correction capability when every client is allowed to
broadcast only one linearly-coded message. Our error correction capability
bound determines the maximum number of clients that can be compromised or
failed without jeopardizing the final decoding solution at each client. We show
that deterministic, feasible linear codes exist that can achieve the derived
bound. We also evaluate random linear codes, where the coding coefficients are
drawn randomly, and then develop the probability for a client to withstand a
certain number of compromised or failed peers and successfully deduce the
complete message for any network size and any initial message distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5213</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5213</id><created>2012-09-24</created><authors><author><keyname>Bjelakovi&#x107;</keyname><forenames>Igor</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Sommerfeld</keyname><forenames>Jochen</forenames></author></authors><title>Capacity Results for Arbitrarily Varying Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>22 pages, 0 figures. Accepted for publication in the LNCS Volume in
  Memory of Rudolf Ahlswede. Some of the results were presented at the ITW 2012
  Lausanne and published in the conference paper available at the IEEE Xplore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work the arbitrarily varying wiretap channel AVWC is studied. We
derive a lower bound on the random code secrecy capacity for the average error
criterion and the strong secrecy criterion in the case of a best channel to the
eavesdropper by using Ahlswede's robustification technique for ordinary AVCs.
We show that in the case of a non-symmetrisable channel to the legitimate
receiver the deterministic code secrecy capacity equals the random code secrecy
capacity, a result similar to Ahlswede's dichotomy result for ordinary AVCs.
Using this we can derive that the lower bound is also valid for the
deterministic code capacity of the AVWC. The proof of the dichotomy result is
based on the elimination technique introduced by Ahlswede for ordinary AVCs. We
further prove upper bounds on the deterministic code secrecy capacity in the
general case, which results in a multi-letter expression for the secrecy
capacity in the case of a best channel to the eavesdropper. Using techniques of
Ahlswede, developed to guarantee the validity of a reliability criterion, the
main contribution of this work is to integrate the strong secrecy criterion
into these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5218</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5218</id><created>2012-09-24</created><updated>2014-01-07</updated><authors><author><keyname>Quan</keyname><forenames>Quan</forenames></author><author><keyname>Cai</keyname><forenames>Kai-Yuan</forenames></author></authors><title>A New Continuous-Time Equality-Constrained Optimization Method to Avoid
  Singularity</title><categories>cs.NE</categories><comments>23 pages. Optimization; equality constraints; control theoretic
  approaches; continuous-time dynamical systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In equality-constrained optimization, a standard regularity assumption is
often associated with feasible point methods, namely the gradients of
constraints are linearly independent. In practice, the regularity assumption
may be violated. To avoid such a singularity, we propose a new projection
matrix, based on which a feasible point method for the continuous-time,
equality-constrained optimization problem is developed. First, the equality
constraint is transformed into a continuous-time dynamical system with
solutions that always satisfy the equality constraint. Then, the singularity is
explained in detail and a new projection matrix is proposed to avoid
singularity. An update (or say a controller) is subsequently designed to
decrease the objective function along the solutions of the transformed system.
The invariance principle is applied to analyze the behavior of the solution. We
also propose a modified approach for addressing cases in which solutions do not
satisfy the equality constraint. Finally, the proposed optimization approaches
are applied to two examples to demonstrate its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5221</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5221</id><created>2012-09-24</created><updated>2013-06-12</updated><authors><author><keyname>H&#xe4;ger</keyname><forenames>Christian</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>Design of APSK Constellations for Coherent Optical Channels with
  Nonlinear Phase Noise</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications</comments><journal-ref>IEEE Transactions on Communications, vol. 61, no. 8, pp.
  3362-3373, Aug. 2013</journal-ref><doi>10.1109/TCOMM.2013.061913.120713</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of amplitude phase-shift keying (APSK) constellations for
a coherent fiber-optical communication system where nonlinear phase noise
(NLPN) is the main system impairment. APSK constellations can be regarded as a
union of phase-shift keying (PSK) signal sets with different amplitude levels.
A practical two-stage (TS) detection scheme is analyzed, which performs close
to optimal detection for high enough input power. We optimize APSK
constellations with 4, 8, and 16 points in terms of symbol error probability
(SEP) under TS detection for several combinations of input power and fiber
length. Our results show that APSK is a promising modulation format in order to
cope with NLPN. As an example, for 16 points, performance gains of 3.2 dB can
be achieved at a SEP of 10^-2 compared to 16-QAM by choosing an optimized APSK
constellation. We also demonstrate that in the presence of severe nonlinear
distortions, it may become beneficial to sacrifice a constellation point or an
entire constellation ring to reduce the average SEP. Finally, we discuss the
problem of selecting a good binary labeling for the found constellations. For
the class of rectangular APSK a labeling design method is proposed, resulting
in near-optimal bit error probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5231</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5231</id><created>2012-09-24</created><authors><author><keyname>Mjolsness</keyname><forenames>Eric</forenames></author></authors><title>Time-Ordered Product Expansions for Computational Stochastic Systems
  Biology</title><categories>q-bio.QM cs.CE nlin.AO</categories><comments>Submitted to Q-Bio 2012 conference, Santa Fe, New Mexico</comments><doi>10.1088/1478-3975/10/3/035009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The time-ordered product framework of quantum field theory can also be used
to understand salient phenomena in stochastic biochemical networks. It is used
here to derive Gillespie's Stochastic Simulation Algorithm (SSA) for chemical
reaction networks; consequently, the SSA can be interpreted in terms of Feynman
diagrams. It is also used here to derive other, more general simulation and
parameter-learning algorithms including simulation algorithms for networks of
stochastic reaction-like processes operating on parameterized objects, and also
hybrid stochastic reaction/differential equation models in which systems of
ordinary differential equations evolve the parameters of objects that can also
undergo stochastic reactions. Thus, the time-ordered product expansion (TOPE)
can be used systematically to derive simulation and parameter-fitting
algorithms for stochastic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5238</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5238</id><created>2012-09-24</created><authors><author><keyname>Barr</keyname><forenames>Katie</forenames></author><author><keyname>Kendon</keyname><forenames>Viv</forenames></author></authors><title>Formal languages analysed by quantum walks</title><categories>quant-ph cs.FL</categories><comments>9 Pages, 5 figures. Accepted to QPL 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete time quantum walks are known to be universal for quantum
computation. This has been proven by showing that they can simulate a universal
gate set. In this paper we examine computation in terms of language acceptance
and present two ways in which discrete time quantum walks can accept some
languages with certainty. These walks can take quantum as well as classical
inputs, and we show that when the input is quantum, the walks can be
interpreted as performing state discrimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5243</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5243</id><created>2012-09-24</created><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author><author><keyname>Ghini</keyname><forenames>Vittorio</forenames></author><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author><author><keyname>Panzieri</keyname><forenames>Fabio</forenames></author></authors><title>Walking with the Oracle: Efficient Use of Mobile Networks through
  Location-Awareness</title><categories>cs.NI</categories><comments>A revised version of this paper appears in Proceedings of Wireless
  Days 2012, November 21-23 2012, Dublin, Ireland</comments><acm-class>C.2.2; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Always Best Packet Switching (ABPS) is a novel approach for wireless
communications that enables mobile nodes, equipped with multiple network
interface cards (NICs), to dynamically determine the most appropriate NIC to
use. Using ABPS, a mobile node can seamlessly switch to a different NIC in
order to get better performance, without causing communication interruptions at
the application level. To make this possible, NICs are kept always active and a
software monitor constantly probes the channels for available access points.
While this ensures maximum connection availability, considerable energy may be
wasted when no access points are available for a given NIC. In this paper we
address this issue by investigating the use of an &quot;oracle&quot; able to provide
information on network availability. This allows to dynamically switch on/off
NICs based on reported availability, thus reducing the power consumption. We
present a Markov model which allows us to estimate the impact of the oracle on
the ABPS mechanism: results show that significant reduction in energy
consumption can be achieved with minimal impact on connection availability. We
conclude by describing a prototype implementation of the oracle based on Web
services and geolocalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5244</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5244</id><created>2012-09-24</created><authors><author><keyname>Srikantaiah</keyname><forenames>K. C.</forenames></author><author><keyname>Srikanth</keyname><forenames>P. L.</forenames></author><author><keyname>Tejaswi</keyname><forenames>V.</forenames></author><author><keyname>Shaila</keyname><forenames>K.</forenames></author><author><keyname>Venugopal</keyname><forenames>K. R.</forenames></author><author><keyname>Patnaik</keyname><forenames>L. M.</forenames></author></authors><title>Ranking Search Engine Result Pages based on Trustworthiness of Websites</title><categories>cs.DB</categories><comments>10 pages; IJCSI International Journal of Computer Science Issues,
  Vol. 9, Issue 4, No 2, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The World Wide Web (WWW) is the repository of large number of web pages which
can be accessed via Internet by multiple users at the same time and therefore
it is Ubiquitous in nature. The search engine is a key application used to
search the web pages from this huge repository, which uses the link analysis
for ranking the web pages without considering the facts provided by them. A new
algorithm called Probability of Correctness of Facts(PCF)-Engine is proposed to
find the accuracy of the facts provided by the web pages. It uses the
Probability based similarity function (SIM) which performs the string matching
between the true facts and the facts of web pages to find their probability of
correctness. The existing semantic search engines, may give the relevant result
to the user query but may not be 100% accurate. Our algorithm computes
trustworthiness of websites to rank the web pages. Simulation results show that
our approach is efficient when compared with existing Voting and Truthfinder[1]
algorithms with respect to the trustworthiness of the websites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5245</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5245</id><created>2012-09-24</created><authors><author><keyname>Behi</keyname><forenames>Tarek</forenames></author><author><keyname>Arous</keyname><forenames>Najet</forenames></author><author><keyname>Ellouze</keyname><forenames>Noureddine</forenames></author></authors><title>Spike Timing Dependent Competitive Learning in Recurrent Self Organizing
  Pulsed Neural Networks Case Study: Phoneme and Word Recognition</title><categories>cs.CV cs.AI q-bio.NC</categories><comments>10 pages, 15 tables</comments><journal-ref>International Journal of Computer Science Issues, Vol. 9, Issue 4,
  No 2, (2012)328-337</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synaptic plasticity seems to be a capital aspect of the dynamics of neural
networks. It is about the physiological modifications of the synapse, which
have like consequence a variation of the value of the synaptic weight. The
information encoding is based on the precise timing of single spike events that
is based on the relative timing of the pre- and post-synaptic spikes, local
synapse competitions within a single neuron and global competition via lateral
connections. In order to classify temporal sequences, we present in this paper
how to use a local hebbian learning, spike-timing dependent plasticity for
unsupervised competitive learning, preserving self-organizing maps of spiking
neurons. In fact we present three variants of self-organizing maps (SOM) with
spike-timing dependent Hebbian learning rule, the Leaky Integrators Neurons
(LIN), the Spiking_SOM and the recurrent Spiking_SOM (RSSOM) models. The case
study of the proposed SOM variants is phoneme classification and word
recognition in continuous speech and speaker independent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5246</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5246</id><created>2012-09-24</created><authors><author><keyname>Sommerville</keyname><forenames>Ian</forenames></author><author><keyname>Lock</keyname><forenames>Russell</forenames></author><author><keyname>Storer</keyname><forenames>Tim</forenames></author></authors><title>Information requirements for enterprise systems</title><categories>cs.SE cs.SI</categories><comments>17 pages, 4 figures. 17th Monterey workshop, Oxford</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss an approach to system requirements engineering,
which is based on using models of the responsibilities assigned to agents in a
multi-agency system of systems. The responsibility models serve as a basis for
identifying the stakeholders that should be considered in establishing the
requirements and provide a basis for a structured approach, described here, for
information requirements elicitation. We illustrate this approach using a case
study drawn from civil emergency management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5251</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5251</id><created>2012-09-24</created><authors><author><keyname>Baudi&#x161;</keyname><forenames>Petr</forenames></author><author><keyname>Moud&#x159;&#xed;k</keyname><forenames>Josef</forenames></author></authors><title>On Move Pattern Trends in a Large Go Games Corpus</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We process a large corpus of game records of the board game of Go and propose
a way of extracting summary information on played moves. We then apply several
basic data-mining methods on the summary information to identify the most
differentiating features within the summary information, and discuss their
correspondence with traditional Go knowledge. We show statistically significant
mappings of the features to player attributes such as playing strength or
informally perceived &quot;playing style&quot; (e.g. territoriality or aggressivity),
describe accurate classifiers for these attributes, and propose applications
including seeding real-work ranks of internet players, aiding in Go study and
tuning of Go-playing programs, or contribution to Go-theoretical discussion on
the scope of &quot;playing style&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5257</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5257</id><created>2012-09-24</created><authors><author><keyname>Hachichi</keyname><forenames>Hiba</forenames></author><author><keyname>Kitouni</keyname><forenames>Ilham</forenames></author><author><keyname>Bouaroudj</keyname><forenames>Kenza</forenames></author><author><keyname>Saidouni</keyname><forenames>Djamel-Eddine</forenames></author></authors><title>A Graphical Tool for Testing Timed Systems based on Meta- Modeling and
  Graph Grammars</title><categories>cs.SE</categories><comments>The International Journal of Computer Science Issues - IJCSI, Vol 9,
  Issue 4, No 1, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The test is one of the approaches commonly used for validating systems to
ensure qualitative and quantitative implementation requirements. In this paper,
we interest in formal testing using graph transformation, thus we propose an
approach for translating a Durational Actions Timed Automata model (DATA*) with
a high number of states into a timed refusals region graph (TRRG) for creating
a canonical tester and generating test cases using graph transformation.
Though, our approach allows to generate automatically a visual modeling tool
for DATA*, TRRG and the canonical tester. The cost of building a visual
modeling tool from scratch is prohibitive. Meta- modeling approach is useful to
deal with this problem since it allows the modeling of the formalisms
themselves, by means of graph grammars. The meta-modeling tool AToM3 is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5259</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5259</id><created>2012-09-24</created><updated>2013-07-23</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>Entropy Bounds for Discrete Random Variables via Maximal Coupling</title><categories>cs.IT math.IT math.PR</categories><comments>Final version. Accepted to the IEEE Trans. on Information Theory,
  July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives new bounds on the difference of the entropies of two
discrete random variables in terms of the local and total variation distances
between their probability mass functions. The derivation of the bounds relies
on maximal coupling, and they apply to discrete random variables which are
defined over finite or countably infinite alphabets. Loosened versions of these
bounds are demonstrated to reproduce some previously reported results. The use
of the new bounds is exemplified for the Poisson approximation, where bounds on
the local and total variation distances follow from Stein's method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5260</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5260</id><created>2012-09-24</created><authors><author><keyname>Tan</keyname><forenames>Mingkui</forenames></author><author><keyname>Tsang</keyname><forenames>Ivor W.</forenames></author><author><keyname>Wang</keyname><forenames>Li</forenames></author></authors><title>Towards Large-scale and Ultrahigh Dimensional Feature Selection via
  Feature Generation</title><categories>cs.LG</categories><comments>45 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world applications such as text mining, it is desirable to
select the most relevant features or variables to improve the generalization
ability, or to provide a better interpretation of the prediction models. {In
this paper, a novel adaptive feature scaling (AFS) scheme is proposed by
introducing a feature scaling {vector $\d \in [0, 1]^m$} to alleviate the bias
problem brought by the scaling bias of the diverse features.} By reformulating
the resultant AFS model to semi-infinite programming problem, a novel feature
generating method is presented to identify the most relevant features for
classification problems. In contrast to the traditional feature selection
methods, the new formulation has the advantage of solving extremely
high-dimensional and large-scale problems. With an exact solution to the
worst-case analysis in the identification of relevant features, the proposed
feature generating scheme converges globally. More importantly, the proposed
scheme facilitates the group selection with or without special structures.
Comprehensive experiments on a wide range of synthetic and real-world datasets
demonstrate that the proposed method {achieves} better or competitive
performance compared with the existing methods on (group) feature selection in
terms of generalization performance and training efficiency. The C++ and MATLAB
implementations of our algorithm can be available at
\emph{http://c2inet.sce.ntu.edu.sg/Mingkui/robust-FGM.rar}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5267</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5267</id><created>2012-09-24</created><updated>2015-01-14</updated><authors><author><keyname>Cattan&#xe9;o</keyname><forenames>David</forenames></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames></author></authors><title>The Parameterized Complexity of Domination-type Problems and Application
  to Linear Codes</title><categories>cs.CC</categories><comments>19 pages, 2 figures</comments><journal-ref>TAMC'14, LNCS vol. 8402, pp.86-103, 2014</journal-ref><doi>10.1007/978-3-319-06089-7_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the parameterized complexity of domination-type problems.
(sigma,rho)-domination is a general and unifying framework introduced by Telle:
a set D of vertices of a graph G is (sigma,rho)-dominating if for any v in D,
|N(v)\cap D| in sigma and for any $v\notin D, |N(v)\cap D| in rho. We mainly
show that for any sigma and rho the problem of (sigma,rho)-domination is W[2]
when parameterized by the size of the dominating set. This general statement is
optimal in the sense that several particular instances of
(sigma,rho)-domination are W[2]-complete (e.g. Dominating Set). We also prove
that (sigma,rho)-domination is W[2] for the dual parameterization, i.e. when
parameterized by the size of the dominated set. We extend this result to a
class of domination-type problems which do not fall into the
(sigma,rho)-domination framework, including Connected Dominating Set. We also
consider problems of coding theory which are related to domination-type
problems with parity constraints. In particular, we prove that the problem of
the minimal distance of a linear code over Fq is W[2] for both standard and
dual parameterizations, and W[1]-hard for the dual parameterization.
  To prove W[2]-membership of the domination-type problems we extend the
Turing-way to parameterized complexity by introducing a new kind of non
deterministic Turing machine with the ability to perform `blind' transitions,
i.e. transitions which do not depend on the content of the tapes. We prove that
the corresponding problem Short Blind Multi-Tape Non-Deterministic Turing
Machine is W[2]-complete. We believe that this new machine can be used to prove
W[2]-membership of other problems, not necessarily related to domination
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5306</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5306</id><created>2012-09-24</created><authors><author><keyname>Madirolas</keyname><forenames>Gabriel</forenames></author><author><keyname>Perez-Escudero</keyname><forenames>Alfonso</forenames></author><author><keyname>de Polavieja</keyname><forenames>Gonzalo G.</forenames></author></authors><title>A Model of Decision-Making in Groups of Humans</title><categories>physics.soc-ph cs.SI q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decisions by humans depend on their estimations given some uncertain sensory
data. These decisions can also be influenced by the behavior of others. Here we
present a mathematical model to quantify this influence, inviting a further
study on the cognitive consequences of social information. We also expect that
the present model can be used for a better understanding of the neural circuits
implicated in social processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5307</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5307</id><created>2012-09-24</created><authors><author><keyname>Allen</keyname><forenames>Sarah R.</forenames></author><author><keyname>Iacono</keyname><forenames>John</forenames></author></authors><title>Packing identical simple polygons is NP-hard</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a small polygon S, a big simple polygon B and a positive integer k, it
is shown to be NP-hard to determine whether k copies of the small polygon
(allowing translation and rotation) can be placed in the big polygon without
overlap. Previous NP-hardness results were only known in the case where the big
polygon is allowed to be non-simple. A novel reduction from Planar-Circuit-SAT
is presented where a small polygon is constructed to encode the entire circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5313</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5313</id><created>2012-09-24</created><updated>2013-10-15</updated><authors><author><keyname>Perkins</keyname><forenames>Will</forenames></author></authors><title>Random k-SAT and the Power of Two Choices</title><categories>math.CO cs.CC cs.DM</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an Achlioptas-process version of the random k-SAT process: a bounded
number of k-clauses are drawn uniformly at random at each step, and exactly one
added to the growing formula according to a particular rule. We prove the
existence of a rule that shifts the satisfiability threshold. This extends a
well-studied area of probabilistic combinatorics (Achlioptas processes) to
random CSP's. In particular, while a rule to delay the 2-SAT threshold was
known previously, this is the first proof of a rule to shift the threshold of
k-SAT for k &gt;= 3.
  We then propose a gap decision problem based upon this semi-random model. The
aim of the problem is to investigate the hardness of the random k-SAT decision
problem, as opposed to the problem of finding an assignment or certificate of
unsatisfiability. Finally, we discuss connections to the study of Achlioptas
random graph processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5319</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5319</id><created>2012-09-24</created><authors><author><keyname>Al-Angari</keyname><forenames>Nourah</forenames></author><author><keyname>ALAbdullatif</keyname><forenames>Abdullatif</forenames></author></authors><title>Multiprocessor Scheduling Using Parallel Genetic Algorithm</title><categories>cs.DC</categories><comments>5 pages, 5 figures, published in (IJCSI, Volume 9, Issue 4, July
  2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tasks scheduling is the most challenging problem in the parallel computing.
Hence, the inappropriate scheduling will reduce or even abort the utilization
of the true potential of the parallelization. Genetic algorithm (GA) has been
successfully applied to solve the scheduling problem. The fitness evaluation is
the most time consuming GA operation for the CPU time, which affect the GA
performance. The proposed synchronous master-slave algorithm outperforms the
sequential algorithm in case of complex and high number of generations problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5325</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5325</id><created>2012-09-24</created><updated>2015-01-20</updated><authors><author><keyname>Grigore</keyname><forenames>Radu</forenames></author><author><keyname>Distefano</keyname><forenames>Dino</forenames></author><author><keyname>Petersen</keyname><forenames>Rasmus Lerchedahl</forenames></author><author><keyname>Tzevelekos</keyname><forenames>Nikos</forenames></author></authors><title>Runtime Verification Based on Register Automata</title><categories>cs.FL cs.SE</categories><comments>TACAS 2013 (plus proofs)</comments><acm-class>D.2.5; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose TOPL automata as a new method for runtime verification of systems
with unbounded resource generation. Paradigmatic such systems are
object-oriented programs which can dynamically generate an unbounded number of
fresh object identities during their execution. Our formalism is based on
register automata, a particularly successful approach in automata over infinite
alphabets which administers a finite-state machine with boundedly many
input-storing registers. We show that TOPL automata are equally expressive to
register automata and yet suitable to express properties of programs. Compared
to other runtime verification methods, our technique can handle a class of
properties beyond the reach of current tools. We show in particular that
properties which require value updates are not expressible with current
techniques yet are naturally captured by TOPL machines. On the practical side,
we present a tool for runtime verification of Java programs via TOPL
properties, where the trade-off between the coverage and the overhead of the
monitoring system is tunable by means of a number of parameters. We validate
our technique by checking properties involving multiple objects and chaining of
values on large open source projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5331</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5331</id><created>2012-09-24</created><authors><author><keyname>Senger</keyname><forenames>Steven</forenames></author></authors><title>Reliability of swarming algorithms for mobile sensor network
  applications</title><categories>cs.MA</categories><comments>Handout for poster given at the 2012 February Fourier Talks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many well-studied swarming algorithms which are often suited to
very specific purposes. As mobile sensor networks become increasingly complex,
and are comprised of more and more agents, it makes sense to consider swarming
algorithms for movement control. We introduce a natural way to measure the
reliability of various swarming algorithms so a balance can be struck between
algorithmic complexity and sampling accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5333</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5333</id><created>2012-09-24</created><authors><author><keyname>Chaurasiya</keyname><forenames>Himanshu</forenames></author></authors><title>Recent Trends of Measurement and Development of Vibration Sensors</title><categories>cs.SY physics.ins-det</categories><comments>6 pages, 8 figures, International Journal of Computer Science
  Issues,Volume 9, Issue 4, No 1, July 2012; http://www.IJCSI.org, ISSN
  (Online): 1694-0814, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent trends, sensors are devices which monitor a parameter of a system,
hopefully without disturbing that parameter. Vibration measurement has become
an important method in mechanical structural products research, design,
produce, apply and maintenance. Vibration sensor is more and more important as
key devices. Nowadays, with the development of computer technology, electronic
technology and manufacturing process, a variety of vibration sensors have come
forth in succession.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5335</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5335</id><created>2012-09-24</created><authors><author><keyname>Ayday</keyname><forenames>Erman</forenames></author><author><keyname>Einolghozati</keyname><forenames>Arash</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>BPRS: Belief Propagation Based Iterative Recommender System</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the first application of the Belief Propagation
(BP) algorithm in the design of recommender systems. We formulate the
recommendation problem as an inference problem and aim to compute the marginal
probability distributions of the variables which represent the ratings to be
predicted. However, computing these marginal probability functions is
computationally prohibitive for large-scale systems. Therefore, we utilize the
BP algorithm to efficiently compute these functions. Recommendations for each
active user are then iteratively computed by probabilistic message passing. As
opposed to the previous recommender algorithms, BPRS does not require solving
the recommendation problem for all the users if it wishes to update the
recommendations for only a single active. Further, BPRS computes the
recommendations for each user with linear complexity and without requiring a
training period. Via computer simulations (using the 100K MovieLens dataset),
we verify that BPRS iteratively reduces the error in the predicted ratings of
the users until it converges. Finally, we confirm that BPRS is comparable to
the state of art methods such as Correlation-based neighborhood model (CorNgbr)
and Singular Value Decomposition (SVD) in terms of rating and precision
accuracy. Therefore, we believe that the BP-based recommendation algorithm is a
new promising approach which offers a significant advantage on scalability
while providing competitive accuracy for the recommender systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5339</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5339</id><created>2012-09-24</created><authors><author><keyname>Ismkhan</keyname><forenames>Hassan</forenames></author><author><keyname>Zamanifar</keyname><forenames>Kamran</forenames></author></authors><title>Developing Improved Greedy Crossover to Solve Symmetric Traveling
  Salesman Problem</title><categories>cs.NE</categories><comments>International Journal of Computer Science Issues, Volume 9, Issue 4,
  No 3, July 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Traveling Salesman Problem (TSP) is one of the most famous optimization
problems. Greedy crossover designed by Greffenstette et al, can be used while
Symmetric TSP (STSP) is resolved by Genetic Algorithm (GA). Researchers have
proposed several versions of greedy crossover. Here we propose improved version
of it. We compare our greedy crossover with some of recent crossovers, we use
our greedy crossover and some recent crossovers in GA then compare crossovers
on speed and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5345</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5345</id><created>2012-09-24</created><authors><author><keyname>Rahman</keyname><forenames>Muhammad Mahbubur</forenames></author></authors><title>Mining Social Data to Extract Intellectual Knowledge</title><categories>cs.AI cs.SI</categories><comments>8 pages, 19 figures, 4 tables, 3 equations, ISSN: 2074-904X (Print),
  ISSN: 2074-9058 (Online)</comments><journal-ref>International Journal of Intelligent Systems and
  Applications(IJISA), vol.4, no.10, pp.15-24, 2012</journal-ref><doi>10.5815/ijisa.2012.10.02</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social data mining is an interesting phe-nomenon which colligates different
sources of social data to extract information. This information can be used in
relationship prediction, decision making, pat-tern recognition, social mapping,
responsibility distri-bution and many other applications. This paper presents a
systematical data mining architecture to mine intellectual knowledge from
social data. In this research, we use social networking site facebook as
primary data source. We collect different attributes such as about me,
comments, wall post and age from facebook as raw data and use advanced data
mining approaches to excavate intellectual knowledge. We also analyze our mined
knowledge with comparison for possible usages like as human behavior
prediction, pattern recognition, job responsibility distribution, decision
making and product promoting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5348</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5348</id><created>2012-09-24</created><updated>2012-10-11</updated><authors><author><keyname>Jain</keyname><forenames>Kamal</forenames></author><author><keyname>Wilkens</keyname><forenames>Christopher A.</forenames></author></authors><title>eBay's Market Intermediation Problem</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimal mechanism design problem faced by a market intermediary
who makes revenue by connecting buyers and sellers. We first show that the
optimal intermediation protocol has substantial structure: it is the solution
to an algorithmic pricing problem in which seller's costs are replaced with
virtual costs, and the sellers' payments need only depend on the buyer's
behavior and not the buyer's actual valuation function.
  Since the underlying algorithmic pricing problem may be difficult to solve
optimally, we study specific models of buyer behavior and give mechanisms with
provable approximation guarantees. We show that offering only the single most
profitable item for sale guarantees an $\Omega(\frac1{\log n})$ fraction of the
optimal revenue when item value distributions are independent and have monotone
hazard rates. We also give constant factor approximations when the buyer
considers all items at once, $k$ items at once, or items in sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5350</identifier>
 <datestamp>2013-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5350</id><created>2012-09-24</created><updated>2013-05-24</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author></authors><title>Learning Topic Models and Latent Bayesian Networks Under Expansion
  Constraints</title><categories>stat.ML cs.LG stat.AP</categories><comments>38 pages, 6 figures, 2 tables, applications in topic models and
  Bayesian networks are studied. Simulation section is added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised estimation of latent variable models is a fundamental problem
central to numerous applications of machine learning and statistics. This work
presents a principled approach for estimating broad classes of such models,
including probabilistic topic models and latent linear Bayesian networks, using
only second-order observed moments. The sufficient conditions for
identifiability of these models are primarily based on weak expansion
constraints on the topic-word matrix, for topic models, and on the directed
acyclic graph, for Bayesian networks. Because no assumptions are made on the
distribution among the latent variables, the approach can handle arbitrary
correlations among the topics or latent factors. In addition, a tractable
learning method via $\ell_1$ optimization is proposed and studied in numerical
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5360</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5360</id><created>2012-09-24</created><updated>2014-01-29</updated><authors><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author></authors><title>Balanced Allocations and Double Hashing</title><categories>cs.DS cs.DC cs.DM</categories><comments>Further updated, small improvements/typos fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Double hashing has recently found more common usage in schemes that use
multiple hash functions. In double hashing, for an item $x$, one generates two
hash values $f(x)$ and $g(x)$, and then uses combinations $(f(x) +k g(x)) \bmod
n$ for $k=0,1,2,...$ to generate multiple hash values from the initial two. We
first perform an empirical study showing that, surprisingly, the performance
difference between double hashing and fully random hashing appears negligible
in the standard balanced allocation paradigm, where each item is placed in the
least loaded of $d$ choices, as well as several related variants. We then
provide theoretical results that explain the behavior of double hashing in this
context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5364</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5364</id><created>2012-09-24</created><authors><author><keyname>Lewitzka</keyname><forenames>Steffen</forenames></author></authors><title>On some many-valued abstract logics and their Epsilon-T-style extensions</title><categories>cs.LO math.LO</categories><comments>42 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logical systems with classical negation and means for sentential or
propositional self-reference involve, in some way, paradoxical statements such
as the liar. However, the paradox disappears if one replaces classical by an
appropriate non-classical negation such as a paraconsistent one (no paradox
arises if the liar is both true and false). We consider a non-Fregean logic
which is a revised and extended version (Lewitzka 2012) of Epsilon-T-Logic
originally introduced by (Straeter 1992) as a logic with a total truth
predicate and propositional quantifiers. Self-reference is achieved by means of
equations between formulas which are interpreted over a model-theoretic
universe of propositions. Paradoxical statements, such as the liar, can be
asserted only by unsatisfiable equations and do not correlate with
propositions. In this paper, we generalize Epsilon-T-Logic to a four-valued
logic related to Dunn/Belnap logic B_4. We also define three-valued versions
related to Kleene's logic K_3 and Priest's Logic of Paradox P_3, respectively.
In this many-valued setting, models may contain liars and other &quot;paradoxical&quot;
propositions which are ruled out by the more restrictive classical semantics.
We introduce these many-valued non-Fregean logics as extensions of abstract
parameter logics such that parameter logic and extension are of the same
logical type. For this purpose, we define and study abstract logics of type
B_4, K_3 and P_3. Using semantic methods we show compactness of the consequence
relation of abstract logics of type B_4, give a representation as minimally
generated logics and establish a connection to the approach of (Font 1997).
Finally, we present a complete sequent calculus for the Epsilon-T-style
extension of classical abstract logics simplifying constructions originally
developed by (Straeter 1992, Zeitz 2000, Lewitzka 1998).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5370</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5370</id><created>2012-09-24</created><authors><author><keyname>Xie</keyname><forenames>Jianwei</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Secure Degrees of Freedom of One-hop Wireless Networks</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, Sept. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the secure degrees of freedom (d.o.f.) of one-hop wireless networks
by considering four fundamental Gaussian network structures: wiretap channel,
broadcast channel with confidential messages, interference channel with
confidential messages, and multiple access wiretap channel. The secure d.o.f.
of the canonical Gaussian wiretap channel with no helpers is zero. It has been
known that a strictly positive secure d.o.f. can be obtained in the Gaussian
wiretap channel by using a helper which sends structured cooperative signals.
We show that the exact secure d.o.f. of the Gaussian wiretap channel with a
helper is 1/2. Our achievable scheme is based on real interference alignment
and cooperative jamming, which renders the message signal and the cooperative
jamming signal separable at the legitimate receiver, but aligns them perfectly
at the eavesdropper preventing any reliable decoding of the message signal. Our
converse is based on two key lemmas. The first lemma quantifies the secrecy
penalty by showing that the net effect of an eavesdropper on the system is that
it eliminates one of the independent channel inputs. The second lemma
quantifies the role of a helper by developing a direct relationship between the
cooperative jamming signal of a helper and the message rate. We extend this
result to the case of M helpers, and show that the exact secure d.o.f. in this
case is M/(M+1). We then generalize this approach to more general network
structures with multiple messages. We show that the sum secure d.o.f. of the
Gaussian broadcast channel with confidential messages and M helpers is 1, the
sum secure d.o.f. of the two-user interference channel with confidential
messages is 2/3, the sum secure d.o.f. of the two-user interference channel
with confidential messages and M helpers is 1, and the sum secure d.o.f. of the
K-user multiple access wiretap channel is K(K-1)/(K(K-1)+1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5374</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5374</id><created>2012-09-24</created><authors><author><keyname>Wasif</keyname><forenames>Mohammed</forenames></author></authors><title>Mobile Location Update using Distance Method</title><categories>cs.NI</categories><comments>This is 4 pages article,published in International Journal of
  Computer Science Issues in July 2012; http://www.ijcsi.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  General Packet Radio Service (GPRS) is a complex data network which upgrades
current second generation GSM networks, offering true high-speed internet (IP)
and network connectivity over existing GSM cellular networks. The increasing
population of mobile users leads to congestion problems in these systems, and
motivates the development of more efficient management schemes. This project
deals with radio resource and mobility management such as location management
and handoff management using distance method in GPRS networks. A simulator
based on MATLAB which can study the location updating is used in this GPRS
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5387</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5387</id><created>2012-09-22</created><authors><author><keyname>Cremene</keyname><forenames>Ligia C.</forenames></author><author><keyname>Dumitrescu</keyname><forenames>D.</forenames></author><author><keyname>Nagy</keyname><forenames>Reka</forenames></author><author><keyname>Cremene</keyname><forenames>Marcel</forenames></author></authors><title>Game Theoretical Modelling for Dynamic Spectrum Access in TV Whitespace</title><categories>cs.GT cs.NI nlin.AO</categories><comments>5 double-column pages, 6 figures, CrownCom 2011. arXiv admin note:
  substantial text overlap with arXiv:1207.3365, arXiv:1209.5013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to assess TV whitespace access scenarios, three oligopoly game
models are considered and reformulated in terms of radio access: Cournot,
Stackelberg, and Bertrand. Besides revisiting the relevance of Nash and Pareto
equilibria, a new equilibrium concept, the joint Nash-Pareto equilibrium, is
considered. An evolutionary game equilibria detection method is used. The
analysis of the simulation results brings relevant insights on the issue of
autonomy vs. regulation in emerging cognitive radio environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5388</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5388</id><created>2012-09-23</created><authors><author><keyname>Miyaji</keyname><forenames>Atsuko</forenames></author><author><keyname>Rahman</keyname><forenames>Mohammad Shahriar</forenames></author></authors><title>KIMAP: Key-Insulated Mutual Authentication Protocol for RFID</title><categories>cs.CR</categories><comments>23 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.5066, arXiv:1209.5076</comments><journal-ref>International Journal of Automated Identification Technology
  (IJAIT), Vol 3, No 2, pp. 61-74, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RFID tags are heavily constrained in computational and storage capabilities,
and raise numerous privacy concerns in everyday life due to their vulnerability
to different attacks. Both forward security and backward security are required
to maintain the privacy of a tag i.e., exposure of a tag's secret key should
not reveal the past or future secret keys of the tag. We envisage the need for
a formal model for backward security for RFID protocol designs in shared key
settings, since the RFID tags are too resource-constrained to support public
key settings. However, there has not been much research on backward security
for shared key environment since Serge Vaudenay in his Asiacrypt 2007 paper
showed that perfect backward security is impossible to achieve without public
key settings. We propose a Key-Insulated Mutual Authentication Protocol for
shared key environment, KIMAP, which minimizes the damage caused by secret key
exposure using insulated keys. Even if a tag's secret key is exposed during an
authentication session, forward security and `restricted' backward security of
the tag are preserved under our assumptions. The notion of `restricted'
backward security is that the adversary misses the protocol transcripts which
are needed to update the compromised secret key. Although our definition does
not capture perfect backward security, it is still suitable for effective
implementation as the tags are highly mobile in practice. We also provide a
formal security model of KIMAP. Our scheme is more efficient than previous
proposals from the viewpoint of computational requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5401</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5401</id><created>2012-09-24</created><authors><author><keyname>Medic</keyname><forenames>Adis</forenames></author><author><keyname>Golubovic</keyname><forenames>Adis</forenames></author></authors><title>P2P Appliance Calculation Method For Trust between Nodes</title><categories>cs.CY</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 4, No 2, July 2012, ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern ways of communications, such as in a web services environment, also
influences trust relationships between organisations. This concept of web-based
(way towards semantic web services) trust is new and has as yet not been
resolved. We hope that some of the trust properties mentioned above can be
successfully employed to improve the understanding of trust between machines.
So, trust is a vital ingredient of any successful interaction between
individuals, among organizations and/or in society at large. In this paper, we
suggested a trust model using fuzzy logic in semantic network of nodes. Trust
is an aggregation of consensus given a set of past interaction among nodes
(semantic network based on machines, agents etc.). We applied our suggested
model to semantic networks in order to show how trust mechanisms are involved
in communicating algorithm to choose the proper path from source to
destination. Authors use the terms untrust and distrust as synonyms for the
condition opposite to the trust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5407</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5407</id><created>2012-09-24</created><authors><author><keyname>Lisitsa</keyname><forenames>Alexei</forenames></author><author><keyname>Nemytykh</keyname><forenames>Andrei P.</forenames></author></authors><title>A Note on Program Specialization. What Can Syntactical Properties of
  Residual Programs Reveal?</title><categories>cs.PL cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents two examples of non-traditional using of program
specialization by Turchin's supercompilation method. In both cases we are
interested in syntactical properties of residual programs produced by
supercompilation. In the first example we apply supercompilation to a program
encoding a word equation and as a result we obtain a program representing a
graph describing the solution set of the word equation. The idea of the second
example belongs to Alexandr V. Korlyukov. He considered an interpreter
simulating the dynamic of the well known missionaries-cannibals puzzle.
Supercompilation of the interpreter allows us to solve the puzzle. The
interpreter may also be seen as an encoding of a non-deterministic protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5417</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5417</id><created>2012-09-24</created><authors><author><keyname>Ekhtiyar</keyname><forenames>Hesam</forenames></author><author><keyname>Sheida</keyname><forenames>Mehdi</forenames></author><author><keyname>Moghadam</keyname><forenames>Somaye Sobati</forenames></author></authors><title>Model based neuro-fuzzy ASR on Texas processor</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an algorithm for recognizing speech has been proposed. The
recognized speech is used to execute related commands which use the MFCC and
two kind of classifiers, first one uses MLP and second one uses fuzzy inference
system as a classifier. The experimental results demonstrate the high gain and
efficiency of the proposed algorithm. We have implemented this system based on
graphical design and tested on a fix point digital signal processor (DSP) of
600 MHz, with reference DM6437-EVM of Texas instrument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5420</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5420</id><created>2012-09-24</created><authors><author><keyname>Ahmed</keyname><forenames>Avishek</forenames></author><author><keyname>Ahmed</keyname><forenames>Tanvir</forenames></author><author><keyname>Ullah</keyname><forenames>Md. Samawat</forenames></author><author><keyname>Islam</keyname><forenames>Md. Manirul</forenames></author></authors><title>Controlling and securing a Digital Home using Multiple Sensor Based
  Perception system Integrated with Mobile and Voice technology</title><categories>cs.OH</categories><comments>8 pages</comments><journal-ref>International Journal of Information and Communication Technology
  Research, Volume 1 No. 5, pp. 189-196, September 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fully controlled digital home had always been considered as a luxury of rich
people because of excessive cost to install the system. It is now within the
reach of mass people with lots of inexpensive cool features. In this paper we
have designed and developed a very low cost, efficient and reliable Digital
home system. Fully Controlled Digital Home is no more a Luxury. Our proposed
system made it affordable. We built a low-cost feature-rich Digital Home System
(DHS). Digital Home System is combination of automated services i.e. Electronic
Device Controller, IR Security System, Web Desktop, Remote Video Surveillance
System and Virtual Mobile by which we can control our home by avoiding old
manual processes e.g. our physical presence at home is optional. The System
provides some of the modern luxury &amp; security features to us. Now we can
control Light, fan, AC or any electronic devices by voice command, Blue-tooth,
GPRS or Website. To control the system remotely, GPRS connectivity is added. We
can also monitor our home from remote area by using Remote Video Surveillance
System. This enables live video into mobile device of the digital home.
Moreover, we can also access our PC and do the necessary tasks from any
internet enabled computer in the world by using Web Desktop which is specially
built for this purpose. Furthermore, access of unauthorized person in the home
will be notified by SMS &amp; store the image of the person and also generate a
voice alarm. So that ensures the security of our valuable things. Also we can
identify and monitor the location of our valuable assets e.g. precious metals
remotely. Finally, Virtual mobile application is a Universal mobile Driver by
which we can exactly perform some same task e.g. Remote call, Phone book
access, SMS read-write of our mobile device from our new invented computer's
virtual mobile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5426</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5426</id><created>2012-09-24</created><updated>2012-10-12</updated><authors><author><keyname>Ahmed</keyname><forenames>Tanvir</forenames></author><author><keyname>Rahaman</keyname><forenames>Mohammad Saiedur</forenames></author><author><keyname>Rahman</keyname><forenames>Mohammad Saidur</forenames></author><author><keyname>Khan</keyname><forenames>Manzur H.</forenames></author></authors><title>A Coherent Distributed Grid Service for Assimilation and Unification of
  Heterogeneous Data Source</title><categories>cs.DB</categories><comments>9 pages; ISSN 1608-3679</comments><acm-class>H.2.5; H.2.4; H.3.3</acm-class><journal-ref>AIUB Journal of Science And Engineering (AJSE) Vol. 9, No. 1, PP
  47-55, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grid services are heavily used for handling large distributed computations.
They are also very useful to handle heavy data intensive applications where
data are distributed in different sites. Most of the data grid services used in
such situations are meant for homogeneous data source. In case of Heterogeneous
data sources, most of the grid services that are available are designed such a
way that they must be identical in schema definition for their smooth
operation. But there can be situations where the grid site databases are
heterogeneous and their schema definition is different from the central schema
definition. In this paper we propose a light weight coherent grid service for
heterogeneous data sources that is very easily install. It can map and convert
the central SQL schema into that of the grid members and send queries to get
according results from heterogeneous data sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5428</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5428</id><created>2012-09-24</created><authors><author><keyname>Sultana</keyname><forenames>Nasrin</forenames></author><author><keyname>Ahmed</keyname><forenames>Tanvir</forenames></author><author><keyname>Hossain</keyname><forenames>A. B. M. Siddique</forenames></author></authors><title>Study of a new link layer security scheme in a wireless sensor network</title><categories>cs.NI</categories><comments>8 pages. arXiv admin note: text overlap with arXiv:0909.0576 by other
  authors</comments><journal-ref>The AIUB Journal of Science and Engineering (AJSE), Vol. 10, No.
  1, pp. 79-86, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security of wireless sensor network (WSN) is always considered a critical
issue and has a number of considerations that separate them from traditional
wireless sensor network. First, sensor devices are typically vulnerable to
physical compromise. Second, they have significant power and processing
constraints. Third, the most critical security issue is protecting the
aggregate output of the system, even if individual nodes may be compromised.
While a variety of security techniques are being developed and lots of
researches are going on security fields. In this paper we have proposed a new
technique to provide data authentication and privacy in faster, scalable and
cost effective way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5429</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5429</id><created>2012-09-24</created><updated>2014-07-01</updated><authors><author><keyname>Gonzalez-Fernandez</keyname><forenames>Yasser</forenames></author><author><keyname>Soto</keyname><forenames>Marta</forenames></author></authors><title>copulaedas: An R Package for Estimation of Distribution Algorithms Based
  on Copulas</title><categories>cs.NE cs.MS</categories><journal-ref>Yasser Gonzalez-Fernandez, Marta Soto (2014). copulaedas: An R
  Package for Estimation of Distribution Algorithms Based on Copulas. Journal
  of Statistical Software, 58(9), 1-34. URL http://www.jstatsoft.org/v58/i09/</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The use of copula-based models in EDAs (estimation of distribution
algorithms) is currently an active area of research. In this context, the
copulaedas package for R provides a platform where EDAs based on copulas can be
implemented and studied. The package offers complete implementations of various
EDAs based on copulas and vines, a group of well-known optimization problems,
and utility functions to study the performance of the algorithms. Newly
developed EDAs can be easily integrated into the package by extending an S4
class with generic functions for their main components. This paper presents
copulaedas by providing an overview of EDAs based on copulas, a description of
the implementation of the package, and an illustration of its use through
examples. The examples include running the EDAs defined in the package,
implementing new algorithms, and performing an empirical study to compare the
behavior of different algorithms on benchmark functions and a real-world
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5430</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5430</id><created>2012-09-24</created><authors><author><keyname>Sioutas</keyname><forenames>Spyros</forenames></author><author><keyname>Panaretos</keyname><forenames>Alexandros</forenames></author><author><keyname>Karydis</keyname><forenames>Ioannis</forenames></author><author><keyname>Tsoumakos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Tzimas</keyname><forenames>Giannis</forenames></author><author><keyname>Tsolis</keyname><forenames>Dimitrios</forenames></author></authors><title>SART: Speeding up Query Processing in Sensor Networks with an Autonomous
  Range Tree Structure</title><categories>cs.DC cs.DB</categories><comments>11 pages, 23 figures, 5 algorithms or operations</comments><acm-class>H.2</acm-class><journal-ref>ACM Applied Computing Review (ACR), Vol. 12, No.3, 2012, pp.60-74</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing efficient P2P overlays for sensornets
providing &quot;Energy-Level Application and Services&quot;. The method presented in
\cite{SOPXM09} presents a novel P2P overlay for Energy Level discovery in a
sensornet. However, this solution is not dynamic, since requires periodical
restructuring. In particular, it is not able to support neither join of
sensor\_nodes with energy level out of the ranges supported by the existing p2p
overlay nor leave of \emph{empty} overlay\_peers to which no sensor\_nodes are
currently associated. On this purpose and based on the efficient P2P method
presented in \cite{SPSTMT10}, we design a dynamic P2P overlay for Energy Level
discovery in a sensornet, the so-called SART (Sensors' Autonomous Range Tree).
The adaptation of the P2P index presented in \cite{SPSTMT10} guarantees the
best-known dynamic query performance of the above operation. We experimentally
verify this performance, via the D-P2P-Sim simulator (D-P2P-Sim is publicly
available at http://code.google.com/p/d-p2p-sim/).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5431</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5431</id><created>2012-09-24</created><authors><author><keyname>Ahmed</keyname><forenames>Tanvir</forenames></author><author><keyname>Miah</keyname><forenames>Md. Suzan</forenames></author><author><keyname>Islam</keyname><forenames>Md. Manirul</forenames></author><author><keyname>Uddin</keyname><forenames>Md. Rakib</forenames></author></authors><title>Automatic Electric Meter Reading System: A Cost-Feasible Alternative
  Approach In Meter Reading For Bangladesh Perspective Using Low-Cost Digital
  Wattmeter And Wimax Technology</title><categories>cs.OH</categories><comments>8 pages</comments><journal-ref>International J. Eng. Tech 8(3):800-807, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy meter reading is a monotonous and an expensive task. Now the meter
reader people goes to each meter and take the meter reading manually to issue
the bill which will later be entered in the billing software for billing and
payment automation. If the manual meter reading and bill data entry process can
be automated then it would reduced the laborious task and financial wastage.
&quot;Automatic Electric Meter Reading (AMR) System&quot; is a metering system that is to
be used for data collecting from the meter and processing the collected data
for billing and other decision purposes. In this paper we have proposed an
automatic meter reading system which is low cost, high performance, highest
data rate, highest coverage area and most appropriate for Bangladesh
perspective. In this AMR system there are four basic units. They are reading
unit, communication unit, data receiving and processing unit and billing
system. For reading unit we identified the disk rotation of the energy meter
and stored the data in microcontroller. So it is not required to change the
current analog energy meter. An external module will be added with the current
energy meter. In the communication unit Wimax transceiver was used for wireless
communication between meter end and the server end because of its wide coverage
area. In the data receiving and processing unit meter reading will be collected
from the transceiver which is controlled by another microcontroller. There will
be a computer application that will take the data from the microcontroller.
This will also help to avoid any tampering or break down of energy meter. There
are various AMR system exists all over the world. Those systems were analyzed
and we found they are not feasible for Bangladesh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5434</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5434</id><created>2012-09-24</created><authors><author><keyname>Kerber</keyname><forenames>Michael</forenames></author><author><keyname>Edelsbrunner</keyname><forenames>Herbert</forenames></author></authors><title>The Medusa of Spatial Sorting: 3D Kinetic Alpha Complexes and
  Implementation</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by an application in cell biology, we consider spatial sorting
processes defined by particles moving from an initial to a final configuration.
We describe an algorithm for constructing a cell complex in space-time, called
the medusa, that measures topological properties of the sorting process. The
algorithm requires an extension of the kinetic data structures framework from
Delaunay triangulations to fixed-radius alpha complexes. We report on several
techniques to accelerate the computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5435</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5435</id><created>2012-09-24</created><authors><author><keyname>Mohammed</keyname><forenames>Muhanad Hayder</forenames></author></authors><title>Secure electronic lock using pic 16f628a microcontroller</title><categories>cs.OH</categories><journal-ref>International Journal of Research in Computer Science IJORCS,
  September 21, 2012</journal-ref><doi>10.7815/ijorcs.25.2012.047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proposed system implements the electronic embedded lock, its provides a
great benefit over traditional lock, which use the manual key, so if the key
lost or theft then anybody could open the lock, while thieving or losing the
long and complex password is harder compare to traditional key, furthermore
combining both manual key with computerized password make the system more
secure. Long password will reduce the possibilities of breaking the code and
opening the lock. The system comprised keypad, and HD44780 20x2 LCD Along with
PIC16f628a microcontroller. The firmware control these components such that
interaction with keypad is very is ver easy and smoothly, the LCD provide user
with messages and notification to be informed about what is the system state.
User can performing opening and closing the lock, changing the current password
in the microcontroller EEPROM and clearing single digit while entering the
password when wrong digit entered (back space). The proposed system firmware
developed using assembly language with MPLAB development environment. It tested
and implemented in real hardware with proper functioning and bug free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5441</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5441</id><created>2012-09-24</created><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Predecessor search with distance-sensitive query time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A predecessor (successor) search finds the largest element $x^-$ smaller than
the input string $x$ (the smallest element $x^+$ larger than or equal to $x$,
respectively) out of a given set $S$; in this paper, we consider the static
case (i.e., $S$ is fixed and does not change over time) and assume that the $n$
elements of $S$ are available for inspection. We present a number of algorithms
that, with a small additional index (usually of O(n log w) bits, where $w$ is
the string length), can answer predecessor/successor queries quickly and with
time bounds that depend on different kinds of distance, improving significantly
several results that appeared in the recent literature. Intuitively, our first
result has a running time that depends on the distance between $x$ and $x^\pm$:
it is especially efficient when the input $x$ is either very close to or very
far from $x^-$ or $x^+$; our second result depends on some global notion of
distance in the set $S$, and is fast when the elements of $S$ are more or less
equally spaced in the universe; finally, for our third result we rely on a
finger (i.e., an element of $S$) to improve upon the first one; its running
time depends on the distance between the input and the finger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5448</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5448</id><created>2012-09-24</created><authors><author><keyname>Mamun</keyname><forenames>Md. Abdullah al</forenames></author><author><keyname>Hanif</keyname><forenames>Md.</forenames></author><author><keyname>Uddin</keyname><forenames>Md. Rakib</forenames></author><author><keyname>Ahmed</keyname><forenames>Tanvir</forenames></author><author><keyname>Islam</keyname><forenames>Md. Mofizul</forenames></author></authors><title>A New Compression Based Index Structure for Efficient Information
  Retrieval</title><categories>cs.IR</categories><comments>5 pages</comments><journal-ref>International Journal of Science and Technology, Volume 2 No.1,
  pp. 10-14, January 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding desired information from large data set is a difficult problem.
Information retrieval is concerned with the structure, analysis, organization,
storage, searching, and retrieval of information. Index is the main constituent
of an IR system. Now a day exponential growth of information makes the index
structure large enough affecting the IR system's quality. So compressing the
Index structure is our main contribution in this paper. We compressed the
document number in inverted file entries using a new coding technique based on
run-length encoding. Our coding mechanism uses a specified code which acts over
run-length coding. We experimented and found that our coding mechanism on an
average compresses 67.34% percent more than the other techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5456</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5456</id><created>2012-09-24</created><updated>2012-11-29</updated><authors><author><keyname>Liu</keyname><forenames>Yanfang</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Relation matroid and its relationship with generalized rough set based
  on relation</title><categories>cs.AI</categories><comments>15 pages</comments><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the relationship between matroids and generalized rough sets based
on relations has been studied from the viewpoint of linear independence of
matrices. In this paper, we reveal more relationships by the predecessor and
successor neighborhoods from relations. First, through these two neighborhoods,
we propose a pair of matroids, namely predecessor relation matroid and
successor relation matroid, respectively. Basic characteristics of this pair of
matroids, such as dependent sets, circuits, the rank function and the closure
operator, are described by the predecessor and successor neighborhoods from
relations. Second, we induce a relation from a matroid through the circuits of
the matroid. We prove that the induced relation is always an equivalence
relation. With these two inductions, a relation induces a relation matroid, and
the relation matroid induces an equivalence relation, then the connection
between the original relation and the induced equivalence relation is studied.
Moreover, the relationships between the upper approximation operator in
generalized rough sets and the closure operator in matroids are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5464</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5464</id><created>2012-09-24</created><authors><author><keyname>Ghaderi</keyname><forenames>Javad</forenames></author><author><keyname>Ji</keyname><forenames>Tianxiong</forenames></author><author><keyname>Srikant</keyname><forenames>R.</forenames></author></authors><title>Flow-Level Stability of Wireless Networks: Separation of Congestion
  Control and Packet Scheduling</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is by now well-known that wireless networks with file arrivals and
departures are stable if one uses alpha-fair congestion control and
back-pressure based scheduling and routing. In this paper, we examine whether
?alpha-fair congestion control is necessary for flow-level stability. We show
that stability can be ensured even with very simple congestion control
mechanisms, such as a fixed window size scheme which limits the maximum number
of packets that are allowed into the ingress queue of a flow. A key ingredient
of our result is the use of the difference between the logarithms of queue
lengths as the link weights. This result is reminiscent of results in the
context of CSMA algorithms, but for entirely different reasons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5467</identifier>
 <datestamp>2013-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5467</id><created>2012-09-24</created><updated>2013-05-07</updated><authors><author><keyname>Abibullaev</keyname><forenames>Berdakh</forenames></author><author><keyname>An</keyname><forenames>Jinung</forenames></author><author><keyname>Lee</keyname><forenames>Seung-Hyun</forenames></author><author><keyname>Jin</keyname><forenames>Sang-Hyeon</forenames></author><author><keyname>Moon</keyname><forenames>Jeon-Il</forenames></author></authors><title>Minimizing inter-subject variability in fNIRS based Brain Computer
  Interfaces via multiple-kernel support vector learning</title><categories>stat.ML cs.LG</categories><comments>This paper has been withdrawn by the author due to an error in
  equation 19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain signal variability in the measurements obtained from different subjects
during different sessions significantly deteriorates the accuracy of most
brain-computer interface (BCI) systems. Moreover these variabilities, also
known as inter-subject or inter-session variabilities, require lengthy
calibration sessions before the BCI system can be used. Furthermore, the
calibration session has to be repeated for each subject independently and
before use of the BCI due to the inter-session variability. In this study, we
present an algorithm in order to minimize the above-mentioned variabilities and
to overcome the time-consuming and usually error-prone calibration time. Our
algorithm is based on linear programming support-vector machines and their
extensions to a multiple kernel learning framework. We tackle the inter-subject
or -session variability in the feature spaces of the classifiers. This is done
by incorporating each subject- or session-specific feature spaces into much
richer feature spaces with a set of optimal decision boundaries. Each decision
boundary represents the subject- or a session specific spatio-temporal
variabilities of neural signals. Consequently, a single classifier with
multiple feature spaces will generalize well to new unseen test patterns even
without the calibration steps. We demonstrate that classifiers maintain good
performances even under the presence of a large degree of BCI variability. The
present study analyzes BCI variability related to oxy-hemoglobin neural signals
measured using a functional near-infrared spectroscopy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5470</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5470</id><created>2012-09-24</created><updated>2012-12-16</updated><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Matroidal structure of generalized rough sets based on symmetric and
  transitive relations</title><categories>cs.AI</categories><comments>5 pages</comments><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rough sets are efficient for data pre-process in data mining. Lower and upper
approximations are two core concepts of rough sets. This paper studies
generalized rough sets based on symmetric and transitive relations from the
operator-oriented view by matroidal approaches. We firstly construct a
matroidal structure of generalized rough sets based on symmetric and transitive
relations, and provide an approach to study the matroid induced by a symmetric
and transitive relation. Secondly, this paper establishes a close relationship
between matroids and generalized rough sets. Approximation quality and
roughness of generalized rough sets can be computed by the circuit of matroid
theory. At last, a symmetric and transitive relation can be constructed by a
matroid with some special properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5473</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5473</id><created>2012-09-24</created><authors><author><keyname>Su</keyname><forenames>Lirun</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Some characteristics of matroids through rough sets</title><categories>cs.AI</categories><comments>13 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At present, practical application and theoretical discussion of rough sets
are two hot problems in computer science. The core concepts of rough set theory
are upper and lower approximation operators based on equivalence relations.
Matroid, as a branch of mathematics, is a structure that generalizes linear
independence in vector spaces. Further, matroid theory borrows extensively from
the terminology of linear algebra and graph theory. We can combine rough set
theory with matroid theory through using rough sets to study some
characteristics of matroids. In this paper, we apply rough sets to matroids
through defining a family of sets which are constructed from the upper
approximation operator with respect to an equivalence relation. First, we prove
the family of sets satisfies the support set axioms of matroids, and then we
obtain a matroid. We say the matroids induced by the equivalence relation and a
type of matroid, namely support matroid, is induced. Second, through rough
sets, some characteristics of matroids such as independent sets, support sets,
bases, hyperplanes and closed sets are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5477</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5477</id><created>2012-09-24</created><updated>2012-09-26</updated><authors><author><keyname>Lu</keyname><forenames>Yichao</forenames></author><author><keyname>Foster</keyname><forenames>Dean P.</forenames></author></authors><title>Optimal Weighting of Multi-View Data with Low Dimensional Hidden States</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Natural Language Processing (NLP) tasks, data often has the following two
properties: First, data can be chopped into multi-views which has been
successfully used for dimension reduction purposes. For example, in topic
classification, every paper can be chopped into the title, the main text and
the references. However, it is common that some of the views are less noisier
than other views for supervised learning problems. Second, unlabeled data are
easy to obtain while labeled data are relatively rare. For example, articles
occurred on New York Times in recent 10 years are easy to grab but having them
classified as 'Politics', 'Finance' or 'Sports' need human labor. Hence less
noisy features are preferred before running supervised learning methods. In
this paper we propose an unsupervised algorithm which optimally weights
features from different views when these views are generated from a low
dimensional hidden state, which occurs in widely used models like Mixture
Gaussian Model, Hidden Markov Model (HMM) and Latent Dirichlet Allocation
(LDA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5480</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5480</id><created>2012-09-24</created><authors><author><keyname>Yao</keyname><forenames>Hua</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Condition for neighborhoods in covering based rough sets to form a
  partition</title><categories>cs.AI</categories><comments>12 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neighborhood is an important concept in covering based rough sets. That under
what condition neighborhoods form a partition is a meaningful issue induced by
this concept. Many scholars have paid attention to this issue and presented
some necessary and sufficient conditions. However, there exists one common
trait among these conditions, that is they are established on the basis of all
neighborhoods have been obtained. In this paper, we provide a necessary and
sufficient condition directly based on the covering itself. First, we
investigate the influence of that there are reducible elements in the covering
on neighborhoods. Second, we propose the definition of uniform block and obtain
a sufficient condition from it. Third, we propose the definitions of repeat
degree and excluded number. By means of the two concepts, we obtain a necessary
and sufficient condition for neighborhoods to form a partition. In a word, we
have gained a deeper and more direct understanding of the essence over that
neighborhoods form a partition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5482</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5482</id><created>2012-09-24</created><authors><author><keyname>Wang</keyname><forenames>Jingqian</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Rough sets and matroidal contraction</title><categories>cs.AI</categories><comments>11 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rough sets are efficient for data pre-processing in data mining. As a
generalization of the linear independence in vector spaces, matroids provide
well-established platforms for greedy algorithms. In this paper, we apply rough
sets to matroids and study the contraction of the dual of the corresponding
matroid. First, for an equivalence relation on a universe, a matroidal
structure of the rough set is established through the lower approximation
operator. Second, the dual of the matroid and its properties such as
independent sets, bases and rank function are investigated. Finally, the
relationships between the contraction of the dual matroid to the complement of
a single point set and the contraction of the dual matroid to the complement of
the equivalence class of this point are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5484</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5484</id><created>2012-09-24</created><authors><author><keyname>Yao</keyname><forenames>Hua</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Condition for neighborhoods induced by a covering to be equal to the
  covering itself</title><categories>cs.AI</categories><comments>11 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a meaningful issue that under what condition neighborhoods induced by a
covering are equal to the covering itself. A necessary and sufficient condition
for this issue has been provided by some scholars. In this paper, through a
counter-example, we firstly point out the necessary and sufficient condition is
false. Second, we present a necessary and sufficient condition for this issue.
Third, we concentrate on the inverse issue of computing neighborhoods by a
covering, namely giving an arbitrary covering, whether or not there exists
another covering such that the neighborhoods induced by it is just the former
covering. We present a necessary and sufficient condition for this issue as
well. In a word, through the study on the two fundamental issues induced by
neighborhoods, we have gained a deeper understanding of the relationship
between neighborhoods and the covering which induce the neighborhoods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5490</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5490</id><created>2012-09-25</created><authors><author><keyname>Kuhn</keyname><forenames>Adrian</forenames></author><author><keyname>Loretan</keyname><forenames>Peter</forenames></author><author><keyname>Nierstrasz</keyname><forenames>Oscar</forenames></author></authors><title>Consistent Layout for Thematic Software Maps</title><categories>cs.SE</categories><comments>In Proceedings of 15th Working Conference on Reverse Engineering
  (WCRE'08), IEEE Computer Society Press, Los Alamitos CA, October 2008, pp.
  209-218</comments><doi>10.1109/WCRE.2008.45</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software visualizations can provide a concise overview of a complex software
system. Unfortunately, since software has no physical shape, there is no
&quot;natural&quot; mapping of software to a two-dimensional space. As a consequence most
visualizations tend to use a layout in which position and distance have no
meaning, and consequently layout typical diverges from one visualization to
another. We propose a consistent layout for software maps in which the position
of a software artifact reflects its \emph{vocabulary}, and distance corresponds
to similarity of vocabulary. We use Latent Semantic Indexing (LSI) to map
software artifacts to a vector space, and then use Multidimensional Scaling
(MDS) to map this vector space down to two dimensions. The resulting consistent
layout allows us to develop a variety of thematic software maps that express
very different aspects of software while making it easy to compare them. The
approach is especially suitable for comparing views of evolving software, since
the vocabulary of software artifacts tends to be stable over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5491</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5491</id><created>2012-09-25</created><authors><author><keyname>Amento</keyname><forenames>Brittanney</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author><author><keyname>Steinwandt</keyname><forenames>Rainer</forenames></author></authors><title>Quantum binary field inversion: improved circuit depth via choice of
  basis representation</title><categories>quant-ph cs.DS cs.ET</categories><comments>17 pages, 7 figures, 2 tables</comments><journal-ref>Quantum Information &amp; Computation 13(1-2): 116-134 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite fields of the form GF(2^m) play an important role in coding theory and
cryptography. We show that the choice of how to represent the elements of these
fields can have a significant impact on the resource requirements for quantum
arithmetic. In particular, we show how the use of Gaussian normal basis
representations and of `ghost-bit basis' representations can be used to
implement inverters with a quantum circuit of depth O(m log(m)). To the best of
our knowledge, this is the first construction with subquadratic depth reported
in the literature. Our quantum circuit for the computation of multiplicative
inverses is based on the Itoh-Tsujii algorithm which exploits that in normal
basis representation squaring corresponds to a permutation of the coefficients.
We give resource estimates for the resulting quantum circuit for inversion over
binary fields GF(2^m) based on an elementary gate set that is useful for
fault-tolerant implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5492</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5492</id><created>2012-09-25</created><authors><author><keyname>Yatawatta</keyname><forenames>Sarod</forenames></author></authors><title>On the interpolation of calibration solutions obtained in radio
  interferometry</title><categories>astro-ph.IM cs.NA</categories><comments>Accepted 2012 September 24. Received 2012 September 21; in original
  form 2012 June 12</comments><journal-ref>Monthly Notices of the Royal Astronomical Society, Volume 428,
  Issue 1, p.828-833, 2013</journal-ref><doi>10.1093/mnras/sts069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full polarimetric radio interferometric calibration is performed by
estimating 2 by 2 Jones matrices representing instrumental and propagation
effects. The solutions obtained in this way differ from the true solutions by a
2 by 2 unitary matrix ambiguity. This ambiguity is common to all stations for
which a solution is obtained but it is different for solutions obtained at
different time and frequency intervals. Therefore, straightforward
interpolation of solutions obtained at different time and frequency intervals
is not possible. In this paper, we propose to use the theory of quotient
manifolds for obtaining correct interpolants that are immune to unitary matrix
ambiguities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5494</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5494</id><created>2012-09-25</created><authors><author><keyname>Saidin</keyname><forenames>Nafiza</forenames></author><author><keyname>Sakim</keyname><forenames>Harsa Amylia Mat</forenames></author><author><keyname>Ngah</keyname><forenames>Umi Kalthum</forenames></author><author><keyname>Shuaib</keyname><forenames>Ibrahim Lutfi</forenames></author></authors><title>Segmentation of Breast Regions in Mammogram Based on Density: A Review</title><categories>cs.CV</categories><comments>9 pages, 2 figures,IJCSI International Journal of Computer Science
  Issues, Vol. 9, Issue 4, No 2, July 2012</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 4, No 2, July 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is to review approaches for segmentation of breast
regions in mammograms according to breast density. Studies based on density
have been undertaken because of the relationship between breast cancer and
density. Breast cancer usually occurs in the fibroglandular area of breast
tissue, which appears bright on mammograms and is described as breast density.
Most of the studies are focused on the classification methods for glandular
tissue detection. Others highlighted on the segmentation methods for
fibroglandular tissue, while few researchers performed segmentation of the
breast anatomical regions based on density. There have also been works on the
segmentation of other specific parts of breast regions such as either detection
of nipple position, skin-air interface or pectoral muscles. The problems on the
evaluation performance of the segmentation results in relation to ground truth
are also discussed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5507</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5507</id><created>2012-09-25</created><authors><author><keyname>Rani</keyname><forenames>Neha</forenames><affiliation>A.B.E.S. Engineering college</affiliation></author><author><keyname>Sharma</keyname><forenames>Preeti</forenames><affiliation>A.B.E.S. Engineering college</affiliation></author><author><keyname>Sharma</keyname><forenames>Pankaj</forenames><affiliation>A.B.E.S. Engineering college</affiliation></author></authors><title>Performance comparison of various routing protocols in different
  mobility models</title><categories>cs.NI</categories><comments>13 pages,13 figures, 1 Table</comments><journal-ref>IJASUC (International journal of Ad-hoc, Sensor and Ubiquitous
  computing)Volume 3, Number 4, August 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad hoc Network (MANET) is a infrastructure less network in which two
or more devices have wireless communication which can communicate with each
other and exchange information without need of any centralized administrator.
Each node in the ad hoc network acts as a router, forwarding data packets for
other nodes. The main issue is to compare the existing routing protocol and
finding the best one. The scope of this study is to test routing performance of
three different routing protocols (AODV, OLSR and DSDV) with respect to various
mobility models using NS2 simulator. In this paper the parameters used for
comparison are packet delivery fraction (PDF), average end to end delay (AEED),
normalized routing load (NRL) and throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5511</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5511</id><created>2012-09-25</created><authors><author><keyname>Arjmandi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Kenari</keyname><forenames>Masoume Nasiri</forenames></author><author><keyname>Bateni</keyname><forenames>Farshid</forenames></author></authors><title>Diffusion Based Nanonetworking: A New Modulation Technique and
  Performance Analysis</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we propose a new molecular modulation scheme for
nanonetworks. To evaluate the scheme we introduce a more realistic system model
for molecule dissemination and propagation processes based on the Poisson
distribution. We derive the probability of error of our proposed scheme as well
as the previously introduced schemes, including concentration and molecular
shift keying modulations by taking into account the error propagation effect of
previously decoded symbols. Since in our scheme the decoding of the current
symbol does not depend on the previously transmitted and decoded symbols, we do
not encounter error propagation; and so as our numerical results indicate, the
proposed scheme outperforms the previously introduced schemes. We then
introduce a general molecular communication system and use information
theoretic tools to derive fundamental limits on its probability of error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5513</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5513</id><created>2012-09-25</created><updated>2012-09-26</updated><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author></authors><title>On Capacity of Large-Scale MIMO Multiple Access Channels with
  Distributed Sets of Correlated Antennas</title><categories>cs.IT math.IT</categories><comments>52 pages, 6 figures. Accepted by the IEEE Journal on Selected Areas
  in Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a deterministic equivalent of ergodic sum rate and an
algorithm for evaluating the capacity-achieving input covariance matrices for
the uplink large-scale multiple-input multiple-output (MIMO) antenna channels
are proposed. We consider a large-scale MIMO system consisting of multiple
users and one base station with several distributed antenna sets. Each link
between a user and an antenna set forms a two-sided spatially correlated MIMO
channel with line-of-sight (LOS) components. Our derivations are based on novel
techniques from large dimensional random matrix theory (RMT) under the
assumption that the numbers of antennas at the terminals approach to infinity
with a fixed ratio. The deterministic equivalent results (the deterministic
equivalent of ergodic sum rate and the capacity-achieving input covariance
matrices) are easy to compute and shown to be accurate for realistic system
dimensions. In addition, they are shown to be invariant to several types of
fading distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5514</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5514</id><created>2012-09-25</created><authors><author><keyname>Baniasadi</keyname><forenames>Pouya</forenames></author><author><keyname>Ejov</keyname><forenames>Vladimir</forenames></author><author><keyname>Filar</keyname><forenames>Jerzy</forenames></author><author><keyname>Haythorpe</keyname><forenames>Michael</forenames></author></authors><title>Genetic Theory for Cubic Graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a partitioning of the set of unlabelled, connected cubic graphs
into two disjoint subsets named genes and descendants, where the cardinality of
the descendants is much larger than that of the genes. The key distinction
between the two subsets is the presence of special edge cut sets, called
crackers, in the descendants. We show that every descendant can be created by
starting from a finite set of genes, and introducing the required crackers by
special breeding operations. We prove that it is always possible to identify
genes that can be used to generate any given descendant, and provide inverse
operations that enable their reconstruction. A number of interesting properties
of genes may be inherited by the descendant, and we therefore propose a natural
algorithm that decomposes a descendant into its ancestor genes. We conjecture
that each descendant can only be generated by starting with a unique set of
ancestor genes. The latter is supported by numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5518</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5518</id><created>2012-09-25</created><authors><author><keyname>Tessone</keyname><forenames>Claudio J.</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Angel</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Diversity-induced resonance in the response to social norms</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>19 pages, 6 figures</comments><doi>10.1103/PhysRevE.87.022803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on diversity-induced resonance, which was recently
found in bistable, excitable and other physical systems. We study the
appearance of this phenomenon in a purely economic model of cooperating and
defecting agents. Agent's contribution to a public good is seen as a social
norm. So defecting agents face a social pressure, which decreases if
free-riding becomes widespread. In this model, diversity among agents naturally
appears because of the different sensitivity towards the social norm. We study
the evolution of cooperation as a response to the social norm (i) for the
replicator dynamics, and (ii) for the logit dynamics by means of numerical
simulations. Diversity-induced resonance is observed as a maximum in the
response of agents to changes in the social norm as a function of the degree of
heterogeneity in the population. We provide an analytical, mean-field approach
for the logit dynamics and find very good agreement with the simulations. From
a socio-economic perspective, our results show that, counter-intuitively,
diversity in the individual sensitivity to social norms may result in a society
that better follows such norms as a whole, even if part of the population is
less prone to follow them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5520</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5520</id><created>2012-09-25</created><updated>2014-12-04</updated><authors><author><keyname>Jeljeli</keyname><forenames>Hamza</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Accelerating Iterative SpMV for Discrete Logarithm Problem Using GPUs</title><categories>cs.CR</categories><comments>International Workshop on the Arithmetic of Finite Fields WAIFI 2014,
  Sep 2014, Gebze, Turkey. \&amp;lt;http://waifi.org/\&amp;gt;</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of cryptanalysis, computing discrete logarithms in large
cyclic groups using index-calculus-based methods, such as the number field
sieve or the function field sieve, requires solving large sparse systems of
linear equations modulo the group order. Most of the fast algorithms used to
solve such systems --- e.g., the conjugate gradient or the Lanczos and
Wiedemann algorithms --- iterate a product of the corresponding sparse matrix
with a vector (SpMV). This central operation can be accelerated on GPUs using
specific computing models and addressing patterns, which increase the
arithmetic intensity while reducing irregular memory accesses. In this work, we
investigate the implementation of SpMV kernels on NVIDIA GPUs, for several
representations of the sparse matrix in memory. We explore the use of Residue
Number System (RNS) arithmetic to accelerate modular operations. We target
linear systems arising when attacking the discrete logarithm problem on groups
of size 100 to 1000 bits, which includes the relevant range for current
cryptanalytic computations. The proposed SpMV implementation contributed to
solving the discrete logarithm problem in GF($2^{619}$) and GF($2^{809}$) using
the FFS algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5527</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5527</id><created>2012-09-25</created><updated>2015-05-30</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Sly</keyname><forenames>Allan</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>Strategic Learning and the Topology of Social Networks</title><categories>cs.GT math.PR</categories><comments>30 pages, one figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a group of strategic agents who must each repeatedly take one of
two possible actions. They learn which of the two actions is preferable from
initial private signals, and by observing the actions of their neighbors in a
social network.
  We show that the question of whether or not the agents learn efficiently
depends on the topology of the social network. In particular, we identify a
geometric &quot;egalitarianism&quot; condition on the social network that guarantees
learning in infinite networks, or learning with high probability in large
finite networks, in any equilibrium. We also give examples of non-egalitarian
networks with equilibria in which learning fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5549</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5549</id><created>2012-09-25</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author><author><keyname>Besserve</keyname><forenames>Michel</forenames></author></authors><title>Towards a learning-theoretic analysis of spike-timing dependent
  plasticity</title><categories>q-bio.NC cs.LG stat.ML</categories><comments>To appear in Adv. Neural Inf. Proc. Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper suggests a learning-theoretic perspective on how synaptic
plasticity benefits global brain functioning. We introduce a model, the
selectron, that (i) arises as the fast time constant limit of leaky
integrate-and-fire neurons equipped with spiking timing dependent plasticity
(STDP) and (ii) is amenable to theoretical analysis. We show that the selectron
encodes reward estimates into spikes and that an error bound on spikes is
controlled by a spiking margin and the sum of synaptic weights. Moreover, the
efficacy of spikes (their usefulness to other reward maximizing selectrons)
also depends on total synaptic strength. Finally, based on our analysis, we
propose a regularized version of STDP, and show the regularization improves the
robustness of neuronal learning when faced with multiple stimuli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5561</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5561</id><created>2012-09-25</created><authors><author><keyname>Peel</keyname><forenames>Leto</forenames></author></authors><title>Supervised Blockmodelling</title><categories>cs.LG cs.SI stat.ML</categories><comments>Workshop on Collective Learning and Inference on Structured Data 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collective classification models attempt to improve classification
performance by taking into account the class labels of related instances.
However, they tend not to learn patterns of interactions between classes and/or
make the assumption that instances of the same class link to each other
(assortativity assumption). Blockmodels provide a solution to these issues,
being capable of modelling assortative and disassortative interactions, and
learning the pattern of interactions in the form of a summary network. The
Supervised Blockmodel provides good classification performance using link
structure alone, whilst simultaneously providing an interpretable summary of
network interactions to allow a better understanding of the data. This work
explores three variants of supervised blockmodels of varying complexity and
tests them on four structurally different real world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5566</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5566</id><created>2012-09-25</created><authors><author><keyname>Barkay</keyname><forenames>Neta</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Shalem</keyname><forenames>Bar</forenames></author></authors><title>Feasible Sampling of Non-strict Turnstile Data Streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first feasible method for sampling a dynamic data stream with
deletions, where the sample consists of pairs $(k,C_k)$ of a value $k$ and its
exact total count $C_k$. Our algorithms are for both Strict Turnstile data
streams and the most general Non-strict Turnstile data streams, where each
element may have a negative total count. Our method improves by an order of
magnitude the known processing time of each element in the stream, which is
extremely crucial for data stream applications. For example, for a sample of
size $O(\epsilon^{-2} \log{(1/\delta)})$ in Non-strict streams, our solution
requires $O((\log\log(1/\epsilon))^2 + (\log\log(1/\delta)) ^ 2)$ operations
per stream element, whereas the best previous solution requires
$O(\epsilon^{-2} \log^2(1/\delta))$ evaluations of a fully independent hash
function per element. Here $1-\delta$ is the success probability and $\epsilon$
is the additive approximation error.
  We achieve this improvement by constructing a single data structure from
which multiple elements can be extracted with very high success probability.
The sample we generate is useful for calculating both forward and inverse
distribution statistics, within an additive error, with provable guarantees on
the success probability. Furthermore, our algorithms can run on distributed
systems and extract statistics on the union or difference between data streams.
They can be used to calculate the Jaccard similarity coefficient as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5567</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5567</id><created>2012-09-25</created><updated>2013-12-14</updated><authors><author><keyname>Li</keyname><forenames>Qingyin</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Closed-set lattice of regular sets based on a serial and transitive
  relation through matroids</title><categories>cs.AI</categories><comments>12 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rough sets are efficient for data pre-processing in data mining. Matroids are
based on linear algebra and graph theory, and have a variety of applications in
many fields. Both rough sets and matroids are closely related to lattices. For
a serial and transitive relation on a universe, the collection of all the
regular sets of the generalized rough set is a lattice. In this paper, we use
the lattice to construct a matroid and then study relationships between the
lattice and the closed-set lattice of the matroid. First, the collection of all
the regular sets based on a serial and transitive relation is proved to be a
semimodular lattice. Then, a matroid is constructed through the height function
of the semimodular lattice. Finally, we propose an approach to obtain all the
closed sets of the matroid from the semimodular lattice. Borrowing from
matroids, results show that lattice theory provides an interesting view to
investigate rough sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5569</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5569</id><created>2012-09-25</created><authors><author><keyname>Li</keyname><forenames>Qingyin</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Lattice structures of fixed points of the lower approximations of two
  types of covering-based rough sets</title><categories>cs.AI</categories><comments>17 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covering is a common type of data structure and covering-based rough set
theory is an efficient tool to process this data. Lattice is an important
algebraic structure and used extensively in investigating some types of
generalized rough sets. In this paper, we propose two family of sets and study
the conditions that these two sets become some lattice structures. These two
sets are consisted by the fixed point of the lower approximations of the first
type and the sixth type of covering-based rough sets, respectively. These two
sets are called the fixed point set of neighborhoods and the fixed point set of
covering, respectively. First, for any covering, the fixed point set of
neighborhoods is a complete and distributive lattice, at the same time, it is
also a double p-algebra. Especially, when the neighborhood forms a partition of
the universe, the fixed point set of neighborhoods is both a boolean lattice
and a double Stone algebra. Second, for any covering, the fixed point set of
covering is a complete lattice.When the covering is unary, the fixed point set
of covering becomes a distributive lattice and a double p-algebra. a
distributive lattice and a double p-algebra when the covering is unary.
Especially, when the reduction of the covering forms a partition of the
universe, the fixed point set of covering is both a boolean lattice and a
double Stone algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5571</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5571</id><created>2012-09-25</created><updated>2014-05-02</updated><authors><author><keyname>Artale</keyname><forenames>Alessandro</forenames></author><author><keyname>Kontchakov</keyname><forenames>Roman</forenames></author><author><keyname>Ryzhikov</keyname><forenames>Vladislav</forenames></author><author><keyname>Zakharyaschev</keyname><forenames>Michael</forenames></author></authors><title>A Cookbook for Temporal Conceptual Data Modelling with Description
  Logics</title><categories>cs.LO cs.AI</categories><comments>Accepted for the ACM Transaction on Computational Logic, (TOCL)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design temporal description logics suitable for reasoning about temporal
conceptual data models and investigate their computational complexity. Our
formalisms are based on DL-Lite logics with three types of concept inclusions
(ranging from atomic concept inclusions and disjointness to the full Booleans),
as well as cardinality constraints and role inclusions. In the temporal
dimension, they capture future and past temporal operators on concepts,
flexible and rigid roles, the operators `always' and `some time' on roles, data
assertions for particular moments of time and global concept inclusions. The
logics are interpreted over the Cartesian products of object domains and the
flow of time (Z,&lt;), satisfying the constant domain assumption. We prove that
the most expressive of our temporal description logics (which can capture
lifespan cardinalities and either qualitative or quantitative evolution
constraints) turn out to be undecidable. However, by omitting some of the
temporal operators on concepts/roles or by restricting the form of concept
inclusions we obtain logics whose complexity ranges between PSpace and
NLogSpace. These positive results were obtained by reduction to various clausal
fragments of propositional temporal logic, which opens a way to employ
propositional or first-order temporal provers for reasoning about temporal data
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5573</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5573</id><created>2012-09-25</created><authors><author><keyname>Suma</keyname><forenames>V.</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author></authors><title>Defect Management Strategies in Software Development</title><categories>cs.SE</categories><comments>27 Pages, 3 Figures, 5 Tables, 12 Equations</comments><journal-ref>Intec Web Publishers, Vienna, Austria, November 2009, ISBN:
  978-953-307-017-9</journal-ref><doi>10.5772/7407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software is a unique entity that has laid a strong impact on all other fields
either related or not related to software. These include medical, scientific,
business, educational, defence, transport, telecommunication to name a few.
State-of-the-art professional domain activities demands the development of high
quality software. High quality software attributes to a defect-free product,
which is competent of producing predictable results and remains deliverable
within time and cost constraints. It should be manageable with minimum
interferences. It should also be maintainable, dependable, understandable and
efficient. Thus, a systematic approach towards high quality software
development is required due to increased competitiveness in today's business
world, technological advances, hardware complexity and frequently changing
business requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5598</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5598</id><created>2012-09-25</created><updated>2016-01-27</updated><authors><author><keyname>Min</keyname><forenames>Fan</forenames></author></authors><title>Granular association rules on two universes with four measures</title><categories>cs.DB</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational association rules reveal patterns hide in multiple tables.
Existing rules are usually evaluated through two measures, namely support and
confidence. However, these two measures may not be enough to describe the
strength of a rule. In this paper, we introduce granular association rules with
four measures to reveal connections between granules in two universes, and
propose three algorithms for rule mining. An example of such a rule might be
&quot;40% men like at least 30% kinds of alcohol; 45% customers are men and 6%
products are alcohol.&quot; Here 45%, 6%, 40%, and 30% are the source coverage, the
target coverage, the source confidence, and the target confidence,
respectively. With these measures, our rules are semantically richer than
existing ones. Three subtypes of rules are obtained through considering special
requirements on the source/target confidence. Then we define a rule mining
problem, and design a sandwich algorithm with different rule checking
approaches for different subtypes. Experiments on a real world dataset show
that the approaches dedicated to three subtypes are 2-3 orders of magnitudes
faster than the one for the general case. A forward algorithm and a backward
algorithm for one particular subtype can speed up the mining process further.
This work opens a new research trend concerning relational association rule
mining, granular computing and rough sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5599</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5599</id><created>2012-09-25</created><authors><author><keyname>Kawamoto</keyname><forenames>Tatsuro</forenames></author></authors><title>A stochastic model of the tweet diffusion on the Twitter network</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>12 pages, 11 figures</comments><doi>10.1016/j.physa.2013.03.048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a stochastic model which describes diffusions of tweets on the
Twitter network. By dividing the followers into generations, we describe the
dynamics of the tweet diffusion as a random multiplicative process. We confirm
our model by directly observing the statistics of the multiplicative factors in
the Twitter data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5601</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5601</id><created>2012-09-25</created><authors><author><keyname>Min</keyname><forenames>Fan</forenames></author><author><keyname>Hu</keyname><forenames>Qinghua</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Feature selection with test cost constraint</title><categories>cs.AI cs.LG</categories><comments>23 pages</comments><doi>10.1016/j.ijar.2013.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection is an important preprocessing step in machine learning and
data mining. In real-world applications, costs, including money, time and other
resources, are required to acquire the features. In some cases, there is a test
cost constraint due to limited resources. We shall deliberately select an
informative and cheap feature subset for classification. This paper proposes
the feature selection with test cost constraint problem for this issue. The new
problem has a simple form while described as a constraint satisfaction problem
(CSP). Backtracking is a general algorithm for CSP, and it is efficient in
solving the new problem on medium-sized data. As the backtracking algorithm is
not scalable to large datasets, a heuristic algorithm is also developed.
Experimental results show that the heuristic algorithm can find the optimal
solution in most cases. We also redefine some existing feature selection
problems in rough sets, especially in decision-theoretic rough sets, from the
viewpoint of CSP. These new definitions provide insight to some new research
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5604</identifier>
 <datestamp>2013-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5604</id><created>2012-09-25</created><updated>2013-05-24</updated><authors><author><keyname>Li</keyname><forenames>Quan-Lin</forenames></author></authors><title>Tail Probabilities in Queueing Processes</title><categories>math.PR cs.NI</categories><comments>Randomized load balancing; supermarket model; work stealing model;
  QBD Process; Markov chain of GI/M/1 type; Markov chain of M/G/1 type</comments><msc-class>60J20, 60J28, 90B18, 90B22</msc-class><acm-class>B.4.4; C.2.1; D.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of large scale stochastic networks with resource management,
differential equations and mean-field limits are two key techniques. Recent
research shows that the expected fraction vector (that is, the tailed
probability vector) plays a key role in setting up mean-field differential
equations. To further apply the technique of tailed probability vector to deal
with resource management of large scale stochastic networks, this paper
discusses tailed probabilities in some basic queueing processes including QBD
processes, Markov chains of GI/M/1 type and of M/G/1 type, and also provides
some effective and efficient algorithms for computing the tailed probabilities
by means of the matrix-geometric solution, the matrix-iterative solution, the
matrix-product solution and the two types of RG-factorizations. Furthermore, we
consider four queueing examples: The M/M/1 retrial queue, the M(n)/M(n)/1
queue, the M/M/1 queue with server multiple vacations and the M/M/1 queue with
repairable server, where the M/M/1 retrial queue is given a detailed
discussion, while the other three examples are analyzed simply. Note that the
results given in this paper will be very useful in the study of large scale
stochastic networks with resource management, including the supermarket models
and the work stealing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5608</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5608</id><created>2012-09-25</created><authors><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Faster Deterministic Fully-Dynamic Graph Connectivity</title><categories>cs.DS cs.DM</categories><comments>To appear at SODA 2013. 19 pages, 1 figure</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give new deterministic bounds for fully-dynamic graph connectivity. Our
data structure supports updates (edge insertions/deletions) in
$O(\log^2n/\log\log n)$ amortized time and connectivity queries in $O(\log
n/\log\log n)$ worst-case time, where $n$ is the number of vertices of the
graph. This improves the deterministic data structures of Holm, de Lichtenberg,
and Thorup (STOC 1998, J.ACM 2001) and Thorup (STOC 2000) which both have
$O(\log^2n)$ amortized update time and $O(\log n/\log\log n)$ worst-case query
time. Our model of computation is the same as that of Thorup, i.e., a pointer
machine with standard $AC^0$ instructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5615</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5615</id><created>2012-09-25</created><updated>2012-11-18</updated><authors><author><keyname>Rettinger</keyname><forenames>Robert</forenames><affiliation>FernUni Hagen</affiliation></author></authors><title>On computable approximations of Landau's constant</title><categories>cs.NA cs.DS math.NA</categories><proxy>LMCS</proxy><acm-class>G.1.0</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (November
  20, 2012) lmcs:1189</journal-ref><doi>10.2168/LMCS-8(4:15)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm which computes the Landau constant up to any given
precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5625</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5625</id><created>2012-09-25</created><authors><author><keyname>Smith</keyname><forenames>Robert</forenames></author></authors><title>Managing Complex Structured Data In a Fast Evolving Environment</title><categories>cs.DB</categories><comments>Accepted for the International Lisp Conference 2012</comments><acm-class>D.2.11; H.2.3; J.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Criminal data comes in a variety of formats, mandated by state, federal, and
international standards. Specifying the data in a unified fashion is necessary
for any system that intends to integrate with state, federal, and international
law enforcement agencies. However, the contents, format, and structure of the
data is highly inconsistent across jurisdictions, and each datum requires
different ways of being printed, transmitted, and displayed. The goal was to
design a system that is unified in its approach to specify data, and is
amenable to future &quot;unknown unknowns&quot;. We have developed a domain-specific
language in Common Lisp which allows the specification of complex data with
evolving formats and structure, and is inter-operable with the Common Lisp
language. The resultant system has enabled the easy handling of complex
evolving information in the general criminal data environment and has made it
possible to manage and extend the system in a high-paced market. The language
has allowed the principal product of Secure Outcomes Inc. to enjoy success with
over 50 users throughout the United States.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5626</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5626</id><created>2012-09-25</created><authors><author><keyname>Smith</keyname><forenames>Robert</forenames></author></authors><title>A Tutorial for Creating and Publishing Open Source Lisp Software</title><categories>cs.OH</categories><comments>Accepted for the International Lisp Conference 2012</comments><acm-class>A.m; D.2.6; D.2.7; D.2.13</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation and accessability of the Internet have made it simple to
view, download, and publish source code. This paper gives a short tutorial on
how to create a new Common Lisp project and publish it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5647</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5647</id><created>2012-09-25</created><updated>2012-12-24</updated><authors><author><keyname>Hien</keyname><forenames>Tran Dang</forenames></author><author><keyname>Van Tuan</keyname><forenames>Do</forenames></author><author><keyname>Van At</keyname><forenames>Pham</forenames></author></authors><title>Additive Update Algorithm for Nonnegative Matrix Factorization</title><categories>cs.DS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) is an emerging technique with a wide
spectrum of potential applications in data analysis. Mathematically, NMF can be
formulated as a minimization problem with nonnegative constraints. This problem
is currently attracting much attention from researchers for theoretical reasons
and for potential applications. Currently, the most popular approach to solve
NMF is the multiplicative update algorithm proposed by D.D. Lee and H.S. Seung.
In this paper, we propose an additive update algorithm, that has faster
computational speed than the algorithm of D.D. Lee and H.S. Seung.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5651</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5651</id><created>2012-09-25</created><authors><author><keyname>Sharma</keyname><forenames>Abhigyan</forenames></author><author><keyname>Venkataramani</keyname><forenames>Arun</forenames></author><author><keyname>Rocha</keyname><forenames>Antonio A.</forenames></author></authors><title>Pros &amp; Cons of Model-based Bandwidth Control for Client-assisted Content
  Delivery</title><categories>cs.NI</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key challenge in \cacd\ is determining how to allocate limited server
bandwidth across a large number of files being concurrently served so as to
optimize global performance and cost objectives. In this paper, we present a
comprehensive experimental evaluation of strategies to control server bandwidth
allocation. As part of this effort, we introduce a new {\em model-based}
control approach that relies on an accurate yet concise &quot;cheat sheet&quot; based on
a priori offline measurement to predict swarm performance as a function of the
server bandwidth and other swarm parameters. Our evaluation using a prototype
system, \cs, instantiating static, dynamic, and model-based controllers shows
that static and dynamic controllers can both be suboptimal due to different
reasons. In comparison, a model-based approach consistently outperforms both
static and dynamic approaches provided it has access to detailed measurements
in the regime of interest. Nevertheless, the broad applicability of a
model-based approach may be limited in practice because of the overhead of
developing and maintaining a comprehensive measurement-based model of swarm
performance in each regime of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5656</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5656</id><created>2012-09-25</created><authors><author><keyname>G&#xf3;mez</keyname><forenames>Vicen&#xe7;</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert J.</forenames></author></authors><title>Learning Price-Elasticity of Smart Consumers in Power Distribution
  Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 5 figures, IEEE SmartGridComm 2012</comments><acm-class>C.2.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demand Response is an emerging technology which will transform the power grid
of tomorrow. It is revolutionary, not only because it will enable peak load
shaving and will add resources to manage large distribution systems, but mainly
because it will tap into an almost unexplored and extremely powerful pool of
resources comprised of many small individual consumers on distribution grids.
However, to utilize these resources effectively, the methods used to engage
these resources must yield accurate and reliable control. A diversity of
methods have been proposed to engage these new resources. As opposed to direct
load control, many methods rely on consumers and/or loads responding to
exogenous signals, typically in the form of energy pricing, originating from
the utility or system operator. Here, we propose an open loop
communication-lite method for estimating the price elasticity of many customers
comprising a distribution system. We utilize a sparse linear regression method
that relies on operator-controlled, inhomogeneous minor price variations, which
will be fair to all the consumers. Our numerical experiments show that reliable
estimation of individual and thus aggregated instantaneous elasticities is
possible. We describe the limits of the reliable reconstruction as functions of
the three key parameters of the system: (i) ratio of the number of
communication slots (time units) per number of engaged consumers; (ii) level of
sparsity (in consumer response); and (iii) signal-to-noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5663</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5663</id><created>2012-09-25</created><authors><author><keyname>Dufour-Lussier</keyname><forenames>Valmi</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Ber</keyname><forenames>Florence Le</forenames><affiliation>INRIA Lorraine - LORIA, LHyGeS</affiliation></author><author><keyname>Lieber</keyname><forenames>Jean</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Meilender</keyname><forenames>Thomas</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Nauer</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Semi-automatic annotation process for procedural texts: An application
  on cooking recipes</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>Cooking with Computers workshop (ECAI 2012) (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taaable is a case-based reasoning system that adapts cooking recipes to user
constraints. Within it, the preparation part of recipes is formalised as a
graph. This graph is a semantic representation of the sequence of instructions
composing the cooking process and is used to compute the procedure adaptation,
conjointly with the textual adaptation. It is composed of cooking actions and
ingredients, among others, represented as vertices, and semantic relations
between those, shown as arcs, and is built automatically thanks to natural
language processing. The results of the automatic annotation process is often a
disconnected graph, representing an incomplete annotation, or may contain
errors. Therefore, a validating and correcting step is required. In this paper,
we present an existing graphic tool named \kcatos, conceived for representing
and editing decision trees, and show how it has been adapted and integrated in
WikiTaaable, the semantic wiki in which the knowledge used by Taaable is
stored. This interface provides the wiki users with a way to correct the case
representation of the cooking process, improving at the same time the quality
of the knowledge about cooking procedures stored in WikiTaaable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5664</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5664</id><created>2012-09-25</created><authors><author><keyname>Dufour-Lussier</keyname><forenames>Valmi</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Ber</keyname><forenames>Florence Le</forenames><affiliation>INRIA Lorraine - LORIA, LHyGeS</affiliation></author><author><keyname>Lieber</keyname><forenames>Jean</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Extension du formalisme des flux op\'erationnels par une alg\`ebre
  temporelle</title><categories>cs.AI cs.LO</categories><proxy>ccsd</proxy><journal-ref>Sixi\`emes Journ\'ees de l'Intelligence Artificielle Fondamentale
  (JIAF) (2012) 133-142</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Workflows constitute an important language to represent knowledge about
processes, but also increasingly to reason on such knowledge. On the other
hand, there is a limit to which time constraints between activities can be
expressed. Qualitative interval algebras can model processes using finer
temporal relations, but they cannot reproduce all workflow patterns. This paper
defines a common ground model-theoretical semantics for both workflows and
interval algebras, making it possible for reasoning systems working with either
to interoperate. Thanks to this, interesting properties and inferences can be
defined, both on workflows and on an extended formalism combining workflows
with interval algebras. Finally, similar formalisms proposing a sound formal
basis for workflows and extending them are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5669</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5669</id><created>2012-09-25</created><updated>2014-08-26</updated><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>Institut de Math&#xe9;matiques de Jussieu, CNRS et Universit&#xe9; Paris</affiliation></author></authors><title>Ambiguity of {\omega}-Languages of Turing Machines</title><categories>cs.LO cs.CC math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  28, 2014) lmcs:765</journal-ref><doi>10.2168/LMCS-10(3:12)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An {\omega}-language is a set of infinite words over a finite alphabet X. We
consider the class of recursive {\omega}-languages, i.e. the class of
{\omega}-languages accepted by Turing machines with a B\&quot;uchi acceptance
condition, which is also the class {\Sigma}11 of (effective) analytic subsets
of X{\omega} for some finite alphabet X. We investigate here the notion of
ambiguity for recursive {\omega}-languages with regard to acceptance by B\&quot;uchi
Turing machines. We first present in detail essentials on the literature on
{\omega}-languages accepted by Turing Machines. Then we give a complete and
broad view on the notion of ambiguity and unambiguity of B\&quot;uchi Turing
machines and of the {\omega}-languages they accept. To obtain our new results,
we make use of results and methods of effective descriptive set theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5683</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5683</id><created>2012-09-25</created><updated>2012-12-07</updated><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author><author><keyname>Anteneodo</keyname><forenames>Celia</forenames></author></authors><title>Role of conviction in nonequilibrium models of opinion formation</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>7 pages, 5 figures, to appear in PRE</comments><journal-ref>Phys. Rev. E 86, 061127 (2012)</journal-ref><doi>10.1103/PhysRevE.86.061127</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the critical behavior of a class of discrete opinion models in the
presence of disorder. Within this class, each agent opinion takes a discrete
value ($\pm 1$ or 0) and its time evolution is ruled by two terms, one
representing agent-agent interactions and the other the degree of conviction or
persuasion (a self-interaction). The mean-field limit, where each agent can
interact evenly with any other, is considered. Disorder is introduced in the
strength of both interactions, with either quenched or annealed random
variables. With probability $p$ (1-$p$), a pairwise interaction reflects a
negative (positive) coupling, while the degree of conviction also follows a
binary probability distribution (two different discrete probability
distributions are considered). Numerical simulations show that a
non-equilibrium continuous phase transition, from a disordered state to a state
with a prevailing opinion, occurs at a critical point $p_{c}$ that depends on
the distribution of the convictions, the transition being spoiled in some
cases. We also show how the critical line, for each model, is affected by the
update scheme (either parallel or sequential) as well as by the kind of
disorder (either quenched or annealed).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5695</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5695</id><created>2012-09-12</created><authors><author><keyname>J</keyname><forenames>Rehna. V.</forenames></author><author><keyname>K</keyname><forenames>Jeyakumar. M.</forenames></author></authors><title>Hybrid Approaches to Image Coding: A Review</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 2, No. 7, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the digital world is most focused on storage space and speed. With
the growing demand for better bandwidth utilization, efficient image data
compression techniques have emerged as an important factor for image data
transmission and storage. To date, different approaches to image compression
have been developed like the classical predictive coding, popular transform
coding and vector quantization. Several second generation coding schemes or the
segmentation based schemes are also gaining popularity. Practically efficient
compression systems based on hybrid coding which combines the advantages of
different traditional methods of image coding have also been developed over the
years. In this paper, different hybrid approaches to image compression are
discussed. Hybrid coding of images, in this context, deals with combining two
or more traditional approaches to enhance the individual methods and achieve
better-quality reconstructed images with higher compression ratio. Literature
on hybrid techniques of image coding over the past years is also reviewed. An
attempt is made to highlight the neuro-wavelet approach for enhancing coding
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5698</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5698</id><created>2012-09-25</created><updated>2012-12-17</updated><authors><author><keyname>Li</keyname><forenames>Youfa</forenames></author><author><keyname>Chen</keyname><forenames>Qiuhui</forenames></author><author><keyname>Qian</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author></authors><title>Sampling Error Analysis and Properties of Non-bandlimited Signals That
  Are Reconstructed by Generalized Sinc Functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently efforts have been made to use generalized sinc functions to
perfectly reconstruct various kinds of non-bandlimited signals. As a
consequence, perfect reconstruction sampling formulas have been established
using such generalized sinc functions. This article studies the error of the
reconstructed non-bandlimited signal when an adaptive truncation scheme is
employed. Further, when there are noises present in the samples, estimation on
the expectation and variance of the error pertinent to the reconstructed signal
is also given. Finally discussed are the reproducing properties and the Sobolev
smoothness of functions in the space of non-bandlimited signals that admits
such a sampling formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5699</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5699</id><created>2012-09-25</created><authors><author><keyname>Czabarka</keyname><forenames>Eva</forenames></author><author><keyname>Sz&#xe9;kely</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Vision</keyname><forenames>Todd</forenames></author></authors><title>Minimizing the number of episodes and Gallai's theorem on intervals</title><categories>math.CO cs.DM</categories><msc-class>05C05, 05C70, 05C85, 92B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1996, Guigo et al. [Mol. Phylogenet. Evol., 6 (1996), 189-203] posed the
following problem: for a given species tree and a number of gene trees, what is
the minimum number of duplication episodes, where several genes could have
undergone duplication together to generate the observed situation. (Gene order
is neglected, but duplication of genes could have happened only on certain
segments that duplicated). We study two versions of this problem, one of which
was algorithmically solved not long ago by Bansal and Eulenstein
[Bioinformatics, 24(13), (2008), 132-138]. We provide min-max theorems for both
versions that generalize Gallai's archetypal min-max theorem on intervals,
allowing simplified proofs to the correctness of the algorithms (as it always
happens with duality) and deeper understanding. An interesting feature of our
approach is that its recursive nature requires a generality that
bioinformaticians attempting to solve a particular problem usually avoid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5715</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5715</id><created>2012-09-25</created><authors><author><keyname>Sharma</keyname><forenames>Abhigyan</forenames></author><author><keyname>Venkataramani</keyname><forenames>Arun</forenames></author><author><keyname>Sitaraman</keyname><forenames>Ramesh</forenames></author></authors><title>Distributing Content Simplifies ISP Traffic Engineering</title><categories>cs.NI</categories><comments>12 pages, 13 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several major Internet service providers (e.g., Level-3, AT&amp;T, Verizon) today
also offer content distribution services. The emergence of such &quot;Network-CDNs&quot;
(NCDNs) are driven by market forces that place more value on content services
than just carrying the bits. NCDNs are also necessitated by the need to reduce
the cost of carrying ever-increasing volumes of traffic across their backbones.
An NCDN has the flexibility to determine both where content is placed and how
traffic is routed within the network. However NCDNs today continue to treat
traffic engineering independently from content placement and request
redirection decisions. In this paper, we investigate the interplay between
content distribution strategies and traffic engineering and ask how an NCDN
should engineer traffic in a content-aware manner. Our experimental analysis,
based on traces from a large content distribution network and real ISP
topologies, shows that effective content placement can significantly simplify
traffic engineering and in most cases obviate the need to engineer NCDN traffic
all together! Further, we show that simple demand-oblivious schemes for routing
and placement such as InverseCap and LRU suffice as they achieve network costs
that are close to the best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5730</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5730</id><created>2012-09-25</created><updated>2012-10-19</updated><authors><author><keyname>Hu</keyname><forenames>Donglin</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author></authors><title>The Feasibility of Scalable Video Streaming over Femtocell Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider femtocell CR networks, where femto base stations
(FBS) are deployed to greatly improve network coverage and capacity. We
investigate the problem of generic data multicast in femtocell networks. We
reformulate the resulting MINLP problem into a simpler form, and derive upper
and lower performance bounds. Then we consider three typical connection
scenarios in the femtocell network, and develop optimal and near-optimal
algorithms for the three scenarios. Second, we tackle the problem of streaming
scalable videos in femtocell CR networks. A framework is developed to captures
the key design issues and trade-offs with a stochastic programming problem
formulation. In the case of a single FBS, we develop an optimum-achieving
distributed algorithm, which is shown also optimal for the case of multiple
non-interfering FBS's. In the case of interfering FBS's, we develop a greedy
algorithm that can compute near-opitmal solutions, and prove a closed-form
lower bound on its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5756</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5756</id><created>2012-09-25</created><authors><author><keyname>Souli</keyname><forenames>Sameh</forenames></author><author><keyname>Lachiri</keyname><forenames>Zied</forenames></author></authors><title>Environmental Sounds Spectrogram Classification using Log-Gabor Filters
  and Multiclass Support Vector Machines</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper presents novel approaches for efficient feature extraction using
environmental sound magnitude spectrogram. We propose approach based on the
visual domain. This approach included three methods. The first method is based
on extraction for each spectrogram a single log-Gabor filter followed by mutual
information procedure. In the second method, the spectrogram is passed by the
same steps of the first method but with an averaged bank of 12 log-Gabor
filter. The third method consists of spectrogram segmentation into three
patches, and after that for each spectrogram patch we applied the second
method. The classification results prove that the second method is the most
efficient in our environmental sound classification system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5762</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5762</id><created>2012-09-25</created><updated>2012-10-16</updated><authors><author><keyname>Piemontese</keyname><forenames>Amina</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author></authors><title>Nonbinary Spatially-Coupled LDPC Codes on the Binary Erasure Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE International Conference on Communications 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the asymptotic performance of nonbinary spatially-coupled
low-density parity-check (SC-LDPC) codes built on the general linear group,
when the transmission takes place over the binary erasure channel. We propose
an efficient method to derive an upper bound to the maximum a posteriori
probability (MAP) threshold for nonbinary LDPC codes, and observe that the MAP
performance of regular LDPC codes improves with the alphabet size. We then
consider nonbinary SC-LDPC codes. We show that the same threshold saturation
effect experienced by binary SC-LDPC codes occurs for the nonbinary codes,
hence we conjecture that the BP threshold for large termination length
approaches the MAP threshold of the underlying regular ensemble.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5765</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5765</id><created>2012-09-25</created><authors><author><keyname>Mote</keyname><forenames>Kevin</forenames></author></authors><title>Fast Point-Feature Label Placement for Dynamic Visualizations (2007)</title><categories>cs.DS</categories><journal-ref>Information Visualization (2007) 6, 249-260</journal-ref><doi>10.1057/PALGRAVE.IVS.9500163</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a fast approach to automatic point label de-confliction
on interactive maps. The general Map Labeling problem is NP-hard and has been
the subject of much study for decades. Computerized maps have introduced
interactive zooming and panning, which has intensified the problem. Providing
dynamic labels for such maps typically requires a time-consuming pre-processing
phase. In the realm of visual analytics, however, the labeling of interactive
maps is further complicated by the use of massive datasets laid out in
arbitrary configurations, thus rendering reliance on a pre-processing phase
untenable. This paper offers a method for labeling point-features on dynamic
maps in real time without pre-processing. The algorithm presented is efficient,
scalable, and exceptionally fast; it can label interactive charts and diagrams
at speeds of multiple frames per second on maps with tens of thousands of
nodes. To accomplish this, the algorithm employs a novel geometric
de-confliction approach, the 'trellis strategy,' along with a unique label
candidate cost analysis to determine the 'least expensive' label configuration.
The speed and scalability of this approach make it well-suited for visual
analytic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5766</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5766</id><created>2012-09-25</created><authors><author><keyname>Mote</keyname><forenames>Kevin</forenames></author></authors><title>Fast Point-Feature Label Placement for Dynamic Visualizations (Thesis)</title><categories>cs.DS</categories><comments>Master's Thesis, Washington State University</comments><doi>10.1057/PALGRAVE.IVS.9500163</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a fast approach to automatic point label de-confliction
on interactive maps. The general Map Labeling problem is NP-hard and has been
the subject of much study for decades. Computerized maps have introduced
interactive zooming and panning, which has intensified the problem. Providing
dynamic labels for such maps typically requires a time-consuming pre-processing
phase. In the realm of visual analytics, however, the labeling of interactive
maps is further complicated by the use of massive datasets laid out in
arbitrary configurations, thus rendering reliance on a pre-processing phase
untenable. This paper offers a method for labeling point-features on dynamic
maps in real time without pre-processing. The algorithm presented is efficient,
scalable, and exceptionally fast; it can label interactive charts and diagrams
at speeds of multiple frames per second on maps with tens of thousands of
nodes. To accomplish this, the algorithm employs a novel geometric
de-confliction approach, the 'trellis strategy,' along with a unique label
candidate cost analysis to determine the &quot;least expensive&quot; label configuration.
The speed and scalability of this approach make it well-suited for visual
analytic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5770</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5770</id><created>2012-09-22</created><authors><author><keyname>Cremene</keyname><forenames>Ligia C.</forenames></author><author><keyname>Dumitrescu</keyname><forenames>D.</forenames></author><author><keyname>Nagy</keyname><forenames>Reka</forenames></author><author><keyname>Gasko</keyname><forenames>Noemi</forenames></author></authors><title>Cognitive Radio Simultaneous Spectrum Access/ One-shot Game Modelling</title><categories>cs.GT cs.NI nlin.AO</categories><comments>6 double-column pages, 8 figures, CSNDSP 2012. arXiv admin note:
  substantial text overlap with arXiv:1207.3365, arXiv:1209.5387,
  arXiv:1209.5013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to asses simultaneous spectrum access situations that
may occur in Cognitive Radio (CR) environments. The approach is that of one
shot, noncooperative games describing CR interactions. Open spectrum access
scenarios are modelled based on continuous and discrete reformulations of the
Cournot game theoretical model. CR interaction situations are described by Nash
and Pareto equilibria. Also, the heterogeneity of players is captured by the
new concept of joint Nash-Pareto equilibrium, allowing CRs to be biased toward
different types of equilibrium. Numerical simulations reveal equilibrium
situations that may be reached in simultaneous access scenarios of two and
three users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5773</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5773</id><created>2012-09-25</created><authors><author><keyname>Macedo</keyname><forenames>Nuno</forenames></author><author><keyname>Cunha</keyname><forenames>Alcino</forenames></author></authors><title>Automatic Unbounded Verification of Alloy Specifications with Prover9</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alloy is an increasingly popular lightweight specification language based on
relational logic. Alloy models can be automatically verified within a bounded
scope using off-the-shelf SAT solvers. Since false assertions can usually be
disproved using small counter-examples, this approach suffices for most
applications. Unfortunately, it can sometimes lead to a false sense of
security, and in critical applications a more traditional unbounded proof may
be required. The automatic theorem prover Prover9 has been shown to be
particularly effective for proving theorems of relation algebras [7], a
quantifier-free (or point-free) axiomatization of a fragment of relational
logic. In this paper we propose a translation from Alloy specifications to fork
algebras (an extension of relation algebras with the same expressive power as
relational logic) which enables their unbounded verification in Prover9. This
translation covers not only logic assertions, but also the structural aspects
(namely type declarations), and was successfully implemented and applied to
several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5779</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5779</id><created>2012-09-25</created><updated>2013-02-04</updated><authors><author><keyname>Bienstock</keyname><forenames>Daniel</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Harnett</keyname><forenames>Sean</forenames></author></authors><title>Chance Constrained Optimal Power Flow: Risk-Aware Network Control under
  Uncertainty</title><categories>math.OC cs.SY physics.soc-ph</categories><comments>36 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When uncontrollable resources fluctuate, Optimum Power Flow (OPF), routinely
used by the electric power industry to re-dispatch hourly controllable
generation (coal, gas and hydro plants) over control areas of transmission
networks, can result in grid instability, and, potentially, cascading outages.
This risk arises because OPF dispatch is computed without awareness of major
uncertainty, in particular fluctuations in renewable output. As a result, grid
operation under OPF with renewable variability can lead to frequent conditions
where power line flow ratings are significantly exceeded. Such a condition,
which is borne by simulations of real grids, would likely resulting in
automatic line tripping to protect lines from thermal stress, a risky and
undesirable outcome which compromises stability. Smart grid goals include a
commitment to large penetration of highly fluctuating renewables, thus calling
to reconsider current practices, in particular the use of standard OPF. Our
Chance Constrained (CC) OPF corrects the problem and mitigates dangerous
renewable fluctuations with minimal changes in the current operational
procedure. Assuming availability of a reliable wind forecast parameterizing the
distribution function of the uncertain generation, our CC-OPF satisfies all the
constraints with high probability while simultaneously minimizing the cost of
economic re-dispatch. CC-OPF allows efficient implementation, e.g. solving a
typical instance over the 2746-bus Polish network in 20 seconds on a standard
laptop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5785</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5785</id><created>2012-09-25</created><updated>2012-12-05</updated><authors><author><keyname>Truhachev</keyname><forenames>Dmitri</forenames></author><author><keyname>Schlegel</keyname><forenames>Christian</forenames></author></authors><title>Coupling Data Transmission for Capacity-Achieving Multiple-Access
  Communications</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a signaling format where information is modulated via a
superposition of independent data streams. Each data stream is formed by
replication and permutation of encoded information bits. The relations between
data bits and modulation symbols transmitted over the channel can be
represented in the form of a sparse graph. The modulated streams are
transmitted with a time offset enabling spatial coupling of the sparse
modulation graphs. We prove that a two-stage demodulation/decoding method, in
which iterative demodulation based on symbol estimation and interference
cancellation is followed by parallel error correction decoding, achieves
capacity on the additive white Gaussian noise (AWGN) channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5791</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5791</id><created>2012-09-25</created><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>DuBois</keyname><forenames>Christopher</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Smyth</keyname><forenames>Padhraic</forenames></author></authors><title>Windows into Relational Events: Data Structures for Contiguous
  Subsequences of Edges</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of analyzing social network data sets in which the
edges of the network have timestamps, and we wish to analyze the subgraphs
formed from edges in contiguous subintervals of these timestamps. We provide
data structures for these problems that use near-linear preprocessing time,
linear space, and sublogarithmic query time to handle queries that ask for the
number of connected components, number of components that contain cycles,
number of vertices whose degree equals or is at most some predetermined value,
number of vertices that can be reached from a starting set of vertices by
time-increasing paths, and related queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5800</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5800</id><created>2012-09-25</created><authors><author><keyname>Kuhn</keyname><forenames>Adrian</forenames></author><author><keyname>Murphy</keyname><forenames>Gail C.</forenames></author></authors><title>Lessons Learned from Evaluating MDE Abstractions in an Industry Case
  Study</title><categories>cs.SE</categories><comments>In Proceedings of EESMOD 2012 Workshop (to be published)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent empirical study we found that evaluating abstractions of
Model-Driven Engineering (MDE) is not as straight forward as it might seem. In
this paper, we report on the challenges that we as researchers faced when we
conducted the aforementioned field study. In our study we found that modeling
happens within a complex ecosystem of different people working in different
roles. An empirical evaluation should thus mind the ecosystem, that is, focus
on both technical and human factors. In the following, we present and discuss
five lessons learnt from our recent work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5803</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5803</id><created>2012-09-25</created><updated>2013-06-30</updated><authors><author><keyname>Li</keyname><forenames>Boyu</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Full-Diversity Precoding Design of Bit-Interleaved Coded Multiple
  Beamforming with Orthogonal Frequency Division Multiplexing</title><categories>cs.IT math.IT</categories><comments>accepted to journal. arXiv admin note: text overlap with
  arXiv:1109.3510</comments><journal-ref>IEEE TCOM, Vol. 61, No. 6, Pages 2432-2445, Jun. 2013</journal-ref><doi>10.1109/TCOMM.2013.041113.120688</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-Input Multi-Output (MIMO) techniques have been incorporated with
Orthogonal Frequency Division Multiplexing (OFDM) for broadband wireless
communication systems. Bit-Interleaved Coded Multiple Beamforming (BICMB) can
achieve both spatial diversity and spatial multiplexing for flat fading MIMO
channels. For frequency selective fading MIMO channels, BICMB with OFDM
(BICMB-OFDM) can be employed to provide both spatial diversity and multipath
diversity, making it an important technique. In our previous work, the
subcarrier grouping technique was applied to combat the negative effect of
subcarrier correlation. It was also proved that full diversity of BICMB-OFDM
with Subcarrier Grouping (BICMB-OFDM-SG) can be achieved within the condition
R_cSL&lt;=1, where R_c, S, and L are the code rate, the number of parallel streams
at each subcarrier, and the number of channel taps, respectively. The full
diversity condition implies that if S increases, R_c may have to decrease to
maintain full diversity. As a result, increasing the number of parallel streams
may not improve the total transmission rate. In this paper, the precoding
technique is employed to overcome the full diversity restriction issue of
R_cSL&lt;=1 for BICMB-OFDM-SG. First, the diversity analysis of precoded
BICMB-OFDM-SG is carried out. Then, the full-diversity precoding design is
developed with the minimum achievable decoding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5805</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5805</id><created>2012-09-25</created><updated>2012-11-08</updated><authors><author><keyname>Arvelo</keyname><forenames>Eduardo</forenames></author><author><keyname>Kim</keyname><forenames>Eric</forenames></author><author><keyname>Martins</keyname><forenames>Nuno C.</forenames></author></authors><title>Memoryless Control Design for Persistent Surveillance under Safety
  Constraints</title><categories>cs.SY cs.RO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the design of time-invariant memoryless control
policies for robots that move in a finite two- dimensional lattice and are
tasked with persistent surveillance of an area in which there are forbidden
regions. We model each robot as a controlled Markov chain whose state comprises
its position in the lattice and the direction of motion. The goal is to find
the minimum number of robots and an associated time-invariant memoryless
control policy that guarantees that the largest number of states are
persistently surveilled without ever visiting a forbidden state. We propose a
design method that relies on a finitely parametrized convex program inspired by
entropy maximization principles. Numerical examples are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5807</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5807</id><created>2012-09-25</created><updated>2014-02-06</updated><authors><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author></authors><title>Fundamental Limits of Caching</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, pp. 2856 - 2867,
  May 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching is a technique to reduce peak traffic rates by prefetching popular
content into memories at the end users. Conventionally, these memories are used
to deliver requested content in part from a locally cached copy rather than
through the network. The gain offered by this approach, which we term local
caching gain, depends on the local cache size (i.e, the memory available at
each individual user). In this paper, we introduce and exploit a second,
global, caching gain not utilized by conventional caching schemes. This gain
depends on the aggregate global cache size (i.e., the cumulative memory
available at all users), even though there is no cooperation among the users.
  To evaluate and isolate these two gains, we introduce an
information-theoretic formulation of the caching problem focusing on its basic
structure. For this setting, we propose a novel coded caching scheme that
exploits both local and global caching gains, leading to a multiplicative
improvement in the peak rate compared to previously known schemes. In
particular, the improvement can be on the order of the number of users in the
network. Moreover, we argue that the performance of the proposed scheme is
within a constant factor of the information-theoretic optimum for all values of
the problem parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5809</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5809</id><created>2012-09-25</created><authors><author><keyname>K&#xfc;&#xe7;&#xfc;ktun&#xe7;</keyname><forenames>Onur</forenames></author><author><keyname>Saule</keyname><forenames>Erik</forenames></author><author><keyname>Kaya</keyname><forenames>Kamer</forenames></author><author><keyname>&#xc7;ataly&#xfc;rek</keyname><forenames>&#xdc;mit V.</forenames></author></authors><title>Diversifying Citation Recommendations</title><categories>cs.IR cs.DL cs.SI</categories><comments>19 pages, manuscript under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Literature search is arguably one of the most important phases of the
academic and non-academic research. The increase in the number of published
papers each year makes manual search inefficient and furthermore insufficient.
Hence, automatized methods such as search engines have been of interest in the
last thirty years. Unfortunately, these traditional engines use keyword-based
approaches to solve the search problem, but these approaches are prone to
ambiguity and synonymy. On the other hand, bibliographic search techniques
based only on the citation information are not prone to these problems since
they do not consider textual similarity. For many particular research areas and
topics, the amount of knowledge to humankind is immense, and obtaining the
desired information is as hard as looking for a needle in a haystack.
Furthermore, sometimes, what we are looking for is a set of documents where
each one is different than the others, but at the same time, as a whole we want
them to cover all the important parts of the literature relevant to our search.
This paper targets the problem of result diversification in citation-based
bibliographic search. It surveys a set of techniques which aim to find a set of
papers with satisfactory quality and diversity. We enhance these algorithms
with a direction-awareness functionality to allow the users to reach either
old, well-cited, well-known research papers or recent, less-known ones. We also
propose a set of novel techniques for a better diversification of the results.
All the techniques considered are compared by performing a rigorous
experimentation. The results show that some of the proposed techniques are very
successful in practice while performing a search in a bibliographic database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5818</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5818</id><created>2012-09-25</created><updated>2012-11-14</updated><authors><author><keyname>Pattabiraman</keyname><forenames>Bharath</forenames></author><author><keyname>Patwary</keyname><forenames>Md. Mostofa Ali</forenames></author><author><keyname>Gebremedhin</keyname><forenames>Assefaw H.</forenames></author><author><keyname>Liao</keyname><forenames>Wei-keng</forenames></author><author><keyname>Choudhary</keyname><forenames>Alok</forenames></author></authors><title>Fast Algorithms for the Maximum Clique Problem on Massive Sparse Graphs</title><categories>cs.DS cs.IR</categories><comments>15 pages (including 2-page appendix), 5 tables, 4 figures</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum clique problem is a well known NP-Hard problem with applications
in data mining, network analysis, informatics, and many other areas. Although
there exist several algorithms with acceptable runtimes for certain classes of
graphs, many of them are infeasible for massive graphs. We present a new exact
algorithm that employs novel pruning techniques to very quickly find maximum
cliques in large sparse graphs. Extensive experiments on several types of
synthetic and real-world graphs show that our new algorithm is up to several
orders of magnitude faster than existing algorithms for most instances. We also
present a heuristic variant that runs orders of magnitude faster than the exact
algorithm, while providing optimal or near-optimal solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5821</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5821</id><created>2012-09-25</created><updated>2013-11-16</updated><authors><author><keyname>Koutis</keyname><forenames>Ioannis</forenames></author><author><keyname>Levin</keyname><forenames>Alex</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author></authors><title>Faster spectral sparsification and numerical algorithms for SDD matrices</title><categories>cs.DS</categories><comments>This work subsumes the results reported in our STACS 2012 paper
  &quot;Improved spectral sparsification and numerical algorithms for SDD matrices&quot;.
  The first two algorithms are identical but the fastest O(mloglog n) time
  algorithm applies now for graphs of average degree log^5 n and more,
  improving upon the average degree n^c, c&gt;0 of our previous work. Version 2
  fixes a few typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study algorithms for spectral graph sparsification. The input is a graph
$G$ with $n$ vertices and $m$ edges, and the output is a sparse graph
$\tilde{G}$ that approximates $G$ in an algebraic sense. Concretely, for all
vectors $x$ and any $\epsilon&gt;0$, $\tilde{G}$ satisfies $$ (1-\epsilon) x^T L_G
x \leq x^T L_{\tilde{G}} x \leq (1+\epsilon) x^T L_G x, $$ where $L_G$ and
$L_{\tilde{G}}$ are the Laplacians of $G$ and $\tilde{G}$ respectively. We show
that the fastest known algorithm for computing a sparsifier with $O(n\log
n/\epsilon^2)$ edges can actually run in $\tilde{O}(m\log^2 n)$ time, an
$O(\log n)$ factor faster than before. We also present faster sparsification
algorithms for slightly dense graphs. Specifically, we give an algorithm that
runs in $\tilde{O}(m\log n)$ time and generates a sparsifier with
$\tilde{O}(n\log^3{n}/\epsilon^2)$ edges. This implies that a sparsifier with
$O(n\log n/\epsilon^2)$ edges can be computed in $\tilde{O}(m\log n)$ time for
graphs with more than $O(n\log^4 n)$ edges. We also give an $\tilde{O}(m)$ time
algorithm for graphs with more than $n\log^5 n (\log \log n)^3$ edges of
polynomially bounded weights, and an $O(m)$ algorithm for unweighted graphs
with more than $n\log^8 n (\log \log n)^3 $ edges and $n\log^{10} n (\log \log
n)^5$ edges in the weighted case. The improved sparsification algorithms are
employed to accelerate linear system solvers and algorithms for computing
fundamental eigenvectors of slightly dense SDD matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5826</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5826</id><created>2012-09-26</created><authors><author><keyname>Peters</keyname><forenames>Jorg</forenames></author></authors><title>Refinability of splines from lattice Voronoi cells</title><categories>math.NA cs.CV</categories><msc-class>41A15, 65D07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Splines can be constructed by convolving the indicator function of the
Voronoi cell of a lattice. This paper presents simple criteria that imply that
only a small subset of such spline families can be refined: essentially the
well-known box splines and tensor-product splines. Among the many non-refinable
constructions are hex-splines and their generalization to non-Cartesian
lattices. An example shows how non-refinable splines can exhibit increased
approximation error upon refinement of the lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5828</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5828</id><created>2012-09-26</created><updated>2015-02-17</updated><authors><author><keyname>Huang</keyname><forenames>Lingxiao</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author></authors><title>Approximating the Expected Values for Combinatorial Optimization
  Problems over Stochastic Points</title><categories>cs.DS</categories><comments>30 pages. This version has several new results, a new title, and
  completely subsumes the previous versions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the stochastic geometry model where the location of each node is
a random point in a given metric space, or the existence of each node is
uncertain. We study the problems of computing the expected lengths of several
combinatorial or geometric optimization problems over stochastic points,
including closest pair, minimum spanning tree, $k$-clustering, minimum perfect
matching, and minimum cycle cover. We also consider the problem of estimating
the probability that the length of closest pair, or the diameter, is at most,
or at least, a given threshold. Most of the above problems are known to be
$\sharpP$-hard. We obtain FPRAS (Fully Polynomial Randomized Approximation
Scheme) for most of them in both the existential and locational uncertainty
models. Our result for stochastic minimum spanning trees in the locational
uncertain model improves upon the previously known constant factor
approximation algorithm. Our results for other problems are the first known to
the best of our knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5829</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5829</id><created>2012-09-26</created><authors><author><keyname>Liu</keyname><forenames>Huaping</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>de Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Zhao</keyname><forenames>Yuping</forenames></author><author><keyname>Sun</keyname><forenames>Fan</forenames></author><author><keyname>Thai</keyname><forenames>Chan Dai Truyen</forenames></author></authors><title>Transmission Schemes for Four-Way Relaying in Wireless Cellular Systems</title><categories>cs.IT math.IT</categories><comments>20 pages,10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-way relaying in wireless systems has initiated a large research effort
during the past few years. While one-way relay with a single data flow
introduces loss in spectral efficiency due to its half-duplex operation,
two-way relaying based on wireless network coding regains part of this loss by
simultaneously processing the two data flows. In a broader perspective, the
two-way traffic pattern is rather limited and it is of interest to investigate
other traffic patterns where such a simultaneous processing of information
flows can bring performance advantage. In this paper we consider a scenario
beyond the usual two-way relaying: a four-way relaying, where each of the two
Mobile Stations (MSs) has a two-way connection to the same Base Station (BS),
while each connection is through a dedicated Relay Station (RS). While both RSs
are in the range of the same BS, they are assumed to have antipodal positions
within the cell, such that they do not interfere with each other. We introduce
and analyze a two-phase transmission scheme to serve the four-way traffic
pattern defined in this scenario. Each phase consists of combined broadcast and
multiple access. We analyze the achievable rate region of the new schemes for
two different operational models for the RS, Decode-and-Forward (DF) and
Amplify-and-Forward (AF), respectively. We compare the performance with a
state-of-the-art reference scheme, time sharing is used between the two MSs,
while each MS is served through a two-way relaying scheme. The results indicate
that, when the RS operates in a DF mode, the achievable rate regions are
significantly enlarged. On the other hand, for AF relaying, the gains are
rather modest. The practical implication of the presented work is a novel
insight on how to improve the spatial reuse in wireless cellular networks by
coordinating the transmissions of the antipodal relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5833</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5833</id><created>2012-09-26</created><updated>2012-10-11</updated><authors><author><keyname>Konoshima</keyname><forenames>Makiko</forenames></author><author><keyname>Noma</keyname><forenames>Yui</forenames></author></authors><title>Locality-Sensitive Hashing with Margin Based Feature Selection</title><categories>cs.LG cs.IR</categories><comments>9 pages, 6 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a learning method with feature selection for Locality-Sensitive
Hashing. Locality-Sensitive Hashing converts feature vectors into bit arrays.
These bit arrays can be used to perform similarity searches and personal
authentication. The proposed method uses bit arrays longer than those used in
the end for similarity and other searches and by learning selects the bits that
will be used. We demonstrated this method can effectively perform optimization
for cases such as fingerprint images with a large number of labels and
extremely few data that share the same labels, as well as verifying that it is
also effective for natural images, handwritten digits, and speech features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5836</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5836</id><created>2012-09-26</created><authors><author><keyname>Farmanbar</keyname><forenames>Mina</forenames></author><author><keyname>Chefranov</keyname><forenames>Alexander G.</forenames></author></authors><title>Investigation of Hill Cipher Modifications Based on Permutation and
  Iteration</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two recent Hill cipher modifications which iteratively use interweaving and
interlacing are considered. We show that strength of these ciphers is due to
non-linear transformation used in them (bit-level permutations). Impact of
number of iterations on the avalanche effect is investigated. We propose two
Hill cipher modifications using column swapping and arbitrary permutation with
significantly less computational complexity (2 iterations are used versus 16).
The proposed modifications decrease encryption time while keeping the strength
of the ciphers. Numerical experiments for two proposed ciphers indicate that
they can provide a substantial avalanche effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5837</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5837</id><created>2012-09-26</created><authors><author><keyname>Vol</keyname><forenames>E. D.</forenames></author></authors><title>Three &quot;quantum&quot; models of competition and cooperation in interacting
  biological populations and social groups</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In present paper we propose the consistent statistical approach which
appropriate for a number of models describing both behavior of biological
populations and various social groups interacting with each other.The approach
proposed based on the ideas of quantum theory of open systems (QTOS) and allows
one to account explicitly both discreteness of a system variables and their
fluctuations near mean values.Therefore this approach can be applied also for
the description of small populations where standard dynamical methods are
failed. We study in detail three typical models of interaction between
populations and groups: 1) antagonistic struggle between two populations 2)
cooperation (or, more precisely, obligatory mutualism) between two species 3)
the formation of coalition between two feeble groups in their conflict with
third one that is more powerful . The models considered in a sense are mutually
complementary and include the most types of interaction between populations and
groups. Besides this method can be generalized on the case of more complex
models in statistical physics and also in ecology, sociology and other &quot;soft'
sciences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5850</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5850</id><created>2012-09-26</created><authors><author><keyname>Zapilko</keyname><forenames>Benjamin</forenames></author><author><keyname>Schaible</keyname><forenames>Johann</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Mathiak</keyname><forenames>Brigitte</forenames></author></authors><title>TheSoz: A SKOS Representation of the Thesaurus for the Social Sciences</title><categories>cs.DL</categories><comments>to appear in Semantic Web - Interoperability, Usability,
  Applicability (IOS Press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Thesaurus for the Social Sciences (TheSoz) is a Linked Dataset in SKOS
format, which serves as a crucial instrument for information retrieval based on
e.g. document indexing or search term recommendation. Thesauri and similar
controlled vocabularies build a linking bridge for other datasets from the
Linked Open Data cloud - even between different domains. The information and
knowledge, which is exposed by such links, can be processed by Semantic Web
applications. In this article the conversion process of the TheSoz to SKOS is
described including the analysis of the original dataset and its structure, the
mapping to adequate SKOS classes and properties, and the technical conversion.
Furthermore mappings to other datasets and the appliance of the TheSoz are
presented. Finally, limitations and modeling issues encountered during the
creation process are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5851</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5851</id><created>2012-09-26</created><authors><author><keyname>Madet</keyname><forenames>Antoine</forenames><affiliation>PPS</affiliation></author></authors><title>A polynomial time {\lambda}-calculus with multithreading and side
  effects</title><categories>cs.PL</categories><comments>PPDP, Leuven : Belgique (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The framework of Light Logics has been extensively studied to control the
complexity of higher-order functional programs. We propose an extension of this
framework to multithreaded programs with side effects, focusing on the case of
polynomial time. After introducing a modal \lambda-calculus with parallel
composition and regions, we prove that a realistic call-by-value evaluation
strategy can be computed in polynomial time for a class of well-formed
programs. The result relies on the simulation of call-by-value by a polynomial
shallow-first strategy which preserves the evaluation order of side effects.
Then, we provide a polynomial type system that guarantees that well-typed
programs do not go wrong. Finally, we illustrate the expressivity of the type
system by giving a programming example of concurrent iteration producing side
effects over an inductive data structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5853</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5853</id><created>2012-09-26</created><authors><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Wierstra</keyname><forenames>Daan</forenames></author><author><keyname>Schaul</keyname><forenames>Tom</forenames></author><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Efficient Natural Evolution Strategies</title><categories>cs.AI</categories><comments>Puslished in GECCO'2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient Natural Evolution Strategies (eNES) is a novel alternative to
conventional evolutionary algorithms, using the natural gradient to adapt the
mutation distribution. Unlike previous methods based on natural gradients, eNES
uses a fast algorithm to calculate the inverse of the exact Fisher information
matrix, thus increasing both robustness and performance of its evolution
gradient estimation, even in higher dimensions. Additional novel aspects of
eNES include optimal fitness baselines and importance mixing (a procedure for
updating the population with very few fitness evaluations). The algorithm
yields competitive results on both unimodal and multimodal benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5860</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5860</id><created>2012-09-26</created><updated>2014-01-27</updated><authors><author><keyname>He</keyname><forenames>Yangbo</forenames></author><author><keyname>Jia</keyname><forenames>Jinzhu</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author></authors><title>Reversible MCMC on Markov equivalence classes of sparse directed acyclic
  graphs</title><categories>stat.ML cs.DM stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/13-AOS1125 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1125</report-no><journal-ref>Annals of Statistics 2013, Vol. 41, No. 4, 1742-1779</journal-ref><doi>10.1214/13-AOS1125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical models are popular statistical tools which are used to represent
dependent or causal complex systems. Statistically equivalent causal or
directed graphical models are said to belong to a Markov equivalent class. It
is of great interest to describe and understand the space of such classes.
However, with currently known algorithms, sampling over such classes is only
feasible for graphs with fewer than approximately 20 vertices. In this paper,
we design reversible irreducible Markov chains on the space of Markov
equivalent classes by proposing a perfect set of operators that determine the
transitions of the Markov chain. The stationary distribution of a proposed
Markov chain has a closed form and can be computed easily. Specifically, we
construct a concrete perfect set of operators on sparse Markov equivalence
classes by introducing appropriate conditions on each possible operator.
Algorithms and their accelerated versions are provided to efficiently generate
Markov chains and to explore properties of Markov equivalence classes of sparse
directed acyclic graphs (DAGs) with thousands of vertices. We find
experimentally that in most Markov equivalence classes of sparse DAGs, (1) most
edges are directed, (2) most undirected subgraphs are small and (3) the number
of these undirected subgraphs grows approximately linearly with the number of
vertices. The article contains supplement arXiv:1303.0632,
http://dx.doi.org/10.1214/13-AOS1125SUPP
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5901</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5901</id><created>2012-09-26</created><authors><author><keyname>Holtman</keyname><forenames>Koen</forenames></author></authors><title>Appcessory Economics: Enabling loosely coupled hardware / software
  innovation</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An appcessory (app + accessory) is a smart phone accessory that is combined
with a specially written app to perform a useful function. An example is a toy
helicopter controlled by a smart phone app: the full value proposition involves
both new hardware outside the phone and new software running inside the phone.
Like the smart phone itself and like a PC, the appcessory hardware is a
platform: it has the property that it becomes even more valuable if innovative
new software is written for it. It is technically possible that innovation in
the appcessory space becomes loosely coupled, with different parties
specializing to innovate on either the hardware (accessory) side or the
software (app) side. However, we are not seeing this loose coupling yet in the
current market. We argue that the reason for this is economical: a market
framework in which loosely coupled software parties are adequately rewarded is
not yet in place. We describe a technical solution for creating the needed
market framework, consisting of an automated micropayment system by which money
can flow from appcessory hardware makers to app software innovators. This
solution also has a broader applicability to the problem of innovation in the
Internet of Things.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5903</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5903</id><created>2012-09-26</created><updated>2013-06-03</updated><authors><author><keyname>Ciancia</keyname><forenames>Vincenzo</forenames></author></authors><title>Interaction and observation: categorical semantics of reactive systems
  trough dialgebras</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use dialgebras, generalising both algebras and coalgebras, as a complement
of the standard coalgebraic framework, aimed at describing the semantics of an
interactive system by the means of reaction rules. In this model, interaction
is built-in, and semantic equivalence arises from it, instead of being
determined by a (possibly difficult) understanding of the side effects of a
component in isolation. Behavioural equivalence in dialgebras is determined by
how a given process interacts with the others, and the obtained observations.
We develop a technique to inter-define categories of dialgebras of different
functors, that in particular permits us to compare a standard coalgebraic
semantics and its dialgebraic counterpart. We exemplify the framework using the
CCS and the pi-calculus. Remarkably, the dialgebra giving semantics to the
pi-calculus does not require the use of presheaf categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5905</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5905</id><created>2012-09-26</created><updated>2012-11-11</updated><authors><author><keyname>Roy</keyname><forenames>Subhankar</forenames></author><author><keyname>Khatua</keyname><forenames>Sunirmal</forenames></author><author><keyname>Roy</keyname><forenames>Sudipta</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Samir K.</forenames></author></authors><title>An Efficient Biological Sequence Compression Technique Using LUT And
  Repeat In The Sequence</title><categories>cs.CE q-bio.QM</categories><comments>9 pages, 3 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data compression plays an important role to deal with high volumes of DNA
sequences in the field of Bioinformatics. Again data compression techniques
directly affect the alignment of DNA sequences. So the time needed to
decompress a compressed sequence has to be given equal priorities as with
compression ratio. This article contains first introduction then a brief review
of different biological sequence compression after that my proposed work then
our two improved Biological sequence compression algorithms after that result
followed by conclusion and discussion, future scope and finally references.
These algorithms gain a very good compression factor with higher saving
percentage and less time for compression and decompression than the previous
Biological Sequence compression algorithms. Keywords: Hash map table, Tandem
repeats, compression factor, compression time, saving percentage, compression,
decompression process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5907</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5907</id><created>2012-09-26</created><authors><author><keyname>Shi</keyname><forenames>Long</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author></authors><title>On Designs of Full Diversity Space-Time Block Codes for Two-User MIMO
  Interference Channels</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a design criterion for space-time block codes (STBC) is
proposed for two-user MIMO interference channels when a group zero-forcing (ZF)
algorithm is applied at each receiver to eliminate the inter-user interference.
Based on the design criterion, a design of STBC for two-user interference
channels is proposed that can achieve full diversity for each user with the
group ZF receiver. The code rate approaches one when the time delay in the
encoding (or code block size) gets large. Performance results demonstrate that
the full diversity can be guaranteed by our proposed STBC with the group ZF
receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5912</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5912</id><created>2012-09-26</created><authors><author><keyname>Iutzeler</keyname><forenames>Franck</forenames></author><author><keyname>Ciblat</keyname><forenames>Philippe</forenames></author><author><keyname>Hachem</keyname><forenames>Walid</forenames></author></authors><title>Analysis of Sum-Weight-like algorithms for averaging in Wireless Sensor
  Networks</title><categories>cs.DC cs.IT math.IT</categories><doi>10.1109/TSP.2013.2256904</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed estimation of the average value over a Wireless Sensor Network
has recently received a lot of attention. Most papers consider single variable
sensors and communications with feedback (e.g. peer-to-peer communications).
However, in order to use efficiently the broadcast nature of the wireless
channel, communications without feedback are advocated. To ensure the
convergence in this feedback-free case, the recently-introduced Sum-Weight-like
algorithms which rely on two variables at each sensor are a promising solution.
In this paper, the convergence towards the consensus over the average of the
initial values is analyzed in depth. Furthermore, it is shown that the squared
error decreases exponentially with the time. In addition, a powerful algorithm
relying on the Sum-Weight structure and taking into account the broadcast
nature of the channel is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5920</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5920</id><created>2012-09-26</created><authors><author><keyname>Boender</keyname><forenames>Jaap</forenames></author><author><keyname>Coen</keyname><forenames>Claudio Sacerdoti</forenames></author></authors><title>On the correctness of a branch displacement algorithm</title><categories>cs.LO cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The branch displacement problem is a well-known problem in assembler design.
It revolves around the feature, present in several processor families, of
having different instructions, of different sizes, for jumps of different
displacements. The problem, which is provably NP-hard, is then to select the
instructions such that one ends up with the smallest possible program.
  During our research with the CerCo project on formally verifying a C
compiler, we have implemented and proven correct an algorithm for this problem.
In this paper, we discuss the problem, possible solutions, our specific
solutions and the proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5922</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5922</id><created>2012-09-26</created><updated>2013-03-06</updated><authors><author><keyname>Keator</keyname><forenames>D. B.</forenames></author><author><keyname>Helmer</keyname><forenames>K.</forenames></author><author><keyname>Steffener</keyname><forenames>J.</forenames></author><author><keyname>Turner</keyname><forenames>J. A.</forenames></author><author><keyname>Van Erp</keyname><forenames>T. G. M.</forenames></author><author><keyname>Gadde</keyname><forenames>S.</forenames></author><author><keyname>Ashish</keyname><forenames>N.</forenames></author><author><keyname>Burns</keyname><forenames>G. A.</forenames></author><author><keyname>Nichols</keyname><forenames>B. N.</forenames></author><author><keyname>Ghosh</keyname><forenames>S. S.</forenames></author></authors><title>Towards structured sharing of raw and derived neuroimaging data across
  existing resources</title><categories>cs.DB q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data sharing efforts increasingly contribute to the acceleration of
scientific discovery. Neuroimaging data is accumulating in distributed
domain-specific databases and there is currently no integrated access mechanism
nor an accepted format for the critically important meta-data that is necessary
for making use of the combined, available neuroimaging data. In this
manuscript, we present work from the Derived Data Working Group, an open-access
group sponsored by the Biomedical Informatics Research Network (BIRN) and the
International Neuroimaging Coordinating Facility (INCF) focused on practical
tools for distributed access to neuroimaging data. The working group develops
models and tools facilitating the structured interchange of neuroimaging
meta-data and is making progress towards a unified set of tools for such data
and meta-data exchange. We report on the key components required for integrated
access to raw and derived neuroimaging data as well as associated meta-data and
provenance across neuroimaging resources. The components include (1) a
structured terminology that provides semantic context to data, (2) a formal
data model for neuroimaging with robust tracking of data provenance, (3) a web
service-based application programming interface (API) that provides a
consistent mechanism to access and query the data model, and (4) a provenance
library that can be used for the extraction of provenance data by image
analysts and imaging software developers. We believe that the framework and set
of tools outlined in this manuscript have great potential for solving many of
the issues the neuroimaging community faces when sharing raw and derived
neuroimaging data across the various existing database systems for the purpose
of accelerating scientific discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5931</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5931</id><created>2012-09-25</created><updated>2013-03-23</updated><authors><author><keyname>Hild</keyname><forenames>Stefan</forenames></author><author><keyname>Leavey</keyname><forenames>Sean</forenames></author><author><keyname>Gr&#xe4;f</keyname><forenames>Christian</forenames></author><author><keyname>Sorazu</keyname><forenames>Borja</forenames></author></authors><title>Smart Charging Technologies for Portable Electronic Devices</title><categories>cs.OH</categories><comments>Updated version with a new section describing a software based
  charging control of a laptop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we describe our efforts of extending demand-side control
concepts to the application in portable electronic devices, such as laptop
computers, mobile phones and tablet computers. As these devices feature
built-in energy storage (in the form of batteries) and the ability to run
complex control routines, they are ideal for the implementation of smart
charging concepts. We developed a prototype of a smart laptop charger that
controls the charging process depending on the locally measured frequency of
the electricity grid. If this technique is incorporated into millions of
devices in UK households, this will contribute significantly to the stability
of the electricity grid, help to mitigate the power production fluctuations
from renewable energy sources and avoid the high cost of building and
maintaining conventional power plants as standby reserve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5932</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5932</id><created>2012-09-26</created><updated>2013-06-17</updated><authors><author><keyname>Chakraborty</keyname><forenames>Kaushik</forenames></author><author><keyname>Choi</keyname><forenames>Byung-Soo</forenames></author><author><keyname>Maitra</keyname><forenames>Arpita</forenames></author><author><keyname>Maitra</keyname><forenames>Subhamoy</forenames></author></authors><title>Efficient quantum algorithm to construct arbitrary Dicke states</title><categories>quant-ph cs.CC cs.DM</categories><comments>After posting this draft to the arxiv (September 26, 2012), we
  received an email from Prof. Andrew M. Childs on October 3, 2012, referring
  [2] that also considered the same problem. However, our work is independent
  and uses a different approach. We were not aware of [2] while posting this
  draft for the first time. We have now included additional material and a
  detailed comparison with [2]</comments><msc-class>81P45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study efficient algorithms towards the construction of any
arbitrary Dicke state. Our contribution is to use proper symmetric Boolean
functions that involve manipulations with Krawtchouk polynomials. Deutsch-Jozsa
algorithm, Grover algorithm and the parity measurement technique are stitched
together to devise the complete algorithm. Further, motivated by the work of
Childs et al (2002), we explore how one can plug the biased Hadamard
transformation in our strategy. Our work compares fairly with the results of
Childs et al (2002).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5969</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5969</id><created>2012-09-26</created><updated>2013-07-19</updated><authors><author><keyname>Riolo</keyname><forenames>Maria A.</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>First-principles multiway spectral partitioning of graphs</title><categories>cs.DS cs.SI</categories><comments>12 pages, 9 figures</comments><journal-ref>Journal of Complex Networks 2, 121-140 (2014)</journal-ref><doi>10.1093/comnet/cnt021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the minimum-cut partitioning of a graph into more than two parts
using spectral methods. While there exist well-established spectral algorithms
for this problem that give good results, they have traditionally not been well
motivated. Rather than being derived from first principles by minimizing graph
cuts, they are typically presented without direct derivation and then proved
after the fact to work. In this paper, we take a contrasting approach in which
we start with a matrix formulation of the minimum cut problem and then show,
via a relaxed optimization, how it can be mapped onto a spectral embedding
defined by the leading eigenvectors of the graph Laplacian. The end result is
an algorithm that is similar in spirit to, but different in detail from,
previous spectral partitioning approaches. In tests of the algorithm we find
that it outperforms previous approaches on certain particularly difficult
partitioning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5978</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5978</id><created>2012-09-26</created><authors><author><keyname>Ahmadi</keyname><forenames>Behzad</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Two-way Communication with Adaptive Data Acquisition</title><categories>cs.IT math.IT</categories><comments>26 pages, 7 figures, 1 table, to be submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by computer networks and machine-to-machine communication
applications, a bidirectional link is studied in which two nodes, Node 1 and
Node 2, communicate to fulfill generally conflicting informational
requirements. Node 2 is able to acquire information from the environment, e.g.,
via access to a remote data base or via sensing. Information acquisition is
expensive in terms of system resources, e.g., time, bandwidth and energy and
thus should be done efficiently by adapting the acquisition process to the
needs of the application. As a result of the forward communication from Node 1
to Node 2, the latter wishes to compute some function, such as a suitable
average, of the data available at Node 1 and of the data obtained from the
environment. The forward link is also used by Node 1 to query Node 2 with the
aim of retrieving suitable information from the environment on the backward
link. The problem is formulated in the context of multi-terminal
rate-distortion theory and the optimal trade-off between communication rates,
distortions of the information produced at the two nodes and costs for
information acquisition at Node 2 is derived. The issue of robustness to
possible malfunctioning of the data acquisition process at Node 2 is also
investigated. The results are illustrated via an example that demonstrates the
different roles played by the forward communication, namely data exchange,
query and control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5982</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5982</id><created>2012-09-26</created><authors><author><keyname>Templeman</keyname><forenames>Robert</forenames></author><author><keyname>Rahman</keyname><forenames>Zahid</forenames></author><author><keyname>Crandall</keyname><forenames>David</forenames></author><author><keyname>Kapadia</keyname><forenames>Apu</forenames></author></authors><title>PlaceRaider: Virtual Theft in Physical Spaces with Smartphones</title><categories>cs.CR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As smartphones become more pervasive, they are increasingly targeted by
malware. At the same time, each new generation of smartphone features
increasingly powerful onboard sensor suites. A new strain of sensor malware has
been developing that leverages these sensors to steal information from the
physical environment (e.g., researchers have recently demonstrated how malware
can listen for spoken credit card numbers through the microphone, or feel
keystroke vibrations using the accelerometer). Yet the possibilities of what
malware can see through a camera have been understudied. This paper introduces
a novel visual malware called PlaceRaider, which allows remote attackers to
engage in remote reconnaissance and what we call virtual theft. Through
completely opportunistic use of the camera on the phone and other sensors,
PlaceRaider constructs rich, three dimensional models of indoor environments.
Remote burglars can thus download the physical space, study the environment
carefully, and steal virtual objects from the environment (such as financial
documents, information on computer monitors, and personally identifiable
information). Through two human subject studies we demonstrate the
effectiveness of using mobile devices as powerful surveillance and virtual
theft platforms, and we suggest several possible defenses against visual
malware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5991</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5991</id><created>2012-09-26</created><authors><author><keyname>Mahalanabis</keyname><forenames>Satyaki</forenames></author><author><keyname>Stefankovic</keyname><forenames>Daniel</forenames></author></authors><title>Subset Selection for Gaussian Markov Random Fields</title><categories>cs.LG stat.ML</categories><comments>40 pages</comments><msc-class>68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a Gaussian Markov random field, we consider the problem of selecting a
subset of variables to observe which minimizes the total expected squared
prediction error of the unobserved variables. We first show that finding an
exact solution is NP-hard even for a restricted class of Gaussian Markov random
fields, called Gaussian free fields, which arise in semi-supervised learning
and computer vision. We then give a simple greedy approximation algorithm for
Gaussian free fields on arbitrary graphs. Finally, we give a message passing
algorithm for general Gaussian Markov random fields on bounded tree-width
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5993</identifier>
 <datestamp>2013-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5993</id><created>2012-09-26</created><updated>2013-09-11</updated><authors><author><keyname>Mulmuley</keyname><forenames>Ketan D.</forenames></author></authors><title>Geometric Complexity Theory V: Equivalence between blackbox
  derandomization of polynomial identity testing and derandomization of
  Noether's Normalization Lemma</title><categories>cs.CC math.AG</categories><comments>60 pages. This article is the full revised version of its abstract in
  FOCS 2012 taking into account some recent developments</comments><msc-class>03D15, 14Q20</msc-class><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that the problem of derandomizing Noether's Normalization Lemma
(NNL) for any explicit variety can be brought down from EXPSPACE, where it is
currently, to P assuming a strengthened form of the black-box derandomization
hypothesis (BDH) for polynomial identity testing (PIT), and to quasi-P assuming
that some exponential-time-computable multilinear polynomial cannot be
approximated infinitesimally closely by arithmetic circuits of sub-exponential
size. The converse also holds for a strict form of NNL. This equivalence
between the strengthened BDH for PIT and the problem of derandomizing NNL in a
strict form reveals that the fundamental problems of Geometry and Complexity
Theory share a common root difficulty, namely, the problem of overcoming the
EXPSPACE vs. P gap in the complexity of NNL for explicit varieties. This gap is
called the GCT chasm.
  On the positive side, it is shown that NNL for the ring of invariants for any
finite dimensional representation of the special linear group of fixed
dimension can be brought down from EXPSPACE to quasi-P unconditionally in
characteristic zero.
  On the positive side, it has also been shown recently by Forbes and Shpilka
that a variant of a conditional derandomization result in this article in
conjunction with the quasi-derandomization of ROABP that was known earlier
implies unconditional quasi-derandomization of NNL for the ring of matrix
invariants in characteristic zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5994</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5994</id><created>2012-09-26</created><authors><author><keyname>Pia</keyname><forenames>M. G.</forenames></author><author><keyname>Basaglia</keyname><forenames>T.</forenames></author><author><keyname>Bell</keyname><forenames>Z. W.</forenames></author><author><keyname>Dressendorfer</keyname><forenames>P. V.</forenames></author></authors><title>Publication patterns in HEP computing</title><categories>physics.comp-ph cs.DL physics.soc-ph</categories><comments>To be published in the Proc. of CHEP (Computing in High Energy
  Physics) 2012</comments><doi>10.1088/1742-6596/396/6/062015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An overview of the evolution of computing-oriented publications in high
energy physics following the start of operation of LHC. Quantitative analyses
are illustrated, which document the production of scholarly papers on
computing-related topics by high energy physics experiments and core tools
projects, and the citations they receive. Several scientometric indicators are
analyzed to characterize the role of computing in high energy physics
literature. Distinctive features of software-oriented and hardware-oriented
scholarly publications are highlighted. Current patterns and trends are
compared to the situation in previous generations' experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.5998</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.5998</id><created>2012-09-26</created><authors><author><keyname>Dandekar</keyname><forenames>Pranav</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Lee</keyname><forenames>David</forenames></author></authors><title>Biased Assimilation, Homophily and the Dynamics of Polarization</title><categories>cs.SI cs.GT physics.soc-ph</categories><doi>10.1073/pnas.1217220110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Are we as a society getting more polarized, and if so, why? We try to answer
this question through a model of opinion formation. Empirical studies have
shown that homophily results in polarization. However, we show that DeGroot's
well-known model of opinion formation based on repeated averaging can never be
polarizing, even if individuals are arbitrarily homophilous. We generalize
DeGroot's model to account for a phenomenon well-known in social psychology as
biased assimilation: when presented with mixed or inconclusive evidence on a
complex issue, individuals draw undue support for their initial position
thereby arriving at a more extreme opinion. We show that in a simple model of
homophilous networks, our biased opinion formation process results in either
polarization, persistent disagreement or consensus depending on how biased
individuals are. In other words, homophily alone, without biased assimilation,
is not sufficient to polarize society. Quite interestingly, biased assimilation
also provides insight into the following related question: do internet based
recommender algorithms that show us personalized content contribute to
polarization? We make a connection between biased assimilation and the
polarizing effects of some random-walk based recommender algorithms that are
similar in spirit to some commonly used recommender algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6001</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6001</id><created>2012-09-26</created><authors><author><keyname>He</keyname><forenames>Ruefei</forenames></author><author><keyname>Shapiro</keyname><forenames>Jonathan</forenames></author></authors><title>Bayesian Mixture Models for Frequent Itemset Discovery</title><categories>cs.LG cs.IR stat.ML</categories><acm-class>H.2.8; H.3.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In binary-transaction data-mining, traditional frequent itemset mining often
produces results which are not straightforward to interpret. To overcome this
problem, probability models are often used to produce more compact and
conclusive results, albeit with some loss of accuracy. Bayesian statistics have
been widely used in the development of probability models in machine learning
in recent years and these methods have many advantages, including their
abilities to avoid overfitting. In this paper, we develop two Bayesian mixture
models with the Dirichlet distribution prior and the Dirichlet process (DP)
prior to improve the previous non-Bayesian mixture model developed for
transaction dataset mining. We implement the inference of both mixture models
using two methods: a collapsed Gibbs sampling scheme and a variational
approximation algorithm. Experiments in several benchmark problems have shown
that both mixture models achieve better performance than a non-Bayesian mixture
model. The variational algorithm is the faster of the two approaches while the
Gibbs sampling method achieves a more accurate results. The Dirichlet process
mixture model can automatically grow to a proper complexity for a better
approximation. Once the model is built, it can be very fast to query and run
analysis on (typically 10 times faster than Eclat, as we will show in the
experiment section). However, these approaches also show that mixture models
underestimate the probabilities of frequent itemsets. Consequently, these
models have a higher sensitivity but a lower specificity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6004</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6004</id><created>2012-09-26</created><authors><author><keyname>Gerrish</keyname><forenames>Sean M.</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>The Issue-Adjusted Ideal Point Model</title><categories>stat.ML cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a model of issue-specific voting behavior. This model can be used
to explore lawmakers' personal voting patterns of voting by issue area,
providing an exploratory window into how the language of the law is correlated
with political support. We derive approximate posterior inference algorithms
based on variational methods. Across 12 years of legislative data, we
demonstrate both improvement in heldout prediction performance and the model's
utility in interpreting an inherently multi-dimensional space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6007</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6007</id><created>2012-09-26</created><authors><author><keyname>Sar&#x131;y&#xfc;ce</keyname><forenames>Ahmet Erdem</forenames></author><author><keyname>Saule</keyname><forenames>Erik</forenames></author><author><keyname>Kaya</keyname><forenames>Kamer</forenames></author><author><keyname>&#xc7;ataly&#xfc;rek</keyname><forenames>&#xdc;mit V.</forenames></author></authors><title>Shattering and Compressing Networks for Centrality Analysis</title><categories>cs.DS cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Who is more important in a network? Who controls the flow between the nodes
or whose contribution is significant for connections? Centrality metrics play
an important role while answering these questions. The betweenness metric is
useful for network analysis and implemented in various tools. Since it is one
of the most computationally expensive kernels in graph mining, several
techniques have been proposed for fast computation of betweenness centrality.
In this work, we propose and investigate techniques which compress a network
and shatter it into pieces so that the rest of the computation can be handled
independently for each piece. Although we designed and tuned the shattering
process for betweenness, it can be adapted for other centrality metrics in a
straightforward manner. Experimental results show that the proposed techniques
can be a great arsenal to reduce the centrality computation time for various
types of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6008</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6008</id><created>2012-09-26</created><updated>2014-11-27</updated><authors><author><keyname>Rowland</keyname><forenames>Eric</forenames></author><author><keyname>Yassawi</keyname><forenames>Reem</forenames></author></authors><title>A characterization of $p$-automatic sequences as columns of linear
  cellular automata</title><categories>math.DS cs.FL nlin.CG</categories><comments>19 pages, 6 figures; final version</comments><msc-class>37B10, 37B15, 68Q80</msc-class><journal-ref>Advances in Applied Mathematics 63 (2015) 68-89</journal-ref><doi>10.1016/j.aam.2014.10.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a sequence over a finite field $\mathbb F_q$ of characteristic
$p$ is $p$-automatic if and only if it occurs as a column of the spacetime
diagram, with eventually periodic initial conditions, of a linear cellular
automaton with memory over $\mathbb F_q$. As a consequence, the subshift
generated by a length-$p$ substitution can be realized as a topological factor
of a linear cellular automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6012</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6012</id><created>2012-09-26</created><authors><author><keyname>Brunetti</keyname><forenames>Sara</forenames></author><author><keyname>Cordasco</keyname><forenames>Gennaro</forenames></author><author><keyname>Gargano</keyname><forenames>Luisa</forenames></author><author><keyname>Lodi</keyname><forenames>Elena</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Minimum Weight Dynamo and Fast Opinion Spreading</title><categories>cs.SI cs.DM math.CO</categories><comments>Multicolored Dynamic Monopolies, Opinion Dynamics, Target Set
  Selection, information spreading</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following multi--level opinion spreading model on networks.
Initially, each node gets a weight from the set [0..k-1], where such a weight
stands for the individuals conviction of a new idea or product. Then, by
proceeding to rounds, each node updates its weight according to the weights of
its neighbors. We are interested in the initial assignments of weights leading
each node to get the value k-1 --e.g. unanimous maximum level acceptance--
within a given number of rounds. We determine lower bounds on the sum of the
initial weights of the nodes under the irreversible simple majority rules,
where a node increases its weight if and only if the majority of its neighbors
have a weight that is higher than its own one. Moreover, we provide
constructive tight upper bounds for some class of regular topologies: rings,
tori, and cliques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6013</identifier>
 <datestamp>2013-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6013</id><created>2012-09-26</created><updated>2013-05-09</updated><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Langiu</keyname><forenames>Alessio</forenames></author><author><keyname>Lecroq</keyname><forenames>Thierry</forenames></author><author><keyname>Lefebvre</keyname><forenames>Arnaud</forenames></author><author><keyname>Mignosi</keyname><forenames>Filippo</forenames></author><author><keyname>Prieur-Gaston</keyname><forenames>&#xc9;lise</forenames></author></authors><title>Abelian Repetitions in Sturmian Words</title><categories>cs.FL cs.DM math.CO</categories><comments>Accepted to DLT 2013</comments><msc-class>68R15</msc-class><acm-class>F.2.2; F.4.3; G.2.1</acm-class><journal-ref>LNCS 7907, pp. 227-238, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate abelian repetitions in Sturmian words. We exploit a bijection
between factors of Sturmian words and subintervals of the unitary segment that
allows us to study the periods of abelian repetitions by using classical
results of elementary Number Theory. We prove that in any Sturmian word the
superior limit of the ratio between the maximal exponent of an abelian
repetition of period $m$ and $m$ is a number $\geq\sqrt{5}$, and the equality
holds for the Fibonacci infinite word. We further prove that the longest prefix
of the Fibonacci infinite word that is an abelian repetition of period $F_j$,
$j&gt;1$, has length $F_j(F_{j+1}+F_{j-1} +1)-2$ if $j$ is even or
$F_j(F_{j+1}+F_{j-1})-2$ if $j$ is odd. This allows us to give an exact formula
for the smallest abelian periods of the Fibonacci finite words. More precisely,
we prove that for $j\geq 3$, the Fibonacci word $f_j$ has abelian period equal
to $F_n$, where $n = \lfloor{j/2}\rfloor$ if $j = 0, 1, 2\mod{4}$, or $n = 1 +
\lfloor{j/2}\rfloor$ if $ j = 3\mod{4}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6017</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6017</id><created>2012-09-26</created><updated>2012-11-08</updated><authors><author><keyname>Gaurav</keyname><forenames>Dinesh Dileep</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>Power Allocation in Amplify and Forward Relays with a Power Constrained
  Relay</title><categories>cs.IT cs.SY math.IT</categories><comments>9 pages, 2 figures, This is to present the new version with updated
  content</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a two-hop Multiple-Input Multiple-Output channel with a source, a
single Amplify and Forward relay, and the destination. We consider the problem
of designing precoders at the source and the relay, and the receiver matrix at
the destination. In particular, we address the problem of optimal power
allocation scheme at the source which minimizes the source transmit power while
satisfying a given Quality of Service requirement at the destination, and a
power constraint at the relay. We consider two types of receiver at the
destination, a Zero Forcing receiver and an Minimum Mean Square Error receiver.
Simulation Results are provided in the end which compare the performance of
both the receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6037</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6037</id><created>2012-09-26</created><authors><author><keyname>Dilawari</keyname><forenames>Jaswinder Singh</forenames></author><author><keyname>Khanna</keyname><forenames>Ravinder</forenames></author></authors><title>Reproduction of Images by Gamut Mapping and Creation of New Test Charts
  in Prepress Process</title><categories>cs.CV</categories><comments>5 Pages,10 Figures; International Journal of Scientific and
  Engineering Research,Volume 3, Issue 10, October 2012 Edition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of digital images the problem of keeping picture
visualization uniformity arises because each printing or scanning device has
its own color chart. So, universal color profiles are made by ICC to bring
uniformity in various types of devices. Keeping that color profile in mind
various new color charts are created and calibrated with the help of standard
IT8 test charts available in the market. The main objective to color
reproduction is to produce the identical picture at device output. For that
principles for gamut mapping has been designed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6050</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6050</id><created>2012-09-26</created><authors><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Filipowski</keyname><forenames>Tomasz</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>An Introduction to Community Detection in Multi-layered Social Network</title><categories>cs.SI physics.soc-ph</categories><comments>M.D. Lytras et al. (Eds.): WSKS 2011, CCIS 278, pp. 185-190, 2012</comments><journal-ref>CCIS 278, pp. 185-190, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social communities extraction and their dynamics are one of the most
important problems in today's social network analysis. During last few years,
many researchers have proposed their own methods for group discovery in social
networks. However, almost none of them have noticed that modern social networks
are much more complex than few years ago. Due to vast amount of different data
about various user activities available in IT systems, it is possible to
distinguish the new class of social networks called multi-layered social
network. For that reason, the new approach to community detection in the
multi-layered social network, which utilizes multi-layered edge clustering
coefficient is proposed in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6070</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6070</id><created>2012-09-26</created><authors><author><keyname>Asad</keyname><forenames>Khalid Ibnal</forenames></author><author><keyname>Ahmed</keyname><forenames>Tanvir</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Saiedur</forenames></author></authors><title>Movie Popularity Classification based on Inherent Movie Attributes using
  C4.5,PART and Correlation Coefficient</title><categories>cs.LG cs.DB cs.IR</categories><comments>6 pages</comments><acm-class>H.2.8</acm-class><journal-ref>IEEE/OSA/IAPR International Conference on Informatics, Electronics
  &amp; Vision (ICIEV2012), pp. 747-752, May 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abundance of movie data across the internet makes it an obvious candidate for
machine learning and knowledge discovery. But most researches are directed
towards bi-polar classification of movie or generation of a movie
recommendation system based on reviews given by viewers on various internet
sites. Classification of movie popularity based solely on attributes of a movie
i.e. actor, actress, director rating, language, country and budget etc. has
been less highlighted due to large number of attributes that are associated
with each movie and their differences in dimensions. In this paper, we propose
classification scheme of pre-release movie popularity based on inherent
attributes using C4.5 and PART classifier algorithm and define the relation
between attributes of post release movies using correlation coefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6125</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6125</id><created>2012-09-27</created><authors><author><keyname>Susanto</keyname><forenames>Heru</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Kang</keyname><forenames>Chen Chin</forenames></author></authors><title>Toward Cloud Computing Evolution</title><categories>cs.CY</categories><journal-ref>Computer Science Journal, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  -Information Technology (IT) shaped the success of organizations, giving them
a solid foundation that increases both their level of efficiency as well as
productivity. The computing industry is witnessing a paradigm shift in the way
computing is performed worldwide. There is a growing awareness among consumers
and enterprises to access their IT resources extensively through a &quot;utility&quot;
model known as &quot;cloud computing.&quot; Cloud computing was initially rooted in
distributed grid-based computing. It has become a significant technology trend
and expect that cloud computing will reshape IT processes and the IT
marketplace. With the cloud computing technology, clients use a variety of
smart mobile devices to access programs, storage, and application-development
platforms over the network and internet, through services offered by cloud
computing providers.An innovative new way to boost capacity and add
capabilities in computing without spending money on a new infrastructure,
training new personnel or licensing software is one of the characteristics of a
cloud computing. Demands for fastest access to information is changing and
increasing, therefore, the availability of cloud computing has made it easier
for organizations to share and store related data and information with their
stakeholders.Moreover, cloud computing is the use of internet-based services to
support business processes.Our research is to find out what the demand and main
emphasized of cloud computing compared with efficiency, trendy and security
that leads to improved business processes within corporate as cloud user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6129</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6129</id><created>2012-09-27</created><authors><author><keyname>Garg</keyname><forenames>Deepak</forenames></author><author><keyname>Saxena</keyname><forenames>S C</forenames></author><author><keyname>Bhardwaj</keyname><forenames>L M</forenames></author></authors><title>A New Middle Path Approach For Alignements In Blast</title><categories>cs.DS cs.CE q-bio.QM</categories><journal-ref>Journal of Biological Systems, Vol. 14, No. 4 , pp. 567-581 ISSN
  0218-3390 2006</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper deals with a new middle path approach developed for reducing
alignment calculations in BLAST algorithm. This is a new step which is
introduced in BLAST algorithm in between the ungapped and gapped alignments.
This step of middle path approach between the ungapped and gapped alignments
reduces the number of sequences going for gapped alignment. This results in the
improvement in speed for alignment up to 30 percent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6139</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6139</id><created>2012-09-27</created><updated>2012-11-19</updated><authors><author><keyname>Firooz</keyname><forenames>Mohammad H.</forenames></author><author><keyname>Roy</keyname><forenames>Sumit</forenames></author></authors><title>Collaborative Downloading in VANET using Network Coding</title><categories>cs.NI</categories><comments>conference, 7 Pages, 3 Figures; Published, in Proceedings of IEEE
  International Conference on Communications (ICC'12) 2012</comments><doi>10.1109/ICC.2012.6364257</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data downloading on the fly is the base of commercial data services in
vehicular networks, such as office-onwheels and entertainment-on-wheels. Due to
the sparse spacial distribution of roadside Base Stations (BS) along the road,
downloading through Roadside-to-Vehicle (R2V) connections is intermittent.When
multiple vehicles with geographical proximity have common interest in certain
objects to download, they can collaborate to reduce significantly their overall
download time. In this paper, we investigate application of Network Coding (NC)
in collaborative downloading (CD). We focus on the R2V part of CD, and
analytically derive probability distribution and expected value of amount of
time needed to deliver all information to the vehicles with and without NC. Our
results show that using NC slightly improves the downloading time in addition
to removing any need for having any sort of uplink communications from vehicles
to the infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6140</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6140</id><created>2012-09-27</created><authors><author><keyname>George</keyname><forenames>Paul</forenames><affiliation>HEUDIASYC</affiliation></author><author><keyname>Thouvenin</keyname><forenames>Indira</forenames><affiliation>HEUDIASYC</affiliation></author><author><keyname>Fremont</keyname><forenames>Vincent</forenames><affiliation>HEUDIASYC</affiliation></author><author><keyname>Cherfaoui</keyname><forenames>V&#xe9;ronique</forenames><affiliation>HEUDIASYC</affiliation></author></authors><title>DAARIA: Driver Assistance by Augmented Reality for Intelligent
  Automobile</title><categories>cs.HC cs.RO</categories><proxy>ccsd</proxy><journal-ref>2012 IEEE Intelligent Vehicles Symposium, Spain (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking into account the drivers' state is a major challenge for designing new
advanced driver assistance systems. In this paper we present a driver
assistance system strongly coupled to the user. DAARIA 1 stands for Driver
Assistance by Augmented Reality for Intelligent Automobile. It is an augmented
reality interface powered by several sensors. The detection has two goals: one
is the position of obstacles and the quantification of the danger represented
by them. The other is the driver's behavior. A suitable visualization metaphor
allows the driver to perceive at any time the location of the relevant hazards
while keeping his eyes on the road. First results show that our method could be
applied to a vehicle but also to aerospace, fluvial or maritime navigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6144</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6144</id><created>2012-09-27</created><authors><author><keyname>Eftekhari</keyname><forenames>Mohammad</forenames><affiliation>LAMFA</affiliation></author></authors><title>A Diffie-Hellman Key Exchange Using Matrices Over Non Commutative Rings</title><categories>cs.CR</categories><proxy>ccsd</proxy><journal-ref>groups, complexity and cryptology 4, issue 1 (2012) 167-176</journal-ref><doi>10.1515/gcc-2012-0001, may 2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a key exchange procedure whose security is based on the
difficulty of computing discrete logarithms in a group, and where
exponentiation is hidden by a conjugation. We give a platform-dependent
cryptanalysis of this protocol. Finally, to take full advantage of this
procedure, we propose a group of matrices over a noncommutative ring as
platform group
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6151</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6151</id><created>2012-09-27</created><authors><author><keyname>Le</keyname><forenames>Thai Hoang</forenames></author><author><keyname>Vo</keyname><forenames>Truong Nhat</forenames></author></authors><title>Face Alignment Using Active Shape Model And Support Vector Machine</title><categories>cs.CV</categories><comments>11 pages and 11 figures</comments><journal-ref>International Journal of Biometrics and Bioinformatics, 2011,
  Volume (4): Issue (6), pp. 224-234</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Active Shape Model (ASM) is one of the most popular local texture models
for face alignment. It applies in many fields such as locating facial features
in the image, face synthesis, etc. However, the experimental results show that
the accuracy of the classical ASM for some applications is not high. This paper
suggests some improvements on the classical ASM to increase the performance of
the model in the application: face alignment. Four of our major improvements
include: i) building a model combining Sobel filter and the 2-D profile in
searching face in image; ii) applying Canny algorithm for the enhancement edge
on image; iii) Support Vector Machine (SVM) is used to classify landmarks on
face, in order to determine exactly location of these landmarks support for
ASM; iv)automatically adjust 2-D profile in the multi-level model based on the
size of the input image. The experimental results on Caltech face database and
Technical University of Denmark database (imm_face) show that our proposed
improvement leads to far better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6152</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6152</id><created>2012-09-27</created><updated>2013-03-15</updated><authors><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Jia</keyname><forenames>Yan</forenames></author><author><keyname>Jin</keyname><forenames>Chao</forenames></author><author><keyname>Xi</keyname><forenames>Weiya</forenames></author><author><keyname>Chan</keyname><forenames>Kheong Sann</forenames></author></authors><title>Parity Declustering for Fault-Tolerant Storage Systems via $t$-designs</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parity declustering allows faster reconstruction of a disk array when some
disk fails. Moreover, it guarantees uniform reconstruction workload on all
surviving disks. It has been shown that parity declustering for one-failure
tolerant array codes can be obtained via Balanced Incomplete Block Designs. We
extend this technique for array codes that can tolerate an arbitrary number of
disk failures via $t$-designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6158</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6158</id><created>2012-09-27</created><updated>2015-01-05</updated><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Doerr</keyname><forenames>Carola</forenames></author><author><keyname>Moran</keyname><forenames>Shay</forenames></author><author><keyname>Moran</keyname><forenames>Shlomo</forenames></author></authors><title>Simple and Optimal Randomized Fault-Tolerant Rumor Spreading</title><categories>cs.DS cs.DC</categories><comments>This is the author-generated version of a paper which is to appear in
  Distributed Computing, Springer, DOI: 10.1007/s00446-014-0238-z It is
  available online from
  http://link.springer.com/article/10.1007/s00446-014-0238-z This version
  contains some new results (Section 6)</comments><journal-ref>Distributed Computing, Springer, 2015</journal-ref><doi>10.1007/s00446-014-0238-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the classic problem of spreading a piece of information in a group
of $n$ fully connected processors. By suitably adding a small dose of
randomness to the protocol of Gasienic and Pelc (1996), we derive for the first
time protocols that (i) use a linear number of messages, (ii) are correct even
when an arbitrary number of adversarially chosen processors does not
participate in the process, and (iii) with high probability have the
asymptotically optimal runtime of $O(\log n)$ when at least an arbitrarily
small constant fraction of the processors are working. In addition, our
protocols do not require that the system is synchronized nor that all
processors are simultaneously woken up at time zero, they are fully based on
push-operations, and they do not need an a priori estimate on the number of
failed nodes.
  Our protocols thus overcome the typical disadvantages of the two known
approaches, algorithms based on random gossip (typically needing a large number
of messages due to their unorganized nature) and algorithms based on fair
workload splitting (which are either not {time-efficient} or require intricate
preprocessing steps plus synchronization).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6163</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6163</id><created>2012-09-27</created><authors><author><keyname>Diertens</keyname><forenames>Bob</forenames></author></authors><title>From Functions to Object-Orientation by Abstraction</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1010.3100, arXiv:1111.5172,
  arXiv:1208.3340</comments><report-no>tcs1202</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work we developed a framework of computational models for
function and object execution. The models on an higher level of abstraction in
this framework allow for concurrent execution of functions and objects. We show
that the computational model for object execution complies with the
fundamentals of object-orientation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6182</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6182</id><created>2012-09-27</created><authors><author><keyname>Pignolet</keyname><forenames>Yvonne-Anne</forenames></author><author><keyname>Rinis</keyname><forenames>Ilias</forenames></author><author><keyname>Dzung</keyname><forenames>Dacfey</forenames></author><author><keyname>Karaagac</keyname><forenames>Abdulkadir</forenames></author></authors><title>Multi-Interface PLC / Wireless Network Simulation</title><categories>cs.NI</categories><comments>A preliminary version of this article appeared at WSPLC, Rome, Italy,
  September 20-21, September 20-21 [6]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many communication networks consist of legacy and new devices using
heterogeneous technologies, such as copper wire, optical fiber, wireless and
power line communication (PLC). Most network simulators, however, have been
designed to work well with a single underlying link layer technology.
Furthermore, there are hardly any suitable models for network simulators of
PLC. In this paper we present extensions of the Contiki OS network simulator
Cooja: A device may support multiple interfaces accessing multiple PLC segments
or wireless channels and a simple PLC medium model is introduced describing
packet loss probability as a function of distance. We test our approach to
simulate a Smart Grid scenario of Ring Main Units equipped with PLC devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6186</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6186</id><created>2012-09-27</created><authors><author><keyname>Yomo</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Kondo</keyname><forenames>Yoshihisa</forenames></author><author><keyname>Miyamoto</keyname><forenames>Noboru</forenames></author><author><keyname>Tang</keyname><forenames>Suhua</forenames></author><author><keyname>Iwai</keyname><forenames>Masahito</forenames></author><author><keyname>Ito</keyname><forenames>Tetsuya</forenames></author></authors><title>Receiver Design for Realizing On-Demand WiFi Wake-up using WLAN Signals</title><categories>cs.NI</categories><comments>6 pages, 10 figures, to be presented at IEEE Globecom 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design a simple, low-cost, and low-power wake-up receiver
which can be used for an IEEE 802.11-compliant device to remotely wake up the
other devices by utilizing its own wireless LAN (WLAN) signals. The employed
wake-up mechanism utilizes the length of 802.11 data frame generated by a WiFi
transmitter to differentiate the information conveyed to the wake-up receiver.
The wake-up receiver is designed to reliably detect the length of transmitted
data frame only with simple envelope detection and limited signal processing.
We develop a prototype of the wake-up receiver and investigate the detection
performance of the envelope of 802.11 signals. Our numerical results show that
the proposed wake-up receiver achieves much larger detection range than the
off-the-shelf, commercial receiver having the similar functionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6189</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6189</id><created>2012-09-27</created><authors><author><keyname>Popescu-Bodorin</keyname><forenames>Nicolaie</forenames></author><author><keyname>Balas</keyname><forenames>Valentina E.</forenames></author><author><keyname>Motoc</keyname><forenames>Iulia M.</forenames></author></authors><title>The Biometric Menagerie - A Fuzzy and Inconsistent Concept</title><categories>cs.CV</categories><comments>5th Int. Conf. on Soft Computing and Applications (Szeged, HU), 22-24
  Aug 2012</comments><msc-class>68U10</msc-class><acm-class>I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proves that in iris recognition, the concepts of sheep, goats,
lambs and wolves - as proposed by Doddington and Yager in the so-called
Biometric Menagerie, are at most fuzzy and at least not quite well defined.
They depend not only on the users or on their biometric templates, but also on
the parameters that calibrate the iris recognition system. This paper shows
that, in the case of iris recognition, the extensions of these concepts have
very unsharp and unstable (non-stationary) boundaries. The membership of a user
to these categories is more often expressed as a degree (as a fuzzy value)
rather than as a crisp value. Moreover, they are defined by fuzzy Sugeno rules
instead of classical (crisp) definitions. For these reasons, we said that the
Biometric Menagerie proposed by Doddington and Yager could be at most a fuzzy
concept of biometry, but even this status is conditioned by improving its
definition. All of these facts are confirmed experimentally in a series of 12
exhaustive iris recognition tests undertaken for University of Bath Iris Image
Database while using three different iris code dimensions (256x16, 128x8 and
64x4), two different iris texture encoders (Log-Gabor and Haar-Hilbert) and two
different types of safety models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6190</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6190</id><created>2012-09-27</created><authors><author><keyname>Motoc</keyname><forenames>Iulia M.</forenames></author><author><keyname>Noaica</keyname><forenames>Cristina M.</forenames></author><author><keyname>Badea</keyname><forenames>Robert</forenames></author><author><keyname>Ghica</keyname><forenames>Claudiu G.</forenames></author></authors><title>Noise Influence on the Fuzzy-Linguistic Partitioning of Iris Code Space</title><categories>cs.CV</categories><comments>5th Int. Conf. on Soft Computing and Applications (Szeged, HU), 22-24
  Aug 2012</comments><msc-class>68U10</msc-class><acm-class>I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyses the set of iris codes stored or used in an iris
recognition system as an f-granular space. The f-granulation is given by
identifying in the iris code space the extensions of the fuzzy concepts wolves,
goats, lambs and sheep (previously introduced by Doddington as 'animals' of the
biometric menagerie) - which together form a partitioning of the iris code
space. The main question here is how objective (stable / stationary) this
partitioning is when the iris segments are subject to noisy acquisition. In
order to prove that the f-granulation of iris code space with respect to the
fuzzy concepts that define the biometric menagerie is unstable in noisy
conditions (is sensitive to noise), three types of noise (localvar, motion
blur, salt and pepper) have been alternatively added to the iris segments
extracted from University of Bath Iris Image Database. The results of 180
exhaustive (all-to-all) iris recognition tests are presented and commented
here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6195</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6195</id><created>2012-09-27</created><authors><author><keyname>Noaica</keyname><forenames>Cristina M.</forenames></author><author><keyname>Badea</keyname><forenames>Robert</forenames></author><author><keyname>Motoc</keyname><forenames>Iulia M.</forenames></author><author><keyname>Ghica</keyname><forenames>Claudiu G.</forenames></author><author><keyname>Rosoiu</keyname><forenames>Alin C.</forenames></author><author><keyname>Popescu-Bodorin</keyname><forenames>Nicolaie</forenames></author></authors><title>Examples of Artificial Perceptions in Optical Character Recognition and
  Iris Recognition</title><categories>cs.AI</categories><comments>5th Int. Conf. on Soft Computing and Applications (Szeged, HU), 22-24
  Aug 2012</comments><msc-class>97R40</msc-class><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper assumes the hypothesis that human learning is perception based,
and consequently, the learning process and perceptions should not be
represented and investigated independently or modeled in different simulation
spaces. In order to keep the analogy between the artificial and human learning,
the former is assumed here as being based on the artificial perception. Hence,
instead of choosing to apply or develop a Computational Theory of (human)
Perceptions, we choose to mirror the human perceptions in a numeric
(computational) space as artificial perceptions and to analyze the
interdependence between artificial learning and artificial perception in the
same numeric space, using one of the simplest tools of Artificial Intelligence
and Soft Computing, namely the perceptrons. As practical applications, we
choose to work around two examples: Optical Character Recognition and Iris
Recognition. In both cases a simple Turing test shows that artificial
perceptions of the difference between two characters and between two irides are
fuzzy, whereas the corresponding human perceptions are, in fact, crisp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6204</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6204</id><created>2012-09-27</created><authors><author><keyname>Kharinov</keyname><forenames>M.</forenames></author></authors><title>Reclassification formula that provides to surpass K-means method</title><categories>cs.CV cs.DS</categories><comments>10 pages, 2 figures, 13 formulas</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a formula for the reclassification of multidimensional
data points (columns of real numbers, &quot;objects&quot;, &quot;vectors&quot;, etc.). This formula
describes the change in the total squared error caused by reclassification of
data points from one cluster into another and prompts the way to calculate the
sequence of optimal partitions, which are characterized by a minimum value of
the total squared error E (weighted sum of within-class variance,
within-cluster sum of squares WCSS etc.), i.e. the sum of squared distances
from each data point to its cluster center. At that source data points are
treated with repetitions allowed, and resulting clusters from different
partitions, in general case, overlap each other. The final partitions are
characterized by &quot;equilibrium&quot; stability with respect to the reclassification
of the data points, where the term &quot;stability&quot; means that any prescribed
reclassification of data points does not increase the total squared error E. It
is important that conventional K-means method, in general case, provides
generation of instable partitions with overstated values of the total squared
error E. The proposed method, based on the formula of reclassification, is more
efficient than K-means method owing to converting of any partition into stable
one, as well as involving into the process of reclassification of certain sets
of data points, in contrast to the classification of individual data points
according to K-means method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6217</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6217</id><created>2012-09-27</created><updated>2014-02-10</updated><authors><author><keyname>Zhang</keyname><forenames>C. -X.</forenames></author><author><keyname>Zhang</keyname><forenames>Z. -K.</forenames></author><author><keyname>Liu</keyname><forenames>C.</forenames></author></authors><title>An Evolving model of online bipartite networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1016/j.physa.2013.07.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the structure and evolution of online bipartite networks is a
significant task since they play a crucial role in various e-commerce services
nowadays. Recently, various attempts have been tried to propose different
models, resulting in either power-law or exponential degree
distributions.However, many empirical results show that the user degree
distribution actually follows a shifted power-law distribution, so-called
\emph{Mandelbrot law}, which cannot be fully described by previous models. In
this paper, we propose an evolving model, considering two different user
behaviors: random and preferential attachment. Extensive empirical results on
two real bipartite networks, \emph{Delicious} and \emph{CiteULike}, show that
the theoretical model can well characterize the structure of real networks for
both user and object degree distributions. In addition, we introduce a
structural parameter $p$, to demonstrate that the hybrid user behavior leads to
the shifted power-law degree distribution, and the region of power-law tail
will increase with the increment of $p$. The proposed model might shed some
lights in understanding the underlying laws governing the structure of real
online bipartite networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6228</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6228</id><created>2012-09-26</created><authors><author><keyname>Tanha</keyname><forenames>Maryam</forenames></author><author><keyname>Hashima</keyname><forenames>Fazirulhisyam</forenames></author><author><keyname>Shamalab</keyname><forenames>S.</forenames></author><author><keyname>Samsudin</keyname><forenames>Khairulmizam</forenames></author></authors><title>Highly Available Smart Grid Control Centers through Intrusion Tolerance</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Societies' norms of operation relies on the proper and secure functioning of
several critical infrastructures, particularly modern power grid which is also
known as smart grid. Smart grid is interwoven with the information and
communication technology infrastructure, and thus it is exposed to cyber
security threats. Intrusion tolerance proves a promising security approach
against malicious attacks and contributes to enhance the resilience and
security of the key components of smart grid, mainly SCADA and control centers.
Hence, an intrusion tolerant system architecture for smart grid control centers
is proposed in this paper. The proposed architecture consists of several
modules namely, replication &amp; diversity, compromised/faulty replica detector,
reconfiuration, auditing and proxy. Some of distinctive features of the
proposed ITS are diversity as well as the combined and fine-grained
rejuvenation approach. The security of the proposed architecture is evaluated
with regard to availability and mean time to security failure as performance
measures. The analysis is conducted using a Discrete Time Semi Markov Model and
the acquired results show improvements compared to two established intrusion
tolerant architectures. The viability of SLA as another performance metric is
also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6238</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6238</id><created>2012-09-25</created><authors><author><keyname>Mote</keyname><forenames>Kevin</forenames></author></authors><title>Natural Language Processing - A Survey</title><categories>cs.CL</categories><comments>Bachelor's Thesis (Washington State University; 2003); 70 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The utility and power of Natural Language Processing (NLP) seems destined to
change our technological society in profound and fundamental ways. However
there are, to date, few accessible descriptions of the science of NLP that have
been written for a popular audience, or even for an audience of intelligent,
but uninitiated scientists. This paper aims to provide just such an overview.
In short, the objective of this article is to describe the purpose, procedures
and practical applications of NLP in a clear, balanced, and readable way. We
will examine the most recent literature describing the methods and processes of
NLP, analyze some of the challenges that researchers are faced with, and
briefly survey some of the current and future applications of this science to
IT research in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6262</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6262</id><created>2012-09-27</created><authors><author><keyname>Bhattasali</keyname><forenames>Tapalina</forenames></author></authors><title>SEGNET: Secure Geo-Sensor Network Model</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Geo-Sensor Networks (GEONET) are suitable for critical applications
in hostile environments due to its flexibility in deployment. But low power
geo-sensor nodes are easily compromised by security threats like battery
exhaustion attack which may give rise to unavoidable circumstances. In this
type of attack, intruder forcefully resists legitimate sensor nodes from going
into low-power sleep state. So that compromised sensor node's battery power is
drained out and it stops working. Due to the limited capability of sensor
nodes, it is very difficult to prevent a sensor node from this type of attack
which apparently appears as innocent interaction. In this paper, a framework of
secure GEONET model (SEGNET) is proposed, based on dynamic load distribution
mechanism for heterogeneous environment. It considers hybrid detection approach
using three modules for anomaly detection, intrusion confirmation and decision
making to reduce the probability of false detection, compared to other existing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6277</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6277</id><created>2012-09-27</created><updated>2013-03-15</updated><authors><author><keyname>Klotz</keyname><forenames>Johannes Georg</forenames></author><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>Schober</keyname><forenames>Steffen</forenames></author></authors><title>Bounds on the Average Sensitivity of Nested Canalizing Functions</title><categories>cs.IT math.IT q-bio.MN</categories><comments>revised submission to PLOS ONE</comments><doi>10.1371/journal.pone.0064371</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nested canalizing Boolean (NCF) functions play an important role in
biological motivated regulative networks and in signal processing, in
particular describing stack filters. It has been conjectured that NCFs have a
stabilizing effect on the network dynamics. It is well known that the average
sensitivity plays a central role for the stability of (random) Boolean
networks. Here we provide a tight upper bound on the average sensitivity for
NCFs as a function of the number of relevant input variables. As conjectured in
literature this bound is smaller than 4/3 This shows that a large number of
functions appearing in biological networks belong to a class that has very low
average sensitivity, which is even close to a tight lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6297</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6297</id><created>2012-09-27</created><authors><author><keyname>Gautam</keyname><forenames>Pratima</forenames></author><author><keyname>Shukla</keyname><forenames>Rahul</forenames></author></authors><title>An Efficient Algorithm for Mining Multilevel Association Rule Based on
  Pincer Search</title><categories>cs.DB</categories><journal-ref>ijcsi international journal vol-9,issue-4,2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering frequent itemset is a key difficulty in significant data mining
applications, such as the discovery of association rules, strong rules,
episodes, and minimal keys. The problem of developing models and algorithms for
multilevel association mining poses for new challenges for mathematics and
computer science. In this paper, we present a model of mining multilevel
association rules which satisfies the different minimum support at each level,
we have employed princer search concepts, multilevel taxonomy and different
minimum supports to find multilevel association rules in a given transaction
data set. This search is used only for maintaining and updating a new data
structure. It is used to prune early candidates that would normally encounter
in the top-down search. A main characteristic of the algorithms is that it does
not require explicit examination of every frequent itemsets, an example is also
given to demonstrate and support that the proposed mining algorithm can derive
the multiple-level association rules under different supports in a simple and
effective manner
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6299</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6299</id><created>2012-09-12</created><updated>2014-11-19</updated><authors><author><keyname>Williams</keyname><forenames>Jason L.</forenames></author><author><keyname>Lau</keyname><forenames>Roslyn A.</forenames></author></authors><title>Approximate evaluation of marginal association probabilities with belief
  propagation</title><categories>cs.AI cs.CV</categories><comments>http://dx.doi.org/10.1109/TAES.2014.120568. appears in IEEE
  Transactions on Aerospace and Electronic Systems, vol. 50, no. 4, October
  2014</comments><journal-ref>IEEE Transactions on Aerospace and Electronic Systems, vol 50, no
  4, pp 2942-2959, October 2014</journal-ref><doi>10.1109/TAES.2014.120568</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data association, the problem of reasoning over correspondence between
targets and measurements, is a fundamental problem in tracking. This paper
presents a graphical model formulation of data association and applies an
approximate inference method, belief propagation (BP), to obtain estimates of
marginal association probabilities. We prove that BP is guaranteed to converge,
and bound the number of iterations necessary. Experiments reveal a favourable
comparison to prior methods in terms of accuracy and computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6308</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6308</id><created>2012-09-27</created><authors><author><keyname>Chin</keyname><forenames>George</forenames><suffix>Jr.</suffix></author><author><keyname>Marquez</keyname><forenames>Andres</forenames></author><author><keyname>Choudhury</keyname><forenames>Sutanay</forenames></author><author><keyname>Feo</keyname><forenames>John</forenames></author></authors><title>Scalable Triadic Analysis of Large-Scale Graphs: Multi-Core vs. Multi-
  Processor vs. Multi-Threaded Shared Memory Architectures</title><categories>cs.DC cs.SI</categories><journal-ref>24th International Symposium on Computer Architecture and High
  Performance Computing (SBAC-PAD), 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Triadic analysis encompasses a useful set of graph mining methods that are
centered on the concept of a triad, which is a subgraph of three nodes. Such
methods are often applied in the social sciences as well as many other diverse
fields. Triadic methods commonly operate on a triad census that counts the
number of triads of every possible edge configuration in a graph. Like other
graph algorithms, triadic census algorithms do not scale well when graphs reach
tens of millions to billions of nodes. To enable the triadic analysis of
large-scale graphs, we developed and optimized a triad census algorithm to
efficiently execute on shared memory architectures. We then conducted
performance evaluations of the parallel triad census algorithm on three
specific systems: Cray XMT, HP Superdome, and AMD multi-core NUMA machine.
These three systems have shared memory architectures but with markedly
different hardware capabilities to manage parallelism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6312</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6312</id><created>2012-09-27</created><updated>2012-09-28</updated><authors><author><keyname>Arora</keyname><forenames>Manuel</forenames></author><author><keyname>Zieschang</keyname><forenames>Paul-Hermann</forenames></author></authors><title>An Algorithmic Approach to the Extensibility of Association Schemes</title><categories>math.CO cs.DM</categories><msc-class>05E30 (Primary) 68R05, 68W30, 03D15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An association scheme which is associated to a height t presuperscheme is
said to be extensible to height t. Smith (1994, 2007) showed that an
association scheme X=(Q,\Gamma) of order d:=|Q| is Schurian iff X is extensible
to height (d-2). In this work, we formalize the maximal height t_max(X) of an
association scheme X as the largest positive integer such that X is extensible
to height t (we also include the possibility t_max(X)=\infty, which is
equivalent to t_max(X)\ge (d-2)). Intuitively, the maximal height provides a
natural measure of how close an association scheme is to being Schurian.
  For the purpose of computing the maximal height, we introduce the association
scheme extension algorithm. On input an association scheme X=(Q,\Gamma) of
order d:=|Q| and an integer t such that 1\le t\le (d-2), the association scheme
extension algorithm decides in time d^(O(t)) if the scheme X is extensible to
height t. In particular, if t is a fixed constant, then the running time of the
association scheme extension algorithm is polynomial in the order of X.
  The association scheme extension algorithm is used to show that all
non-Schurian association schemes up to order 26 are completely inextensible,
i.e. they are not extensible to a positive height. Via the tensor product of
association schemes, the latter result gives rise to a multitude of examples of
infinite families of completely inextensible association schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6325</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6325</id><created>2012-09-27</created><updated>2012-09-28</updated><authors><author><keyname>Bjelakovi&#x107;</keyname><forenames>Igor</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>en</keyname><forenames>Gisbert Jan&#xdf;</forenames></author><author><keyname>N&#xf6;tzel</keyname><forenames>Janis</forenames></author></authors><title>Arbitrarily varying and compound classical-quantum channels and a note
  on quantum zero-error capacities</title><categories>quant-ph cs.IT math.IT</categories><comments>37 pages, 0 figures. Accepted for publication in the LNCS Volume in
  Memory of Rudolf Ahlswede. Includes a section on certain differences between
  classical and classical-quantum channels regarding their zero-error
  capacities</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider compound as well as arbitrarily varying classical-quantum channel
models. For classical-quantum compound channels, we give an elementary proof of
the direct part of the coding theorem. A weak converse under average error
criterion to this statement is also established. We use this result together
with the robustification and elimination technique developed by Ahlswede in
order to give an alternative proof of the direct part of the coding theorem for
a finite classical-quantum arbitrarily varying channels with the criterion of
success being average error probability. Moreover we provide a proof of the
strong converse to the random coding capacity in this setting.The notion of
symmetrizability for the maximal error probability is defined and it is shown
to be both necessary and sufficient for the capacity for message transmission
with maximal error probability criterion to equal zero. Finally, it is shown
that the connection between zero-error capacity and certain arbitrarily varying
channels is, just like in the case of quantum channels, only partially valid
for classical-quantum channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6329</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6329</id><created>2012-09-27</created><authors><author><keyname>Haimovitch</keyname><forenames>Yoav</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>More Is Better: Large Scale Partially-supervised Sentiment
  Classification - Appendix</title><categories>cs.LG</categories><comments>This is the appendix to the paper &quot;More Is Better: Large Scale
  Partially-supervised Sentiment Classification&quot; accepted to ACML 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a bootstrapping algorithm to learn from partially labeled data,
and the results of an empirical study for using it to improve performance of
sentiment classification using up to 15 million unlabeled Amazon product
reviews. Our experiments cover semi-supervised learning, domain adaptation and
weakly supervised learning. In some cases our methods were able to reduce test
error by more than half using such large amount of data.
  NOTICE: This is only the supplementary material.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6336</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6336</id><created>2012-09-27</created><authors><author><keyname>Keller</keyname><forenames>Chantal</forenames><affiliation>INRIA Saclay - Ile de France, LIX</affiliation></author><author><keyname>Lasson</keyname><forenames>Marc</forenames><affiliation>LIP</affiliation></author></authors><title>Parametricity in an Impredicative Sort</title><categories>cs.LO</categories><proxy>ccsd</proxy><journal-ref>CSL - 26th International Workshop/21st Annual Conference of the
  EACSL, CSL 2012 16 (2012) 381-395</journal-ref><doi>10.4230/LIPIcs.CSL.2012.399</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reynold's abstraction theorem is now a well-established result for a large
class of type systems. We propose here a definition of relational parametricity
and a proof of the abstraction theorem in the Calculus of Inductive
Constructions (CIC), the underlying formal language of Coq, in which
parametricity relations' codomain is the impredicative sort of propositions. To
proceed, we need to refine this calculus by splitting the sort hierarchy to
separate informative terms from non-informative terms. This refinement is very
close to CIC, but with the property that typing judgments can distinguish
informative terms. Among many applications, this natural encoding of
parametricity inside CIC serves both theoretical purposes (proving the
independence of propositions with respect to the logical system) as well as
practical aspirations (proving properties of finite algebraic structures). We
finally discuss how we can simply build, on top of our calculus, a new
reflexive Coq tactic that constructs proof terms by parametricity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6337</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6337</id><created>2012-09-27</created><authors><author><keyname>Eftekhari</keyname><forenames>Mohammad</forenames><affiliation>LAMFA</affiliation></author><author><keyname>Abdullah</keyname><forenames>Herish</forenames></author></authors><title>Cryptanalysis and Improvements on Some Graph-based Authentication
  Schemes</title><categories>cs.CR</categories><proxy>ccsd</proxy><journal-ref>cryptanalysis and improvements on some graph-based authentication
  schemes (2012) 7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2010, Grigoriev and Shpilrain, introduced some graph-based authentication
schemes. We present a cryptanalysis of some of these protocols, and introduce
some new schemes to fix the problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6342</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6342</id><created>2012-09-27</created><authors><author><keyname>Cheng</keyname><forenames>Jie</forenames></author><author><keyname>Levina</keyname><forenames>Elizaveta</forenames></author><author><keyname>Wang</keyname><forenames>Pei</forenames></author><author><keyname>Zhu</keyname><forenames>Ji</forenames></author></authors><title>Sparse Ising Models with Covariates</title><categories>stat.ML cs.LG</categories><comments>32 pages (including 5 pages of appendix), 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a lot of work fitting Ising models to multivariate binary data
in order to understand the conditional dependency relationships between the
variables. However, additional covariates are frequently recorded together with
the binary data, and may influence the dependence relationships. Motivated by
such a dataset on genomic instability collected from tumor samples of several
types, we propose a sparse covariate dependent Ising model to study both the
conditional dependency within the binary data and its relationship with the
additional covariates. This results in subject-specific Ising models, where the
subject's covariates influence the strength of association between the genes.
As in all exploratory data analysis, interpretability of results is important,
and we use L1 penalties to induce sparsity in the fitted graphs and in the
number of selected covariates. Two algorithms to fit the model are proposed and
compared on a set of simulated data, and asymptotic results are established.
The results on the tumor dataset and their biological significance are
discussed in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6348</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6348</id><created>2012-09-27</created><authors><author><keyname>Amento</keyname><forenames>Brittanney</forenames></author><author><keyname>Steinwandt</keyname><forenames>Rainer</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author></authors><title>Efficient quantum circuits for binary elliptic curve arithmetic:
  reducing T-gate complexity</title><categories>quant-ph cs.DS cs.ET</categories><comments>14 pages</comments><journal-ref>Quantum Information &amp; Computation 13(7-8): 631-644 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elliptic curves over finite fields GF(2^n) play a prominent role in modern
cryptography. Published quantum algorithms dealing with such curves build on a
short Weierstrass form in combination with affine or projective coordinates. In
this paper we show that changing the curve representation allows a substantial
reduction in the number of T-gates needed to implement the curve arithmetic. As
a tool, we present a quantum circuit for computing multiplicative inverses in
GF(2^n) in depth O(n log n) using a polynomial basis representation, which may
be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6367</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6367</id><created>2012-09-27</created><authors><author><keyname>Popovski</keyname><forenames>P.</forenames></author><author><keyname>Fouladgar</keyname><forenames>A. M.</forenames></author><author><keyname>Simeone</keyname><forenames>O.</forenames></author></authors><title>Interactive Joint Transfer of Energy and Information</title><categories>cs.IT math.IT</categories><comments>29 pages, 11 figures, Submitted in IEEE Transactions on
  Communications. arXiv admin note: substantial text overlap with
  arXiv:1204.1924</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some communication networks, such as passive RFID systems, the energy used
to transfer information between a sender and a recipient can be reused for
successive communication tasks. In fact, from known results in physics, any
system that exchanges information via the transfer of given physical resources,
such as radio waves, particles and qubits, can conceivably reuse, at least
part, of the received resources. This paper aims at illustrating some of the
new challenges that arise in the design of communication networks in which the
signals exchanged by the nodes carry both information and energy. To this end,
a baseline two-way communication system is considered in which two nodes
communicate in an interactive fashion. In the system, a node can either send an
&quot;on&quot; symbol (or &quot;1&quot;), which costs one unit of energy, or an &quot;off&quot; signal (or
&quot;0&quot;), which does not require any energy expenditure. Upon reception of a &quot;1&quot;
signal, the recipient node &quot;harvests&quot;, with some probability, the energy
contained in the signal and stores it for future communication tasks. Inner and
outer bounds on the achievable rates are derived. Numerical results demonstrate
the effectiveness of the proposed strategies and illustrate some key design
insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6393</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6393</id><created>2012-09-27</created><authors><author><keyname>Sprechmann</keyname><forenames>Pablo</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex M.</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Learning Robust Low-Rank Representations</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a comprehensive framework for learning robust
low-rank representations by combining and extending recent ideas for learning
fast sparse coding regressors with structured non-convex optimization
techniques. This approach connects robust principal component analysis (RPCA)
with dictionary learning techniques and allows its approximation via trainable
encoders. We propose an efficient feed-forward architecture derived from an
optimization algorithm designed to exactly solve robust low dimensional
projections. This architecture, in combination with different training
objective functions, allows the regressors to be used as online approximants of
the exact offline RPCA problem or as RPCA-based neural networks. Simple
modifications of these encoders can handle challenging extensions, such as the
inclusion of geometric data transformations. We present several examples with
real data from image, audio, and video processing. When used to approximate
RPCA, our basic implementation shows several orders of magnitude speedup
compared to the exact solvers with almost no performance degradation. We show
the strength of the inclusion of learning to the RPCA approach on a music
source separation application, where the encoders outperform the exact RPCA
algorithms, which are already reported to produce state-of-the-art results on a
benchmark database. Our preliminary implementation on an iPad shows
faster-than-real-time performance with minimal latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6395</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6395</id><created>2012-09-27</created><authors><author><keyname>Zouhair</keyname><forenames>Abdelhamid</forenames></author><author><keyname>En-Naimi</keyname><forenames>El Mokhtar</forenames></author><author><keyname>Amami</keyname><forenames>Benaissa</forenames></author><author><keyname>Boukachour</keyname><forenames>Hadhoum</forenames></author><author><keyname>Person</keyname><forenames>Patrick</forenames></author><author><keyname>Bertelle</keyname><forenames>Cyrille</forenames></author></authors><title>Multi-Agents Dynamic Case Based Reasoning and The Inverse Longest Common
  Sub-Sequence And Individualized Follow-up of Learners in The CEHL</title><categories>cs.AI</categories><comments>International Journal of Computer Science Issues, Volume 9, Issue 4,
  No 2, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In E-learning, there is still the problem of knowing how to ensure an
individualized and continuous learner's follow-up during learning process,
indeed among the numerous tools proposed, very few systems concentrate on a
real time learner's follow-up. Our work in this field develops the design and
implementation of a Multi-Agents System Based on Dynamic Case Based Reasoning
which can initiate learning and provide an individualized follow-up of learner.
When interacting with the platform, every learner leaves his/her traces in the
machine. These traces are stored in a basis under the form of scenarios which
enrich collective past experience. The system monitors, compares and analyses
these traces to keep a constant intelligent watch and therefore detect
difficulties hindering progress and/or avoid possible dropping out. The system
can support any learning subject. The success of a case-based reasoning system
depends critically on the performance of the retrieval step used and, more
specifically, on similarity measure used to retrieve scenarios that are similar
to the course of the learner (traces in progress). We propose a complementary
similarity measure, named Inverse Longest Common Sub-Sequence (ILCSS). To help
and guide the learner, the system is equipped with combined virtual and human
tutors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6396</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6396</id><created>2012-09-27</created><updated>2013-02-19</updated><authors><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author></authors><title>Chernoff-Hoeffding Inequality and Applications</title><categories>cs.DS cs.DB</categories><comments>Expository document hopefully at the level of an advanced undergrad
  or beginning graduate student. The update corrects a missing bound on a
  parameter in one form of the main theorem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dealing with modern big data sets, a very common theme is reducing the
set through a random process. These generally work by making &quot;many simple
estimates&quot; of the full data set, and then judging them as a whole. Perhaps
magically, these &quot;many simple estimates&quot; can provide a very accurate and small
representation of the large data set. The key tool in showing how many of these
simple estimates are needed for a fixed accuracy trade-off is the
Chernoff-Hoeffding inequality[Che52,Hoe63]. This document provides a simple
form of this bound, and two examples of its use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6398</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6398</id><created>2012-09-27</created><authors><author><keyname>Anderson</keyname><forenames>Collin</forenames></author></authors><title>The Hidden Internet of Iran: Private Address Allocations on a National
  Network</title><categories>cs.NI cs.CY</categories><comments>Working Draft</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  While funding agencies have provided substantial support for the developers
and vendors of services that facilitate the unfettered flow of information
through the Internet, little consolidated knowledge exists on the basic
communications network infrastructure of the Islamic Republic of Iran. In the
absence open access and public data, rumors and fear have reigned supreme.
During provisional research on the country's censorship regime, we found
initial indicators that telecommunications entities in Iran allowed private
addresses to route domestically, whether intentionally or unintentionally,
creating a hidden network only reachable within the country. Moreover, records
such as DNS entries lend evidence of a 'dual stack' approach, wherein servers
are assigned a domestic IP addresses, in addition to a global one. Despite the
clear political implications of the claim we put forward, particularly in light
of rampant speculation regarding the mandate of Article 46 of the 'Fifth Five
Year Development Plan' to establish a &quot;national information network,&quot; we
refrain from hypothesizing the purpose of this structure. In order to solicit
critical feedback for future research, we outline our initial findings and
attempt to demonstrate that the matter under contention is a nation-wide
phenomenom that warrants broader attention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6405</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6405</id><created>2012-09-27</created><authors><author><keyname>Donmez</keyname><forenames>Mehmet A.</forenames></author><author><keyname>Inan</keyname><forenames>Huseyin A.</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>Robust Estimation in Rayleigh Fading Channels Under Bounded Channel
  Uncertainties</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate channel equalization for Rayleigh fading channels under
bounded channel uncertainties. We analyze three robust methods to estimate an
unknown signal transmitted through a Rayleigh fading channel, where we avoid
directly tuning the equalizer parameters to the available inaccurate channel
information. These methods are based on minimizing certain mean-square error
criteria that incorporate the channel uncertainties into the problem
formulations. We present closed-form solutions to the channel equalization
problems for each method and for both zero mean and nonzero mean signals. We
illustrate the performances of the equalization methods through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6409</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6409</id><created>2012-09-27</created><authors><author><keyname>Donmez</keyname><forenames>Mehmet A.</forenames></author><author><keyname>Tunc</keyname><forenames>Sait</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>A Deterministic Analysis of an Online Convex Mixture of Expert
  Algorithms</title><categories>cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1203.4209</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze an online learning algorithm that adaptively combines outputs of
two constituent algorithms (or the experts) running in parallel to model an
unknown desired signal. This online learning algorithm is shown to achieve (and
in some cases outperform) the mean-square error (MSE) performance of the best
constituent algorithm in the mixture in the steady-state. However, the MSE
analysis of this algorithm in the literature uses approximations and relies on
statistical models on the underlying signals and systems. Hence, such an
analysis may not be useful or valid for signals generated by various real life
systems that show high degrees of nonstationarity, limit cycles and, in many
cases, that are even chaotic. In this paper, we produce results in an
individual sequence manner. In particular, we relate the time-accumulated
squared estimation error of this online algorithm at any time over any interval
to the time accumulated squared estimation error of the optimal convex mixture
of the constituent algorithms directly tuned to the underlying signal in a
deterministic sense without any statistical assumptions. In this sense, our
analysis provides the transient, steady-state and tracking behavior of this
algorithm in a strong sense without any approximations in the derivations or
statistical assumptions on the underlying signals such that our results are
guaranteed to hold. We illustrate the introduced results through examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6412</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6412</id><created>2012-09-27</created><updated>2013-07-13</updated><authors><author><keyname>Sakzad</keyname><forenames>Amin</forenames></author><author><keyname>Harshan</keyname><forenames>J.</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Integer-Forcing MIMO Linear Receivers Based on Lattice Reduction</title><categories>cs.IT math.IT</categories><comments>9 figures and 11 pages. Modified the title, abstract and some parts
  of the paper. Major change from v1: Added new results on applicability of the
  CLLL reduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new architecture called integer-forcing (IF) linear receiver has been
recently proposed for multiple-input multiple-output (MIMO) fading channels,
wherein an appropriate integer linear combination of the received symbols has
to be computed as a part of the decoding process. In this paper, we propose a
method based on Hermite-Korkine-Zolotareff (HKZ) and Minkowski lattice basis
reduction algorithms to obtain the integer coefficients for the IF receiver. We
show that the proposed method provides a lower bound on the ergodic rate, and
achieves the full receive diversity. Suitability of complex
Lenstra-Lenstra-Lovasz (LLL) lattice reduction algorithm (CLLL) to solve the
problem is also investigated. Furthermore, we establish the connection between
the proposed IF linear receivers and lattice reduction-aided MIMO detectors
(with equivalent complexity), and point out the advantages of the former class
of receivers over the latter. For the $2 \times 2$ and $4\times 4$ MIMO
channels, we compare the coded-block error rate and bit error rate of the
proposed approach with that of other linear receivers. Simulation results show
that the proposed approach outperforms the zero-forcing (ZF) receiver, minimum
mean square error (MMSE) receiver, and the lattice reduction-aided MIMO
detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6419</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6419</id><created>2012-09-28</created><authors><author><keyname>Yuan</keyname><forenames>Xiao-Tong</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Partial Gaussian Graphical Model Estimation</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>32 pages, 5 figures, 4tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the partial estimation of Gaussian graphical models from
high-dimensional empirical observations. We derive a convex formulation for
this problem using $\ell_1$-regularized maximum-likelihood estimation, which
can be solved via a block coordinate descent algorithm. Statistical estimation
performance can be established for our method. The proposed approach has
competitive empirical performance compared to existing methods, as demonstrated
by various experiments on synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6425</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6425</id><created>2012-09-28</created><updated>2013-06-20</updated><authors><author><keyname>Deng</keyname><forenames>Houtao</forenames></author><author><keyname>Runger</keyname><forenames>George</forenames></author></authors><title>Gene selection with guided regularized random forest</title><categories>cs.LG cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The regularized random forest (RRF) was recently proposed for feature
selection by building only one ensemble. In RRF the features are evaluated on a
part of the training data at each tree node. We derive an upper bound for the
number of distinct Gini information gain values in a node, and show that many
features can share the same information gain at a node with a small number of
instances and a large number of features. Therefore, in a node with a small
number of instances, RRF is likely to select a feature not strongly relevant.
Here an enhanced RRF, referred to as the guided RRF (GRRF), is proposed. In
GRRF, the importance scores from an ordinary random forest (RF) are used to
guide the feature selection process in RRF. Experiments on 10 gene data sets
show that the accuracy performance of GRRF is, in general, more robust than RRF
when their parameters change. GRRF is computationally efficient, can select
compact feature subsets, and has competitive accuracy performance, compared to
RRF, varSelRF and LASSO logistic regression (with evaluations from an RF
classifier). Also, RF applied to the features selected by RRF with the minimal
regularization outperforms RF applied to all the features for most of the data
sets considered here. Therefore, if accuracy is considered more important than
the size of the feature subset, RRF with the minimal regularization may be
considered. We use the accuracy performance of RF, a strong classifier, to
evaluate feature selection methods, and illustrate that weak classifiers are
less capable of capturing the information contained in a feature subset. Both
RRF and GRRF were implemented in the &quot;RRF&quot; R package available at CRAN, the
official R package archive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6448</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6448</id><created>2012-09-28</created><authors><author><keyname>Duetting</keyname><forenames>Paul</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Starnberger</keyname><forenames>Martin</forenames></author></authors><title>Auctions with Heterogeneous Items and Budget Limits</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study individual rational, Pareto optimal, and incentive compatible
mechanisms for auctions with heterogeneous items and budget limits. For
multi-dimensional valuations we show that there can be no deterministic
mechanism with these properties for divisible items. We use this to show that
there can also be no randomized mechanism that achieves this for either
divisible or indivisible items. For single-dimensional valuations we show that
there can be no deterministic mechanism with these properties for indivisible
items, but that there is a randomized mechanism that achieves this for either
divisible or indivisible items. The impossibility results hold for public
budgets, while the mechanism allows private budgets, which is in both cases the
harder variant to show. While all positive results are polynomial-time
algorithms, all negative results hold independent of complexity considerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6449</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6449</id><created>2012-09-28</created><authors><author><keyname>Faro</keyname><forenames>Simone</forenames></author><author><keyname>K&#xfc;lekci</keyname><forenames>M. Oguzhan</forenames></author></authors><title>Fast Packed String Matching for Short Patterns</title><categories>cs.IR cs.DS cs.PF</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching for all occurrences of a pattern in a text is a fundamental problem
in computer science with applications in many other fields, like natural
language processing, information retrieval and computational biology. In the
last two decades a general trend has appeared trying to exploit the power of
the word RAM model to speed-up the performances of classical string matching
algorithms. In this model an algorithm operates on words of length w, grouping
blocks of characters, and arithmetic and logic operations on the words take one
unit of time. In this paper we use specialized word-size packed string matching
instructions, based on the Intel streaming SIMD extensions (SSE) technology, to
design very fast string matching algorithms in the case of short patterns. From
our experimental results it turns out that, despite their quadratic worst case
time complexity, the new presented algorithms become the clear winners on the
average for short patterns, when compared against the most effective algorithms
known in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6459</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6459</id><created>2012-09-28</created><authors><author><keyname>Musmeci</keyname><forenames>Nicol&#xf3;</forenames></author><author><keyname>Battiston</keyname><forenames>Stefano</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Puliga</keyname><forenames>Michelangelo</forenames></author><author><keyname>Gabrielli</keyname><forenames>Andrea</forenames></author></authors><title>Bootstrapping topology and systemic risk of complex network using the
  fitness model</title><categories>physics.soc-ph cs.SI q-fin.GN</categories><comments>17 pages, 3 figures</comments><doi>10.1007/s10955-013-0720-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method to reconstruct complex network from partial
information. We assume to know the links only for a subset of the nodes and to
know some non-topological quantity (fitness) characterising every node. The
missing links are generated on the basis of the latter quan- tity according to
a fitness model calibrated on the subset of nodes for which links are known. We
measure the quality of the reconstruction of several topological properties,
such as the network density and the degree distri- bution as a function of the
size of the initial subset of nodes. Moreover, we also study the resilience of
the network to distress propagation. We first test the method on ensembles of
synthetic networks generated with the Exponential Random Graph model which
allows to apply common tools from statistical mechanics. We then test it on the
empirical case of the World Trade Web. In both cases, we find that a subset of
10 % of nodes is enough to reconstruct the main features of the network along
with its resilience with an error of 5%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6462</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6462</id><created>2012-09-28</created><authors><author><keyname>Nordo</keyname><forenames>Giorgio</forenames></author><author><keyname>Maimone</keyname><forenames>Angelo</forenames></author></authors><title>A formula for the number of (n-2)-gap in digital n-objects</title><categories>cs.DM math.CO</categories><msc-class>52C99, 52C45</msc-class><acm-class>G.2.1; F.2.2; I.4.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a formula that expresses the number of (n-2)-gaps of a generic
digital n-object. Such a formula has the advantage to involve only a few simple
intrinsic parameters of the object and it is obtained by using a combinatorial
technic based on incidence structure and on the notion of free cells. This
approach seems suitable as a model for an automatic computation, and also allow
us to find some expressions for the maximum number of i-cells that bound or are
bounded by a fixed j-cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6466</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6466</id><created>2012-09-28</created><authors><author><keyname>V.</keyname><forenames>Suma</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author></authors><title>Four-Step Approach Model of Inspection (FAMI) for Effective Defect
  Management in Software Development</title><categories>cs.SE</categories><comments>13 Pages, 8 Tables, 6 Figures; InterJRI Science and Technology,
  Volume3, Issue 1, August 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IT industry should inculcate effective defect management on a continual basis
to deploy nearly a zerodefect product to their customers. Inspection is one of
the most imperative and effective strategies of defect management.
Nevertheless, existing defect management strategies in leading software
industries are successful to deliver a maximum of 96% defect-free product. An
empirical study of various projects across several service-based and
product-based industries proves the above affirmations. This paper provides an
enhanced approach of inspection through a Four-Step Approach Model of
Inspection (FAMI). FAMI consists of i) integration of Inspection Life Cycle in
V-model of software development, ii) implementation of process metric Depth of
Inspection (DI), iii) implementation of people metric Inspection Performance
Metric (IPM), iv) application of Bayesian probability approach for selection of
appropriate values of inspection affecting parameters to achieve the desirable
DI. The managers of software houses can make use of P2 metric as a benchmarking
tool for the projects in order to improve the in-house defect management
process. Implementation of FAMI in software industries reflects a continual
process improvement and leads to the development of nearly a zero-defect
product through effective defect management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6470</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6470</id><created>2012-09-28</created><authors><author><keyname>Rashmi</keyname><forenames>K. S.</forenames></author><author><keyname>Suma</keyname><forenames>V.</forenames></author><author><keyname>Vaidehi</keyname><forenames>M.</forenames></author></authors><title>Enhanced Load Balancing Approach to Avoid Deadlocks in Cloud</title><categories>cs.DC cs.NI</categories><comments>5 Pages, 4 Figures, 5 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state-of-art of the technology focuses on data processing to deal with
massive amount of data. Cloud computing is an emerging technology, which
enables one to accomplish the aforementioned objective, leading towards
improved business performance. It comprises of users requesting for the
services of diverse applications from various distributed virtual servers. The
cloud should provide resources on demand to its clients with high availability,
scalability and with reduced cost. Load balancing is one of the essential
factors to enhance the working performance of the cloud service provider.
Since, cloud has inherited characteristic of distributed computing and
virtualization there is a possibility of occurrence of deadlock. Hence, in this
paper, a load balancing algorithm has been proposed to avoid deadlocks among
the Virtual Machines (VMs) while processing the requests received from the
users by VM migration. Further, this paper also provides the anticipated
results with the implementation of the proposed algorithm. The deadlock
avoidance enhances the number of jobs to be serviced by cloud service provider
and thereby improving working performance and the business of the cloud service
provider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6476</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6476</id><created>2012-09-28</created><authors><author><keyname>Rashmi</keyname><forenames>K. S.</forenames></author><author><keyname>Suma</keyname><forenames>V.</forenames></author><author><keyname>Vaidehi</keyname><forenames>M.</forenames></author></authors><title>Factors Influencing Job Rejections in Cloud Environment</title><categories>cs.DC cs.NI</categories><comments>6 Pages, 5 Figures, 8 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IT organizations invests heavy capital by consuming large scale
infrastructure and advanced operating platforms. The advances in technology has
resulted in emergence of cloud computing, which is promising technology to
achieve the aforementioned objective. At the peak hours, the jobs arriving to
the cloud system are normally high demanding efficient execution and dispatch.
An observation that has been carried out in this paper by capturing a job
arriving pattern from a monitoring system explains that most of the jobs get
rejected because of lack of efficient technology. The job rejections can be
controlled by certain factors such as job scheduling and load balancing.
Therefore, in this paper the efficiency of Round Robin (RR) scheduling strategy
used for job scheduling and Shortest Job First Scheduling (SJFS) technique used
for load balancing in reducing the job rejections are analyzed. Further, a
proposal for an effective load balancing approach to avoid deadlocks has been
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6481</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6481</id><created>2012-09-28</created><updated>2012-11-23</updated><authors><author><keyname>Bampis</keyname><forenames>Evripidis</forenames></author><author><keyname>Lucarelli</keyname><forenames>Giorgio</forenames></author><author><keyname>Nemparis</keyname><forenames>Ioannis</forenames></author></authors><title>Improved Approximation Algorithms for the Non-preemptive Speed-scaling
  Problem</title><categories>cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are given a set of jobs, each one specified by its release date, its
deadline and its processing volume (work), and a single (or a set of)
speed-scalable processor(s). We adopt the standard model in speed-scaling in
which if a processor runs at speed s then the energy consumption is s^{\alpha}
per time unit, where \alpha&gt;1. Our goal is to find a schedule respecting the
release dates and the deadlines of the jobs so that the total energy
consumption is minimized. While most previous works have studied the preemptive
case of the problem, where a job may be interrupted and resumed later, we focus
on the non-preemptive case where once a job starts its execution, it has to
continue until its completion without any interruption. We propose improved
approximation algorithms for particular instances of the multiprocessor
non-preemptive speed-scaling problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6484</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6484</id><created>2012-09-28</created><authors><author><keyname>Goel</keyname><forenames>Shivani</forenames></author><author><keyname>Kiran</keyname><forenames>Ravi</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author></authors><title>Vulnerability Management for an Enterprise Resource Planning System</title><categories>cs.OH</categories><journal-ref>International Journal of Computer Applications Foundation of
  Computer Science, Volume 53, No.4, 2012, pp. 19-22</journal-ref><doi>10.5120/8409-2043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprise resource planning (ERP) systems are commonly used in technical
educational institutions(TEIs). ERP systems should continue providing services
to its users irrespective of the level of failure. There could be many types of
failures in the ERP systems. There are different types of measures or
characteristics that can be defined for ERP systems to handle the levels of
failure. Here in this paper, various types of failure levels are identified
along with various characteristics which are concerned with those failures. The
relation between all these is summarized. The disruptions causing
vulnerabilities in TEIs are identified .A vulnerability management cycle has
been suggested along with many commercial and open source vulnerability
management tools. The paper also highlights the importance of resiliency in ERP
systems in TEIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6486</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6486</id><created>2012-09-28</created><authors><author><keyname>Tyagi</keyname><forenames>Megha</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author></authors><title>Comparative Analysis of dynamic graph techniques and data strucutre</title><categories>cs.DS</categories><journal-ref>International Journal of Computer Applications 45(5):41-46, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamically changing graphs are used in many applications of graph
algorithms. The scope of these graphs are in graphics, communication networks
and in VLSI designs where graphs are subjected to change, such as addition and
deletion of edges and vertices. There is a rich body of the algorithms and data
structures used for dynamic graphs. The paper overview the techniques and data
structures used in various dynamic algorithms. The effort is tried to find out
the comparison in these techniques namely the hierarchical decomposition of
graphs and highlighting the ingenuity used in designing these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6489</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6489</id><created>2012-09-28</created><authors><author><keyname>Kumar</keyname><forenames>Sandeep</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author></authors><title>Online Financial Algorithms Competitive Analysis</title><categories>cs.CE cs.GT</categories><journal-ref>International Journal of Computer Applications 40(7):8-14, 2012</journal-ref><doi>10.5120/4974-7228</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of algorithms with complete knowledge of its inputs is sometimes not
up to our expectations. Many times we are surrounded with such scenarios where
inputs are generated without any prior knowledge. Online Algorithms have found
their applicability in broad areas of computer engineering. Among these, an
online financial algorithm is one of the most important areas where lots of
efforts have been used to produce an efficient algorithm. In this paper various
Online Algorithms have been reviewed for their efficiency and various
alternative measures have been explored for analysis purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6490</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6490</id><created>2012-09-28</created><authors><author><keyname>Csabai</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Trencs&#xe9;ni</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Herczegh</keyname><forenames>G&#xe9;za</forenames></author><author><keyname>Dobos</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>J&#xf3;zsa</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Purger</keyname><forenames>Norbert</forenames></author><author><keyname>Budav&#xe1;ri</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander</forenames></author></authors><title>Spatial Indexing of Large Multidimensional Databases</title><categories>cs.DB</categories><comments>12 pages, 16 figures; CIDR 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific endeavors such as large astronomical surveys generate databases on
the terabyte scale. These, usually multidimensional databases must be
visualized and mined in order to find interesting objects or to extract
meaningful and qualitatively new relationships. Many statistical algorithms
required for these tasks run reasonably fast when operating on small sets of
in-memory data, but take noticeable performance hits when operating on large
databases that do not fit into memory. We utilize new software technologies to
develop and evaluate fast multidimensional indexing schemes that inherently
follow the underlying, highly non-uniform distribution of the data: they are
layered uniform grid indices, hierarchical binary space partitioning, and
sampled flat Voronoi tessellation of the data. Our working database is the
5-dimensional magnitude space of the Sloan Digital Sky Survey with more than
270 million data points, where we show that these techniques can dramatically
speed up data mining operations such as finding similar objects by example,
classifying objects or comparing extensive simulation sets with observations.
We are also developing tools to interact with the multidimensional database and
visualize the data at multiple resolutions in an adaptive manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6491</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6491</id><created>2012-09-28</created><updated>2014-05-04</updated><authors><author><keyname>Brunton</keyname><forenames>Alan</forenames></author><author><keyname>Salazar</keyname><forenames>Augusto</forenames></author><author><keyname>Bolkart</keyname><forenames>Timo</forenames></author><author><keyname>Wuhrer</keyname><forenames>Stefanie</forenames></author></authors><title>Review of Statistical Shape Spaces for 3D Data with Comparative Analysis
  for Human Faces</title><categories>cs.CV cs.GR</categories><comments>revised literature review, improved experiments, statistical models
  and code published</comments><journal-ref>Computer Vision and Image Understanding, 128, pp. 1-17, 2014</journal-ref><doi>10.1016/j.cviu.2014.05.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With systems for acquiring 3D surface data being evermore commonplace, it has
become important to reliably extract specific shapes from the acquired data. In
the presence of noise and occlusions, this can be done through the use of
statistical shape models, which are learned from databases of clean examples of
the shape in question. In this paper, we review, analyze and compare different
statistical models: from those that analyze the variation in geometry globally
to those that analyze the variation in geometry locally. We first review how
different types of models have been used in the literature, then proceed to
define the models and analyze them theoretically, in terms of both their
statistical and computational aspects. We then perform extensive experimental
comparison on the task of model fitting, and give intuition about which type of
model is better for a few applications. Due to the wide availability of
databases of high-quality data, we use the human face as the specific shape we
wish to extract from corrupted data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6492</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6492</id><created>2012-09-28</created><authors><author><keyname>Sharma</keyname><forenames>Deepika</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author></authors><title>Information Retrieval on the web and its evaluation</title><categories>cs.IR</categories><journal-ref>International Journal of Computer Applications 40(3):26-31, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet is one of the main sources of information for millions of people.
One can find information related to practically all matters on internet.
Moreover if we want to retrieve information about some particular topic we may
find thousands of Web Pages related to that topic. But our main concern is to
find relevant Web Pages from among that collection. So in this paper I have
discussed that how information is retrieved from the web and the efforts
required for retrieving this information in terms of system and users efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6495</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6495</id><created>2012-09-28</created><authors><author><keyname>Patel</keyname><forenames>Parth</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author></authors><title>Comparison of Advance Tree Data Structures</title><categories>cs.DS</categories><journal-ref>International Journal of Computer Applications (2012) Vol. 41 No.2
  pp. 11-21</journal-ref><doi>10.5120/5512-7504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Btree and Rtree are two basic index structures; many different variants of
them are proposed after them. Different variants are used in specific
application for the performance optimization. In this paper different variants
of Btree and Rtree are discussed and compared. Index structures are different
in terms of structure, query support, data type support and application. Index
structures are discussed first. Btree and its variants are discussed and them
Rtree and its variants are discussed. Some structures example is also shown for
the more clear idea. Then comparison is made between all structure with respect
to complexity, query type support, data type support and application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6506</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6506</id><created>2012-09-28</created><authors><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author><author><keyname>Verbeek</keyname><forenames>Kevin</forenames></author></authors><title>Combinatorial and Geometric Properties of Planar Laman Graphs</title><categories>math.CO cs.DM cs.DS</categories><comments>17 pages, 11 figures, SODA 2013</comments><msc-class>05C62</msc-class><acm-class>G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Laman graphs naturally arise in structural mechanics and rigidity theory.
Specifically, they characterize minimally rigid planar bar-and-joint systems
which are frequently needed in robotics, as well as in molecular chemistry and
polymer physics. We introduce three new combinatorial structures for planar
Laman graphs: angular structures, angle labelings, and edge labelings. The
latter two structures are related to Schnyder realizers for maximally planar
graphs. We prove that planar Laman graphs are exactly the class of graphs that
have an angular structure that is a tree, called angular tree, and that every
angular tree has a corresponding angle labeling and edge labeling.
  Using a combination of these powerful combinatorial structures, we show that
every planar Laman graph has an L-contact representation, that is, planar Laman
graphs are contact graphs of axis-aligned L-shapes. Moreover, we show that
planar Laman graphs and their subgraphs are the only graphs that can be
represented this way.
  We present efficient algorithms that compute, for every planar Laman graph G,
an angular tree, angle labeling, edge labeling, and finally an L-contact
representation of G. The overall running time is O(n^2), where n is the number
of vertices of G, and the L-contact representation is realized on the n x n
grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6509</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6509</id><created>2012-09-28</created><authors><author><keyname>Lang</keyname><forenames>Guangming</forenames></author><author><keyname>Li</keyname><forenames>Qingguo</forenames></author></authors><title>Data compression of dynamic set-valued information systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper further investigates the set-valued information system. First, we
bring forward three tolerance relations for set-valued information systems and
explore their basic properties in detail. Then the data compression is
investigated for attribute reductions of set-valued information systems.
Afterwards, we discuss the data compression of dynamic set-valued information
systems by utilizing the precious compression of the original systems. Several
illustrative examples are employed to show that attribute reductions of
set-valued information systems can be simplified significantly by our proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6525</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6525</id><created>2012-09-28</created><authors><author><keyname>Fiori</keyname><forenames>Marcelo</forenames></author><author><keyname>Mus&#xe9;</keyname><forenames>Pablo</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>A Complete System for Candidate Polyps Detection in Virtual Colonoscopy</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer tomographic colonography, combined with computer-aided detection, is
a promising emerging technique for colonic polyp analysis. We present a
complete pipeline for polyp detection, starting with a simple colon
segmentation technique that enhances polyps, followed by an adaptive-scale
candidate polyp delineation and classification based on new texture and
geometric features that consider both the information in the candidate polyp
location and its immediate surrounding area. The proposed system is tested with
ground truth data, including flat and small polyps which are hard to detect
even with optical colonoscopy. For polyps larger than 6mm in size we achieve
100% sensitivity with just 0.9 false positives per case, and for polyps larger
than 3mm in size we achieve 93% sensitivity with 2.8 false positives per case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6528</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6528</id><created>2012-09-28</created><updated>2013-02-15</updated><authors><author><keyname>Crowston</keyname><forenames>Robert</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Muciaccia</keyname><forenames>Gabriele</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>Parameterizations of Test Cover with Bounded Test Sizes</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the {\sc Test Cover} problem we are given a hypergraph $H=(V,
\mathcal{E})$ with $|V|=n, |\mathcal{E}|=m$, and we assume that $\mathcal{E}$
is a test cover, i.e. for every pair of vertices $x_i, x_j$, there exists an
edge $e \in \mathcal{E}$ such that $|{x_i,x_j}\cap e|=1$. The objective is to
find a minimum subset of $\mathcal{E}$ which is a test cover. The problem is
used for identification across many areas, and is NP-complete. From a
parameterized complexity standpoint, many natural parameterizations of {\sc
Test Cover} are either $W[1]$-complete or have no polynomial kernel unless
$coNP\subseteq NP/poly$, and thus are unlikely to be solveable efficiently.
  However, in practice the size of the edges is often bounded. In this paper we
study the parameterized complexity of {\sc Test-$r$-Cover}, the restriction of
{\sc Test Cover} in which each edge contains at most $r \ge 2$ vertices. In
contrast to the unbounded case, we show that the following below-bound
parameterizations of {\sc Test-$r$-Cover} are fixed-parameter tractable with a
polynomial kernel: (1) Decide whether there exists a test cover of size $n-k$,
and (2) decide whether there exists a test cover of size $m-k$, where $k$ is
the parameter. In addition, we prove a new lower bound $\lceil
\frac{2(n-1)}{r+1} \rceil$ on the minimum size of a test cover when the size of
each edge is bounded by $r$. {\sc Test-$r$-Cover} parameterized above this
bound is unlikely to be fixed-parameter tractable; in fact, we show that it is
para-NP-complete, as it is NP-hard to decide whether an instance of {\sc
Test-$r$-Cover} has a test cover of size exactly $\frac{2(n-1)}{r+1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6539</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6539</id><created>2012-09-28</created><authors><author><keyname>Qureshi</keyname><forenames>Jalaluddin</forenames></author><author><keyname>Foh</keyname><forenames>Chuan Heng</forenames></author><author><keyname>Cai</keyname><forenames>Jianfei</forenames></author></authors><title>Optimal Solution for the Index Coding Problem Using Network Coding over
  GF(2)</title><categories>cs.IT cs.NI math.IT</categories><doi>10.1109/SECON.2012.6275780</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The index coding problem is a fundamental transmission problem which occurs
in a wide range of multicast networks. Network coding over a large finite field
size has been shown to be a theoretically efficient solution to the index
coding problem. However the high computational complexity of packet encoding
and decoding over a large finite field size, and its subsequent penalty on
encoding and decoding throughput and higher energy cost makes it unsuitable for
practical implementation in processor and energy constraint devices like mobile
phones and wireless sensors. While network coding over GF(2) can alleviate
these concerns, it comes at a tradeoff cost of degrading throughput
performance. To address this tradeoff, we propose a throughput optimal
triangular network coding scheme over GF(2). We show that such a coding scheme
can supply unlimited number of innovative packets and the decoding involves the
simple back substitution. Such a coding scheme provides an efficient solution
to the index coding problem and its lower computation and energy cost makes it
suitable for practical implementation on devices with limited processing and
energy capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6540</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6540</id><created>2012-09-28</created><authors><author><keyname>S&#xe1;rk&#xf6;zy</keyname><forenames>G&#xe1;bor N.</forenames></author><author><keyname>Song</keyname><forenames>Fei</forenames></author><author><keyname>Szemer&#xe9;di</keyname><forenames>Endre</forenames></author><author><keyname>Trivedi</keyname><forenames>Shubhendu</forenames></author></authors><title>A Practical Regularity Partitioning Algorithm and its Applications in
  Clustering</title><categories>math.CO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a new clustering technique called Regularity
Clustering. This new technique is based on the practical variants of the two
constructive versions of the Regularity Lemma, a very useful tool in graph
theory. The lemma claims that every graph can be partitioned into pseudo-random
graphs. While the Regularity Lemma has become very important in proving
theoretical results, it has no direct practical applications so far. An
important reason for this lack of practical applications is that the graph
under consideration has to be astronomically large. This requirement makes its
application restrictive in practice where graphs typically are much smaller. In
this paper we propose modifications of the constructive versions of the
Regularity Lemma that work for smaller graphs as well. We call this the
Practical Regularity partitioning algorithm. The partition obtained by this is
used to build the reduced graph which can be viewed as a compressed
representation of the original graph. Then we apply a pairwise clustering
method such as spectral clustering on this reduced graph to get a clustering of
the original graph that we call Regularity Clustering. We present results of
using Regularity Clustering on a number of benchmark datasets and compare them
with standard clustering techniques, such as $k$-means and spectral clustering.
These empirical results are very encouraging. Thus in this paper we report an
attempt to harness the power of the Regularity Lemma for real-world
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6547</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6547</id><created>2012-09-28</created><authors><author><keyname>Gawronski</keyname><forenames>P.</forenames></author><author><keyname>Malarz</keyname><forenames>K.</forenames></author><author><keyname>Krawczyk</keyname><forenames>M. J.</forenames></author><author><keyname>Malinowski</keyname><forenames>J.</forenames></author><author><keyname>Kupczak</keyname><forenames>A.</forenames></author><author><keyname>Sikora</keyname><forenames>W.</forenames></author><author><keyname>Kulakowski</keyname><forenames>K.</forenames></author><author><keyname>Was</keyname><forenames>J.</forenames></author><author><keyname>Kantelhardt</keyname><forenames>J.</forenames></author></authors><title>Strategies in crowd and crowd structure</title><categories>physics.soc-ph cs.SI nlin.CD</categories><comments>5 pages, 7 figures</comments><journal-ref>Acta Phys. Pol. A 123 (3), 522 (2013)</journal-ref><doi>10.12693/APhysPolA.123.522</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an emergency situation, imitation of strategies of neighbours can lead to
an order-disorder phase transition, where spatial clusters of pedestrians adopt
the same strategy. We assume that there are two strategies, cooperating and
competitive, which correspond to a smaller or larger desired velocity. The
results of our simulations within the Social Force Model indicate that the
ordered phase can be detected as an increase of spatial order of positions of
the pedestrians in the crowd.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6558</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6558</id><created>2012-09-28</created><updated>2013-07-23</updated><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author></authors><title>Closure solvability for network coding and secret sharing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding is a new technique to transmit data through a network by
letting the intermediate nodes combine the packets they receive. Given a
network, the network coding solvability problem decides whether all the packets
requested by the destinations can be transmitted. In this paper, we introduce a
new approach to this problem. We define a closure operator on a digraph closely
related to the network coding instance and we show that the constraints for
network coding can all be expressed according to that closure operator. Thus, a
solution for the network coding problem is equivalent to a so-called solution
of the closure operator. We can then define the closure solvability problem in
general, which surprisingly reduces to finding secret-sharing matroids when the
closure operator is a matroid. Based on this reformulation, we can easily prove
that any multiple unicast where each node receives at least as many arcs as
there are sources is solvable by linear functions. We also give an alternative
proof that any nontrivial multiple unicast with two source-receiver pairs is
always solvable over all sufficiently large alphabets. Based on singular
properties of the closure operator, we are able to generalise the way in which
networks can be split into two distinct parts; we also provide a new way of
identifying and removing useless nodes in a network. We also introduce the
concept of network sharing, where one solvable network can be used to
accommodate another solvable network coding instance. Finally, the guessing
graph approach to network coding solvability is generalised to any closure
operator, which yields bounds on the amount of information that can be
transmitted through a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6560</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6560</id><created>2012-09-28</created><authors><author><keyname>Pokrass</keyname><forenames>J.</forenames></author><author><keyname>Bronstein</keyname><forenames>A. M.</forenames></author><author><keyname>Bronstein</keyname><forenames>M. M.</forenames></author><author><keyname>Sprechmann</keyname><forenames>P.</forenames></author><author><keyname>Sapiro</keyname><forenames>G.</forenames></author></authors><title>Sparse Modeling of Intrinsic Correspondences</title><categories>cs.GR cs.CG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel sparse modeling approach to non-rigid shape matching using
only the ability to detect repeatable regions. As the input to our algorithm,
we are given only two sets of regions in two shapes; no descriptors are
provided so the correspondence between the regions is not know, nor we know how
many regions correspond in the two shapes. We show that even with such scarce
information, it is possible to establish very accurate correspondence between
the shapes by using methods from the field of sparse modeling, being this, the
first non-trivial use of sparse models in shape correspondence. We formulate
the problem of permuted sparse coding, in which we solve simultaneously for an
unknown permutation ordering the regions on two shapes and for an unknown
correspondence in functional representation. We also propose a robust variant
capable of handling incomplete matches. Numerically, the problem is solved
efficiently by alternating the solution of a linear assignment and a sparse
coding problem. The proposed methods are evaluated qualitatively and
quantitatively on standard benchmarks containing both synthetic and scanned
objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6561</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6561</id><created>2012-09-28</created><updated>2013-07-31</updated><authors><author><keyname>Borboudakis</keyname><forenames>Giorgos</forenames></author><author><keyname>Tsamardinos</keyname><forenames>Ioannis</forenames></author></authors><title>Scoring and Searching over Bayesian Networks with Causal and Associative
  Priors</title><categories>cs.AI cs.LG stat.ML</categories><comments>Accepted for publication to the 29th Conference on Uncertainty in
  Artificial Intelligence (UAI-2013). The content of the paper is identical to
  the published one, but the compiler at arXiv produces a 11 page long paper,
  whereas the compiler we used produces a 10 page long paper (page limit for
  the conference)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant theoretical advantage of search-and-score methods for learning
Bayesian Networks is that they can accept informative prior beliefs for each
possible network, thus complementing the data. In this paper, a method is
presented for assigning priors based on beliefs on the presence or absence of
certain paths in the true network. Such beliefs correspond to knowledge about
the possible causal and associative relations between pairs of variables. This
type of knowledge naturally arises from prior experimental and observational
data, among others. In addition, a novel search-operator is proposed to take
advantage of such prior knowledge. Experiments show that, using path beliefs
improves the learning of the skeleton, as well as the edge directions in the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6570</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6570</id><created>2012-09-28</created><authors><author><keyname>Bal</keyname><forenames>Deepak</forenames></author><author><keyname>Bennett</keyname><forenames>Patrick</forenames></author><author><keyname>Bohman</keyname><forenames>Tom</forenames></author><author><keyname>Frieze</keyname><forenames>Alan</forenames></author></authors><title>A greedy algorithm for finding a large 2-matching on a random cubic
  graph</title><categories>math.CO cs.DM</categories><comments>23pp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 2-matching of a graph $G$ is a spanning subgraph with maximum degree two.
The size of a 2-matching $U$ is the number of edges in $U$ and this is at least
$n-\k(U)$ where $n$ is the number of vertices of $G$ and $\k$ denotes the
number of components. In this paper, we analyze the performance of a greedy
algorithm \textsc{2greedy} for finding a large 2-matching on a random 3-regular
graph. We prove that with high probability, the algorithm outputs a 2-matching
$U$ with $\k(U) = \tilde{\Theta}\of{n^{1/5}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6577</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6577</id><created>2012-09-28</created><updated>2013-02-04</updated><authors><author><keyname>Yousaf</keyname><forenames>Suhail</forenames></author><author><keyname>Bakhshi</keyname><forenames>Rena</forenames></author><author><keyname>van Steen</keyname><forenames>Maarten</forenames></author><author><keyname>Voulgaris</keyname><forenames>Spyros</forenames></author><author><keyname>Kelley</keyname><forenames>John L.</forenames></author></authors><title>Exploring Design Tradeoffs Of A Distributed Algorithm For Cosmic Ray
  Event Detection</title><categories>physics.ins-det astro-ph.IM cs.DC</categories><comments>submitted to JINST</comments><doi>10.1088/1748-0221/8/03/P03011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many sensor networks, including large particle detector arrays measuring
high-energy cosmic-ray air showers, traditionally rely on centralised trigger
algorithms to find spatial and temporal coincidences of individual nodes. Such
schemes suffer from scalability problems, especially if the nodes communicate
wirelessly or have bandwidth limitations. However, nodes which instead
communicate with each other can, in principle, use a distributed algorithm to
find coincident events themselves without communication with a central node. We
present such an algorithm and consider various design tradeoffs involved, in
the context of a potential trigger for the Auger Engineering Radio Array
(AERA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6578</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6578</id><created>2012-09-27</created><authors><author><keyname>Nielsen</keyname><forenames>Bo Friis</forenames></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames></author><author><keyname>Pilegaard</keyname><forenames>Henrik</forenames></author><author><keyname>Smith</keyname><forenames>Michael James Andrew</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Ender</forenames></author><author><keyname>Zeng</keyname><forenames>Kebin</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>Roadmap Document on Stochastic Analysis</title><categories>cs.LO cs.FL</categories><comments>This work has been supported by MT-LAB, a VKR Centre of Excellence
  for the Modelling of Information Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document was prepared as part of the MT-LAB research centre. The
research centre studies the Modelling of Information Technology and is a VKR
Centre of Excellence funded for five years by the VILLUM Foundation. You can
read more about MT-LAB at its webpage www.MT-LAB.dk.
  The goal of the document is to serve as an introduction to new PhD students
addressing the research goals of MT-LAB. As such it aims to provide an overview
of a number of selected approaches to the modelling of stochastic systems. It
should be readable not only by computers scientists with a background in formal
methods but also by PhD students in stochastics that are interested in
understanding the computer science approach to stochastic model checking.
  We have no intention of being encyclopedic in our treatment of the approaches
or the literature. Rather we have made the selection of material based on the
competences of the groups involved in or closely affiliated to MT-LAB, so as to
ease the task of the PhD students in navigating an otherwise vast amount of
literature.
  We have decided to publish the document in case other young researchers may
find it helpful. The list of authors reflect those that have at times played a
significant role in the production of the document.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6580</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6580</id><created>2012-09-28</created><updated>2013-02-07</updated><authors><author><keyname>Marynowski</keyname><forenames>Jo&#xe3;o Eugenio</forenames></author><author><keyname>Albonico</keyname><forenames>Michel</forenames></author><author><keyname>de Almeida</keyname><forenames>Eduardo Cunha</forenames></author><author><keyname>Suny&#xe9;</keyname><forenames>Gerson</forenames></author></authors><title>Testing MapReduce-Based Systems</title><categories>cs.DC cs.DB cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MapReduce (MR) is the most popular solution to build applications for
large-scale data processing. These applications are often deployed on large
clusters of commodity machines, where failures happen constantly due to bugs,
hardware problems, and outages. Testing MR-based systems is hard, since it is
needed a great effort of test harness to execute distributed test cases upon
failures. In this paper, we present a novel testing solution to tackle this
issue called HadoopTest. This solution is based on a scalable harness approach,
where distributed tester components are hung around each map and reduce worker
(i.e., node). Testers are allowed to stimulate each worker to inject failures
on them, monitor their behavior, and validate testing results. HadoopTest was
used to test two applications bundled into Hadoop, the Apache open source
MapReduce implementation. Our initial implementation demonstrates promising
results, with HadoopTest coordinating test cases across distributed MapReduce
workers, and finding bugs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6600</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6600</id><created>2012-09-26</created><authors><author><keyname>Lawyer</keyname><forenames>Glenn</forenames></author></authors><title>Measuring node spreading power by expected cluster degree</title><categories>cs.SI physics.soc-ph</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional metrics of node influence such as degree or betweenness identify
highly influential nodes, but are rarely usefully accurate in quantifying the
spreading power of nodes which are not. Such nodes are the vast majority of the
network, and the most likely entry points for novel influences, be they
pandemic disease or new ideas. Several recent works have suggested metrics
based on path counting. The current work proposes instead using the expected
number of infected-susceptible edges, and shows that this measure predicts
spreading power in discrete time, continuous time, and competitive spreading
processes simulated on large random networks and on real world networks.
Applied to the Ugandan road network, it predicts that Ebola is unlikely to pose
a pandemic threat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6615</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6615</id><created>2012-09-28</created><updated>2012-10-18</updated><authors><author><keyname>Birkholz</keyname><forenames>Julie M.</forenames></author><author><keyname>Bakhshi</keyname><forenames>Rena</forenames></author><author><keyname>Harige</keyname><forenames>Ravindra</forenames></author><author><keyname>van Steen</keyname><forenames>Maarten</forenames></author><author><keyname>Groenewegen</keyname><forenames>Peter</forenames></author></authors><title>Scalable Analysis for Large Social Networks: the data-aware mean-field
  approach</title><categories>cs.SI cs.PF math.PR physics.soc-ph</categories><comments>Accepted to SocInfo 2012; full version including appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies on social networks have proved that endogenous and exogenous factors
influence dynamics. Two streams of modeling exist on explaining the dynamics of
social networks: 1) models predicting links through network properties, and 2)
models considering the effects of social attributes. In this interdisciplinary
study we work to overcome a number of computational limitations within these
current models. We employ a mean-field model which allows for the construction
of a population-specific socially informed model for predicting links from both
network and social properties in large social networks. The model is tested on
a population of conference coauthorship behavior, considering a number of
parameters from available Web data. We address how large social networks can be
modeled preserving both network and social parameters. We prove that the
mean-field model, using a data-aware approach, allows us to overcome
computational burdens and thus scalability issues in modeling large social
networks in terms of both network and social parameters. Additionally, we
confirm that large social networks evolve through both network and
social-selection decisions; asserting that the dynamics of networks cannot
singly be studied from a single perspective but must consider effects of social
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6626</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6626</id><created>2012-09-28</created><updated>2012-10-09</updated><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author></authors><title>On Newton-Raphson iteration for multiplicative inverses modulo prime
  powers</title><categories>cs.SC cs.MS</categories><proxy>ccsd</proxy><journal-ref>IEEE Transactions on Computers (2013) 4</journal-ref><doi>10.1109/TC.2013.94</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study algorithms for the fast computation of modular inverses.
Newton-Raphson iteration over $p$-adic numbers gives a recurrence relation
computing modular inverse modulo $p^m$, that is logarithmic in $m$. We solve
the recurrence to obtain an explicit formula for the inverse. Then we study
different implementation variants of this iteration and show that our explicit
formula is interesting for small exponent values but slower or large exponent,
say of more than 700 bits. Overall we thus propose a hybrid combination of our
explicit formula and the best asymptotic variants. This hybrid combination
yields then a constant factor improvement, also for large exponents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1209.6630</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1209.6630</id><created>2012-09-28</created><updated>2012-10-01</updated><authors><author><keyname>Scemama</keyname><forenames>Anthony</forenames><affiliation>LCPQ</affiliation></author><author><keyname>Caffarel</keyname><forenames>Michel</forenames><affiliation>LCPQ</affiliation></author><author><keyname>Oseret</keyname><forenames>Emmanuel</forenames><affiliation>ECR</affiliation></author><author><keyname>Jalby</keyname><forenames>William</forenames><affiliation>ECR, PRISM</affiliation></author></authors><title>Quantum Monte Carlo for large chemical systems: Implementing efficient
  strategies for petascale platforms and beyond</title><categories>cs.PF cs.CE physics.comp-ph</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various strategies to implement efficiently QMC simulations for large
chemical systems are presented. These include: i.) the introduction of an
efficient algorithm to calculate the computationally expensive Slater matrices.
This novel scheme is based on the use of the highly localized character of
atomic Gaussian basis functions (not the molecular orbitals as usually done),
ii.) the possibility of keeping the memory footprint minimal, iii.) the
important enhancement of single-core performance when efficient optimization
tools are employed, and iv.) the definition of a universal, dynamic,
fault-tolerant, and load-balanced computational framework adapted to all kinds
of computational platforms (massively parallel machines, clusters, or
distributed grids). These strategies have been implemented in the QMC=Chem code
developed at Toulouse and illustrated with numerical applications on small
peptides of increasing sizes (158, 434, 1056 and 1731 electrons). Using 10k-80k
computing cores of the Curie machine (GENCI-TGCC-CEA, France) QMC=Chem has been
shown to be capable of running at the petascale level, thus demonstrating that
for this machine a large part of the peak performance can be achieved.
Implementation of large-scale QMC simulations for future exascale platforms
with a comparable level of efficiency is expected to be feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0003</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0003</id><created>2012-09-28</created><updated>2015-02-21</updated><authors><author><keyname>Cai</keyname><forenames>Mingjie</forenames></author><author><keyname>Li</keyname><forenames>Qingguo</forenames></author><author><keyname>Lang</keyname><forenames>Guangming</forenames></author></authors><title>Compression of dynamic fuzzy relation information systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper further investigates the data compression of fuzzy relation
information systems. First, we introduce an algorithm for constructing the
homomorphism between fuzzy relation information systems. Then, we discuss that
how to compress the dynamic fuzzy relation information systems by utilizing the
compression of the original systems. Afterwards, several illustrative examples
are employed to show that the data compression of fuzzy relation information
systems and dynamic fuzzy relation information systems can be simplified
significantly by our proposed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0010</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0010</id><created>2012-09-28</created><authors><author><keyname>Krotov</keyname><forenames>Denis</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>A partition of the hypercube into cosets of maximally nonparallel
  Hamming codes</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>6 pages</comments><msc-class>94B25, 05B30</msc-class><journal-ref>J. Comb. Des. 24(4) 2014, 179-187</journal-ref><doi>10.1002/jcd.21363</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By use of the Gold map, we construct a partition of the hypercube into cosets
of Hamming codes that have minimal possible pairwise intersection cardinality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0026</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0026</id><created>2012-09-28</created><authors><author><keyname>Kovnatsky</keyname><forenames>A.</forenames></author><author><keyname>Bronstein</keyname><forenames>M. M.</forenames></author><author><keyname>Bronstein</keyname><forenames>A. M.</forenames></author><author><keyname>Glashoff</keyname><forenames>K.</forenames></author><author><keyname>Kimmel</keyname><forenames>R.</forenames></author></authors><title>Coupled quasi-harmonic bases</title><categories>cs.CV cs.GR</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of Laplacian eigenbases has been shown to be fruitful in many
computer graphics applications. Today, state-of-the-art approaches to shape
analysis, synthesis, and correspondence rely on these natural harmonic bases
that allow using classical tools from harmonic analysis on manifolds. However,
many applications involving multiple shapes are obstacled by the fact that
Laplacian eigenbases computed independently on different shapes are often
incompatible with each other. In this paper, we propose the construction of
common approximate eigenbases for multiple shapes using approximate joint
diagonalization algorithms. We illustrate the benefits of the proposed approach
on tasks from shape editing, pose transfer, correspondence, and similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0049</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0049</id><created>2012-09-28</created><authors><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author><author><keyname>Reingold</keyname><forenames>Omer</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author></authors><title>Better Pseudorandom Generators from Milder Pseudorandom Restrictions</title><categories>cs.CC</categories><comments>To appear in FOCS 2012</comments><msc-class>68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an iterative approach to constructing pseudorandom generators,
based on the repeated application of mild pseudorandom restrictions. We use
this template to construct pseudorandom generators for combinatorial rectangles
and read-once CNFs and a hitting set generator for width-3 branching programs,
all of which achieve near-optimal seed-length even in the low-error regime: We
get seed-length O(log (n/epsilon)) for error epsilon. Previously, only
constructions with seed-length O(\log^{3/2} n) or O(\log^2 n) were known for
these classes with polynomially small error.
  The (pseudo)random restrictions we use are milder than those typically used
for proving circuit lower bounds in that we only set a constant fraction of the
bits at a time. While such restrictions do not simplify the functions
drastically, we show that they can be derandomized using small-bias spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0052</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0052</id><created>2012-09-28</created><authors><author><keyname>Sarhrouni</keyname><forenames>ELkebir</forenames></author><author><keyname>Hammouch</keyname><forenames>Ahmed</forenames></author><author><keyname>Aboutajdine</keyname><forenames>Driss</forenames></author></authors><title>Dimensionality Reduction and Classification feature using Mutual
  Information applied to Hyperspectral Images : A Filter strategy based
  algorithm</title><categories>cs.CV</categories><comments>11 pages, 5 figures, journal paper</comments><msc-class>68U10, 68R05</msc-class><acm-class>I.4.7; I.4.8; I.4.9</acm-class><journal-ref>Applied Mathematical Sciences, Vol. 6, 2012, no. 102, 5085 - 5095</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral images (HIS) classification is a high technical remote sensing
tool. The goal is to reproduce a thematic map that will be compared with a
reference ground truth map (GT), constructed by expecting the region. The HIS
contains more than a hundred bidirectional measures, called bands (or simply
images), of the same region. They are taken at juxtaposed frequencies.
Unfortunately, some bands contain redundant information, others are affected by
the noise, and the high dimensionality of features made the accuracy of
classification lower. The problematic is how to find the good bands to classify
the pixels of regions. Some methods use Mutual Information (MI) and threshold,
to select relevant bands, without treatment of redundancy. Others control and
eliminate redundancy by selecting the band top ranking the MI, and if its
neighbors have sensibly the same MI with the GT, they will be considered
redundant and so discarded. This is the most inconvenient of this method,
because this avoids the advantage of hyperspectral images: some precious
information can be discarded. In this paper we'll accept the useful redundancy.
A band contains useful redundancy if it contributes to produce an estimated
reference map that has higher MI with the GT.nTo control redundancy, we
introduce a complementary threshold added to last value of MI. This process is
a Filter strategy; it gets a better performance of classification accuracy and
not expensive, but less preferment than Wrapper strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0056</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0056</id><created>2012-09-28</created><updated>2013-06-25</updated><authors><author><keyname>Li</keyname><forenames>Xiao</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>Convergence and Applications of a Gossip-based Gauss-Newton Algorithm</title><categories>math.NA cs.DC math.OC</categories><comments>accepted by IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gauss-Newton algorithm is a popular and efficient centralized method for
solving non-linear least squares problems. In this paper, we propose a
multi-agent distributed version of this algorithm, named Gossip-based
Gauss-Newton (GGN) algorithm, which can be applied in general problems with
non-convex objectives. Furthermore, we analyze and present sufficient
conditions for its convergence and show numerically that the GGN algorithm
achieves performance comparable to the centralized algorithm, with graceful
degradation in case of network failures. More importantly, the GGN algorithm
provides significant performance gains compared to other distributed first
order methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0061</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0061</id><created>2012-09-28</created><updated>2013-01-28</updated><authors><author><keyname>Wozniak</keyname><forenames>Sander</forenames></author><author><keyname>Rossberg</keyname><forenames>Michael</forenames></author><author><keyname>Girlich</keyname><forenames>Franz</forenames></author><author><keyname>Schaefer</keyname><forenames>Guenter</forenames></author></authors><title>Geocast into the Past: Towards a Privacy-Preserving Spatiotemporal
  Multicast for Cellular Networks</title><categories>cs.CR cs.NI</categories><comments>- added IEEE copyright notice</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces the novel concept of Spatiotemporal Multicast (STM),
which is the issue of sending a message to mobile devices that have been
residing at a specific area during a certain time span in the past. A wide
variety of applications can be envisioned for this concept, including crime
investigation, disease control, and social applications. An important aspect of
these applications is the need to protect the privacy of its users. In this
article, we present an extensive overview of applications and objectives to be
fulfilled by an STM service. Furthermore, we propose a first Cluster-based
Spatiotemporal Multicast (CSTM) approach and provide a detailed discussion of
its privacy features. Finally, we evaluate the performance of our scheme in a
large-scale simulation setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0063</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0063</id><created>2012-09-28</created><updated>2013-07-01</updated><authors><author><keyname>Valdez</keyname><forenames>L. D.</forenames></author><author><keyname>Macri</keyname><forenames>P. A.</forenames></author><author><keyname>Braunstein</keyname><forenames>L. A.</forenames></author></authors><title>Temporal percolation of a susceptible adaptive network</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Physica A 392 (2013) 4172</journal-ref><doi>10.1016/j.physa.2013.05.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decades, many authors have used the
susceptible-infected-recovered model to study the impact of the disease
spreading on the evolution of the infected individuals. However, few authors
focused on the temporal unfolding of the susceptible individuals. In this
paper, we study the dynamic of the susceptible-infected-recovered model in an
adaptive network that mimics the transitory deactivation of permanent social
contacts, such as friendship and work-ship ties. Using an edge-based
compartmental model and percolation theory, we obtain the evolution equations
for the fraction susceptible individuals in the susceptible biggest component.
In particular, we focus on how the individual's behavior impacts on the
dilution of the susceptible network. We show that, as a consequence, the
spreading of the disease slows down, protecting the biggest susceptible cluster
by increasing the critical time at which the giant susceptible component is
destroyed. Our theoretical results are fully supported by extensive
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0065</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0065</id><created>2012-09-28</created><updated>2013-07-15</updated><authors><author><keyname>Min</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Granular association rule mining through parametric rough sets for cold
  start recommendation</title><categories>cs.DB cs.IR</categories><comments>The theory part of the paper should be replaced by formal context
  analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Granular association rules reveal patterns hide in many-to-many relationships
which are common in relational databases. In recommender systems, these rules
are appropriate for cold start recommendation, where a customer or a product
has just entered the system. An example of such rules might be &quot;40% men like at
least 30% kinds of alcohol; 45% customers are men and 6% products are alcohol.&quot;
Mining such rules is a challenging problem due to pattern explosion. In this
paper, we propose a new type of parametric rough sets on two universes to study
this problem. The model is deliberately defined such that the parameter
corresponds to one threshold of rules. With the lower approximation operator in
the new parametric rough sets, a backward algorithm is designed for the rule
mining problem. Experiments on two real world data sets show that the new
algorithm is significantly faster than the existing sandwich algorithm. This
study indicates a new application area, namely recommender systems, of
relational data mining, granular computing and rough sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0066</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0066</id><created>2012-09-28</created><authors><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author></authors><title>Iterative Reweighted Minimization Methods for $l_p$ Regularized
  Unconstrained Nonlinear Programming</title><categories>math.OC cs.LG stat.CO stat.ML</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study general $l_p$ regularized unconstrained minimization
problems. In particular, we derive lower bounds for nonzero entries of first-
and second-order stationary points, and hence also of local minimizers of the
$l_p$ minimization problems. We extend some existing iterative reweighted $l_1$
(IRL1) and $l_2$ (IRL2) minimization methods to solve these problems and
proposed new variants for them in which each subproblem has a closed form
solution. Also, we provide a unified convergence analysis for these methods. In
addition, we propose a novel Lipschitz continuous $\epsilon$-approximation to
$\|x\|^p_p$. Using this result, we develop new IRL1 methods for the $l_p$
minimization problems and showed that any accumulation point of the sequence
generated by these methods is a first-order stationary point, provided that the
approximation parameter $\epsilon$ is below a computable threshold value. This
is a remarkable result since all existing iterative reweighted minimization
methods require that $\epsilon$ be dynamically updated and approach zero. Our
computational results demonstrate that the new IRL1 method is generally more
stable than the existing IRL1 methods [21,18] in terms of objective function
value and CPU time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0071</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0071</id><created>2012-09-28</created><authors><author><keyname>Zhang</keyname><forenames>Jing</forenames></author></authors><title>A Literature Survey of Cooperative Caching in Content Distribution
  Networks</title><categories>cs.NI</categories><comments>5 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Content distribution networks (CDNs) which serve to deliver web objects
(e.g., documents, applications, music and video, etc.) have seen tremendous
growth since its emergence. To minimize the retrieving delay experienced by a
user with a request for a web object, caching strategies are often applied -
contents are replicated at edges of the network which is closer to the user
such that the network distance between the user and the object is reduced. In
this literature survey, evolution of caching is studied. A recent research
paper [15] in the field of large-scale caching for CDN was chosen to be the
anchor paper which serves as a guide to the topic. Research studies after and
relevant to the anchor paper are also analyzed to better evaluate the
statements and results of the anchor paper and more importantly, to obtain an
unbiased view of the large scale collaborate caching systems as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0074</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0074</id><created>2012-09-28</created><authors><author><keyname>Huang</keyname><forenames>Aiping</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Topological characterizations to three types of covering approximation
  operators</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covering-based rough set theory is a useful tool to deal with inexact,
uncertain or vague knowledge in information systems. Topology, one of the most
important subjects in mathematics, provides mathematical tools and interesting
topics in studying information systems and rough sets. In this paper, we
present the topological characterizations to three types of covering
approximation operators. First, we study the properties of topology induced by
the sixth type of covering lower approximation operator. Second, some
topological characterizations to the covering lower approximation operator to
be an interior operator are established. We find that the topologies induced by
this operator and by the sixth type of covering lower approximation operator
are the same. Third, we study the conditions which make the first type of
covering upper approximation operator be a closure operator, and find that the
topology induced by the operator is the same as the topology induced by the
fifth type of covering upper approximation operator. Forth, the conditions of
the second type of covering upper approximation operator to be a closure
operator and the properties of topology induced by it are established. Finally,
these three topologies space are compared. In a word, topology provides a
useful method to study the covering-based rough sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0075</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0075</id><created>2012-09-28</created><authors><author><keyname>Huang</keyname><forenames>Aiping</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Geometric lattice structure of covering-based rough sets through
  matroids</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covering-based rough set theory is a useful tool to deal with inexact,
uncertain or vague knowledge in information systems. Geometric lattice has
widely used in diverse fields, especially search algorithm design which plays
important role in covering reductions. In this paper, we construct four
geometric lattice structures of covering-based rough sets through matroids, and
compare their relationships. First, a geometric lattice structure of
covering-based rough sets is established through the transversal matroid
induced by the covering, and its characteristics including atoms, modular
elements and modular pairs are studied. We also construct a one-to-one
correspondence between this type of geometric lattices and transversal matroids
in the context of covering-based rough sets. Second, sufficient and necessary
conditions for three types of covering upper approximation operators to be
closure operators of matroids are presented. We exhibit three types of matroids
through closure axioms, and then obtain three geometric lattice structures of
covering-based rough sets. Third, these four geometric lattice structures are
compared. Some core concepts such as reducible elements in covering-based rough
sets are investigated with geometric lattices. In a word, this work points out
an interesting view, namely geometric lattice, to study covering-based rough
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0077</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0077</id><created>2012-09-29</created><authors><author><keyname>Sunehag</keyname><forenames>Peter</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Optimistic Agents are Asymptotically Optimal</title><categories>cs.AI cs.LG</categories><comments>13 LaTeX pages</comments><journal-ref>Proc. 25th Australasian Joint Conference on Artificial
  Intelligence (AusAI 2012) 15-26</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use optimism to introduce generic asymptotically optimal reinforcement
learning agents. They achieve, with an arbitrary finite or compact class of
environments, asymptotically optimal behavior. Furthermore, in the finite
deterministic case we provide finite error bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0083</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0083</id><created>2012-09-29</created><authors><author><keyname>Matsui</keyname><forenames>Hajime</forenames></author></authors><title>Decoding a Class of Affine Variety Codes with Fast DFT</title><categories>cs.IT cs.DM math.AC math.AG math.IT</categories><comments>5 pages, 1 figure, to be presented at 2012 International Symposium on
  Information Theory and its Applications (ISITA2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient procedure for error-value calculations based on fast discrete
Fourier transforms (DFT) in conjunction with Berlekamp-Massey-Sakata algorithm
for a class of affine variety codes is proposed. Our procedure is achieved by
multidimensional DFT and linear recurrence relations from Grobner basis and is
applied to erasure-and-error decoding and systematic encoding. The
computational complexity of error-value calculations in our algorithm improves
that in solving systems of linear equations from error correcting pairs in many
cases. A motivating example of our algorithm in case of a Reed-Solomon code and
a numerical example of our algorithm in case of a Hermitian code are also
described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0086</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0086</id><created>2012-09-29</created><updated>2013-03-26</updated><authors><author><keyname>Pezeshki</keyname><forenames>Hamed</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Maham</keyname><forenames>Behrouz</forenames></author></authors><title>Jamming Energy Allocation in Training-Based Multiple Access Systems</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, To appear in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of jamming attack in a multiple access channel with
training-based transmission. First, we derive upper and lower bounds on the
maximum achievable ergodic sum-rate which explicitly shows the impact of
jamming during both the training phase and the data transmission phase. Then,
from the jammer's design perspective, we analytically find the optimal jamming
energy allocation between the two phases that minimizes the derived bounds on
the ergodic sum-rate. Numerical results demonstrate that the obtained optimal
jamming design reduces the ergodic sum-rate of the legitimate users
considerably in comparison to fixed power jamming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0090</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0090</id><created>2012-09-29</created><updated>2014-01-20</updated><authors><author><keyname>Zhang</keyname><forenames>Zhongzhi</forenames></author><author><keyname>Wu</keyname><forenames>Bin</forenames></author><author><keyname>Comellas</keyname><forenames>Francesc</forenames></author></authors><title>The Number of Spanning Trees in Apollonian Networks</title><categories>math.CO cs.DM</categories><comments>Manuscript accepted for publication (Discrete Applied Mathematics)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we find an exact analytical expression for the number of
spanning trees in Apollonian networks. This parameter can be related to
significant topological and dynamic properties of the networks, including
percolation, epidemic spreading, synchronization, and random walks. As
Apollonian networks constitute an interesting family of maximal planar graphs
which are simultaneously small-world, scale-free, Euclidean and space filling
and highly clustered, the study of their spanning trees is of particular
relevance. Our results allow also the calculation of the spanning tree entropy
of Apollonian networks, which then we compare with those of other graphs with
the same average degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0091</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0091</id><created>2012-09-29</created><updated>2013-06-02</updated><authors><author><keyname>Zhao</keyname><forenames>Hong</forenames></author><author><keyname>Min</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Test-cost-sensitive attribute reduction of data with normal distribution
  measurement errors</title><categories>cs.AI</categories><comments>This paper has been withdrawn by the author due to the error of the
  title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The measurement error with normal distribution is universal in applications.
Generally, smaller measurement error requires better instrument and higher test
cost. In decision making based on attribute values of objects, we shall select
an attribute subset with appropriate measurement error to minimize the total
test cost. Recently, error-range-based covering rough set with uniform
distribution error was proposed to investigate this issue. However, the
measurement errors satisfy normal distribution instead of uniform distribution
which is rather simple for most applications. In this paper, we introduce
normal distribution measurement errors to covering-based rough set model, and
deal with test-cost-sensitive attribute reduction problem in this new model.
The major contributions of this paper are four-fold. First, we build a new data
model based on normal distribution measurement errors. With the new data model,
the error range is an ellipse in a two-dimension space. Second, the
covering-based rough set with normal distribution measurement errors is
constructed through the &quot;3-sigma&quot; rule. Third, the test-cost-sensitive
attribute reduction problem is redefined on this covering-based rough set.
Fourth, a heuristic algorithm is proposed to deal with this problem. The
algorithm is tested on ten UCI (University of California - Irvine) datasets.
The experimental results show that the algorithm is more effective and
efficient than the existing one. This study is a step toward realistic
applications of cost-sensitive learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0092</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0092</id><created>2012-09-29</created><updated>2014-01-20</updated><authors><author><keyname>Comellas</keyname><forenames>Francesc</forenames></author><author><keyname>Miralles</keyname><forenames>Alicia</forenames></author><author><keyname>Liu</keyname><forenames>Hongxiao</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongzhi</forenames></author></authors><title>The Number of Spanning Trees of an Infinite Family of Outerplanar,
  Small-World and Self-Similar Graphs</title><categories>math.CO cs.DM</categories><comments>Manuscript submitted for publication</comments><doi>10.1016/j.physa.2012.10.047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give an exact analytical expression for the number of
spanning trees of an infinite family of outerplanar, small-world and
self-similar graphs. This number is an important graph invariant related to
different topological and dynamic properties of the graph, such as its
reliability, synchronization capability and diffusion properties. The
calculation of the number of spanning trees is a demanding and difficult task,
in particular for large graphs, and thus there is much interest in obtaining
closed expressions for relevant infinite graph families. We have also
calculated the spanning tree entropy of the graphs which we have compared with
those for graphs with the same average degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0100</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0100</id><created>2012-09-29</created><authors><author><keyname>Ansari</keyname><forenames>Imran Shafique</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ferkan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>On the Sum of Squared \eta-\mu Random Variates With Application to the
  Performance of Wireless Communication Systems</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><comments>6 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1202.2576</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probability density function (PDF) and cumulative distribution function
of the sum of L independent but not necessarily identically distributed squared
\eta-\mu variates, applicable to the output statistics of maximal ratio
combining (MRC) receiver operating over \eta-\mu fading channels that includes
the Hoyt and the Nakagami-m models as special cases, is presented in
closed-form in terms of the Fox's H-bar function. Further analysis,
particularly on the bit error rate via PDF-based approach, is also represented
in closed form in terms of the extended Fox's H-bar function (H-hat). The
proposed new analytical results complement previous results and are illustrated
by extensive numerical and Monte Carlo simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0115</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0115</id><created>2012-09-29</created><updated>2013-07-03</updated><authors><author><keyname>Sun</keyname><forenames>Guangling</forenames></author></authors><title>Demosaicing and Superresolution for Color Filter Array via Residual
  Image Reconstruction and Sparse Representation</title><categories>cs.CV</categories><comments>the paper has been accepted by a journal</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A framework of demosaicing and superresolution for color filter array (CFA)
via residual image reconstruction and sparse representation is presented.Given
the intermediate image produced by certain demosaicing and interpolation
technique, a residual image between the final reconstruction image and the
intermediate image is reconstructed using sparse representation.The final
reconstruction image has richer edges and details than that of the intermediate
image. Specifically, a generic dictionary is learned from a large set of
composite training data composed of intermediate data and residual data. The
learned dictionary implies a mapping between the two data. A specific
dictionary adaptive to the input CFA is learned thereafter. Using the adaptive
dictionary, the sparse coefficients of intermediate data are computed and
transformed to predict residual image. The residual image is added back into
the intermediate image to obtain the final reconstruction image. Experimental
results demonstrate the state-of-the-art performance in terms of PSNR and
subjective visual perception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0118</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0118</id><created>2012-09-29</created><authors><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Self-Delimiting Neural Networks</title><categories>cs.NE</categories><comments>15 pages</comments><report-no>IDSIA-08-12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-delimiting (SLIM) programs are a central concept of theoretical computer
science, particularly algorithmic information &amp; probability theory, and
asymptotically optimal program search (AOPS). To apply AOPS to (possibly
recurrent) neural networks (NNs), I introduce SLIM NNs. Neurons of a typical
SLIM NN have threshold activation functions. During a computational episode,
activations are spreading from input neurons through the SLIM NN until the
computation activates a special halt neuron. Weights of the NN's used
connections define its program. Halting programs form a prefix code. The reset
of the initial NN state does not cost more than the latest program execution.
Since prefixes of SLIM programs influence their suffixes (weight changes
occurring early in an episode influence which weights are considered later),
SLIM NN learning algorithms (LAs) should execute weight changes online during
activation spreading. This can be achieved by applying AOPS to growing SLIM
NNs. To efficiently teach a SLIM NN to solve many tasks, such as correctly
classifying many different patterns, or solving many different robot control
tasks, each connection keeps a list of tasks it is used for. The lists may be
efficiently updated during training. To evaluate the overall effect of
currently tested weight changes, a SLIM NN LA needs to re-test performance only
on the efficiently computable union of tasks potentially affected by the
current weight changes. Future SLIM NNs will be implemented on 3-dimensional
brain-like multi-processor hardware. Their LAs will minimize task-specific
total wire length of used connections, to encourage efficient solutions of
subtasks by subsets of neurons that are physically close. The novel class of
SLIM NN LAs is currently being probed in ongoing experiments to be reported in
separate papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0128</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0128</id><created>2012-09-29</created><updated>2013-09-05</updated><authors><author><keyname>Hoffmann</keyname><forenames>Till</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Decentralized Routing on Spatial Networks with Stochastic Edge Weights</title><categories>cs.SI cond-mat.dis-nn physics.soc-ph</categories><comments>10 pages, 9 figures (some with multiple parts)</comments><journal-ref>Phys. Rev. E 88, 022815 (2013)</journal-ref><doi>10.1103/PhysRevE.88.022815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate algorithms to find short paths in spatial networks with
stochastic edge weights. Our formulation of the problem of finding short paths
differs from traditional formulations because we specifically do not make two
of the usual simplifying assumptions: (1) we allow edge weights to be
stochastic rather than deterministic; and (2) we do not assume that global
knowledge of a network is available. We develop a decentralized routing
algorithm that provides en route guidance for travelers on a spatial network
with stochastic edge weights without the need to rely on global knowledge about
the network. To guide a traveler, our algorithm uses an estimation function
that evaluates cumulative arrival probability distributions based on distances
between pairs of nodes. The estimation function carries a notion of proximity
between nodes and thereby enables routing without global knowledge. In testing
our decentralized algorithm, we define a criterion that allows one to
discriminate among arrival probability distributions, and we test our algorithm
and this criterion using both synthetic and real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0137</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0137</id><created>2012-09-29</created><updated>2013-01-28</updated><authors><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author><author><keyname>Esch</keyname><forenames>Markus</forenames></author><author><keyname>Chan</keyname><forenames>Connie</forenames></author><author><keyname>Clerot</keyname><forenames>Fabrice</forenames></author><author><keyname>Deville</keyname><forenames>Pierre</forenames></author><author><keyname>Huens</keyname><forenames>Etienne</forenames></author><author><keyname>Morlot</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Smoreda</keyname><forenames>Zbigniew</forenames></author><author><keyname>Ziemlicki</keyname><forenames>Cezary</forenames></author></authors><title>Data for Development: the D4D Challenge on Mobile Phone Data</title><categories>cs.CY cs.SI physics.soc-ph stat.CO</categories><comments>10 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Orange &quot;Data for Development&quot; (D4D) challenge is an open data challenge
on anonymous call patterns of Orange's mobile phone users in Ivory Coast. The
goal of the challenge is to help address society development questions in novel
ways by contributing to the socio-economic development and well-being of the
Ivory Coast population. Participants to the challenge are given access to four
mobile phone datasets and the purpose of this paper is to describe the four
datasets. The website http://www.d4d.orange.com contains more information about
the participation rules. The datasets are based on anonymized Call Detail
Records (CDR) of phone calls and SMS exchanges between five million of Orange's
customers in Ivory Coast between December 1, 2011 and April 28, 2012. The
datasets are: (a) antenna-to-antenna traffic on an hourly basis, (b) individual
trajectories for 50,000 customers for two week time windows with antenna
location information, (3) individual trajectories for 500,000 customers over
the entire observation period with sub-prefecture location information, and (4)
a sample of communication graphs for 5,000 customers
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0140</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0140</id><created>2012-09-29</created><authors><author><keyname>Lopez-Permouth</keyname><forenames>Sergio R.</forenames></author><author><keyname>Ozadam</keyname><forenames>Hakan</forenames></author><author><keyname>Ozbudak</keyname><forenames>Ferruh</forenames></author><author><keyname>Szabo</keyname><forenames>Steve</forenames></author></authors><title>Polycyclic codes over Galois rings with applications to repeated-root
  constacyclic codes</title><categories>math.RA cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:0906.4008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic, negacyclic and constacyclic codes are part of a larger class of codes
called polycyclic codes; namely, those codes which can be viewed as ideals of a
factor ring of a polynomial ring. The structure of the ambient ring of
polycyclic codes over GR(p^a,m) and generating sets for its ideals are
considered. Along with some structure details of the ambient ring, the
existance of a certain type of generating set for an ideal is proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0149</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0149</id><created>2012-09-29</created><authors><author><keyname>Wang</keyname><forenames>Jiadong</forenames></author><author><keyname>Dong</keyname><forenames>Guiqiang</forenames></author><author><keyname>Courtade</keyname><forenames>Thomas</forenames></author><author><keyname>Shankar</keyname><forenames>Hari</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author><author><keyname>Wesel</keyname><forenames>Richard</forenames></author></authors><title>LDPC Decoding with Limited-Precision Soft Information in Flash Memories</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the application of low-density parity-check (LDPC)
codes to Flash memories. Multiple cell reads with distinct word-line voltages
provide limited-precision soft information for the LDPC decoder. The values of
the word-line voltages (also called reference voltages) are optimized by
maximizing the mutual information (MI) between the input and output of the
multiple-read channel. Constraining the maximum mutual-information (MMI)
quantization to enforce a constant-ratio constraint provides a significant
simplification with no noticeable loss in performance.
  Our simulation results suggest that for a well-designed LDPC code, the
quantization that maximizes the mutual information will also minimize the frame
error rate. However, care must be taken to design the code to perform well in
the quantized channel. An LDPC code designed for a full-precision Gaussian
channel may perform poorly in the quantized setting. Our LDPC code designs
provide an example where quantization increases the importance of absorbing
sets thus changing how the LDPC code should be optimized.
  Simulation results show that small increases in precision enable the LDPC
code to significantly outperform a BCH code with comparable rate and block
length (but without the benefit of the soft information) over a range of frame
error rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0151</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0151</id><created>2012-09-29</created><authors><author><keyname>Chu</keyname><forenames>Yu-Wei</forenames></author><author><keyname>Tai</keyname><forenames>Chih-Hua</forenames></author><author><keyname>Chen</keyname><forenames>Ming-Syan</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Implementation of Privacy-preserving SimRank over Distributed
  Information Network</title><categories>cs.CR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information network analysis has drawn a lot attention in recent years. Among
all the aspects of network analysis, similarity measure of nodes has been shown
useful in many applications, such as clustering, link prediction and community
identification, to name a few. As linkage data in a large network is inherently
sparse, it is noted that collecting more data can improve the quality of
similarity measure. This gives different parties a motivation to cooperate. In
this paper, we address the problem of link-based similarity measure of nodes in
an information network distributed over different parties. Concerning the data
privacy, we propose a privacy-preserving SimRank protocol based on
fully-homomorphic encryption to provide cryptographic protection for the links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0153</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0153</id><created>2012-09-29</created><authors><author><keyname>Aalsalem</keyname><forenames>Mohammed Y</forenames></author><author><keyname>Khan</keyname><forenames>Wazir Zada</forenames></author><author><keyname>Arshad</keyname><forenames>Quratul Ain</forenames></author></authors><title>A Low Cost Vision Based Hybrid Fiducial Mark Tracking Technique for
  Mobile Industrial Robots</title><categories>cs.CV cs.RO</categories><comments>6 pages, 7 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 4, No 2, July 2012 ISSN (Online): 1694-0814 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of robotic vision is developing rapidly. Robots can react
intelligently and provide assistance to user activities through sentient
computing. Since industrial applications pose complex requirements that cannot
be handled by humans, an efficient low cost and robust technique is required
for the tracking of mobile industrial robots. The existing sensor based
techniques for mobile robot tracking are expensive and complex to deploy,
configure and maintain. Also some of them demand dedicated and often expensive
hardware. This paper presents a low cost vision based technique called Hybrid
Fiducial Mark Tracking (HFMT) technique for tracking mobile industrial robot.
HFMT technique requires off-the-shelf hardware (CCD cameras) and printable 2-D
circular marks used as fiducials for tracking a mobile industrial robot on a
pre-defined path. This proposed technique allows the robot to track on a
predefined path by using fiducials for the detection of Right and Left turns on
the path and White Strip for tracking the path. The HFMT technique is
implemented and tested on an indoor mobile robot at our laboratory.
Experimental results from robot navigating in real environments have confirmed
that our approach is simple and robust and can be adopted in any hostile
industrial environment where humans are unable to work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0155</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0155</id><created>2012-09-29</created><authors><author><keyname>Maya</keyname><forenames>Avishay</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>Incentive Compatible Two Player Cake Cutting</title><categories>cs.GT</categories><comments>19 pages, WINE 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize methods of dividing a cake between two bidders in a way that
is incentive-compatible and Pareto-efficient. In our cake cutting model, each
bidder desires a subset of the cake (with a uniform value over this subset),
and is allocated some subset. Our characterization proceeds via reducing to a
simple one-dimensional version of the problem, and yields, for example, a tight
bound on the social welfare achievable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0160</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0160</id><created>2012-09-29</created><updated>2013-03-05</updated><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Compute-and-Forward Strategies for Cooperative Distributed Antenna
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a distributed antenna system where $L$ antenna terminals (ATs) are
connected to a Central Processor (CP) via digital error-free links of finite
capacity $R_0$, and serve $K$ user terminals (UTs). We contribute to the
subject in the following ways: 1) for the uplink, we apply the &quot;Compute and
Forward&quot; (CoF) approach and examine the corresponding system optimization at
finite SNR; 2) For the downlink, we propose a novel precoding scheme nicknamed
&quot;Reverse Compute and Forward&quot; (RCoF); 3) In both cases, we present
low-complexity versions of CoF and RCoF based on standard scalar quantization
at the receivers, that lead to discrete-input discrete-output symmetric
memoryless channel models for which near-optimal performance can be achieved by
standard single-user linear coding; 4) For the case of large $R_0$, we propose
a novel &quot;Integer Forcing Beamforming&quot; (IFB) scheme that generalizes the popular
zero-forcing beamforming and achieves sum rate performance close to the optimal
Gaussian Dirty-Paper Coding.
  The proposed uplink and downlink system optimization focuses specifically on
the ATs and UTs selection problem. We present low-complexity ATs and UTs
selection schemes and demonstrate, through Monte Carlo simulation in a
realistic environment with fading and shadowing, that the proposed schemes
essentially eliminate the problem of rank deficiency of the system matrix and
greatly mitigate the non-integer penalty affecting CoF/RCoF at high SNR.
Comparison with other state-of-the art information theoretic schemes, such as
&quot;Quantize reMap and Forward&quot; for the uplink and &quot;Compressed Dirty Paper Coding&quot;
for the downlink, show competitive performance of the proposed approaches with
significantly lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0167</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0167</id><created>2012-09-30</created><authors><author><keyname>Waskita</keyname><forenames>A. A.</forenames></author><author><keyname>Suhartanto</keyname><forenames>H.</forenames></author><author><keyname>Akbar</keyname><forenames>Z.</forenames></author><author><keyname>Handoko</keyname><forenames>L. T.</forenames></author></authors><title>Exhaustive Search-based Model for Hybrid Sensor Network</title><categories>cs.AI cs.CG</categories><comments>6 pages, Proceeding of the International Conference on Intelligent &amp;
  Advanced Systems 2012 pp. 557-561</comments><report-no>FISIKALIPI-12011</report-no><acm-class>F.2.2; G.2.2</acm-class><doi>10.1109/ICIAS.2012.6306077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new model for a cluster of hybrid sensors network with multi sub-clusters
is proposed. The model is in particular relevant to the early warning system in
a large scale monitoring system in, for example, a nuclear power plant. It
mainly addresses to a safety critical system which requires real-time processes
with high accuracy. The mathematical model is based on the extended
conventional search algorithm with certain interactions among the nearest
neighborhood of sensors. It is argued that the model could realize a highly
accurate decision support system with less number of parameters. A case of one
dimensional interaction function is discussed, and a simple algorithm for the
model is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0171</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0171</id><created>2012-09-30</created><authors><author><keyname>Hallikar</keyname><forenames>Rohini S.</forenames></author><author><keyname>Kumari</keyname><forenames>Uttara</forenames></author><author><keyname>Padmaraju</keyname><forenames>K</forenames></author></authors><title>A novel method for obtaining a better quality speech signal for cochlear
  Implants using kalman with drnl and ssb technique</title><categories>cs.SD</categories><comments>5 pages</comments><journal-ref>Advanced Computing: An International Journal ( ACIJ ), 3(4), 71 -
  75. 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cochlear implant devices are known to exist since a long time. The purpose of
the present work is to develop a speech algorithm for obtaining robust speech.
In this paper, the technique of cochlear implant is first introduced, followed
by discussions of some of the existing techniques available for obtaining
speech. The next section introduces a new technique for obtaining robust
speech. The key feature of this technique lies in the use of the advantages of
an integrated approach involving the use of an estimation technique such as a
kalman filter with non linear filter bank strategy, using Dual Resonance Non
Linear(DRNL) and Single Side Band(SSB) Encoding method. A comparative study of
the proposed method with the existing method indicates that the proposed method
performs well compared to the existing method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0187</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0187</id><created>2012-09-30</created><authors><author><keyname>Gupta</keyname><forenames>Sandeep</forenames></author></authors><title>External Memory based Distributed Generation of Massive Scale Social
  Networks on Small Clusters</title><categories>cs.DB cs.DC</categories><comments>8 pages, 4 pics</comments><acm-class>H.2.4; C.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small distributed systems are limited by their main memory to generate
massively large graphs. Trivial extension to current graph generators to
utilize external memory leads to large amount of random I/O hence do not scale
with size. In this work we offer a technique to generate massive scale graphs
on small cluster of compute nodes with limited main memory. We develop several
distributed and external memory algorithms, primarily, shuffle, relabel,
redistribute, and, compressed-sparse-row (csr) convert. The algorithms are
implemented in MPI/pthread model to help parallelize the operations across
multicores within each core. Using our scheme it is feasible to generate a
graph of size $2^{38}$ nodes (scale 38) using only 64 compute nodes. This can
be compared with the current scheme would require at least 8192 compute node,
assuming 64GB of main memory.
  Our work has broader implications for external memory graph libraries such as
STXXL and graph processing on SSD-based supercomputers such as Dash and Gordon
[1][2].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0203</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0203</id><created>2012-09-30</created><updated>2014-12-30</updated><authors><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Deng</keyname><forenames>Xiaotie</forenames></author><author><keyname>Goldberg</keyname><forenames>Paul. W.</forenames></author><author><keyname>Zhang</keyname><forenames>Jinshan</forenames></author></authors><title>On Revenue Maximization with Sharp Multi-Unit Demands</title><categories>cs.GT</categories><comments>page23</comments><msc-class>91-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider markets consisting of a set of indivisible items, and buyers that
have {\em sharp} multi-unit demand. This means that each buyer $i$ wants a
specific number $d_i$ of items; a bundle of size less than $d_i$ has no value,
while a bundle of size greater than $d_i$ is worth no more than the most valued
$d_i$ items (valuations being additive). We consider the objective of setting
prices and allocations in order to maximize the total revenue of the market
maker. The pricing problem with sharp multi-unit demand buyers has a number of
properties that the unit-demand model does not possess, and is an important
question in algorithmic pricing. We consider the problem of computing a revenue
maximizing solution for two solution concepts: competitive equilibrium and
envy-free pricing.
  For unrestricted valuations, these problems are NP-complete; we focus on a
realistic special case of &quot;correlated values&quot; where each buyer $i$ has a
valuation $v_i\qual_j$ for item $j$, where $v_i$ and $\qual_j$ are positive
quantities associated with buyer $i$ and item $j$ respectively. We present a
polynomial time algorithm to solve the revenue-maximizing competitive
equilibrium problem. For envy-free pricing, if the demand of each buyer is
bounded by a constant, a revenue maximizing solution can be found efficiently;
the general demand case is shown to be NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0210</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0210</id><created>2012-09-30</created><authors><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author><author><keyname>Kumar</keyname><forenames>M. Sathish</forenames></author><author><keyname>Mruthyunjaya</keyname><forenames>H. S.</forenames></author></authors><title>A New Generalized Closed Form Expression for Average Bit Error
  Probability Over Rayleigh Fading Channel</title><categories>cs.IT math.IT</categories><comments>8 pages, 4 figures, ICIIS 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Except for a few simple digital modulation techniques, derivation of average
bit error probability over fading channels is difficult and is an involved
process. In this letter, curve fitting technique has been employed to express
bit error probability over AWGN of any digital modulation scheme in terms of a
simple Gaussian function. Using this Gaussian function, a generalized closed
form expression for computing average probability of bit error over Rayleigh
fading channels has been derived. Excellent agreement has been found between
error probabilities computed with our method and the rigorously calculated
error probabilities of several digital modulation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0225</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0225</id><created>2012-09-30</created><authors><author><keyname>Buda</keyname><forenames>Andrzej</forenames></author><author><keyname>Jarynowski</keyname><forenames>Andrzej</forenames></author></authors><title>Network structure of phonographic market with characteristic
  similarities between musicians</title><categories>nlin.AO cs.SI physics.soc-ph stat.AP</categories><comments>15p</comments><journal-ref>ACTA PHYSICA POLONICA A, Vol. 123, No. 3 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate relations between best selling artists in last decade on
phonographic market and from perspective of listeners by using the Social
Network Analyzes. Starting network is obtained from the matrix of correlations
between the world's best selling artists by considering the synchronous time
evolution of weekly record sales. This method reveals the structure of
phonographic market, but we claim that it has no impact on people who see
relationship between artists and music genres. We compare 'sale' (based on
correlation of record sales) or 'popularity' (based on data mining of the
record charts) networks with 'similarity' (obtained mainly from survey within
music experts opinion) and find no significant relations. We postulate that
non-laminar phenomena on this specific market introduce turbulence to how
people view relations of artists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0228</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0228</id><created>2012-09-30</created><authors><author><keyname>Mondal</keyname><forenames>Nabarun</forenames></author><author><keyname>Ghosh</keyname><forenames>Partha P.</forenames></author></authors><title>Invariance And Inner Fractals In Polynomial And Transcendental Fractals</title><categories>math.DS cs.GR nlin.PS</categories><comments>22 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lot of formal and informal recreational study took place in the fields of
Meromorphic Maps, since Mandelbrot popularized the map z &lt;- z^2 + c. An
immediate generalization of the Mandelbrot z &lt;-z^n + c also known as the
Multibrot family were also studied. In the current paper, general truncated
polynomial maps of the form z &lt;- \sum_{p&gt;=2} a_px^p +c are studied. Two
fundamental properties of these polynomial maps are hereby presented. One of
them is the existence of shape preserving transformations on fractal images,
and another one is the existence of embedded Multibrot fractals inside a
polynomial fractal. Any transform expression with transcendental terms also
shows embedded Multibrot fractals, due to Taylor series expansion possible on
the transcendental functions. We present a method by which existence of
embedded fractals can be predicted. A gallery of images is presented alongside
to showcase the findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0230</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0230</id><created>2012-09-30</created><authors><author><keyname>Chen</keyname><forenames>Xujin</forenames></author><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Hu</keyname><forenames>Xiaodong</forenames></author><author><keyname>Ma</keyname><forenames>Weidong</forenames></author><author><keyname>van Stee</keyname><forenames>Rob</forenames></author><author><keyname>Winzen</keyname><forenames>Carola</forenames></author></authors><title>The Price of Anarchy for Selfish Ring Routing is Two</title><categories>cs.GT</categories><comments>Full version of WINE 2012 paper, 24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the network congestion game with atomic players, asymmetric
strategies, and the maximum latency among all players as social cost. This
important social cost function is much less understood than the average
latency. We show that the price of anarchy is at most two, when the network is
a ring and the link latencies are linear. Our bound is tight. This is the first
sharp bound for the maximum latency objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0234</identifier>
 <datestamp>2014-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0234</id><created>2012-09-30</created><updated>2014-01-07</updated><authors><author><keyname>Herlin</keyname><forenames>Jacob</forenames></author><author><keyname>Nelson</keyname><forenames>Anna</forenames></author><author><keyname>Scheepers</keyname><forenames>Marion</forenames></author></authors><title>Using Ciliate Operations to construct Chromosome Phylogenies</title><categories>q-bio.GN cs.CE cs.DM math.CO</categories><comments>31 pages, 14 figures. Preliminary report</comments><report-no>REUG01</report-no><msc-class>05E15, 20B99, 92-08, 92D15, 92D99</msc-class><acm-class>F.2.2; G.2.1; G.2.3; I.1.2; I.5.3; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an algorithm based on three basic DNA editing operations suggested
by a model for ciliate micronuclear decryption, to transform a given
permutation into another. The number of ciliate operations performed by our
algorithm during such a transformation is taken to be the distance between two
such permutations. Applying well-known clustering methods to such distance
functions enables one to determine phylogenies among the items to which the
distance functions apply. As an application of these ideas we explore the
relationships among the chromosomes of eight fruitfly (drosophila) species,
using the well-known UPGMA algorithm on the distance function provided by our
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0252</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0252</id><created>2012-09-30</created><updated>2014-02-15</updated><authors><author><keyname>Fkih</keyname><forenames>Fethi</forenames></author><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author><author><keyname>Toumia</keyname><forenames>Imen</forenames></author></authors><title>A Linguistic Model for Terminology Extraction based Conditional Random
  Fields</title><categories>cs.CL cs.AI</categories><comments>This paper has been withdrawn by the author due to the poor
  readability and the low quality of the English</comments><acm-class>I.2.6; I.2.7</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we show the possibility of using a linear Conditional Random
Fields (CRF) for terminology extraction from a specialized text corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0257</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0257</id><created>2012-09-30</created><updated>2014-11-14</updated><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>Kernels for (connected) Dominating Set on graphs with Excluded
  Topological subgraphs</title><categories>cs.DS cs.DM</categories><msc-class>05C85, 68R10, 05C69, 05C83</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first linear kernels for Dominating Set and Connected Dominating
Set problems on graphs excluding a fixed graph H as a topological minor. In
other words, we give polynomial time algorithms that, for a given
H-topological-minor-free graph G and a positive integer k, output an
H-topological-minor-free graph G' on O(k) vertices such that G has a
(connected) dominating set of size k if and only if G' has. Our results extend
the known classes of graphs on which Dominating Set and Connected Dominating
Set problems admit linear kernels. Prior to our work, it was known that these
prob- lems admit linear kernels on graphs excluding a fixed apex graph H as a
minor. Moreover, for Dominating Set, a kernel of size k^c(H), where c(H) is a
constant depending on the size of H, follows from a more general result on the
kernelization of Dominating Set on graphs of bounded degeneracy. Alon and
Gutner asked explicitly, whether one can obtain a linear kernel for Dominating
Set on H-minor-free graphs. We answer this question in affirmative and in fact
prove a more general result. For Connected Dominating Set no polynomial kernel
even on H-minor-free graphs was known prior to our work. On the negative side,
it is known that Connected Dominating Set on 2-degenerated graphs does not
admit a polynomial kernel unless coNP \subseteq NP/poly. Our kernelization
algorithm is based on a non-trivial combination of the following ingredients
The structural theorem of Grohe and Marx [STOC 2012] for graphs excluding a
fixed graph H as a topological subgraph; Our results are based on a generic
reduction rule producing an equivalent instance (in case the input graph is
H-minor-free) of the problem with treewidth O(sqrt{k}). ...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0258</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0258</id><created>2012-09-30</created><authors><author><keyname>Dieker</keyname><forenames>Antonius B.</forenames></author><author><keyname>Shin</keyname><forenames>Jinwoo</forenames></author></authors><title>From Local to Global Stability in Stochastic Processing Networks through
  Quadratic Lyapunov Functions</title><categories>math.PR cs.NI</categories><comments>39 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a generic, simple, and efficient scheduling policy for
stochastic processing networks, and provide a general framework to establish
its stability. Our policy is randomized and prioritized: with high probability
it prioritizes jobs which have been least routed through the network. We show
that the network is globally stable under this policy if there exists an
appropriate quadratic local Lyapunov function that provides a negative drift
with respect to nominal loads at servers. Applying this generic framework, we
obtain stability results for our policy in many important examples of
stochastic processing networks: open multiclass queueing networks, parallel
server networks, networks of input-queued switches, and a variety of wireless
network models with interference constraints. Our main novelty is the
construction of an appropriate global Lyapunov function from quadratic local
Lyapunov functions, which we believe to be of broader interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0260</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0260</id><created>2012-09-30</created><authors><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Ramanujan</keyname><forenames>M. S.</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author><author><keyname>Such&#xfd;</keyname><forenames>Ond&#x159;ej</forenames></author></authors><title>Parameterized Complexity of Directed Steiner Tree on Sparse Graphs</title><categories>cs.DS cs.DM</categories><comments>28</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the parameterized complexity of the directed variant of the
classical {\sc Steiner Tree} problem on various classes of directed sparse
graphs. While the parameterized complexity of {\sc Steiner Tree} parameterized
by the number of terminals is well understood, not much is known about the
parameterization by the number of non-terminals in the solution tree. All that
is known for this parameterization is that both the directed and the undirected
versions are W[2]-hard on general graphs, and hence unlikely to be fixed
parameter tractable FPT. The undirected {\sc Steiner Tree} problem becomes FPT
when restricted to sparse classes of graphs such as planar graphs, but the
techniques used to show this result break down on directed planar graphs.
  In this article we precisely chart the tractability border for {\sc Directed
Steiner Tree} (DST) on sparse graphs parameterized by the number of
non-terminals in the solution tree. Specifically, we show that the problem is
fixed parameter tractable on graphs excluding a topological minor, but becomes
W[2]-hard on graphs of degeneracy 2. On the other hand we show that if the
subgraph induced by the terminals is required to be acyclic then the problem
becomes FPT on graphs of bounded degeneracy.
  We further show that our algorithm achieves the best possible running time
dependence on the solution size and degeneracy of the input graph, under
standard complexity theoretic assumptions. Using the ideas developed for DST,
we also obtain improved algorithms for {\sc Dominating Set} on sparse
undirected graphs. These algorithms are asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0268</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0268</id><created>2012-09-30</created><authors><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author><author><keyname>Mercer</keyname><forenames>Douglas</forenames></author><author><keyname>Fan</keyname><forenames>James</forenames></author><author><keyname>Squicciarini</keyname><forenames>Anna</forenames></author></authors><title>Two Species Evolutionary Game Model of User and Moderator Dynamics</title><categories>cs.GT cs.SI</categories><comments>8 pages, 4 figures, submitted to 2012 ASE Conference on Social
  Informatics</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We construct a two species evolutionary game model of an online society
consisting of ordinary users and behavior enforcers (moderators). Among
themselves, moderators play a coordination game choosing between being
&quot;positive&quot; or &quot;negative&quot; (or harsh) while ordinary users play prisoner's
dilemma. When interacting, moderators motivate good behavior (cooperation)
among the users through punitive actions while the moderators themselves are
encouraged or discouraged in their strategic choice by these interactions. We
show the following results: (i) We show that the $\omega$-limit set of the
proposed system is sensitive both to the degree of punishment and the
proportion of moderators in closed form. (ii) We demonstrate that the basin of
attraction for the Pareto optimal strategy $(\text{Cooperate},\text{Positive})$
can be computed exactly. (iii) We demonstrate that for certain initial
conditions the system is self-regulating. These results partially explain the
stability of many online users communities such as Reddit. We illustrate our
results with examples from this online system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0271</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0271</id><created>2012-09-30</created><authors><author><keyname>Timo</keyname><forenames>Roy</forenames></author><author><keyname>Lechner</keyname><forenames>Gottfried</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>Multi-Way Relay Networks: Orthogonal Uplink, Source-Channel Separation
  and Code Design</title><categories>cs.IT math.IT</categories><comments>Authors' final version (accepted and to appear in IEEE Transactions
  on Communications)</comments><journal-ref>IEEE Transactions on Communications, Vol. 61, No. 2, pp. 753-768,
  Feb. 2013</journal-ref><doi>10.1109/TCOMM.2012.121112.110730</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-way relay network with an orthogonal uplink and
correlated sources, and we characterise reliable communication (in the usual
Shannon sense) with a single-letter expression. The characterisation is
obtained using a joint source-channel random-coding argument, which is based on
a combination of Wyner et al.'s &quot;Cascaded Slepian-Wolf Source Coding&quot; and
Tuncel's &quot;Slepian-Wolf Coding over Broadcast Channels&quot;. We prove a separation
theorem for the special case of two nodes; that is, we show that a modular code
architecture with separate source and channel coding functions is
(asymptotically) optimal. Finally, we propose a practical coding scheme based
on low-density parity-check codes, and we analyse its performance using
multi-edge density evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0275</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0275</id><created>2012-09-30</created><authors><author><keyname>Sivan</keyname><forenames>Balasubramanian</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>Lower Bounds on Revenue of Approximately Optimal Auctions</title><categories>cs.GT</categories><comments>The 8th Workshop on Internet and Network Economics (WINE)</comments><journal-ref>Internet and Network Economics, Lecture Notes in Computer Science,
  2012, Volume 7695, pp 526-531</journal-ref><doi>10.1007/978-3-642-35311-6_42</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain revenue guarantees for the simple pricing mechanism of a single
posted price, in terms of a natural parameter of the distribution of buyers'
valuations. Our revenue guarantee applies to the single item n buyers setting,
with values drawn from an arbitrary joint distribution. Specifically, we show
that a single price drawn from the distribution of the maximum valuation Vmax =
max {V_1, V_2, ...,V_n} achieves a revenue of at least a 1/e fraction of the
geometric expecation of Vmax. This generic bound is a measure of how revenue
improves/degrades as a function of the concentration/spread of Vmax.
  We further show that in absence of buyers' valuation distributions,
recruiting an additional set of identical bidders will yield a similar
guarantee on revenue. Finally, our bound also gives a measure of the extent to
which one can simultaneously approximate welfare and revenue in terms of the
concentration/spread of Vmax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0293</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0293</id><created>2012-10-01</created><authors><author><keyname>Ntranos</keyname><forenames>Vasilis</forenames></author><author><keyname>Cadambe</keyname><forenames>Viveck R.</forenames></author><author><keyname>Nazer</keyname><forenames>Bobak</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Feedback Interference Alignment: Exact Alignment for Three Users in Two
  Time Slots</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the three-user interference channel where each transmitter has local
feedback of the signal from its targeted receiver. We show that in the
important case where the channel coefficients are static, exact alignment can
be achieved over two time slots using linear schemes. This is in contrast with
the interference channel where no feedback is utilized, where it seems that
either an infinite number of channel extensions or infinite precision is
required for exact alignment. We also demonstrate, via simulations, that our
scheme significantly outperforms time-sharing even at finite SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0295</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0295</id><created>2012-10-01</created><authors><author><keyname>Haukkanen</keyname><forenames>Pentti</forenames></author></authors><title>Discrete Ramanujan-Fourier Transform of Even Functions (mod $r$)</title><categories>math.NT cs.IT math.IT</categories><msc-class>11A25, 11L03, 94A11</msc-class><journal-ref>Indian J. Math. Math. Sci. 3 (2007), no. 1, 75--80</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An arithmetical function $f$ is said to be even (mod r) if f(n)=f((n,r)) for
all n\in\Z^+, where (n, r) is the greatest common divisor of n and r. We adopt
a linear algebraic approach to show that the Discrete Fourier Transform of an
even function (mod r) can be written in terms of Ramanujan's sum and may thus
be referred to as the Discrete Ramanujan-Fourier Transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0297</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0297</id><created>2012-10-01</created><updated>2012-10-05</updated><authors><author><keyname>Sahidullah</keyname><forenames>Md.</forenames></author><author><keyname>Saha</keyname><forenames>Goutam</forenames></author></authors><title>Comparison of Speech Activity Detection Techniques for Speaker
  Recognition</title><categories>cs.MM cs.SD</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech activity detection (SAD) is an essential component for a variety of
speech processing applications. It has been observed that performances of
various speech based tasks are very much dependent on the efficiency of the
SAD. In this paper, we have systematically reviewed some popular SAD techniques
and their applications in speaker recognition. Speaker verification system
using different SAD technique are experimentally evaluated on NIST speech
corpora using Gaussian mixture model- universal background model (GMM-UBM)
based classifier for clean and noisy conditions. It has been found that two
Gaussian modeling based SAD is comparatively better than other SAD techniques
for different types of noises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0298</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0298</id><created>2012-10-01</created><authors><author><keyname>Jacobs</keyname><forenames>Bart</forenames></author><author><keyname>Selinger</keyname><forenames>Peter</forenames></author><author><keyname>Spitters</keyname><forenames>Bas</forenames></author></authors><title>Proceedings 8th International Workshop on Quantum Physics and Logic</title><categories>quant-ph cs.LO</categories><comments>EPTCS 95, 2012</comments><proxy>EPTCS</proxy><doi>10.4204/EPTCS.95</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 8th International Workshop on
Quantum Physics and Logic (QPL 2011), which was held October 27-29, 2011 at
Radboud University Nijmegen. The goal of this workshop series is to bring
together researchers working on mathematical foundations of quantum physics,
quantum computing and spatio-temporal causal structures, and in particular
those that use logical tools, ordered algebraic and category-theoretic
structures, formal languages, semantic methods and other computer science
methods for the study of physical behavior in general. Over the past few years,
there has been growing activity in these foundational approaches, together with
a renewed interest in the foundations of quantum theory, which complement the
more mainstream research in quantum computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0306</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0306</id><created>2012-10-01</created><authors><author><keyname>Bokowski</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Pilaud</keyname><forenames>Vincent</forenames></author></authors><title>Enumerating topological $(n_k)$-configurations</title><categories>cs.CG math.CO</categories><comments>18 pages, 11 figures</comments><msc-class>52C30, 68-04</msc-class><journal-ref>Comput. Geom., 47(2):175-186, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $(n_k)$-configuration is a set of $n$ points and $n$ lines in the
projective plane such that their point-line incidence graph is $k$-regular. The
configuration is geometric, topological, or combinatorial depending on whether
lines are considered to be straight lines, pseudolines, or just combinatorial
lines. We provide an algorithm for generating, for given $n$ and $k$, all
topological $(n_k)$-configurations up to combinatorial isomorphism, without
enumerating first all combinatorial $(n_k)$-configurations. We apply this
algorithm to confirm efficiently a former result on topological
$(18_4)$-configurations, from which we obtain a new geometric
$(18_4)$-configuration. Preliminary results on $(19_4)$-configurations are also
briefly reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0310</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0310</id><created>2012-10-01</created><updated>2012-10-08</updated><authors><author><keyname>Kafieh</keyname><forenames>Raheleh</forenames></author><author><keyname>Rabbani</keyname><forenames>Hossein</forenames></author><author><keyname>Abramoff</keyname><forenames>Michael D.</forenames></author><author><keyname>Sonka</keyname><forenames>Milan</forenames></author></authors><title>Intra-Retinal Layer Segmentation of 3D Optical Coherence Tomography
  Using Coarse Grained Diffusion Map</title><categories>cs.CV</categories><comments>30 pages,32 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical coherence tomography (OCT) is a powerful and noninvasive method for
retinal imaging. In this paper, we introduce a fast segmentation method based
on a new variant of spectral graph theory named diffusion maps. The research is
performed on spectral domain (SD) OCT images depicting macular and optic nerve
head appearance. The presented approach does not require edge-based image
information and relies on regional image texture. Consequently, the proposed
method demonstrates robustness in situations of low image contrast or poor
layer-to-layer image gradients. Diffusion mapping is applied to 2D and 3D OCT
datasets composed of two steps, one for partitioning the data into important
and less important sections, and another one for localization of internal
layers.In the first step, the pixels/voxels are grouped in rectangular/cubic
sets to form a graph node.The weights of a graph are calculated based on
geometric distances between pixels/voxels and differences of their mean
intensity.The first diffusion map clusters the data into three parts, the
second of which is the area of interest. The other two sections are eliminated
from the remaining calculations. In the second step, the remaining area is
subjected to another diffusion map assessment and the internal layers are
localized based on their textural similarities.The proposed method was tested
on 23 datasets from two patient groups (glaucoma and normals). The mean
unsigned border positioning errors(mean - SD) was 8.52 - 3.13 and 7.56 - 2.95
micrometer for the 2D and 3D methods, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0330</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0330</id><created>2012-10-01</created><updated>2013-05-11</updated><authors><author><keyname>Csermely</keyname><forenames>Peter</forenames></author><author><keyname>Korcsmaros</keyname><forenames>Tamas</forenames></author><author><keyname>Kiss</keyname><forenames>Huba J. M.</forenames></author><author><keyname>London</keyname><forenames>Gabor</forenames></author><author><keyname>Nussinov</keyname><forenames>Ruth</forenames></author></authors><title>Structure and dynamics of molecular networks: A novel paradigm of drug
  discovery. A comprehensive review</title><categories>q-bio.MN cond-mat.dis-nn cs.SI nlin.AO physics.bio-ph</categories><comments>76 pages, 23 Figures, 12 Tables and 1270 references</comments><journal-ref>Pharmacology and Therapeutics 138:333-408 (2013)</journal-ref><doi>10.1016/j.pharmthera.2013.01.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite considerable progress in genome- and proteome-based high-throughput
screening methods and in rational drug design, the increase in approved drugs
in the past decade did not match the increase of drug development costs.
Network description and analysis not only give a systems-level understanding of
drug action and disease complexity, but can also help to improve the efficiency
of drug design. We give a comprehensive assessment of the analytical tools of
network topology and dynamics. The state-of-the-art use of chemical similarity,
protein structure, protein-protein interaction, signaling, genetic interaction
and metabolic networks in the discovery of drug targets is summarized. We
propose that network targeting follows two basic strategies. The central hit
strategy selectively targets central nodes/edges of the flexible networks of
infectious agents or cancer cells to kill them. The network influence strategy
works against other diseases, where an efficient reconfiguration of rigid
networks needs to be achieved by targeting the neighbors of central nodes or
edges. It is shown how network techniques can help in the identification of
single-target, edgetic, multi-target and allo-network drug target candidates.
We review the recent boom in network methods helping hit identification, lead
selection optimizing drug efficacy, as well as minimizing side-effects and drug
toxicity. Successful network-based drug development strategies are shown
through the examples of infections, cancer, metabolic diseases,
neurodegenerative diseases and aging. Summarizing more than 1200 references we
suggest an optimized protocol of network-aided drug development, and provide a
list of systems-level hallmarks of drug quality. Finally, we highlight
network-related drug development trends helping to achieve these hallmarks by a
cohesive, global approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0340</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0340</id><created>2012-10-01</created><authors><author><keyname>Lingas</keyname><forenames>Andrzej</forenames></author><author><keyname>Persson</keyname><forenames>Mia</forenames></author></authors><title>A fast parallel algorithm for minimum-cost small integral flows</title><categories>cs.DC cs.CC cs.DS</categories><comments>This is an improved version of a preliminary version which appeared
  in proc. EUROPAR 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to the minimum-cost integral flow problem for small
values of the flow. It reduces the problem to the tests of simple multi-variate
polynomials over a finite field of characteristic two for non-identity with
zero. In effect, we show that a minimum-cost flow of value k in a network with
n vertices, a sink and a source, integral edge capacities and positive integral
edge costs polynomially bounded in n can be found by a randomized PRAM, with
errors of exponentially small probability in n, running in O(k\log (kn)+\log^2
(kn)) time and using 2^{k}(kn)^{O(1)} processors. Thus, in particular, for the
minimum-cost flow of value O(\log n), we obtain an RNC^2 algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0347</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0347</id><created>2012-10-01</created><authors><author><keyname>Sasirekha</keyname><forenames>D.</forenames></author><author><keyname>Chandra</keyname><forenames>E.</forenames></author></authors><title>Enhanced Techniques for PDF Image Segmentation and Text Extraction</title><categories>cs.CV</categories><comments>5 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Extracting text objects from the PDF images is a challenging problem. The
text data present in the PDF images contain certain useful information for
automatic annotation, indexing etc. However variations of the text due to
differences in text style, font, size, orientation, alignment as well as
complex structure make the problem of automatic text extraction extremely
difficult and challenging job. This paper presents two techniques under
block-based classification. After a brief introduction of the classification
methods, two methods were enhanced and results were evaluated. The performance
metrics for segmentation and time consumption are tested for both the models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0368</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0368</id><created>2012-10-01</created><authors><author><keyname>Trivellato</keyname><forenames>Daniel</forenames></author><author><keyname>Zannone</keyname><forenames>Nicola</forenames></author><author><keyname>Etalle</keyname><forenames>Sandro</forenames></author></authors><title>GEM: a Distributed Goal Evaluation Algorithm for Trust Management</title><categories>cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><doi>10.1017/S1471068412000397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trust management is an approach to access control in distributed systems
where access decisions are based on policy statements issued by multiple
principals and stored in a distributed manner. In trust management, the policy
statements of a principal can refer to other principals' statements; thus, the
process of evaluating an access request (i.e., a goal) consists of finding a
&quot;chain&quot; of policy statements that allows the access to the requested resource.
Most existing goal evaluation algorithms for trust management either rely on a
centralized evaluation strategy, which consists of collecting all the relevant
policy statements in a single location (and therefore they do not guarantee the
confidentiality of intensional policies), or do not detect the termination of
the computation (i.e., when all the answers of a goal are computed). In this
paper we present GEM, a distributed goal evaluation algorithm for trust
management systems that relies on function-free logic programming for the
specification of policy statements. GEM detects termination in a completely
distributed way without disclosing intensional policies, thereby preserving
their confidentiality. We demonstrate that the algorithm terminates and is
sound and complete with respect to the standard semantics for logic programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0374</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0374</id><created>2012-10-01</created><authors><author><keyname>Runarsson</keyname><forenames>Thomas Philip</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Sebag</keyname><forenames>Mich&#xe8;le</forenames><affiliation>LRI</affiliation></author></authors><title>Pilot, Rollout and Monte Carlo Tree Search Methods for Job Shop
  Scheduling</title><categories>cs.DS</categories><comments>Learning and Intelligent OptimizatioN (LION'6) 7219 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Greedy heuristics may be attuned by looking ahead for each possible choice,
in an approach called the rollout or Pilot method. These methods may be seen as
meta-heuristics that can enhance (any) heuristic solution, by repetitively
modifying a master solution: similarly to what is done in game tree search,
better choices are identified using lookahead, based on solutions obtained by
repeatedly using a greedy heuristic. This paper first illustrates how the Pilot
method improves upon some simple well known dispatch heuristics for the
job-shop scheduling problem. The Pilot method is then shown to be a special
case of the more recent Monte Carlo Tree Search (MCTS) methods: Unlike the
Pilot method, MCTS methods use random completion of partial solutions to
identify promising branches of the tree. The Pilot method and a simple version
of MCTS, using the $\varepsilon$-greedy exploration paradigms, are then
compared within the same framework, consisting of 300 scheduling problems of
varying sizes with fixed-budget of rollouts. Results demonstrate that MCTS
reaches better or same results as the Pilot methods in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0386</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0386</id><created>2012-10-01</created><updated>2012-10-02</updated><authors><author><keyname>Hu</keyname><forenames>Junlin</forenames></author><author><keyname>Guo</keyname><forenames>Ping</forenames></author></authors><title>Combined Descriptors in Spatial Pyramid Domain for Image Classification</title><categories>cs.CV</categories><comments>9 pages, 5 figures</comments><acm-class>I.4.9; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently spatial pyramid matching (SPM) with scale invariant feature
transform (SIFT) descriptor has been successfully used in image classification.
Unfortunately, the codebook generation and feature quantization procedures
using SIFT feature have the high complexity both in time and space. To address
this problem, in this paper, we propose an approach which combines local binary
patterns (LBP) and three-patch local binary patterns (TPLBP) in spatial pyramid
domain. The proposed method does not need to learn the codebook and feature
quantization processing, hence it becomes very efficient. Experiments on two
popular benchmark datasets demonstrate that the proposed method always
significantly outperforms the very popular SPM based SIFT descriptor method
both in time and classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0408</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0408</id><created>2012-10-01</created><authors><author><keyname>Sharma</keyname><forenames>Arpit</forenames></author></authors><title>A Two Step Perspective for Kripke Structure Reduction</title><categories>cs.FL cs.LO</categories><comments>Accepted for Student Research Forum, 39th International Conference on
  Current Trends in Theory and Practice of Computer Science (SOFSEM 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel theoretical framework for the state space
reduction of Kripke structures. We define two equivalence relations, Kripke
minimization equivalence (KME) and weak Kripke minimization equivalence (WKME).
We define the quotient system under these relations and show that these
relations are strictly coarser than strong (bi)simulation and
divergence-sensitive stutter (bi)simulation, respectively. We prove that the
quotient system obtained under KME and WKME preserves linear-time and
stutter-insensitive linear-time properties. Finally, we show that KME is
compositional w.r.t. synchronous parallel composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0420</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0420</id><created>2012-10-01</created><updated>2012-10-09</updated><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames><affiliation>LIX, Ecole Polytechnique</affiliation></author><author><keyname>Jonsson</keyname><forenames>Peter</forenames><affiliation>Department of Computer and System Science, Linkoepings Universitet</affiliation></author><author><keyname>von Oertzen</keyname><forenames>Timo</forenames><affiliation>Max-Planck-Institute for Human Development</affiliation></author></authors><title>Essential Convexity and Complexity of Semi-Algebraic Constraints</title><categories>cs.CC cs.DM math.LO</categories><comments>25 pages, 3 Figures. An extended abstract of a preliminary version of
  this paper appeared in the proceedings of ICALP 2009 under the title
  `Semilinear Program Feasibility'</comments><proxy>LMCS</proxy><acm-class>F.2.2, F.4.1, G.1.6</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (October
  10, 2012) lmcs:1218</journal-ref><doi>10.2168/LMCS-8(4:5)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let \Gamma be a structure with a finite relational signature and a
first-order definition in (R;*,+) with parameters from R, that is, a relational
structure over the real numbers where all relations are semi-algebraic sets. In
this article, we study the computational complexity of constraint satisfaction
problem (CSP) for \Gamma: the problem to decide whether a given primitive
positive sentence is true in \Gamma. We focus on those structures \Gamma that
contain the relations \leq, {(x,y,z) | x+y=z} and {1}. Hence, all CSPs studied
in this article are at least as expressive as the feasibility problem for
linear programs. The central concept in our investigation is essential
convexity: a relation S is essentially convex if for all a,b\inS, there are
only finitely many points on the line segment between a and b that are not in
S. If \Gamma contains a relation S that is not essentially convex and this is
witnessed by rational points a,b, then we show that the CSP for \Gamma is
NP-hard. Furthermore, we characterize essentially convex relations in logical
terms. This different view may open up new ways for identifying tractable
classes of semi-algebraic CSPs. For instance, we show that if \Gamma is a
first-order expansion of (R;*,+), then the CSP for \Gamma can be solved in
polynomial time if and only if all relations in \Gamma are essentially convex
(unless P=NP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0437</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0437</id><created>2012-10-01</created><authors><author><keyname>Villadsen</keyname><forenames>J&#xf8;rgen</forenames></author><author><keyname>Jensen</keyname><forenames>Andreas Schmidt</forenames></author><author><keyname>Ettienne</keyname><forenames>Mikko Berggren</forenames></author><author><keyname>Vester</keyname><forenames>Steen</forenames></author><author><keyname>Andersen</keyname><forenames>Kenneth Balsiger</forenames></author><author><keyname>Fr&#xf8;sig</keyname><forenames>Andreas</forenames></author></authors><title>Multi-Agent Programming Contest 2012 - The Python-DTU Team</title><categories>cs.MA</categories><comments>4 pages. arXiv admin note: text overlap with arXiv:1110.0105</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a brief description of the Python-DTU system, including the
overall design, the tools and the algorithms that we plan to use in the agent
contest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0442</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0442</id><created>2012-10-01</created><updated>2013-04-28</updated><authors><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Raan</keyname><forenames>Anthony F. J.</forenames></author><author><keyname>Klautz</keyname><forenames>Robert J. M.</forenames></author><author><keyname>Peul</keyname><forenames>Wilco C.</forenames></author></authors><title>Citation analysis may severely underestimate the impact of clinical
  research as compared to basic research</title><categories>cs.DL</categories><doi>10.1371/journal.pone.0062395</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Citation analysis has become an important tool for research
performance assessment in the medical sciences. However, different areas of
medical research may have considerably different citation practices, even
within the same medical field. Because of this, it is unclear to what extent
citation-based bibliometric indicators allow for valid comparisons between
research units active in different areas of medical research.
  Methodology: A visualization methodology is introduced that reveals
differences in citation practices between medical research areas. The
methodology extracts terms from the titles and abstracts of a large collection
of publications and uses these terms to visualize the structure of a medical
field and to indicate how research areas within this field differ from each
other in their average citation impact.
  Results: Visualizations are provided for 32 medical fields, defined based on
journal subject categories in the Web of Science database. The analysis focuses
on three fields. In each of these fields, there turn out to be large
differences in citation practices between research areas. Low-impact research
areas tend to focus on clinical intervention research, while high-impact
research areas are often more oriented on basic and diagnostic research.
  Conclusions: Popular bibliometric indicators, such as the h-index and the
impact factor, do not correct for differences in citation practices between
medical fields. These indicators therefore cannot be used to make accurate
between-field comparisons. More sophisticated bibliometric indicators do
correct for field differences but still fail to take into account within-field
heterogeneity in citation practices. As a consequence, the citation impact of
clinical intervention research may be substantially underestimated in
comparison with basic and diagnostic research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0451</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0451</id><created>2012-10-01</created><updated>2012-10-17</updated><authors><author><keyname>John</keyname><forenames>Audrey Lee-St.</forenames></author><author><keyname>Sidman</keyname><forenames>Jessica</forenames></author></authors><title>Combinatorics and the Rigidity of CAD Systems</title><categories>cs.DM math.CO</categories><comments>17 pages, 7 figures, version to appear in Symposium on Solid and
  Physical Modeling '12 and associated special issue of Computer Aided Design</comments><msc-class>68R10, 05C50</msc-class><acm-class>G.2.1; J.6; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the rigidity of body-and-cad frameworks which capture the majority
of the geometric constraints used in 3D mechanical engineering CAD software. We
present a combinatorial characterization of the generic minimal rigidity of a
subset of body-and-cad frameworks in which we treat 20 of the 21 body-and-cad
constraints, omitting only point-point coincidences. While the handful of
classical combinatorial characterizations of rigidity focus on distance
constraints between points, this is the first result simultaneously addressing
coincidence, angular, and distance constraints. Our result is stated in terms
of the partitioning of a graph into edge-disjoint spanning trees. This
combinatorial approach provides the theoretical basis for the development of
deterministic algorithms (that will not depend on numerical methods) for
analyzing the rigidity of body-and-cad frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0460</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0460</id><created>2012-10-01</created><authors><author><keyname>Kurant</keyname><forenames>Maciej</forenames></author><author><keyname>Butts</keyname><forenames>Carter T.</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author></authors><title>Graph Size Estimation</title><categories>cs.SI cs.CY physics.soc-ph stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many online networks are not fully known and are often studied via sampling.
Random Walk (RW) based techniques are the current state-of-the-art for
estimating nodal attributes and local graph properties, but estimating global
properties remains a challenge. In this paper, we are interested in a
fundamental property of this type - the graph size N, i.e., the number of its
nodes. Existing methods for estimating N are (i) inefficient and (ii) cannot be
easily used with RW sampling due to dependence between successive samples. In
this paper, we address both problems. First, we propose IE (Induced Edges), an
efficient technique for estimating N from an independence sample of graph's
nodes. IE exploits the edges induced on the sampled nodes. Second, we introduce
SafetyMargin, a method that corrects estimators for dependence in RW samples.
Finally, we combine these two stand-alone techniques to obtain a RW-based graph
size estimator. We evaluate our approach in simulations on a wide range of
real-life topologies, and on several samples of Facebook. IE with SafetyMargin
typically requires at least 10 times fewer samples than the state-of-the-art
techniques (over 100 times in the case of Facebook) for the same estimation
error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0461</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0461</id><created>2012-10-01</created><updated>2012-11-19</updated><authors><author><keyname>Campagna</keyname><forenames>Andrea</forenames></author><author><keyname>Kutzkov</keyname><forenames>Konstantin</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author></authors><title>On Parallelizing Matrix Multiplication by the Column-Row Method</title><categories>cs.DS</categories><comments>To appear in ALENEX 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sparse matrix multiplication by the column row
method in a distributed setting where the matrix product is not necessarily
sparse. We present a surprisingly simple method for &quot;consistent&quot; parallel
processing of sparse outer products (column-row vector products) over several
processors, in a communication-avoiding setting where each processor has a copy
of the input. The method is consistent in the sense that a given output entry
is always assigned to the same processor independently of the specific
structure of the outer product. We show guarantees on the work done by each
processor, and achieve linear speedup down to the point where the cost is
dominated by reading the input. Our method gives a way of distributing (or
parallelizing) matrix product computations in settings where the main
bottlenecks are storing the result matrix, and inter-processor communication.
Motivated by observations on real data that often the absolute values of the
entries in the product adhere to a power law, we combine our approach with
frequent items mining algorithms and show how to obtain a tight approximation
of the weight of the heaviest entries in the product matrix.
  As a case study we present the application of our approach to frequent pair
mining in transactional data streams, a problem that can be phrased in terms of
sparse ${0,1}$-integer matrix multiplication by the column-row method.
Experimental evaluation of the proposed method on real-life data supports the
theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0473</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0473</id><created>2012-10-01</created><authors><author><keyname>Cavallanti</keyname><forenames>Giovanni</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author></authors><title>Memory Constraint Online Multitask Classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate online kernel algorithms which simultaneously process multiple
classification tasks while a fixed constraint is imposed on the size of their
active sets. We focus in particular on the design of algorithms that can
efficiently deal with problems where the number of tasks is extremely high and
the task data are large scale. Two new projection-based algorithms are
introduced to efficiently tackle those issues while presenting different trade
offs on how the available memory is managed with respect to the prior
information about the learning tasks. Theoretically sound budget algorithms are
devised by coupling the Randomized Budget Perceptron and the Forgetron
algorithms with the multitask kernel. We show how the two seemingly contrasting
properties of learning from multiple tasks and keeping a constant memory
footprint can be balanced, and how the sharing of the available space among
different tasks is automatically taken care of. We propose and discuss new
insights on the multitask kernel. Experiments show that online kernel multitask
algorithms running on a budget can efficiently tackle real world learning
problems involving multiple tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0477</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0477</id><created>2012-10-01</created><authors><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Think Locally, Act Globally: Perfectly Balanced Graph Partitioning</title><categories>cs.DS cs.DC cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel local improvement scheme for the perfectly balanced graph
partitioning problem. This scheme encodes local searches that are not
restricted to a balance constraint into a model allowing us to find
combinations of these searches maintaining balance by applying a negative cycle
detection algorithm. We combine this technique with an algorithm to balance
unbalanced solutions and integrate it into a parallel multi-level evolutionary
algorithm, KaFFPaE, to tackle the problem. Overall, we obtain a system that is
fast on the one hand and on the other hand is able to improve or reproduce most
of the best known perfectly balanced partitioning results ever reported in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0481</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0481</id><created>2012-10-01</created><updated>2013-12-20</updated><authors><author><keyname>Veldhuizen</keyname><forenames>Todd L.</forenames></author></authors><title>Leapfrog Triejoin: a worst-case optimal join algorithm</title><categories>cs.DB cs.DS</categories><report-no>LB1201</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen exciting developments in join algorithms. In 2008,
Atserias, Grohe and Marx (henceforth AGM) proved a tight bound on the maximum
result size of a full conjunctive query, given constraints on the input
relation sizes. In 2012, Ngo, Porat, R{\'e} and Rudra (henceforth NPRR) devised
a join algorithm with worst-case running time proportional to the AGM bound.
Our commercial Datalog system LogicBlox employs a novel join algorithm,
\emph{leapfrog triejoin}, which compared conspicuously well to the NPRR
algorithm in preliminary benchmarks. This spurred us to analyze the complexity
of leapfrog triejoin. In this paper we establish that leapfrog triejoin is also
worst-case optimal, up to a log factor, in the sense of NPRR. We improve on the
results of NPRR by proving that leapfrog triejoin achieves worst-case
optimality for finer-grained classes of database instances, such as those
defined by constraints on projection cardinalities. We show that NPRR is
\emph{not} worst-case optimal for such classes, giving a counterexample where
leapfrog triejoin runs in $O(n \log n)$ time, compared to $\Theta(n^{1.375})$
time for NPRR. On a practical note, leapfrog triejoin can be implemented using
conventional data structures such as B-trees, and extends naturally to
$\exists_1$ queries. We believe our algorithm offers a useful addition to the
existing toolbox of join algorithms, being easy to absorb, simple to implement,
and having a concise optimality proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0490</identifier>
 <datestamp>2012-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0490</id><created>2012-10-01</created><updated>2012-10-19</updated><authors><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Physical Layer Network Coding for the Multiple Access Relay Channel</title><categories>cs.IT math.IT</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the two user wireless Multiple Access Relay Channel (MARC), in
which nodes $A$ and $B$ want to transmit messages to a destination node $D$
with the help of a relay node $R$. For the MARC, Wang and Giannakis proposed a
Complex Field Network Coding (CFNC) scheme. As an alternative, we propose a
scheme based on Physical layer Network Coding (PNC), which has so far been
studied widely only in the context of two-way relaying. For the proposed PNC
scheme, transmission takes place in two phases: (i) Phase 1 during which $A$
and $B$ simultaneously transmit and, $R$ and $D$ receive, (ii) Phase 2 during
which $A$, $B$ and $R$ simultaneously transmit to $D$. At the end of Phase 1,
$R$ decodes the messages $x_A$ of $A$ and $x_B$ of $B,$ and during Phase 2
transmits $f(x_A,x_B),$ where $f$ is many-to-one. Communication protocols in
which the relay node decodes are prone to loss of diversity order, due to error
propagation from the relay node. To counter this, we propose a novel decoder
which takes into account the possibility of an error event at $R$, without
having any knowledge about the links from $A$ to $R$ and $B$ to $R$. It is
shown that if certain parameters are chosen properly and if the map $f$
satisfies a condition called exclusive law, the proposed decoder offers the
maximum diversity order of two. Also, it is shown that for a proper choice of
the parameters, the proposed decoder admits fast decoding, with the same
decoding complexity order as that of the CFNC scheme. Simulation results
indicate that the proposed PNC scheme performs better than the CFNC scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0508</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0508</id><created>2012-10-01</created><updated>2012-12-29</updated><authors><author><keyname>Takhanov</keyname><forenames>Rustem</forenames></author><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author></authors><title>Inference algorithms for pattern-based CRFs on sequence data</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Conditional Random Fields (CRFs) with pattern-based potentials
defined on a chain. In this model the energy of a string (labeling) $x_1...x_n$
is the sum of terms over intervals $[i,j]$ where each term is non-zero only if
the substring $x_i...x_j$ equals a prespecified pattern $\alpha$. Such CRFs can
be naturally applied to many sequence tagging problems.
  We present efficient algorithms for the three standard inference tasks in a
CRF, namely computing (i) the partition function, (ii) marginals, and (iii)
computing the MAP. Their complexities are respectively $O(n L)$, $O(n L
\ell_{max})$ and $O(n L \min\{|D|,\log (\ell_{max}+1)\})$ where $L$ is the
combined length of input patterns, $\ell_{max}$ is the maximum length of a
pattern, and $D$ is the input alphabet. This improves on the previous
algorithms of (Ye et al., 2009) whose complexities are respectively $O(n L
|D|)$, $O(n |\Gamma| L^2 \ell_{max}^2)$ and $O(n L |D|)$, where $|\Gamma|$ is
the number of input patterns.
  In addition, we give an efficient algorithm for sampling. Finally, we
consider the case of non-positive weights. (Komodakis &amp; Paragios, 2009) gave an
$O(n L)$ algorithm for computing the MAP. We present a modification that has
the same worst-case complexity but can beat it in the best case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0510</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0510</id><created>2012-10-01</created><authors><author><keyname>Lemlouma</keyname><forenames>Tayeb</forenames></author><author><keyname>Lefebvre</keyname><forenames>Yoann</forenames></author><author><keyname>Cespedes</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>PCNM: A New Platform for Cellular Networks Measurements and Optimization</title><categories>cs.NI cs.OH cs.PF</categories><comments>IEEE International Conference on Wireless Communications, Networking
  and Mobile Computing (WiCOM 2007)</comments><acm-class>C.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present PCNM, a new mobile platform for cellular networks
measurements. PCNM is based on a set of techniques that tailors theoretical
calculations and simulations to the real cellular network environment. It
includes: (a) modules that measure different parameters of a base station (BS)
such as localization, cells identification, time advance information, reception
level and quality, (b) a new protocol that optimizes the task of network
measurement by monitoring a set of mobile nodes and finally (c) the ability to
extend an existing cellular network by adding new base stations. We evaluate
our genetic algorithm used to reduce the nodes mobility and optimize the
measurement extraction of N base stations using k mobile sensors (k &gt;= 1). We
show how connecting real measurements (using mobile sensors in a collaborative
way) to theoretical and prediction methods is of high benefits for cellular
networks maintenance, extension and performances evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0511</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0511</id><created>2012-10-01</created><authors><author><keyname>Lemlouma</keyname><forenames>Tayeb</forenames></author><author><keyname>Luciana</keyname><forenames>Julien</forenames></author><author><keyname>Oza</keyname><forenames>Bastien</forenames></author><author><keyname>Sierra</keyname><forenames>Leandro</forenames></author><author><keyname>Sala&#xfc;n</keyname><forenames>Mika&#xeb;l</forenames></author></authors><title>An Easy Cellular Gateway for Providing Shared Services and Data</title><categories>cs.NI</categories><comments>6 pages; The Fifth International Conference on Systems and Networks
  Communications, ICSNC 2010</comments><msc-class>C.2, J.7</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new framework that links the two worlds of wired
and cellular users sharing systems. The approach is to propose an easy gateway
that enables the use of cellular networks based services by wireline users and
applications. The idea is to use a mobile terminal or wireless equipment for
sharing cellular services, available thanks to its cellular network, to other
users that use the wireline Internet. The software application acts as a
gateway between the cellular and the wired network; it is responsible for
supporting the services provided by the wireless network and make them
accessible and usable, in a standard and easy way, by anyone on the wireline
network. The gateway software can be integrated easily on any complex
architecture since it can interact with any cellular modem. The paper describes
an implementation prototype where some examples of services, such as the
ability of using messaging services and calls streaming, are experimented. The
proposed platform combines different standards to guarantee the use of our
gateway in heterogeneous environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0516</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0516</id><created>2012-10-01</created><updated>2013-04-09</updated><authors><author><keyname>Abediseid</keyname><forenames>Walid</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>On Lattice Sequential Decoding for The Unconstrained AWGN Channel</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transaction on Communications, April
  4, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the performance limits and the computational complexity of the
lattice sequential decoder are analyzed for the unconstrained additive white
Gaussian noise channel. The performance analysis available in the literature
for such a channel has been studied only under the use of the minimum Euclidean
distance decoder that is commonly referred to as the lattice decoder. Lattice
decoders based on solutions to the NP-hard closest vector problem are very
complex to implement, and the search for low complexity receivers for the
detection of lattice codes is considered a challenging problem. However, the
low computational complexity advantage that sequential decoding promises, makes
it an alternative solution to the lattice decoder. In this work, we will
characterize the performance and complexity tradeoff via the error exponent and
the decoding complexity, respectively, of such a decoder as a function of the
decoding parameter --- the bias term. For the above channel, we derive the
cut-off volume-to-noise ratio that is required to achieve a good error
performance with low decoding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0528</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0528</id><created>2012-09-28</created><authors><author><keyname>Sarhrouni</keyname><forenames>Elkebir</forenames></author><author><keyname>Hammouch</keyname><forenames>Ahmed</forenames></author><author><keyname>Aboutajdine</keyname><forenames>Driss</forenames></author></authors><title>Band Selection and Classification of Hyperspectral Images using Mutual
  Information: An algorithm based on minimizing the error probability using the
  inequality of Fano</title><categories>cs.CV</categories><comments>5 pages, 5 figures, ieee conference ICMCS'12 Tanger, Morocco. arXiv
  admin note: text overlap with arXiv:1210.0052</comments><msc-class>68U10, 68R05</msc-class><acm-class>I.4.7; I.4.8; I.4.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral image is a substitution of more than a hundred images, called
bands, of the same region. They are taken at juxtaposed frequencies. The
reference image of the region is called Ground Truth map (GT). the problematic
is how to find the good bands to classify the pixels of regions; because the
bands can be not only redundant, but a source of confusion, and decreasing so
the accuracy of classification. Some methods use Mutual Information (MI) and
threshold, to select relevant bands. Recently there's an algorithm selection
based on mutual information, using bandwidth rejection and a threshold to
control and eliminate redundancy. The band top ranking the MI is selected, and
if its neighbors have sensibly the same MI with the GT, they will be considered
redundant and so discarded. This is the most inconvenient of this method,
because this avoids the advantage of hyperspectral images: some precious
information can be discarded. In this paper we'll make difference between
useful and useless redundancy. A band contains useful redundancy if it
contributes to decreasing error probability. According to this scheme, we
introduce new algorithm using also mutual information, but it retains only the
bands minimizing the error probability of classification. To control
redundancy, we introduce a complementary threshold. So the good band candidate
must contribute to decrease the last error probability augmented by the
threshold. This process is a wrapper strategy; it gets high performance of
classification accuracy but it is expensive than filter strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0530</identifier>
 <datestamp>2013-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0530</id><created>2012-09-30</created><updated>2013-09-26</updated><authors><author><keyname>Wilson</keyname><forenames>Greg</forenames></author><author><keyname>Aruliah</keyname><forenames>D. A.</forenames></author><author><keyname>Brown</keyname><forenames>C. Titus</forenames></author><author><keyname>Hong</keyname><forenames>Neil P. Chue</forenames></author><author><keyname>Davis</keyname><forenames>Matt</forenames></author><author><keyname>Guy</keyname><forenames>Richard T.</forenames></author><author><keyname>Haddock</keyname><forenames>Steven H. D.</forenames></author><author><keyname>Huff</keyname><forenames>Katy</forenames></author><author><keyname>Mitchell</keyname><forenames>Ian M.</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark</forenames></author><author><keyname>Waugh</keyname><forenames>Ben</forenames></author><author><keyname>White</keyname><forenames>Ethan P.</forenames></author><author><keyname>Wilson</keyname><forenames>Paul</forenames></author></authors><title>Best Practices for Scientific Computing</title><categories>cs.MS cs.SE</categories><comments>18 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Scientists spend an increasing amount of time building and using software.
However, most scientists are never taught how to do this efficiently. As a
result, many are unaware of tools and practices that would allow them to write
more reliable and maintainable code with less effort. We describe a set of best
practices for scientific software development that have solid foundations in
research and experience, and that improve scientists' productivity and the
reliability of their software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0558</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0558</id><created>2012-10-01</created><updated>2014-04-07</updated><authors><author><keyname>Zhu</keyname><forenames>Junjie</forenames></author><author><keyname>Govindasamy</keyname><forenames>Siddhartan</forenames></author></authors><title>Performance of Multi-Antenna Linear MMSE Receivers in Non-homogeneous
  Poisson and Poisson Cluster Networks</title><categories>cs.IT math.IT</categories><comments>A new version of this manuscript with a revised title and more
  general results is available at arXiv:1404.1107</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A technique is presented to evaluate the performance of a wireless link with
a multi-antenna linear Minimum-Mean-Square Error (MMSE) receiver in the
presence of interferers distributed according to non-homogeneous Poisson
processes or Poisson cluster processes on the plane. The Cumulative
Distribution Function (CDF) of the Signal-to-Interference-plus-Noise Ratio
(SINR) of a representative link is derived for both types of networks assuming
independent Rayleigh fading between antennas. Several representative spatial
node distributions are considered, for which the derived CDFs are verified by
numerical simulations. In addition, for non-homogeneous Poisson networks, it is
shown that the Signal-to-Interference Ratio (SIR) converges to a deterministic
non-zero value if the number of antennas at the representative receiver
increases linearly with the nominal interferer density. This indicates that to
the extent that the system assumptions hold, it is possible to scale such
networks by increasing the number of receiver antennas linearly with user
density. The results presented here are useful in characterizing the
performance of multiantenna wireless networks with non-homogenous spatial node
distributions and networks with clusters of users which often arise in
practice, but for which few results are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0563</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0563</id><created>2012-10-01</created><authors><author><keyname>Hu</keyname><forenames>Tao</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>Sparse LMS via Online Linearized Bregman Iteration</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>11 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a version of least-mean-square (LMS) algorithm for sparse system
identification. Our algorithm called online linearized Bregman iteration (OLBI)
is derived from minimizing the cumulative prediction error squared along with
an l1-l2 norm regularizer. By systematically treating the non-differentiable
regularizer we arrive at a simple two-step iteration. We demonstrate that OLBI
is bias free and compare its operation with existing sparse LMS algorithms by
rederiving them in the online convex optimization framework. We perform
convergence analysis of OLBI for white input signals and derive theoretical
expressions for both the steady state and instantaneous mean square deviations
(MSD). We demonstrate numerically that OLBI improves the performance of LMS
type algorithms for signals generated from sparse tap weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0564</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0564</id><created>2012-10-01</created><authors><author><keyname>Hu</keyname><forenames>Tao</forenames></author><author><keyname>Nunez-Iglesias</keyname><forenames>Juan</forenames></author><author><keyname>Vitaladevuni</keyname><forenames>Shiv</forenames></author><author><keyname>Scheffer</keyname><forenames>Lou</forenames></author><author><keyname>Xu</keyname><forenames>Shan</forenames></author><author><keyname>Bolorizadeh</keyname><forenames>Mehdi</forenames></author><author><keyname>Hess</keyname><forenames>Harald</forenames></author><author><keyname>Fetter</keyname><forenames>Richard</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri</forenames></author></authors><title>Super-resolution using Sparse Representations over Learned Dictionaries:
  Reconstruction of Brain Structure using Electron Microscopy</title><categories>cs.CV q-bio.NC stat.ML</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem in neuroscience is reconstructing neuronal circuits on the
synapse level. Due to a wide range of scales in brain architecture such
reconstruction requires imaging that is both high-resolution and
high-throughput. Existing electron microscopy (EM) techniques possess required
resolution in the lateral plane and either high-throughput or high depth
resolution but not both. Here, we exploit recent advances in unsupervised
learning and signal processing to obtain high depth-resolution EM images
computationally without sacrificing throughput. First, we show that the brain
tissue can be represented as a sparse linear combination of localized basis
functions that are learned using high-resolution datasets. We then develop
compressive sensing-inspired techniques that can reconstruct the brain tissue
from very few (typically 5) tomographic views of each section. This enables
tracing of neuronal processes and, hence, high throughput reconstruction of
neural circuits on the level of individual synapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0568</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0568</id><created>2012-10-01</created><authors><author><keyname>Bursalioglu</keyname><forenames>Ozgun Y.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Divsalar</keyname><forenames>Dariush</forenames></author></authors><title>Joint Source-Channel Coding for Deep-Space Image Transmission using
  Rateless Codes</title><categories>cs.IT math.IT</categories><comments>36 pages, 5 figures, to be submitted to IEEE Trans. on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new coding scheme for image transmission over noisy channel is proposed.
Similar to standard image compression, the scheme includes a linear transform
followed by successive refinement scalar quantization. Unlike conventional
schemes, in the proposed system the quantized transform coefficients are
linearly mapped into channel symbols using systematic linear encoders. This
fixed-to-fixed length &quot;linear index coding&quot; approach avoids the use of an
explicit entropy coding stage (e.g., arithmetic or Huffman coding), which is
typically fragile to channel post-decoding residual errors. We use linear codes
over GF(4), which are particularly suited for this application, since they are
matched to the dead-zone quantizer symbol alphabet and to the QPSK modulation
used on the deep-space communication channel. We optimize the proposed system
where the linear codes are systematic Raptor codes over GF(4). The rateless
property of Raptor encoders allows to achieve a &quot;continuum&quot; of coding rates, in
order to accurately match the channel coding rate to the transmission channel
capacity and to the quantized source entropy rate for each transform subband
and refinement level. Comparisons are provided with respect to the
concatenation of state-of-the-art image coding and channel coding schemes used
by Jet Propulsion Laboratories (JPL) for the Mars Exploration Rover (MER)
Mission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0572</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0572</id><created>2012-10-01</created><updated>2013-10-05</updated><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author></authors><title>A Lower Bound for the Discrepancy of a Random Point Set</title><categories>math.NA cs.DM math.CO</categories><comments>7 pages</comments><msc-class>11K38, 60C05, 65C05</msc-class><doi>10.1016/j.jco.2013.06.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there is a constant $K &gt; 0$ such that for all $N, s \in \N$, $s
\le N$, the point set consisting of $N$ points chosen uniformly at random in
the $s$-dimensional unit cube $[0,1]^s$ with probability at least
$1-\exp(-\Theta(s))$ admits an axis parallel rectangle $[0,x] \subseteq
[0,1]^s$ containing $K \sqrt{sN}$ points more than expected. Consequently, the
expected star discrepancy of a random point set is of order $\sqrt{s/N}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0574</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0574</id><created>2012-10-01</created><updated>2012-10-19</updated><authors><author><keyname>Kuhtz</keyname><forenames>Lars</forenames><affiliation>Universit&#xe4;t des Saarlandes</affiliation></author><author><keyname>Finkbeiner</keyname><forenames>Bernd</forenames><affiliation>Universit&#xe4;t des Saarlandes</affiliation></author></authors><title>Efficient Parallel Path Checking for Linear-Time Temporal Logic With
  Past and Bounds</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1, F.2.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (October
  23, 2012) lmcs:1219</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path checking, the special case of the model checking problem where the model
under consideration is a single path, plays an important role in monitoring,
testing, and verification. We prove that for linear-time temporal logic (LTL),
path checking can be efficiently parallelized. In addition to the core logic,
we consider the extensions of LTL with bounded-future (BLTL) and past-time
(LTL+Past) operators. Even though both extensions improve the succinctness of
the logic exponentially, path checking remains efficiently parallelizable: Our
algorithm for LTL, LTL+Past, and BLTL+Past is in AC^1(logDCFL) \subseteq NC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0576</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0576</id><created>2012-10-01</created><authors><author><keyname>Dilawari</keyname><forenames>Jaswinder Singh</forenames></author><author><keyname>Sandhu</keyname><forenames>Gurpreet Singh</forenames></author></authors><title>A Review Paper on Microprocessor Based Controller Programming</title><categories>cs.OH</categories><comments>5 pages,4 Figures</comments><journal-ref>International Journal of Advanced Research in Computer Science and
  Software Engineering ,Volume 2,Issue 8,August2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing of microprocessor based controllers requires specific hardware as
well as software programming. Programming depends upon type of the software
whether operating software or application software. Programming requires
knowledge of system configuration and controller specific programming. Programs
are always in digital form so microprocessor can control directly at digital
level called Direct Digital Control (DDC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0577</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0577</id><created>2012-10-01</created><updated>2013-05-19</updated><authors><author><keyname>Antil</keyname><forenames>Harbir</forenames></author><author><keyname>Field</keyname><forenames>Scott E.</forenames></author><author><keyname>Herrmann</keyname><forenames>Frank</forenames></author><author><keyname>Nochetto</keyname><forenames>Ricardo H.</forenames></author><author><keyname>Tiglio</keyname><forenames>Manuel</forenames></author></authors><title>Two-step greedy algorithm for reduced order quadratures</title><categories>cs.NA gr-qc math.NA</categories><comments>27 pages, 9 figures, uses svjour3</comments><journal-ref>Journal of Scientific Computing, December 2013, Volume 57, Issue
  3, pp 604-637</journal-ref><doi>10.1007/s10915-013-9722-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm to generate application-specific, global reduced
order quadratures (ROQ) for multiple fast evaluations of weighted inner
products between parameterized functions. If a reduced basis (RB) or any other
projection-based model reduction technique is applied, the dimensionality of
integrands is reduced dramatically; however, the cost of approximating the
integrands by projection still scales as the size of the original problem. In
contrast, using discrete empirical interpolation (DEIM) points as ROQ nodes
leads to a computational cost which depends linearly on the dimension of the
reduced space. Generation of a reduced basis via a greedy procedure requires a
training set, which for products of functions can be very large. Since this
direct approach can be impractical in many applications, we propose instead a
two-step greedy targeted towards approximation of such products. We present
numerical experiments demonstrating the accuracy and the efficiency of the
two-step approach. The presented ROQ are expected to display very fast
convergence whenever there is regularity with respect to parameter variation.
We find that for the particular application here considered, one driven by
gravitational wave physics, the two-step approach speeds up the offline
computations to build the ROQ by more than two orders of magnitude.
Furthermore, the resulting ROQ rule is found to converge exponentially with the
number of nodes, and a factor of ~50 savings, without loss of accuracy, is
observed in evaluations of inner products when ROQ are used as a downsampling
strategy for equidistant samples using the trapezoidal rule. While the primary
focus of this paper is on quadrature rules for inner products of parameterized
functions, our method can be easily adapted to integrations of single
parameterized functions, and some examples of this type are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0595</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0595</id><created>2012-10-01</created><authors><author><keyname>Asiaee</keyname><forenames>Amir H.</forenames></author><author><keyname>Doshi</keyname><forenames>Prashant</forenames></author><author><keyname>Minning</keyname><forenames>Todd</forenames></author><author><keyname>Sahoo</keyname><forenames>Satya</forenames></author><author><keyname>Parikh</keyname><forenames>Priti</forenames></author><author><keyname>Sheth</keyname><forenames>Amit</forenames></author><author><keyname>Tarleton</keyname><forenames>Rick L.</forenames></author></authors><title>From Questions to Effective Answers: On the Utility of Knowledge-Driven
  Querying Systems for Life Sciences Data</title><categories>cs.IR cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We compare two distinct approaches for querying data in the context of the
life sciences. The first approach utilizes conventional databases to store the
data and intuitive form-based interfaces to facilitate easy querying of the
data. These interfaces could be seen as implementing a set of &quot;pre-canned&quot;
queries commonly used by the life science researchers that we study. The second
approach is based on semantic Web technologies and is knowledge (model) driven.
It utilizes a large OWL ontology and same datasets as before but associated as
RDF instances of the ontology concepts. An intuitive interface is provided that
allows the formulation of RDF triples-based queries. Both these approaches are
being used in parallel by a team of cell biologists in their daily research
activities, with the objective of gradually replacing the conventional approach
with the knowledge-driven one. This provides us with a valuable opportunity to
compare and qualitatively evaluate the two approaches. We describe several
benefits of the knowledge-driven approach in comparison to the traditional way
of accessing data, and highlight a few limitations as well. We believe that our
analysis not only explicitly highlights the specific benefits and limitations
of semantic Web technologies in our context but also contributes toward
effective ways of translating a question in a researcher's mind into precise
computational queries with the intent of obtaining effective answers from the
data. While researchers often assume the benefits of semantic Web technologies,
we explicitly illustrate these in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0607</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0607</id><created>2012-10-01</created><authors><author><keyname>Milenkovic</keyname><forenames>Victor</forenames></author><author><keyname>Sacks</keyname><forenames>Elisha</forenames></author><author><keyname>Trac</keyname><forenames>Steven</forenames></author></authors><title>Planar shape manipulation using approximate geometric primitives</title><categories>cs.CG</categories><msc-class>65D17</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present robust algorithms for set operations and Euclidean transformations
of curved shapes in the plane using approximate geometric primitives. We use a
refinement algorithm to ensure consistency. Its computational complexity is
$\bigo(n\log n+k)$ for an input of size $n$ with $k=\bigo(n^2)$ consistency
violations. The output is as accurate as the geometric primitives. We validate
our algorithms in floating point using sequences of six set operations and
Euclidean transforms on shapes bounded by curves of algebraic degree~1 to~6. We
test generic and degenerate inputs.
  Keywords: robust computational geometry, plane subdivisions, set operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0611</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0611</id><created>2012-10-01</created><authors><author><keyname>Barratt</keyname><forenames>James</forenames></author></authors><title>On the Automation of Encoding Processes in the Quantum IO Monad</title><categories>quant-ph cs.LO</categories><comments>In Proceedings QPL 2011, arXiv:1210.0298. In memory of my brother,
  Bill Barratt (1972- 2010)</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 95, 2012, pp. 15-19</journal-ref><doi>10.4204/EPTCS.95.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is now clear that the use of resilient encoding schemes will be required
for any quantum computing device to be realised. However, quantum programmers
of the future will not wish to be tied up in the particulars of such encoding
schemes. Quantum programming languages and libraries are already being
developed, one of which is the Quantum IO Monad. QIO, as it is often
abbreviated to, provides an interface to define and simulate quantum
computations via a library of functions written in Haskell, a purely functional
programming language. A solution is presented that takes an arbitrary QIO
program and returns an equivalent program incorporating some specified quantum
error correction techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0613</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0613</id><created>2012-10-01</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames><affiliation>Universit&#xe0; di Bologna &amp; INRIA Sophia Antipolis</affiliation></author><author><keyname>Faggian</keyname><forenames>Claudia</forenames><affiliation>CNRS &amp; Universit&#xe9; Denis-Diderot Paris 7</affiliation></author></authors><title>On Multiplicative Linear Logic, Modality and Quantum Circuits</title><categories>cs.LO quant-ph</categories><comments>In Proceedings QPL 2011, arXiv:1210.0298</comments><proxy>EPTCS</proxy><acm-class>F.4.1; F.1.1</acm-class><journal-ref>EPTCS 95, 2012, pp. 55-66</journal-ref><doi>10.4204/EPTCS.95.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A logical system derived from linear logic and called QMLL is introduced and
shown able to capture all unitary quantum circuits. Conversely, any proof is
shown to compute, through a concrete GoI interpretation, some quantum circuits.
The system QMLL, which enjoys cut-elimination, is obtained by endowing
multiplicative linear logic with a quantum modality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0614</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0614</id><created>2012-10-01</created><authors><author><keyname>Davidson</keyname><forenames>Timothy A. S.</forenames><affiliation>University of Warwick, UK</affiliation></author><author><keyname>Gay</keyname><forenames>Simon J.</forenames><affiliation>University of Glasgow, UK</affiliation></author><author><keyname>Nagarajan</keyname><forenames>Rajagopal</forenames><affiliation>University of Warwick, UK</affiliation></author><author><keyname>Puthoor</keyname><forenames>Ittoop Vergheese</forenames><affiliation>University of Glasgow, UK</affiliation></author></authors><title>Analysis of a Quantum Error Correcting Code using Quantum Process
  Calculus</title><categories>cs.LO cs.PL quant-ph</categories><comments>In Proceedings QPL 2011, arXiv:1210.0298</comments><proxy>EPTCS</proxy><acm-class>D.3.1; F.3.1</acm-class><journal-ref>EPTCS 95, 2012, pp. 67-80</journal-ref><doi>10.4204/EPTCS.95.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the use of quantum process calculus to describe and analyze
quantum communication protocols, following the successful field of formal
methods from classical computer science. The key idea is to define two systems,
one modelling a protocol and one expressing a specification, and prove that
they are behaviourally equivalent. We summarize the necessary theory in the
process calculus CQP, including the crucial result that equivalence is a
congruence, meaning that it is preserved by embedding in any context. We
illustrate the approach by analyzing two versions of a quantum error correction
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0621</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0621</id><created>2012-10-01</created><authors><author><keyname>Stepney</keyname><forenames>Susan</forenames><affiliation>Department of Computer Science, University of York, UK</affiliation></author><author><keyname>Kendon</keyname><forenames>Viv</forenames><affiliation>School of Physics and Astronomy, University of Leeds, UK</affiliation></author><author><keyname>Hines</keyname><forenames>Peter</forenames><affiliation>Department of Computer Science, University of York, UK</affiliation></author><author><keyname>Sebald</keyname><forenames>Angelika</forenames><affiliation>Department of Chemistry, University of York, UK</affiliation></author></authors><title>A Framework for Heterotic Computing</title><categories>cs.ET cs.LO</categories><comments>In Proceedings QPL 2011, arXiv:1210.0298</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 95, 2012, pp. 263-273</journal-ref><doi>10.4204/EPTCS.95.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational devices combining two or more different parts, one controlling
the operation of the other, for example, derive their power from the
interaction, in addition to the capabilities of the parts. Non-classical
computation has tended to consider only single computational models: neural,
analog, quantum, chemical, biological, neglecting to account for the
contribution from the experimental controls. In this position paper, we propose
a framework suitable for analysing combined computational models, from abstract
theory to practical programming tools. Focusing on the simplest example of one
system controlled by another through a sequence of operations in which only one
system is active at a time, the output from one system becomes the input to the
other for the next step, and vice versa. We outline the categorical machinery
required for handling diverse computational systems in such combinations, with
their interactions explicitly accounted for. Drawing on prior work in
refinement and retrenchment, we suggest an appropriate framework for developing
programming tools from the categorical framework. We place this work in the
context of two contrasting concepts of &quot;efficiency&quot;: theoretical comparisons to
determine the relative computational power do not always reflect the practical
comparison of real resources for a finite-sized computational task, especially
when the inputs include (approximations of) real numbers. Finally we outline
the limitations of our simple model, and identify some of the extensions that
will be required to treat more complex interacting computational systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0622</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0622</id><created>2012-10-01</created><authors><author><keyname>Wilce</keyname><forenames>Alexander</forenames></author></authors><title>Symmetry and Self-Duality in Categories of Probabilistic Models</title><categories>math-ph cs.LO math.MP quant-ph</categories><comments>In Proceedings QPL 2011, arXiv:1210.0298</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 95, 2012, pp. 275-279</journal-ref><doi>10.4204/EPTCS.95.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note adds to the recent spate of derivations of the probabilistic
apparatus of finite-dimensional quantum theory from various axiomatic packages.
We offer two different axiomatic packages that lead easily to the Jordan
algebraic structure of finite-dimensional quantum theory. The derivation relies
on the Koecher-Vinberg Theorem, which sets up an equivalence between order-unit
spaces having homogeneous, self-dual cones, and formally real Jordan algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0623</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0623</id><created>2012-10-01</created><updated>2013-05-12</updated><authors><author><keyname>Xie</keyname><forenames>Lexing</forenames></author><author><keyname>Natsev</keyname><forenames>Apostol</forenames></author><author><keyname>He</keyname><forenames>Xuming</forenames></author><author><keyname>Kender</keyname><forenames>John</forenames></author><author><keyname>Hill</keyname><forenames>Matthew</forenames></author><author><keyname>Smith</keyname><forenames>John R</forenames></author></authors><title>Tracking Large-Scale Video Remix in Real-World Events</title><categories>cs.SI cs.MM</categories><comments>11 pages, accepted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social information networks, such as YouTube, contains traces of both
explicit online interaction (such as &quot;like&quot;, leaving a comment, or subscribing
to video feed), and latent interactions (such as quoting, or remixing parts of
a video). We propose visual memes, or frequently re-posted short video
segments, for tracking such latent video interactions at scale. Visual memes
are extracted by scalable detection algorithms that we develop, with high
accuracy. We further augment visual memes with text, via a statistical model of
latent topics. We model content interactions on YouTube with visual memes,
defining several measures of influence and building predictive models for meme
popularity. Experiments are carried out on with over 2 million video shots from
more than 40,000 videos on two prominent news events in 2009: the election in
Iran and the swine flu epidemic. In these two events, a high percentage of
videos contain remixed content, and it is apparent that traditional news media
and citizen journalists have different roles in disseminating remixed content.
We perform two quantitative evaluations for annotating visual memes and
predicting their popularity. The joint statistical model of visual memes and
words outperform a concurrence model, and the average error is ~2% for
predicting meme volume and ~17% for their lifespan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0645</identifier>
 <datestamp>2013-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0645</id><created>2012-10-02</created><updated>2013-05-20</updated><authors><author><keyname>Yang</keyname><forenames>Yingzhen</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Nonparametric Unsupervised Classification</title><categories>cs.LG stat.ML</categories><comments>Submitted to ALT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised classification methods learn a discriminative classifier from
unlabeled data, which has been proven to be an effective way of simultaneously
clustering the data and training a classifier from the data. Various
unsupervised classification methods obtain appealing results by the classifiers
learned in an unsupervised manner. However, existing methods do not consider
the misclassification error of the unsupervised classifiers except unsupervised
SVM, so the performance of the unsupervised classifiers is not fully evaluated.
In this work, we study the misclassification error of two popular classifiers,
i.e. the nearest neighbor classifier (NN) and the plug-in classifier, in the
setting of unsupervised classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0650</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0650</id><created>2012-10-02</created><authors><author><keyname>Hillebrand</keyname><forenames>Anne</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Superdense Coding with GHZ and Quantum Key Distribution with W in the
  ZX-calculus</title><categories>quant-ph cs.CR</categories><comments>In Proceedings QPL 2011, arXiv:1210.0298</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 95, 2012, pp. 103-121</journal-ref><doi>10.4204/EPTCS.95.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum entanglement is a key resource in many quantum protocols, such as
quantum teleportation and quantum cryptography. Yet entanglement makes
protocols presented in Dirac notation difficult to verify. This is why Coecke
and Duncan have introduced a diagrammatic language for quantum protocols,
called the ZX-calculus. This diagrammatic notation is both intuitive and
formally rigorous. It is a simple, graphical, high level language that
emphasises the composition of systems and naturally captures the essentials of
quantum mechanics. In the author's MSc thesis it has been shown for over 25
quantum protocols that the ZX-calculus provides a relatively easy and more
intuitive presentation. Moreover, the author embarked on the task to apply
categorical quantum mechanics on quantum security; earlier works did not touch
anything but Bennett and Brassard's quantum key distribution protocol, BB84.
Superdense coding with the Greenberger-Horne-Zeilinger state and quantum key
distribution with the W-state are presented in the ZX-calculus in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0660</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0660</id><created>2012-10-02</created><authors><author><keyname>Dinh</keyname><forenames>Tien Tuan Anh</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>Stream on the Sky: Outsourcing Access Control Enforcement for Stream
  Data to the Cloud</title><categories>cs.CR cs.DB cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an increasing trend for businesses to migrate their systems towards
the cloud. Security concerns that arise when outsourcing data and computation
to the cloud include data confidentiality and privacy. Given that a tremendous
amount of data is being generated everyday from plethora of devices equipped
with sensing capabilities, we focus on the problem of access controls over live
streams of data based on triggers or sliding windows, which is a distinct and
more challenging problem than access control over archival data. Specifically,
we investigate secure mechanisms for outsourcing access control enforcement for
stream data to the cloud. We devise a system that allows data owners to specify
fine-grained policies associated with their data streams, then to encrypt the
streams and relay them to the cloud for live processing and storage for future
use. The access control policies are enforced by the cloud, without the latter
learning about the data, while ensuring that unauthorized access is not
feasible. To realize these ends, we employ a novel cryptographic primitive,
namely proxy-based attribute-based encryption, which not only provides security
but also allows the cloud to perform expensive computations on behalf of the
users. Our approach is holistic, in that these controls are integrated with an
XML based framework (XACML) for high-level management of policies. Experiments
with our prototype demonstrate the feasibility of such mechanisms, and early
evaluations suggest graceful scalability with increasing numbers of policies,
data streams and users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0664</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0664</id><created>2012-10-02</created><authors><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Lee</keyname><forenames>David</forenames></author></authors><title>Triadic Consensus: A Randomized Algorithm for Voting in a Crowd</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical voting rules do not work well in settings with many candidates. If
there are just several hundred candidates, then even a simple task such as
choosing a top candidate becomes impractical. Motivated by the hope of
developing group consensus mechanisms over the internet, where the numbers of
candidates could easily number in the thousands, we study an urn-based voting
rule where each participant acts as a voter and a candidate. We prove that when
participants lie in a one-dimensional space, this voting protocol finds a
$(1-\epsilon/sqrt{n})$ approximation of the Condorcet winner with high
probability while only requiring an expected $O(\frac{1}{\epsilon^2}\log^2
\frac{n}{\epsilon^2})$ comparisons on average per voter. Moreover, this voting
protocol is shown to have a quasi-truthful Nash equilibrium: namely, a Nash
equilibrium exists which may not be truthful, but produces a winner with the
same probability distribution as that of the truthful strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0685</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0685</id><created>2012-10-02</created><authors><author><keyname>Jenatton</keyname><forenames>Rodolphe</forenames><affiliation>CMAP</affiliation></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Local stability and robustness of sparse dictionary learning in the
  presence of noise</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A popular approach within the signal processing and machine learning
communities consists in modelling signals as sparse linear combinations of
atoms selected from a learned dictionary. While this paradigm has led to
numerous empirical successes in various fields ranging from image to audio
processing, there have only been a few theoretical arguments supporting these
evidences. In particular, sparse coding, or sparse dictionary learning, relies
on a non-convex procedure whose local minima have not been fully analyzed yet.
In this paper, we consider a probabilistic model of sparse signals, and show
that, with high probability, sparse coding admits a local minimum around the
reference dictionary generating the signals. Our study takes into account the
case of over-complete dictionaries and noisy signals, thus extending previous
work limited to noiseless settings and/or under-complete dictionaries. The
analysis we conduct is non-asymptotic and makes it possible to understand how
the key quantities of the problem, such as the coherence or the level of noise,
can scale with respect to the dimension of the signals, the number of atoms,
the sparsity and the number of observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0690</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0690</id><created>2012-10-02</created><updated>2012-12-22</updated><authors><author><keyname>Videla</keyname><forenames>Santiago</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Guziolowski</keyname><forenames>Carito</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Eduati</keyname><forenames>Federica</forenames><affiliation>DEI, EBI</affiliation></author><author><keyname>Thiele</keyname><forenames>Sven</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Grabe</keyname><forenames>Niels</forenames><affiliation>EBI</affiliation></author><author><keyname>Saez-Rodriguez</keyname><forenames>Julio</forenames><affiliation>EBI</affiliation></author><author><keyname>Siegel</keyname><forenames>Anne</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Revisiting the Training of Logic Models of Protein Signaling Networks
  with a Formal Approach based on Answer Set Programming</title><categories>q-bio.QM cs.AI cs.CE cs.LG</categories><proxy>ccsd</proxy><journal-ref>CMSB - 10th Computational Methods in Systems Biology 2012 7605
  (2012) 342-361</journal-ref><doi>10.1007/978-3-642-33636-2_20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental question in systems biology is the construction and training to
data of mathematical models. Logic formalisms have become very popular to model
signaling networks because their simplicity allows us to model large systems
encompassing hundreds of proteins. An approach to train (Boolean) logic models
to high-throughput phospho-proteomics data was recently introduced and solved
using optimization heuristics based on stochastic methods. Here we demonstrate
how this problem can be solved using Answer Set Programming (ASP), a
declarative problem solving paradigm, in which a problem is encoded as a
logical program such that its answer sets represent solutions to the problem.
ASP has significant improvements over heuristic methods in terms of efficiency
and scalability, it guarantees global optimality of solutions as well as
provides a complete set of solutions. We illustrate the application of ASP with
in silico cases based on realistic networks and data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0693</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0693</id><created>2012-10-02</created><authors><author><keyname>Stefanovi&#x107;</keyname><forenames>&#x10c;edomir</forenames></author><author><keyname>Trilingsgaard</keyname><forenames>Kasper F.</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Joint Estimation and Contention-Resolution Protocol for Wireless Random
  Access</title><categories>cs.IT math.IT</categories><comments>Submitted to ICC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a contention-based random-access protocol, designed for wireless
networks where the number of users is not a priori known. The protocol operates
in rounds divided into equal-duration slots, performing at the same time
estimation of the number of users and resolution of their transmissions. The
users independently access the wireless link on a slot basis with a predefined
probability, resulting in a distribution of user transmissions over slots,
based on which the estimation and contention resolution are performed.
Specifically, the contention resolution is performed using successive
interference cancellation which, coupled with the use of the optimized access
probabilities, enables throughputs that are substantially higher than the
traditional slotted ALOHA-like protocols. The key feature of the proposed
protocol is that the round durations are not a priori set and they are
terminated when the estimation/contention-resolution performance reach the
satisfactory levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0697</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0697</id><created>2012-10-02</created><updated>2013-09-08</updated><authors><author><keyname>Higuchi</keyname><forenames>Kojiro</forenames></author><author><keyname>Kihara</keyname><forenames>Takayuki</forenames></author></authors><title>Inside the Muchnik Degrees I: Discontinuity, Learnability, and
  Constructivism</title><categories>math.LO cs.LO</categories><msc-class>(2010): 03D30 (Primary) 03D78, 03F60, 03E15, 03B55, 68Q32
  (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every computable function has to be continuous. To develop computability
theory of discontinuous functions, we study low levels of the arithmetical
hierarchy of nonuniformly computable functions on Baire space. First, we
classify nonuniformly computable functions on Baire space from the viewpoint of
learning theory and piecewise computability. For instance, we show that
mind-change-bounded-learnability is equivalent to finite
$(\Pi^0_1)_2$-piecewise computability (where $(\Pi^0_1)_2$ denotes the
difference of two $\Pi^0_1$ sets), error-bounded-learnability is equivalent to
finite $\Delta^0_2$-piecewise computability, and learnability is equivalent to
countable $\Pi^0_1$-piecewise computability (equivalently, countable
$\Sigma^0_2$-piecewise computability). Second, we introduce disjunction-like
operations such as the coproduct based on BHK-like interpretations, and then,
we see that these operations induce Galois connections between the Medvedev
degree structure and associated Medvedev/Muchnik-like degree structures.
Finally, we interpret these results in the context of the Weihrauch degrees and
Wadge-like games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0699</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0699</id><created>2012-10-02</created><authors><author><keyname>Bresson</keyname><forenames>Xavier</forenames></author><author><keyname>Zhang</keyname><forenames>Ruiliang</forenames></author></authors><title>TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data
  Classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce semi-supervised data classification algorithms based on total
variation (TV), Reproducing Kernel Hilbert Space (RKHS), support vector machine
(SVM), Cheeger cut, labeled and unlabeled data points. We design binary and
multi-class semi-supervised classification algorithms. We compare the TV-based
classification algorithms with the related Laplacian-based algorithms, and show
that TV classification perform significantly better when the number of labeled
data is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0713</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0713</id><created>2012-10-02</created><authors><author><keyname>Vornanen</keyname><forenames>Tommi</forenames></author></authors><title>Open Science Project in White Dwarf Research</title><categories>astro-ph.IM astro-ph.SR cs.DL</categories><comments>To be published in the proceedings of the 18th European White Dwarf
  Workshop. 4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I will propose a new way of advancing white dwarf research. Open science is a
method of doing research that lets everyone who has something to say about the
subject take part in the problem solving process.
  Already now, the amount of information we gather from observations, theory
and modelling is too vast for any one individual to comprehend and turn into
knowledge. And the amount of information just keeps growing in the future. A
platform that promotes sharing of thoughts and ideas allows us to pool our
collective knowledge of white dwarfs and get a clear picture of our research
field. It will also make it possible for researchers in fields closely related
to ours (AGB stars, planetary nebulae etc.) to join the scientific discourse.
  In the first stage this project would allow us to summarize what we know and
what we don't, and what we should search for next. Later, it could grow into a
large collaboration that would have the impact to, for example, suggest
instrument requirements for future telescopes to satisfy the needs of the white
dwarf community, or propose large surveys.
  A simple implementation would be a wiki page for collecting knowledge
combined with a forum for more extensive discussions. These would be simple and
cheap to maintain. A large community effort on the whole would be needed for
the project to succeed, but individual workload should stay at a low level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0732</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0732</id><created>2012-10-02</created><authors><author><keyname>Mryglod</keyname><forenames>O.</forenames></author><author><keyname>Kenna</keyname><forenames>R.</forenames></author><author><keyname>Holovatch</keyname><forenames>Yu.</forenames></author><author><keyname>Berche</keyname><forenames>B.</forenames></author></authors><title>Absolute and specific measures of research group excellence</title><categories>stat.AP cs.DL physics.soc-ph</categories><journal-ref>Scientometrics 95 (2013) 115-127</journal-ref><doi>10.1007/s11192-012-0874-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A desirable goal of scientific management is to introduce, if it exists, a
simple and reliable way to measure the scientific excellence of publicly-funded
research institutions and universities to serve as a basis for their ranking
and financing. While citation-based indicators and metrics are easily
accessible, they are far from being universally accepted as way to automate or
inform evaluation processes or to replace evaluations based on peer review.
Here we consider absolute measurements of research excellence at an
amalgamated, institutional level and specific measures of research excellence
as performance per head. Using biology research institutions in the UK as a
test case, we examine the correlations between peer-review-based and
citation-based measures of research excellence on these two scales. We find
that citation-based indicators are very highly correlated with peer-evaluated
measures of group strength but are poorly correlated with group quality. Thus,
and almost paradoxically, our analysis indicates that citation counts could
possibly form a basis for deciding on how to fund research institutions but
they should not be used as a basis for ranking them in terms of quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0734</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0734</id><created>2012-10-02</created><authors><author><keyname>Kolchinsky</keyname><forenames>Artemy</forenames></author><author><keyname>Louren&#xe7;o</keyname><forenames>An&#xe1;lia</forenames></author><author><keyname>Li</keyname><forenames>Lang</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Evaluation of linear classifiers on articles containing pharmacokinetic
  evidence of drug-drug interactions</title><categories>stat.ML cs.LG q-bio.QM</categories><comments>Pacific Symposium on Biocomputing, 2013</comments><acm-class>H.2.8; H.3.1; J.3</acm-class><journal-ref>Pac Symp Biocomput. 2013:409-20</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background. Drug-drug interaction (DDI) is a major cause of morbidity and
mortality. [...] Biomedical literature mining can aid DDI research by
extracting relevant DDI signals from either the published literature or large
clinical databases. However, though drug interaction is an ideal area for
translational research, the inclusion of literature mining methodologies in DDI
workflows is still very preliminary. One area that can benefit from literature
mining is the automatic identification of a large number of potential DDIs,
whose pharmacological mechanisms and clinical significance can then be studied
via in vitro pharmacology and in populo pharmaco-epidemiology. Experiments. We
implemented a set of classifiers for identifying published articles relevant to
experimental pharmacokinetic DDI evidence. These documents are important for
identifying causal mechanisms behind putative drug-drug interactions, an
important step in the extraction of large numbers of potential DDIs. We
evaluate performance of several linear classifiers on PubMed abstracts, under
different feature transformation and dimensionality reduction methods. In
addition, we investigate the performance benefits of including various
publicly-available named entity recognition features, as well as a set of
internally-developed pharmacokinetic dictionaries. Results. We found that
several classifiers performed well in distinguishing relevant and irrelevant
abstracts. We found that the combination of unigram and bigram textual features
gave better performance than unigram features alone, and also that
normalization transforms that adjusted for feature frequency and document
length improved classification. For some classifiers, such as linear
discriminant analysis (LDA), proper dimensionality reduction had a large impact
on performance. Finally, the inclusion of NER features and dictionaries was
found not to help classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0748</identifier>
 <datestamp>2013-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0748</id><created>2012-10-02</created><updated>2013-05-02</updated><authors><author><keyname>Luo</keyname><forenames>Yongming</forenames></author><author><keyname>Fletcher</keyname><forenames>George H. L.</forenames></author><author><keyname>Hidders</keyname><forenames>Jan</forenames></author><author><keyname>Wu</keyname><forenames>Yuqing</forenames></author><author><keyname>De Bra</keyname><forenames>Paul</forenames></author></authors><title>External memory bisimulation reduction of big graphs</title><categories>cs.DB cs.DS</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present, to our knowledge, the first known I/O efficient
solutions for computing the k-bisimulation partition of a massive directed
graph, and performing maintenance of such a partition upon updates to the
underlying graph. Ubiquitous in the theory and application of graph data,
bisimulation is a robust notion of node equivalence which intuitively groups
together nodes in a graph which share fundamental structural features.
k-bisimulation is the standard variant of bisimulation where the topological
features of nodes are only considered within a local neighborhood of radius
$k\geqslant 0$.
  The I/O cost of our partition construction algorithm is bounded by $O(k\cdot
\mathit{sort}(|\et|) + k\cdot scan(|\nt|) + \mathit{sort}(|\nt|))$, while our
maintenance algorithms are bounded by $O(k\cdot \mathit{sort}(|\et|) + k\cdot
\mathit{sort}(|\nt|))$. The space complexity bounds are $O(|\nt|+|\et|)$ and
$O(k\cdot|\nt|+k\cdot|\et|)$, resp. Here, $|\et|$ and $|\nt|$ are the number of
disk pages occupied by the input graph's edge set and node set, resp., and
$\mathit{sort}(n)$ and $\mathit{scan}(n)$ are the cost of sorting and scanning,
resp., a file occupying $n$ pages in external memory. Empirical analysis on a
variety of massive real-world and synthetic graph datasets shows that our
algorithms perform efficiently in practice, scaling gracefully as graphs grow
in size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0754</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0754</id><created>2012-10-02</created><authors><author><keyname>Lindeberg</keyname><forenames>Tony</forenames></author></authors><title>Invariance of visual operations at the level of receptive fields</title><categories>q-bio.NC cs.CV</categories><comments>40 pages, 17 figures</comments><journal-ref>PLoS ONE 8(7):e66990, 2013</journal-ref><doi>10.1371/journal.pone.0066990</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Receptive field profiles registered by cell recordings have shown that
mammalian vision has developed receptive fields tuned to different sizes and
orientations in the image domain as well as to different image velocities in
space-time. This article presents a theoretical model by which families of
idealized receptive field profiles can be derived mathematically from a small
set of basic assumptions that correspond to structural properties of the
environment. The article also presents a theory for how basic invariance
properties to variations in scale, viewing direction and relative motion can be
obtained from the output of such receptive fields, using complementary
selection mechanisms that operate over the output of families of receptive
fields tuned to different parameters. Thereby, the theory shows how basic
invariance properties of a visual system can be obtained already at the level
of receptive fields, and we can explain the different shapes of receptive field
profiles found in biological vision from a requirement that the visual system
should be invariant to the natural types of image transformations that occur in
its environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0756</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0756</id><created>2012-10-02</created><authors><author><keyname>Golosovsky</keyname><forenames>Michael</forenames></author><author><keyname>Solomon</keyname><forenames>Sorin</forenames></author></authors><title>Stochastic dynamical model of a growing network based on self-exciting
  point process</title><categories>physics.soc-ph cond-mat.stat-mech cs.DL cs.SI stat.OT</categories><comments>16 pages, 9 figures</comments><journal-ref>Phys. Rev. Lett. 109, 098701 (2012)</journal-ref><doi>10.1103/PhysRevLett.109.098701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform experimental verification of the preferential attachment model
that is commonly accepted as a generating mechanism of the scale-free complex
networks. To this end we chose citation network of Physics papers and traced
citation history of 40,195 papers published in one year. Contrary to common
belief, we found that citation dynamics of the individual papers follows the
\emph{superlinear} preferential attachment, with the exponent $\alpha=
1.25-1.3$. Moreover, we showed that the citation process cannot be described as
a memoryless Markov chain since there is substantial correlation between the
present and recent citation rates of a paper. Basing on our findings we
constructed a stochastic growth model of the citation network, performed
numerical simulations based on this model and achieved an excellent agreement
with the measured citation distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0758</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0758</id><created>2012-10-02</created><authors><author><keyname>Cerra</keyname><forenames>Daniele</forenames></author><author><keyname>Datcu</keyname><forenames>Mihai</forenames></author></authors><title>A fast compression-based similarity measure with applications to
  content-based image retrieval</title><categories>stat.ML cs.IR cs.LG</categories><comments>Pre-print</comments><journal-ref>Journal of Visual Communication and Image Representation, vol. 23,
  no. 2, pp. 293-302, 2012</journal-ref><doi>10.1016/j.jvcir.2011.10.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compression-based similarity measures are effectively employed in
applications on diverse data types with a basically parameter-free approach.
Nevertheless, there are problems in applying these techniques to
medium-to-large datasets which have been seldom addressed. This paper proposes
a similarity measure based on compression with dictionaries, the Fast
Compression Distance (FCD), which reduces the complexity of these methods,
without degradations in performance. On its basis a content-based color image
retrieval system is defined, which can be compared to state-of-the-art methods
based on invariant color features. Through the FCD a better understanding of
compression-based techniques is achieved, by performing experiments on datasets
which are larger than the ones analyzed so far in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0762</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0762</id><created>2012-10-02</created><authors><author><keyname>Mahrsi</keyname><forenames>Mohamed Khalil El</forenames><affiliation>LTCI</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Graph-Based Approaches to Clustering Network-Constrained Trajectory Data</title><categories>cs.LG stat.ML</categories><proxy>ccsd</proxy><journal-ref>Workshop on New Frontiers in Mining Complex Patterns (NFMCP 2012),
  held at ECML-PKDD 2012, Bristol : United Kingdom (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though clustering trajectory data attracted considerable attention in
the last few years, most of prior work assumed that moving objects can move
freely in an euclidean space and did not consider the eventual presence of an
underlying road network and its influence on evaluating the similarity between
trajectories. In this paper, we present two approaches to clustering
network-constrained trajectory data. The first approach discovers clusters of
trajectories that traveled along the same parts of the road network. The second
approach is segment-oriented and aims to group together road segments based on
trajectories that they have in common. Both approaches use a graph model to
depict the interactions between observations w.r.t. their similarity and
cluster this similarity graph using a community detection algorithm. We also
present experimental results obtained on synthetic data to showcase our
propositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0772</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0772</id><created>2012-10-02</created><authors><author><keyname>Liu</keyname><forenames>Yanfang</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Relationship between the second type of covering-based rough set and
  matroid via closure operator</title><categories>cs.AI</categories><comments>10 pages</comments><acm-class>I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, in order to broad the application and theoretical areas of rough
sets and matroids, some authors have combined them from many different
viewpoints, such as circuits, rank function, spanning sets and so on. In this
paper, we connect the second type of covering-based rough sets and matroids
from the view of closure operators. On one hand, we establish a closure system
through the fixed point family of the second type of covering lower
approximation operator, and then construct a closure operator. For a covering
of a universe, the closure operator is a closure one of a matroid if and only
if the reduct of the covering is a partition of the universe. On the other
hand, we investigate the sufficient and necessary condition that the second
type of covering upper approximation operation is a closure one of a matroid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0794</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0794</id><created>2012-10-02</created><authors><author><keyname>Jlaiel</keyname><forenames>Nahla</forenames></author><author><keyname>Madhbouh</keyname><forenames>Khouloud</forenames></author><author><keyname>Ahmed</keyname><forenames>Mohamed Ben</forenames></author></authors><title>A Semantic Approach for Automatic Structuring and Analysis of Software
  Process Patterns</title><categories>cs.AI cs.CL</categories><comments>08 pages, 10 figures, Published with International Journal of
  Computer Applications (IJCA)</comments><journal-ref>Nahla Jlaiel, Khouloud Madhbouh and Mohamed Ben Ahmed. Article: A
  Semantic Approach for Automatic Structuring and Analysis of Software Process
  Patterns. International Journal of Computer Applications 54(15):24-31,
  September 2012</journal-ref><doi>10.5120/8643-2503</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The main contribution of this paper, is to propose a novel semantic approach
based on a Natural Language Processing technique in order to ensure a semantic
unification of unstructured process patterns which are expressed not only in
different formats but also, in different forms. This approach is implemented
using the GATE text engineering framework and then evaluated leading up to
high-quality results motivating us to continue in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0800</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0800</id><created>2012-10-02</created><updated>2013-01-13</updated><authors><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author><author><keyname>Yoffe</keyname><forenames>Genady</forenames></author></authors><title>Orthogononalization on a general purpose graphics processing unit with
  double double and quad double arithmetic</title><categories>cs.MS cs.DC math.NA</categories><comments>replaced with revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our problem is to accurately solve linear systems on a general purpose
graphics processing unit with double double and quad double arithmetic. The
linear systems originate from the application of Newton's method on polynomial
systems. Newton's method is applied as a corrector in a path following method,
so the linear systems are solved in sequence and not simultaneously. One
solution path may require the solution of thousands of linear systems. In
previous work we reported good speedups with our implementation to evaluate and
differentiate polynomial systems on the NVIDIA Tesla C2050. Although the cost
of evaluation and differentiation often dominates the cost of linear system
solving in Newton's method, because of the limited bandwidth of the
communication between CPU and GPU, we cannot afford to send the linear system
to the CPU for solving during path tracking.
  Because of large degrees, the Jacobian matrix may contain extreme values,
requiring extended precision, leading to a significant overhead. This overhead
of multiprecision arithmetic is our main motivation to develop a massively
parallel algorithm. To allow overdetermined linear systems we solve linear
systems in the least squares sense, computing the QR decomposition of the
matrix by the modified Gram-Schmidt algorithm. We describe our implementation
of the modified Gram-Schmidt orthogonalization method for the NVIDIA Tesla
C2050, using double double and quad double arithmetic. Our experimental results
show that the achieved speedups are sufficiently high to compensate for the
overhead of one extra level of precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0808</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0808</id><created>2012-10-02</created><authors><author><keyname>Grabowicz</keyname><forenames>Przemyslaw A.</forenames></author><author><keyname>Ramasco</keyname><forenames>Jose J.</forenames></author><author><keyname>Eguiluz</keyname><forenames>Victor M.</forenames></author></authors><title>Dynamics in online social networks</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 4 figures, book chapter</comments><journal-ref>Dynamics On and Of Complex Networks, Volume 2, pp. 3-17 (Springer
  New York, 2013)</journal-ref><doi>10.1007/978-1-4614-6729-8_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of today's social interactions occurs using online
social media as communication channels. Some online social networks have become
extremely popular in the last decade. They differ among themselves in the
character of the service they provide to online users. For instance, Facebook
can be seen mainly as a platform for keeping in touch with close friends and
relatives, Twitter is used to propagate and receive news, LinkedIn facilitates
the maintenance of professional contacts, Flickr gathers amateurs and
professionals of photography, etc. Albeit different, all these online platforms
share an ingredient that pervades all their applications. There exists an
underlying social network that allows their users to keep in touch with each
other and helps to engage them in common activities or interactions leading to
a better fulfillment of the service's purposes. This is the reason why these
platforms share a good number of functionalities, e.g., personal communication
channels, broadcasted status updates, easy one-step information sharing, news
feeds exposing broadcasted content, etc. As a result, online social networks
are an interesting field to study an online social behavior that seems to be
generic among the different online services. Since at the bottom of these
services lays a network of declared relations and the basic interactions in
these platforms tend to be pairwise, a natural methodology for studying these
systems is provided by network science. In this chapter we describe some of the
results of research studies on the structure, dynamics and social activity in
online social networks. We present them in the interdisciplinary context of
network science, sociological studies and computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0811</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0811</id><created>2012-10-02</created><updated>2012-11-13</updated><authors><author><keyname>Bapst</keyname><forenames>Victor</forenames></author><author><keyname>Foini</keyname><forenames>Laura</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Semerjian</keyname><forenames>Guilhem</forenames></author><author><keyname>Zamponi</keyname><forenames>Francesco</forenames></author></authors><title>The Quantum Adiabatic Algorithm applied to random optimization problems:
  the quantum spin glass perspective</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CC quant-ph</categories><comments>151 pages, 21 figures</comments><journal-ref>Physics Reports 523, 127 (2013)</journal-ref><doi>10.1016/j.physrep.2012.10.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among various algorithms designed to exploit the specific properties of
quantum computers with respect to classical ones, the quantum adiabatic
algorithm is a versatile proposition to find the minimal value of an arbitrary
cost function (ground state energy). Random optimization problems provide a
natural testbed to compare its efficiency with that of classical algorithms.
These problems correspond to mean field spin glasses that have been extensively
studied in the classical case. This paper reviews recent analytical works that
extended these studies to incorporate the effect of quantum fluctuations, and
presents also some original results in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0818</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0818</id><created>2012-10-02</created><authors><author><keyname>AlMahafzah</keyname><forenames>Harbi</forenames></author><author><keyname>Imran</keyname><forenames>Mohammad</forenames></author><author><keyname>Sheshadri</keyname><forenames>H. S.</forenames></author></authors><title>Multibiometric: Feature Level Fusion Using FKP Multi-Instance biometric</title><categories>cs.CV</categories><comments>8 pages paper</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 4, No 3, July 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposed the use of multi-instance feature level fusion as a means
to improve the performance of Finger Knuckle Print (FKP) verification. A
log-Gabor filter has been used to extract the image local orientation
information, and represent the FKP features. Experiments are performed using
the FKP database, which consists of 7,920 images. Results indicate that the
multi-instance verification approach outperforms higher performance than using
any single instance. The influence on biometric performance using feature level
fusion under different fusion rules have been demonstrated in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0822</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0822</id><created>2012-10-02</created><authors><author><keyname>Rumpf</keyname><forenames>Martin</forenames></author><author><keyname>Wirth</keyname><forenames>Benedikt</forenames></author></authors><title>Discrete geodesic calculus in the space of viscous fluidic objects</title><categories>math.NA cs.CV</categories><msc-class>68U10, 53C22, 74B20, 49M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a local approximation of the Riemannian distance on a manifold by a
computationally cheap dissimilarity measure, a time discrete geodesic calculus
is developed, and applications to shape space are explored. The dissimilarity
measure is derived from a deformation energy whose Hessian reproduces the
underlying Riemannian metric, and it is used to define length and energy of
discrete paths in shape space. The notion of discrete geodesics defined as
energy minimizing paths gives rise to a discrete logarithmic map, a variational
definition of a discrete exponential map, and a time discrete parallel
transport. This new concept is applied to a shape space in which shapes are
considered as boundary contours of physical objects consisting of viscous
material. The flexibility and computational efficiency of the approach is
demonstrated for topology preserving shape morphing, the representation of
paths in shape space via local shape variations as path generators, shape
extrapolation via discrete geodesic flow, and the transfer of geometric
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0824</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0824</id><created>2012-10-02</created><authors><author><keyname>Szabo</keyname><forenames>Zoltan</forenames></author><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Distributed High Dimensional Information Theoretical Image Registration
  via Random Projections</title><categories>cs.IT cs.LG math.IT stat.ML</categories><msc-class>62B10, 94A15, 94A17, 68W10, 68W15, 68W20</msc-class><acm-class>E.4; H.1.1; G.1.0</acm-class><journal-ref>Zoltan Szabo, Andras Lorincz: Distributed High Dimensional
  Information Theoretical Image Registration via Random Projections. Digital
  Signal Processing, 22(6):894-902, 2012</journal-ref><doi>10.1016/j.dsp.2012.04.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theoretical measures, such as entropy, mutual information, and
various divergences, exhibit robust characteristics in image registration
applications. However, the estimation of these quantities is computationally
intensive in high dimensions. On the other hand, consistent estimation from
pairwise distances of the sample points is possible, which suits random
projection (RP) based low dimensional embeddings. We adapt the RP technique to
this task by means of a simple ensemble method. To the best of our knowledge,
this is the first distributed, RP based information theoretical image
registration approach. The efficiency of the method is demonstrated through
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0828</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0828</id><created>2012-10-02</created><authors><author><keyname>Kock</keyname><forenames>Joachim</forenames></author></authors><title>Data types with symmetries and polynomial functors over groupoids</title><categories>cs.LO math.CO math.CT</categories><comments>This is the final version of my conference paper presented at the
  28th Conference on the Mathematical Foundations of Programming Semantics
  (Bath, June 2012); to appear in the Electronic Notes in Theoretical Computer
  Science. 16pp</comments><msc-class>68Q65, 18C50, 20L05, 05E18, 05C05</msc-class><acm-class>D.3.3; F.3.2</acm-class><journal-ref>Proceedings of the 28th Conference on the Mathematical Foundations
  of Programming Semantics, Bath 2012, Electronic Notes in Theoretical Computer
  Science 286 (2012), 351-365</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial functors are useful in the theory of data types, where they are
often called containers. They are also useful in algebra, combinatorics,
topology, and higher category theory, and in this broader perspective the
polynomial aspect is often prominent and justifies the terminology. For
example, Tambara's theorem states that the category of finite polynomial
functors is the Lawvere theory for commutative semirings. In this talk I will
explain how an upgrade of the theory from sets to groupoids is useful to deal
with data types with symmetries, and provides a common generalisation of and a
clean unifying framework for quotient containers (cf. Abbott et al.), species
and analytic functors (Joyal 1985), as well as the stuff types of Baez-Dolan.
The multi-variate setting also includes relations and spans, multispans, and
stuff operators. An attractive feature of this theory is that with the correct
homotopical approach - homotopy slices, homotopy pullbacks, homotopy colimits,
etc. - the groupoid case looks exactly like the set case. After some standard
examples, I will illustrate the notion of data-types-with-symmetries with
examples from quantum field theory, where the symmetries of complicated tree
structures of graphs play a crucial role, and can be handled elegantly using
polynomial functors over groupoids. (These examples, although beyond species,
are purely combinatorial and can be appreciated without background in quantum
field theory.) Locally cartesian closed 2-categories provide semantics for
2-truncated intensional type theory. For a fullfledged type theory, locally
cartesian closed \infty-categories seem to be needed. The theory of these is
being developed by D.Gepner and the author as a setting for homotopical
species, and several of the results exposed in this talk are just truncations
of \infty-results obtained in joint work with Gepner. Details will appear
elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0829</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0829</id><created>2012-10-02</created><authors><author><keyname>AlMahafzah</keyname><forenames>Harbi</forenames></author><author><keyname>AlRwashdeh</keyname><forenames>Maen Zaid</forenames></author></authors><title>A Survey of Multibiometric Systems</title><categories>cs.CV</categories><journal-ref>International Journal of Computer Application volume 43 No 15
  April 2012</journal-ref><doi>10.5120/6182-8612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most biometric systems deployed in real-world applications are unimodal.
Using unimodal biometric systems have to contend with a variety of problems
such as: Noise in sensed data; Intra-class variations; Inter-class
similarities; Non-universality; Spoof attacks. These problems have addressed by
using multibiometric systems, which expected to be more reliable due to the
presence of multiple, independent pieces of evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0840</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0840</id><created>2012-10-02</created><authors><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author><author><keyname>Thompson</keyname><forenames>Donna</forenames></author></authors><title>ADS Labs - Supporting Information Discovery in Science Education</title><categories>astro-ph.IM cs.DL</categories><comments>6 pages, 3 figures, to appear in the proceedings of the 2012 ASP
  Meeting &quot;Communicating Science&quot;, held August 4-8, in Tucson, AZ</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The SAO/NASA Astrophysics Data System (ADS) is an open access digital library
portal for researchers in astronomy and physics, operated by the Smithsonian
Astrophysical Observatory (SAO) under a NASA grant, successfully serving the
professional science community for two decades. Currently there are about
55,000 frequent users (100+ queries per year), and up to 10 million infrequent
users per year. Access by the general public now accounts for about half of all
ADS use, demonstrating the vast reach of the content in our databases. The
visibility and use of content in the ADS can be measured by the fact that there
are over 17,000 links from Wikipedia pages to ADS content, a figure comparable
to the number of links that Wikipedia has to OCLCs WorldCat catalog. The ADS,
through its holdings and innovative techniques available in ADS Labs
(http://adslabs.org), offers an environment for information discovery that is
unlike any other service currently available to the astrophysics community.
Literature discovery and review are important components of science education,
aiding the process of preparing for a class, project, or presentation. The ADS
has been recognized as a rich source of information for the science education
community in astronomy, thanks to its collaborations within the astronomy
community, publishers and projects like Com- PADRE. One element that makes the
ADS uniquely relevant for the science education community is the availability
of powerful tools to explore aspects of the astronomy literature as well as the
relationship between topics, people, observations and scientific papers. The
other element is the extensive repository of scanned literature, a significant
fraction of which consists of historical literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0848</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0848</id><created>2012-10-02</created><authors><author><keyname>Doan</keyname><forenames>Son</forenames></author><author><keyname>Ohno-Machado</keyname><forenames>Lucila</forenames></author><author><keyname>Collier</keyname><forenames>Nigel</forenames></author></authors><title>Enhancing Twitter Data Analysis with Simple Semantic Filtering: Example
  in Tracking Influenza-Like Illnesses</title><categories>cs.SI cs.CL physics.soc-ph</categories><comments>10 pages, 5 figures, IEEE HISB 2012 conference, Sept 27-28, 2012, La
  Jolla, California, US</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems that exploit publicly available user generated content such as
Twitter messages have been successful in tracking seasonal influenza. We
developed a novel filtering method for Influenza-Like-Illnesses (ILI)-related
messages using 587 million messages from Twitter micro-blogs. We first filtered
messages based on syndrome keywords from the BioCaster Ontology, an extant
knowledge model of laymen's terms. We then filtered the messages according to
semantic features such as negation, hashtags, emoticons, humor and geography.
The data covered 36 weeks for the US 2009 influenza season from 30th August
2009 to 8th May 2010. Results showed that our system achieved the highest
Pearson correlation coefficient of 98.46% (p-value&lt;2.2e-16), an improvement of
3.98% over the previous state-of-the-art method. The results indicate that
simple NLP-based enhancements to existing approaches to mine Twitter data can
increase the value of this inexpensive resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0852</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0852</id><created>2012-10-02</created><authors><author><keyname>G&#xf6;dert</keyname><forenames>Winfried</forenames></author></authors><title>Detecting multiword phrases in mathematical text corpora</title><categories>cs.CL cs.IR</categories><comments>20 pages, 3 figures</comments><msc-class>68P20</msc-class><acm-class>H.3.1; I.7.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for detecting multiword phrases in mathematical text
corpora. The method used is based on characteristic features of mathematical
terminology. It makes use of a software tool named Lingo which allows to
identify words by means of previously defined dictionaries for specific word
classes as adjectives, personal names or nouns. The detection of multiword
groups is done algorithmically. Possible advantages of the method for indexing
and information retrieval and conclusions for applying dictionary-based methods
of automatic indexing instead of stemming procedures are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0862</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0862</id><created>2012-10-02</created><updated>2012-10-03</updated><authors><author><keyname>Li</keyname><forenames>Qian</forenames></author><author><keyname>Braunstein</keyname><forenames>Lidia A.</forenames></author><author><keyname>Wang</keyname><forenames>Huijuan</forenames></author><author><keyname>Shao</keyname><forenames>Jia</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author></authors><title>Non-consensus opinion models on complex networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1007/s10955-012-0625-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on non-consensus opinion models in which above a certain threshold
two opinions coexist in a stable relationship. We revisit and extend the
non-consensus opinion (NCO) model introduced by Shao. We generalize the NCO
model by adding a weight factor W to individual's own opinion when determining
its future opinion (NCOW model). We find that as W increases the minority
opinion holders tend to form stable clusters with a smaller initial minority
fraction compared to the NCO model. We also revisit another non-consensus
opinion, the inflexible contrarian opinion (ICO) model, which introduces
inflexible contrarians to model a competition between two opinions in the
steady state. In the ICO model, the inflexible contrarians effectively decrease
the size of the largest cluster of the rival opinion. All of the above models
have previously been explored in terms of a single network. However opinions
propagate not only within single networks but also between networks, we study
here the opinion dynamics in coupled networks. We apply the NCO rule on each
individual network and the global majority rule on interdependent pairs. We
find that the interdependent links effectively force the system from a second
order phase transition, which is characteristic of the NCO model on a single
network, to a hybrid phase transition, i.e., a mix of second-order and abrupt
jump-like transitions that ultimately becomes, as we increase the percentage of
interdependent agents, a pure abrupt transition. We conclude that for the NCO
model on coupled networks, interactions through interdependent links could push
the non-consensus opinion type model to a consensus opinion type model, which
mimics the reality that increased mass communication causes people to hold
opinions that are increasingly similar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0864</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0864</id><created>2012-10-02</created><authors><author><keyname>Chan</keyname><forenames>Siu-on</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author><author><keyname>Sun</keyname><forenames>Xiaorui</forenames></author></authors><title>Learning mixtures of structured distributions over discrete domains</title><categories>cs.LG cs.DS math.ST stat.TH</categories><comments>preliminary full version of soda'13 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathfrak{C}$ be a class of probability distributions over the discrete
domain $[n] = \{1,...,n\}.$ We show that if $\mathfrak{C}$ satisfies a rather
general condition -- essentially, that each distribution in $\mathfrak{C}$ can
be well-approximated by a variable-width histogram with few bins -- then there
is a highly efficient (both in terms of running time and sample complexity)
algorithm that can learn any mixture of $k$ unknown distributions from
$\mathfrak{C}.$
  We analyze several natural types of distributions over $[n]$, including
log-concave, monotone hazard rate and unimodal distributions, and show that
they have the required structural property of being well-approximated by a
histogram with few bins. Applying our general algorithm, we obtain
near-optimally efficient algorithms for all these mixture learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0866</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0866</id><created>2012-10-02</created><authors><author><keyname>Adcock</keyname><forenames>Aaron</forenames></author><author><keyname>Rubin</keyname><forenames>Daniel</forenames></author><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author></authors><title>Classification of Hepatic Lesions using the Matching Metric</title><categories>cs.CV cs.CG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a methodology of classifying hepatic (liver) lesions
using multidimensional persistent homology, the matching metric (also called
the bottleneck distance), and a support vector machine. We present our
classification results on a dataset of 132 lesions that have been outlined and
annotated by radiologists. We find that topological features are useful in the
classification of hepatic lesions. We also find that two-dimensional persistent
homology outperforms one-dimensional persistent homology in this application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0880</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0880</id><created>2012-10-02</created><authors><author><keyname>Iglesias</keyname><forenames>Jose A.</forenames></author><author><keyname>Kimmel</keyname><forenames>Ron</forenames></author></authors><title>Schr\&quot;{o}dinger Diffusion for Shape Analysis with Texture</title><categories>cs.CV cs.CG cs.GR math.AP</categories><comments>Extended version of NORDIA'12 paper. Includes one more figure and
  appendix with proof</comments><msc-class>68U05, 35K08</msc-class><acm-class>I.3.5; I.2.10; G.1.8</acm-class><doi>10.1007/978-3-642-33863-2_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, quantities derived from the heat equation have become
popular in shape processing and analysis of triangulated surfaces. Such
measures are often robust with respect to different kinds of perturbations,
including near-isometries, topological noise and partialities. Here, we propose
to exploit the semigroup of a Schr\&quot;{o}dinger operator in order to deal with
texture data, while maintaining the desirable properties of the heat kernel. We
define a family of Schr\&quot;{o}dinger diffusion distances analogous to the ones
associated to the heat kernels, and show that they are continuous under
perturbations of the data. As an application, we introduce a method for
retrieval of textured shapes through comparison of Schr\&quot;{o}dinger diffusion
distance histograms with the earth's mover distance, and present some numerical
experiments showing superior performance compared to an analogous method that
ignores the texture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0887</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0887</id><created>2012-10-02</created><authors><author><keyname>Dobrev</keyname><forenames>Dimiter</forenames></author></authors><title>The Definition of AI in Terms of Multi Agent Systems</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The questions which we will consider here are &quot;What is AI?&quot; and &quot;How can we
make AI?&quot;. Here we will present the definition of AI in terms of multi-agent
systems. This means that here you will not find a new answer to the question
&quot;What is AI?&quot;, but an old answer in a new form.
  This new form of the definition of AI is of interest for the theory of
multi-agent systems because it gives us better understanding of this theory.
More important is that this work will help us answer the second question. We
want to make a program which is capable of constructing a model of its
environment. Every multi-agent model is equivalent to a single-agent model but
multi-agent models are more natural and accordingly more easily discoverable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0888</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0888</id><created>2012-10-02</created><authors><author><keyname>Majumdar</keyname><forenames>Anirudha</forenames></author><author><keyname>Ahmadi</keyname><forenames>Amir Ali</forenames></author><author><keyname>Tedrake</keyname><forenames>Russ</forenames></author></authors><title>Control Design along Trajectories with Sums of Squares Programming</title><categories>cs.RO cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the need for formal guarantees on the stability and safety of
controllers for challenging robot control tasks, we present a control design
procedure that explicitly seeks to maximize the size of an invariant &quot;funnel&quot;
that leads to a predefined goal set. Our certificates of invariance are given
in terms of sums of squares proofs of a set of appropriately defined Lyapunov
inequalities. These certificates, together with our proposed polynomial
controllers, can be efficiently obtained via semidefinite optimization. Our
approach can handle time-varying dynamics resulting from tracking a given
trajectory, input saturations (e.g. torque limits), and can be extended to deal
with uncertainty in the dynamics and state. The resulting controllers can be
used by space-filling feedback motion planning algorithms to fill up the space
with significantly fewer trajectories. We demonstrate our approach on a
severely torque limited underactuated double pendulum (Acrobot) and provide
extensive simulation and hardware validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0891</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0891</id><created>2012-10-02</created><updated>2013-02-25</updated><authors><author><keyname>Alexandropoulos</keyname><forenames>George C.</forenames></author><author><keyname>Papadias</keyname><forenames>Constantinos B.</forenames></author></authors><title>A Reconfigurable Distributed Algorithm for K-user MIMO Interference
  Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures, accepted for IEEE ICC 2013</comments><msc-class>94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is already well-known that interference alignment (IA) achieves the sum
capacity of the K-user interference channel at the high interference regime. On
the other hand, it is intuitively clear that when the interference levels are
very low, a sum-rate scaling of K (as opposed to K/2 for IA) should be accessed
at high signal-to-noise ratio values by simple (&quot;myopic&quot;) single-link
multiple-input multiple-output (MIMO) techniques such as waterfilling. Recent
results have indicated that in certain low-to-moderate interference cases,
treating interference as noise may in fact be preferable. In this paper, we
present a distributed iterative algorithm for K-user MIMO interference networks
which attempts to adjust itself to the interference regime at hand, in the
above sense, as well as to the channel conditions. The proposed algorithm
combines the system-wide mean squared error minimization with the waterfilling
solution to adjust to the interference levels and channel conditions and
maximize accordingly each user's transmission rate. Sum-rate computer
simulations for the proposed algorithm over Ricean fading channels show that,
in the interference-limited regime, the proposed algorithm reconfigures itself
in order to achieve the IA scaling whereas, in the low-to-moderate interference
regime, it leads itself towards interference-myopic MIMO transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0930</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0930</id><created>2012-10-02</created><authors><author><keyname>Ciuonzo</keyname><forenames>D.</forenames></author><author><keyname>Romano</keyname><forenames>G.</forenames></author><author><keyname>Rossi</keyname><forenames>P. Salvo</forenames></author></authors><title>Optimality of Received Energy in Decision Fusion over Rayleigh Fading
  Diversity MAC with Non-Identical Sensors</title><categories>cs.IT math.IT</categories><comments>14 pages, 3 figures, accepted at IEEE Transactions on Signal
  Processing</comments><journal-ref>IEEE Transactions on Signal Processing, 2013, Vol. 61, Issue 1,
  pp. 22-27</journal-ref><doi>10.1109/TSP.2012.2223694</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Received-energy test for non-coherent decision fusion over a Rayleigh fading
multiple access channel (MAC) without diversity was recently shown to be
optimum in the case of conditionally mutually independent and identically
distributed (i.i.d.) sensor decisions under specific conditions [1], [2]. Here,
we provide a twofold generalization, allowing sensors to be non identical on
one hand and introducing diversity on the other hand. Along with the
derivation, we provide also a general tool to verify optimality of the the
received energy test in scenarios with correlated sensor decisions. Finally, we
derive an analytical expression of the effect of the diversity on the
large-system performances, under both individual and total power constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0954</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0954</id><created>2012-10-02</created><authors><author><keyname>Qi</keyname><forenames>Guo-Jun</forenames></author><author><keyname>Aggarwal</keyname><forenames>Charu</forenames></author><author><keyname>Moulin</keyname><forenames>Pierre</forenames></author><author><keyname>Huang</keyname><forenames>Thomas</forenames></author></authors><title>Learning from Collective Intelligence in Groups</title><categories>cs.SI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collective intelligence, which aggregates the shared information from large
crowds, is often negatively impacted by unreliable information sources with the
low quality data. This becomes a barrier to the effective use of collective
intelligence in a variety of applications. In order to address this issue, we
propose a probabilistic model to jointly assess the reliability of sources and
find the true data. We observe that different sources are often not independent
of each other. Instead, sources are prone to be mutually influenced, which
makes them dependent when sharing information with each other. High dependency
between sources makes collective intelligence vulnerable to the overuse of
redundant (and possibly incorrect) information from the dependent sources.
Thus, we reveal the latent group structure among dependent sources, and
aggregate the information at the group level rather than from individual
sources directly. This can prevent the collective intelligence from being
inappropriately dominated by dependent sources. We will also explicitly reveal
the reliability of groups, and minimize the negative impacts of unreliable
groups. Experimental results on real-world data sets show the effectiveness of
the proposed approach with respect to existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0962</identifier>
 <datestamp>2013-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0962</id><created>2012-10-02</created><authors><author><keyname>Chandler</keyname><forenames>Dana</forenames></author><author><keyname>Kapelner</keyname><forenames>Adam</forenames></author></authors><title>Breaking Monotony with Meaning: Motivation in Crowdsourcing Markets</title><categories>stat.OT cs.HC</categories><comments>26 pages, 15 figures, 4 tables</comments><acm-class>G.3; J.4</acm-class><journal-ref>Journal of Economic Behavior &amp; Organization, 90, 2013, pp 123-133</journal-ref><doi>10.1016/j.jebo.2013.03.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct the first natural field experiment to explore the relationship
between the &quot;meaningfulness&quot; of a task and worker effort. We employed about
2,500 workers from Amazon's Mechanical Turk (MTurk), an online labor market, to
label medical images. Although given an identical task, we experimentally
manipulated how the task was framed. Subjects in the meaningful treatment were
told that they were labeling tumor cells in order to assist medical
researchers, subjects in the zero-context condition (the control group) were
not told the purpose of the task, and, in stark contrast, subjects in the
shredded treatment were not given context and were additionally told that their
work would be discarded. We found that when a task was framed more
meaningfully, workers were more likely to participate. We also found that the
meaningful treatment increased the quantity of output (with an insignificant
change in quality) while the shredded treatment decreased the quality of output
(with no change in quantity). We believe these results will generalize to other
short-term labor markets. Our study also discusses MTurk as an exciting
platform for running natural field experiments in economics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0965</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0965</id><created>2012-10-02</created><authors><author><keyname>Feng</keyname><forenames>Zhenxin</forenames></author><author><keyname>Chin</keyname><forenames>Kwan-Wu</forenames></author></authors><title>A Survey of Delay Tolerant Networks Routing Protocols</title><categories>cs.NI cs.ET</categories><comments>56 pages</comments><report-no>WTL-TechReport-1-12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in Micro-Electro-Mechanical Systems (MEMS) have revolutionized the
digital age to a point where animate and inanimate objects can be used as a
communication channel. In addition, the ubiquity of mobile phones with
increasing capabilities and ample resources means people are now effectively
mobile sensors that can be used to sense the environment as well as data
carriers. These objects, along with their devices, form a new kind of networks
that are characterized by frequent disconnections, resource constraints and
unpredictable or stochastic mobility patterns. A key underpinning in these
networks is routing or data dissemination protocols that are designed
specifically to handle the aforementioned characteristics. Therefore, there is
a need to review state-of-the-art routing protocols, categorize them, and
compare and contrast their approaches in terms of delivery rate, resource
consumption and end-to-end delay. To this end, this paper reviews 63 unicast,
multicast and coding-based routing protocols that are designed specifically to
run in delay tolerant or challenged networks. We provide an extensive
qualitative comparison of all protocols, highlight their experimental setup and
outline their deficiencies in terms of design and research methodology. Apart
from that, we review research that aims to exploit studies on social networks
and epidemiology in order to improve routing protocol performance. Lastly, we
provide a list of future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0974</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0974</id><created>2012-10-03</created><updated>2013-04-03</updated><authors><author><keyname>Selinger</keyname><forenames>Peter</forenames></author></authors><title>Quantum circuits of T-depth one</title><categories>quant-ph cs.ET</categories><comments>5 pages</comments><journal-ref>Phys. Rev. A 87, 042302 (2013)</journal-ref><doi>10.1103/PhysRevA.87.042302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a Clifford+T representation of the Toffoli gate of T-depth 1, using
four ancillas. More generally, we describe a class of circuits whose T-depth
can be reduced to 1 by using sufficiently many ancillas. We show that the cost
of adding an additional control to any controlled gate is at most 8 additional
T-gates, and T-depth 2. We also show that the circuit THT does not possess a
T-depth 1 representation with an arbitrary number of ancillas initialized to 0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.0999</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.0999</id><created>2012-10-03</created><authors><author><keyname>Palfray</keyname><forenames>Thomas</forenames><affiliation>LITIS</affiliation></author><author><keyname>H&#xe9;bert</keyname><forenames>David</forenames><affiliation>LITIS</affiliation></author><author><keyname>Nicolas</keyname><forenames>St&#xe9;phane</forenames><affiliation>LITIS</affiliation></author><author><keyname>Tranouez</keyname><forenames>Pierrick</forenames><affiliation>LITIS</affiliation></author><author><keyname>Paquet</keyname><forenames>Thierry</forenames><affiliation>LITIS</affiliation></author></authors><title>Logical segmentation for article extraction in digitized old newspapers</title><categories>cs.IR cs.CV cs.DL</categories><comments>ACM Document Engineering, France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Newspapers are documents made of news item and informative articles. They are
not meant to be red iteratively: the reader can pick his items in any order he
fancies. Ignoring this structural property, most digitized newspaper archives
only offer access by issue or at best by page to their content. We have built a
digitization workflow that automatically extracts newspaper articles from
images, which allows indexing and retrieval of information at the article
level. Our back-end system extracts the logical structure of the page to
produce the informative units: the articles. Each image is labelled at the
pixel level, through a machine learning based method, then the page logical
structure is constructed up from there by the detection of structuring entities
such as horizontal and vertical separators, titles and text lines. This logical
structure is stored in a METS wrapper associated to the ALTO file produced by
the system including the OCRed text. Our front-end system provides a web high
definition visualisation of images, textual indexing and retrieval facilities,
searching and reading at the article level. Articles transcriptions can be
collaboratively corrected, which as a consequence allows for better indexing.
We are currently testing our system on the archives of the Journal de Rouen,
one of France eldest local newspaper. These 250 years of publication amount to
300 000 pages of very variable image quality and layout complexity. Test year
1808 can be consulted at plair.univ-rouen.fr.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1013</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1013</id><created>2012-10-03</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>On the SCALE Algorithm for Multiuser Multicarrier Power Spectrum
  Management</title><categories>cs.IT math.IT</categories><comments>8 pages, 5 figures; IEEE Transactions on Signal Processing, vol. 60,
  no. 9, Sep. 2012</comments><doi>10.1109/TSP.2012.2199986</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the successive convex approximation for low complexity
(SCALE) algorithm, which was proposed to address the weighted sum rate (WSR)
maximized dynamic power spectrum management (DSM) problem for multiuser
multicarrier systems. To this end, we first revisit the algorithm, and then
present geometric interpretation and properties of the algorithm. A geometric
programming (GP) implementation approach is proposed and compared with the
low-complexity approach proposed previously. In particular, an analytical
method is proposed to set up the default lower-bound constraints added by a GP
solver. Finally, numerical experiments are used to illustrate the analysis and
compare the two implementation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1026</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1026</id><created>2012-10-03</created><updated>2012-12-26</updated><authors><author><keyname>Quang-Hung</keyname><forenames>Nguyen</forenames></author><author><keyname>Thoai</keyname><forenames>Nam</forenames></author><author><keyname>Son</keyname><forenames>Nguyen Thanh</forenames></author></authors><title>Performance Constraint and Power-Aware Allocation For User Requests In
  Virtual Computing Lab</title><categories>cs.DC</categories><comments>10 pages</comments><journal-ref>Journal of Science and Technology, Special on International
  Conference on Advanced Computing and Applications 49 4A (2011) 383-392</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cloud computing is driven by economies of scale. A cloud system uses
virtualization technology to provide cloud resources (e.g. CPU, memory) to
users in form of virtual machines. Virtual machine (VM), which is a sandbox for
user application, fits well in the education environment to provide
computational resources for teaching and research needs. In resource
management, they want to reduce costs in operations by reducing expensive cost
of electronic bill of large-scale data center system. A lease-based model is
suitable for our Virtual Computing Lab, in which users ask resources on a lease
of virtual machines. This paper proposes two host selection policies, named MAP
(minimum of active physical hosts) and MAP-H2L, and four algorithms solving the
lease scheduling problem. FF-MAP, FF-MAP-H2L algorithms meet a trade-off
between the energy consumption and Quality of Service (e.g. performance). The
simulation on 7-day workload, which converted from LLNL Atlas log, showed the
FF-MAP and FF-MAP-H2L algorithms reducing 7.24% and 7.42% energy consumption
than existing greedy mapping algorithm in the leasing scheduler Haizea. In
addition, we introduce a ratio \theta of consolidation in HalfPI-FF-MAP and
PI-FF-MAP algorithms, in which \theta is \pi/2 and \pi, and results on their
simulations show that energy consumption decreased by 34.87% and 63.12%
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1029</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1029</id><created>2012-10-03</created><authors><author><keyname>Sun</keyname><forenames>Guangling</forenames></author><author><keyname>Li</keyname><forenames>Guoqing</forenames></author><author><keyname>Yin</keyname><forenames>Jie</forenames></author></authors><title>Blurred Image Classification based on Adaptive Dictionary</title><categories>cs.CV</categories><comments>10 pages,2 figures</comments><journal-ref>The International Journal of Multimedia &amp; Its Applications.
  5(1):1-9, 2013</journal-ref><doi>10.5121/ijma.2013.5101</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Two types of framework for blurred image classification based on adaptive
dictionary are proposed. Given a blurred image, instead of image deblurring,
the semantic category of the image is determined by blur insensitive sparse
coefficients calculated depending on an adaptive dictionary. The dictionary is
adaptive to the Point Spread Function (PSF) estimated from input blurred image.
The PSF is assumed to be space invariant and inferred separately in one
framework or updated combining with sparse coefficients calculation in an
alternative and iterative algorithm in the other framework. The experiment has
evaluated three types of blur, naming defocus blur, simple motion blur and
camera shake blur. The experiment results confirm the effectiveness of the
proposed frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1033</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1033</id><created>2012-10-03</created><authors><author><keyname>Sun</keyname><forenames>Guangling</forenames></author><author><keyname>Li</keyname><forenames>Guoqing</forenames></author><author><keyname>Zhang</keyname><forenames>Xinpeng</forenames></author></authors><title>Robust Degraded Face Recognition Using Enhanced Local Frequency
  Descriptor and Multi-scale Competition</title><categories>cs.CV</categories><comments>7 pages,7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recognizing degraded faces from low resolution and blurred images are common
yet challenging task. Local Frequency Descriptor (LFD) has been proved to be
effective for this task yet it is extracted from a spatial neighborhood of a
pixel of a frequency plane independently regardless of correlations between
frequencies. In addition, it uses a fixed window size named single scale of
short-term Frequency transform (STFT). To explore the frequency correlations
and preserve low resolution and blur insensitive simultaneously, we propose
Enhanced LFD in which information in space and frequency is jointly utilized so
as to be more descriptive and discriminative than LFD. The multi-scale
competition strategy that extracts multiple descriptors corresponding to
multiple window sizes of STFT and take one corresponding to maximum confidence
as the final recognition result. The experiments conducted on Yale and FERET
databases demonstrate that promising results have been achieved by the proposed
Enhanced LFD and multi-scale competition strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1037</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1037</id><created>2012-10-03</created><authors><author><keyname>Wu</keyname><forenames>Huasen</forenames></author><author><keyname>Zhang</keyname><forenames>Youguang</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author></authors><title>Laxity-Based Opportunistic Scheduling with Flow-Level Dynamics and
  Deadlines</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures, and 1 table, main part of it is submitted to WCNC
  2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Many data applications in the next generation cellular networks, such as
content precaching and video progressive downloading, require flow-level
quality of service (QoS) guarantees. One such requirement is deadline, where
the transmission task needs to be completed before the application-specific
time. To minimize the number of uncompleted transmission tasks, we study
laxity-based scheduling policies in this paper. We propose a
Less-Laxity-Higher-Possible-Rate (L$^2$HPR) policy and prove its asymptotic
optimality in underloaded identical-deadline systems. The asymptotic optimality
of L$^2$HPR can be applied to estimate the schedulability of a system and
provide insights on the design of scheduling policies for general systems.
Based on it, we propose a framework and three heuristic policies for practical
systems. Simulation results demonstrate the asymptotic optimality of L$^2$HPR
and performance improvement of proposed policies over greedy policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1039</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1039</id><created>2012-10-03</created><authors><author><keyname>Ponge</keyname><forenames>Julien</forenames><affiliation>CITI</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CITI</affiliation></author></authors><title>JooFlux: Hijacking Java 7 InvokeDynamic To Support Live Code
  Modifications</title><categories>cs.OS cs.PL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Changing functional and non-functional software implementation at runtime is
useful and even sometimes critical both in development and production
environments. JooFlux is a JVM agent that allows both the dynamic replacement
of method implementations and the application of aspect advices. It works by
doing bytecode transformation to take advantage of the new invokedynamic
instruction added in Java SE 7 to help implementing dynamic languages for the
JVM. JooFlux can be managed using a JMX agent so as to operate dynamic
modifications at runtime, without resorting to a dedicated domain-specific
language. We compared JooFlux with existing AOP platforms and dynamic
languages. Results demonstrate that JooFlux performances are close to the Java
ones --- with most of the time a marginal overhead, and sometimes a gain ---
where AOP platforms and dynamic languages present significant overheads. This
paves the way for interesting future evolutions and applications of JooFlux.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1040</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1040</id><created>2012-10-03</created><authors><author><keyname>Christa</keyname><forenames>Sharon</forenames></author><author><keyname>Madhuri</keyname><forenames>K. Lakshmi</forenames></author><author><keyname>Suma</keyname><forenames>V.</forenames></author></authors><title>A Comparative Analysis of Data Mining Tools in Agent Based Systems</title><categories>cs.DB</categories><comments>6 Pages, 2 Figures, 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  World wide technological advancement has brought in a widespread change in
adoption and utilization of open source tools. Since, most of the organizations
across the globe deal with a large amount of data to be updated online and
transactions are made every second, managing, mining and processing this
dynamic data is very complex. Successful implementation of the data mining
technique requires a careful assessment of the various tools and algorithms
available to mining experts. This paper provides a comparative study of open
source data mining tools available to the professionals. Parameters influencing
the choice of apt tools in addition to the real time challenges are discussed.
However, it is well proven that agents aid in improving the performance of data
mining tools. This paper provides information on an agent-based framework for
data preprocessing with implementation details for the development of better
tool in the market. An integration of open source data mining tools with agent
simulation enable one to implement an effective data pre processing
architecture thereby providing robust capabilities of the application which can
be upgraded using a minimum of pre planning requirement from the application
developer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1048</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1048</id><created>2012-10-03</created><authors><author><keyname>Guimera</keyname><forenames>Roger</forenames></author><author><keyname>Llorente</keyname><forenames>Alejandro</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author><author><keyname>Sales-Pardo</keyname><forenames>Marta</forenames></author></authors><title>Predicting human preferences using the block structure of complex social
  networks</title><categories>physics.soc-ph cs.SI physics.data-an stat.ML</categories><journal-ref>PLOS ONE 7(9): e44620 (2012)</journal-ref><doi>10.1371/journal.pone.0044620</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With ever-increasing available data, predicting individuals' preferences and
helping them locate the most relevant information has become a pressing need.
Understanding and predicting preferences is also important from a fundamental
point of view, as part of what has been called a &quot;new&quot; computational social
science. Here, we propose a novel approach based on stochastic block models,
which have been developed by sociologists as plausible models of complex
networks of social interactions. Our model is in the spirit of predicting
individuals' preferences based on the preferences of others but, rather than
fitting a particular model, we rely on a Bayesian approach that samples over
the ensemble of all possible models. We show that our approach is considerably
more accurate than leading recommender algorithms, with major relative
improvements between 38% and 99% over industry-level algorithms. Besides, our
approach sheds light on decision-making processes by identifying groups of
individuals that have consistently similar preferences, and enabling the
analysis of the characteristics of those groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1091</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1091</id><created>2012-10-03</created><updated>2014-04-16</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>A Formula for the Capacity of the General Gel'fand-Pinsker Channel</title><categories>cs.IT math.IT</categories><comments>Accepted to the IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Gel'fand-Pinsker problem in which the channel and state are
general, i.e., possibly non-stationary, non-memoryless and non-ergodic. Using
the information spectrum method and a non-trivial modification of the piggyback
coding lemma by Wyner, we prove that the capacity can be expressed as an
optimization over the difference of a spectral inf- and a spectral sup-mutual
information rate. We consider various specializations including the case where
the channel and state are memoryless but not necessarily stationary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1095</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1095</id><created>2012-10-03</created><authors><author><keyname>Vezzi</keyname><forenames>Francesco</forenames></author><author><keyname>Narzisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Mishra</keyname><forenames>Bud</forenames></author></authors><title>Reevaluating Assembly Evaluations with Feature Response Curves: GAGE and
  Assemblathons</title><categories>q-bio.GN cs.DS</categories><comments>Submitted to PLoS One. Supplementary material available at
  http://www.nada.kth.se/~vezzi/publications/supplementary.pdf and
  http://cs.nyu.edu/mishra/PUBLICATIONS/12.supplementaryFRC.pdf</comments><msc-class>92-08</msc-class><doi>10.1371/journal.pone.0052210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In just the last decade, a multitude of bio-technologies and software
pipelines have emerged to revolutionize genomics. To further their central
goal, they aim to accelerate and improve the quality of de novo whole-genome
assembly starting from short DNA reads. However, the performance of each of
these tools is contingent on the length and quality of the sequencing data, the
structure and complexity of the genome sequence, and the resolution and quality
of long-range information. Furthermore, in the absence of any metric that
captures the most fundamental &quot;features&quot; of a high-quality assembly, there is
no obvious recipe for users to select the most desirable assembler/assembly.
International competitions such as Assemblathons or GAGE tried to identify the
best assembler(s) and their features. Some what circuitously, the only
available approach to gauge de novo assemblies and assemblers relies solely on
the availability of a high-quality fully assembled reference genome sequence.
Still worse, reference-guided evaluations are often both difficult to analyze,
leading to conclusions that are difficult to interpret. In this paper, we
circumvent many of these issues by relying upon a tool, dubbed FRCbam, which is
capable of evaluating de novo assemblies from the read-layouts even when no
reference exists. We extend the FRCurve approach to cases where lay-out
information may have been obscured, as is true in many deBruijn-graph-based
algorithms. As a by-product, FRCurve now expands its applicability to a much
wider class of assemblers -- thus, identifying higher-quality members of this
group, their inter-relations as well as sensitivity to carefully selected
features, with or without the support of a reference sequence or layout for the
reads. The paper concludes by reevaluating several recently conducted assembly
competitions and the datasets that have resulted from them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1097</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1097</id><created>2012-10-01</created><authors><author><keyname>Shang</keyname><forenames>Yun</forenames></author><author><keyname>Lu</keyname><forenames>Xian</forenames></author><author><keyname>Lu</keyname><forenames>Ruqian</forenames></author></authors><title>Turing machines based on unsharp quantum logic</title><categories>cs.LO cs.FL quant-ph</categories><comments>In Proceedings QPL 2011, arXiv:1210.0298</comments><proxy>EPTCS</proxy><acm-class>F.4.1;F.1.1</acm-class><journal-ref>EPTCS 95, 2012, pp. 251-261</journal-ref><doi>10.4204/EPTCS.95.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider Turing machines based on unsharp quantum logic.
For a lattice-ordered quantum multiple-valued (MV) algebra E, we introduce
E-valued non-deterministic Turing machines (ENTMs) and E-valued deterministic
Turing machines (EDTMs). We discuss different E-valued recursively enumerable
languages from width-first and depth-first recognition. We find that
width-first recognition is equal to or less than depth-first recognition in
general. The equivalence requires an underlying E value lattice to degenerate
into an MV algebra. We also study variants of ENTMs. ENTMs with a classical
initial state and ENTMs with a classical final state have the same power as
ENTMs with quantum initial and final states. In particular, the latter can be
simulated by ENTMs with classical transitions under a certain condition. Using
these findings, we prove that ENTMs are not equivalent to EDTMs and that ENTMs
are more powerful than EDTMs. This is a notable difference from the classical
Turing machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1100</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1100</id><created>2012-10-01</created><updated>2013-04-11</updated><authors><author><keyname>Zankl</keyname><forenames>Harald</forenames></author></authors><title>Confluence by Decreasing Diagrams -- Formalized</title><categories>cs.LO</categories><comments>17 pages; valley and conversion version; RTA 2013</comments><acm-class>F.3.1; F.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a formalization of decreasing diagrams in the theorem
prover Isabelle. It discusses mechanical proofs showing that any locally
decreasing abstract rewrite system is confluent. The valley and the conversion
version of decreasing diagrams are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1104</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1104</id><created>2012-10-03</created><authors><author><keyname>Ribes</keyname><forenames>Arturo</forenames></author><author><keyname>Cerquides</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Demiris</keyname><forenames>Yiannis</forenames></author><author><keyname>de M&#xe1;ntaras</keyname><forenames>Ram&#xf3;n L&#xf3;pez</forenames></author></authors><title>Sensory Anticipation of Optical Flow in Mobile Robotics</title><categories>cs.RO cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to anticipate dangerous events, like a collision, an agent needs to
make long-term predictions. However, those are challenging due to uncertainties
in internal and external variables and environment dynamics. A sensorimotor
model is acquired online by the mobile robot using a state-of-the-art method
that learns the optical flow distribution in images, both in space and time.
The learnt model is used to anticipate the optical flow up to a given time
horizon and to predict an imminent collision by using reinforcement learning.
We demonstrate that multi-modal predictions reduce to simpler distributions
once actions are taken into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1121</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1121</id><created>2012-10-03</created><authors><author><keyname>Balasubramanian</keyname><forenames>Krishnakumar</forenames></author><author><keyname>Yu</keyname><forenames>Kai</forenames></author><author><keyname>Lebanon</keyname><forenames>Guy</forenames></author></authors><title>Smooth Sparse Coding via Marginal Regression for Learning Sparse
  Representations</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and analyze a novel framework for learning sparse representations,
based on two statistical techniques: kernel smoothing and marginal regression.
The proposed approach provides a flexible framework for incorporating feature
similarity or temporal information present in data sets, via non-parametric
kernel smoothing. We provide generalization bounds for dictionary learning
using smooth sparse coding and show how the sample complexity depends on the L1
norm of kernel function used. Furthermore, we propose using marginal regression
for obtaining sparse codes, which significantly improves the speed and allows
one to scale to large dictionary sizes easily. We demonstrate the advantages of
the proposed approach, both in terms of accuracy and speed by extensive
experimentation on several real data sets. In addition, we demonstrate how the
proposed approach could be used for improving semi-supervised sparse coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1139</identifier>
 <datestamp>2013-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1139</id><created>2012-10-03</created><updated>2013-08-16</updated><authors><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Huang</keyname><forenames>Pengfei</forenames></author><author><keyname>Wang</keyname><forenames>Xudong</forenames></author></authors><title>Cross-Layer Scheduling in Multi-user System with Delay and Secrecy
  Constraints</title><categories>cs.IT math.IT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, physical layer security based approaches have drawn considerable
attentions and are envisaged to provide secure communications in the wireless
networks. However, most existing literatures only focus on the physical layer.
Thus, how to design an effective transmission scheme which also considers the
requirements from the upper layers is still an unsolved problem. We consider
such cross-layer resource allocation problem in the multi-user downlink
environment for both having instantaneous and partial eavesdropping channel
information scenarios. The problem is first formulated in a new security
framework. Then, the control scheme is designed to maximize the average
admission rate of the data, incorporating delay, power, and secrecy as
constraints, for both non-colluding and colluding eavesdropping cases in each
scenario. Performance analysis is given based on the stochastic optimization
theory and the simulations are carried out to validate the effectiveness of our
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1157</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1157</id><created>2012-10-03</created><authors><author><keyname>Hanlon</keyname><forenames>James</forenames></author><author><keyname>Hollis</keyname><forenames>Simon J.</forenames></author><author><keyname>May</keyname><forenames>David</forenames></author></authors><title>Scalable data abstractions for distributed parallel computations</title><categories>cs.PL cs.DC</categories><acm-class>D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to express a program as a hierarchical composition of parts is an
essential tool in managing the complexity of software and a key abstraction
this provides is to separate the representation of data from the computation.
Many current parallel programming models use a shared memory model to provide
data abstraction but this doesn't scale well with large numbers of cores due to
non-determinism and access latency. This paper proposes a simple programming
model that allows scalable parallel programs to be expressed with distributed
representations of data and it provides the programmer with the flexibility to
employ shared or distributed styles of data-parallelism where applicable. It is
capable of an efficient implementation, and with the provision of a small set
of primitive capabilities in the hardware, it can be compiled to operate
directly on the hardware, in the same way stack-based allocation operates for
subroutines in sequential machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1158</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1158</id><created>2012-10-03</created><updated>2015-11-14</updated><authors><author><keyname>Hanlon</keyname><forenames>James</forenames></author></authors><title>Emulating a large memory with a collection of small ones</title><categories>cs.AR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential computation is well understood but does not scale well with
current technology. Within the next decade, systems will contain large numbers
of processors with potentially thousands of processors per chip. Despite this,
many computational problems exhibit little or no parallelism and many existing
formulations are sequential. It is therefore essential that highly-parallel
architectures can support sequential computation by emulating large memories
with collections of smaller ones, thus supporting efficient execution of
sequential programs or sequential components of parallel programs.
  This paper demonstrates that a realistic parallel architecture with scalable
low-latency communications can execute large-memory sequential programs with a
factor of only 2 to 3 slowdown, when compared to a conventional sequential
architecture. This overhead seems an acceptable price to pay to be able to
switch between executing highly-parallel programs and sequential programs with
large memory requirements. Efficient emulation of large memories could
therefore facilitate a transition from sequential machines by allowing existing
programs to be compiled directly to a highly-parallel architecture and then for
their performance to be improved by exploiting parallelism in memory accesses
and computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1161</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1161</id><created>2012-10-03</created><authors><author><keyname>Papatheocharous</keyname><forenames>Efi</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Harris</forenames></author><author><keyname>Andreou</keyname><forenames>Andreas S.</forenames></author></authors><title>Feature Subset Selection for Software Cost Modelling and Estimation</title><categories>cs.SE cs.AI</categories><comments>Engineering Intelligent Systems Vol 18 (3/4) September/December 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection has been recently used in the area of software engineering
for improving the accuracy and robustness of software cost models. The idea
behind selecting the most informative subset of features from a pool of
available cost drivers stems from the hypothesis that reducing the
dimensionality of datasets will significantly minimise the complexity and time
required to reach to an estimation using a particular modelling technique. This
work investigates the appropriateness of attributes, obtained from empirical
project databases and aims to reduce the cost drivers used while preserving
performance. Finding suitable subset selections that may cater improved
predictions may be considered as a pre-processing step of a particular
technique employed for cost estimation (filter or wrapper) or an internal
(embedded) step to minimise the fitting error. This paper compares nine
relatively popular feature selection methods and uses the empirical values of
selected attributes recorded in the ISBSG and Desharnais datasets to estimate
software development effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1172</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1172</id><created>2012-10-03</created><authors><author><keyname>Albi</keyname><forenames>Giacomo</forenames></author><author><keyname>Pareschi</keyname><forenames>Lorenzo</forenames></author></authors><title>Modeling self-organized systems interacting with few individuals: from
  microscopic to macroscopic dynamics</title><categories>physics.bio-ph cs.SI physics.soc-ph q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In nature self-organized systems as flock of birds, school of fishes or herd
of sheeps have to deal with the presence of external agents such as predators
or leaders which modify their internal dynamic. Such situations take into
account a large number of individuals with their own social behavior which
interact with a few number of other individuals acting as external point source
forces. Starting from the microscopic description we derive the kinetic model
through a mean-field limit and finally the macroscopic system through a
suitable hydrodynamic limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1178</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1178</id><created>2012-10-03</created><authors><author><keyname>Badawy</keyname><forenames>Abdel-Hameed A.</forenames></author></authors><title>Students Perceptions of the Effectiveness of Discussion Boards What can
  we get from our students for a freebie point</title><categories>cs.CY</categories><comments>9 pages</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), Vol. 3, No. 9, 2012, 136-144</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate how the students think of their experience in a junior 300
level computer science course that uses blackboard as the underlying course
management system. The discussion boards in Blackboard are heavily used for
programming project support and to foster cooperation among students to answer
their questions and concerns. A survey is conducted through blackboard as a
voluntary quiz and the student who participated were given a participation
point for their effort. The results and the participation were very
interesting. We obtained statistics from the answers to the questions. The
students also have given us feedback in the form of comments to all questions
except for two only. The students have shown understanding, maturity and
willingness to participate in pedagogy-enhancing endeavors with the premise
that it might help their education and other people education as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1179</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1179</id><created>2012-10-01</created><authors><author><keyname>Wali</keyname><forenames>Bacem</forenames><affiliation>VISAGES</affiliation></author><author><keyname>Gibaud</keyname><forenames>Bernard</forenames><affiliation>VISAGES</affiliation></author></authors><title>Extending OWL-S for the Composition of Web Services Generated With a
  Legacy Application Wrapper</title><categories>cs.SE</categories><comments>ICIW 2012, The Seventh International Conference on Internet and Web
  Applications and Services, Stuttgart : Germany (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite numerous efforts by various developers, web service composition is
still a difficult problem to tackle. Lot of progressive research has been made
on the development of suitable standards. These researches help to alleviate
and overcome some of the web services composition issues. However, the legacy
application wrappers generate nonstandard WSDL which hinder the progress.
Indeed, in addition to their lack of semantics, WSDLs have sometimes different
shapes because they are adapted to circumvent some technical implementation
aspect. In this paper, we propose a method for the semi automatic composition
of web services in the context of the NeuroLOG project. In this project the
reuse of processing tools relies on a legacy application wrapper called jGASW.
The paper describes the extensions to OWL-S in order to introduce and enable
the composition of web services generated using the jGASW wrapper and also to
implement consistency checks regarding these services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1184</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1184</id><created>2012-10-03</created><authors><author><keyname>Simons</keyname><forenames>Christopher L.</forenames></author><author><keyname>Parmee</keyname><forenames>Ian C.</forenames></author></authors><title>Elegant Object-oriented Software Design via Interactive, Evolutionary
  Computation</title><categories>cs.SE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design is fundamental to software development but can be demanding to
perform. Thus to assist the software designer, evolutionary computing is being
increasingly applied using machine-based, quantitative fitness functions to
evolve software designs. However, in nature, elegance and symmetry play a
crucial role in the reproductive fitness of various organisms. In addition,
subjective evaluation has also been exploited in Interactive Evolutionary
Computation (IEC). Therefore to investigate the role of elegance and symmetry
in software design, four novel elegance measures are proposed based on the
evenness of distribution of design elements. In controlled experiments in a
dynamic interactive evolutionary computation environment, designers are
presented with visualizations of object-oriented software designs, which they
rank according to a subjective assessment of elegance. For three out of the
four elegance measures proposed, it is found that a significant correlation
exists between elegance values and reward elicited. These three elegance
measures assess the evenness of distribution of (a) attributes and methods
among classes, (b) external couples between classes, and (c) the ratio of
attributes to methods. It is concluded that symmetrical elegance is in some way
significant in software design, and that this can be exploited in dynamic,
multi-objective interactive evolutionary computation to produce elegant
software designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1185</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1185</id><created>2012-10-03</created><updated>2015-04-30</updated><authors><author><keyname>Azimdoost</keyname><forenames>Bita</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author><author><keyname>Sadjadpour</keyname><forenames>Hamid R.</forenames></author></authors><title>Scaling Laws of the Throughput Capacity and Latency in
  Information-Centric Networks</title><categories>cs.NI cs.DC</categories><comments>12 pages, 6 figures, This is the journal version of the paper
  presented in ITC25 under the name &quot;On the throughput capacity of
  information-centric networks&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless information-centric networks consider storage as one of the network
primitives, and propose to cache data within the network in order to improve
latency and reduce bandwidth consumption. We study the throughput capacity and
delay in an information-centric network when the data cached in each node has a
limited lifetime. The results show that with some fixed request and cache
expiration rates, the order of the data access time does not change with
network growth, and the maximum throughput order is inversely proportional to
the square root and logarithm of the network size $n$ in cases of grid and
random networks, respectively. Comparing these values with the corresponding
throughput and latency with no cache capability (throughput inversely
proportional to the network size, and latency of order $\sqrt{n}$ and
$\sqrt{\frac{n}{\log n}}$ in grid and random networks, respectively), we can
actually quantify the asymptotic advantage of caching. Moreover, we compare
these scaling laws for different content discovery mechanisms and illustrate
that not much gain is lost when a simple path search is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1190</identifier>
 <datestamp>2012-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1190</id><created>2012-10-03</created><authors><author><keyname>Kumar</keyname><forenames>Abhishek</forenames></author><author><keyname>Sindhwani</keyname><forenames>Vikas</forenames></author><author><keyname>Kambadur</keyname><forenames>Prabhanjan</forenames></author></authors><title>Fast Conical Hull Algorithms for Near-separable Non-negative Matrix
  Factorization</title><categories>stat.ML cs.LG</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The separability assumption (Donoho &amp; Stodden, 2003; Arora et al., 2012)
turns non-negative matrix factorization (NMF) into a tractable problem.
Recently, a new class of provably-correct NMF algorithms have emerged under
this assumption. In this paper, we reformulate the separable NMF problem as
that of finding the extreme rays of the conical hull of a finite set of
vectors. From this geometric perspective, we derive new separable NMF
algorithms that are highly scalable and empirically noise robust, and have
several other favorable properties in relation to existing methods. A parallel
implementation of our algorithm demonstrates high scalability on shared- and
distributed-memory machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1192</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1192</id><created>2012-10-03</created><updated>2014-02-11</updated><authors><author><keyname>Singh</keyname><forenames>Sukhpal</forenames></author></authors><title>Reduction of Blocking Artifacts In JPEG Compressed Image</title><categories>cs.GR cs.MM</categories><comments>10 Pages including 6 figures, Presented in the TECHNOVISION-10 in
  G.N.D.E.C., Ludhiana, Punjab, India</comments><report-no>idcs-110 10</report-no><msc-class>68U10</msc-class><acm-class>I.4.2</acm-class><journal-ref>Conference Proceedings of the National Level Technical Symposium
  on Emerging Trends in Technology, 9th-10th April, 2010, 234-243</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In JPEG (DCT based) compresses image data by representing the original image
with a small number of transform coefficients. It exploits the fact that for
typical images a large amount of signal energy is concentrated in a small
number of coefficients. The goal of DCT transform coding is to minimize the
number of retained transform coefficients while keeping distortion at an
acceptable level.In JPEG, it is done in 8X8 non overlapping blocks. It divides
an image into blocks of equal size and processes each block independently.
Block processing allows the coder to adapt to the local image statistics,
exploit the correlation present among neighboring image pixels, and to reduce
computational and storage requirements. One of the most degradation of the
block transform coding is the blocking artifact. These artifacts appear as a
regular pattern of visible block boundaries. This degradation is a direct
result of the coarse quantization of the coefficients and the independent
processing of the blocks which does not take into account the existing
correlations among adjacent block pixels. In this paper attempt is being made
to reduce the blocking artifact introduced by the Block DCT Transform in JPEG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1193</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1193</id><created>2012-10-03</created><updated>2014-04-04</updated><authors><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author></authors><title>Simple, Fast and Deterministic Gossip and Rumor Spreading</title><categories>cs.DS cs.DC math.CO</categories><doi>10.1137/1.9781611973105.51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study gossip algorithms for the rumor spreading problem which asks each
node to deliver a rumor to all nodes in an unknown network. Gossip algorithms
allow nodes only to call one neighbor per round and have recently attracted
attention as message efficient, simple and robust solutions to the rumor
spreading problem.
  Recently, non-uniform random gossip schemes were devised to allow efficient
rumor spreading in networks with bottlenecks. In particular, [Censor-Hillel et
al., STOC'12] gave an O(log^3 n) algorithm to solve the 1-local broadcast
problem in which each node wants to exchange rumors locally with its
1-neighborhood. By repeatedly applying this protocol one can solve the global
rumor spreading quickly for all networks with small diameter, independently of
the conductance.
  This and all prior gossip algorithms for the rumor spreading problem have
been inherently randomized in their design and analysis. This resulted in a
parallel research direction trying to reduce and determine the amount of
randomness needed for efficient rumor spreading. This has been done via lower
bounds for restricted models and by designing gossip algorithms with a reduced
need for randomness. The general intuition and consensus of these results has
been that randomization plays a important role in effectively spreading rumors.
  In this paper we improves over this state of the art in several ways by
presenting a deterministic gossip algorithm that solves the the k-local
broadcast problem in 2(k+log n)log n rounds. Besides being the first efficient
deterministic solution to the rumor spreading problem this algorithm is
interesting in many aspects: It is simpler, more natural, more robust and
faster than its randomized pendant and guarantees success with certainty
instead of with high probability. Its analysis is furthermore simple,
self-contained and fundamentally different from prior works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1200</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1200</id><created>2012-10-03</created><updated>2012-10-08</updated><authors><author><keyname>Bezem</keyname><forenames>Marc</forenames><affiliation>Department of Informatics, University of Bergen</affiliation></author><author><keyname>Nakata</keyname><forenames>Keiko</forenames><affiliation>Institute of Cybernetics at Tallinn University of Technology</affiliation></author><author><keyname>Uustalu</keyname><forenames>Tarmo</forenames><affiliation>Institute of Cybernetics at Tallinn University of Technology</affiliation></author></authors><title>On streams that are finitely red</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (October
  10, 2012) lmcs:1048</journal-ref><doi>10.2168/LMCS-8(4:4)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixing induction and coinduction, we study alternative definitions of streams
being finitely red. We organize our definitions into a hierarchy including also
some well-known alternatives in intuitionistic analysis. The hierarchy
collapses classically, but is intuitionistically of strictly decreasing
strength. We characterize the differences in strength in a precise way by weak
instances of the Law of Excluded Middle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1207</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1207</id><created>2012-10-04</created><updated>2013-05-05</updated><authors><author><keyname>Koppula</keyname><forenames>Hema Swetha</forenames></author><author><keyname>Gupta</keyname><forenames>Rudhir</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Learning Human Activities and Object Affordances from RGB-D Videos</title><categories>cs.RO cs.AI cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1208.0967</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding human activities and object affordances are two very important
skills, especially for personal robots which operate in human environments. In
this work, we consider the problem of extracting a descriptive labeling of the
sequence of sub-activities being performed by a human, and more importantly, of
their interactions with the objects in the form of associated affordances.
Given a RGB-D video, we jointly model the human activities and object
affordances as a Markov random field where the nodes represent objects and
sub-activities, and the edges represent the relationships between object
affordances, their relations with sub-activities, and their evolution over
time. We formulate the learning problem using a structural support vector
machine (SSVM) approach, where labelings over various alternate temporal
segmentations are considered as latent variables. We tested our method on a
challenging dataset comprising 120 activity videos collected from 4 subjects,
and obtained an accuracy of 79.4% for affordance, 63.4% for sub-activity and
75.0% for high-level activity labeling. We then demonstrate the use of such
descriptive labeling in performing assistive tasks by a PR2 robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1223</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1223</id><created>2012-10-03</created><authors><author><keyname>Neswold</keyname><forenames>R.</forenames><affiliation>Fermilab</affiliation></author><author><keyname>King</keyname><forenames>C.</forenames><affiliation>Fermilab</affiliation></author></authors><title>Further developments in generating type-safe messaging</title><categories>physics.ins-det cs.SE</categories><comments>4 pp. 13th International Conference on Accelerator and Large
  Experimental Physics Control Systems (ICALEPCS 2011). 10-14 Oct 2011.
  Grenoble, France</comments><proxy>Fermilab Proxy</proxy><report-no>FERMILAB-CONF-11-594-AD</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At ICALEPCS '09, we introduced a source code generator that allows processes
to communicate safely using data types native to each host language. In this
paper, we discuss further development that has occurred since the conference in
Kobe, Japan, including the addition of three more client languages, an
optimization in network packet size and the addition of a new protocol data
type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1230</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1230</id><created>2012-10-03</created><authors><author><keyname>Badawy</keyname><forenames>AbdelHameed A.</forenames></author><author><keyname>Hugue</keyname><forenames>Michelle M.</forenames></author></authors><title>Evaluating Discussion Boards on BlackBoard as a Collaborative Learning
  Tool A Students Survey and Reflections</title><categories>cs.CV cs.CY</categories><comments>5 pages, 12 tables, appears in proceedings of the IEEE International
  Conference on Education and Management Technology (ICEMT 2010), Cairo, Egypt
  November 2010. arXiv admin note: substantial text overlap with
  arXiv:1210.1178</comments><journal-ref>In proceedings of the IEEE International Conference on Education
  and Management Technology (ICEMT 2010), pages 79 - 82, Cairo, Egypt November
  2010</journal-ref><doi>10.1109/ICEMT.2010.5657540</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate how the students think of their experience in a
junior level course that has a blackboard course presence where the students
use the discussion boards extensively. A survey is set up through blackboard as
a voluntary quiz and the student who participated were given a freebie point.
The results and the participation were very interesting in terms of the
feedback we got via open comments from the students as well as the statistics
we gathered from the answers to the questions. The students have shown
understanding and willingness to participate in pedagogy-enhancing endeavors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1258</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1258</id><created>2012-10-03</created><authors><author><keyname>Ishteva</keyname><forenames>Mariya</forenames></author><author><keyname>Park</keyname><forenames>Haesun</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>Unfolding Latent Tree Structures using 4th Order Tensors</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering the latent structure from many observed variables is an important
yet challenging learning task. Existing approaches for discovering latent
structures often require the unknown number of hidden states as an input. In
this paper, we propose a quartet based approach which is \emph{agnostic} to
this number. The key contribution is a novel rank characterization of the
tensor associated with the marginal distribution of a quartet. This
characterization allows us to design a \emph{nuclear norm} based test for
resolving quartet relations. We then use the quartet test as a subroutine in a
divide-and-conquer algorithm for recovering the latent tree structure. Under
mild conditions, the algorithm is consistent and its error probability decays
exponentially with increasing sample size. We demonstrate that the proposed
approach compares favorably to alternatives. In a real world stock dataset, it
also discovers meaningful groupings of variables, and produces a model that
fits the data better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1266</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1266</id><created>2012-10-03</created><updated>2013-09-18</updated><authors><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author><author><keyname>Ahmed</keyname><forenames>Nasir U.</forenames></author></authors><title>Nonanticipative Rate Distortion Function and Relations to Filtering
  Theory</title><categories>cs.IT math.IT math.OC</categories><comments>41 pages,5 figures, Provisionally Accepted Version of the manuscript
  submitted to IEEE Transactions on Automatic Control; Part of this work was
  presented in 20th International Symposium on Mathematical Theory of Networks
  and Systems (MTNS '12)</comments><msc-class>28A33, 28A35, 60B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relation between nonanticipative Rate Distortion Function (RDF) and
filtering theory is discussed on abstract spaces. The relation is established
by imposing a realizability constraint on the reconstruction conditional
distribution of the classical RDF. Existence of the extremum solution of the
nonanticipative RDF is shown using weak$^*$-convergence on appropriate
topology. The extremum reconstruction conditional distribution is derived in
closed form, for the case of stationary processes. The realization of the
reconstruction conditional distribution which achieves the infimum of the
nonanticipative RDF is described. Finally, an example is presented to
illustrate the concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1280</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1280</id><created>2012-10-03</created><authors><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author></authors><title>A Pseudorandom Generator for Polynomial Threshold Functions of Gaussian
  with Subpolynomial Seed Length</title><categories>cs.CC math.PR</categories><msc-class>60G15</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a pseudorandom generator that fools degree-$d$ polynomial
threshold functions in $n$ variables with respect to the Gaussian distribution
and has seed length $O_{c,d}(\log(n) \epsilon^{-c})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1283</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1283</id><created>2012-10-03</created><authors><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author></authors><title>The Correct Exponent for the Gotsman-Linial Conjecture</title><categories>math.CO cs.CC math.PR</categories><msc-class>60B10, 68R05,</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a new bound on the average sensitivity of polynomial threshold
functions. In particular we show that a polynomial threshold function of degree
$d$ in at most $n$ variables has average sensitivity at most
$\sqrt{n}(\log(n))^{O(d\log(d))}2^{O(d^2\log(d)}$. For fixed $d$ the exponent
in terms of $n$ in this bound is known to be optimal. This bound makes
significant progress towards the Gotsman-Linial Conjecture which would put the
correct bound at $\Theta(d\sqrt{n})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1290</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1290</id><created>2012-10-04</created><updated>2013-05-17</updated><authors><author><keyname>Kobayashi</keyname><forenames>Hirotada</forenames></author><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author></authors><title>Stronger Methods of Making Quantum Interactive Proofs Perfectly Complete</title><categories>quant-ph cs.CC</categories><comments>41 pages; v2: soundness parameters improved, correction of a minor
  error in Lemma 23, and removal of the sentences claiming that our techniques
  are quantumly nonrelativizing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents stronger methods of achieving perfect completeness in
quantum interactive proofs. First, it is proved that any problem in QMA has a
two-message quantum interactive proof system of perfect completeness with
constant soundness error, where the verifier has only to send a constant number
of halves of EPR pairs. This in particular implies that the class QMA is
necessarily included by the class QIP_1(2) of problems having two-message
quantum interactive proofs of perfect completeness, which gives the first
nontrivial upper bound for QMA in terms of quantum interactive proofs. It is
also proved that any problem having an $m$-message quantum interactive proof
system necessarily has an $(m+1)$-message quantum interactive proof system of
perfect completeness. This improves the previous result due to Kitaev and
Watrous, where the resulting system of perfect completeness requires $m+2$
messages if not using the parallelization result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1291</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1291</id><created>2012-10-04</created><authors><author><keyname>Bhujang</keyname><forenames>Raghavi K</forenames></author><author><keyname>V</keyname><forenames>Suma.</forenames></author></authors><title>Graphical Visualization of Risk Assessment for Effective Risk Management
  during Software Development Process</title><categories>cs.SE</categories><comments>5 Pages, 5 Figures, 3 Tables</comments><journal-ref>International Joint Conference on Emerging Intelligent Sustainable
  Technologies, EISTCON-2012,Volume - 3, 3rd and 4th May 2012,
  ISBN:978-93-81693-76-6</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Success of any IT industry depends on the success rate of their projects,
which in turn depends on several factors such as cost, time, and availability
of resources. These factors formulate the risk areas, which needs to be
addressed in a proactive way. The rudimentary objective of risk management is
to circumvent the possibility of their occurrence by identifying the risks,
preparing the contingency plans and mitigation plans in order to reduce the
consequences of the risks. Hence, effective risk management becomes one of the
imperative challenges in any organization, which if deemed in an apt way
assures the continued sustainability of the organization in the high-end
competitive environment. This paper provides visualization of risk assessment
through a graphical model. Further, the matrix representation of the risk
assessment aids the project personnel to identify all the risks, comprehend
their frequency and probability of their occurrence. In addition, the graphical
model enables one to analyze the impact of identified risks and henceforth to
assign their priorities. This mode of representation of risk assessment factors
helps the organization in accurate prediction of success rate of the project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1300</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1300</id><created>2012-10-04</created><authors><author><keyname>Nizam</keyname><forenames>Ahmed Mehedi</forenames></author><author><keyname>Adnan</keyname><forenames>Md. Nasim</forenames></author><author><keyname>Islam</keyname><forenames>Md. Rashedul</forenames></author><author><keyname>Kabir</keyname><forenames>Mohammad Akbar</forenames></author></authors><title>Properties of Stochastic Kronecker Graph</title><categories>cs.SI cs.DM</categories><comments>5 pages</comments><msc-class>68</msc-class><acm-class>G.2.2</acm-class><journal-ref>IJCSI Volume 9, Issue 4, July 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic Kronecker Graph model can generate large random graph that
closely resembles many real world networks. For example, the output graph has a
heavy-tailed degree distribution, has a (low) diameter that effectively remains
constant over time and obeys the so-called densification power law [1]. Aside
from this list of very important graph properties, one may ask for some
additional information about the output graph: What will be the expected number
of isolated vertices? How many edges, self loops are there in the graph? What
will be the expected number of triangles in a random realization? Here we try
to answer the above questions. In the first phase, we bound the expected values
of the aforementioned features from above. Next we establish the sufficient
conditions to generate stochastic Kronecker graph with a wide range of
interesting properties. Finally we show two phase transitions for the
appearance of edges and self loops in stochastic Kronecker graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1316</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1316</id><created>2012-10-04</created><updated>2013-03-30</updated><authors><author><keyname>Peng</keyname><forenames>Xi</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author><author><keyname>Tan</keyname><forenames>Kok Kiong</forenames></author></authors><title>Learning Locality-Constrained Collaborative Representation for Face
  Recognition</title><categories>cs.CV</categories><comments>16 pages, v2</comments><journal-ref>Pattern Recognition, 47 (9), 2794-2806, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model of low-dimensional manifold and sparse representation are two
well-known concise models that suggest each data can be described by a few
characteristics. Manifold learning is usually investigated for dimension
reduction by preserving some expected local geometric structures from the
original space to a low-dimensional one. The structures are generally
determined by using pairwise distance, e.g., Euclidean distance. Alternatively,
sparse representation denotes a data point as a linear combination of the
points from the same subspace. In practical applications, however, the nearby
points in terms of pairwise distance may not belong to the same subspace, and
vice versa. Consequently, it is interesting and important to explore how to get
a better representation by integrating these two models together. To this end,
this paper proposes a novel coding algorithm, called Locality-Constrained
Collaborative Representation (LCCR), which improves the robustness and
discrimination of data representation by introducing a kind of local
consistency. The locality term derives from a biologic observation that the
similar inputs have similar code. The objective function of LCCR has an
analytical solution, and it does not involve local minima. The empirical
studies based on four public facial databases, ORL, AR, Extended Yale B, and
Multiple PIE, show that LCCR is promising in recognizing human faces from
frontal views with varying expression and illumination, as well as various
corruptions and occlusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1317</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1317</id><created>2012-10-04</created><authors><author><keyname>Nguyen</keyname><forenames>Phong</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Hilario</keyname><forenames>Melanie</forenames></author><author><keyname>Kalousis</keyname><forenames>Alexandros</forenames></author></authors><title>Learning Heterogeneous Similarity Measures for Hybrid-Recommendations in
  Meta-Mining</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of meta-mining has appeared recently and extends the traditional
meta-learning in two ways. First it does not learn meta-models that provide
support only for the learning algorithm selection task but ones that support
the whole data-mining process. In addition it abandons the so called black-box
approach to algorithm description followed in meta-learning. Now in addition to
the datasets, algorithms also have descriptors, workflows as well. For the
latter two these descriptions are semantic, describing properties of the
algorithms. With the availability of descriptors both for datasets and data
mining workflows the traditional modelling techniques followed in
meta-learning, typically based on classification and regression algorithms, are
no longer appropriate. Instead we are faced with a problem the nature of which
is much more similar to the problems that appear in recommendation systems. The
most important meta-mining requirements are that suggestions should use only
datasets and workflows descriptors and the cold-start problem, e.g. providing
workflow suggestions for new datasets.
  In this paper we take a different view on the meta-mining modelling problem
and treat it as a recommender problem. In order to account for the meta-mining
specificities we derive a novel metric-based-learning recommender approach. Our
method learns two homogeneous metrics, one in the dataset and one in the
workflow space, and a heterogeneous one in the dataset-workflow space. All
learned metrics reflect similarities established from the dataset-workflow
preference matrix. We demonstrate our method on meta-mining over biological
(microarray datasets) problems. The application of our method is not limited to
the meta-mining problem, its formulations is general enough so that it can be
applied on problems with similar requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1326</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1326</id><created>2012-10-04</created><authors><author><keyname>Firooz</keyname><forenames>Mohammad H.</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Roy</keyname><forenames>Sumit</forenames></author><author><keyname>Liu</keyname><forenames>Hui</forenames></author></authors><title>Wireless Network Coding via Modified 802.11 MAC/PHY: Design and
  Implementation on SDR</title><categories>cs.IT cs.NI cs.PF math.IT</categories><doi>10.1109/JSAC.2013.130823</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding (NC), in principle, is a Layer-3 innovation that improves
network throughput in wired networks for multicast/broadcast scenarios. Due to
the fundamental differences between wired and wireless networks, extending NC
to wireless networks generates several new and significant practical
challenges. Two-way information exchange (both symmetric and asymmetric).
Network coding (NC), in principle, is a Layer-3 innovation that improves
network throughput in wired networks for multicast/broadcast scenarios. Due to
the fundamental differences between wired and wireless networks, extending NC
to wireless networks generates several new and significant practical
challenges. Two-way information exchange (both symmetric and asymmetric)
between a pair of 802.11 sources/sinks using an intermediate relay node is a
canonical scenario for evaluating the effectiveness of Wireless Network Coding
(WNC) in a practical setting. Our primary objective in this work is to suggest
pragmatic and novel modifications at the MAC and PHY layers of the 802.11
protocol stack on a Software Radio (SORA) platform to support WNC and obtain
achievable throughput estimates via lab-scale experiments. Our results show
that network coding (at the MAC or PHY layer) increases system
throughput-typically by 20-30%%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1356</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1356</id><created>2012-10-04</created><updated>2013-03-01</updated><authors><author><keyname>Wieland</keyname><forenames>Stefan</forenames></author><author><keyname>Parisi</keyname><forenames>Andrea</forenames></author><author><keyname>Nunes</keyname><forenames>Ana</forenames></author></authors><title>Detecting and Describing Dynamic Equilibria in Adaptive Networks</title><categories>nlin.AO cs.SI physics.soc-ph q-bio.PE</categories><comments>14 pages, 10 figures; typo in the third of Eqs. (1) corrected</comments><journal-ref>EPJ-ST 212, 99-113 (2012)</journal-ref><doi>10.1140/epjst/e2012-01656-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review modeling attempts for the paradigmatic contact process (or SIS
model) on adaptive networks. Elaborating on one particular proposed mechanism
of topology change (rewiring) and its mean field analysis, we obtain a
coarse-grained view of coevolving network topology in the stationary active
phase of the system. Introducing an alternative framework applicable to a wide
class of adaptive networks, active stationary states are detected, and an
extended description of the resulting steady-state statistics is given for
three different rewiring schemes. We find that slight modifications of the
standard rewiring rule can result in either minuscule or drastic change of
steady-state network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1357</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1357</id><created>2012-10-04</created><updated>2013-03-23</updated><authors><author><keyname>Qin</keyname><forenames>Jun</forenames></author><author><keyname>Wu</keyname><forenames>Hongrun</forenames></author><author><keyname>Tong</keyname><forenames>Xiaonian</forenames></author><author><keyname>Zheng</keyname><forenames>Bojin</forenames></author></authors><title>A quantitative method for determining the robustness of complex networks</title><categories>cs.SI nlin.AO physics.soc-ph</categories><journal-ref>Physica D 2013 253 85--90</journal-ref><doi>10.1016/j.physd.2013.03.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most current studies estimate the invulnerability of complex networks using a
qualitative method that analyzes the inaccurate decay rate of network
efficiency. This method results in confusion over the invulnerability of
various types of complex networks. By normalizing network efficiency and
defining a baseline, this paper defines the invulnerability index as the
integral of the difference between the normalized network efficiency curve and
the baseline. This quantitative method seeks to establish a benchmark for the
robustness and fragility of networks and to measure network invulnerability
under both edge and node attacks. To validate the reliability of the proposed
method, three small-world networks were selected as test beds. The simulation
results indicate that the proposed invulnerability index can effectively and
accurately quantify network resilience. The index should provide a valuable
reference for determining network invulnerability in future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1390</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1390</id><created>2012-10-04</created><authors><author><keyname>Biernacki</keyname><forenames>Arkadiusz</forenames></author></authors><title>Methods of QoS improvement for P2P IPTV based on traffic modelling</title><categories>cs.NI</categories><journal-ref>Complex, Intelligent and Software Intensive Systems (CISIS), 2010
  International Conference on, pp. 445-450</journal-ref><doi>10.1109/CISIS.2010.173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last years many technological advances were introduced in Internet
television to meet user needs and expectations. However due to an overwhelming
bandwidth requirements traditional IP-based television service based on simple
client-server approach remains restricted to small group of clients. In such
situation the use of the peer-to-peer overlay paradigm to deliver live
television on the Internet is gaining increasing attention. Unfortunately the
current Internet infrastructure provides only best effort services for this
kind of applications and do not offer quality of service.
  This paper is a research proposition which presents potential solutions for
efficient IPTV streaming over P2P networks. We assume that the solutions will
not directly modify existing P2P IPTV protocols but rather will be dedicated
for a network engineer or an Internet service provider which will be able to
introduce and configure the proposed mechanisms in network routers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1394</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1394</id><created>2012-10-04</created><authors><author><keyname>Schi&#xf6;berg</keyname><forenames>Doris</forenames></author><author><keyname>Schneider</keyname><forenames>Fabian</forenames></author><author><keyname>Tredan</keyname><forenames>Gilles</forenames></author><author><keyname>Uhlig</keyname><forenames>Steve</forenames></author><author><keyname>Feldmann</keyname><forenames>Anja</forenames></author></authors><title>Revisiting Content Availability in Distributed Online Social Networks</title><categories>cs.NI cs.SI</categories><comments>11pages, 12 figures; Technical report at TU Berlin, Department of
  Electrical Engineering and Computer Science (ISSN 1436-9915)</comments><report-no>TU Berlin/EECS TR No. 2012 - 05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Social Networks (OSN) are among the most popular applications in
today's Internet. Decentralized online social networks (DOSNs), a special class
of OSNs, promise better privacy and autonomy than traditional centralized OSNs.
However, ensuring availability of content when the content owner is not online
remains a major challenge. In this paper, we rely on the structure of the
social graphs underlying DOSN for replication. In particular, we propose that
friends, who are anyhow interested in the content, are used to replicate the
users content. We study the availability of such natural replication schemes
via both theoretical analysis as well as simulations based on data from OSN
users. We find that the availability of the content increases drastically when
compared to the online time of the user, e. g., by a factor of more than 2 for
90% of the users. Thus, with these simple schemes we provide a baseline for any
more complicated content replication scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1429</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1429</id><created>2012-10-04</created><updated>2012-10-25</updated><authors><author><keyname>D&#x142;otko</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Wagner</keyname><forenames>Hubert</forenames></author></authors><title>Computing homology and persistent homology using iterated Morse
  decomposition</title><categories>math.AT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new approach to computing homology (with field
coefficients) and persistent homology. We use concepts from discrete Morse
theory, to provide an algorithm which can be expressed solely in terms of
simple graph theoretical operations. We use iterated Morse decomposition, which
allows us to sidetrack many problems related to the standard discrete Morse
theory. In particular, this approach is provably correct in any dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1433</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1433</id><created>2012-10-04</created><authors><author><keyname>Bilkova</keyname><forenames>Marta</forenames></author><author><keyname>Kurz</keyname><forenames>Alexander</forenames></author><author><keyname>Petrisan</keyname><forenames>Daniela</forenames></author><author><keyname>Velebil</keyname><forenames>Jiri</forenames></author></authors><title>Relation Liftings on Preorders and Posets</title><categories>cs.LO math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The category Rel(Set) of sets and relations can be described as a category of
spans and as the Kleisli category for the powerset monad. A set-functor can be
lifted to a functor on Rel(Set) iff it preserves weak pullbacks. We show that
these results extend to the enriched setting, if we replace sets by posets or
preorders. Preservation of weak pullbacks becomes preservation of exact lax
squares. As an application we present Moss's coalgebraic over posets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1441</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1441</id><created>2012-10-04</created><authors><author><keyname>Chen</keyname><forenames>Duanbing</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Adaptive social recommendation in a multiple category landscape</title><categories>physics.soc-ph cs.IR</categories><comments>7 pages, 5 figures</comments><journal-ref>Eur. Phys. J. B. 86, 61 (2013)</journal-ref><doi>10.1140/epjb/e2012-30899-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People in the Internet era have to cope with the information overload,
striving to find what they are interested in, and usually face this situation
by following a limited number of sources or friends that best match their
interests. A recent line of research, namely adaptive social recommendation,
has therefore emerged to optimize the information propagation in social
networks and provide users with personalized recommendations. Validation of
these methods by agent-based simulations often assumes that the tastes of users
and can be represented by binary vectors, with entries denoting users'
preferences. In this work we introduce a more realistic assumption that users'
tastes are modeled by multiple vectors. We show that within this framework the
social recommendation process has a poor outcome. Accordingly, we design novel
measures of users' taste similarity that can substantially improve the
precision of the recommender system. Finally, we discuss the issue of enhancing
the recommendations' diversity while preserving their accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1451</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1451</id><created>2012-10-04</created><authors><author><keyname>Grenet</keyname><forenames>Bruno</forenames></author><author><keyname>Koiran</keyname><forenames>Pascal</forenames></author><author><keyname>Portier</keyname><forenames>Natacha</forenames></author></authors><title>On the Complexity of the Multivariate Resultant</title><categories>cs.CC cs.SC</categories><comments>25 pages. arXiv admin note: substantial text overlap with
  arXiv:0912.2607</comments><journal-ref>Journal of Complexity, 29(2), pp 142-157, 2013</journal-ref><doi>10.1016/j.jco.2012.10.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multivariate resultant is a fundamental tool of computational algebraic
geometry. It can in particular be used to decide whether a system of n
homogeneous equations in n variables is satisfiable (the resultant is a
polynomial in the system's coefficients which vanishes if and only if the
system is satisfiable). In this paper, we investigate the complexity of
computing the multivariate resultant.
  First, we study the complexity of testing the multivariate resultant for
zero. Our main result is that this problem is NP-hard under deterministic
reductions in any characteristic, for systems of low-degree polynomials with
coefficients in the ground field (rather than in an extension). In
characteristic zero, we observe that this problem is in the Arthur-Merlin class
AM if the generalized Riemann hypothesis holds true, while the best known upper
bound in positive characteristic remains PSPACE.
  Second, we study the classical algorithms to compute the resultant. They
usually rely on the computation of the determinant of an exponential-size
matrix, known as Macaulay matrix. We show that this matrix belongs to a class
of succinctly representable matrices, for which testing the determinant for
zero is proved PSPACE-complete. This means that improving Canny's PSPACE upper
bound requires either to look at the fine structure of the Macaulay matrix to
find an ad hoc algorithm for computing its determinant, or to use altogether
different techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1456</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1456</id><created>2012-10-04</created><authors><author><keyname>Goel</keyname><forenames>Gagan</forenames></author><author><keyname>Mirrokni</keyname><forenames>Vahab</forenames></author><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author></authors><title>Clinching Auctions with Online Supply</title><categories>cs.GT</categories><comments>Accepted to SODA'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auctions for perishable goods such as internet ad inventory need to make
real-time allocation and pricing decisions as the supply of the good arrives in
an online manner, without knowing the entire supply in advance. These
allocation and pricing decisions get complicated when buyers have some global
constraints. In this work, we consider a multi-unit model where buyers have
global {\em budget} constraints, and the supply arrives in an online manner.
Our main contribution is to show that for this setting there is an
individually-rational, incentive-compatible and Pareto-optimal auction that
allocates these units and calculates prices on the fly, without knowledge of
the total supply. We do so by showing that the Adaptive Clinching Auction
satisfies a {\em supply-monotonicity} property.
  We also analyze and discuss, using examples, how the insights gained by the
allocation and payment rule can be applied to design better ad allocation
heuristics in practice. Finally, while our main technical result concerns
multi-unit supply, we propose a formal model of online supply that captures
scenarios beyond multi-unit supply and has applications to sponsored search. We
conjecture that our results for multi-unit auctions can be extended to these
more general models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1460</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1460</id><created>2012-09-24</created><updated>2013-09-17</updated><authors><author><keyname>Ericson</keyname><forenames>Josh</forenames></author><author><keyname>Poggi-Corradini</keyname><forenames>Pietro</forenames></author><author><keyname>Zhang</keyname><forenames>Hainan</forenames></author></authors><title>Effective resistance on graphs and the Epidemic quasimetric</title><categories>math.CO cs.SI math.CV q-bio.PE q-bio.QM</categories><comments>Typos corrected, to appear in Involve</comments><msc-class>31A15, 05C82</msc-class><journal-ref>Involve 7 (2014) 97-124</journal-ref><doi>10.2140/involve.2014.7.97</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the epidemic quasimetric on graphs and study its behavior with
respect to clustering techniques. In particular we compare its behavior to
known objects such as the graph distance, effective resistance, and modulus of
path families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1461</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1461</id><created>2012-10-04</created><authors><author><keyname>Wang</keyname><forenames>Shusen</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author></authors><title>A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and
  Tighter Bound</title><categories>cs.LG cs.DM stat.ML</categories><comments>accepted by NIPS 2012</comments><journal-ref>Shusen Wang and Zhihua Zhang. A Scalable CUR Matrix Decomposition
  Algorithm: Lower Time Complexity and Tighter Bound. In Advances in Neural
  Information Processing Systems 25, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CUR matrix decomposition is an important extension of Nystr\&quot;{o}m
approximation to a general matrix. It approximates any data matrix in terms of
a small number of its columns and rows. In this paper we propose a novel
randomized CUR algorithm with an expected relative-error bound. The proposed
algorithm has the advantages over the existing relative-error CUR algorithms
that it possesses tighter theoretical bound and lower time complexity, and that
it can avoid maintaining the whole data matrix in main memory. Finally,
experiments on several real-world datasets demonstrate significant improvement
over the existing relative-error algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1464</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1464</id><created>2012-10-04</created><authors><author><keyname>Pahlajani</keyname><forenames>Chetan D.</forenames></author><author><keyname>Poulakakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Tanner</keyname><forenames>Herbert G.</forenames></author></authors><title>Networked Decision Making for Poisson Processes: Application to nuclear
  detection</title><categories>math.PR cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a detection problem where several spatially distributed
sensors independently observe a time-inhomogeneous stochastic process. The task
is to decide between two hypotheses regarding the statistics of the observed
process at the end of a fixed time interval. In the proposed method, each of
the sensors transmits once to a fusion center a locally processed summary of
its information in the form of a likelihood ratio. The fusion center then
combines these messages to arrive at an optimal decision in the Neyman-Pearson
framework. The approach is motivated by applications arising in the detection
of mobile radioactive sources, and offers a pathway toward the development of
novel fixed- interval detection algorithms that combine decentralized
processing with optimal centralized decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1470</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1470</id><created>2012-10-04</created><authors><author><keyname>Pastore</keyname><forenames>Adriano</forenames></author><author><keyname>Joham</keyname><forenames>Michael</forenames></author><author><keyname>Fonollosa</keyname><forenames>Javier Rodr&#xed;guez</forenames></author></authors><title>A Framework for Joint Design of Pilot Sequence and Linear Precoder</title><categories>cs.IT math.IT</categories><comments>32 pages, 9 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most performance measures of pilot-assisted multiple-input multiple-output
(MIMO) systems are functions that depend on both the linear precoding filter
and the pilot sequence. A framework for the optimization of these two
parameters is proposed, based on a matrix-valued generalization of the concept
of effective signal-to-noise ratio (SNR) introduced in a famous work by Hassibi
and Hochwald. The framework applies to a wide class of utility functions of
said effective SNR matrix, most notably a well-known mutual information
expression for Gaussian inputs, an upper bound on the minimum mean-square error
(MMSE), as well as approximations thereof. The approach consists in decomposing
the joint optimization problem into three subproblems: first, we describe how
to reformulate the optimization of the linear precoder subject to a fixed pilot
sequence as a convex problem. Second, we do likewise for the optimization of
the pilot sequence subject to a fixed precoder. Third, we describe how to
generate pairs of precoders and pilot sequences that are Pareto optimal in the
sense that they attain the Pareto boundary of the set of feasible effective SNR
matrices. By combining these three optimization problems into an iteration, we
obtain an algorithm which allows to compute jointly optimal pairs of precoders
and pilot sequences with respect to some generic utility function of the
effective SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1472</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1472</id><created>2012-10-04</created><authors><author><keyname>Turakhia</keyname><forenames>Naman</forenames></author><author><keyname>Chheda</keyname><forenames>Nilay</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K.</forenames></author><author><keyname>Shah</keyname><forenames>Ruchin</forenames></author><author><keyname>Raisinghani</keyname><forenames>Jigar</forenames></author></authors><title>Biospectrogram: a tool for spectral analysis of biological sequences</title><categories>q-bio.QM cs.CE q-bio.GN</categories><comments>2 pages, 1 figure, submitted to Bioinformatics Journal,
  Biospectrogram is available at http://www.guptalab.org/biospectrogram</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Summary: Biospectrogam is an open-source software for the spectral analysis
of DNA and protein sequences. The software can fetch (from NCBI server), import
and manage biological data. One can analyze the data using Digital Signal
Processing (DSP) techniques since the software allows the user to convert the
symbolic data into numerical data using 23 popular encodings and then apply
popular transformations such as Fast Fourier Transform (FFT) etc. and export
it. The ability of exporting (both encoding files and transform files) as a
MATLAB .m file gives the user an option to apply variety of techniques of DSP.
User can also do window analysis (both sliding in forward and backward
directions and stagnant) with different size windows and search for meaningful
spectral pattern with the help of exported MATLAB file in a dynamic manner by
choosing time delay in the plot using Biospectrogram. Random encodings and user
choice encoding allows software to search for many possibilities in spectral
space.
  Availability: Biospectrogam is written in Java and is available to download
freely from http://www.guptalab.org/biospectrogram. Software has been optimized
to run on Windows, Mac OSX and Linux. User manual and you-tube (product demo)
tutorial is also available on the website. We are in the process of acquiring
open source license for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1480</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1480</id><created>2012-10-04</created><authors><author><keyname>van Harmelen</keyname><forenames>Frank</forenames></author><author><keyname>Kampis</keyname><forenames>George</forenames></author><author><keyname>Borner</keyname><forenames>Katy</forenames></author><author><keyname>Besselaar</keyname><forenames>Peter van den</forenames></author><author><keyname>Schultes</keyname><forenames>Erik</forenames></author><author><keyname>Goble</keyname><forenames>Carole</forenames></author><author><keyname>Groth</keyname><forenames>Paul</forenames></author><author><keyname>Mons</keyname><forenames>Barend</forenames></author><author><keyname>Anderson</keyname><forenames>Stuart</forenames></author><author><keyname>Decker</keyname><forenames>Stefan</forenames></author><author><keyname>Hayes</keyname><forenames>Conor</forenames></author><author><keyname>Buecheler</keyname><forenames>Thierry</forenames></author><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Theoretical And Technological Building Blocks For An Innovation
  Accelerator</title><categories>cs.DL</categories><doi>10.1140/epjst/e2012-01692-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scientific system that we use today was devised centuries ago and is
inadequate for our current ICT-based society: the peer review system encourages
conservatism, journal publications are monolithic and slow, data is often not
available to other scientists, and the independent validation of results is
limited. Building on the Innovation Accelerator paper by Helbing and Balietti
(2011) this paper takes the initial global vision and reviews the theoretical
and technological building blocks that can be used for implementing an
innovation (in first place: science) accelerator platform driven by
re-imagining the science system. The envisioned platform would rest on four
pillars: (i) Redesign the incentive scheme to reduce behavior such as
conservatism, herding and hyping; (ii) Advance scientific publications by
breaking up the monolithic paper unit and introducing other building blocks
such as data, tools, experiment workflows, resources; (iii) Use machine
readable semantics for publications, debate structures, provenance etc. in
order to include the computer as a partner in the scientific process, and (iv)
Build an online platform for collaboration, including a network of trust and
reputation among the different types of stakeholders in the scientific system:
scientists, educators, funding agencies, policy makers, students and industrial
innovators among others. Any such improvements to the scientific system must
support the entire scientific process (unlike current tools that chop up the
scientific process into disconnected pieces), must facilitate and encourage
collaboration and interdisciplinarity (again unlike current tools), must
facilitate the inclusion of intelligent computing in the scientific process,
must facilitate not only the core scientific process, but also accommodate
other stakeholders such science policy makers, industrial innovators, and the
general public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1505</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1505</id><created>2012-10-04</created><updated>2014-07-30</updated><authors><author><keyname>Hong</keyname><forenames>Yang</forenames></author><author><keyname>Huang</keyname><forenames>Changcheng</forenames></author><author><keyname>Yan</keyname><forenames>James</forenames></author></authors><title>A Comparative Study of SIP Overload Control Algorithms</title><categories>cs.NI</categories><comments>Network and Traffic Engineering in Emerging Distributed Computing
  Applications, Edited by J. Abawajy, M. Pathan, M. Rahman, A.K. Pathan, and
  M.M. Deris, IGI Global, 2012, pp. 1-20</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent collapses of SIP servers in the carrier networks indicates two
potential problems of SIP: (1) the current SIP design does not easily scale up
to large network sizes, and (2) the built-in SIP overload control mechanism
cannot handle overload conditions effectively. In order to help carriers
prevent widespread SIP network failure effectively, this chapter presents a
systematic investigation of current state-of-the-art overload control
algorithms. To achieve this goal, this chapter first reviews two basic
mechanisms of SIP, and summarizes numerous experiment results reported in the
literatures which demonstrate the impact of overload on SIP networks. After
surveying the approaches for modeling the dynamic behaviour of SIP networks
experiencing overload, the chapter presents a comparison and assessment of
different types of SIP overload control solutions. Finally it outlines some
research opportunities for managing SIP overload control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1507</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1507</id><created>2012-10-04</created><updated>2015-10-20</updated><authors><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author></authors><title>Decomposition by Successive Convex Approximation: A Unifying Approach
  for Linear Transceiver Design in Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Wireless Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the downlink linear precoder design problem in a multi-cell dense
heterogeneous network (HetNet). The problem is formulated as a general
sum-utility maximization (SUM) problem, which includes as special cases many
practical precoder design problems such as multi-cell coordinated linear
precoding, full and partial per-cell coordinated multi-point transmission,
zero-forcing precoding and joint BS clustering and beamforming/precoding. The
SUM problem is difficult due to its non-convexity and the tight coupling of the
users' precoders. In this paper we propose a novel convex approximation
technique to approximate the original problem by a series of convex
subproblems, each of which decomposes across all the cells. The convexity of
the subproblems allows for efficient computation, while their decomposability
leads to distributed implementation. {Our approach hinges upon the
identification of certain key convexity properties of the sum-utility
objective, which allows us to transform the problem into a form that can be
solved using a popular algorithmic framework called BSUM (Block Successive
Upper-Bound Minimization).} Simulation experiments show that the proposed
framework is effective for solving interference management problems in large
HetNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1524</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1524</id><created>2012-10-04</created><updated>2012-10-10</updated><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author></authors><title>An Emergency System for Succoring Children using Mobile GIS</title><categories>cs.CY</categories><comments>6 Pages, 8 Figures, 3 Tables</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), Vol. 3, No. 9, 2012, pages 218-223</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The large numbers of sick children in different diseases are very dreaded,
and when there isn't succor at the proper time and in the type the sick child
need it that makes us lose child. This paper suggested an emergency system for
succoring sick child locally when he required that, and there isn't someone
knows his disease. The proposed system is the first tracking system works
online (24 hour in the day) but only when the sick children requiring the help
using mobile GIS. In, this emergency system the child will send SMS (for easy
he click one button) contains his ID and coordinates (Longitude and Latitude)
via GPRS network to the web server (the child was registered previously on that
server), in this step the server will locate the sick child on Google map and
retrieve the child's information from the database which saved this information
in registration stage, and base on these information will send succoring
facility and at the same time informing the hospital, his parents, doctor, etc.
about that emergency case of the child using the SMS mode through GPRS network
again. The design and implement of the proposed system shows more effective
cost than other systems because it used a minimum configuration (hardware and
software) and works in economic mode.
  Keywords- GPS; GPRS; Mobile GIS; SMS; Tracking device; Emergency System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1530</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1530</id><created>2012-10-04</created><authors><author><keyname>Hu</keyname><forenames>Tao</forenames></author><author><keyname>Genkin</keyname><forenames>Alexander</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>A network of spiking neurons for computing sparse representations in an
  energy efficient way</title><categories>cs.NE q-bio.NC</categories><comments>5 figures Early Access:
  http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00353</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing sparse redundant representations is an important problem both in
applied mathematics and neuroscience. In many applications, this problem must
be solved in an energy efficient way. Here, we propose a hybrid distributed
algorithm (HDA), which solves this problem on a network of simple nodes
communicating via low-bandwidth channels. HDA nodes perform both
gradient-descent-like steps on analog internal variables and
coordinate-descent-like steps via quantized external variables communicated to
each other. Interestingly, such operation is equivalent to a network of
integrate-and-fire neurons, suggesting that HDA may serve as a model of neural
computation. We show that the numerical performance of HDA is on par with
existing algorithms. In the asymptotic regime the representation error of HDA
decays with time, t, as 1/t. HDA is stable against time-varying noise,
specifically, the representation error decays as 1/sqrt(t) for Gaussian white
noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1535</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1535</id><created>2012-10-04</created><updated>2013-03-06</updated><authors><author><keyname>Gavinsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Ito</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Wang</keyname><forenames>Guoming</forenames></author></authors><title>Shared Randomness and Quantum Communication in the Multi-Party Model</title><categories>quant-ph cs.CC</categories><comments>14 pages; v2: improved presentation, corrected statement of Theorem
  2.1, corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study shared randomness in the context of multi-party number-in-hand
communication protocols in the simultaneous message passing model. We show that
with three or more players, shared randomness exhibits new interesting
properties that have no direct analogues in the two-party case.
  First, we demonstrate a hierarchy of modes of shared randomness, with the
usual shared randomness where all parties access the same random string as the
strongest form in the hierarchy. We show exponential separations between its
levels, and some of our bounds may be of independent interest. For example, we
show that the equality function can be solved by a protocol of constant length
using the weakest form of shared randomness, which we call &quot;XOR-shared
randomness.&quot;
  Second, we show that quantum communication cannot replace shared randomness
in the k-party case, where k &gt;= 3 is any constant. We demonstrate a promise
function GP_k that can be computed by a classical protocol of constant length
when (the strongest form of) shared randomness is available, but any quantum
protocol without shared randomness must send n^Omega(1) qubits to compute it.
Moreover, the quantum complexity of GP_k remains n^Omega(1) even if the &quot;second
strongest&quot; mode of shared randomness is available. While a somewhat similar
separation was already known in the two-party case, in the multi-party case our
statement is qualitatively stronger:
  * In the two-party case, only a relational communication problem with similar
properties is known.
  * In the two-party case, the gap between the two complexities of a problem
can be at most exponential, as it is known that 2^(O(c)) log n qubits can
always replace shared randomness in any c-bit protocol. Our bounds imply that
with quantum communication alone, in general, it is not possible to simulate
efficiently even a three-bit three-party classical protocol that uses shared
randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1544</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1544</id><created>2012-10-04</created><authors><author><keyname>Hu</keyname><forenames>Tao</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>Reconstruction of Sparse Circuits Using Multi-neuronal Excitation
  (RESCUME)</title><categories>q-bio.NC cs.DS</categories><comments>9 pages, 6 figures. Advances in Neural Information Processing Systems
  (NIPS) 22, 790 (2009)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the central problems in neuroscience is reconstructing synaptic
connectivity in neural circuits. Synapses onto a neuron can be probed by
sequentially stimulating potentially pre-synaptic neurons while monitoring the
membrane voltage of the post-synaptic neuron. Reconstructing a large neural
circuit using such a &quot;brute force&quot; approach is rather time-consuming and
inefficient because the connectivity in neural circuits is sparse. Instead, we
propose to measure a post-synaptic neuron's voltage while stimulating
sequentially random subsets of multiple potentially pre-synaptic neurons. To
reconstruct these synaptic connections from the recorded voltage we apply a
decoding algorithm recently developed for compressive sensing. Compared to the
brute force approach, our method promises significant time savings that grow
with the size of the circuit. We use computer simulations to find optimal
stimulation parameters and explore the feasibility of our reconstruction method
under realistic experimental conditions including noise and non-linear synaptic
integration. Multineuronal stimulation allows reconstructing synaptic
connectivity just from the spiking activity of post-synaptic neurons, even when
sub-threshold voltage is unavailable. By using calcium indicators,
voltage-sensitive dyes, or multi-electrode arrays one could monitor activity of
multiple postsynaptic neurons simultaneously, thus mapping their synaptic
inputs in parallel, potentially reconstructing a complete neural circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1549</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1549</id><created>2012-10-04</created><updated>2012-10-06</updated><authors><author><keyname>Schieler</keyname><forenames>Curt</forenames></author><author><keyname>Song</keyname><forenames>Eva C.</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Source-Channel Secrecy with Causal Disclosure</title><categories>cs.IT math.IT</categories><comments>Allerton 2012, 6 pages. Updated version includes acknowledgements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imperfect secrecy in communication systems is investigated. Instead of using
equivocation as a measure of secrecy, the distortion that an eavesdropper
incurs in producing an estimate of the source sequence is examined. The
communication system consists of a source and a broadcast (wiretap) channel,
and lossless reproduction of the source sequence at the legitimate receiver is
required. A key aspect of this model is that the eavesdropper's actions are
allowed to depend on the past behavior of the system. Achievability results are
obtained by studying the performance of source and channel coding operations
separately, and then linking them together digitally. Although the problem
addressed here has been solved when the secrecy resource is shared secret key,
it is found that substituting secret key for a wiretap channel brings new
insights and challenges: the notion of weak secrecy provides just as much
distortion at the eavesdropper as strong secrecy, and revealing public messages
freely is detrimental.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1550</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1550</id><created>2012-10-04</created><updated>2013-07-24</updated><authors><author><keyname>Krovi</keyname><forenames>Hari</forenames></author><author><keyname>Russell</keyname><forenames>Alexander</forenames></author></authors><title>Quantum Fourier Transforms and the Complexity of Link Invariants for
  Quantum Doubles of Finite Groups</title><categories>quant-ph cs.CC</categories><comments>30 pages</comments><journal-ref>Commun. Math. Phys. 334, 743-777 (2015)</journal-ref><doi>10.1007/s00220-014-2285-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knot and link invariants naturally arise from any braided Hopf algebra. We
consider the computational complexity of the invariants arising from an
elementary family of finite-dimensional Hopf algebras: quantum doubles of
finite groups (denoted D(G), for a group G). Regarding algorithms for these
invariants, we develop quantum circuits for the quantum Fourier transform over
D(G); in general, we show that when one can uniformly and efficiently carry out
the quantum Fourier transform over the centralizers Z(g) of the elements of G,
one can efficiently carry out the quantum Fourier transform over D(G). We apply
these results to the symmetric groups to yield efficient circuits for the
quantum Fourier transform over D(S_n). With such a Fourier transform, it is
straightforward to obtain additive approximation algorithms for the related
link invariant. Additionally, we show that certain D(G) invariants (such as
D(A_n) invariants) are BPP-hard to additively approximate, SBP-hard to
multiplicatively approximate, and #P-hard to exactly evaluate. Finally, we make
partial progress on the question of simulating anyonic computation in groups
uniformly as a function of the group size. In this direction, we provide
efficient quantum circuits for the Clebsch-Gordan transform over D(G) for
&quot;fluxon&quot; irreps, i.e., irreps of D(G) characterized by a conjugacy class of G.
For general irreps, i.e., those which are associated with a conjugacy class of
G and an irrep of a centralizer, we present an efficient implementation under
certain conditions such as when there is an efficient Clebsch-Gordan transform
over the centralizers. We remark that this also provides a simulation of
certain anyonic models of quantum computation, even in circumstances where the
group may have size exponential in the size of the circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1568</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1568</id><created>2012-10-03</created><authors><author><keyname>Dobrev</keyname><forenames>Dimiter</forenames></author></authors><title>A Definition of Artificial Intelligence</title><categories>cs.AI</categories><journal-ref>Dobrev D. A Definition of Artificial Intelligence. In: Mathematica
  Balkanica, New Series, Vol. 19, 2005, Fasc. 1-2, pp.67-74</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we offer a formal definition of Artificial Intelligence and
this directly gives us an algorithm for construction of this object. Really,
this algorithm is useless due to the combinatory explosion.
  The main innovation in our definition is that it does not include the
knowledge as a part of the intelligence. So according to our definition a newly
born baby also is an Intellect. Here we differs with Turing's definition which
suggests that an Intellect is a person with knowledge gained through the years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1572</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1572</id><created>2012-10-04</created><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>Turing Patterns with Turing Machines: Emergence and Low-level Structure
  Formation</title><categories>cs.CC nlin.CG nlin.PS</categories><comments>27 pages, 14 figures. Forthcoming in Natural Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite having advanced a reaction-diffusion model of ODE's in his 1952 paper
on morphogenesis, reflecting his interest in mathematical biology, Alan Turing
has never been considered to have approached a definition of Cellular Automata.
However, his treatment of morphogenesis, and in particular a difficulty he
identified relating to the uneven distribution of certain forms as a result of
symmetry breaking, are key to connecting his theory of universal computation
with his theory of biological pattern formation. Making such a connection would
not overcome the particular difficulty that Turing was concerned about, which
has in any case been resolved in biology. But instead the approach developed
here captures Turing's initial concern and provides a low-level solution to a
more general question by way of the concept of algorithmic probability, thus
bridging two of his most important contributions to science: Turing pattern
formation and universal computation. I will provide experimental results of
one-dimensional patterns using this approach, with no loss of generality to a
n-dimensional pattern generalisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1593</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1593</id><created>2012-10-04</created><authors><author><keyname>Swierczewski</keyname><forenames>Lukasz</forenames></author></authors><title>The Distributed Computing Model Based on The Capabilities of The
  Internet</title><categories>cs.NI cs.DC</categories><comments>6 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paper describes the theoretical and practical aspects of the proposed model
that uses distributed computing to a global network of Internet communication.
Distributed computing are widely used in modern solutions such as research,
where the requirement is very high processing power, which can not be placed in
one centralized point. The presented solution is based on open technologies and
computers to perform calculations provided mainly by Internet users who are
volunteers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1611</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1611</id><created>2012-10-04</created><authors><author><keyname>Zhou</keyname><forenames>Neng-Fa</forenames></author><author><keyname>Have</keyname><forenames>Christian Theil</forenames></author></authors><title>Efficient Tabling of Structured Data with Enhanced Hash-Consing</title><categories>cs.PL</categories><comments>16 pages; TPLP, 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Current tabling systems suffer from an increase in space complexity, time
complexity or both when dealing with sequences due to the use of data
structures for tabled subgoals and answers and the need to copy terms into and
from the table area. This symptom can be seen in not only B-Prolog, which uses
hash tables, but also systems that use tries such as XSB and YAP. In this
paper, we apply hash-consing to tabling structured data in B-Prolog. While
hash-consing can reduce the space consumption when sharing is effective, it
does not change the time complexity. We enhance hash-consing with two
techniques, called input sharing and hash code memoization, for reducing the
time complexity by avoiding computing hash codes for certain terms. The
improved system is able to eliminate the extra linear factor in the old system
for processing sequences, thus significantly enhancing the scalability of
applications such as language parsing and bio-sequence analysis applications.
We confirm this improvement with experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1613</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1613</id><created>2012-10-04</created><authors><author><keyname>Manipatruni</keyname><forenames>Sasikanth</forenames></author><author><keyname>Nikonov</keyname><forenames>Dmitri E.</forenames></author><author><keyname>Young</keyname><forenames>Ian A.</forenames></author></authors><title>All Spin Nano-magnetic State Elements</title><categories>cond-mat.mes-hall cs.ET</categories><comments>21 pages, 6 figures</comments><acm-class>B.3; B.6.1; B.7.0; B.7.1; B.7.2; J.2; G.4; I.6.1</acm-class><doi>10.1063/1.4810904</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an all spin state element to enable all spin state machines using
spin currents and nanomagnets. We demonstrate via numerical simulations the
operation of a state element a critical building block for synchronous,
sequential logic computation. The numerical models encompass
Landau-Lifshitz-Gilbert (LLG) nanomagnet dynamics with stochastic models and
vector spin-transport in metallic magnetic and non-magnetic channels. Combined
with all spin combinatorial logic, the state elements can enable synchronous
and asynchronous computing elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1624</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1624</id><created>2012-10-04</created><updated>2012-10-12</updated><authors><author><keyname>Kar</keyname><forenames>Swarnendu</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Controlled Collaboration for Linear Coherent Estimation in Wireless
  Sensor Networks</title><categories>cs.IT math.IT</categories><comments>8 pages double column, 11 figures, final pre-print as will appear in
  Proceedings of 50th Annual Allerton Conference on Communication, Control and
  Computing - 2012. This paper still does not contain the proofs, which will be
  provided in a future version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless sensor network consisting of multiple nodes that are
coordinated by a fusion center (FC) in order to estimate a common signal of
interest. In addition to being coordinated, the sensors are also able to
collaborate, i.e., share observations with other neighboring nodes, prior to
transmission. In an earlier work, we derived the energy-optimal collaboration
strategy for the single-snapshot framework, where the inference has to be made
based on observations collected at one particular instant. In this paper, we
make two important contributions. Firstly, for the single-snapshot framework,
we gain further insights into partially connected collaboration networks
(nearest-neighbor and random geometric graphs for example) through the analysis
of a family of topologies with regular structure. Secondly, we explore the
estimation problem by adding the dimension of time, where the goal is to
estimate a time-varying signal in a power-constrained network. To model the
time dynamics, we consider the stationary Gaussian process with exponential
covariance (sometimes referred to as Ornstein-Uhlenbeck process) as our
representative signal. For such a signal, we show that it is always beneficial
to sample as frequently as possible, despite the fact that the samples get
increasingly noisy due to the power-constrained nature of the problem.
Simulation results are presented to corroborate our analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1626</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1626</id><created>2012-10-04</created><authors><author><keyname>Yao</keyname><forenames>Hengshuai</forenames></author></authors><title>Discovering and Leveraging the Most Valuable Links for Ranking</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the Web, visits of a page are often introduced by one or more valuable
linking sources. Indeed, good back links are valuable resources for Web pages
and sites. We propose to discovering and leveraging the best backlinks of pages
for ranking. Similar to PageRank, MaxRank scores are updated {recursively}. In
particular, with probability $\lambda$, the MaxRank of a document is updated
from the backlink source with the maximum score; with probability $1-\lambda$,
the MaxRank of a document is updated from a random backlink source. MaxRank has
an interesting relation to PageRank. When $\lambda=0$, MaxRank reduces to
PageRank; when $\lambda=1$, MaxRank only looks at the best backlink it thinks.
Empirical results on Wikipedia shows that the global authorities are very
influential; Overall large $\lambda$s (but smaller than 1) perform best: the
convergence is dramatically faster than PageRank, but the performance is still
comparable. We study the influence of these sources and propose a few measures
such as the times of being the best backlink for others, and related properties
of the proposed algorithm. The introduction of best backlink sources provides
new insights for link analysis. Besides ranking, our method can be used to
discover the most valuable linking sources for a page or Website, which is
useful for both search engines and site owners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1630</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1630</id><created>2012-10-04</created><authors><author><keyname>Fu</keyname><forenames>Jie</forenames></author><author><keyname>Tanner</keyname><forenames>Herbert G.</forenames></author><author><keyname>Heinz</keyname><forenames>Jeffrey</forenames></author><author><keyname>Chandlee</keyname><forenames>Jane</forenames></author><author><keyname>Karydis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Koirala</keyname><forenames>Cesar</forenames></author></authors><title>Symbolic Planning and Control Using Game Theory and Grammatical
  Inference</title><categories>cs.RO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach that brings together game theory with
grammatical inference and discrete abstractions in order to synthesize control
strategies for hybrid dynamical systems performing tasks in partially unknown
but rule-governed adversarial environments. The combined formulation guarantees
that a system specification is met if (a) the true model of the environment is
in the class of models inferable from a positive presentation, (b) a
characteristic sample is observed, and (c) the task specification is
satisfiable given the capabilities of the system (agent) and the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1633</identifier>
 <datestamp>2013-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1633</id><created>2012-10-04</created><updated>2013-09-25</updated><authors><author><keyname>Bao</keyname><forenames>Wei</forenames></author><author><keyname>Liang</keyname><forenames>Ben</forenames></author></authors><title>Insensitivity of User Distribution in Multicell Networks under General
  Mobility and Session Patterns</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The location of active users is an important factor in the performance
analysis of mobile multicell networks, but it is difficult to quantify due to
the wide variety of user mobility and session patterns. In particular, the
channel holding times in each cell may be arbitrarily distributed and dependent
on those in other cells. In this work, we study the stationary distribution of
users by modeling the system as a multi-route queueing network with Poisson
inputs. We consider arbitrary routing and arbitrary joint probability
distributions for the channel holding times in each route. Using a
decomposition-composition approach, we show that the user distribution (1) is
insensitive to the user movement patterns, (2) is insensitive to general and
dependently distributed channel holding times, (3) depends only on the average
arrival rate and average channel holding time at each cell, and (4) is
completely characterized by an open network with M/M/infinity queues. This
result is validated by experiments with the Dartmouth user mobility traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1639</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1639</id><created>2012-10-04</created><updated>2012-10-07</updated><authors><author><keyname>Duarte</keyname><forenames>Melissa</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Jana</keyname><forenames>Rittwik</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>K. K.</forenames></author><author><keyname>Rice</keyname><forenames>Christopher</forenames></author><author><keyname>Shankaranarayanan</keyname><forenames>N. K.</forenames></author></authors><title>Design and Characterization of a Full-duplex Multi-antenna System for
  WiFi networks</title><categories>cs.NI</categories><comments>44 page, 11 figures, 8 tables. Submitted to IEEE Transactions on
  Vehicular Technology, Oct 2012</comments><journal-ref>IEEE Transactions on Vehicular Technology, vol.63, no.3,
  pp.1160--1177, March 2014</journal-ref><doi>10.1109/TVT.2013.2284712</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an experimental and simulation based study to
evaluate the use of full-duplex as a mode in practical IEEE 802.11 networks. To
enable the study, we designed a 20 MHz multi-antenna OFDM full-duplex physical
layer and a full-duplex capable MAC protocol which is backward compatible with
current 802.11. Our extensive over-the-air experiments, simulations and
analysis demonstrate the following two results. First, the use of multiple
antennas at the physical layer leads to a higher ergodic throughput than its
hardware-equivalent multi-antenna half-duplex counterparts, for SNRs above the
median SNR encountered in practical WiFi deployments. Second, the proposed MAC
translates the physical layer rate gain into near doubling of throughput for
multi-node single-AP networks. The two combined results allow us to conclude
that there are potentially significant benefits gained from including a
full-duplex mode in future WiFi standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1645</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1645</id><created>2012-10-05</created><authors><author><keyname>Adeyemi</keyname><forenames>Ikuesan R.</forenames></author><author><keyname>Razak</keyname><forenames>Shukor Abd</forenames></author><author><keyname>Azhan</keyname><forenames>Nor Amira Nor</forenames></author></authors><title>Identifying critical features for network forensics investigation
  perspectives</title><categories>cs.CR cs.CY</categories><comments>International Journal of Computer Science and Information Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in the field of network forensics is gradually expanding with the
propensity to fully accommodate the tenacity to help in adjudicating, curbing
and apprehending the exponential growth of cyber crimes. However, investigating
cyber crime differs, depending on the perspective of investigation. There is
therefore the need for a comprehensive model, containing relevant critical
features required for a thorough investigation for each perspective, which can
be adopted by investigators. This paper therefore presents the findings on the
critical features for each perspective, as well as their characteristics. The
paper also presents a review of existing frameworks on network forensics.
Furthermore, the paper discussed an illustrative methodological process for
each perspective encompassing the relevant critical features. These
illustrations present a procedure for the thorough investigation in network
forensics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1646</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1646</id><created>2012-10-05</created><authors><author><keyname>Ormerod</keyname><forenames>Paul</forenames></author><author><keyname>Tarbush</keyname><forenames>Bassel</forenames></author><author><keyname>Bentley</keyname><forenames>R. Alexander</forenames></author></authors><title>Social network markets: the influence of network structure when
  consumers face decisions over many similar choices</title><categories>cs.SI physics.soc-ph</categories><comments>14 pages, 5 figures</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social network markets, the act of consumer choice in these industries is
governed not just by the set of incentives described by conventional consumer
demand theory, but by the choices of others in which an individual's payoff is
an explicit function of the actions of others. We observe two key empirical
features of outcomes in social networked markets. First, a highly right-skewed,
non-Gaussian distribution of the number of times competing alternatives are
selected at a point in time. Second, there is turnover in the rankings of
popularity over time. We show here that such outcomes can arise either when
there is no alternative which exhibits inherent superiority in its attributes,
or when agents find it very difficult to discern any differences in quality
amongst the alternatives which are available so that it is as if no superiority
exists. These features appear to obtain, as a reasonable approximation, in many
social network markets. We examine the impact of network structure on both the
rank-size distribution of choices at a point in time, and on the life spans of
the most popular choices. We show that a key influence on outcomes is the
extent to which the network follows a hierarchical structure. It is the social
network properties of the markets, the meso-level structure, which determine
outcomes rather than the objective attributes of the products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1647</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1647</id><created>2012-10-05</created><authors><author><keyname>Adeyemi</keyname><forenames>Ikuesan R.</forenames></author><author><keyname>Ithnin</keyname><forenames>Norafida Bt.</forenames></author></authors><title>Users Authentication and Privacy control of RFID Card</title><categories>cs.CR</categories><comments>International Journal of Scientific &amp; Engineering Research (IJSER)
  ISSN 2229-5518 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security and Privacy concerns in Radio frequency identification (RFID)
technology particularly RFID Card, is a wide research area which have attracted
researchers for over a decade. Authenticating users at the Card end of the RFID
technology constitutes one of the major sources of attacks on the system. In
this research, we studied the various known attacks and mitigation available.
We proposed a conceptual framework that that can be used to mitigate the
unauthorized use of RFID Card. This concept will mitigate the single point of
the RFID card failure: unauthorized use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1649</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1649</id><created>2012-10-05</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Fink</keyname><forenames>Michael</forenames></author><author><keyname>Krennwallner</keyname><forenames>Thomas</forenames></author><author><keyname>Redl</keyname><forenames>Christoph</forenames></author></authors><title>Conflict-driven ASP Solving with External Sources</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming</comments><journal-ref>Theor. Pract. Log. Prog. 12:4-5 (2012) 659-679</journal-ref><doi>10.1017/S1471068412000233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is a well-known problem solving approach based
on nonmonotonic logic programs and efficient solvers. To enable access to
external information, HEX-programs extend programs with external atoms, which
allow for a bidirectional communication between the logic program and external
sources of computation (e.g., description logic reasoners and Web resources).
Current solvers evaluate HEX-programs by a translation to ASP itself, in which
values of external atoms are guessed and verified after the ordinary answer set
computation. This elegant approach does not scale with the number of external
accesses in general, in particular in presence of nondeterminism (which is
instrumental for ASP). In this paper, we present a novel, native algorithm for
evaluating HEX-programs which uses learning techniques. In particular, we
extend conflict-driven ASP solving techniques, which prevent the solver from
running into the same conflict again, from ordinary to HEX-programs. We show
how to gain additional knowledge from external source evaluations and how to
use it in a conflict-driven algorithm. We first target the uninformed case,
i.e., when we have no extra information on external sources, and then extend
our approach to the case where additional meta-information is available.
Experiments show that learning from external sources can significantly decrease
both the runtime and the number of considered candidate compatible sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1653</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1653</id><created>2012-10-05</created><authors><author><keyname>Cervesato</keyname><forenames>Iliano</forenames></author></authors><title>An Improved Proof-Theoretic Compilation of Logic Programs</title><categories>cs.LO cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In prior work, we showed that logic programming compilation can be given a
proof-theoretic justification for generic abstract logic programming languages,
and demonstrated this technique in the case of hereditary Harrop formulas and
their linear variant. Compiled clauses were themselves logic formulas except
for the presence of a second-order abstraction over the atomic goals matching
their head. In this paper, we revisit our previous results into a more detailed
and fully logical justification that does away with this spurious abstraction.
We then refine the resulting technique to support well-moded programs
efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1665</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1665</id><created>2012-10-05</created><authors><author><keyname>Vidal</keyname><forenames>Germ&#xe1;n</forenames></author></authors><title>Annotation of Logic Programs for Independent AND-Parallelism by Partial
  Evaluation</title><categories>cs.PL cs.SE</categories><comments>22 pages, includes an appendix</comments><acm-class>D.1.6; F.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, Volume12, Special
  Issue4-5, 2012, pp 583-600</journal-ref><doi>10.1017/S1471068412000191</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional approaches to automatic AND-parallelization of logic programs
rely on some static analysis to identify independent goals that can be safely
and efficiently run in parallel in any possible execution. In this paper, we
present a novel technique for generating annotations for independent
AND-parallelism that is based on partial evaluation. Basically, we augment a
simple partial evaluation procedure with (run-time) groundness and variable
sharing information so that parallel conjunctions are added to the residual
clauses when the conditions for independence are met. In contrast to previous
approaches, our partial evaluator is able to transform the source program in
order to expose more opportunities for parallelism. To the best of our
knowledge, we present the first approach to a parallelizing partial evaluator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1685</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1685</id><created>2012-10-05</created><authors><author><keyname>Ablinger</keyname><forenames>J.</forenames></author><author><keyname>Bl&#xfc;mlein</keyname><forenames>S.</forenames></author><author><keyname>Round</keyname><forenames>M.</forenames></author><author><keyname>Schneider</keyname><forenames>C.</forenames></author></authors><title>Advanced Computer Algebra Algorithms for the Expansion of Feynman
  Integrals</title><categories>cs.SC hep-ph hep-th math-ph math.MP</categories><comments>14 pages, Proceedings, Loops and Legs in Quantum Field Theory 2012,
  Wernigerode,D; PoS(2012)</comments><report-no>DESY 12-164; DO-Th 12/30; SFB/CPP-12-73; LPN12-104</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-point Feynman parameter integrals, with at most one mass and containing
local operator insertions in $4+\ep$-dimensional Minkowski space, can be
transformed to multi-integrals or multi-sums over hyperexponential and/or
hypergeometric functions depending on a discrete parameter $n$. Given such a
specific representation, we utilize an enhanced version of the multivariate
Almkvist--Zeilberger algorithm (for multi-integrals) and a common summation
framework of the holonomic and difference field approach (for multi-sums) to
calculate recurrence relations in $n$. Finally, solving the recurrence we can
decide efficiently if the first coefficients of the Laurent series expansion of
a given Feynman integral can be expressed in terms of indefinite nested sums
and products; if yes, the all $n$ solution is returned in compact
representations, i.e., no algebraic relations exist among the occurring sums
and products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1686</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1686</id><created>2012-10-05</created><authors><author><keyname>Shayeji</keyname><forenames>Mohammad H. Al</forenames></author><author><keyname>Al-Azmi</keyname><forenames>AbdulRahman R.</forenames></author><author><keyname>Al-Azmi</keyname><forenames>AbdulAziz R.</forenames></author><author><keyname>Samrajesh</keyname><forenames>M. D.</forenames></author></authors><title>Analysis and Enhancements of Leader Elections algorithms in Mobile Ad
  Hoc Networks</title><categories>cs.DC cs.CR</categories><comments>5 Pages, 4 Figures, 1 Table, 2011 Second International Conference on
  Advances in Information and Communication Technologies; ACEEE International
  Journal on Network Security, Vol. 02, No. 04, Oct 2011</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Mobile Ad Hoc networks (MANET), distinct from traditional distributed
systems, are dynamic and self-organizing networks. MANET requires a leader to
coordinate and organize tasks. The challenge is to have the right election
algorithm that chooses the right leader based on various factors in MANET. In
this paper, we analyze four leader election algorithms used in mobile Ad Hoc
Networks. Factors considered in our analysis are time complexity, message
complexity, assumptions considered, fault tolerance and timing model. Our
proposed enhancements include recovered nodes inquiring about the current
leader and the use of candidates during election to reduce the overhead of
starting a new election session. In addition, better election criteria specific
to MANET, such as battery life and signal strength, are proposed. Our
evaluation and discussion shows that the proposed enhancements are effective.
The analysis can be used as a reference for system designers in choosing the
right election algorithm for MANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1689</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1689</id><created>2012-10-05</created><updated>2014-02-27</updated><authors><author><keyname>Beigi</keyname><forenames>Salman</forenames></author></authors><title>A New Quantum Data Processing Inequality</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages, fixed an error in the statement of Theorem 2 (thanks to
  Dong Yang)</comments><journal-ref>J. Math. Phys. 54, 082202 (2013)</journal-ref><doi>10.1063/1.4818985</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum data processing inequality bounds the set of bipartite states that
can be generated by two far apart parties under local operations; Having access
to a bipartite state as a resource, two parties cannot locally transform it to
another bipartite state with a mutual information greater than that of the
resource state. But due to the additivity of quantum mutual information under
tensor product, the data processing inequality gives no bound when the parties
are provided with arbitrary number of copies of the resource state. In this
paper we introduce a measure of correlation on bipartite quantum states, called
maximal correlation, that is not additive and gives the same number when
computed for multiple copies. Then by proving a data processing inequality for
this measure, we find a bound on the set of states that can be generated under
local operations even when an arbitrary number of copies of the resource state
is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1708</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1708</id><created>2012-10-05</created><updated>2012-12-15</updated><authors><author><keyname>Yang</keyname><forenames>Yaoqing</forenames></author><author><keyname>Liu</keyname><forenames>Keqin</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author></authors><title>Distributed Flow Scheduling in an Unknown Environment</title><categories>cs.GT</categories><comments>10 pages, 3 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flow scheduling tends to be one of the oldest and most stubborn problems in
networking. It becomes more crucial in the next generation network, due to fast
changing link states and tremendous cost to explore the global structure. In
such situation, distributed algorithms often dominate. In this paper, we design
a distributed virtual game to solve the flow scheduling problem and then
generalize it to situations of unknown environment, where online learning
schemes are utilized. In the virtual game, we use incentives to stimulate
selfish users to reach a Nash Equilibrium Point which is valid based on the
analysis of the `Price of Anarchy'. In the unknown-environment generalization,
our ultimate goal is the minimization of cost in the long run. In order to
achieve balance between exploration of routing cost and exploitation based on
limited information, we model this problem based on Multi-armed Bandit Scenario
and combined newly proposed DSEE with the virtual game design. Armed with these
powerful tools, we find a totally distributed algorithm to ensure the
logarithmic growing of regret with time, which is optimum in classic
Multi-armed Bandit Problem. Theoretical proof and simulation results both
affirm this claim. To our knowledge, this is the first research to combine
multi-armed bandit with distributed flow scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1709</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1709</id><created>2012-10-05</created><authors><author><keyname>Murugesan</keyname><forenames>Ravi</forenames></author></authors><title>Promising outcomes of an online course in research writing at a Rwandan
  university</title><categories>cs.CY</categories><journal-ref>European Science Editing, August 2012, 38(3), 60-64</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Researchers in developing countries often do not have access to
training on research writing. The purpose of this study was to test whether
researchers in Rwanda might complete and benefit from a pilot online course in
research writing. Methods: The pilot course was set up on Moodle, an
open-source online learning environment, and facilitated by the author. The
lessons and assignment were spread over six weeks, followed by a two-week
extension period. Twenty-eight faculty members of the National University of
Rwanda enrolled themselves in the course. Results: Twenty-five of the 28
learners completed the course. After the course, these learners expressed high
satisfaction, e.g, 24 of them felt that they were ready to write a research
paper for publication. Conclusion: The high completion rate (89%) is noteworthy
for two reasons: e-learning courses tend to have lower completion rates than
classroom courses, and 76% of the learners in the pilot course had not taken an
e-learning course before. This result and the positive feedback indicate that
online courses can benefit researchers in developing countries who may not have
access to classroom courses on research writing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1714</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1714</id><created>2012-10-05</created><authors><author><keyname>Jackson</keyname><forenames>Andrew N.</forenames></author></authors><title>Formats over Time: Exploring UK Web History</title><categories>cs.DL</categories><comments>4 pages, 6 figures, presented at iPres 2012 in Toronto</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Is software obsolescence a significant risk? To explore this issue, we
analysed a corpus of over 2.5 billion resources corresponding to the UK Web
domain, as crawled between 1996 and 2010. Using the DROID and Apache Tika
identification tools, we examined each resource and captured the results as
extended MIME types, embedding version, software and hardware identifiers
alongside the format information. The combined results form a detailed temporal
format profile of the corpus, which we have made available as open data. We
present the results of our initial analysis of this dataset. We look at image,
HTML and PDF resources in some detail, showing how the usage of different
formats, versions and software implementations has changed over time.
Furthermore, we show that software obsolescence is rare on the web and uncover
evidence indicating that network effects act to stabilise formats against
obsolescence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1741</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1741</id><created>2012-10-05</created><updated>2013-05-21</updated><authors><author><keyname>Foldes</keyname><forenames>Stephan</forenames></author><author><keyname>Horv&#xe1;th</keyname><forenames>Eszter K.</forenames></author><author><keyname>Radeleczki</keyname><forenames>S&#xe1;ndor</forenames></author><author><keyname>Waldhauser</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>A general framework for island systems</title><categories>math.CO cs.DM</categories><comments>17 pages, 3 figures; minor corrections</comments><journal-ref>Acta Sci. Math. (Szeged) 81 (2015) 3--24</journal-ref><doi>10.14232/actasm-013-279-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of an island defined on a rectangular board is an elementary
combinatorial concept that occurred first in [G. Cz\'edli, The number of
rectangular islands by means of distributive lattices, European J. Combin. 30
(2009), 208-215]. Results of this paper were starting points for investigations
exploring several variations and various aspects of this notion. In this paper
we introduce a general framework for islands that subsumes all earlier studied
concepts of islands on finite boards, moreover we show that the prime
implicants of a Boolean function, the formal concepts of a formal context,
convex subgraphs of a simple graph, and some particular subsets of a projective
plane also fit into this framework. We axiomatize those cases where islands
have the comparable or disjoint property, or they are distant, introducing the
notion of a connective island domain and of a proximity domain, respectively.
In the general case the maximal systems of islands are characterised by using
the concept of an admissible system. We also characterise all possible island
systems in the case of island domains and proximity domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1745</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1745</id><created>2012-10-05</created><authors><author><keyname>Delavar</keyname><forenames>Arash Ghorbannia</forenames></author><author><keyname>Keshani</keyname><forenames>Golnoosh</forenames></author></authors><title>Providing an Object Allocation Algorithm in Distributed Databases Using
  Efficient Factors</title><categories>cs.DB cs.DC</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 9, Issue
  4, No 3, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data replication is a common method used to improve the performance of data
access in distributed database systems. In this paper, we present an object
replication algorithm in distributed database systems (ORAD). We optimize the
created replicated data in distributed database systems by using activity
functions of previous algorithms, changing them with new technical ways and
applying ORAD algorithm for making decisions. We propose ORAD algorithm with
using effective factors and observe its results in several valid situations.
Our objective is to propose an optimum method that replies read and write
requests with less cost in distributed database systems. Finally, we implement
ORAD and ADRW algorithms in a PC based network system and demonstrate that ORAD
algorithm is superior to ADRW algorithm in the field of average request
servicing cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1752</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1752</id><created>2012-10-05</created><authors><author><keyname>Kayhan</keyname><forenames>Farbod</forenames></author><author><keyname>Montorsi</keyname><forenames>Guido</forenames></author></authors><title>Constellation Design for Channels Affected by Phase Noise</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, submitted to IEEE Int. Conf. on Communications
  (ICC) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we optimize constellation sets to be used for channels affected
by phase noise. The main objective is to maximize the achievable mutual
information of the constellation under a given power constraint. The mutual
information and pragmatic mutual information of a given constellation is
calculated approximately assuming that both the channel and phase noise are
white. Then a simulated annealing algorithm is used to jointly optimize the
constellation and the binary labeling. The performance of optimized
constellations is compared with conventional constellations showing
considerable gains in all system scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1753</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1753</id><created>2012-10-03</created><authors><author><keyname>Choy</keyname><forenames>Murphy</forenames></author><author><keyname>Cheong</keyname><forenames>Michelle</forenames></author></authors><title>Intelligent Search Heuristics for Cost Based Scheduling</title><categories>cs.AI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nurse scheduling is a difficult optimization problem with multiple
constraints. There is extensive research in the literature solving the problem
using meta-heuristics approaches. In this paper, we will investigate an
intelligent search heuristics that handles cost based scheduling problem. The
heuristics demonstrated superior performances compared to the original
algorithms used to solve the problems described in Li et. Al. (2003) and
Ozkarahan (1989) in terms of time needed to establish a feasible solution. Both
problems can be formulated as a cost problem. The search heuristic consists of
several phrases of search and input based on the cost of each assignment and
how the assignment will interact with the cost of the resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1757</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1757</id><created>2012-10-05</created><authors><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>The AND-OR game: Equilibrium Characterization (Working Paper)</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a simple simultaneous first price auction for multiple items in a
complete information setting. Our goal is to completely characterize the mixed
equilibria in this setting, for a simple, yet highly interesting, {\tt
AND}-{\tt OR} game, where one agent is single minded and the other is unit
demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1762</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1762</id><created>2012-10-05</created><authors><author><keyname>Kayhan</keyname><forenames>Farbod</forenames></author><author><keyname>Montorsi</keyname><forenames>Guido</forenames></author></authors><title>Constellation Design for Transmission over Nonlinear Satellite Channels</title><categories>cs.IT math.IT</categories><comments>7 pages, 9 figures, A shorter version is accepted for presentation in
  GLOBECOM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use a variation of simulated annealing algorithm for
optimizing two-dimensional constellations with 32 signals. The main objective
is to maximize the symmetric pragmatic capacity under the peak-power
constraint. The method allows the joint optimization of constellation and
binary labeling. We also investigate the performance of the optimized
constellation over nonlinear satellite channel under additive white Gaussian
noise. We consider the performance over systems with and without
pre-distorters. In both cases the optimized constellations perform considerably
better than the conventional Amplitude Phase Shift Keying (APSK) modulations,
used in the current digital video broadcasting standard (DVB-S2) on satellite
channels. Based on our optimized constellations, we also propose a new labeling
for the 4+12+16-APSK constellation of the DVB-S2 standard which is Gray over
all rings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1765</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1765</id><created>2012-10-05</created><updated>2014-07-13</updated><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author></authors><title>Better Space Bounds for Parameterized Range Majority and Minority</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Karpinski and Nekrich (2008) introduced the problem of parameterized range
majority, which asks to preprocess a string of length $n$ such that, given the
endpoints of a range, one can quickly find all the distinct elements whose
relative frequencies in that range are more than a threshold $\tau$. Subsequent
authors have reduced their time and space bounds such that, when $\tau$ is
given at preprocessing time, we need either $\Oh{n \log (1 / \tau)}$ space and
optimal $\Oh{1 / \tau}$ query time or linear space and $\Oh{(1 / \tau) \log
\log \sigma}$ query time, where $\sigma$ is the alphabet size. In this paper we
give the first linear-space solution with optimal $\Oh{1 / \tau}$ query time.
For the case when $\tau$ is given at query time, we significantly improve
previous bounds, achieving either $\Oh{n \log \log \sigma}$ space and optimal
$\Oh{1 / \tau}$ query time or compressed space and $\Oh{(1 / \tau) \log
\frac{\log (1 / \tau)}{\log w}}$ query time. Along the way, we consider the
complementary problem of parameterized range minority that was recently
introduced by Chan et al.\ (2012), who achieved linear space and $\Oh{1 /
\tau}$ query time even for variable $\tau$. We improve their solution to use
either nearly optimally compressed space with no slowdown, or optimally
compressed space with nearly no slowdown. Some of our intermediate results,
such as density-sensitive query time for one-dimensional range counting, may be
of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1766</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1766</id><created>2012-10-05</created><updated>2014-02-12</updated><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Bayesian Inference with Posterior Regularization and applications to
  Infinite Latent SVMs</title><categories>cs.LG cs.AI stat.ME stat.ML</categories><comments>49 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing Bayesian models, especially nonparametric Bayesian methods, rely on
specially conceived priors to incorporate domain knowledge for discovering
improved latent representations. While priors can affect posterior
distributions through Bayes' rule, imposing posterior regularization is
arguably more direct and in some cases more natural and general. In this paper,
we present regularized Bayesian inference (RegBayes), a novel computational
framework that performs posterior inference with a regularization term on the
desired post-data posterior distribution under an information theoretical
formulation. RegBayes is more flexible than the procedure that elicits expert
knowledge via priors, and it covers both directed Bayesian networks and
undirected Markov networks whose Bayesian formulation results in hybrid chain
graph models. When the regularization is induced from a linear operator on the
posterior distributions, such as the expectation operator, we present a general
convex-analysis theorem to characterize the solution of RegBayes. Furthermore,
we present two concrete examples of RegBayes, infinite latent support vector
machines (iLSVM) and multi-task infinite latent support vector machines
(MT-iLSVM), which explore the large-margin idea in combination with a
nonparametric Bayesian model for discovering predictive latent features for
classification and multi-task learning, respectively. We present efficient
inference methods and report empirical studies on several benchmark datasets,
which appear to demonstrate the merits inherited from both large-margin
learning and Bayesian nonparametrics. Such results were not available until
now, and contribute to push forward the interface between these two important
subfields, which have been largely treated as isolated in the community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1771</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1771</id><created>2012-10-05</created><authors><author><keyname>Cetin</keyname><forenames>A. Emre</forenames></author></authors><title>In-place associative permutation sort</title><categories>cs.DS</categories><comments>25 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.0572, arXiv:1209.3668, arXiv:1209.1942, arXiv:1209.4714</comments><msc-class>68P05, 68P10</msc-class><acm-class>E.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In-place associative integer sorting technique was developed, improved and
specialized for distinct integers. The technique is suitable for integer
sorting. Hence, given a list S of n integers S[0...n-1], the technique sorts
the integers in ascending or descending order. It replaces bucket sort,
distribution counting sort and address calculation sort family of algorithms
and requires only constant amount of additional memory for storing counters and
indices beside the input list. The technique was inspired from one of the
ordinal theories of &quot;serial order in behavior&quot; and explained by the analogy
with the three main stages in the formation and retrieval of memory in
cognitive neuroscience: (i) practicing, (ii) storing and (iii) retrieval.
  In this study in-place associative permutation technique is introduced for
integer key sorting problem. Given a list S of n elements S[0...n-1] each have
an integer key in the range [0,m-1], the technique sorts the elements according
to their integer keys in O(n) time using only O(1) amount of memory if m&lt;=n. On
the other hand, if m&gt;n, it sorts in O(n+m) time for the worst, O(m) time for
the average (uniformly distributed keys) and O(n) time for the best case using
O(1) extra space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1785</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1785</id><created>2012-10-05</created><authors><author><keyname>Maher</keyname><forenames>Michael</forenames></author></authors><title>Relative Expressiveness of Defeasible Logics</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the relative expressiveness of defeasible logics in the framework
DL. Relative expressiveness is formulated as the ability to simulate the
reasoning of one logic within another logic. We show that such simulations must
be modular, in the sense that they also work if applied only to part of a
theory, in order to achieve a useful notion of relative expressiveness. We
present simulations showing that logics in DL with and without the capability
of team defeat are equally expressive. We also show that logics that handle
ambiguity differently -- ambiguity blocking versus ambiguity propagating --
have distinct expressiveness, with neither able to simulate the other under a
different formulation of expressiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1790</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1790</id><created>2012-10-05</created><updated>2013-11-15</updated><authors><author><keyname>Sheikholeslami</keyname><forenames>Azadeh</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Pishro-Nik</keyname><forenames>Hossein</forenames></author></authors><title>Everlasting Secrecy by Exploiting Non-Idealities of the Eavesdropper's
  Receiver</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure communication over a memoryless wiretap channel in the presence of a
passive eavesdropper is considered. Traditional information-theoretic security
methods require an advantage for the main channel over the eavesdropper channel
to achieve a positive secrecy rate, which in general cannot be guaranteed in
wireless systems. Here, we exploit the non-linear conversion operation in the
eavesdropper's receiver to obtain the desired advantage - even when the
eavesdropper has perfect access to the transmitted signal at the input to their
receiver. The basic idea is to employ an ephemeral cryptographic key to force
the eavesdropper to conduct two operations, at least one of which is
non-linear, in a different order than the desired recipient. Since non-linear
operations are not necessarily commutative, the desired advantage can be
obtained and information-theoretic secrecy achieved even if the eavesdropper is
given the cryptographic key immediately upon transmission completion. In
essence, the lack of knowledge of the key during the short transmission time
inhibits the recording of the signal in such a way that the secret information
can never be extracted from it. The achievable secrecy rates for different
countermeasures that the eavesdropper might employ are evaluated. It is shown
that even in the case of an eavesdropper with uniformly better conditions
(channel and receiver quality) than the intended recipient, a positive secure
rate can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1791</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1791</id><created>2012-10-05</created><authors><author><keyname>De Bock</keyname><forenames>Jasper</forenames></author><author><keyname>de Cooman</keyname><forenames>Gert</forenames></author></authors><title>An efficient algorithm for estimating state sequences in imprecise
  hidden Markov models</title><categories>cs.AI math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient exact algorithm for estimating state sequences from
outputs (or observations) in imprecise hidden Markov models (iHMM), where both
the uncertainty linking one state to the next, and that linking a state to its
output, are represented using coherent lower previsions. The notion of
independence we associate with the credal network representing the iHMM is that
of epistemic irrelevance. We consider as best estimates for state sequences the
(Walley--Sen) maximal sequences for the posterior joint state model conditioned
on the observed output sequence, associated with a gain function that is the
indicator of the state sequence. This corresponds to (and generalises) finding
the state sequence with the highest posterior probability in HMMs with precise
transition and output probabilities (pHMMs). We argue that the computational
complexity is at worst quadratic in the length of the Markov chain, cubic in
the number of states, and essentially linear in the number of maximal state
sequences. For binary iHMMs, we investigate experimentally how the number of
maximal state sequences depends on the model parameters. We also present a
simple toy application in optical character recognition, demonstrating that our
algorithm can be used to robustify the inferences made by precise probability
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1804</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1804</id><created>2012-10-05</created><updated>2013-02-17</updated><authors><author><keyname>Jurdzinski</keyname><forenames>Tomasz</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author><author><keyname>Stachowiak</keyname><forenames>Grzegorz</forenames></author></authors><title>Distributed Deterministic Broadcasting in Wireless Networks of Weak
  Devices under the SINR Model</title><categories>cs.DC</categories><comments>33 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we initiate a study of distributed deterministic broadcasting
in ad-hoc wireless networks with uniform transmission powers under the SINR
model. We design algorithms in two settings: with and without local knowledge
about immediate neighborhood. In the former setting, our solution has almost
optimal O(Dlog2 n) time cost, where n is the size of a network, D is the
eccentricity of the network and {1,...,N} is the set of possible node IDs. In
the latter case, we prove an Omega(n log N) lower bound and develop an
algorithm matching this formula, where n is the number of network nodes. As one
of the conclusions, we derive that the inherited cost of broadcasting
techniques in wireless networks is much smaller, by factor around
min{n/D,Delta}, than the cost of learning the immediate neighborhood. Finally,
we develop a O(D Delta log2 N) algorithm for the setting without local
knowledge, where Delta is the upper bound on the degree of the communication
graph of a network. This algorithm is close to a lower bound Omega(D Delta).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1840</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1840</id><created>2012-10-05</created><updated>2012-10-25</updated><authors><author><keyname>Slater</keyname><forenames>Paul B.</forenames></author></authors><title>A Further (Itakura-Saito/beta=0) Bi-stochaticization and Associated
  Clustering/Regionalization of the 3,107-County 1995-2000 U. S. Migration
  Network</title><categories>physics.soc-ph cs.SI stat.AP</categories><comments>39 pages, one 34-page dendrogram. Through further iterations of our
  heuristic, greedy algorithm, we are able to reduce the (Burg-entropy-based)
  objective function from the previously-reported 1.60316 x 10^{11} to 1.59538
  x 10^{11}. Some significant clustering changes (in the ordering of
  cosmopolitan counties) are noted</comments><msc-class>91C20, 62H30, 05C82</msc-class><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend to the beta-divergence (Itakura-Saito) case beta =0, the
comparative bi-stochaticization analyses-previously conducted (arXiv:1208.3428)
for the (Kullback-Leibler) beta=1 and (squared-Euclidean) beta = 2 cases -of
the 3,107 - county 1995-2000 U. S. migration network. A heuristic, &quot;greedy&quot;
algorithm is devised. While the largest 25,329 entries of the 735,531 non-zero
entries of the bi-stochasticized table - in the beta=1 case - are required to
complete the widely-applied two-stage (double-standardization and
strong-component hierarchical clustering) procedure, 105,363 of the 735,531 are
needed (reflective of greater uniformity of entries) in the beta=0 instance.
The North Carolina counties of Mecklenburg (Charlotte) and Wake (Raleigh) are
considerably relatively more cosmopolitan in the beta=0 study. The Colorado
county of El Paso (Colorado Springs) replaces the Florida Atlantic county of
Brevard (the &quot;Space Coast&quot;) as the most cosmopolitan, with Brevard becoming the
second-most. Honolulu County splinters away from the other four (still-grouped)
Hawaiian counties, becoming the fifth most cosmopolitan county nation-wide. The
five counties of Rhode Island remain intact as a regional entity, but the eight
counties of Connecticut fragment, leaving only five counties clustered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1841</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1841</id><created>2012-10-05</created><authors><author><keyname>Lang</keyname><forenames>John</forenames></author><author><keyname>De Sterck</keyname><forenames>Hans</forenames></author></authors><title>The Arab Spring: A Simple Compartmental Model for the Dynamics of a
  Revolution</title><categories>math.DS cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The self-immolation of Mohamed Bouazizi on December 17, 2011 in the small
Tunisian city of Sidi Bouzid, set off a sequence of events culminating in the
revolutions of the Arab Spring. It is widely believed that the Internet and
social media played a critical role in the growth and success of protests that
led to the downfall of the regimes in Egypt and Tunisia. However, the precise
mechanisms by which these new media affected the course of events remain
unclear. We introduce a simple compartmental model for the dynamics of a
revolution in a dictatorial regime such as Tunisia or Egypt which takes into
account the role of the Internet and social media. An elementary mathematical
analysis of the model identifies four main parameter regions: stable police
state, meta-stable police state, unstable police state, and failed state. We
illustrate how these regions capture, at least qualitatively, a wide range of
scenarios observed in the context of revolutionary movements by considering the
revolutions in Tunisia and Egypt, as well as the situation in Iran, China, and
Somalia, as case studies. We pose four questions about the dynamics of the Arab
Spring revolutions and formulate answers informed by the model. We conclude
with some possible directions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1885</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1885</id><created>2012-10-05</created><authors><author><keyname>Shankar</keyname><forenames>Varun</forenames></author><author><keyname>Wright</keyname><forenames>Grady B.</forenames></author><author><keyname>Fogelson</keyname><forenames>Aaron L.</forenames></author><author><keyname>Kirby</keyname><forenames>R. M.</forenames></author></authors><title>A Study of Different Modeling Choices For Simulating Platelets Within
  the Immersed Boundary Method</title><categories>math.NA cs.NA math.DG q-bio.QM</categories><comments>33 pages, 17 figures, Accepted (in press) by APNUM</comments><msc-class>76Z05 (Primary), 92B99 (Secondary)</msc-class><doi>10.1016/j.apnum.2012.09.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Immersed Boundary (IB) method is a widely-used numerical methodology for
the simulation of fluid-structure interaction problems. The IB method utilizes
an Eulerian discretization for the fluid equations of motion while maintaining
a Lagrangian representation of structural objects. Operators are defined for
transmitting information (forces and velocities) between these two
representations. Most IB simulations represent their structures with
piecewise-linear approximations and utilize Hookean spring models to
approximate structural forces. Our specific motivation is the modeling of
platelets in hemodynamic flows. In this paper, we study two alternative
representations - radial basis functions (RBFs) and Fourier-based
(trigonometric polynomials and spherical harmonics) representations - for the
modeling of platelets in two and three dimensions within the IB framework, and
compare our results with the traditional piecewise-linear approximation
methodology. For different representative shapes, we examine the geometric
modeling errors (position and normal vectors), force computation errors, and
computational cost and provide an engineering trade-off strategy for when and
why one might select to employ these different representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1890</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1890</id><created>2012-10-05</created><updated>2013-03-04</updated><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author></authors><title>Local Search is Better than Random Assignment for Bounded Occurrence
  Ordering k-CSPs</title><categories>cs.DS</categories><comments>Published at STACS 2013: Konstantin Makarychev. Local Search is
  Better than Random Assignment for Bounded Occurrence Ordering k-CSPs. STACS
  2013, pp. 139-147</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the Bounded Occurrence Ordering k-CSP Problem is not
approximation resistant. We give a very simple local search algorithm that
always performs better than the random assignment algorithm. Specifically, the
expected value of the solution returned by the algorithm is at least Alg &gt; Avg
+ a(B,k) (Opt - Avg), where &quot;Opt&quot; is the value of the optimal solution; &quot;Avg&quot;
is the expected value of the random solution; and a(B,k)=Omega_k(B^{-(k+O(1))}
is a parameter depending only on &quot;k&quot; (the arity of the CSP) and &quot;B&quot; (the
maximum number of times each variable is used in constraints). The question
whether bounded occurrence ordering k-CSPs are approximation resistant was
raised by Guruswami and Zhou (APPROX 2012) who recently showed that bounded
occurrence 3-CSPs and &quot;monotone&quot; k-CSPs admit a non-trivial approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1892</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1892</id><created>2012-10-05</created><authors><author><keyname>Cheng</keyname><forenames>Zhiyu</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author></authors><title>On Constant Gaps for the Two-way Gaussian Interference Channel</title><categories>cs.IT math.IT</categories><comments>presented at 50th Annual Allerton Conference on Communication,
  Control, and Computing, Monticello, IL, October 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the two-way Gaussian interference channel in which there are
four nodes with four independent messages: two-messages to be transmitted over
a Gaussian interference channel in the $\rightarrow$ direction, simultaneously
with two-messages to be transmitted over an interference channel (in-band,
full-duplex) in the $\leftarrow$ direction. In such a two-way network, all
nodes are transmitters and receivers of messages, allowing them to adapt
current channel inputs to previously received channel outputs. We propose two
new outer bounds on the symmetric sum-rate for the two-way Gaussian
interference channel with complex channel gains: one under full adaptation (all
4 nodes are permitted to adapt inputs to previous outputs), and one under
partial adaptation (only 2 nodes are permitted to adapt, the other 2 are
restricted). We show that simple non-adaptive schemes such as the Han and
Kobayashi scheme, where inputs are functions of messages only and not past
outputs, utilized in each direction are sufficient to achieve within a constant
gap of these fully or partially adaptive outer bounds for all channel regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1904</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1904</id><created>2012-10-05</created><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Zhang</keyname><forenames>Guanghui</forenames></author></authors><title>Self-dual Permutation Codes of Finite Groups in Semisimple Case</title><categories>cs.IT math.IT math.RT</categories><comments>The main results of the manuscript have been published in DCC listed
  below, but the manuscript contains some more detailed annalysis and arguments</comments><msc-class>94B05, 11T71</msc-class><journal-ref>Des. Codes Cryptogr. (2012) 62: 19-29</journal-ref><doi>10.1007/s10623-011-9487-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence and construction of self-dual codes in a permutation module of
a finite group for the semisimple case are described from two aspects, one is
from the point of view of the composition factors which are self-dual modules,
the other one is from the point of view of the Galois group of the coefficient
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1915</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1915</id><created>2012-10-06</created><authors><author><keyname>Li</keyname><forenames>Yuan</forenames></author></authors><title>The Limitation of Random Network Coding</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is already known that in multicast (single source, multiple sinks)
network, random linear network coding can achieve the maximum flow upper bound.
In this paper, we investigate how random linear network coding behaves in
general multi-source multi-sink case, where each sink has different demands,
and characterize all achievable rate of random linear network coding by a
simple maximum flow condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1916</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1916</id><created>2012-10-06</created><authors><author><keyname>Rahman</keyname><forenames>Meftah Ur</forenames></author></authors><title>A comparative study on face recognition techniques and neural network</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern times, face recognition has become one of the key aspects of
computer vision. There are at least two reasons for this trend; the first is
the commercial and law enforcement applications, and the second is the
availability of feasible technologies after years of research. Due to the very
nature of the problem, computer scientists, neuro-scientists and psychologists
all share a keen interest in this field. In plain words, it is a computer
application for automatically identifying a person from a still image or video
frame. One of the ways to accomplish this is by comparing selected features
from the image and a facial database. There are hundreds if not thousand
factors associated with this. In this paper some of the most common techniques
available including applications of neural network in facial recognition are
studied and compared with respect to their performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1925</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1925</id><created>2012-10-06</created><authors><author><keyname>Berisha</keyname><forenames>Artan</forenames></author><author><keyname>Baxhaku</keyname><forenames>Behar</forenames></author><author><keyname>Alidema</keyname><forenames>Artan</forenames></author></authors><title>A Class of Non Invertible Matrices in GF (2) for Practical One Way Hash
  Algorithm</title><categories>cs.CR</categories><comments>6 pages, 5 figures, &quot;Published with International Journal of Computer
  Applications (IJCA)&quot;</comments><journal-ref>Artan Berisha, Behar Baxhaku and Artan Alidema. Article: A Class
  of Non Invertible Matrices in GF(2) for Practical One Way Hash Algorithm.
  International Journal of Computer Applications 54(18):19-20, September 2012</journal-ref><doi>10.5120/8667-2574</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe non invertible matrix in GF(2)which can be used as
multiplication matrix in Hill Cipher technique for one way hash algorithm. The
matrices proposed are permutation matrices with exactly one entry 1 in each row
and each column and 0 elsewhere. Such matrices represent a permutation of m
elements. Since the invention, Hill cipher algorithm was used for symmetric
encryption, where the multiplication matrix is the key. The Hill cipher
requires the inverse of the matrix to recover the plaintext from cipher text.
We propose a class of matrices in GF(2) which are non invertible and easy to
generate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1928</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1928</id><created>2012-10-06</created><updated>2013-09-04</updated><authors><author><keyname>Vasudevan</keyname><forenames>Shrihari</forenames></author><author><keyname>Melkumyan</keyname><forenames>Arman</forenames></author><author><keyname>Scheding</keyname><forenames>Steven</forenames></author></authors><title>Information fusion in multi-task Gaussian processes</title><categories>stat.ML cs.AI cs.LG</categories><comments>53 pages, 33 figures; improved presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper evaluates heterogeneous information fusion using multi-task
Gaussian processes in the context of geological resource modeling.
Specifically, it empirically demonstrates that information integration across
heterogeneous information sources leads to superior estimates of all the
quantities being modeled, compared to modeling them individually. Multi-task
Gaussian processes provide a powerful approach for simultaneous modeling of
multiple quantities of interest while taking correlations between these
quantities into consideration. Experiments are performed on large scale real
sensor data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1931</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1931</id><created>2012-10-06</created><authors><author><keyname>Bliem</keyname><forenames>Bernhard</forenames></author><author><keyname>Morak</keyname><forenames>Michael</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>D-FLAT: Declarative Problem Solving Using Tree Decompositions and
  Answer-Set Programming</title><categories>cs.AI cs.LO</categories><comments>18 pages, 5 figures. To appear in Theory and Practice of Logic
  Programming (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose Answer-Set Programming (ASP) as a tool for rapid
prototyping of dynamic programming algorithms based on tree decompositions. In
fact, many such algorithms have been designed, but only a few of them found
their way into implementation. The main obstacle is the lack of easy-to-use
systems which (i) take care of building a tree decomposition and (ii) provide
an interface for declarative specifications of dynamic programming algorithms.
In this paper, we present D-FLAT, a novel tool that relieves the user of having
to handle all the technical details concerned with parsing, tree decomposition,
the handling of data structures, etc. Instead, it is only the dynamic
programming algorithm itself which has to be specified in the ASP language.
D-FLAT employs an ASP solver in order to compute the local solutions in the
dynamic programming algorithm. In the paper, we give a few examples
illustrating the use of D-FLAT and describe the main features of the system.
Moreover, we report experiments which show that ASP-based D-FLAT encodings for
some problems outperform monolithic ASP encodings on instances of small
treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1932</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1932</id><created>2012-10-06</created><updated>2015-12-21</updated><authors><author><keyname>Patriarca</keyname><forenames>Antonio</forenames></author><author><keyname>Scolamiero</keyname><forenames>Martina</forenames></author><author><keyname>Vaccarino</keyname><forenames>Francesco</forenames></author></authors><title>A presentation of general multipersistence modules computable in
  polynomial time?</title><categories>math.AT cs.CG math.AC</categories><comments>This paper has been overcome by Combinatorial presentation of
  multidimensional persistent homology. Wojciech Chacholski, Martina
  Scolamiero, Francesco Vaccarino. arXiv:1409.7936</comments><msc-class>55U99, 55N99, 13P10</msc-class><acm-class>I.1.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multipersistence homology modules were introduced by G.Carlsson and
A.Zomorodian which gave, together with G.Singh, an algorithm to compute their
Groebner bases. Although their algorithm has polynomial complexity when the
chain modules are free, i.e. in the one-critical case, it might be exponential
in general. We give a new presentation of multipersistence homology modules,
which allows us to design an algorithm to compute their Groebner bases always
in polynomial time by avoiding the mapping telescope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1935</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1935</id><created>2012-10-06</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author></authors><title>Saddle-Node Bifurcation Associated with Parasitic Inductor Resistance in
  Boost Converters</title><categories>cs.SY math.DS nlin.CD</categories><comments>Minor revision from a manuscript dated in 2010</comments><journal-ref>Some parts of this manuscript are published in International
  Journal of Electronics, 100(8), pp. 1147-1174, Aug. 2013</journal-ref><doi>10.1080/00207217.2012.743072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Saddle-node bifurcation occurs in a boost converter when parasitic inductor
resistance is modeled. Closed-form critical conditions of the bifurcation are
derived. If the parasitic inductor resistance is modeled, the saddle-node
bifurcation occurs in the voltage mode control or in the current mode control
with the voltage loop closed, but not in the current mode control with the
voltage loop open. If the parasitic inductor resistance is not modeled, the
saddle-node bifurcation does not occur, and one may be misled by the wrong
dynamics and the wrong steady-state solutions. The saddle-node bifurcation
still exists even in a boost converter with a popular type-III compensator.
When the saddle-node bifurcation occurs, multiple steady-state solutions may
coexist. The converter may operate with a voltage jump from one solution to
another. Care should be taken in the compensator design to ensure that only the
desired solution is stabilized. In industry practice, the solution with a
higher duty cycle (and thus the saddle-node bifurcation) may be prevented by
placing a limitation on the maximum duty cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1940</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1940</id><created>2012-10-06</created><authors><author><keyname>Magamba</keyname><forenames>Kondwani</forenames></author><author><keyname>Kadaleka</keyname><forenames>Solomon</forenames></author><author><keyname>Kasambara</keyname><forenames>Ansley</forenames></author></authors><title>Variable-length Hill Cipher with MDS Key Matrix</title><categories>cs.CR cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Hill Cipher is a classical symmetric cipher which breaks plaintext into
blocks of size m and then multiplies each block by an m by m key matrix to
yield ciphertext. However, it is well known that the Hill cipher succumbs to
cryptanalysis relatively easily. As a result, there have been efforts to
strengthen the cipher through the use of various techniques e.g. permuting rows
and columns of the key matrix to encrypt each plaintext vector with a new key
matrix. In this paper, we strengthen the security of the Hill cipher against a
known-plaintext attack by encrypting each plaintext matrix by a variable-length
key matrix obtained from a Maximum Distance Separable (MDS) master key matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1959</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1959</id><created>2012-10-06</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author></authors><title>Modeling and Instability of Average Current Control</title><categories>cs.SY math.DS nlin.CD</categories><comments>Published in the Proceedings of EPE International Power Electronics
  And Motion Control Conference, Cavtat &amp; Dubrovnik, Croatia, September 9-11,
  2002, paper number SSIN-03, 10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamics and stability of average current control of DC-DC converters are
analyzed by sampled-data modeling. Orbital stability is studied and it is found
unrelated to the ripple size of the orbit. Compared with the averaged modeling,
the sampled-data modeling is more accurate and systematic. An unstable range of
compensator pole is found by simulations, and is predicted by sampled-data
modeling and harmonic balance modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1960</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1960</id><created>2012-10-06</created><authors><author><keyname>Jitkrittum</keyname><forenames>Wittawat</forenames></author><author><keyname>Hachiya</keyname><forenames>Hirotaka</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Feature Selection via L1-Penalized Squared-Loss Mutual Information</title><categories>stat.ML cs.LG</categories><comments>25 pages</comments><doi>10.1587/transinf.E96.D.1513</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection is a technique to screen out less important features. Many
existing supervised feature selection algorithms use redundancy and relevancy
as the main criteria to select features. However, feature interaction,
potentially a key characteristic in real-world problems, has not received much
attention. As an attempt to take feature interaction into account, we propose
L1-LSMI, an L1-regularization based algorithm that maximizes a squared-loss
variant of mutual information between selected features and outputs. Numerical
results show that L1-LSMI performs well in handling redundancy, detecting
non-linear dependency, and considering feature interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1975</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1975</id><created>2012-10-06</created><authors><author><keyname>Zheng</keyname><forenames>Bojin</forenames></author><author><keyname>Huang</keyname><forenames>Dan</forenames></author><author><keyname>Li</keyname><forenames>Deyi</forenames></author><author><keyname>Chen</keyname><forenames>Guisheng</forenames></author><author><keyname>Lan</keyname><forenames>Wenfei</forenames></author></authors><title>Some scale-free networks could be robust under the selective node
  attacks</title><categories>physics.soc-ph cs.NI cs.SI</categories><journal-ref>Bojin Zheng, Dan Huang, Deyi Li, Guisheng Chen and Wenfei Lan.
  Some scale-free networks could be robust under the selective node attacks.
  Europhysics Letter. 2011, 94: 028010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a mainstream idea that scale-free network would be fragile under the
selective attacks. Internet is a typical scale-free network in the real world,
but it never collapses under the selective attacks of computer viruses and
hackers. This phenomenon is different from the deduction of the idea above
because this idea assumes the same cost to delete an arbitrary node. Hence this
paper discusses the behaviors of the scale-free network under the selective
node attack with different cost. Through the experiments on five complex
networks, we show that the scale-free network is possibly robust under the
selective node attacks; furthermore, the more compact the network is, and the
larger the average degree is, then the more robust the network is; With the
same average degrees, the more compact the network is, the more robust the
network is. This result would enrich the theory of the invulnerability of the
network, and can be used to build the robust social, technological and
biological networks, and also has the potential to find the target of drugs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1980</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1980</id><created>2012-10-06</created><authors><author><keyname>Duclos-Cianci</keyname><forenames>Guillaume</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M.</forenames></author></authors><title>A State Distillation Protocol to Implement Arbitrary Single-qubit
  Rotations</title><categories>quant-ph cs.ET</categories><comments>10 pages, 18 figures, 5 tables</comments><journal-ref>Phys. Rev. A 88, 042325 (2013)</journal-ref><doi>10.1103/PhysRevA.88.042325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important task required to build a scalable, fault-tolerant quantum
computer is to efficiently represent an arbitrary single-qubit rotation by
fault-tolerant quantum operations. Traditionally, the method for decomposing a
single-qubit unitary into a discrete set of gates is Solovay-Kitaev
decomposition, which in practice produces a sequence of depth
O(\log^c(1/\epsilon)), where c~3.97 is the state-of-the-art. The proven lower
bound is c=1, however an efficient algorithm that saturates this bound is
unknown. In this paper, we present an alternative to Solovay-Kitaev
decomposition employing state distillation techniques which reduces c to
between 1.12 and 2.27, depending on the setting. For a given single-qubit
rotation, our protocol significantly lowers the length of the approximating
sequence and the number of required resource states (ancillary qubits). In
addition, our protocol is robust to noise in the resource states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1983</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1983</id><created>2012-10-06</created><authors><author><keyname>Aur</keyname><forenames>Dorian</forenames></author></authors><title>Reply to Comments on Neuroelectrodynamics: Where are the Real Conceptual
  Pitfalls?</title><categories>cs.NE nlin.AO physics.bio-ph q-bio.NC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The fundamental, powerful process of computation in the brain has been widely
misunderstood. The paper [1] associates the general failure to build
intelligent thinking machines with current reductionist principles of temporal
coding and advocates for a change in paradigm regarding the brain analogy.
Since fragments of information are stored in proteins which can shift between
several structures to perform their function, the biological substrate is
actively involved in physical computation. The intrinsic nonlinear dynamics of
action potentials and synaptic activities maintain physical interactions within
and between neurons in the brain. During these events the required information
is exchanged between molecular structures (proteins) which store fragments of
information and the generated electric flux which carries and integrates
information in the brain. The entire process of physical interaction explains
how the brain actively creates or experiences meaning. This process of
interaction during an action potential generation can be simply seen as the
moment when the neuron solves a many-body problem. A neuroelectrodynamic theory
shows that the neuron solves equations rather than exclusively computes
functions. With the main focus on temporal patterns, the spike timing dogma
(STD) has neglected important forms of computation which do occur inside
neurons. In addition, artificial neural models have missed the most important
part since the real super-computing power of the brain has its origins in
computations that occur within neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1988</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1988</id><created>2012-10-06</created><updated>2012-10-22</updated><authors><author><keyname>Hernandez-Velez</keyname><forenames>Cesar</forenames></author><author><keyname>Medina</keyname><forenames>Carolina</forenames></author><author><keyname>Salazar</keyname><forenames>Gelasio</forenames></author></authors><title>The optimal drawings of K_{5,n}</title><categories>math.CO cs.CG</categories><comments>In the previous version the bibliography was missing. Otherwise this
  is identical as Version 1</comments><msc-class>05C10, 05C62, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zarankiewicz's Conjecture (ZC) states that the crossing number cr$(K_{m,n})$
equals $Z(m,n):=\floor{\frac{m}{2}} \floor{\frac{m-1}{2}} \floor{\frac{n}{2}}
\floor{\frac{n-1}{2}}$. Since Kleitman's verification of ZC for $K_{5,n}$ (from
which ZC for $K_{6,n}$ easily follows), very little progress has been made
around ZC; the most notable exceptions involve computer-aided results. With the
aim of gaining a more profound understanding of this notoriously difficult
conjecture, we investigate the optimal (that is, crossing-minimal) drawings of
$K_{5,n}$. The widely known natural drawings of $K_{m,n}$ (the so-called
Zarankiewicz drawings) with $Z(m,n)$ crossings contain antipodal vertices, that
is, pairs of degree-$m$ vertices such that their induced drawing of $K_{m,2}$
has no crossings. Antipodal vertices also play a major role in Kleitman's
inductive proof that cr$(K_{5,n}) = Z(5,n)$. We explore in depth the role of
antipodal vertices in optimal drawings of $K_{5,n}$, for $n$ even. We prove
that if {$n \equiv 2$ (mod 4)}, then every optimal drawing of $K_{5,n}$ has
antipodal vertices. We also exhibit a two-parameter family of optimal drawings
$D_{r,s}$ of $K_{5,4(r+s)}$ (for $r,s\ge 0$), with no antipodal vertices, and
show that if $n\equiv 0$ (mod 4), then every optimal drawing of $K_{5,n}$
without antipodal vertices is (vertex rotation) isomorphic to $D_{r,s}$ for
some integers $r,s$. As a corollary, we show that if $n$ is even, then every
optimal drawing of $K_{5,n}$ is the superimposition of Zarankiewicz drawings
with a drawing isomorphic to $D_{r,s}$ for some nonnegative integers $r,s$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.1996</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.1996</id><created>2012-10-06</created><updated>2013-04-11</updated><authors><author><keyname>Halu</keyname><forenames>Arda</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author></authors><title>Connect and win: The role of social networks in political elections</title><categories>physics.soc-ph cs.SI</categories><comments>(5 pages, 5 figures)</comments><journal-ref>EPL 102 (2013) 16002</journal-ref><doi>10.1209/0295-5075/102/16002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many networks do not live in isolation but are strongly interacting, with
profound consequences on their dynamics. Here, we consider the case of two
interacting social networks and, in the context of a simple model, we address
the case of political elections. Each network represents a competing party and
every agent on the election day can choose to be either active in one of the
two networks (vote for the corresponding party) or to be inactive in both (not
vote). The opinion dynamics during the election campaign is described through a
simulated annealing algorithm. We find that for a large region of the parameter
space the result of the competition between the two parties allows for the
existence of pluralism in the society, where both parties have a finite share
of the votes. The central result is that a densely connected social network is
key for the final victory of a party. However, small committed minorities can
play a crucial role, and even reverse the election outcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2005</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2005</id><created>2012-10-06</created><authors><author><keyname>Al-Fedaghi</keyname><forenames>Sabah</forenames></author></authors><title>Diagrammatization of the Transmission Control Protocol</title><categories>cs.NI cs.CR</categories><comments>9 pages, 10 figures</comments><journal-ref>International Journal of Computer Science Issues, Volume 9, Issue
  5, September, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the wide spread of Internet services, developers and users need a
greater understanding of the technology of networking. Acquiring a clear
understanding of communication protocols is an important step in understanding
how a network functions; however, many protocols are complicated, and
explaining them can be demanding. In addition, protocols are often explained in
terms of traffic analysis and oriented toward technical staff and those already
familiar with network protocols. This paper aims at proposing a diagrammatic
methodology to represent protocols in general, with a focus on the Transmission
Control Protocol and Secure Sockets Layer in particular. The purpose is to
facilitate understanding of protocols for learning and communication purposes.
The methodology is based on the notion of flow of primitive things in a system
with six stages: creation, release, transfer, arrival, acceptance, and
processing. Though the method presents a basic description of protocols without
in-depth analysis of all aspects and mechanisms, the resultant conceptual
description is a systematic specification that utilizes a few basic notions
that assist in illustrating functionality and support comprehension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2013</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2013</id><created>2012-10-06</created><authors><author><keyname>Prpic</keyname><forenames>John</forenames></author><author><keyname>Shukla</keyname><forenames>Prashant</forenames></author></authors><title>The Theory of Crowd Capital</title><categories>cs.CY</categories><comments>Crowd Capital, Crowd Capability, KBV, RBV, Heterogeneous Knowledge,
  Dispersed Knowledge, Hayek, IS, Social Capital, Intellectual Capital,
  Crowdsourcing, Prediction Markets, Peer Production, Open Innovation, Wisdom
  of the Crowds, Collective Intelligence, Wikis, Wikipedia, Innovation
  Communities, ReCaptcha, Gamification, Citizen Science, Innocentive,
  GalaxyZoo, SETILive, M-Turk, FoldIt, HICSS #46</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are seeing more and more organizations undertaking activities to engage
dispersed populations through IS. Using the knowledge-based view of the
organization, this work conceptualizes a theory of Crowd Capital to explain
this phenomenon. Crowd Capital is a heterogeneous knowledge resource generated
by an organization, through its use of Crowd Capability, which is defined by
the structure, content, and process by which an organization engages with the
dispersed knowledge of individuals (the Crowd). Our work draws upon a diverse
literature and builds upon numerous examples of practitioner implementations to
support our theorizing. We present a model of Crowd Capital generation in
organizations and discuss the implications of Crowd Capital on organizational
boundary and on IS research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2018</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2018</id><created>2012-10-06</created><updated>2013-03-22</updated><authors><author><keyname>Zhang</keyname><forenames>Zhong-Yuan</forenames></author></authors><title>Community Structure Detection in Complex Networks with Partial
  Background Information</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Zhong-Yuan Zhang 2013 EPL 101 48005</journal-ref><doi>10.1209/0295-5075/101/48005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constrained clustering has been well-studied in the unsupervised learning
society. However, how to encode constraints into community structure detection,
within complex networks, remains a challenging problem. In this paper, we
propose a semi-supervised learning framework for community structure detection.
This framework implicitly encodes the must-link and cannot-link constraints by
modifying the adjacency matrix of network, which can also be regarded as
de-noising the consensus matrix of community structures. Our proposed method
gives consideration to both the topology and the functions (background
information) of complex network, which enhances the interpretability of the
results. The comparisons performed on both the synthetic benchmarks and the
real-world networks show that the proposed framework can significantly improve
the community detection performance with few constraints, which makes it an
attractive methodology in the analysis of complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2019</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2019</id><created>2012-10-07</created><updated>2013-04-25</updated><authors><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author></authors><title>On the relation of nonanticipative rate distortion function and
  filtering theory</title><categories>cs.IT cs.SY math.IT</categories><comments>6 pages, 4 figures, final version submitted for publication at 12th
  Biannual European Control Conference (ECC), 2013</comments><msc-class>60B11, 37A50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the relation between nonanticipative rate distortion function
(RDF) and Bayesian filtering theory is investigated using the topology of weak
convergence of probability measures on Polish spaces. The relation is
established via an optimization on the space of conditional distributions of
the so-called directed information subject to fidelity constraints. Existence
of the optimal reproduction distribution of the nonanticipative RDF is shown,
while the optimal nonanticipative reproduction conditional distribution for
stationary processes is derived in closed form. The realization procedure of
nonanticipative RDF which is equivalent to joint-source channel matching for
symbol-by-symbol transmission is described, while an example is introduced to
illustrate the concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2023</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2023</id><created>2012-10-07</created><authors><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author><author><keyname>Elleithy</keyname><forenames>Khalid</forenames></author><author><keyname>Salama</keyname><forenames>Nyembo</forenames></author></authors><title>Novel Framework for Mobile Collaborative learning (MCL)to substantiate
  pedagogical activities</title><categories>cs.CY</categories><comments>8 pages, 5 figure IADIS International Conference on Internet
  Technologies &amp; Society 2011</comments><journal-ref>International Association for Development of the Information
  Society 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latest study shows that MCL is highly focusing paradigm for research
particularity in distance and online education. MCL provides some features and
functionalities for all participants to obtain the knowledge. Deployment of new
emerging technologies and fast growing trends toward MCL boom attract people to
develop learning management system, virtual learning environment and conference
system with support of MCL. All these environments lack the most promising
supportive framework. In addition some of major challenges in open, large
scale, dynamic and heterogeneous environments are not still handled in
developing MCL for education and other organizations. These issues includes
such as knowledge sharing, faster delivery of contents, request for modified
contents, complete access to enterprise data warehouse, delivery of large rich
multimedia contents (video-on-demand), asynchronous collaboration, synchronous
collaboration, support for multi model, provision for archive updating, user
friendly interface, middleware support and virtual support. To overcome these
issues; the paper introduces novel framework for MCL consisting of four layers
with many promising functional components, which provide access to users for
obtaining required contents from enterprise data warehouse (EDW). Novel
framework provides information regarding the course materials, easy access to
check the grades and use of labs. The applications running on this framework
give substantial feedback for collaboration such as exchange for delivery of
communication contents including platform for group discussion, short message
service (SMS), emails, audio, video and video-on-demand to obtain on-line
information to students and other persons who will be part of collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2025</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2025</id><created>2012-10-07</created><authors><author><keyname>Elmannai</keyname><forenames>Wafa</forenames></author><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author><author><keyname>Elleithy</keyname><forenames>Khaled</forenames></author></authors><title>TCP-UB: A New congestion aware transmission control protocolvariant</title><categories>cs.NI</categories><comments>13 Pages, 11 figures</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.4, No.4, July 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission control protocol (TCP) is a connection oriented protocol for
several types of distributed applications. TCP is reliable particularly for
traditional fixed networks. With emergence of faster wireless networks, TCP has
been performing poorly in its original format. The performance of TCP is
affected due to assorted factors including congestion window, maximum packet
size, retry limit, recovery mechanism, backup mechanism and mobility. To
overcome deficiency of original TCP, Several modifications have been introduced
to improve network quality. The mobility is a major hurdle in degrading the
performance of mobile wireless networks. In this paper, we introduce and
implement new TCP variant University of Bridgeport (UB) that combines the
features of TCP Westwood and Vegas. We examine the performance of TCP-UB, Vegas
and Westwood using different realistic scenarios. NS2 simulator demonstrates
the stability of TCP-UB as compared with TCP Vegas and Westwood in highly
congested networks from the mobility point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2028</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2028</id><created>2012-10-07</created><authors><author><keyname>Faella</keyname><forenames>Marco</forenames><affiliation>Universit&#xe0; di Napoli &quot;Federico II&quot;</affiliation></author><author><keyname>Murano</keyname><forenames>Aniello</forenames><affiliation>Universit&#xe0; di Napoli &quot;Federico II&quot;</affiliation></author></authors><title>Proceedings Third International Symposium on Games, Automata, Logics and
  Formal Verification</title><categories>cs.LO cs.GT</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012</journal-ref><doi>10.4204/EPTCS.96</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Third International Symposium on
Games, Automata, Logic and Formal Verification (GandALF), held in Naples
(Italy) from September 6th to 8th, 2012.
  GandALF was founded by a number of Italian computer scientists interested in
mathematical logic, automata theory, game theory, and their applications to the
specification, design, and verification of complex systems. Its aim is to
provide a forum where people from different areas, and possibly with different
backgrounds, can fruitfully interact. Even though the idea of the symposium
emerged within the Italian research community, the event has a truly
international nature, as witnessed by the composition of the conference
committees and the programme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2030</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2030</id><created>2012-10-07</created><authors><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author><author><keyname>Elleithy</keyname><forenames>Khaled</forenames></author></authors><title>Multi-frame Signature-cum Anomaly-based Intrusion Detection Systems
  (MSAIDS) to Protect Privacy of Users over Mobile Collaborative Learning (MCL)</title><categories>cs.CR</categories><comments>20 pages, 9 figures International Journal of E -Technology Volume 3
  Number 2 May 2012. arXiv admin note: substantial text overlap with
  arXiv:1208.2073</comments><journal-ref>International Journal of E -Technology Volume 3 Number 2 May 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rogue DHCP is unauthorized server that releases the incorrect IP address
to users and sniffs the traffic illegally. The contribution specially provides
privacy to users and enhances the security aspects of mobile supported
collaborative framework (MSCF) explained in [24].The paper introduces
multi-frame signature-cum anomaly-based intrusion detection systems (MSAIDS)
supported with novel algorithms and inclusion of new rules in existing IDS. The
major target of contribution is to detect the malicious attacks and blocks the
illegal activities of rogue DHCP server. This innovative security mechanism
reinforces the confidence of users, protects network from illicit intervention
and restore the privacy of users. Finally, the paper validates the idea through
simulation and compares the findings with known existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2035</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2035</id><created>2012-10-07</created><authors><author><keyname>Wiltsche</keyname><forenames>Clemens</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author><author><keyname>Murray</keyname><forenames>Richard M.</forenames></author></authors><title>Synthesis of Reactive Protocols for Vehicle-to-Vehicle Communication</title><categories>cs.SY</categories><comments>Technical report for the paper with the same title prepared for
  submission to ICCPS'13, April 8--11, 2013, Philadelphia, PA, USA</comments><acm-class>C.2.2; B.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a synthesis method for communication protocols for active safety
applications that satisfy certain formal specifications on quality of service
requirements. The protocols are developed to provide reliable communication
services for automobile active safety applications. The synthesis method
transforms a specification into a distributed implementation of senders and
receivers that together satisfy the quality of service requirements by
transmitting messages over an unreliable medium. We develop a specification
language and an execution model for the implementations, and demonstrate the
viability of our method by developing a protocol for a traffic scenario in
which a car runs a red light at a busy intersection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2039</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2039</id><created>2012-10-07</created><updated>2013-01-10</updated><authors><author><keyname>Czajka</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Partiality and Recursion in Higher-order Logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an illative system I_s of classical higher-order logic with
subtyping and basic inductive types. The system I_s allows for direct
definitions of partial and general recursive functions, and provides means for
handling functions whose termination has not been proven. We give examples of
how properties of some recursive functions may be established in our system. In
a technical appendix to the paper we prove consistency of I_s. The proof is by
model construction. We then use this construction to show conservativity of I_s
over classical first-order logic. Conservativity over higher-order logic is
conjectured, but not proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2047</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2047</id><created>2012-10-07</created><updated>2012-11-29</updated><authors><author><keyname>Zhang</keyname><forenames>Miranda</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Nepal</keyname><forenames>Surya</forenames></author><author><keyname>Menzel</keyname><forenames>Michael</forenames></author><author><keyname>Haller</keyname><forenames>Armin</forenames></author></authors><title>A Declarative Recommender System for Cloud Infrastructure Services
  Selection</title><categories>cs.DC</categories><comments>Supplement document to the conference paper accepted at the 9th
  International Conference on Economics of Grids, Clouds, Systems, and
  Services, Berlin, Germany, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud infrastructure services landscape advances steadily leaving users
in the agony of choice...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2051</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2051</id><created>2012-10-07</created><updated>2013-02-09</updated><authors><author><keyname>Beros</keyname><forenames>Achilles</forenames></author></authors><title>Anomalous Vacillatory Learning</title><categories>math.LO cs.LG cs.LO</categories><comments>6 Pages</comments><msc-class>03D80, 68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1986, Osherson, Stob and Weinstein asked whether two variants of anomalous
vacillatory learning, TxtFex^*_* and TxtFext^*_*, could be distinguished. In
both, a machine is permitted to vacillate between a finite number of hypotheses
and to make a finite number of errors. TxtFext^*_*-learning requires that
hypotheses output infinitely often must describe the same finite variant of the
correct set, while TxtFex^*_*-learning permits the learner to vacillate between
finitely many different finite variants of the correct set. In this paper we
show that TxtFex^*_* \neq TxtFext^*_*, thereby answering the question posed by
Osherson, \textit{et al}. We prove this in a strong way by exhibiting a family
in TxtFex^*_2 \setminus {TxtFext}^*_*.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2067</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2067</id><created>2012-10-07</created><updated>2013-01-18</updated><authors><author><keyname>Bocus</keyname><forenames>Mohammud</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl</forenames></author><author><keyname>Coon</keyname><forenames>Justin</forenames></author></authors><title>An Approximation of the First Order Marcum $Q$-Function with Application
  to Network Connectivity Analysis</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An exponential-type approximation of the first order Marcum $Q$-function is
presented, which is robust to changes in its first argument and can easily be
integrated with respect to the second argument. Such characteristics are
particularly useful in network connectivity analysis. The proposed
approximation is exact in the limit of small first argument of the Marcum
$Q$-function, in which case the optimal parameters can be obtained
analytically. For larger values of the first argument, an optimization problem
is solved, and the parameters can be accurately represented using regression
analysis. Numerical results indicate that the proposed methods result in
approximations very close to the actual Marcum $Q$-function for small and
moderate values of the first argument. We demonstrate the accuracy of the
approximation by using it to analyze the connectivity properties of random ad
hoc networks operating in a Rician fading environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2085</identifier>
 <datestamp>2013-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2085</id><created>2012-10-07</created><updated>2013-10-10</updated><authors><author><keyname>Duchi</keyname><forenames>John C.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Privacy Aware Learning</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>60 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study statistical risk minimization problems under a privacy model in
which the data is kept confidential even from the learner. In this local
privacy framework, we establish sharp upper and lower bounds on the convergence
rates of statistical estimation procedures. As a consequence, we exhibit a
precise tradeoff between the amount of privacy the data preserves and the
utility, as measured by convergence rate, of any statistical estimator or
learning procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2089</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2089</id><created>2012-10-07</created><authors><author><keyname>Bernard</keyname><forenames>Alain</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Delplace</keyname><forenames>Jean-Charles</forenames><affiliation>Cetim</affiliation></author><author><keyname>Perry</keyname><forenames>Nicolas</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Gabriel</keyname><forenames>Serge</forenames></author></authors><title>Integration of CAD and rapid manufacturing for sand casting optimisation</title><categories>cs.OH</categories><proxy>ccsd</proxy><doi>10.1108/13552540310502220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to reduce the time and costs of the products development in the sand
casting process, the SMC Colombier Fontaine company has carried out a study
based on tooling manufacturing with a new rapid prototyping process. This
evolution allowed the adequacy of the geometry used for the simulation to the
tooling employed physically in the production. This allowed a reduction of the
wall thickness to 4mm and retained reliable manufacturing process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2090</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2090</id><created>2012-10-07</created><authors><author><keyname>Preez</keyname><forenames>Niek Du</forenames><affiliation>GCC</affiliation></author><author><keyname>Perry</keyname><forenames>Nicolas</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Candlot</keyname><forenames>Alexandre</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Bernard</keyname><forenames>Alain</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Uys</keyname><forenames>Wilhelm</forenames><affiliation>GCC</affiliation></author><author><keyname>Louw</keyname><forenames>Louis</forenames><affiliation>GCC</affiliation></author></authors><title>Customised high-value document generation</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>CIRP Annals 54, 1 (2005) 123-126</journal-ref><doi>10.1016/S0007-8506(07)60064-X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contributions of different experts to innovation projects improve enterprise
value, captured in documents. A subset of them is the centre of expert
constraint convergence. Their production needs to be tailored case by case.
Documents are often considered as knowledge transcription. As the base of a
structured knowledge-based information environment, this paper presents a
global approach that helps knowledge-integration tool deployment. An example,
based on process plan in aircraft manufacturing, indicates how fundamental
understanding of domain infrastructure contributes to a more coherent
architecture of knowledge-based information environments. A comparison with an
experiment in insurance services generalised the application of presented
principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2091</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2091</id><created>2012-10-07</created><authors><author><keyname>Mauchand</keyname><forenames>Magali</forenames><affiliation>UTT</affiliation></author><author><keyname>Siadat</keyname><forenames>Ali</forenames><affiliation>LGM2B</affiliation></author><author><keyname>Perry</keyname><forenames>Nicolas</forenames><affiliation>LGM2B</affiliation></author><author><keyname>Bernard</keyname><forenames>Alain</forenames><affiliation>IRCCyN</affiliation></author></authors><title>VCS: Value Chains Simulator, a Tool for Value Analysis of Manufacturing
  Enterprise Processes (A Value-Based Decision Support Tool)</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>Journal of Intelligent Manufacturing (2010) 700x-700x</journal-ref><doi>10.1007/s10845-010-0452</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manufacturing enterprises are facing a competitive challenge. This paper
proposes the use of a value chain based approach to support the modelling and
simulation of manufacturing enterprise processes. The aim is to help experts to
make relevant decisions on product design and/or product manufacturing process
planning. This decision tool is based on the value chain modelling, by
considering the product requirements. In order to evaluate several performance
indicators, a simulation of various potential value chains adapted to market
demand was conducted through a Value Chains Simulator (VCS). A discrete event
simulator is used to perform the simulation of these scenarios and to evaluate
the value as a global performance criterion (balancing cost, quality, delivery
time, services, etc.). An Analytical Hierarchy Process module supports the
analysis process. The value chain model is based on activities and uses the
concepts of resource consumption, while integrating the benefiting entities
view point. A case study in the microelectronic field is carried out to
corroborate the validity of the proposed VCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2094</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2094</id><created>2012-10-07</created><updated>2013-09-05</updated><authors><author><keyname>Ilik</keyname><forenames>Danko</forenames></author></authors><title>Type Directed Partial Evaluation for Level-1 Shift and Reset</title><categories>cs.PL</categories><comments>In Proceedings COS 2013, arXiv:1309.0924</comments><proxy>EPTCS</proxy><acm-class>F.3.1, F.3.2, F.3.3</acm-class><journal-ref>EPTCS 127, 2013, pp. 86-100</journal-ref><doi>10.4204/EPTCS.127.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an implementation in the Coq proof assistant of type directed
partial evaluation (TDPE) algorithms for call-by-name and call-by-value
versions of shift and reset delimited control operators, and in presence of
strong sum types. We prove that the algorithm transforms well-typed programs to
ones in normal form. These normal forms can not always be arrived at using the
so far known equational theories. The typing system does not allow answer-type
modification for function types and allows delimiters to be set on at most one
atomic type. The semantic domain for evaluation is expressed in Constructive
Type Theory as a dependently typed monadic structure combining Kripke models
and continuation passing style translations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2107</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2107</id><created>2012-10-07</created><updated>2013-03-25</updated><authors><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Brannstrom</keyname><forenames>Fredrik</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>On Optimal TCM Encoders</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Communications, vol. 61, no. 6, pp.
  2178-2189, June 2013</journal-ref><doi>10.1109/TCOMM.2013.042313.120760</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An asymptotically optimal trellis-coded modulation (TCM) encoder requires the
joint design of the encoder and the binary labeling of the constellation. Since
analytical approaches are unknown, the only available solution is to perform an
exhaustive search over the encoder and the labeling. For large constellation
sizes and/or many encoder states, however, an exhaustive search is unfeasible.
Traditional TCM designs overcome this problem by using a labeling that follows
the set-partitioning principle and by performing an exhaustive search over the
encoders. In this paper we study binary labelings for TCM and show how they can
be grouped into classes, which considerably reduces the search space in a joint
design. For 8-ary constellations, the number of different binary labelings that
must be tested is reduced from 8!=40320 to 240. For the particular case of an
8-ary pulse amplitude modulation constellation, this number is further reduced
to 120 and for 8-ary phase shift keying to only 30. An algorithm to generate
one labeling in each class is also introduced. Asymptotically optimal TCM
encoders are tabulated which are up to 0.3 dB better than the previously best
known encoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2110</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2110</id><created>2012-10-07</created><authors><author><keyname>Olmez</keyname><forenames>Oktay</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>Repairable Replication-based Storage Systems Using Resolvable Designs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the design of regenerating codes for distributed storage systems
at the minimum bandwidth regeneration (MBR) point. The codes allow for a repair
process that is exact and uncoded, but table-based. These codes were introduced
in prior work and consist of an outer MDS code followed by an inner fractional
repetition (FR) code where copies of the coded symbols are placed on the
storage nodes. The main challenge in this domain is the design of the inner FR
code.
  In our work, we consider generalizations of FR codes, by establishing their
connection with a family of combinatorial structures known as resolvable
designs. Our constructions based on affine geometries, Hadamard designs and
mutually orthogonal Latin squares allow the design of systems where a new node
can be exactly regenerated by downloading $\beta \geq 1$ packets from a subset
of the surviving nodes (prior work only considered the case of $\beta = 1$).
Our techniques allow the design of systems over a large range of parameters.
Specifically, the repetition degree of a symbol, which dictates the resilience
of the system can be varied over a large range in a simple manner. Moreover,
the actual table needed for the repair can also be implemented in a rather
straightforward way. Furthermore, we answer an open question posed in prior
work by demonstrating the existence of codes with parameters that are not
covered by Steiner systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2122</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2122</id><created>2012-10-07</created><authors><author><keyname>Ashkiani</keyname><forenames>Saman</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>Discrete Dithered Desynchronization</title><categories>cs.NI</categories><comments>submitted to the 32th IEEE International Conference on Computer
  Communications, IEEE INFOCOM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Discrete Dithered Desynchronization (D3sync)
algorithm which is a decentralized Time Division Multiple Access (TDMA)
technique in which a set of network nodes computes iteratively a conflict-free
schedule so that each node obtains a portion of a frame that is an integer
multiple of a fixed slot size. The algorithm is inspired by the dynamics of
Pulse Coupled Oscillators (PCO), but unlike its predecessors that divide
arbitrarily the frame among the nodes in the network, the D3sync allocates
discrete resources among the network nodes.
  Our paper proves the convergence of the D3sync algorithm and gives an upper
bound on the convergence time of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2123</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2123</id><created>2012-10-07</created><authors><author><keyname>Calmon</keyname><forenames>Flavio du Pin</forenames></author><author><keyname>Fawaz</keyname><forenames>Nadia</forenames></author></authors><title>Privacy Against Statistical Inference</title><categories>cs.IT cs.CR math.IT</categories><comments>Allerton 2012, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general statistical inference framework to capture the privacy
threat incurred by a user that releases data to a passive but curious
adversary, given utility constraints. We show that applying this general
framework to the setting where the adversary uses the self-information cost
function naturally leads to a non-asymptotic information-theoretic approach for
characterizing the best achievable privacy subject to utility constraints.
Based on these results we introduce two privacy metrics, namely average
information leakage and maximum information leakage. We prove that under both
metrics the resulting design problem of finding the optimal mapping from the
user's data to a privacy-preserving output can be cast as a modified
rate-distortion problem which, in turn, can be formulated as a convex program.
Finally, we compare our framework with differential privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2125</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2125</id><created>2012-10-07</created><authors><author><keyname>Su</keyname><forenames>Guoxin</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author><author><keyname>Zhang</keyname><forenames>Chengqi</forenames></author></authors><title>Session Communication and Integration</title><categories>cs.PL cs.SE</categories><comments>A short version of this paper is submitted for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scenario-based specification of a large distributed system is usually
naturally decomposed into various modules. The integration of specification
modules contrasts to the parallel composition of program components, and
includes various ways such as scenario concatenation, choice, and nesting. The
recent development of multiparty session types for process calculi provides
useful techniques to accommodate the protocol modularisation, by encoding
fragments of communication protocols in the usage of private channels for a
class of agents. In this paper, we extend forgoing session type theories by
enhancing the session integration mechanism. More specifically, we propose a
novel synchronous multiparty session type theory, in which sessions are
separated into the communicating and integrating levels. Communicating sessions
record the message-based communications between multiple agents, whilst
integrating sessions describe the integration of communicating ones. A
two-level session type system is developed for pi-calculus with syntactic
primitives for session establishment, and several key properties of the type
system are studied. Applying the theory to system description, we show that a
channel safety property and a session conformance property can be analysed.
Also, to improve the utility of the theory, a process slicing method is used to
help identify the violated sessions in the type checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2126</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2126</id><created>2012-10-07</created><authors><author><keyname>Calmon</keyname><forenames>Flavio du Pin</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author><author><keyname>Zeger</keyname><forenames>Linda M.</forenames></author><author><keyname>Barros</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Christiansen</keyname><forenames>Mark M.</forenames></author><author><keyname>Duffy</keyname><forenames>Ken. R.</forenames></author></authors><title>Lists that are smaller than their parts: A coding approach to tunable
  secrecy</title><categories>cs.IT cs.CR math.IT</categories><comments>Allerton 2012, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new information-theoretic definition and associated results,
based on list decoding in a source coding setting. We begin by presenting
list-source codes, which naturally map a key length (entropy) to list size. We
then show that such codes can be analyzed in the context of a novel
information-theoretic metric, \epsilon-symbol secrecy, that encompasses both
the one-time pad and traditional rate-based asymptotic metrics, but, like most
cryptographic constructs, can be applied in non-asymptotic settings. We derive
fundamental bounds for \epsilon-symbol secrecy and demonstrate how these bounds
can be achieved with MDS codes when the source is uniformly distributed. We
discuss applications and implementation issues of our codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2132</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2132</id><created>2012-10-07</created><authors><author><keyname>Zheng</keyname><forenames>Bojin</forenames></author><author><keyname>Du</keyname><forenames>Wenhua</forenames></author><author><keyname>Shu</keyname><forenames>Wanneng</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author><author><keyname>Li</keyname><forenames>Deyi</forenames></author></authors><title>Equalitarian Societies are Economically Impossible</title><categories>physics.soc-ph cs.SI nlin.AO q-fin.GN</categories><journal-ref>Romanian Journal of Physics 2013 58(7-8):778-789</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inequality of wealth distribution is a universal phenomenon in the
civilized nations, and it is often imputed to the Matthew effect, that is, the
rich get richer and the poor get poorer. Some philosophers unjustified this
phenomenon and tried to put the human civilization upon the evenness of wealth.
Noticing the facts that 1) the emergence of the centralism is the starting
point of human civilization, i.e., people in a society were organized
hierarchically, 2) the inequality of wealth emerges simultaneously, this paper
proposes a wealth distribution model based on the hidden tree structure from
the viewpoint of complex network. This model considers the organized structure
of people in a society as a hidden tree, and the cooperations among human
beings as the transactions on the hidden tree, thereby explains the
distribution of wealth. This model shows that the scale-free phenomenon of
wealth distribution can be produced by the cascade controlling of human
society, that is, the inequality of wealth can parasitize in the social
organizations, such that any actions in eliminating the unequal wealth
distribution would lead to the destroy of social or economic structures,
resulting in the collapse of the economic system, therefore, would fail in
vain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2143</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2143</id><created>2012-10-08</created><updated>2013-05-14</updated><authors><author><keyname>Shomorony</keyname><forenames>Ilan</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Degrees of Freedom of Two-Hop Wireless Networks: &quot;Everyone Gets the
  Entire Cake&quot;</title><categories>cs.IT math.IT</categories><comments>Presented at the 2012 Allerton Conference. Submitted to IEEE
  Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that fully connected two-hop wireless networks with K sources, K
relays and K destinations have K degrees of freedom both in the case of
time-varying channel coefficients and in the case of constant channel
coefficients (in which case the result holds for almost all values of constant
channel coefficients). Our main contribution is a new achievability scheme
which we call Aligned Network Diagonalization. This scheme allows the data
streams transmitted by the sources to undergo a diagonal linear transformation
from the sources to the destinations, thus being received free of interference
by their intended destination. In addition, we extend our scheme to multi-hop
networks with fully connected hops, and multi-hop networks with MIMO nodes, for
which the degrees of freedom are also fully characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2144</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2144</id><created>2012-10-08</created><authors><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Network Compression: Memory-Assisted Universal Coding of Sources with
  Correlated Parameters</title><categories>cs.IT math.IT</categories><comments>2012 Allerton Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose {\em distributed network compression via memory}.
We consider two spatially separated sources with correlated unknown source
parameters. We wish to study the universal compression of a sequence of length
$n$ from one of the sources provided that the decoder has access to (i.e.,
memorized) a sequence of length $m$ from the other source. In this setup, the
correlation does not arise from symbol-by-symbol dependency of two outputs from
the two sources (as in Slepian-Wolf setup). Instead, the two sequences are
correlated because they are originated from the two sources with \emph{unknown}
correlated parameters. The finite-length nature of the compression problem at
hand requires considering a notion of almost lossless source coding, where
coding incurs an error probability $p_e(n)$ that vanishes as sequence length
$n$ grows to infinity. We obtain bounds on the redundancy of almost lossless
codes when the decoder has access to a random memory of length $m$ as a
function of the sequence length $n$ and the permissible error probability
$p_e(n)$. Our results demonstrate that distributed network compression via
memory has the potential to significantly improve over conventional end-to-end
compression when sufficiently large memory from previous communications is
available to the decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2146</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2146</id><created>2012-10-08</created><authors><author><keyname>Tian</keyname><forenames>Yafei</forenames></author><author><keyname>Lu</keyname><forenames>Songtao</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author></authors><title>Amplitude Space Sharing among the Macro-Cell and Small-Cell Users</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, submitted to IEEE Int. Conf. on Communications
  (ICC) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crushing demand for wireless data services will soon exceed the
capability of the current homogeneous cellular architecture. An emerging
solution is to overlay small-cell networks with the macro-cell networks. In
this paper, we propose an amplitude space sharing (ASS) method among the
macro-cell user and small-cell users. By transmit layer design and data-rate
optimization, the signals and interferences are promised to be separable at
each receiver and the network sum-rate is maximized. The Han-Koboyashi coding
is employed and optimal power allocation is derived for the one small-cell
scenario, and a simple ASS transmission scheme is developed for the multiple
small-cells scenarios. Simulation results show great superiority over other
interference management schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2149</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2149</id><created>2012-10-08</created><authors><author><keyname>Zawoad</keyname><forenames>Shams</forenames></author><author><keyname>Hasan</keyname><forenames>Ragib</forenames></author></authors><title>The Enemy Within: The Emerging Threats to Healthcare from Malicious
  Mobile Devices</title><categories>cs.CR cs.NI</categories><comments>To appear at MobiHealth 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With the proliferation of wireless networks, mobile devices and medical
devices are increasingly being equipped with wireless interfaces, such as
Bluetooth and WiFi to allow easy access to and control of the medical devices.
Unfortunately, the very presence and usage of such interfaces also expose the
medical devices to novel attacks from malicious parties. The emerging threat
from malicious mobile devices is significant and severe, since attackers can
steal confidential data from a patient's medical device. Also, attackers can
compromise the medical device and either feed doctors bad data from it or issue
potentially fatal commands to the device, which may even result in the death of
the patient. As the mobile devices are often at close proximity to the patient
(either in the hospital or home settings), attacks from such devices are hard
to prevent. In this paper, we present a systematic analysis of this new threat
from mobile devices on medical devices and healthcare infrastructure. We also
perform a thorough security analysis of a major hospital and uncover potential
vulnerabilities. Finally, we propose a set of potential solutions and defenses
against such attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2159</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2159</id><created>2012-10-08</created><authors><author><keyname>Bloch</keyname><forenames>Matthieu R.</forenames></author><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author></authors><title>Strong Coordination with Polar Codes</title><categories>cs.IT math.IT</categories><comments>7 pages doublespaced, presented at the 50th Annual Allerton
  Conference on Communication, Control and Computing 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design explicit codes for strong coordination in two-node
networks. Specifically, we consider a two-node network in which the action
imposed by nature is binary and uniform, and the action to coordinate is
obtained via a symmetric discrete memoryless channel. By observing that polar
codes are useful for channel resolvability over binary symmetric channels, we
prove that nested polar codes achieve a subset of the strong coordination
capacity region, and therefore provide a constructive and low complexity
solution for strong coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2162</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2162</id><created>2012-10-08</created><authors><author><keyname>Welinder</keyname><forenames>Peter</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author><author><keyname>Perona</keyname><forenames>Pietro</forenames></author></authors><title>Semisupervised Classifier Evaluation and Recalibration</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How many labeled examples are needed to estimate a classifier's performance
on a new dataset? We study the case where data is plentiful, but labels are
expensive. We show that by making a few reasonable assumptions on the structure
of the data, it is possible to estimate performance curves, with confidence
bounds, using a small number of ground truth labels. Our approach, which we
call Semisupervised Performance Evaluation (SPE), is based on a generative
model for the classifier's confidence scores. In addition to estimating the
performance of classifiers on new datasets, SPE can be used to recalibrate a
classifier by re-estimating the class-conditional confidence distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2164</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2164</id><created>2012-10-08</created><updated>2012-12-21</updated><authors><author><keyname>Hu</keyname><forenames>Yuheng</forenames></author><author><keyname>John</keyname><forenames>Ajita</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Seligmann</keyname><forenames>Doree Duncan</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>ET-LDA: Joint Topic Modeling For Aligning, Analyzing and Sensemaking of
  Public Events and Their Twitter Feeds</title><categories>cs.LG cs.AI cs.SI physics.soc-ph</categories><comments>errors in reference, delete for now</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media channels such as Twitter have emerged as popular platforms for
crowds to respond to public events such as speeches, sports and debates. While
this promises tremendous opportunities to understand and make sense of the
reception of an event from the social media, the promises come entwined with
significant technical challenges. In particular, given an event and an
associated large scale collection of tweets, we need approaches to effectively
align tweets and the parts of the event they refer to. This in turn raises
questions about how to segment the event into smaller yet meaningful parts, and
how to figure out whether a tweet is a general one about the entire event or
specific one aimed at a particular segment of the event. In this work, we
present ET-LDA, an effective method for aligning an event and its tweets
through joint statistical modeling of topical influences from the events and
their associated tweets. The model enables the automatic segmentation of the
events and the characterization of tweets into two categories: (1) episodic
tweets that respond specifically to the content in the segments of the events,
and (2) steady tweets that respond generally about the events. We present an
efficient inference method for this model, and a comprehensive evaluation of
its effectiveness over existing methods. In particular, through a user study,
we demonstrate that users find the topics, the segments, the alignment, and the
episodic tweets discovered by ET-LDA to be of higher quality and more
interesting as compared to the state-of-the-art, with improvements in the range
of 18-41%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2174</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2174</id><created>2012-10-08</created><authors><author><keyname>Biernacki</keyname><forenames>Arkadiusz</forenames></author></authors><title>Analysis of the flooding search algorithm with OPNET</title><categories>cs.NI</categories><journal-ref>Novel Algorithms and Techniques in Telecommunications and
  Networking 2010, pp. 339-342, Springer</journal-ref><doi>10.1007/978-90-481-3662-9_58</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the popular OPNET simulator as a tool for
performance evaluation of algorithms operating in peer-to-peer (P2P) networks.
We created simple framework and used it to analyse the flooding search
algorithm which is a popular technique for searching files in an unstructured
P2P network. We investigated the influence of the number of replicas and time
to live (TTL) of search queries on the algorithm performance. Preparing the
simulation we did not reported the problems which are commonly encountered in
P2P dedicated simulators although the size of simulated network was limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2179</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2179</id><created>2012-10-08</created><updated>2015-12-07</updated><authors><author><keyname>Zeng</keyname><forenames>Jia</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Cao</keyname><forenames>Xiao-Qin</forenames></author></authors><title>Fast Online EM for Big Topic Modeling</title><categories>cs.LG</categories><comments>14 pages, 12 figures in IEEE Transactions on Knowledge and Data
  Engineering, 2016</comments><doi>10.1109/TKDE.2015.2492565</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expectation-maximization (EM) algorithm can compute the
maximum-likelihood (ML) or maximum a posterior (MAP) point estimate of the
mixture models or latent variable models such as latent Dirichlet allocation
(LDA), which has been one of the most popular probabilistic topic modeling
methods in the past decade. However, batch EM has high time and space
complexities to learn big LDA models from big data streams. In this paper, we
present a fast online EM (FOEM) algorithm that infers the topic distribution
from the previously unseen documents incrementally with constant memory
requirements. Within the stochastic approximation framework, we show that FOEM
can converge to the local stationary point of the LDA's likelihood function. By
dynamic scheduling for the fast speed and parameter streaming for the low
memory usage, FOEM is more efficient for some lifelong topic modeling tasks
than the state-of-the-art online LDA algorithms to handle both big data and big
models (aka, big topic modeling) on just a PC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2182</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2182</id><created>2012-10-08</created><authors><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Wang</keyname><forenames>Chien-Yi</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Approximate Ergodic Capacity of a Class of Fading 2-user 2-hop Networks</title><categories>cs.IT math.IT</categories><comments>22 pages, 8 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a fading AWGN 2-user 2-hop network where the channel coefficients
are independent and identically distributed (i.i.d.) drawn from a continuous
distribution and vary over time. For a broad class of channel distributions, we
characterize the ergodic sum capacity to within a constant number of
bits/sec/Hz, independent of signal-to-noise ratio. The achievability follows
from the analysis of an interference neutralization scheme where the relays are
partitioned into $M$ pairs, and interference is neutralized separately by each
pair of relays. When $M=1$, the proposed ergodic interference neutralization
characterizes the ergodic sum capacity to within $4$ bits/sec/Hz for i.i.d.
uniform phase fading and approximately $4.7$ bits/sec/Hz for i.i.d. Rayleigh
fading. We further show that this gap can be tightened to $4\log \pi-4$
bits/sec/Hz (approximately $2.6$) for i.i.d. uniform phase fading and $4-4\log(
\frac{3\pi}{8})$ bits/sec/Hz (approximately $3.1$) for i.i.d. Rayleigh fading
in the limit of large $M$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2195</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2195</id><created>2012-10-08</created><authors><author><keyname>De Vos</keyname><forenames>Marina</forenames></author><author><keyname>K&#x131;za</keyname><forenames>Do&#x11f;a Gizem</forenames></author><author><keyname>Oetsch</keyname><forenames>Johannes</forenames></author><author><keyname>P&#xfc;hrer</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Tompits</keyname><forenames>Hans</forenames></author></authors><title>Annotating Answer-Set Programs in LANA?</title><categories>cs.SE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While past research in answer-set programming (ASP) mainly focused on theory,
ASP solver technology, and applications, the present work situates itself in
the context of a quite recent research trend: development support for ASP. In
particular, we propose to augment answer-set programs with additional
meta-information formulated in a dedicated annotation language, called LANA.
This language allows the grouping of rules into coherent blocks and to specify
language signatures, types, pre- and postconditions, as well as unit tests for
such blocks. While these annotations are invisible to an ASP solver, as they
take the form of program comments, they can be interpreted by tools for
documentation, testing, and verification purposes, as well as to eliminate
sources of common programming errors by realising syntax checking or code
completion features. To demonstrate its versatility, we introduce two such
tools, viz. (i) ASPDOC, for generating an HTML documentation for a program
based on the annotated information, and (ii) ASPUNIT, for running and
monitoring unit tests on program blocks. LANA is also exploited in the SeaLion
system, an integrated development environment for ASP based on Eclipse. To
appear in Theory and Practice of Logic Programming
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2211</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2211</id><created>2012-10-08</created><updated>2014-01-10</updated><authors><author><keyname>Mondragon</keyname><forenames>R. J.</forenames></author></authors><title>Network Null Model based on Maximal Entropy and the Rich-Club</title><categories>physics.soc-ph cs.SI stat.ME</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to construct a network null-model based on the maximum
entropy principle and where the restrictions that the rich-club and the degree
sequence impose are conserved. We show that the probability that two nodes
share a link can be described with a simple probability function. The
null-model closely approximates the assortative properties of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2246</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2246</id><created>2012-10-08</created><updated>2013-11-05</updated><authors><author><keyname>van Zyl</keyname><forenames>J. Martin</forenames></author><author><keyname>van der Merwe</keyname><forenames>Sean</forenames></author></authors><title>An empirical study to order citation statistics between subject fields</title><categories>cs.DL physics.soc-ph</categories><msc-class>62P99</msc-class><journal-ref>GSTF Journal of Mathematics, Statistics and Operations Research 2
  (2013) 93 - 97</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An empirical study is conducted to compare citations per publication,
statistics and observed Hirsch indexes between subject fields using summary
statistics of countries. No distributional assumptions are made and ratios are
calculated. These ratios can be used to make approximate comparisons between
researchers of different subject fields with respect to the Hirsch index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2259</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2259</id><created>2012-10-08</created><updated>2016-03-07</updated><authors><author><keyname>Stotz</keyname><forenames>David</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Degrees of freedom in vector interference channels</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Trans. on Inf. Theory; replaces conference version
  presented at the 50th Annual Allerton Conference on Communication, Control,
  and Computing (2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues the Wu-Shamai-Verdu program [3] on characterizing the
degrees of freedom (DoF) of interference channels (ICs) through Renyi
information dimension. Specifically, we find a single-letter formula for the
DoF of vector ICs, encompassing multiple-input multiple-output (MIMO) ICs,
time- and/or frequency-selective ICs, and combinations thereof, as well as
scalar ICs as considered in [3]. The DoF-formula we obtain lower-bounds the DoF
of all channels--with respect to the choice of the channel matrix--and
upper-bounds the DoF of almost all channels. It applies to a large class of
noise distributions, and its proof is based on an extension of a result by
Guionnet and Shlyakthenko [3] to the vector case in combination with the Ruzsa
triangle inequality for differential entropy introduced by Kontoyiannis and
Madiman [4]. As in scalar ICs, achieving full DoF requires the use of singular
input distributions. Strikingly, in the vector case it suffices to enforce
singularity on the joint distribution of each individual transmit vector. This
can be realized through signaling in subspaces of the ambient signal space,
which is in accordance with the idea of interference alignment, and, most
importantly, allows the scalar entries of the transmit vectors to have
non-singular distributions. The DoF-formula for vector ICs we obtain enables a
unified treatment of &quot;classical&quot; interference alignment a la Cadambe and Jafar
[5], and Maddah-Ali et al. [6], and the number-theoretic schemes proposed in
[7], [8]. Moreover, it allows to calculate the DoF achieved by new signaling
schemes for vector ICs. We furthermore recover the result by Cadambe and Jafar
on the non-separability of parallel ICs [9] and we show that almost all
parallel ICs are separable in terms of DoF. Finally, our results apply to
complex vector ICs, thereby extending the main findings of [2] to the complex
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2260</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2260</id><created>2012-10-08</created><authors><author><keyname>Takhanov</keyname><forenames>Rustem</forenames></author></authors><title>Extensions of the Minimum Cost Homomorphism Problem</title><categories>cs.CC</categories><comments>arXiv admin note: substantial text overlap with arXiv:0708.3226</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assume $D$ is a finite set and $R$ is a finite set of functions from $D$ to
the natural numbers. An instance of the minimum $R$-cost homomorphism problem
($MinHom_R$) is a set of variables $V$ subject to specified constraints
together with a positive weight $c_{vr}$ for each combination of $v \in V$ and
$r \in R$. The aim is to find a function $f:V \rightarrow D$ such that $f$
satisfies all constraints and $\sum_{v \in V} \sum_{r \in R} c_{vr}r(f(v))$ is
minimized.
  This problem unifies well-known optimization problems such as the minimum
cost homomorphism problem and the maximum solution problem, and this makes it a
computationally interesting fragment of the valued CSP framework for
optimization problems. We parameterize $MinHom_R\left(\Gamma\right)$ by {\em
constraint languages}, i.e. sets $\Gamma$ of relations that are allowed in
constraints. A constraint language is called {\em conservative} if every unary
relation is a member of it; such constraint languages play an important role in
understanding the structure of constraint problems. The dichotomy conjecture
for $MinHom_R$ is the following statement: if $\Gamma$ is a constraint
language, then $MinHom_R\left(\Gamma\right)$ is either polynomial-time solvable
or NP-complete. For $MinHom$ the dichotomy result has been recently obtained
[Takhanov, STACS, 2010] and the goal of this paper is to expand this result to
the case of $MinHom_R$ with conservative constraint language. For arbitrary $R$
this problem is still open, but assuming certain restrictions on $R$ we prove a
dichotomy. As a consequence of this result we obtain a dichotomy for the
conservative maximum solution problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2272</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2272</id><created>2012-10-08</created><authors><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Joint Sparsity with Different Measurement Matrices</title><categories>cs.IT math.IT</categories><comments>Allerton 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a generalization of the multiple measurement vector (MMV)
problem, where the measurement matrices are allowed to differ across
measurements. This problem arises naturally when multiple measurements are
taken over time, e.g., and the measurement modality (matrix) is time-varying.
We derive probabilistic recovery guarantees showing that---under certain (mild)
conditions on the measurement matrices---l2/l1-norm minimization and a variant
of orthogonal matching pursuit fail with a probability that decays
exponentially in the number of measurements. This allows us to conclude that,
perhaps surprisingly, recovery performance does not suffer from the individual
measurements being taken through different measurement matrices. What is more,
recovery performance typically benefits (significantly) from diversity in the
measurement matrices; we specify conditions under which such improvements are
obtained. These results continue to hold when the measurements are subject to
(bounded) noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2273</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2273</id><created>2012-10-08</created><authors><author><keyname>Forejt</keyname><forenames>Vojtech</forenames></author><author><keyname>Jancar</keyname><forenames>Petr</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Bisimilarity of Probabilistic Pushdown Automata</title><categories>cs.FL cs.LO</categories><comments>technical report accompanying an FSTTCS'12 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the bisimilarity problem for probabilistic pushdown automata (pPDA)
and subclasses thereof. Our definition of pPDA allows both probabilistic and
non-deterministic branching, generalising the classical notion of pushdown
automata (without epsilon-transitions). Our first contribution is a general
construction that reduces checking bisimilarity of probabilistic transition
systems to checking bisimilarity of non-deterministic transition systems. This
construction directly yields decidability of bisimilarity for pPDA, as well as
an elementary upper bound for the bisimilarity problem on the subclass of
probabilistic basic process algebras, i.e., single-state pPDA. We further show
that, with careful analysis, the general reduction can be used to prove an
EXPTIME upper bound for bisimilarity of probabilistic visibly pushdown
automata. Here we also provide a matching lower bound, establishing
EXPTIME-completeness. Finally we prove that deciding bisimilarity of
probabilistic one-counter automata, another subclass of pPDA, is
PSPACE-complete. Here we use a more specialised argument to obtain optimal
complexity bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2276</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2276</id><created>2012-10-08</created><updated>2013-02-22</updated><authors><author><keyname>Alimguzhin</keyname><forenames>Vadim</forenames></author><author><keyname>Mari</keyname><forenames>Federico</forenames></author><author><keyname>Melatti</keyname><forenames>Igor</forenames></author><author><keyname>Salvo</keyname><forenames>Ivano</forenames></author><author><keyname>Tronci</keyname><forenames>Enrico</forenames></author></authors><title>A Map-Reduce Parallel Approach to Automatic Synthesis of Control
  Software</title><categories>cs.DC cs.SY</categories><comments>To be submitted to TACAS 2013. arXiv admin note: substantial text
  overlap with arXiv:1207.4474, arXiv:1207.4098</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many Control Systems are indeed Software Based Control Systems, i.e. control
systems whose controller consists of control software running on a
microcontroller device. This motivates investigation on Formal Model Based
Design approaches for automatic synthesis of control software.
  Available algorithms and tools (e.g., QKS) may require weeks or even months
of computation to synthesize control software for large-size systems. This
motivates search for parallel algorithms for control software synthesis.
  In this paper, we present a Map-Reduce style parallel algorithm for control
software synthesis when the controlled system (plant) is modeled as discrete
time linear hybrid system. Furthermore we present an MPI-based implementation
PQKS of our algorithm. To the best of our knowledge, this is the first parallel
approach for control software synthesis.
  We experimentally show effectiveness of PQKS on two classical control
synthesis problems: the inverted pendulum and the multi-input buck DC/DC
converter. Experiments show that PQKS efficiency is above 65%. As an example,
PQKS requires about 16 hours to complete the synthesis of control software for
the pendulum on a cluster with 60 processors, instead of the 25 days needed by
the sequential algorithm in QKS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2282</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2282</id><created>2012-10-08</created><updated>2012-10-09</updated><authors><author><keyname>Areias</keyname><forenames>Miguel</forenames></author><author><keyname>Rocha</keyname><forenames>Ricardo</forenames></author></authors><title>Towards Multi-Threaded Local Tabling Using a Common Table Space</title><categories>cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming</comments><acm-class>D.1.6</acm-class><journal-ref>Theory and Practice of Logic Programming, Volume 12, Special Issue
  4-5, 2012, pp 427-443</journal-ref><doi>10.1017/S1471068412000117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-threading is currently supported by several well-known Prolog systems
providing a highly portable solution for applications that can benefit from
concurrency. When multi-threading is combined with tabling, we can exploit the
power of higher procedural control and declarative semantics. However, despite
the availability of both threads and tabling in some Prolog systems, the
implementation of these two features implies complex ties to each other and to
the underlying engine. Until now, XSB was the only Prolog system combining
multi-threading with tabling. In XSB, tables may be either private or shared
between threads. While thread-private tables are easier to implement, shared
tables have all the associated issues of locking, synchronization and potential
deadlocks. In this paper, we propose an alternative view to XSB's approach. In
our proposal, each thread views its tables as private but, at the engine level,
we use a common table space where tables are shared among all threads. We
present three designs for our common table space approach: No-Sharing (NS)
(similar to XSB's private tables), Subgoal-Sharing (SS) and Full-Sharing (FS).
The primary goal of this work was to reduce the memory usage for the table
space but, our experimental results, using the YapTab tabling system with a
local evaluation strategy, show that we can also achieve significant reductions
on running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2283</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2283</id><created>2012-10-08</created><authors><author><keyname>Lentz</keyname><forenames>Hartmut H K</forenames></author><author><keyname>Selhorst</keyname><forenames>Thomas</forenames></author><author><keyname>Sokolov</keyname><forenames>Igor M</forenames></author></authors><title>Unfolding accessibility provides a macroscopic approach to temporal
  networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An accessibility graph of a network contains a link, wherever there is a path
of arbitrary length between two nodes. We generalize the concept of
accessibility to temporal networks. Building an accessibility graph by
consecutively adding paths of growing length (unfolding), we obtain information
about the distribution of shortest path durations and characteristic
time-scales in temporal networks. Moreover, we define causal fidelity to
measure the goodness of their static representation. The practicability of our
proposed methods is demonstrated for three examples: networks of social
contacts, livestock trade and sexual contacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2287</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2287</id><created>2012-10-05</created><authors><author><keyname>Ostrowski</keyname><forenames>Max</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>ASP modulo CSP: The clingcon system</title><categories>cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the hybrid ASP solver clingcon, combining the simple modeling
language and the high performance Boolean solving capacities of Answer Set
Programming (ASP) with techniques for using non-Boolean constraints from the
area of Constraint Programming (CP). The new clingcon system features an
extended syntax supporting global constraints and optimize statements for
constraint variables. The major technical innovation improves the interaction
between ASP and CP solver through elaborated learning techniques based on
irreducible inconsistent sets. A broad empirical evaluation shows that these
techniques yield a performance improvement of an order of magnitude. To appear
in Theory and Practice of Logic Programming
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2289</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2289</id><created>2012-10-08</created><authors><author><keyname>Chen</keyname><forenames>Annie I.</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author></authors><title>A Fast Distributed Proximal-Gradient Method</title><categories>cs.DC cs.LG stat.ML</categories><comments>10 pages (including 2-page appendix); 1 figure; submitted to Allerton
  2012 on July 10, 2012; accepted by Allerton 2012, October 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a distributed proximal-gradient method for optimizing the average
of convex functions, each of which is the private local objective of an agent
in a network with time-varying topology. The local objectives have distinct
differentiable components, but they share a common nondifferentiable component,
which has a favorable structure suitable for effective computation of the
proximal operator. In our method, each agent iteratively updates its estimate
of the global minimum by optimizing its local objective function, and
exchanging estimates with others via communication in the network. Using
Nesterov-type acceleration techniques and multiple communication steps per
iteration, we show that this method converges at the rate 1/k (where k is the
number of communication rounds between the agents), which is faster than the
convergence rate of the existing distributed methods for solving this problem.
The superior convergence rate of our method is also verified by numerical
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2297</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2297</id><created>2012-10-08</created><updated>2012-10-09</updated><authors><author><keyname>Haemmerl&#xe9;</keyname><forenames>R&#xe9;my</forenames></author></authors><title>Diagrammatic confluence for Constraint Handling Rules</title><categories>cs.PL</categories><acm-class>F.3.1; F.4.1</acm-class><journal-ref>Theory and Practice of Logic Programming, 12(4-5): 737-753, 2012</journal-ref><doi>10.1017/S1471068412000270</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Confluence is a fundamental property of Constraint Handling Rules (CHR)
since, as in other rewriting formalisms, it guarantees that the computations
are not dependent on rule application order, and also because it implies the
logical consistency of the program declarative view. In this paper we are
concerned with proving the confluence of non-terminating CHR programs. For this
purpose, we derive from van Oostrom's decreasing diagrams method a novel
criterion on CHR critical pairs that generalizes all preexisting criteria. We
subsequently improve on a result on the modularity of CHR confluence, which
permits modular combinations of possibly non-terminating confluent programs,
without loss of confluence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2316</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2316</id><created>2012-10-08</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author><author><keyname>Manna</keyname><forenames>Marco</forenames></author></authors><title>Disjunctive Datalog with Existential Quantifiers: Semantics,
  Decidability, and Complexity Issues</title><categories>cs.AI cs.LO</categories><comments>18 pages, 1 figure, 1 table, 1 procedure, presented at ICLP 2012</comments><msc-class>68T27</msc-class><acm-class>I.2.3; I.2.4</acm-class><journal-ref>Theory and Practice of Logic Programming 12 (4-5): 701-718, 2012</journal-ref><doi>10.1017/S1471068412000257</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Datalog is one of the best-known rule-based languages, and extensions of it
are used in a wide context of applications. An important Datalog extension is
Disjunctive Datalog, which significantly increases the expressivity of the
basic language. Disjunctive Datalog is useful in a wide range of applications,
ranging from Databases (e.g., Data Integration) to Artificial Intelligence
(e.g., diagnosis and planning under incomplete knowledge). However, in recent
years an important shortcoming of Datalog-based languages became evident, e.g.
in the context of data-integration (consistent query-answering, ontology-based
data access) and Semantic Web applications: The language does not permit any
generation of and reasoning with unnamed individuals in an obvious way. In
general, it is weak in supporting many cases of existential quantification. To
overcome this problem, Datalogex has recently been proposed, which extends
traditional Datalog by existential quantification in rule heads. In this work,
we propose a natural extension of Disjunctive Datalog and Datalogex, called
Datalogexor, which allows both disjunctions and existential quantification in
rule heads and is therefore an attractive language for knowledge representation
and reasoning, especially in domains where ontology-based reasoning is needed.
We formally define syntax and semantics of the language Datalogexor, and
provide a notion of instantiation, which we prove to be adequate for
Datalogexor. A main issue of Datalogex and hence also of Datalogexor is that
decidability is no longer guaranteed for typical reasoning tasks. In order to
address this issue, we identify many decidable fragments of the language, which
extend, in a natural way, analog classes defined in the non-disjunctive case.
Moreover, we carry out an in-depth complexity analysis, deriving interesting
results which range from Logarithmic Space to Exponential Time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2343</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2343</id><created>2012-10-08</created><authors><author><keyname>Schaeffer</keyname><forenames>Luke</forenames></author></authors><title>Ostrowski Numeration and the Local Period of Sturmian Words</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the local period at position n in a characteristic Sturmian word
can be given in terms of the Ostrowski representation for n + 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2346</identifier>
 <datestamp>2013-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2346</id><created>2012-10-08</created><updated>2013-08-30</updated><authors><author><keyname>Hazan</keyname><forenames>Tamir</forenames></author><author><keyname>Schwing</keyname><forenames>Alexander</forenames></author><author><keyname>McAllester</keyname><forenames>David</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author></authors><title>Blending Learning and Inference in Structured Prediction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we derive an efficient algorithm to learn the parameters of
structured predictors in general graphical models. This algorithm blends the
learning and inference tasks, which results in a significant speedup over
traditional approaches, such as conditional random fields and structured
support vector machines. For this purpose we utilize the structures of the
predictors to describe a low dimensional structured prediction task which
encourages local consistencies within the different structures while learning
the parameters of the model. Convexity of the learning task provides the means
to enforce the consistencies between the different parts. The
inference-learning blending algorithm that we propose is guaranteed to converge
to the optimum of the low dimensional primal and dual programs. Unlike many of
the existing approaches, the inference-learning blending allows us to learn
efficiently high-order graphical models, over regions of any size, and very
large number of parameters. We demonstrate the effectiveness of our approach,
while presenting state-of-the-art results in stereo estimation, semantic
segmentation, shape reconstruction, and indoor scene understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2352</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2352</id><created>2012-10-08</created><updated>2013-09-17</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author></authors><title>A notion of continuity in discrete spaces and applications</title><categories>math.MG cs.CV math.CO math.GN</categories><comments>arXiv admin note: text overlap with arXiv:1111.0268</comments><journal-ref>Applied General Topology 14 (1) (2013) 61-72</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a notion of continuous path for locally finite metric spaces,
taking inspiration from the recent development of A-theory for locally finite
connected graphs. We use this notion of continuity to derive an analogue in Z^2
of the Jordan curve theorem and to extend to a quite large class of locally
finite metric spaces (containing all finite metric spaces) an inequality for
the \ell^p-distortion of a metric space that has been recently proved by
Pierre-Nicolas Jolissaint and Alain Valette for finite connected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2354</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2354</id><created>2012-10-08</created><updated>2014-01-10</updated><authors><author><keyname>Costa</keyname><forenames>Sueli I. R.</forenames></author><author><keyname>Santos</keyname><forenames>Sandra A.</forenames></author><author><keyname>Strapasson</keyname><forenames>Jo&#xe3;o E.</forenames></author></authors><title>Fisher information distance: a geometrical reading</title><categories>stat.ME cs.IT math-ph math.IT math.MP</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a strongly geometrical approach to the Fisher distance, which
is a measure of dissimilarity between two probability distribution functions.
The Fisher distance, as well as other divergence measures, are also used in
many applications to establish a proper data average. The main purpose is to
widen the range of possible interpretations and relations of the Fisher
distance and its associated geometry for the prospective applications. It
focuses on statistical models of the normal probability distribution functions
and takes advantage of the connection with the classical hyperbolic geometry to
derive closed forms for the Fisher distance in several cases. Connections with
the well-known Kullback-Leibler divergence measure are also devised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2376</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2376</id><created>2012-10-08</created><updated>2013-07-16</updated><authors><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>Lima</keyname><forenames>Antonio</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Interdependence and Predictability of Human Mobility and Social
  Interactions</title><categories>physics.soc-ph cs.SI nlin.CD</categories><comments>21 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous studies have shown that human movement is predictable to a certain
extent at different geographic scales. Existing prediction techniques exploit
only the past history of the person taken into consideration as input of the
predictors. In this paper, we show that by means of multivariate nonlinear time
series prediction techniques it is possible to increase the forecasting
accuracy by considering movements of friends, people, or more in general
entities, with correlated mobility patterns (i.e., characterised by high mutual
information) as inputs. Finally, we evaluate the proposed techniques on the
Nokia Mobile Data Challenge and Cabspotting datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2380</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2380</id><created>2012-10-08</created><updated>2013-10-21</updated><authors><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>Stable and robust sampling strategies for compressive imaging</title><categories>cs.CV cs.IT math.IT math.NA</categories><comments>17 pages, 4 figures</comments><msc-class>94A08, 68U10, 65D18, 92C55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many signal processing applications, one wishes to acquire images that are
sparse in transform domains such as spatial finite differences or wavelets
using frequency domain samples. For such applications, overwhelming empirical
evidence suggests that superior image reconstruction can be obtained through
variable density sampling strategies that concentrate on lower frequencies. The
wavelet and Fourier transform domains are not incoherent because low-order
wavelets and low-order frequencies are correlated, so compressive sensing
theory does not immediately imply sampling strategies and reconstruction
guarantees. In this paper we turn to a more refined notion of coherence -- the
so-called local coherence -- measuring for each sensing vector separately how
correlated it is to the sparsity basis. For Fourier measurements and Haar
wavelet sparsity, the local coherence can be controlled and bounded explicitly,
so for matrices comprised of frequencies sampled from a suitable inverse square
power-law density, we can prove the restricted isometry property with
near-optimal embedding dimensions. Consequently, the variable-density sampling
strategy we provide allows for image reconstructions that are stable to
sparsity defects and robust to measurement noise. Our results cover both
reconstruction by $\ell_1$-minimization and by total variation minimization.
The local coherence framework developed in this paper should be of independent
interest in sparse recovery problems more generally, as it implies that for
optimal sparse recovery results, it suffices to have bounded \emph{average}
coherence from sensing basis to sparsity basis -- as opposed to bounded maximal
coherence -- as long as the sampling strategy is adapted accordingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2381</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2381</id><created>2012-10-08</created><authors><author><keyname>Kasiviswanathan</keyname><forenames>Shiva Prasad</forenames></author><author><keyname>Rudelson</keyname><forenames>Mark</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author></authors><title>The Power of Linear Reconstruction Attacks</title><categories>cs.DS cs.CR cs.LG math.PR</categories><comments>30 pages, to appear in ACM-SIAM Symposium on Discrete Algorithms
  (SODA 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the power of linear reconstruction attacks in statistical data
privacy, showing that they can be applied to a much wider range of settings
than previously understood. Linear attacks have been studied before (Dinur and
Nissim PODS'03, Dwork, McSherry and Talwar STOC'07, Kasiviswanathan, Rudelson,
Smith and Ullman STOC'10, De TCC'12, Muthukrishnan and Nikolov STOC'12) but
have so far been applied only in settings with releases that are obviously
linear.
  Consider a database curator who manages a database of sensitive information
but wants to release statistics about how a sensitive attribute (say, disease)
in the database relates to some nonsensitive attributes (e.g., postal code,
age, gender, etc). We show one can mount linear reconstruction attacks based on
any release that gives: a) the fraction of records that satisfy a given
non-degenerate boolean function. Such releases include contingency tables
(previously studied by Kasiviswanathan et al., STOC'10) as well as more complex
outputs like the error rate of classifiers such as decision trees; b) any one
of a large class of M-estimators (that is, the output of empirical risk
minimization algorithms), including the standard estimators for linear and
logistic regression.
  We make two contributions: first, we show how these types of releases can be
transformed into a linear format, making them amenable to existing
polynomial-time reconstruction algorithms. This is already perhaps surprising,
since many of the above releases (like M-estimators) are obtained by solving
highly nonlinear formulations. Second, we show how to analyze the resulting
attacks under various distributional assumptions on the data. Specifically, we
consider a setting in which the same statistic (either a) or b) above) is
released about how the sensitive attribute relates to all subsets of size k
(out of a total of d) nonsensitive boolean attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2388</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2388</id><created>2012-10-08</created><authors><author><keyname>Mu</keyname><forenames>Yadong</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Video De-fencing</title><categories>cs.CV cs.MM</categories><comments>To appear in IEEE transactions on Circuits and Systems for Video
  Technology (T-CSVT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes and provides an initial solution to a novel video
editing task, i.e., video de-fencing. It targets automatic restoration of the
video clips that are corrupted by fence-like occlusions during capture. Our key
observation lies in the visual parallax between fences and background scenes,
which is caused by the fact that the former are typically closer to the camera.
Unlike in traditional image inpainting, fence-occluded pixels in the videos
tend to appear later in the temporal dimension and are therefore recoverable
via optimized pixel selection from relevant frames. To eventually produce
fence-free videos, major challenges include cross-frame sub-pixel image
alignment under diverse scene depth, and &quot;correct&quot; pixel selection that is
robust to dominating fence pixels. Several novel tools are developed in this
paper, including soft fence detection, weighted truncated optical flow method
and robust temporal median filter. The proposed algorithm is validated on
several real-world video clips with fences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2401</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2401</id><created>2012-10-05</created><authors><author><keyname>Xu</keyname><forenames>Biao</forenames></author><author><keyname>de Fr&#xe9;in</keyname><forenames>Ruair&#xed;</forenames></author><author><keyname>Robson</keyname><forenames>Eric</forenames></author><author><keyname>Foghl&#xfa;</keyname><forenames>M&#xed;che&#xe1;l &#xd3;</forenames></author></authors><title>Distributed Formal Concept Analysis Algorithms Based on an Iterative
  MapReduce Framework</title><categories>cs.DC</categories><comments>17 pages, ICFCA 201, Formal Concept Analysis 2012</comments><doi>10.1007/978-3-642-29892-9_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While many existing formal concept analysis algorithms are efficient, they
are typically unsuitable for distributed implementation. Taking the MapReduce
(MR) framework as our inspiration we introduce a distributed approach for
performing formal concept mining. Our method has its novelty in that we use a
light-weight MapReduce runtime called Twister which is better suited to
iterative algorithms than recent distributed approaches. First, we describe the
theoretical foundations underpinning our distributed formal concept analysis
approach. Second, we provide a representative exemplar of how a classic
centralized algorithm can be implemented in a distributed fashion using our
methodology: we modify Ganter's classic algorithm by introducing a family of
MR* algorithms, namely MRGanter and MRGanter+ where the prefix denotes the
algorithm's lineage. To evaluate the factors that impact distributed algorithm
performance, we compare our MR* algorithms with the state-of-the-art.
Experiments conducted on real datasets demonstrate that MRGanter+ is efficient,
scalable and an appealing algorithm for distributed problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2406</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2406</id><created>2012-10-08</created><authors><author><keyname>Tajer</keyname><forenames>Ali</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Quick Search for Rare Events</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rare events can potentially occur in many applications. When manifested as
opportunities to be exploited, risks to be ameliorated, or certain features to
be extracted, such events become of paramount significance. Due to their
sporadic nature, the information-bearing signals associated with rare events
often lie in a large set of irrelevant signals and are not easily accessible.
This paper provides a statistical framework for detecting such events so that
an optimal balance between detection reliability and agility, as two opposing
performance measures, is established. The core component of this framework is a
sampling procedure that adaptively and quickly focuses the
information-gathering resources on the segments of the dataset that bear the
information pertinent to the rare events. Particular focus is placed on
Gaussian signals with the aim of detecting signals with rare mean and variance
values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2421</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2421</id><created>2012-10-08</created><authors><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>Hamza</keyname><forenames>T. T.</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>A. H.</forenames></author></authors><title>Simulated Tom Thumb, the Rule Of Thumb for Autonomous Robots</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a mobile robot to be truly autonomous, it must solve the simultaneous
localization and mapping (SLAM) problem. We develop a new metaheuristic
algorithm called Simulated Tom Thumb (STT), based on the detailed adventure of
the clever Tom Thumb and advances in researches relating to path planning based
on potential functions. Investigations show that it is very promising and could
be seen as an optimization of the powerful solution of SLAM with data
association and learning capabilities. STT outperform JCBB. The performance is
100 % match.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2429</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2429</id><created>2012-10-08</created><authors><author><keyname>Frank</keyname><forenames>Mario</forenames></author><author><keyname>Dong</keyname><forenames>Ben</forenames></author><author><keyname>Felt</keyname><forenames>Adrienne Porter</forenames></author><author><keyname>Song</keyname><forenames>Dawn</forenames></author></authors><title>Mining Permission Request Patterns from Android and Facebook
  Applications (extended author version)</title><categories>cs.CR cs.AI stat.ML</categories><comments>To be presented at the IEEE International Conference on Data Mining
  (ICDM) in Brussels, Belgium. This extended author version contains additional
  analysis of the dataset(price distribution, rating distribution), more
  details about model-order selection, and more experiments. Please download
  the dataset from http://www.mariofrank.net/andrApps/index.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Android and Facebook provide third-party applications with access to users'
private data and the ability to perform potentially sensitive operations (e.g.,
post to a user's wall or place phone calls). As a security measure, these
platforms restrict applications' privileges with permission systems: users must
approve the permissions requested by applications before the applications can
make privacy- or security-relevant API calls. However, recent studies have
shown that users often do not understand permission requests and lack a notion
of typicality of requests. As a first step towards simplifying permission
systems, we cluster a corpus of 188,389 Android applications and 27,029
Facebook applications to find patterns in permission requests. Using a method
for Boolean matrix factorization for finding overlapping clusters, we find that
Facebook permission requests follow a clear structure that exhibits high
stability when fitted with only five clusters, whereas Android applications
demonstrate more complex permission requests. We also find that low-reputation
applications often deviate from the permission request patterns that we
identified for high-reputation applications suggesting that permission request
patterns are indicative for user satisfaction or application quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2440</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2440</id><created>2012-10-08</created><authors><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Group Model Selection Using Marginal Correlations: The Good, the Bad and
  the Ugly</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>Accepted for publication in Proc. 50th Annu. Allerton Conf.
  Communication, Control, and Computing, Monticello, IL, Oct. 1-5, 2012; 8
  pages and 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group model selection is the problem of determining a small subset of groups
of predictors (e.g., the expression data of genes) that are responsible for
majority of the variation in a response variable (e.g., the malignancy of a
tumor). This paper focuses on group model selection in high-dimensional linear
models, in which the number of predictors far exceeds the number of samples of
the response variable. Existing works on high-dimensional group model selection
either require the number of samples of the response variable to be
significantly larger than the total number of predictors contributing to the
response or impose restrictive statistical priors on the predictors and/or
nonzero regression coefficients. This paper provides comprehensive
understanding of a low-complexity approach to group model selection that avoids
some of these limitations. The proposed approach, termed Group Thresholding
(GroTh), is based on thresholding of marginal correlations of groups of
predictors with the response variable and is reminiscent of existing
thresholding-based approaches in the literature. The most important
contribution of the paper in this regard is relating the performance of GroTh
to a polynomial-time verifiable property of the predictors for the general case
of arbitrary (random or deterministic) predictors and arbitrary nonzero
regression coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2448</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2448</id><created>2012-10-08</created><authors><author><keyname>Capiluppi</keyname><forenames>Marta</forenames><affiliation>Universit&#xe0; di Verona</affiliation></author><author><keyname>Segala</keyname><forenames>Roberto</forenames><affiliation>Universit&#xe0; di Verona</affiliation></author></authors><title>Modelling Implicit Communication in Multi-Agent Systems with Hybrid
  Input/Output Automata</title><categories>cs.FL cs.MA</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 1-14</journal-ref><doi>10.4204/EPTCS.96.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an extension of Hybrid I/O Automata (HIOAs) to model agent systems
and their implicit communication through perturbation of the environment, like
localization of objects or radio signals diffusion and detection. To this end
we decided to specialize some variables of the HIOAs whose values are functions
both of time and space. We call them world variables. Basically they are
treated similarly to the other variables of HIOAs, but they have the function
of representing the interaction of each automaton with the surrounding
environment, hence they can be output, input or internal variables. Since these
special variables have the role of simulating implicit communication, their
dynamics are specified both in time and space, because they model the
perturbations induced by the agent to the environment, and the perturbations of
the environment as perceived by the agent. Parallel composition of world
variables is slightly different from parallel composition of the other
variables, since their signals are summed. The theory is illustrated through a
simple example of agents systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2449</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2449</id><created>2012-10-08</created><authors><author><keyname>Huang</keyname><forenames>Chung-Hao</forenames><affiliation>National Taiwan University</affiliation></author><author><keyname>Peled</keyname><forenames>Doron</forenames><affiliation>Bar Ilan University</affiliation></author><author><keyname>Schewe</keyname><forenames>Sven</forenames><affiliation>University of Liverpool</affiliation></author><author><keyname>Wang</keyname><forenames>Farn</forenames><affiliation>National Taiwan University</affiliation></author></authors><title>Rapid Recovery for Systems with Scarce Faults</title><categories>cs.SY</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><acm-class>I.2.2</acm-class><journal-ref>EPTCS 96, 2012, pp. 15-28</journal-ref><doi>10.4204/EPTCS.96.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to achieve a high degree of fault tolerance through the control
of a safety critical systems. This reduces to solving a game between a
malicious environment that injects failures and a controller who tries to
establish a correct behavior. We suggest a new control objective for such
systems that offers a better balance between complexity and precision: we seek
systems that are k-resilient. In order to be k-resilient, a system needs to be
able to rapidly recover from a small number, up to k, of local faults
infinitely many times, provided that blocks of up to k faults are separated by
short recovery periods in which no fault occurs. k-resilience is a simple but
powerful abstraction from the precise distribution of local faults, but much
more refined than the traditional objective to maximize the number of local
faults. We argue why we believe this to be the right level of abstraction for
safety critical systems when local faults are few and far between. We show that
the computational complexity of constructing optimal control with respect to
resilience is low and demonstrate the feasibility through an implementation and
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2450</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2450</id><created>2012-10-08</created><authors><author><keyname>&#x10c;ern&#xfd;</keyname><forenames>Pavol</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Chmel&#xed;k</keyname><forenames>Martin</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Radhakrishna</keyname><forenames>Arjun</forenames><affiliation>IST Austria</affiliation></author></authors><title>Interface Simulation Distances</title><categories>cs.SE cs.LO</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 29-42</journal-ref><doi>10.4204/EPTCS.96.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical (boolean) notion of refinement for behavioral interfaces of
system components is the alternating refinement preorder. In this paper, we
define a distance for interfaces, called interface simulation distance. It
makes the alternating refinement preorder quantitative by, intuitively,
tolerating errors (while counting them) in the alternating simulation game. We
show that the interface simulation distance satisfies the triangle inequality,
that the distance between two interfaces does not increase under parallel
composition with a third interface, and that the distance between two
interfaces can be bounded from above and below by distances between
abstractions of the two interfaces. We illustrate the framework, and the
properties of the distances under composition of interfaces, with two case
studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2451</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2451</id><created>2012-10-08</created><authors><author><keyname>Lange</keyname><forenames>Martin</forenames><affiliation>University of Kassel</affiliation></author><author><keyname>Lozes</keyname><forenames>Etienne</forenames><affiliation>University of Kassel</affiliation></author><author><keyname>Guzm&#xe1;n</keyname><forenames>Manuel Vargas</forenames><affiliation>University of Kassel</affiliation></author></authors><title>Model-Checking Process Equivalences</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 96, 2012, pp. 43-56</journal-ref><doi>10.4204/EPTCS.96.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process equivalences are formal methods that relate programs and system
which, informally, behave in the same way. Since there is no unique notion of
what it means for two dynamic systems to display the same behaviour there are a
multitude of formal process equivalences, ranging from bisimulation to trace
equivalence, categorised in the linear-time branching-time spectrum.
  We present a logical framework based on an expressive modal fixpoint logic
which is capable of defining many process equivalence relations: for each such
equivalence there is a fixed formula which is satisfied by a pair of processes
if and only if they are equivalent with respect to this relation. We explain
how to do model checking, even symbolically, for a significant fragment of this
logic that captures many process equivalences. This allows model checking
technology to be used for process equivalence checking. We show how partial
evaluation can be used to obtain decision procedures for process equivalences
from the generic model checking scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2452</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2452</id><created>2012-10-08</created><authors><author><keyname>Barth</keyname><forenames>Stephan</forenames></author><author><keyname>Hofmann</keyname><forenames>Martin</forenames></author></authors><title>Learn with SAT to Minimize B\&quot;uchi Automata</title><categories>cs.FL</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 71-84</journal-ref><doi>10.4204/EPTCS.96.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a minimization procedure for nondeterministic B\&quot;uchi automata
(NBA). For an automaton A another automaton A_min with the minimal number of
states is learned with the help of a SAT-solver.
  This is done by successively computing automata A' that approximate A in the
sense that they accept a given finite set of positive examples and reject a
given finite set of negative examples. In the course of the procedure these
example sets are successively increased. Thus, our method can be seen as an
instance of a generic learning algorithm based on a &quot;minimally adequate
teacher&quot; in the sense of Angluin.
  We use a SAT solver to find an NBA for given sets of positive and negative
examples. We use complementation via construction of deterministic parity
automata to check candidates computed in this manner for equivalence with A.
Failure of equivalence yields new positive or negative examples. Our method
proved successful on complete samplings of small automata and of quite some
examples of bigger automata.
  We successfully ran the minimization on over ten thousand automata with
mostly up to ten states, including the complements of all possible automata
with two states and alphabet size three and discuss results and runtimes;
single examples had over 100 states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2453</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2453</id><created>2012-10-08</created><authors><author><keyname>Solimando</keyname><forenames>Alessandro</forenames><affiliation>University of Genova, Italy</affiliation></author><author><keyname>Delzanno</keyname><forenames>Giorgio</forenames><affiliation>University of Genova, Italy</affiliation></author><author><keyname>Guerrini</keyname><forenames>Giovanna</forenames><affiliation>University of Genova, Italy</affiliation></author></authors><title>Automata-based Static Analysis of XML Document Adaptation</title><categories>cs.DB cs.DS cs.FL</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><acm-class>H.2, I.7</acm-class><journal-ref>EPTCS 96, 2012, pp. 85-98</journal-ref><doi>10.4204/EPTCS.96.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of an XML document can be optionally specified by means of XML
Schema, thus enabling the exploitation of structural information for efficient
document handling. Upon schema evolution, or when exchanging documents among
different collections exploiting related but not identical schemas, the need
may arise of adapting a document, known to be valid for a given schema S, to a
target schema S'. The adaptation may require knowledge of the element semantics
and cannot always be automatically derived. In this paper, we present an
automata-based method for the static analysis of user-defined XML document
adaptations, expressed as sequences of XQuery Update update primitives. The key
feature of the method is the use of an automatic inference method for
extracting the type, expressed as a Hedge Automaton, of a sequence of document
updates. The type is computed starting from the original schema S and from
rewriting rules that formally define the operational semantics of a sequence of
document updates. Type inclusion can then be used as conformance test w.r.t.
the type extracted from the target schema S'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2454</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2454</id><created>2012-10-08</created><authors><author><keyname>Dimovski</keyname><forenames>Aleksandar S.</forenames></author></authors><title>Symbolic Representation of Algorithmic Game Semantics</title><categories>cs.FL cs.GT cs.LO cs.PL</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 99-112</journal-ref><doi>10.4204/EPTCS.96.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we revisit the regular-language representation of game
semantics of second-order recursion free Idealized Algol with infinite data
types. By using symbolic values instead of concrete ones we generalize the
standard notion of regular-language and automata representations to that of
corresponding symbolic representations. In this way terms with infinite data
types, such as integers, can be expressed as finite symbolic-automata although
the standard automata interpretation is infinite. Moreover, significant
reductions of the state space of game semantics models are obtained. This
enables efficient verification of terms, which is illustrated with several
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2455</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2455</id><created>2012-10-08</created><authors><author><keyname>Gutierrez</keyname><forenames>Julian</forenames><affiliation>University of Cambridge, United Kingdom</affiliation></author><author><keyname>Klaedtke</keyname><forenames>Felix</forenames><affiliation>ETH Zurich, Switzerland</affiliation></author><author><keyname>Lange</keyname><forenames>Martin</forenames><affiliation>University of Kassel, Germany</affiliation></author></authors><title>The \mu-Calculus Alternation Hierarchy Collapses over Structures with
  Restricted Connectivity</title><categories>cs.LO cs.CC cs.FL</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.4.1</acm-class><journal-ref>EPTCS 96, 2012, pp. 113-126</journal-ref><doi>10.4204/EPTCS.96.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the alternation hierarchy of least and greatest fixpoint
operators in the mu-calculus is strict. However, the strictness of the
alternation hierarchy does not necessarily carry over when considering
restricted classes of structures. A prominent instance is the class of infinite
words over which the alternation-free fragment is already as expressive as the
full mu-calculus. Our current understanding of when and why the mu-calculus
alternation hierarchy is not strict is limited. This paper makes progress in
answering these questions by showing that the alternation hierarchy of the
mu-calculus collapses to the alternation-free fragment over some classes of
structures, including infinite nested words and finite graphs with feedback
vertex sets of a bounded size. Common to these classes is that the connectivity
between the components in a structure from such a class is restricted in the
sense that the removal of certain vertices from the structure's graph
decomposes it into graphs in which all paths are of finite length. Our collapse
results are obtained in an automata-theoretic setting. They subsume,
generalize, and strengthen several prior results on the expressivity of the
mu-calculus over restricted classes of structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2456</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2456</id><created>2012-10-08</created><authors><author><keyname>Almeida</keyname><forenames>Ricardo</forenames></author><author><keyname>Broda</keyname><forenames>Sabine</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author></authors><title>Deciding KAT and Hoare Logic with Derivatives</title><categories>cs.FL cs.LO</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 127-140</journal-ref><doi>10.4204/EPTCS.96.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kleene algebra with tests (KAT) is an equational system for program
verification, which is the combination of Boolean algebra (BA) and Kleene
algebra (KA), the algebra of regular expressions. In particular, KAT subsumes
the propositional fragment of Hoare logic (PHL) which is a formal system for
the specification and verification of programs, and that is currently the base
of most tools for checking program correctness. Both the equational theory of
KAT and the encoding of PHL in KAT are known to be decidable. In this paper we
present a new decision procedure for the equivalence of two KAT expressions
based on the notion of partial derivatives. We also introduce the notion of
derivative modulo particular sets of equations. With this we extend the
previous procedure for deciding PHL. Some experimental results are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2457</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2457</id><created>2012-10-08</created><authors><author><keyname>Neider</keyname><forenames>Daniel</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Rabinovich</keyname><forenames>Roman</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames><affiliation>RWTH Aachen University and University of Warsaw</affiliation></author></authors><title>Down the Borel Hierarchy: Solving Muller Games via Safety Games</title><categories>cs.LO cs.GT</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 169-182</journal-ref><doi>10.4204/EPTCS.96.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We transform a Muller game with n vertices into a safety game with (n!)^3
vertices whose solution allows to determine the winning regions of the Muller
game and to compute a finite-state winning strategy for one player. This yields
a novel antichain-based memory structure and a natural notion of permissive
strategies for Muller games. Moreover, we generalize our construction by
presenting a new type of game reduction from infinite games to safety games and
show its applicability to several other winning conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2458</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2458</id><created>2012-10-08</created><authors><author><keyname>Fridman</keyname><forenames>Wladimir</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames><affiliation>University of Warsaw</affiliation></author></authors><title>Playing Pushdown Parity Games in a Hurry</title><categories>cs.GT</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 183-196</journal-ref><doi>10.4204/EPTCS.96.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the investigation of finite-duration variants of
infinite-duration games by extending known results for games played on finite
graphs to those played on infinite ones. In particular, we establish an
equivalence between pushdown parity games and a finite-duration variant. This
allows us to determine the winner of a pushdown parity game by solving a
reachability game on a finite tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2459</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2459</id><created>2012-10-08</created><authors><author><keyname>Canavoi</keyname><forenames>Felix</forenames></author><author><keyname>Gr&#xe4;del</keyname><forenames>Erich</forenames></author><author><keyname>Rabinovich</keyname><forenames>Roman</forenames></author></authors><title>The discrete strategy improvement algorithm for parity games and
  complexity measures for directed graphs</title><categories>cs.CC cs.GT</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 197-209</journal-ref><doi>10.4204/EPTCS.96.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For some time the discrete strategy improvement algorithm due to Jurdzinski
and Voge had been considered as a candidate for solving parity games in
polynomial time. However, it has recently been proved by Oliver Friedmann that
the strategy improvement algorithm requires super-polynomially many iteration
steps, for all popular local improvements rules, including switch-all (also
with Fearnley's snare memorisation), switch-best, random-facet, random-edge,
switch-half, least-recently-considered, and Zadeh's Pivoting rule.
  We analyse the examples provided by Friedmann in terms of complexity measures
for directed graphs such as treewidth, DAG-width, Kelly-width, entanglement,
directed pathwidth, and cliquewidth. It is known that for every class of parity
games on which one of these parameters is bounded, the winning regions can be
efficiently computed. It turns out that with respect to almost all of these
measures, the complexity of Friedmann's counterexamples is bounded, and indeed
in most cases by very small numbers. This analysis strengthens in some sense
Friedmann's results and shows that the discrete strategy improvement algorithm
is even more limited than one might have thought. Not only does it require
super-polynomial running time in the general case, where the problem of
polynomial-time solvability is open, it even has super-polynomial lower time
bounds on natural classes of parity games on which efficient algorithms are
known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2460</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2460</id><created>2012-10-08</created><authors><author><keyname>Parys</keyname><forenames>Pawe&#x142;</forenames><affiliation>University of Warsaw</affiliation></author></authors><title>Higher-Order Pushdown Systems with Data</title><categories>cs.FL</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.4.3</acm-class><journal-ref>EPTCS 96, 2012, pp. 210-223</journal-ref><doi>10.4204/EPTCS.96.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new extension of higher-order pushdown automata, which allows to
use an infinite alphabet. The new automata recognize languages of data words
(instead of normal words), which beside each its letter from a finite alphabet
have a data value from an infinite alphabet. Those data values can be loaded to
the stack of the automaton, and later compared with some farther data values on
the input. Our main purpose for introducing these automata is that they may
help in analyzing normal automata (without data). As an example, we give a
proof that deterministic automata with collapse can recognize more languages
than deterministic automata without collapse. This proof is simpler than in the
no-data case. We also state a hypothesis how the new automaton model can be
related to the original model of higher-order pushdown automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2461</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2461</id><created>2012-10-08</created><authors><author><keyname>Cantone</keyname><forenames>Domenico</forenames><affiliation>University of Catania</affiliation></author><author><keyname>Longo</keyname><forenames>Cristiano</forenames><affiliation>NCE</affiliation></author></authors><title>A decidable quantified fragment of set theory with ordered pairs and
  some undecidable extensions</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 96, 2012, pp. 224-237</journal-ref><doi>10.4204/EPTCS.96.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the decision problem for a fragment of set theory
with restricted quantification which extends the language studied in [4] with
pair related quantifiers and constructs, in view of possible applications in
the field of knowledge representation. We will also show that the decision
problem for our language has a non-deterministic exponential time complexity.
However, for the restricted case of formulae whose quantifier prefixes have
length bounded by a constant, the decision problem becomes NP-complete. We also
observe that in spite of such restriction, several useful set-theoretic
constructs, mostly related to maps, are expressible. Finally, we present some
undecidable extensions of our language, involving any of the operators domain,
range, image, and map composition.
  [4] Michael Breban, Alfredo Ferro, Eugenio G. Omodeo and Jacob T. Schwartz
(1981): Decision procedures for elementary sublanguages of set theory. II.
Formulas involving restricted quantifiers, together with ordinal, integer, map,
and domain notions. Communications on Pure and Applied Mathematics 34, pp.
177-195
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2462</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2462</id><created>2012-10-08</created><authors><author><keyname>Kruckman</keyname><forenames>Alex</forenames><affiliation>Berkeley University</affiliation></author><author><keyname>Rubin</keyname><forenames>Sasha</forenames><affiliation>TU Vienna and IST Austria</affiliation></author><author><keyname>Sheridan</keyname><forenames>John</forenames></author><author><keyname>Zax</keyname><forenames>Ben</forenames></author></authors><title>A Myhill-Nerode theorem for automata with advice</title><categories>cs.FL cs.LO</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 238-246</journal-ref><doi>10.4204/EPTCS.96.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An automaton with advice is a finite state automaton which has access to an
additional fixed infinite string called an advice tape. We refine the
Myhill-Nerode theorem to characterize the languages of finite strings that are
accepted by automata with advice. We do the same for tree automata with advice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2463</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2463</id><created>2012-10-08</created><authors><author><keyname>Hummel</keyname><forenames>Szczepan</forenames><affiliation>University of Warsaw</affiliation></author></authors><title>Unambiguous Tree Languages Are Topologically Harder Than Deterministic
  Ones</title><categories>cs.FL</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 247-260</journal-ref><doi>10.4204/EPTCS.96.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper gives an example of a tree language G that is recognised by an
unambiguous parity automaton and is analytic-complete as a set in Cantor space.
This already shows that the unambiguous languages are topologically more
complex than the deterministic ones, that are all coanalytic.
  Using set G as a building block we construct an unambiguous language that is
topologically harder than any countable boolean combination of analytic and
coanalytic sets. In particular the language is harder than any set in
difference hierarchy of analytic sets considered by O.Finkel and P.Simonnet in
the context of nondeterministic automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2473</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2473</id><created>2012-10-08</created><updated>2013-05-12</updated><authors><author><keyname>Zhang</keyname><forenames>Zhong-Yuan</forenames></author><author><keyname>Sun</keyname><forenames>Kai-Di</forenames></author><author><keyname>Wang</keyname><forenames>Si-Qi</forenames></author></authors><title>Enhanced Community Structure Detection in Complex Networks with Partial
  Background Information</title><categories>cs.SI physics.comp-ph physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community structure detection in complex networks is important since it can
help better understand the network topology and how the network works. However,
there is still not a clear and widely-accepted definition of community
structure, and in practice, different models may give very different results of
communities, making it hard to explain the results. In this paper, different
from the traditional methodologies, we design an enhanced semi-supervised
learning framework for community detection, which can effectively incorporate
the available prior information to guide the detection process and can make the
results more explainable. By logical inference, the prior information is more
fully utilized. The experiments on both the synthetic and the real-world
networks confirm the effectiveness of the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2474</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2474</id><created>2012-10-08</created><authors><author><keyname>Soni</keyname><forenames>Akshay</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis</forenames></author></authors><title>Level Set Estimation from Compressive Measurements using Box Constrained
  Total Variation Regularization</title><categories>cs.CV stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the level set of a signal from measurements is a task that arises
in a variety of fields, including medical imaging, astronomy, and digital
elevation mapping. Motivated by scenarios where accurate and complete
measurements of the signal may not available, we examine here a simple
procedure for estimating the level set of a signal from highly incomplete
measurements, which may additionally be corrupted by additive noise. The
proposed procedure is based on box-constrained Total Variation (TV)
regularization. We demonstrate the performance of our approach, relative to
existing state-of-the-art techniques for level set estimation from compressive
measurements, via several simulation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2479</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2479</id><created>2012-10-08</created><authors><author><keyname>Bresolin</keyname><forenames>Davide</forenames><affiliation>University of Verona, Italy</affiliation></author><author><keyname>Della Monica</keyname><forenames>Dario</forenames><affiliation>Reykjavik University, Iceland</affiliation></author><author><keyname>Montanari</keyname><forenames>Angelo</forenames><affiliation>University of Udine, Italy</affiliation></author><author><keyname>Sala</keyname><forenames>Pietro</forenames><affiliation>University of Verona, Italy</affiliation></author><author><keyname>Sciavicco</keyname><forenames>Guido</forenames><affiliation>University of Murcia, Spain</affiliation></author></authors><title>Interval Temporal Logics over Strongly Discrete Linear Orders: the
  Complete Picture</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 96, 2012, pp. 155-168</journal-ref><doi>10.4204/EPTCS.96.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interval temporal logics provide a general framework for temporal reasoning
about interval structures over linearly ordered domains, where intervals are
taken as the primitive ontological entities. In this paper, we identify all
fragments of Halpern and Shoham's interval temporal logic HS with a decidable
satisfiability problem over the class of strongly discrete linear orders. We
classify them in terms of both their relative expressive power and their
complexity. We show that there are exactly 44 expressively different decidable
fragments, whose complexity ranges from NP to EXPSPACE. In addition, we
identify some new undecidable fragments (all the remaining HS fragments were
already known to be undecidable over strongly discrete linear orders). We
conclude the paper by an analysis of the specific case of natural numbers,
whose behavior slightly differs from that of the whole class of strongly
discrete linear orders. The number of decidable fragments over natural numbers
raises up to 47: three undecidable fragments become decidable with a
non-primitive recursive complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2481</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2481</id><created>2012-10-08</created><authors><author><keyname>Michaliszyn</keyname><forenames>Jakub</forenames></author><author><keyname>Otop</keyname><forenames>Jan</forenames></author><author><keyname>Witkowski</keyname><forenames>Piotr</forenames></author></authors><title>Satisfiability vs. Finite Satisfiability in Elementary Modal Logics</title><categories>cs.LO cs.CC</categories><comments>In Proceedings GandALF 2012, arXiv:1210.2028</comments><proxy>EPTCS</proxy><acm-class>F.4.1; F.2.2</acm-class><journal-ref>EPTCS 96, 2012, pp. 141-154</journal-ref><doi>10.4204/EPTCS.96.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study elementary modal logics, i.e. modal logic considered over
first-order definable classes of frames. The classical semantics of modal logic
allows infinite structures, but often practical applications require to
restrict our attention to finite structures. Many decidability and
undecidability results for the elementary modal logics were proved separately
for general satisfiability and for finite satisfiability [11, 12, 16, 17]. In
this paper, we show that there is a reason why we must deal with both kinds of
satisfiability separately -- we prove that there is a universal first-order
formula that defines an elementary modal logic with decidable (global)
satisfiability problem, but undecidable finite satisfiability problem, and, the
other way round, that there is a universal formula that defines an elementary
modal logic with decidable finite satisfiability problem, but undecidable
general satisfiability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2484</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2484</id><created>2012-10-09</created><updated>2013-10-08</updated><authors><author><keyname>Emad</keyname><forenames>Amin</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Semi-Quantitative Group Testing: A Unifying Framework for Group Testing
  with Applications in Genotyping</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. Inf. Theory, vol. 60, pp. 4614-4636, 2014</journal-ref><doi>10.1109/TIT.2014.2327630</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel group testing method, termed semi-quantitative group
testing, motivated by a class of problems arising in genome screening
experiments. Semi-quantitative group testing (SQGT) is a (possibly) non-binary
pooling scheme that may be viewed as a concatenation of an adder channel and an
integer-valued quantizer. In its full generality, SQGT may be viewed as a
unifying framework for group testing, in the sense that most group testing
models are special instances of SQGT. For the new testing scheme, we define the
notion of SQ-disjunct and SQ-separable codes, representing generalizations of
classical disjunct and separable codes. We describe several combinatorial and
probabilistic constructions for such codes. While for most of these
constructions we assume that the number of defectives is much smaller than
total number of test subjects, we also consider the case in which there is no
restriction on the number of defectives and they may be as large as the total
number of subjects. For the codes constructed in this paper, we describe a
number of efficient decoding algorithms. In addition, we describe a belief
propagation decoder for sparse SQGT codes for which no other efficient decoder
is currently known. Finally, we define the notion of capacity of SQGT and
evaluate it for some special choices of parameters using information theoretic
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2485</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2485</id><created>2012-10-09</created><authors><author><keyname>Mohan</keyname><forenames>J.</forenames></author><author><keyname>Maheshwari</keyname><forenames>S.</forenames></author><author><keyname>Chauhan</keyname><forenames>D. S.</forenames></author></authors><title>Minimum Component Based First-Order Inverting and Non-inverting Outputs
  of All-Pass Filter at the Same Circuit</title><categories>cs.OH</categories><comments>ACEEE International Journal on Electrical and Power Engineering, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new voltage-mode first order all-pass filter using minimum
active and passive components is presented. The proposed circuit employs one
fully differential second generation current conveyor (FDCCII), one grounded
capacitor, one resistor and offers the following advantages: the use of only
grounded capacitor which is attractive for integrated circuit implementation,
low active and passive sensitivities, providing inverting and non-inverting
voltage-mode all-pass responses simultaneously from the single circuit and no
requirement for component matching conditions. The theory is validated through
PSPICE simulation using TSMC 0.35micrometer CMOS process parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2502</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2502</id><created>2012-10-09</created><authors><author><keyname>Rajashekar</keyname><forenames>Rakshith</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author><author><keyname>Hanzo</keyname><forenames>L.</forenames></author></authors><title>Structured Dispersion Matrices from Space-Time Block Codes for
  Space-Time Shift Keying</title><categories>cs.IT math.IT</categories><comments>30 pages 9 figures. Ignore the 31st page which has a copy of Fig. 5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coherent Space-Time Shift Keying (CSTSK) is a recently developed generalized
shift-keying framework for Multiple-Input Multiple-Output systems, which uses a
set of Space-Time matrices termed as Dispersion Matrices (DM). CSTSK may be
combined with a classic signaling set (eg. QAM, PSK) in order to strike a
flexible tradeoff between the achievable diversity and multiplexing gain. One
of the key benefits of the CSTSK scheme is its Inter-Channel Interference (ICI)
free system that makes single-stream Maximum Likelihood detection possible at
low-complexity. In the existing CSTSK scheme, DMs are chosen by maximizing the
mutual information over a large set of complex valued, Gaussian random matrices
through numerical simulations. We refer to them as Capacity-Optimized (CO) DMs.
In this contribution we establish a connection between the STSK scheme as well
as the Space-Time Block Codes (STBC) and show that a class of STBCs termed as
Decomposable Dispersion Codes (DDC) enjoy all the benefits that are specific to
the STSK scheme. Two STBCs belonging to this class are proposed, a rate-one
code from Field Extensions and a full-rate code from Cyclic Division Algebras,
that offer structured DMs with desirable properties such as full-diversity, and
a high coding gain. We show that the DMs derived from these codes are capable
of achieving a performance than CO-DMs, and emphasize the importance of DMs
having a higher coding gain than CO-DMs in scenarios having realistic,
imperfect channel state information at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2506</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2506</id><created>2012-10-09</created><updated>2013-02-06</updated><authors><author><keyname>Singh</keyname><forenames>Sukhpal</forenames></author><author><keyname>Chana</keyname><forenames>Inderveer</forenames></author></authors><title>Enabling Reusability in Agile Software Development</title><categories>cs.SE</categories><comments>8 Pages including 5 figures, Published by Foundation of Computer
  Science, New York, USA</comments><report-no>pxc3881132</report-no><msc-class>68N30</msc-class><acm-class>D.2.13</acm-class><journal-ref>International Journal of Computer Applications, Volume 50, No-13,
  2012, 33-40</journal-ref><doi>10.5120/7834-1132</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Software Engineering Discipline is constantly achieving momentum from past
two decades. In last decade, remarkable progress has been observed. New process
models that are introduced from time to time in order to keep pace with
multidimensional demands of the industry. New software development paradigms
are finding its place in industry such as Agile Software Development, Reuse
based Development and Component based Development. But different software
development models fail to satisfy many needs of software industry. As aim of
all the process models is same, i.e., to get quality product, reduce time of
development, productivity improvement and reduction in cost. Still, no single
process model is complete in itself. Software industry is moving towards Agile
Software Development. Agile development does not obviously fit well for
building reusable artifacts. However, with careful attention, and important
modifications made to agile processes, it may be possible to successfully adapt
and put on agile methods to development of reusable objects. The model being
proposed here combines the features of Agile Software Development and
reusability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2514</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2514</id><created>2012-10-09</created><authors><author><keyname>Mohan</keyname><forenames>J.</forenames></author><author><keyname>Maheshwari</keyname><forenames>S.</forenames></author><author><keyname>Chauhan</keyname><forenames>D. S.</forenames></author></authors><title>Minimum Grounded Component Based Voltage-Mode Quadrature Oscillator
  using DVCC</title><categories>cs.OH</categories><comments>International Journal on Recent Trends in Engineering &amp; Technology,
  vol.3, issue 4, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new voltage-mode quadrature oscillator using minimum number
of active and passive component is proposed. The proposed circuit employs
single modified DVCC, two grounded capacitor and two grounded resistors, which
is ideal for IC implementation. The active and passive sensitivity are no more
than unity. The proposed circuit is verified through PSPICE simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2515</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2515</id><created>2012-10-09</created><authors><author><keyname>Huang</keyname><forenames>Ting</forenames></author><author><keyname>Zhu</keyname><forenames>Peijun</forenames></author><author><keyname>He</keyname><forenames>Zengyou</forenames></author></authors><title>Protein Inference and Protein Quantification: Two Sides of the Same Coin</title><categories>cs.CE cs.DS q-bio.QM</categories><comments>14 Pages, This paper has submitted to RECOMB2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: In mass spectrometry-based shotgun proteomics, protein
quantification and protein identification are two major computational problems.
To quantify the protein abundance, a list of proteins must be firstly inferred
from the sample. Then the relative or absolute protein abundance is estimated
with quantification methods, such as spectral counting. Until now, researchers
have been dealing with these two processes separately. In fact, they are two
sides of same coin in the sense that truly present proteins are those proteins
with non-zero abundances. Then, one interesting question is if we regard the
protein inference problem as a special protein quantification problem, is it
possible to achieve better protein inference performance?
  Contribution: In this paper, we investigate the feasibility of using protein
quantification methods to solve the protein inference problem. Protein
inference is to determine whether each candidate protein is present in the
sample or not. Protein quantification is to calculate the abundance of each
protein. Naturally, the absent proteins should have zero abundances. Thus, we
argue that the protein inference problem can be viewed as a special case of
protein quantification problem: present proteins are those proteins with
non-zero abundances. Based on this idea, our paper tries to use three very
simple protein quantification methods to solve the protein inference problem
effectively.
  Results: The experimental results on six datasets show that these three
methods are competitive with previous protein inference algorithms. This
demonstrates that it is plausible to take the protein inference problem as a
special case of protein quantification, which opens the door of devising more
effective protein inference algorithms from a quantification perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2529</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2529</id><created>2012-10-09</created><authors><author><keyname>Eslamifar</keyname><forenames>Mahshad</forenames></author><author><keyname>Chin</keyname><forenames>Woon Hau</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Performance Analysis of Two-Step Bi-Directional Relaying with Multiple
  Antennas</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study decode-and-forward multi-antenna relay systems that
achieve bi-directional communication in two time slots. We investigate
different downlink broadcast schemes which employ binary or analog network
coding at the relay. We also analyze and compare their performances in terms of
diversity order and symbol error probability. It is shown that if exact
downlink channel state information is available at the relay, using analog
network coding in the form of multi-antenna maximal-ratio transmit beamforming
to precode the information vectors at the relay gives the best performance.
Then, we propose a Max-Min antenna selection with binary network coding scheme
that can approach this performance with only partial channel state information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2536</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2536</id><created>2012-10-09</created><authors><author><keyname>Li</keyname><forenames>Jiajia</forenames></author><author><keyname>Zhang</keyname><forenames>Xiuxia</forenames></author><author><keyname>Tan</keyname><forenames>Guangming</forenames></author><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author></authors><title>SMAT: An Input Adaptive Sparse Matrix-Vector Multiplication Auto-Tuner</title><categories>cs.MS cs.DC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sparse matrix vector multiplication (SpMV) is an important kernel in
scientific and engineering applications. The previous optimizations are sparse
matrix format specific and expose the choice of the best format to application
programmers. In this work we develop an auto-tuning framework to bridge gap
between the specific optimized kernels and their general-purpose use. We
propose an SpMV auto-tuner (SMAT) that provides an unified interface based on
compressed sparse row (CSR) to programmers by implicitly choosing the best
format and the fastest implementation of any input sparse matrix in runtime.
SMAT leverage a data mining model, which is formulated based on a set of
performance parameters extracted from 2373 matrices in UF sparse matrix
collection, to fast search the best combination. The experiments show that SMAT
achieves the maximum performance of 75 GFLOP/s in single-precision and 33
GFLOP/s in double-precision on Intel, and 41 GFLOP/s in single-precision and 34
GFLOP/s in double-precision on AMD. Compared with the sparse functions in MKL
library, SMAT runs faster by more than 3 times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2540</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2540</id><created>2012-10-09</created><authors><author><keyname>Bouyuklieva</keyname><forenames>Stefka</forenames></author><author><keyname>de la Cruz</keyname><forenames>Javier</forenames></author><author><keyname>Willems</keyname><forenames>Wolfgang</forenames></author></authors><title>On the Automorphism Group of a Binary Self-dual [120, 60, 24] Code</title><categories>math.CO cs.DM math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that an automorphism of order 3 of a putative binary self-dual [120,
60, 24] code C has no fixed points. Moreover, the order of the automorphism
group of C divides 2^a.3.5.7.19.23.29 where a is a nonegative integer.
Automorphisms of odd composite order r may occur only for r=15, 57 or r=115
with corresponding cycle structures 15-(0,0,8;0), 57-(2,0,2;0) or
115-(1,0,1;0), respectively. In case that all involutions act fixed point
freely we have |Aut(C)|&lt;=920, and Aut(C) is solvable if it contains an element
of prime order p&gt;=7. Moreover, the alternating group A_5 is the only
non-abelian composition factor which may occur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2544</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2544</id><created>2012-10-09</created><updated>2013-04-23</updated><authors><author><keyname>B&#xf8;g</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>St&#xf6;ckel</keyname><forenames>Morten</forenames></author><author><keyname>Vildh&#xf8;j</keyname><forenames>Hjalte Wedel</forenames></author></authors><title>The Hardness of the Functional Orientation 2-Color Problem</title><categories>cs.CC cs.DM math.CO</categories><journal-ref>Australas. J. Combin., vol. 56 (2013), pages 225-234</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Functional Orientation 2-Color problem, which was introduced
by Valiant in his seminal paper on holographic algorithms [SIAM J. Comput.,
37(5), 2008]. For this decision problem, Valiant gave a polynomial time
holographic algorithm for planar graphs of maximum degree 3, and showed that
the problem is NP-complete for planar graphs of maximum degree 10. A recent
result on defective graph coloring by Corr\^ea et al. [Australas. J. Combin.,
43, 2009] implies that the problem is already hard for planar graphs of maximum
degree 8. Together, these results leave open the hardness question for graphs
of maximum degree between 4 and 7. We close this gap by showing that the answer
is always yes for arbitrary graphs of maximum degree 5, and that the problem is
NP-complete for planar graphs of maximum degree 6. Moreover, for graphs of
maximum degree 5, we note that a linear time algorithm for finding a solution
exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2580</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2580</id><created>2012-10-09</created><authors><author><keyname>Marchal</keyname><forenames>Loris</forenames></author><author><keyname>Sinnen</keyname><forenames>Oliver</forenames></author><author><keyname>Vivien</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Scheduling tree-shaped task graphs to minimize memory and makespan</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the execution of tree-shaped task graphs using
multiple processors. Each edge of such a tree represents a large IO file. A
task can only be executed if all input and output files fit into memory, and a
file can only be removed from memory after it has been consumed. Such trees
arise, for instance, in the multifrontal method of sparse matrix factorization.
The maximum amount of memory needed depends on the execution order of the
tasks. With one processor the objective of the tree traversal is to minimize
the required memory. This problem was well studied and optimal polynomial
algorithms were proposed.
  Here, we extend the problem by considering multiple processors, which is of
obvious interest in the application area of matrix factorization. With the
multiple processors comes the additional objective to minimize the time needed
to traverse the tree, i.e., to minimize the makespan. Not surprisingly, this
problem proves to be much harder than the sequential one. We study the
computational complexity of this problem and provide an inapproximability
result even for unit weight trees. Several heuristics are proposed, each with a
different optimization focus, and they are analyzed in an extensive
experimental evaluation using realistic trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2582</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2582</id><created>2012-10-09</created><authors><author><keyname>Agustin</keyname><forenames>Adrian</forenames></author><author><keyname>Vidal</keyname><forenames>Josep</forenames></author></authors><title>Degrees of Freedom Region of the MIMO X channel with an Arbitrary Number
  of Antennas</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the total degrees of freedom (DoF) of the full-rank MIMO X
channel with arbitrary number of antennas at each node. We elucidate that the
existing outer bound is tight for any antenna configuration and provide
transmit and receive filter designs that attain this outer bound. The proposed
achievable scheme exploits channel extensions in terms of both, symbol and
asymmetric complex signaling when the communication is carried out over a
constant channel case, and is also applicable to time varying channels. The
proposed scheme represents a general framework for the derivation of the total
DoF of any two-by-two multiuser channels. Furthermore, the rank-deficient MIMO
channels case is naturally addressed, and it is shown that the total DoF of the
interference (IC) and MIMO X channels are in general superior to the full rank
MIMO case
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2584</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2584</id><created>2012-10-09</created><authors><author><keyname>Moufid</keyname><forenames>Mansour</forenames></author></authors><title>Sudoku as a special transportation problem</title><categories>cs.DS</categories><comments>6 pages; 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sudoku is a popular combinatorial puzzle. A new method of solving Sudoku is
presented, which involves formulating a puzzle as a special type of
transportation problem. This model allows one to solve puzzles with more than
one solution, keeping the constraints of the problem fixed, and simply changing
a cost matrix between solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2587</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2587</id><created>2012-10-09</created><authors><author><keyname>N&#xe1;n&#xe1;si</keyname><forenames>Michal</forenames></author><author><keyname>Vina&#x159;</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Brejov&#xe1;</keyname><forenames>Bro&#x148;a</forenames></author></authors><title>Sequence Annotation with HMMs: New Problems and Their Complexity</title><categories>cs.DS</categories><comments>10 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov models (HMMs) and their variants were successfully used for
several sequence annotation tasks. Traditionally, inference with HMMs is done
using the Viterbi and posterior decoding algorithms. However, recently a
variety of different optimization criteria and associated computational
problems were proposed. In this paper, we consider three HMM decoding criteria
and prove their NP hardness. These criteria consider the set of states used to
generate a certain sequence, but abstract from the exact locations of regions
emitted by individual states. We also illustrate experimentally that these
criteria are useful for HIV recombination detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2592</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2592</id><created>2012-10-09</created><updated>2012-10-10</updated><authors><author><keyname>Mori</keyname><forenames>Ryuhei</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshiyuki</forenames></author></authors><title>New Generalizations of the Bethe Approximation via Asymptotic Expansion</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>6 pages, submitted to the Japanese conference SITA; Errors of
  notations are fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bethe approximation, discovered in statistical physics, gives an
efficient algorithm called belief propagation (BP) for approximating a
partition function. BP empirically gives an accurate approximation for many
problems, e.g., low-density parity-check codes, compressed sensing, etc.
Recently, Vontobel gives a novel characterization of the Bethe approximation
using graph cover. In this paper, a new approximation based on the Bethe
approximation is proposed. The new approximation is derived from Vontobel's
characterization using graph cover, and expressed by using the edge zeta
function, which is related with the Hessian of the Bethe free energy as shown
by Watanabe and Fukumizu. On some conditions, it is proved that the new
approximation is asymptotically better than the Bethe approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2605</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2605</id><created>2012-10-08</created><authors><author><keyname>Adj&#xe9;</keyname><forenames>Assal&#xe9;</forenames></author><author><keyname>Goubault-Larrecq</keyname><forenames>Jean</forenames></author></authors><title>Concrete Semantics of Programs with Non-Deterministic and Random Inputs</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document gives semantics to programs written in a C-like programming
language, featuring interactions with an external environment with noisy and
imprecise data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2610</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2610</id><created>2012-10-09</created><updated>2013-07-04</updated><authors><author><keyname>Grygiel</keyname><forenames>Katarzyna</forenames><affiliation>LIP</affiliation></author><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author></authors><title>Counting and generating lambda terms</title><categories>cs.LO cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lambda calculus is the basis of functional programming and higher order proof
assistants. However, little is known about combinatorial properties of lambda
terms, in particular, about their asymptotic distribution and random
generation. This paper tries to answer questions like: How many terms of a
given size are there? What is a &quot;typical&quot; structure of a simply typable term?
Despite their ostensible simplicity, these questions still remain unanswered,
whereas solutions to such problems are essential for testing compilers and
optimizing programs whose expected efficiency depends on the size of terms. Our
approach toward the afore-mentioned problems may be later extended to any
language with bound variables, i.e., with scopes and declarations. This paper
presents two complementary approaches: one, theoretical, uses complex analysis
and generating functions, the other, experimental, is based on a generator of
lambda-terms. Thanks to de Bruijn indices, we provide three families of
formulas for the number of closed lambda terms of a given size and we give four
relations between these numbers which have interesting combinatorial
interpretations. As a by-product of the counting formulas, we design an
algorithm for generating lambda terms. Performed tests provide us with
experimental data, like the average depth of bound variables and the average
number of head lambdas. We also create random generators for various sorts of
terms. Thereafter, we conduct experiments that answer questions like: What is
the ratio of simply typable terms among all terms? (Very small!) How are simply
typable lambda terms distributed among all lambda terms? (A typable term almost
always starts with an abstraction.) In this paper, abstractions and
applications have size 1 and variables have size 0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2613</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2613</id><created>2012-10-09</created><updated>2012-12-18</updated><authors><author><keyname>Perduca</keyname><forenames>Vittorio</forenames></author><author><keyname>Nuel</keyname><forenames>Gregory</forenames></author></authors><title>Measuring the Influence of Observations in HMMs through the
  Kullback-Leibler Distance</title><categories>cs.IT cs.LG math.IT math.PR</categories><doi>10.1109/LSP.2012.2235830</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We measure the influence of individual observations on the sequence of the
hidden states of the Hidden Markov Model (HMM) by means of the Kullback-Leibler
distance (KLD). Namely, we consider the KLD between the conditional
distribution of the hidden states' chain given the complete sequence of
observations and the conditional distribution of the hidden chain given all the
observations but the one under consideration. We introduce a linear complexity
algorithm for computing the influence of all the observations. As an
illustration, we investigate the application of our algorithm to the problem of
detecting outliers in HMM data series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2620</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2620</id><created>2012-10-09</created><updated>2012-10-21</updated><authors><author><keyname>Gheerbrant</keyname><forenames>Am&#xe9;lie</forenames><affiliation>School of Informatics, University of Edinburgh</affiliation></author><author><keyname>Cate</keyname><forenames>Balder ten</forenames><affiliation>Santa Cruz Department of Computer Science, University of California</affiliation></author></authors><title>Complete Axiomatizations of Fragments of Monadic Second-Order Logic on
  Finite Trees</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>E.1; F.4.1; F.4.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (October
  23, 2012) lmcs:1017</journal-ref><doi>10.2168/LMCS-8(4:12)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a specific class of tree structures that can represent basic
structures in linguistics and computer science such as XML documents, parse
trees, and treebanks, namely, finite node-labeled sibling-ordered trees. We
present axiomatizations of the monadic second-order logic (MSO), monadic
transitive closure logic (FO(TC1)) and monadic least fixed-point logic
(FO(LFP1)) theories of this class of structures. These logics can express
important properties such as reachability. Using model-theoretic techniques, we
show by a uniform argument that these axiomatizations are complete, i.e., each
formula that is valid on all finite trees is provable using our axioms. As a
backdrop to our positive results, on arbitrary structures, the logics that we
study are known to be non-recursively axiomatizable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2629</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2629</id><created>2012-10-09</created><authors><author><keyname>Arabadjis</keyname><forenames>Dimitris</forenames></author><author><keyname>Rousopoulos</keyname><forenames>Panayiotis</forenames></author><author><keyname>Papaodysseus</keyname><forenames>Constantin</forenames></author><author><keyname>Exarhos</keyname><forenames>Michalis</forenames></author><author><keyname>Panagopoulos</keyname><forenames>Michalis</forenames></author><author><keyname>Papazoglou-Manioudaki</keyname><forenames>Lena</forenames></author></authors><title>Optimization in Differentiable Manifolds in Order to Determine the
  Method of Construction of Prehistoric Wall-Paintings</title><categories>cs.CV cs.AI cs.CG</categories><journal-ref>IEEE Transactions on Pattern Analysis and Machine Intelligence,
  vol. 33, no. 11, pp. 2229-2244, November 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a general methodology is introduced for the determination of
potential prototype curves used for the drawing of prehistoric wall-paintings.
The approach includes a) preprocessing of the wall-paintings contours to
properly partition them, according to their curvature, b) choice of prototype
curves families, c) analysis and optimization in 4-manifold for a first
estimation of the form of these prototypes, d) clustering of the contour parts
and the prototypes, to determine a minimal number of potential guides, e)
further optimization in 4-manifold, applied to each cluster separately, in
order to determine the exact functional form of the potential guides, together
with the corresponding drawn contour parts. The introduced methodology
simultaneously deals with two problems: a) the arbitrariness in data-points
orientation and b) the determination of one proper form for a prototype curve
that optimally fits the corresponding contour data. Arbitrariness in
orientation has been dealt with a novel curvature based error, while the proper
forms of curve prototypes have been exhaustively determined by embedding
curvature deformations of the prototypes into 4-manifolds. Application of this
methodology to celebrated wall-paintings excavated at Tyrins, Greece and the
Greek island of Thera, manifests it is highly probable that these
wall-paintings had been drawn by means of geometric guides that correspond to
linear spirals and hyperbolae. These geometric forms fit the drawings' lines
with an exceptionally low average error, less than 0.39mm. Hence, the approach
suggests the existence of accurate realizations of complicated geometric
entities, more than 1000 years before their axiomatic formulation in Classical
Ages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2640</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2640</id><created>2012-10-09</created><authors><author><keyname>Eaton</keyname><forenames>Eric</forenames></author><author><keyname>desJardins</keyname><forenames>Marie</forenames></author><author><keyname>Jacob</keyname><forenames>Sara</forenames></author></authors><title>Multi-view constrained clustering with an incomplete mapping between
  views</title><categories>cs.LG cs.AI</categories><journal-ref>Knowledge and Information Systems 38(1): 231-257, 2014</journal-ref><doi>10.1007/s10115-012-0577-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-view learning algorithms typically assume a complete bipartite mapping
between the different views in order to exchange information during the
learning process. However, many applications provide only a partial mapping
between the views, creating a challenge for current methods. To address this
problem, we propose a multi-view algorithm based on constrained clustering that
can operate with an incomplete mapping. Given a set of pairwise constraints in
each view, our approach propagates these constraints using a local similarity
measure to those instances that can be mapped to the other views, allowing the
propagated constraints to be transferred across views via the partial mapping.
It uses co-EM to iteratively estimate the propagation within each view based on
the current clustering model, transfer the constraints across views, and then
update the clustering model. By alternating the learning process between views,
this approach produces a unified clustering model that is consistent with all
views. We show that this approach significantly improves clustering performance
over several other methods for transferring constraints and allows multi-view
clustering to be reliably applied when given a limited mapping between the
views. Our evaluation reveals that the propagated constraints have high
precision with respect to the true clusters in the data, explaining their
benefit to clustering performance in both single- and multi-view learning
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2646</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2646</id><created>2012-10-09</created><authors><author><keyname>Arabadjis</keyname><forenames>Dimitris</forenames></author><author><keyname>Rousopoulos</keyname><forenames>Panayiotis</forenames></author><author><keyname>Papaodysseus</keyname><forenames>Constantin</forenames></author><author><keyname>Panagopoulos</keyname><forenames>Michalis</forenames></author><author><keyname>Loumou</keyname><forenames>Panayiota</forenames></author><author><keyname>Theodoropoulos</keyname><forenames>Georgios</forenames></author></authors><title>A General Methodology for the Determination of 2D Bodies Elastic
  Deformation Invariants. Application to the Automatic Identification of
  Parasites</title><categories>cs.CV cs.AI</categories><journal-ref>IEEE Transactions on Pattern Analysis and Machine Intelligence,
  vol. 32, no. 5, pp. 799-814, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel methodology is introduced here that exploits 2D images of arbitrary
elastic body deformation instances, so as to quantify mechano-elastic
characteristics that are deformation invariant. Determination of such
characteristics allows for developing methods offering an image of the
undeformed body. General assumptions about the mechano-elastic properties of
the bodies are stated, which lead to two different approaches for obtaining
bodies' deformation invariants. One was developed to spot deformed body's
neutral line and its cross sections, while the other solves deformation PDEs by
performing a set of equivalent image operations on the deformed body images.
Both these processes may furnish a body undeformed version from its deformed
image. This was confirmed by obtaining the undeformed shape of deformed
parasites, cells (protozoa), fibers and human lips. In addition, the method has
been applied to the important problem of parasite automatic classification from
their microscopic images. To achieve this, we first apply the previous method
to straighten the highly deformed parasites and then we apply a dedicated curve
classification method to the straightened parasite contours. It is demonstrated
that essentially different deformations of the same parasite give rise to
practically the same undeformed shape, thus confirming the consistency of the
introduced methodology. Finally, the developed pattern recognition method
classifies the unwrapped parasites into 6 families, with an accuracy rate of
97.6 %.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2665</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2665</id><created>2012-10-09</created><authors><author><keyname>Chaudhary</keyname><forenames>Ruchi</forenames></author><author><keyname>Burleigh</keyname><forenames>J. Gordon</forenames></author><author><keyname>Fern&#xe1;ndez-Baca</keyname><forenames>David</forenames></author></authors><title>Inferring Species Trees from Incongruent Multi-Copy Gene Trees Using the
  Robinson-Foulds Distance</title><categories>cs.DS q-bio.PE</categories><comments>16 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for inferring species trees from multi-copy gene
trees. Our method is based on a generalization of the Robinson-Foulds (RF)
distance to multi-labeled trees (mul-trees), i.e., gene trees in which multiple
leaves can have the same label. Unlike most previous phylogenetic methods using
gene trees, this method does not assume that gene tree incongruence is caused
by a single, specific biological process, such as gene duplication and loss,
deep coalescence, or lateral gene transfer. We prove that it is NP-hard to
compute the RF distance between two mul-trees, but it is easy to calculate the
generalized RF distance between a mul-tree and a singly-labeled tree. Motivated
by this observation, we formulate the RF supertree problem for mul-trees
(MulRF), which takes a collection of mul-trees and constructs a species tree
that minimizes the total RF distance from the input mul-trees. We present a
fast heuristic algorithm for the MulRF supertree problem. Simulation
experiments demonstrate that the MulRF method produces more accurate species
trees than gene tree parsimony methods when incongruence is caused by gene tree
error, duplications and losses, and/or lateral gene transfer. Furthermore, the
MulRF heuristic runs quickly on data sets containing hundreds of trees with up
to a hundred taxa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2686</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2686</id><created>2012-10-09</created><authors><author><keyname>Li</keyname><forenames>J.</forenames></author><author><keyname>Peters</keyname><forenames>T. J.</forenames></author><author><keyname>Roulier</keyname><forenames>J. A.</forenames></author></authors><title>Angular Convergence during Bezier Curve Approximation</title><categories>math.GT cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Properties of a parametric curve in R^3 are often determined by analysis of
its piecewise linear (PL) approximation. For Bezier curves, there are standard
algorithms, known as subdivision, that recursively create PL curves that
converge to the curve in distance . The exterior angles of PL curves under
subdivision are shown to converge to 0 at the rate of
$O(\sqrt{\frac{1}{2^i}})$, where i is the number of subdivisions. This angular
convergence is useful for determining self-intersections and knot type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2687</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2687</id><created>2012-10-09</created><updated>2013-03-07</updated><authors><author><keyname>Almeida</keyname><forenames>Mariana S. C.</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author></authors><title>Deconvolving Images with Unknown Boundaries Using the Alternating
  Direction Method of Multipliers</title><categories>math.OC cs.CV</categories><comments>Submitted to the IEEE Transactions on Image Processing in August 2012</comments><msc-class>68U10</msc-class><acm-class>I.4.4</acm-class><doi>10.1109/TIP.2013.2258354</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The alternating direction method of multipliers (ADMM) has recently sparked
interest as a flexible and efficient optimization tool for imaging inverse
problems, namely deconvolution and reconstruction under non-smooth convex
regularization. ADMM achieves state-of-the-art speed by adopting a divide and
conquer strategy, wherein a hard problem is split into simpler, efficiently
solvable sub-problems (e.g., using fast Fourier or wavelet transforms, or
simple proximity operators). In deconvolution, one of these sub-problems
involves a matrix inversion (i.e., solving a linear system), which can be done
efficiently (in the discrete Fourier domain) if the observation operator is
circulant, i.e., under periodic boundary conditions. This paper extends
ADMM-based image deconvolution to the more realistic scenario of unknown
boundary, where the observation operator is modeled as the composition of a
convolution (with arbitrary boundary conditions) with a spatial mask that keeps
only pixels that do not depend on the unknown boundary. The proposed approach
also handles, at no extra cost, problems that combine the recovery of missing
pixels (i.e., inpainting) with deconvolution. We show that the resulting
algorithms inherit the convergence guarantees of ADMM and illustrate its
performance on non-periodic deblurring (with and without inpainting of interior
pixels) under total-variation and frame-based regularization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2688</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2688</id><created>2012-10-09</created><updated>2014-03-28</updated><authors><author><keyname>Fletcher</keyname><forenames>George H. L.</forenames></author><author><keyname>Gyssens</keyname><forenames>Marc</forenames></author><author><keyname>Leinders</keyname><forenames>Dirk</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author><author><keyname>Van Gucht</keyname><forenames>Dirk</forenames></author><author><keyname>Vansummeren</keyname><forenames>Stijn</forenames></author></authors><title>Similarity and bisimilarity notions appropriate for characterizing
  indistinguishability in fragments of the calculus of relations</title><categories>cs.LO cs.DB</categories><comments>36 pages, Journal of Logic and Computation 2014</comments><doi>10.1093/logcom/exu018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in databases, this paper considers various
fragments of the calculus of binary relations. The fragments are obtained by
leaving out, or keeping in, some of the standard operators, along with some
derived operators such as set difference, projection, coprojection, and
residuation. For each considered fragment, a characterization is obtained for
when two given binary relational structures are indistinguishable by
expressions in that fragment. The characterizations are based on appropriately
adapted notions of simulation and bisimulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2698</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2698</id><created>2012-10-09</created><authors><author><keyname>Gast</keyname><forenames>Mikael</forenames></author><author><keyname>Hauptmann</keyname><forenames>Mathias</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author></authors><title>Improved Approximation Lower Bounds for Vertex Cover on Power Law Graphs
  and Some Generalizations</title><categories>cs.CC cs.DM cs.DS math.CO math.OC</categories><comments>26 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove new explicit inapproximability results for the Vertex Cover Problem
on the Power Law Graphs and some functional generalizations of that class of
graphs. Our results depend on special bounded degree amplifier constructions
for those classes of graphs and could be also of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2704</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2704</id><created>2012-10-09</created><authors><author><keyname>Mirghasemi</keyname><forenames>Hamed</forenames></author><author><keyname>Tchamkerten</keyname><forenames>Aslan</forenames></author></authors><title>On the Capacity of the One-Bit Deletion and Duplication Channel</title><categories>cs.IT math.IT</categories><comments>Allerton 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The one-bit deletion and duplication channel is investigated. An input to
this channel consists of a block of bits which experiences either a deletion,
or a duplication, or remains unchanged. For this channel a capacity expression
is obtained in a certain asymptotic regime where the deletion and duplication
probabilities tend to zero. As a corollary, we obtain an asymptotic expression
for the capacity of the segmented deletion and duplication channel where the
input now consists of several blocks and each block independently experiences
either a deletion, or a duplication, or remains unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2715</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2715</id><created>2012-10-09</created><authors><author><keyname>Dobrev</keyname><forenames>Dimiter</forenames></author></authors><title>AI in arbitrary world</title><categories>cs.AI</categories><journal-ref>5th Panhellenic Logic Symposium, July 2005, University of Athens,
  Athens, Greece, pp. 62-67</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to build AI we have to create a program which copes well in an
arbitrary world. In this paper we will restrict our attention on one concrete
world, which represents the game Tick-Tack-Toe. This world is a very simple one
but it is sufficiently complicated for our task because most people cannot
manage with it. The main difficulty in this world is that the player cannot see
the entire internal state of the world so he has to build a model in order to
understand the world. The model which we will offer will consist of final
automata and first order formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2748</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2748</id><created>2012-10-09</created><updated>2012-11-21</updated><authors><author><keyname>Runge</keyname><forenames>Jakob</forenames></author><author><keyname>Heitzig</keyname><forenames>Jobst</forenames></author><author><keyname>Marwan</keyname><forenames>Norbert</forenames></author><author><keyname>Kurths</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Quantifying Causal Coupling Strength: A Lag-specific Measure For
  Multivariate Time Series Related To Transfer Entropy</title><categories>physics.data-an cs.IT math.IT stat.ML</categories><comments>15 pages, 6 figures; accepted for publication in Physical Review E</comments><journal-ref>Physical Review E, 86, 061121 (2012)</journal-ref><doi>10.1103/PhysRevE.86.061121</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While it is an important problem to identify the existence of causal
associations between two components of a multivariate time series, a topic
addressed in Runge et al. (2012), it is even more important to assess the
strength of their association in a meaningful way. In the present article we
focus on the problem of defining a meaningful coupling strength using
information theoretic measures and demonstrate the short-comings of the
well-known mutual information and transfer entropy. Instead, we propose a
certain time-delayed conditional mutual information, the momentary information
transfer (MIT), as a measure of association that is general, causal and
lag-specific, reflects a well interpretable notion of coupling strength and is
practically computable. MIT is based on the fundamental concept of source
entropy, which we utilize to yield a notion of coupling strength that is,
compared to mutual information and transfer entropy, well interpretable, in
that for many cases it solely depends on the interaction of the two components
at a certain lag. In particular, MIT is thus in many cases able to exclude the
misleading influence of autodependency within a process in an
information-theoretic way. We formalize and prove this idea analytically and
numerically for a general class of nonlinear stochastic processes and
illustrate the potential of MIT on climatological data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2752</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2752</id><created>2012-10-09</created><authors><author><keyname>Capocci</keyname><forenames>Andrea</forenames></author><author><keyname>Baldassarri</keyname><forenames>Andrea</forenames></author><author><keyname>Servedio</keyname><forenames>Vito D. P.</forenames></author><author><keyname>Loreto</keyname><forenames>Vittorio</forenames></author></authors><title>Statistical Properties of Inter-arrival Times Distribution in Social
  Tagging Systems</title><categories>physics.soc-ph cs.IR cs.SI</categories><comments>6 pages, 10 figures; Proceedings of the 20th ACM conference on
  Hypertext and hypermedia, 2009</comments><acm-class>H.3.4; H.3.1</acm-class><doi>10.1145/1557914.1557955</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Folksonomies provide a rich source of data to study social patterns taking
place on the World Wide Web. Here we study the temporal patterns of users'
tagging activity. We show that the statistical properties of inter-arrival
times between subsequent tagging events cannot be explained without taking into
account correlation in users' behaviors. This shows that social interaction in
collaborative tagging communities shapes the evolution of folksonomies. A
consensus formation process involving the usage of a small number of tags for a
given resources is observed through a numerical and analytical analysis of some
well-known folksonomy datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2771</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2771</id><created>2012-10-09</created><updated>2013-04-22</updated><authors><author><keyname>Xu</keyname><forenames>Zhixiang</forenames></author><author><keyname>Kusner</keyname><forenames>Matt J.</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian Q.</forenames></author><author><keyname>Chen</keyname><forenames>Minmin</forenames></author></authors><title>Cost-Sensitive Tree of Classifiers</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, machine learning algorithms have successfully entered large-scale
real-world industrial applications (e.g. search engines and email spam
filters). Here, the CPU cost during test time must be budgeted and accounted
for. In this paper, we address the challenge of balancing the test-time cost
and the classifier accuracy in a principled fashion. The test-time cost of a
classifier is often dominated by the computation required for feature
extraction-which can vary drastically across eatures. We decrease this
extraction time by constructing a tree of classifiers, through which test
inputs traverse along individual paths. Each path extracts different features
and is optimized for a specific sub-partition of the input space. By only
computing features for inputs that benefit from them the most, our cost
sensitive tree of classifiers can match the high accuracies of the current
state-of-the-art at a small fraction of the computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2776</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2776</id><created>2012-10-09</created><updated>2013-03-26</updated><authors><author><keyname>Liu</keyname><forenames>Suyu</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author></authors><title>Contagion dynamics in time-varying metapopulation networks</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 5 figures</comments><journal-ref>Physical Review E 87, 032805 (2013)</journal-ref><doi>10.1103/PhysRevE.87.032805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The metapopulation framework is adopted in a wide array of disciplines to
describe systems of well separated yet connected subpopulations. The subgroups
or patches are often represented as nodes in a network whose links represent
the migration routes among them. The connections have been so far mostly
considered as static, but in general evolve in time. Here we address this case
by investigating simple contagion processes on time-varying metapopulation
networks. We focus on the SIR process and determine analytically the mobility
threshold for the onset of an epidemic spreading in the framework of
activity-driven network models. We find profound differences from the case of
static networks. The threshold is entirely described by the dynamical
parameters defining the average number of instantaneously migrating individuals
and does not depend on the properties of the static network representation.
Remarkably, the diffusion and contagion processes are slower in time-varying
graphs than in their aggregated static counterparts, the mobility threshold
being even two orders of magnitude larger in the first case. The presented
results confirm the importance of considering the time-varying nature of
complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2806</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2806</id><created>2012-10-10</created><authors><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Risk-Sensitive Mean Field Games</title><categories>math.OC cs.GT cs.SY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we study a class of risk-sensitive mean-field stochastic
differential games. We show that under appropriate regularity conditions, the
mean-field value of the stochastic differential game with exponentiated
integral cost functional coincides with the value function described by a
Hamilton-Jacobi-Bellman (HJB) equation with an additional quadratic term. We
provide an explicit solution of the mean-field best response when the
instantaneous cost functions are log-quadratic and the state dynamics are
affine in the control. An equivalent mean-field risk-neutral problem is
formulated and the corresponding mean-field equilibria are characterized in
terms of backward-forward macroscopic McKean-Vlasov equations,
Fokker-Planck-Kolmogorov equations, and HJB equations. We provide numerical
examples on the mean field behavior to illustrate both linear and McKean-Vlasov
dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2814</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2814</id><created>2012-10-10</created><updated>2014-04-25</updated><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>On the probability of two randomly generated S-permutation matrices to
  be disjoint</title><categories>math.CO cs.DM</categories><msc-class>05B20, 60C05</msc-class><journal-ref>Statistics &amp; Probability Letters, Volume 91, August 2014, Pages
  47-51, ISSN 0167-7152</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of S-permutation matrix is considered in this paper. It defines
when two binary matrices are disjoint. For an arbitrary $n^2 \times n^2$
S-permutation matrix, a lower band of the number of all disjoint with it
S-permutation matrices is found. A formula for counting a lower band of the
number of all disjoint pairs of $n^2 \times n^2$ S-permutation matrices is
formulated and proven. As a consequence, a lower band of the probability of two
randomly generated S-permutation matrices to be disjoint is found. In
particular, a different proof of a known assertion is obtained in the work. The
cases when $n=2$ and $n=3$ are discussed in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2815</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2815</id><created>2012-10-10</created><updated>2014-03-01</updated><authors><author><keyname>Arslan</keyname><forenames>Suayb S.</forenames></author></authors><title>Minimum Distortion Variance Concatenated Block Codes for Embedded Source
  Transmission</title><categories>cs.MM</categories><comments>6 pages, 4 figures, In Proc. of International Conference on
  Computing, Networking and Communications, ICNC 2014, Hawaii, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some state-of-art multimedia source encoders produce embedded source bit
streams that upon the reliable reception of only a fraction of the total bit
stream, the decoder is able reconstruct the source up to a basic quality.
Reliable reception of later source bits gradually improve the reconstruction
quality. Examples include scalable extensions of H.264/AVC and progressive
image coders such as JPEG2000. To provide an efficient protection for embedded
source bit streams, a concatenated block coding scheme using a minimum mean
distortion criterion was considered in the past. Although, the original design
was shown to achieve better mean distortion characteristics than previous
studies, the proposed coding structure was leading to dramatic quality
fluctuations. In this paper, a modification of the original design is first
presented and then the second order statistics of the distortion is taken into
account in the optimization. More specifically, an extension scheme is proposed
using a minimum distortion variance optimization criterion. This robust system
design is tested for an image transmission scenario. Numerical results show
that the proposed extension achieves significantly lower variance than the
original design, while showing similar mean distortion performance using both
convolutional codes and low density parity check codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2826</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2826</id><created>2012-10-10</created><authors><author><keyname>Collard</keyname><forenames>Anne</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author><author><keyname>Phillips</keyname><forenames>Christophe</forenames></author><author><keyname>Sepulchre</keyname><forenames>Rodolphe</forenames></author></authors><title>An anisotropy preserving metric for DTI processing</title><categories>cs.CV math.DG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical analysis of Diffusion Tensor Imaging (DTI) data requires a
computational framework that is both numerically tractable (to account for the
high dimensional nature of the data) and geometric (to account for the
nonlinear nature of diffusion tensors). Building upon earlier studies that have
shown that a Riemannian framework is appropriate to address these challenges,
the present paper proposes a novel metric and an accompanying computational
framework for DTI data processing. The proposed metric retains the geometry and
the computational tractability of earlier methods grounded in the affine
invariant metric. In addition, and in contrast to earlier methods, it provides
an interpolation method which preserves anisotropy, a central information
carried by diffusion tensor data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2838</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2838</id><created>2012-10-10</created><authors><author><keyname>Seer</keyname><forenames>Stefan</forenames></author><author><keyname>Br&#xe4;ndle</keyname><forenames>Norbert</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Kinects and Human Kinetics: A New Approach for Studying Crowd Behavior</title><categories>cs.CV physics.soc-ph</categories><comments>Preprint submitted to Transportation Research Part C: Emerging
  Technologies, September 11, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling crowd behavior relies on accurate data of pedestrian movements at a
high level of detail. Imaging sensors such as cameras provide a good basis for
capturing such detailed pedestrian motion data. However, currently available
computer vision technologies, when applied to conventional video footage, still
cannot automatically unveil accurate motions of groups of people or crowds from
the image sequences. We present a novel data collection approach for studying
crowd behavior which uses the increasingly popular low-cost sensor Microsoft
Kinect. The Kinect captures both standard camera data and a three-dimensional
depth map. Our human detection and tracking algorithm is based on agglomerative
clustering of depth data captured from an elevated view - in contrast to the
lateral view used for gesture recognition in Kinect gaming applications. Our
approach transforms local Kinect 3D data to a common world coordinate system in
order to stitch together human trajectories from multiple Kinects, which allows
for a scalable and flexible capturing area. At a testbed with real-world
pedestrian traffic we demonstrate that our approach can provide accurate
trajectories from three Kinects with a Pedestrian Detection Rate of up to 94%
and a Multiple Object Tracking Precision of 4 cm. Using a comprehensive dataset
of 2240 captured human trajectories we calibrate three variations of the Social
Force model. The results of our model validations indicate their particular
ability to reproduce the observed crowd behavior in microscopic simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2856</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2856</id><created>2012-10-10</created><authors><author><keyname>Imre</keyname><forenames>Sandor</forenames></author></authors><title>Quantum Hyperdense Coding for Distributed Communications</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Superdense coding proved that entanglement-assisted quantum communications
can improve the data transmission rates compared to classical systems. It
allows sending 2 classical bits between the parties in exchange of 1 quantum
bit and a pre-shared entangled Bell pair. This paper introduces a new protocol
which is intended for distributed communication. Using a pre-shared entangled
Bell pair and 1 classical bit 2,5 classical bits can be transmitted in average.
This means not only valuable increase in capacity but the two-way distributed
operation opens new fields of investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2857</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2857</id><created>2012-10-10</created><authors><author><keyname>Jaberi</keyname><forenames>Nasrin</forenames></author></authors><title>An Introduction on Dependency Between Hardware Life Time Components and
  Dynamic Voltage Scaling</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main open question is how to calculate the effect of switching between
frequencies in DVFS technique on the lifetime of the cluster components. As
moving from one frequency to another in DVFS technique always gives a shock to
the component and consequently decreases the component lifetime, therefore, it
becomes interesting to answer the question of how fast a component can change
its speed in order to decrease power without changing its lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2864</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2864</id><created>2012-10-10</created><authors><author><keyname>Morales</keyname><forenames>Jose F.</forenames></author><author><keyname>Haemmerl&#xe9;</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Carro</keyname><forenames>Manuel</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author></authors><title>Lightweight compilation of (C)LP to JavaScript</title><categories>cs.PL</categories><acm-class>D.3.4; D.1.6</acm-class><journal-ref>Theory and Practice of Logic Programming, 12(4-5): 755-773, 2012</journal-ref><doi>10.1017/S1471068412000336</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and evaluate a compiler from Prolog (and extensions) to JavaScript
which makes it possible to use (constraint) logic programming to develop the
client side of web applications while being compliant with current industry
standards. Targeting JavaScript makes (C)LP programs executable in virtually
every modern computing device with no additional software requirements from the
point of view of the user. In turn, the use of a very high-level language
facilitates the development of high-quality, complex software. The compiler is
a back end of the Ciao system and supports most of its features, including its
module system and its rich language extension mechanism based on packages. We
present an overview of the compilation process and a detailed description of
the run-time system, including the support for modular compilation into
separate JavaScript code. We demonstrate the maturity of the compiler by
testing it with complex code such as a CLP(FD) library written in Prolog with
attributed variables. Finally, we validate our proposal by measuring the
performance of some LP and CLP(FD) benchmarks running on top of major
JavaScript engines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2872</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2872</id><created>2012-10-10</created><authors><author><keyname>Silwattananusarn</keyname><forenames>Tipawan</forenames></author><author><keyname>Tuamsuk</keyname><forenames>Kulthida</forenames></author></authors><title>Data Mining and Its Applications for Knowledge Management: A Literature
  Review from 2007 to 2012</title><categories>cs.DB</categories><comments>12 pages, 4 figures</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.2, No.5, 2012, pp. 13-24</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining is one of the most important steps of the knowledge discovery in
databases process and is considered as significant subfield in knowledge
management. Research in data mining continues growing in business and in
learning organization over coming decades. This review paper explores the
applications of data mining techniques which have been developed to support
knowledge management process. The journal articles indexed in ScienceDirect
Database from 2007 to 2012 are analyzed and classified. The discussion on the
findings is divided into 4 topics: (i) knowledge resource; (ii) knowledge types
and/or knowledge datasets; (iii) data mining tasks; and (iv) data mining
techniques and applications used in knowledge management. The article first
briefly describes the definition of data mining and data mining functionality.
Then the knowledge management rationale and major knowledge management tools
integrated in knowledge management cycle are described. Finally, the
applications of data mining techniques in the process of knowledge management
are summarized and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2877</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2877</id><created>2012-10-10</created><authors><author><keyname>Papaodysseus</keyname><forenames>Constantin</forenames></author><author><keyname>Arabadjis</keyname><forenames>Dimitris</forenames></author><author><keyname>Exarhos</keyname><forenames>Michalis</forenames></author><author><keyname>Rousopoulos</keyname><forenames>Panayiotis</forenames></author><author><keyname>Zannos</keyname><forenames>Solomon</forenames></author><author><keyname>Panagopoulos</keyname><forenames>Michail</forenames></author><author><keyname>Papazoglou-Manioudaki</keyname><forenames>Lena</forenames></author></authors><title>Efficient Solution to the 3D Problem of Automatic Wall Paintings
  Reassembly</title><categories>cs.CV math.DG</categories><msc-class>49J40, 53A05, 68U99, 68T10</msc-class><journal-ref>Mathematics &amp; Computers with Applications, vol. 64, pp. 2712-2734,
  2012</journal-ref><doi>10.1016/j.bbr.2011.03.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new approach for the automated reconstruction -
reassembly of fragmented objects having one surface near to plane, on the basis
of the 3D representation of their constituent fragments. The whole process
starts by 3D scanning of the available fragments. The obtained representations
are properly processed so that they can be tested for possible matches. Next,
four novel criteria are introduced, that lead to the determination of pairs of
matching fragments. These criteria have been chosen so as the whole process
imitates the instinctive reassembling method dedicated scholars apply. The
first criterion exploits the volume of the gap between two properly placed
fragments. The second one considers the fragments' overlapping in each possible
matching position. Criteria 3,4 employ principles from calculus of variations
to obtain bounds for the area and the mean curvature of the contact surfaces
and the length of contact curves, which must hold if the two fragments match.
The method has been applied, with great success, both in the reconstruction of
objects artificially broken by the authors and, most importantly, in the
virtual reassembling of parts of wall paintings belonging to the Mycenaic
civilization (c. 1300 B.C.), excavated in a highly fragmented condition in
Tyrins, Greece.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2882</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2882</id><created>2012-10-10</created><authors><author><keyname>Naseer</keyname><forenames>Oumair</forenames></author><author><keyname>Khan</keyname><forenames>Rana Atif Ali</forenames></author></authors><title>Online Adaptive Fault Tolerant based Feedback Control Scheduling
  Algorithm for Multiprocessor Embedded Systems</title><categories>cs.SY cs.OS</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Since some years ago, use of Feedback Control Scheduling Algorithm (FCSA) in
the control scheduling co-design of multiprocessor embedded system has
increased. FCSA provides Quality of Service (QoS) in terms of overall system
performance and resource allocation in open and unpredictable environment. FCSA
uses quality control feedback loop to keep CPU utilization under desired
unitization bound by avoiding overloading and deadline miss ratio. Integrated
Fault tolerance (FT) based FCSA design methodology guarantees that the Safety
Critical (SC) tasks will meet their deadlines in the presence of faults.
However, current FCSA design model does not provide the optimal solution with
dynamic load fluctuation. This paper presented a novel methodology of designing
an online adaptive fault tolerant based feedback control algorithm for
multiprocessor embedded systems. This procedure is important for control
scheduling co-design for multiprocessor embedded systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2890</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2890</id><created>2012-10-09</created><authors><author><keyname>Geuvers</keyname><forenames>Herman</forenames><affiliation>Nijmegen University</affiliation></author><author><keyname>de'Liguoro</keyname><forenames>Ugo</forenames><affiliation>Torino University</affiliation></author></authors><title>Proceedings Fourth Workshop on Classical Logic and Computation</title><categories>cs.LO</categories><comments>EPTCS 97, 2012</comments><proxy>EPTCS</proxy><doi>10.4204/EPTCS.97</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CL&amp;C'12 was the fourth of a conference series on &quot;Classical Logic and
Computation&quot;, held as satellite to ICALP'12 on Sunday July 8, 2012 in Warwick,
England.
  CL&amp;C intends to cover all work aiming to explore computational aspects of
classical logic and mathematics, and is focused on the exploration of the
computational content of mathematical and logical principles. The scientific
aim of this workshop is to bring together researchers from both fields and
exchange ideas. The intention of the organisers is for CL&amp;C to be an informal
workshop. Participants are encouraged to present work in progress, overviews of
more extensive work, and programmatic/position papers, as well as completed
projects. Submission of both short abstracts and of longer papers were invited.
Four submissions were accepted as full papers and are included in these
proceedings. Three more were accepted only as communications for the
conference. Paulo Oliva gave an invited talk on &quot;Some connections between Game
Theory and Proof Theory&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2897</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2897</id><created>2012-10-10</created><authors><author><keyname>OBrien</keyname><forenames>Francis J.</forenames><suffix>Jr</suffix></author><author><keyname>Johnnie</keyname><forenames>Nathan</forenames></author><author><keyname>Maloney</keyname><forenames>Susan</forenames></author><author><keyname>Ross</keyname><forenames>Aimee</forenames></author></authors><title>A Proposed General Method for Parameter Estimation of Noise Corrupted
  Oscillator Systems</title><categories>cs.SY physics.data-an</categories><comments>33 pages, 9 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper provides a proposed means to estimate parameters of noise
corrupted oscillator systems. An application for a submarine combat control
systems (CCS) rack is described as exemplary of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2906</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2906</id><created>2012-10-10</created><authors><author><keyname>Chakaravarthy</keyname><forenames>Venkatesan</forenames></author><author><keyname>Pal</keyname><forenames>Arindam</forenames></author><author><keyname>Roy</keyname><forenames>Sambuddha</forenames></author><author><keyname>Sabharwal</keyname><forenames>Yogish</forenames></author></authors><title>Scheduling Resources for Executing a Partial Set of Jobs</title><categories>cs.DS</categories><comments>Full version of paper accepted to FSTTCS'2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of choosing a minimum cost set of
resources for executing a specified set of jobs. Each input job is an interval,
determined by its start-time and end-time. Each resource is also an interval
determined by its start-time and end-time; moreover, every resource has a
capacity and a cost associated with it. We consider two versions of this
problem. In the partial covering version, we are also given as input a number
k, specifying the number of jobs that must be performed. The goal is to choose
k jobs and find a minimum cost set of resources to perform the chosen k jobs
(at any point of time the capacity of the chosen set of resources should be
sufficient to execute the jobs active at that time). We present an O(log
n)-factor approximation algorithm for this problem.
  We also consider the prize collecting version, wherein every job also has a
penalty associated with it. The feasible solution consists of a subset of the
jobs, and a set of resources, to perform the chosen subset of jobs. The goal is
to find a feasible solution that minimizes the sum of the costs of the selected
resources and the penalties of the jobs that are not selected. We present a
constant factor approximation algorithm for this problem
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2907</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2907</id><created>2012-10-10</created><authors><author><keyname>Bocharov</keyname><forenames>Yuri</forenames></author><author><keyname>Butuzov</keyname><forenames>Vladimir</forenames></author><author><keyname>Osipov</keyname><forenames>Dmitry</forenames></author></authors><title>A Low-Power 9-bit Pipelined CMOS ADC with Amplifier and Comparator
  Sharing Technique</title><categories>physics.ins-det cs.AR</categories><comments>Fringe Poster session, 37th European Solid-State Circuits Conference
  (ESSCIRC-2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a pipelined analog-to-digital converter (ADC) employing
a power and area efficient architecture. The adjacent stages of a pipeline
share operational amplifiers. In order to keep accuracy of the amplifiers in
the first stages, they use a partially sharing technique. The feature of the
proposed scheme is that it also shares the comparators. The capacitors of the
first stages of a pipeline are scaled down along a pipeline for a further
reducing the chip area and its power consumption. A 9-bit 20-MSamples/s ADC,
intended for use in multi-channel mixed-signal chips, has been fabricated via
Europractice in a 180-nm CMOS process from UMC. The prototype ADC shows a
spurious-free dynamic range of 58.5 dB at a sample rate of 20 MSamples/s, when
a 400 kHz input signal with a swing of 1 dB below full scale is applied. The
effective number of bits is 8.0 at the same conditions. ADC occupies an active
area of 0.4 mm2 and dissipates 8.6 mW at a 1.8 V supply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2911</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2911</id><created>2012-10-10</created><authors><author><keyname>Ghazvini</keyname><forenames>Mahdieh</forenames></author><author><keyname>Movahedinia</keyname><forenames>Naser</forenames></author><author><keyname>Jamshidi</keyname><forenames>Kamal</forenames></author></authors><title>Carrier Sense Multiple Access Tuning Parameters using Game Theory</title><categories>cs.GT cs.NI</categories><comments>9 pages</comments><doi>10.5121/ijwmn.2012.4419</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ad Hoc and Mesh networks are good samples of multi agent systems, where their
nodes access the channel through carrier sense multiple access method, while a
node channel access influence the access of neighbor nodes to the channel.
Hence, game theory is a strong tool for studying this kind of networks. Carrier
sense multiple access parameters such as minimum and maximum size of contention
window and persistence factor can be modified based on game theoretic methods.
In this study different games for tuning the parameters is investigated and
different challenges are examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2934</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2934</id><created>2012-10-10</created><authors><author><keyname>Muniyal</keyname><forenames>Balachandra</forenames></author><author><keyname>Prema</keyname><forenames>K. V.</forenames></author><author><keyname>Balachandra</keyname><forenames>Mamatha</forenames></author></authors><title>Comparison of Certificate Policies for Merging Public Key
  Infrastructures during Merger and Acquisition of Companies</title><categories>cs.CR</categories><comments>14 pages</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.4, No.5, 2012, 83-96</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Public Key Infrastructure(PKI) provides facilities for data encryption,
digital signature and time stamping. It is a system where different authorities
verify and authenticate the validity of each participant with the use of
digital certificates. A Certificate Policy (CP) is a named set of rules and it
indicates the applicability of a certificate in a Public Key Infrastructure.
Sometimes two companies or organizations with different PKIs merge. Therefore
it would be necessary that their PKIs are also able to merge. Sometimes, the
unification of different PKIs is not possible because of the different
certificate policies. This paper presents a method to compare and assess
certificate policies during merger and acquisition of companies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2935</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2935</id><created>2012-10-10</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author><author><keyname>Abed</keyname><forenames>Eyad H.</forenames></author></authors><title>Local Bifurcations in DC-DC Converters</title><categories>cs.SY math.DS nlin.CD</categories><comments>Published in the proceedings of EPE International Power Electronics
  And Motion Control Conference, Cavtat &amp; Dubrovnik, Croatia, September 9-11,
  2002, paper number SSIN-02, 11 pages. Condensed from Technical Report TR
  1999-5, Institute for Systems Research, University of Maryland,
  http://drum.lib.umd.edu/handle/1903/6011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three local bifurcations in DC-DC converters are reviewed. They are
period-doubling bifurcation, saddle-node bifurcation, and Neimark bifurcation.
A general sampled-data model is employed to study the types of loss of
stability of the nominal (periodic) solution and their connection with local
bifurcations. More accurate prediction of instability and bifurcation than
using the averaging approach is obtained. Examples of bifurcations associated
with instabilities in DC-DC converters are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2940</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2940</id><created>2012-10-10</created><authors><author><keyname>Rathi</keyname><forenames>Neha</forenames></author><author><keyname>Saraswat</keyname><forenames>Jyoti</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Partha Pratim</forenames></author></authors><title>A review on routing protocols for application in wireless sensor
  networks</title><categories>cs.NI</categories><comments>20 pages, 16 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are harshly restricted by storage capacity, energy
and computing power. So it is essential to design effective and energy aware
protocol in order to enhance the network lifetime. In this paper, a review on
routing protocol in WSNs is carried out which are classified as data-centric,
hierarchical and location based depending on the network structure. Then some
of the multipath routing protocols which are widely used in WSNs to improve
network performance are also discussed. Advantages and disadvantages of each
routing algorithm are discussed thereafter. Furthermore, this paper compares
and summarizes the performances of routing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2942</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2942</id><created>2012-10-10</created><updated>2012-10-10</updated><authors><author><keyname>Mori</keyname><forenames>Hiromu</forenames></author><author><keyname>Matsumito</keyname><forenames>Yoshihiro</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Kryssanov</keyname><forenames>Victor</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Vibrotactile Stimulus Frequency Optimization for the Haptic BCI
  Prototype</title><categories>cs.HC q-bio.NC</categories><comments>The 6th International Conference on Soft Computing and Intelligent
  Systems and The 13th International Symposium on Advanced Intelligent Systems,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents results from a psychophysical study conducted to optimize
vibrotactile stimuli delivered to subject finger tips in order to evoke the
somatosensory responses to be utilized next in a haptic brain computer
interface (hBCI) paradigm. We also present the preliminary EEG evoked responses
for the chosen stimulating frequency. The obtained results confirm our
hypothesis that the hBCI paradigm concept is valid and it will allow for rapid
stimuli presentation in order to improve information-transfer-rate (ITR) of the
BCI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2943</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2943</id><created>2012-10-10</created><authors><author><keyname>Matsumoto</keyname><forenames>Yoshihiro</forenames></author><author><keyname>Nishikawa</keyname><forenames>Nozomu</forenames></author><author><keyname>Yamada</keyname><forenames>Takeshi</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Auditory Steady-State Response Stimuli based BCI Application - The
  Optimization of the Stimuli Types and Lengths</title><categories>cs.HC q-bio.NC</categories><comments>APSIPA ASC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for an improvement of auditory BCI (aBCI) paradigm based
on a combination of ASSR stimuli optimization by choosing the subjects' best
responses to AM-, flutter-, AM/FM and click-envelope modulated sounds. As the
ASSR response features we propose pairwise phase-locking-values calculated from
the EEG and next classified using binary classifier to detect attended and
ignored stimuli. We also report on a possibility to use the stimuli as short as
half a second, which is a step forward in ASSR based aBCI. The presented
results are helpful for optimization of the aBCI stimuli for each subject.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2944</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2944</id><created>2012-10-10</created><authors><author><keyname>Cai</keyname><forenames>Zhenyu</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Yamada</keyname><forenames>Takeshi</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Spatial Auditory BCI Paradigm Utilizing N200 and P300 Responses</title><categories>q-bio.NC cs.SD</categories><comments>APSIPA ASC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents our recent results obtained with a new auditory spatial
localization based BCI paradigm in which the ERP shape differences at early
latencies are employed to enhance the traditional P300 responses in an oddball
experimental setting. The concept relies on the recent results in auditory
neuroscience showing a possibility to differentiate early anterior
contralateral responses to attended spatial sources. Contemporary
stimuli-driven BCI paradigms benefit mostly from the P300 ERP latencies in so
called &quot;aha-response&quot; settings. We show the further enhancement of the
classification results in spatial auditory paradigms by incorporating the N200
latencies, which differentiate the brain responses to lateral, in relation to
the subject head, sound locations in the auditory space. The results reveal
that those early spatial auditory ERPs boost online classification results of
the BCI application. The online BCI experiments with the multi-command BCI
prototype support our research hypothesis with the higher classification
results and the improved information-transfer-rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2945</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2945</id><created>2012-10-10</created><authors><author><keyname>Nishikawa</keyname><forenames>Nozomu</forenames></author><author><keyname>Matsumoto</keyname><forenames>Yoshihiro</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>The Spatial Real and Virtual Sound Stimuli Optimization for the Auditory
  BCI</title><categories>cs.HC cs.SD q-bio.NC</categories><comments>APSIPA ASC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents results from a project aiming to create horizontally
distributed surround sound sources and virtual sound images as auditory BCI
(aBCI) stimuli. The purpose is to create evoked brain wave response patterns
depending on attended or ignored sound directions. We propose to use a modified
version of the vector based amplitude panning (VBAP) approach to achieve the
goal. The so created spatial sound stimulus system for the novel oddball aBCI
paradigm allows us to create a multi-command experimental environment with very
encouraging results reported in this paper. We also present results showing
that a modulation of the sound image depth changes also the subject responses.
Finally, we also compare the proposed virtual sound approach with the
traditional one based on real sound sources generated from the real loudspeaker
directions. The so obtained results confirm the hypothesis of the possibility
to modulate independently the brain responses to spatial types and depths of
sound sources which allows for the development of the novel multi-command aBCI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2950</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2950</id><created>2012-10-10</created><authors><author><keyname>Rosenkranz</keyname><forenames>Markus</forenames></author><author><keyname>Regensburger</keyname><forenames>Georg</forenames></author><author><keyname>Tec</keyname><forenames>Loredana</forenames></author><author><keyname>Buchberger</keyname><forenames>Bruno</forenames></author></authors><title>Symbolic Analysis for Boundary Problems: From Rewriting to Parametrized
  Gr\&quot;obner Bases</title><categories>cs.SC cs.MS math.CA</categories><comments>54 pages</comments><msc-class>65L10, 34B05, 13P10, 54J05, 45P05, 68W30</msc-class><journal-ref>Numerical and Symbolic Scientific Computing, Vol. 1, pp. 273-331,
  2012</journal-ref><doi>10.1007/978-3-7091-0794-2_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review our algebraic framework for linear boundary problems (concentrating
on ordinary differential equations). Its starting point is an appropriate
algebraization of the domain of functions, which we have named
integro-differential algebras. The algebraic treatment of boundary problems
brings up two new algebraic structures whose symbolic representation and
computational realization is based on canonical forms in certain commutative
and noncommutative polynomial domains. The first of these, the ring of
integro-differential operators, is used for both stating and solving linear
boundary problems. The other structure, called integro-differential
polynomials, is the key tool for describing extensions of integro-differential
algebras. We use the canonical simplifier for integro-differential polynomials
for generating an automated proof establishing a canonical simplifier for
integro-differential operators. Our approach is fully implemented in the
Theorema system; some code fragments and sample computations are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2951</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2951</id><created>2012-10-10</created><authors><author><keyname>Korporal</keyname><forenames>Anja</forenames></author><author><keyname>Regensburger</keyname><forenames>Georg</forenames></author><author><keyname>Rosenkranz</keyname><forenames>Markus</forenames></author></authors><title>Regular and Singular Boundary Problems in Maple</title><categories>cs.SC cs.MS</categories><comments>14 pages; Berlin/Heidelberg, Springer</comments><msc-class>68W30</msc-class><journal-ref>Computer Algebra in Scientific Computing (CASC 2011), LNCS 6885,
  pp. 280-293, 2011</journal-ref><doi>10.1007/978-3-642-23568-9_22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new Maple package for treating boundary problems for linear
ordinary differential equations, allowing two-/multipoint as well as Stieltjes
boundary conditions. For expressing differential operators, boundary
conditions, and Green's operators, we employ the algebra of
integro-differential operators. The operations implemented for regular boundary
problems include computing Green's operators as well as composing and factoring
boundary problems. Our symbolic approach to singular boundary problems is new;
it provides algorithms for computing compatibility conditions and generalized
Green's operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2959</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2959</id><created>2012-10-10</created><authors><author><keyname>Chang</keyname><forenames>Moonjeong</forenames></author><author><keyname>Nishikawa</keyname><forenames>Nozomu</forenames></author><author><keyname>Cai</keyname><forenames>Zhenyu</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Psychophysical Responses Comparison in Spatial Visual, Audiovisual, and
  Auditory BCI-Spelling Paradigms</title><categories>cs.HC q-bio.NC</categories><comments>The 6th International Conference on Soft Computing and Intelligent
  Systems and The 13th International Symposium on Advanced Intelligent Systems,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a pilot study conducted with spatial visual, audiovisual
and auditory brain-computer-interface (BCI) based speller paradigms. The
psychophysical experiments are conducted with healthy subjects in order to
evaluate a difficulty and a possible response accuracy variability. We also
present preliminary EEG results in offline BCI mode. The obtained results
validate a thesis, that spatial auditory only paradigm performs as good as the
traditional visual and audiovisual speller BCI tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2967</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2967</id><created>2012-10-10</created><authors><author><keyname>Goldenbaum</keyname><forenames>Mario</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Robust Analog Function Computation via Wireless Multiple-Access Channels</title><categories>cs.IT cs.DC cs.MA math.IT</categories><comments>30 pages (onecolumn), 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various wireless sensor network applications involve the computation of a
pre-defined function of the measurements without the need for reconstructing
each individual sensor reading. Widely-considered examples of such functions
include the arithmetic mean and the maximum value. Standard approaches to the
computation problem separate computation from communication: quantized sensor
readings are transmitted interference-free to a fusion center that reconstructs
each sensor reading and subsequently computes the sought function value. Such
separation-based computation schemes are generally highly inefficient as a
complete reconstruction of individual sensor readings is not necessary for the
fusion center to compute a function of them. In particular, if the mathematical
structure of the wireless channel is suitably matched (in some sense) to the
function, then channel collisions induced by concurrent transmissions of
different nodes can be beneficially exploited for computation purposes.
Therefore, in this paper a practically relevant analog computation scheme is
proposed that allows for an efficient estimate of linear and nonlinear
functions over the wireless multiple-access channel. After analyzing the
asymptotic properties of the estimation error, numerical simulations are
presented to show the potential for huge performance gains when compared with
time-division multiple-access based computation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2971</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2971</id><created>2012-10-10</created><authors><author><keyname>Saravanan</keyname><forenames>K.</forenames></author><author><keyname>Saranya</keyname><forenames>C.</forenames></author><author><keyname>Saranya</keyname><forenames>M.</forenames></author></authors><title>A new application of Multi modal Biometrics in home and office security
  system</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric door lock security systems are used at those places where you have
important information and stuffs. In that kind of places multibiometric
electronic door lock security systems that are based on finger print and iris
recognization.Multibiometric door lock security systems are used to prevent the
door related burglaries such as break ins occurred in different forms so this
is the best method to prevent this type of happenings. Unlike keyed locks,
there is no need to take the keys with you when you go out without necessary of
worrying about losing keys.This paper proposes a multimodal biometrics door
lock system with iris and fingerprint as a computer application for
automatically identifying or verifying a person from fingerprint iris
recognition system. The first stage is identification and second one is
verifying that whether he is a genuine user or imposter. During second stage
system compares the input set with all available stored set in database. This
comparison gives a ranked list of matches. Based on the rank retrieved an alarm
is activated automatically when any unauthorized person tries to open the door.
So this kind of multimodal biometrics will provide a highly secured and
authenticated access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2972</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2972</id><created>2012-10-10</created><updated>2012-10-19</updated><authors><author><keyname>Darondeau</keyname><forenames>Philippe</forenames><affiliation>INRIA Rennes Bretagne Atlantique</affiliation></author><author><keyname>Demri</keyname><forenames>Stephane</forenames><affiliation>LSV, ENS Cachan, CNRS, INRIA</affiliation></author><author><keyname>Meyer</keyname><forenames>Roland</forenames><affiliation>University of Kaiserslautern</affiliation></author><author><keyname>Morvan</keyname><forenames>Christophe</forenames><affiliation>Universit&#xe9; Paris-est Marne-la-Vall&#xe9;e</affiliation></author></authors><title>Petri Net Reachability Graphs: Decidability Status of First Order
  Properties</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.1.1; F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 4 (October
  22, 2012) lmcs:872</journal-ref><doi>10.2168/LMCS-8(4:9)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the decidability and complexity status of model-checking
problems on unlabelled reachability graphs of Petri nets by considering
first-order and modal languages without labels on transitions or atomic
propositions on markings. We consider several parameters to separate decidable
problems from undecidable ones. Not only are we able to provide precise borders
and a systematic analysis, but we also demonstrate the robustness of our proof
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2977</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2977</id><created>2012-10-10</created><authors><author><keyname>Saravanan</keyname><forenames>K.</forenames></author><author><keyname>Akshaya</keyname><forenames>S.</forenames></author><author><keyname>Pavithra</keyname><forenames>R.</forenames></author><author><keyname>Pushpavalli</keyname><forenames>K.</forenames></author></authors><title>An Effective Fusion Technique of Cloud Computing and Networking Series</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is making it possible to separate the process of building an
infrastructure for service provisioning from the business of providing end user
services. Today, such infrastructures are normally provided in large data
centres and the applications are executed remotely from the users. One reason
for this is that cloud computing requires a reasonably stable infrastructure
and networking environment, largely due to management reasons. Networking of
Information (NetInf) is an information centric networking paradigm that can
support cloud computing by providing new possibilities for network transport
and storage. It offers direct access to information objects through a simple
API, independent of their location in the network. This abstraction can hide
much of the complexity of storage and network transport systems that cloud
computing today has to deal with. In this paper we analyze how cloud computing
and NetInf can be combined to make cloud computing infrastructures easier to
manage, and potentially enable deployment in smaller and more dynamic
networking environments. NetInf should thus be understood as an enhancement to
the infrastructure for cloud computing rather than a change to cloud computing
technology as such. To illustrate the approach taken by NetInf, we also
describe how it can be implemented by introducing a specific name resolution
and routing mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2984</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2984</id><created>2012-10-10</created><updated>2012-10-29</updated><authors><author><keyname>Lisi</keyname><forenames>Francesca A.</forenames></author></authors><title>Learning Onto-Relational Rules with Inductive Logic Programming</title><categories>cs.AI cs.DB cs.LG cs.LO</categories><comments>18 pages. arXiv admin note: text overlap with arXiv:1003.2586</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rules complement and extend ontologies on the Semantic Web. We refer to these
rules as onto-relational since they combine DL-based ontology languages and
Knowledge Representation formalisms supporting the relational data model within
the tradition of Logic Programming and Deductive Databases. Rule authoring is a
very demanding Knowledge Engineering task which can be automated though
partially by applying Machine Learning algorithms. In this chapter we show how
Inductive Logic Programming (ILP), born at the intersection of Machine Learning
and Logic Programming and considered as a major approach to Relational
Learning, can be adapted to Onto-Relational Learning. For the sake of
illustration, we provide details of a specific Onto-Relational Learning
solution to the problem of learning rule-based definitions of DL concepts and
roles with ILP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.2987</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.2987</id><created>2012-10-10</created><updated>2015-02-11</updated><authors><author><keyname>Thapper</keyname><forenames>Johan</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>The complexity of finite-valued CSPs</title><categories>cs.CC</categories><comments>A full version of a STOC'13 paper, submitted for journal publication</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of exact minimisation of separable
rational-valued discrete functions. Let $\Gamma$ be a set of rational-valued
functions on a fixed finite domain; such a set is called a finite-valued
constraint language. The valued constraint satisfaction problem,
$\operatorname{VCSP}(\Gamma)$, is the problem of minimising a function given as
a sum of functions from $\Gamma$. We establish a dichotomy theorem with respect
to exact solvability for all finite-valued constraint languages defined on
domains of arbitrary finite size.
  We show that every constraint language $\Gamma$ either admits a binary
symmetric fractional polymorphism in which case the basic linear programming
relaxation solves any instance of $\operatorname{VCSP}(\Gamma)$ exactly, or
$\Gamma$ satisfies a simple hardness condition that allows for a
polynomial-time reduction from Max-Cut to $\operatorname{VCSP}(\Gamma)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3012</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3012</id><created>2012-10-08</created><authors><author><keyname>Joshi</keyname><forenames>Gauri</forenames></author><author><keyname>Liu</keyname><forenames>Yanpei</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author></authors><title>Coding for Fast Content Download</title><categories>cs.IT cs.DC math.IT</categories><comments>8 pages, 6 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental trade-off between storage and content download time.
We show that the download time can be significantly reduced by dividing the
content into chunks, encoding it to add redundancy and then distributing it
across multiple disks. We determine the download time for two content access
models - the fountain and fork-join models that involve simultaneous content
access, and individual access from enqueued user requests respectively. For the
fountain model we explicitly characterize the download time, while in the
fork-join model we derive the upper and lower bounds. Our results show that
coding reduces download time, through the diversity of distributing the data
across more disks, even for the total storage used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3039</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3039</id><created>2012-10-10</created><authors><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author></authors><title>Sequential Convex Programming Methods for A Class of Structured
  Nonlinear Programming</title><categories>math.OC cs.NA stat.CO stat.ML</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a broad class of structured nonlinear programming
(SNLP) problems. In particular, we first establish the first-order optimality
conditions for them. Then we propose sequential convex programming (SCP)
methods for solving them in which each iteration is obtained by solving a
convex programming problem exactly or inexactly. Under some suitable
assumptions, we establish that any accumulation point of the sequence generated
by the methods is a KKT point of the SNLP problems. In addition, we propose a
variant of the exact SCP method for SNLP in which nonmonotone scheme and
&quot;local&quot; Lipschitz constants of the associated functions are used. And a similar
convergence result as mentioned above is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3047</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3047</id><created>2012-10-09</created><authors><author><keyname>Das</keyname><forenames>Sanjoy</forenames></author><author><keyname>Lobiyal</keyname><forenames>D. K</forenames></author></authors><title>A Performance Analysis of LAR Protocol for Vehicular Ad Hoc Networks in
  City Scenarios</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, performance analysis of Location Aided Routing (LAR) protocol
in different city scenarios has been done. The mobility model considered is
Manhattan model. This mobility model used to emulate the movement pattern of
nodes i.e., vehicles on streets defined by maps. Our objective is to provide a
qualitative analysis of the LAR protocol in different city scenarios in
Vehicular Ad hoc Networks. We have considered three different city scenarios
for the analysis of the protocol. The simulation work has been conducted using
the Glomosim 2.03 simulator. The results show that LAR1 protocol achieves
maximum packet delivery ratio is 99.68 % and maximum average end-to-end delay
is 7.319969 ms when the network is sparsely populated. Further, for densely
populated network maximum achieved packet delivery ratio is 87.58% and average
end-to-end delay is 0.017684 ms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3050</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3050</id><created>2012-10-10</created><authors><author><keyname>Nargesi</keyname><forenames>Abbas</forenames></author><author><keyname>Ghasemi</keyname><forenames>Mehdi</forenames></author></authors><title>Using Orthogonal Channels for Supporting Multicast Service in
  Multi-channel Wireless Mesh Networks</title><categories>cs.NI</categories><comments>11 pages, 9 figures, 2 tables, nternational Journal of Computer
  Networks &amp; Communications (IJCNC) Vol.4, No.5, September 2012</comments><msc-class>2012</msc-class><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.4, No.5, September 2012</journal-ref><doi>10.5121/ijcnc.2012.4506</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike wired networks, the capacity of a wireless network is interference
limited due to the broadcast nature of wireless medium. Some multicast wireless
network protocols do not consider channel assignment issue, that they cause
interference at transmission nodes, hence do not use full capacity of the
network. Interference can be reduced and throughput improved with the use of
multichannel features. Therefore, this paper used orthogonal channels for
sending and receiving nodes in the network. We propose EWM (Efficient Wireless
Multicast) method that is distributed scheme for constructing multicast tree in
multi-channel multi-interface wireless mesh networks (MIMC-WMN) which selects
relay nodes and in distributed form assign orthogonal radio channels to them.
To more decrease of interference in adding a branch to the tree, the route with
minimum end-to-end delay from the source to the multicast receiver will be
chosen. Thus, the tree is suitable for multimedia applicants. We also employ
the broadcast nature of the wireless media to reduce the number of relay nodes.
The proposed algorithm is compared with MCM algorithm in NS2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3061</identifier>
 <datestamp>2013-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3061</id><created>2012-10-10</created><updated>2013-01-24</updated><authors><author><keyname>Leone</keyname><forenames>Pierre</forenames></author><author><keyname>Schiller</keyname><forenames>Elad M.</forenames></author></authors><title>Self-Stabilizing TDMA Algorithms for Dynamic Wireless Ad-hoc Networks</title><categories>cs.NI cs.DC</categories><doi>10.1155/2013/639761</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dynamic wireless ad-hoc networks (DynWANs), autonomous computing devices
set up a network for the communication needs of the moment. These networks
require the implementation of a medium access control (MAC) layer. We consider
MAC protocols for DynWANs that need to be autonomous and robust as well as have
high bandwidth utilization, high predictability degree of bandwidth allocation,
and low communication delay in the presence of frequent topological changes to
the communication network. Recent studies have shown that existing
implementations cannot guarantee the necessary satisfaction of these timing
requirements. We propose a self-stabilizing MAC algorithm for DynWANs that
guarantees a short convergence period, and by that, it can facilitate the
satisfaction of severe timing requirements, such as the above. Besides the
contribution in the algorithmic front of research, we expect that our proposal
can enable quicker adoption by practitioners and faster deployment of DynWANs
that are subject changes in the network topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3075</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3075</id><created>2012-10-10</created><authors><author><keyname>Tsybakov</keyname><forenames>B. S.</forenames></author><author><keyname>Tsybakov</keyname><forenames>A. B.</forenames></author></authors><title>On Walsh code assignment</title><categories>cs.IT math.IT</categories><msc-class>68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers the problem of orthogonal variable spreading Walsh-code
assignments. The aim of the paper is to provide assignments that can avoid both
complicated signaling from the BS to the users and blind rate and code
detection amongst a great number of possible codes. The assignments considered
here use a partition of all users into several pools. Each pool can use its own
codes that are different for different pools. Each user has only a few codes
assigned to it within the pool. We state the problem as a combinatorial one
expressed in terms of a binary n x k matrix M where is the number n of users,
and k is the number of Walsh codes in the pool. A solution to the problem is
given as a construction of M, which has the assignment property defined in the
paper. Two constructions of such M are presented under different conditions on
n and k. The first construction is optimal in the sense that it gives the
minimal number of Walsh codes assigned to each user for given n and k. The
optimality follows from a proved necessary condition for the existence of M
with the assignment property. In addition, we propose a simple algorithm of
optimal assignment for the first construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3077</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3077</id><created>2012-10-10</created><authors><author><keyname>Zhang</keyname><forenames>Miranda</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Haller</keyname><forenames>Armin</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Strazdins</keyname><forenames>Peter</forenames></author></authors><title>Investigating Decision Support Techniques for Automating Cloud Service
  Selection</title><categories>cs.DC</categories><comments>Accepted by IEEE Cloudcom 2012 - PhD consortium track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The compass of Cloud infrastructure services advances steadily leaving users
in the agony of choice. To be able to select the best mix of service offering
from an abundance of possibilities, users must consider complex dependencies
and heterogeneous sets of criteria. Therefore, we present a PhD thesis proposal
on investigating an intelligent decision support system for selecting Cloud
based infrastructure services (e.g. storage, network, CPU).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3091</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3091</id><created>2012-10-10</created><authors><author><keyname>Bejuri</keyname><forenames>Wan Mohd Yaakob Wan</forenames></author><author><keyname>Mohamad</keyname><forenames>Mohd Murtadha</forenames></author><author><keyname>Sapri</keyname><forenames>Maimunah</forenames></author><author><keyname>Rosly</keyname><forenames>Mohd Adly</forenames></author></authors><title>Performance Evaluation of Mobile U-Navigation based on GPS/WLAN
  Hybridization</title><categories>cs.OH</categories><comments>Journal of Convergence Information Technology(JCIT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper present our mobile u-navigation system. This approach utilizes
hybridization of wireless local area network and Global Positioning System
internal sensor which to receive signal strength from access point and the same
time retrieve Global Navigation System Satellite signal. This positioning
information will be switched based on type of environment in order to ensure
the ubiquity of positioning system. Finally we present our results to
illustrate the performance of the localization system for an indoor/ outdoor
environment set-up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3092</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3092</id><created>2012-10-10</created><updated>2014-04-20</updated><authors><author><keyname>Du</keyname><forenames>Dong</forenames></author></authors><title>Contributions to Persistence Theory</title><categories>cs.CG math.AT</categories><comments>arXiv admin note: text overlap with arXiv:1104.5646 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a method to calculate the bar codes of a PCD (point cloud
data) with real coefficients in Section 3. With Dan Burghelea and Tamal Dey we
developed a persistence theory which involves level sets discussed in Section
4. This paper is the Ph.D thesis written under the direction of Dan Burghelea
at OSU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3098</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3098</id><created>2012-10-10</created><updated>2013-03-23</updated><authors><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>Near-optimal compressed sensing guarantees for total variation
  minimization</title><categories>math.NA cs.CV cs.IT math.IT</categories><msc-class>41A46, 68Q25, 68W20, 90C27</msc-class><doi>10.1109/TIP.2013.2264681</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of reconstructing a multidimensional signal from an
underdetermined set of measurements, as in the setting of compressed sensing.
Without any additional assumptions, this problem is ill-posed. However, for
signals such as natural images or movies, the minimal total variation estimate
consistent with the measurements often produces a good approximation to the
underlying signal, even if the number of measurements is far smaller than the
ambient dimensionality. This paper extends recent reconstruction guarantees for
two-dimensional images to signals of arbitrary dimension d&gt;1 and to isotropic
total variation problems. To be precise, we show that a multidimensional signal
x can be reconstructed from O(sd*log(N^d)) linear measurements using total
variation minimization to within a factor of the best s-term approximation of
its gradient. The reconstruction guarantees we provide are necessarily optimal
up to polynomial factors in the spatial dimension d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3101</identifier>
 <datestamp>2012-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3101</id><created>2012-10-10</created><updated>2012-11-02</updated><authors><author><keyname>Lee</keyname><forenames>Kwankyu</forenames></author><author><keyname>Bras-Amor&#xf3;s</keyname><forenames>Maria</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Michael E.</forenames></author></authors><title>Unique Decoding of General AG Codes</title><categories>cs.IT math.IT</categories><comments>11 pages,submitted to the IEEE Transactions on Information Theory;
  added a citation in the section III-C</comments><msc-class>94B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A unique decoding algorithm for general AG codes, namely multipoint
evaluation codes on algebraic curves, is presented. It is a natural
generalization of the previous decoding algorithm which was only for one-point
AG codes. As such, it retains the same advantages of fast speed and regular
structure with the previous algorithm. Compared with other known decoding
algorithms for general AG codes, it is much simpler in its description and
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3110</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3110</id><created>2012-10-10</created><authors><author><keyname>Lai</keyname><forenames>Han</forenames></author><author><keyname>Peng</keyname><forenames>Rong</forenames></author><author><keyname>Sun</keyname><forenames>Dong</forenames></author><author><keyname>Liu</keyname><forenames>Jia</forenames></author></authors><title>A lightweight forum-based distributed requirement elicitation process
  for open source community</title><categories>cs.SE cs.CY</categories><doi>10.4156/ijact.vol4.issue7.15</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Nowadays, lots of open source communities adopt forum to acquire scattered
stakeholders' requirements. But the requirements collection process always
suffers from the unformatted description and unfocused discussions. In this
paper, we establish a framework ReqForum to define the metamodel of the
requirement elicitation forum. Based on it, we propose a lightweight
forum-based requirements elicitation process which includes six steps:
template-based requirements creation, opinions collection, requirements
collection, requirements management, capability identification and the
incentive mechanism. According to the proposed process, the prototype
SKLSEForum is established by composing the Discuz and its existed pulg-ins. The
implementation indicates that the process is feasible and the cost is economic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3114</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3114</id><created>2012-10-10</created><authors><author><keyname>Aschieri</keyname><forenames>Federico</forenames><affiliation>Lab. PPS, Paris 7</affiliation></author><author><keyname>Zorzi</keyname><forenames>Margherita</forenames><affiliation>Laboratoire d&#xc2;'Informatique de Paris-Nord</affiliation></author></authors><title>Interactive Realizability and the elimination of Skolem functions in
  Peano Arithmetic</title><categories>cs.LO</categories><comments>In Proceedings CL&amp;C 2012, arXiv:1210.2890</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 97, 2012, pp. 1-18</journal-ref><doi>10.4204/EPTCS.97.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new syntactical proof that first-order Peano Arithmetic with
Skolem axioms is conservative over Peano Arithmetic alone for arithmetical
formulas. This result - which shows that the Excluded Middle principle can be
used to eliminate Skolem functions - has been previously proved by other
techniques, among them the epsilon substitution method and forcing. In our
proof, we employ Interactive Realizability, a computational semantics for Peano
Arithmetic which extends Kreisel's modified realizability to the classical
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3115</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3115</id><created>2012-10-10</created><authors><author><keyname>Krebbers</keyname><forenames>Robbert</forenames><affiliation>Radboud University Nijmegen</affiliation></author></authors><title>A call-by-value lambda-calculus with lists and control</title><categories>cs.LO</categories><comments>In Proceedings CL&amp;C 2012, arXiv:1210.2890</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 97, 2012, pp. 19-33</journal-ref><doi>10.4204/EPTCS.97.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculi with control operators have been studied to reason about control in
programming languages and to interpret the computational content of classical
proofs. To make these calculi into a real programming language, one should also
include data types.
  As a step into that direction, this paper defines a simply typed
call-by-value lambda calculus with the control operators catch and throw, a
data type of lists, and an operator for primitive recursion (a la Goedel's T).
We prove that our system satisfies subject reduction, progress, confluence for
untyped terms, and strong normalization for well-typed terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3116</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3116</id><created>2012-10-10</created><authors><author><keyname>Nakazawa</keyname><forenames>Koji</forenames><affiliation>Graduate School of Informatics, Kyoto University</affiliation></author><author><keyname>Katsumata</keyname><forenames>Shin-ya</forenames><affiliation>Research Institute for Mathematical Sciences, Kyoto University</affiliation></author></authors><title>Extensional Models of Untyped Lambda-mu Calculus</title><categories>cs.LO</categories><comments>In Proceedings CL&amp;C 2012, arXiv:1210.2890</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 97, 2012, pp. 35-47</journal-ref><doi>10.4204/EPTCS.97.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes new mathematical models of the untyped Lambda-mu
calculus. One is called the stream model, which is an extension of the lambda
model, in which each term is interpreted as a function from streams to
individual data. The other is called the stream combinatory algebra, which is
an extension of the combinatory algebra, and it is proved that the extensional
equality of the Lambda-mu calculus is equivalent to equality in stream
combinatory algebras. In order to define the stream combinatory algebra, we
introduce a combinatory calculus SCL, which is an abstraction-free system
corresponding to the Lambda-mu calculus. Moreover, it is shown that stream
models are algebraically characterized as a particular class of stream
combinatory algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3117</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3117</id><created>2012-10-10</created><authors><author><keyname>Powell</keyname><forenames>Thomas</forenames><affiliation>Queen Mary, University of London, United Kingdom</affiliation></author></authors><title>Applying G\&quot;odel's Dialectica Interpretation to Obtain a Constructive
  Proof of Higman's Lemma</title><categories>cs.LO cs.DM</categories><comments>In Proceedings CL&amp;C 2012, arXiv:1210.2890</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 97, 2012, pp. 49-62</journal-ref><doi>10.4204/EPTCS.97.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use G\&quot;odel's Dialectica interpretation to analyse Nash-Williams' elegant
but non-constructive &quot;minimal bad sequence&quot; proof of Higman's Lemma. The result
is a concise constructive proof of the lemma (for arbitrary decidable
well-quasi-orders) in which Nash-Williams' combinatorial idea is clearly
present, along with an explicit program for finding an embedded pair in
sequences of words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3121</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3121</id><created>2012-10-11</created><updated>2014-08-30</updated><authors><author><keyname>Zheng</keyname><forenames>Bojin</forenames></author><author><keyname>Wu</keyname><forenames>Hongrun</forenames></author><author><keyname>Kuang</keyname><forenames>Li</forenames></author><author><keyname>Qin</keyname><forenames>Jun</forenames></author><author><keyname>Du</keyname><forenames>Wenhua</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author><author><keyname>Li</keyname><forenames>Deyi</forenames></author></authors><title>A simple model clarifies the complicated relationships of complex
  networks</title><categories>cs.SI nlin.AO physics.soc-ph</categories><journal-ref>Sci. Rep. 4, 6197 (2014)</journal-ref><doi>10.1038/srep06197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world networks such as the Internet and WWW have many common traits.
Until now, hundreds of models were proposed to characterize these traits for
understanding the networks. Because different models used very different
mechanisms, it is widely believed that these traits origin from different
causes. However, we find that a simple model based on optimisation can produce
many traits, including scale-free, small-world, ultra small-world,
Delta-distribution, compact, fractal, regular and random networks. Moreover, by
revising the proposed model, the community-structure networks are generated. By
this model and the revised versions, the complicated relationships of complex
networks are illustrated. The model brings a new universal perspective to the
understanding of complex networks and provide a universal method to model
complex networks from the viewpoint of optimisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3131</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3131</id><created>2012-10-11</created><authors><author><keyname>Ghiam</keyname><forenames>Shekoofeh</forenames></author><author><keyname>Pour</keyname><forenames>Alireza Nemaney</forenames></author></authors><title>A Survey on Web Spam Detection Methods: Taxonomy</title><categories>cs.IR cs.CR</categories><comments>16 pages, 7 figures, 7 tables</comments><doi>10.5121/ijnsa.2012.4510</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web spam refers to some techniques, which try to manipulate search engine
ranking algorithms in order to raise web page position in search engine
results. In the best case, spammers encourage viewers to visit their sites, and
provide undeserved advertisement gains to the page owner. In the worst case,
they use malicious contents in their pages and try to install malware on the
victims machine. Spammers use three kinds of spamming techniques to get higher
score in ranking. These techniques are Link based techniques, hiding techniques
and content-based techniques. Existing spam pages cause distrust to search
engine results. This not only wastes the time of visitors, but also wastes lots
of search engine resources. Hence spam detection methods have been proposed as
a solution for web spam in order to reduce negative effects of spam pages.
Experimental results show that some of these techniques are working well and
can find spam pages more accurate than the others. This paper classifies web
spam techniques and the related detection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3135</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3135</id><created>2012-10-11</created><updated>2013-03-21</updated><authors><author><keyname>Meng</keyname><forenames>Xiangrui</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Low-distortion Subspace Embeddings in Input-sparsity Time and
  Applications to Robust Linear Regression</title><categories>cs.DS</categories><comments>22 pages</comments><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-distortion embeddings are critical building blocks for developing random
sampling and random projection algorithms for linear algebra problems. We show
that, given a matrix $A \in \R^{n \times d}$ with $n \gg d$ and a $p \in [1,
2)$, with a constant probability, we can construct a low-distortion embedding
matrix $\Pi \in \R^{O(\poly(d)) \times n}$ that embeds $\A_p$, the $\ell_p$
subspace spanned by $A$'s columns, into $(\R^{O(\poly(d))}, \| \cdot \|_p)$;
the distortion of our embeddings is only $O(\poly(d))$, and we can compute $\Pi
A$ in $O(\nnz(A))$ time, i.e., input-sparsity time. Our result generalizes the
input-sparsity time $\ell_2$ subspace embedding by Clarkson and Woodruff
[STOC'13]; and for completeness, we present a simpler and improved analysis of
their construction for $\ell_2$. These input-sparsity time $\ell_p$ embeddings
are optimal, up to constants, in terms of their running time; and the improved
running time propagates to applications such as $(1\pm \epsilon)$-distortion
$\ell_p$ subspace embedding and relative-error $\ell_p$ regression. For
$\ell_2$, we show that a $(1+\epsilon)$-approximate solution to the $\ell_2$
regression problem specified by the matrix $A$ and a vector $b \in \R^n$ can be
computed in $O(\nnz(A) + d^3 \log(d/\epsilon) /\epsilon^2)$ time; and for
$\ell_p$, via a subspace-preserving sampling procedure, we show that a $(1\pm
\epsilon)$-distortion embedding of $\A_p$ into $\R^{O(\poly(d))}$ can be
computed in $O(\nnz(A) \cdot \log n)$ time, and we also show that a
$(1+\epsilon)$-approximate solution to the $\ell_p$ regression problem $\min_{x
\in \R^d} \|A x - b\|_p$ can be computed in $O(\nnz(A) \cdot \log n + \poly(d)
\log(1/\epsilon)/\epsilon^2)$ time. Moreover, we can improve the embedding
dimension or equivalently the sample size to $O(d^{3+p/2} \log(1/\epsilon) /
\epsilon^2)$ without increasing the complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3139</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3139</id><created>2012-10-11</created><authors><author><keyname>Kumar</keyname><forenames>Pardeep</forenames></author><author><keyname>Nitin</keyname></author><author><keyname>Sehgal</keyname><forenames>Vivek Kumar</forenames></author><author><keyname>Chauhan</keyname><forenames>Durg Singh</forenames></author></authors><title>A Benchmark to Select Data Mining Based Classification Algorithms For
  Business Intelligence And Decision Support Systems</title><categories>cs.DB cs.LG</categories><comments>18 Pages, 11 Figures, 6 Tables, Journal</comments><journal-ref>International Journal of Data Mining and Knowledge Discovery
  Process, September 2012, ISSN: 2230-9608</journal-ref><doi>10.5121/ijdkp.2012.2503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DSS serve the management, operations, and planning levels of an organization
and help to make decisions, which may be rapidly changing and not easily
specified in advance. Data mining has a vital role to extract important
information to help in decision making of a decision support system.
Integration of data mining and decision support systems (DSS) can lead to the
improved performance and can enable the tackling of new types of problems.
Artificial Intelligence methods are improving the quality of decision support,
and have become embedded in many applications ranges from ant locking
automobile brakes to these days interactive search engines. It provides various
machine learning techniques to support data mining. The classification is one
of the main and valuable tasks of data mining. Several types of classification
algorithms have been suggested, tested and compared to determine the future
trends based on unseen data. There has been no single algorithm found to be
superior over all others for all data sets. The objective of this paper is to
compare various classification algorithms that have been frequently used in
data mining for decision support systems. Three decision trees based
algorithms, one artificial neural network, one statistical, one support vector
machines with and without ada boost and one clustering algorithm are tested and
compared on four data sets from different domains in terms of predictive
accuracy, error rate, classification index, comprehensibility and training
time. Experimental results demonstrate that Genetic Algorithm (GA) and support
vector machines based algorithms are better in terms of predictive accuracy.
SVM without adaboost shall be the first choice in context of speed and
predictive accuracy. Adaboost improves the accuracy of SVM but on the cost of
large training time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3141</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3141</id><created>2012-10-11</created><updated>2013-08-08</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Velner</keyname><forenames>Yaron</forenames></author></authors><title>Hyperplane Separation Technique for Multidimensional Mean-Payoff Games</title><categories>cs.GT cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1201.2829</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider both finite-state game graphs and recursive game graphs (or
pushdown game graphs), that can model the control flow of sequential programs
with recursion, with multi-dimensional mean-payoff objectives. In pushdown
games two types of strategies are relevant: global strategies, that depend on
the entire global history; and modular strategies, that have only local memory
and thus do not depend on the context of invocation. We present solutions to
several fundamental algorithmic questions and our main contributions are as
follows: (1) We show that finite-state multi-dimensional mean-payoff games can
be solved in polynomial time if the number of dimensions and the maximal
absolute value of the weight is fixed; whereas if the number of dimensions is
arbitrary, then problem is already known to be coNP-complete. (2) We show that
pushdown graphs with multi-dimensional mean-payoff objectives can be solved in
polynomial time. (3) For pushdown games under global strategies both single and
multi-dimensional mean-payoff objectives problems are known to be undecidable,
and we show that under modular strategies the multi-dimensional problem is also
undecidable (whereas under modular strategies the single dimensional problem is
NP-complete). We show that if the number of modules, the number of exits, and
the maximal absolute value of the weight is fixed, then pushdown games under
modular strategies with single dimensional mean-payoff objectives can be solved
in polynomial time, and if either of the number of exits or the number of
modules is not bounded, then the problem is NP-hard. (4) Finally we show that a
fixed parameter tractable algorithm for finite-state multi-dimensional
mean-payoff games or pushdown games under modular strategies with
single-dimensional mean-payoff objectives would imply the solution of the
long-standing open problem of fixed parameter tractability of parity games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3146</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3146</id><created>2012-10-11</created><updated>2013-05-20</updated><authors><author><keyname>Peltom&#xe4;ki</keyname><forenames>Jarkko</forenames></author></authors><title>Introducing Privileged Words: Privileged Complexity of Sturmian Words</title><categories>math.CO cs.DM</categories><comments>15 pages, 1 figure</comments><msc-class>68R15</msc-class><journal-ref>Theoretical Computer Science 500 (2013) pp. 57-67</journal-ref><doi>10.1016/j.tcs.2013.05.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the class of so-called privileged words which have
been previously considered only a little. We develop the basic properties of
privileged words, which turn out to share similar properties with palindromes.
Privileged words are studied in relation to previously studied classes of
words, rich words, Sturmian words and episturmian words. A new characterization
of Sturmian words is given in terms of privileged complexity. The privileged
complexity of the Thue-Morse word is also briefly studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3147</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3147</id><created>2012-10-11</created><authors><author><keyname>Rekha</keyname><forenames>S. Nithya</forenames></author><author><keyname>Chandrasekar</keyname><forenames>Dr. C.</forenames></author></authors><title>Performance Analysis of Probabilistic Rebroadcasting in Grid FSR for
  MANET</title><categories>cs.NI</categories><comments>8 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad-hoc Network (MANET) is the self organizing collection of mobile
nodes. The communication in MANET is done via a wireless media. Ad hoc wireless
networks have massive commercial and military potential because of their
mobility support. Due to demanding real time multimedia applications, Quality
of Services (QoS) support in such infrastructure less networks have become
essential. QoS routing in mobile Ad-Hoc networks is challenging due to rapid
change in network topology. In this paper, we focused to reduce flooding
performance of the Fisheye State Routing (FSR) protocol in Grid using ns-2
network simulator under different performance metrics scenario in respect to
number of Nodes. For example, the connection establishment is costly in terms
of time and resource where the network is mostly affected by connection request
flooding. The proposed approach presents a way to reduce flooding in MANETs.
Flooding is dictated by the propagation of connection-request packets from the
source to its neighborhood nodes. The proposed architecture embarks on the
concept of sharing neighborhood information. The proposed approach focuses on
exposing its neighborhood peer to another node that is referred to as its
friend-node, which had requested/forwarded connection request. If there is a
high probability for the friend node to communicate through the exposed routes,
this could improve the efficacy of bandwidth utilization by reducing flooding,
as the routes have been acquired, without any broadcasts. Friendship between
nodes is quantized based on empirical computations and heuristic algorithms.
The nodes store the neighborhood information in their cache that is
periodically verified for consistency. Simulation results show the performance
of this proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3165</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3165</id><created>2012-10-11</created><authors><author><keyname>Mollah</keyname><forenames>Ayatullah Faruk</forenames></author><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author></authors><title>Computationally Efficient Implementation of Convolution-based Locally
  Adaptive Binarization Techniques</title><categories>cs.CV</categories><journal-ref>Proc. of Int'l Conf. on Information Processing, Springer, CCIS
  292, pp. 159-168, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important steps of document image processing is binarization.
The computational requirements of locally adaptive binarization techniques make
them unsuitable for devices with limited computing facilities. In this paper,
we have presented a computationally efficient implementation of convolution
based locally adaptive binarization techniques keeping the performance
comparable to the original implementation. The computational complexity has
been reduced from O(W2N2) to O(WN2) where WxW is the window size and NxN is the
image size. Experiments over benchmark datasets show that the computation time
has been reduced by 5 to 15 times depending on the window size while memory
consumption remains the same with respect to the state-of-the-art algorithmic
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3171</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3171</id><created>2012-10-11</created><authors><author><keyname>Daltrophe</keyname><forenames>Hadassa</forenames></author><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Lotker</keyname><forenames>Zvi</forenames></author></authors><title>Data Interpolation: An Efficient Sampling Alternative for Big Data
  Aggregation</title><categories>cs.NI cs.DC</categories><comments>The Lynne and William Frankel Center for Computer Science, Ben-Gurion
  University of the Negev, 2012 #13-01</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Given a large set of measurement sensor data, in order to identify a simple
function that captures the essence of the data gathered by the sensors, we
suggest representing the data by (spatial) functions, in particular by
polynomials. Given a (sampled) set of values, we interpolate the datapoints to
define a polynomial that would represent the data. The interpolation is
challenging, since in practice the data can be noisy and even Byzantine, where
the Byzantine data represents an adversarial value that is not limited to being
close to the correct measured data. We present two solutions, one that extends
the Welch-Berlekamp technique in the case of multidimensional data, and copes
with discrete noise and Byzantine data, and the other based on Arora and Khot
techniques, extending them in the case of multidimensional noisy and Byzantine
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3187</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3187</id><created>2012-10-11</created><updated>2013-02-08</updated><authors><author><keyname>Swamy</keyname><forenames>Vasuki Narasimha</forenames></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>An asymptotically optimal push-pull method for multicasting over a
  random network</title><categories>cs.IT cs.NI math.IT</categories><comments>13 pages, extended version of paper presented at the IEEE
  International Symposium on Information Theory (ISIT) 2012, minor revision to
  text to address review comments, to appear in IEEE Transactions in
  information theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider allcast and multicast flow problems where either all of the nodes
or only a subset of the nodes may be in session. Traffic from each node in the
session has to be sent to every other node in the session. If the session does
not consist of all the nodes, the remaining nodes act as relays. The nodes are
connected by undirected links whose capacities are independent and identically
distributed random variables. We study the asymptotics of the capacity region
(with network coding) in the limit of a large number of nodes, and show that
the normalized sum rate converges to a constant almost surely. We then provide
a decentralized push-pull algorithm that asymptotically achieves this
normalized sum rate without network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3197</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3197</id><created>2012-10-11</created><authors><author><keyname>Babka</keyname><forenames>Martin</forenames></author><author><keyname>Bul&#xe1;nek</keyname><forenames>Jan</forenames></author><author><keyname>&#x10c;un&#xe1;t</keyname><forenames>Vladim&#xed;r</forenames></author><author><keyname>Kouck&#xfd;</keyname><forenames>Michal</forenames></author><author><keyname>Saks</keyname><forenames>Michael</forenames></author></authors><title>On Online Labeling with Polynomially Many Labels</title><categories>cs.DS</categories><comments>15 pages, Presented at European Symposium on Algorithms 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the online labeling problem with parameters n and m we are presented with
a sequence of n keys from a totally ordered universe U and must assign each
arriving key a label from the label set {1,2,...,m} so that the order of labels
(strictly) respects the ordering on U. As new keys arrive it may be necessary
to change the labels of some items; such changes may be done at any time at
unit cost for each change. The goal is to minimize the total cost. An
alternative formulation of this problem is the file maintenance problem, in
which the items, instead of being labeled, are maintained in sorted order in an
array of length m, and we pay unit cost for moving an item.
  For the case m=cn for constant c&gt;1, there are known algorithms that use at
most O(n log(n)^2) relabelings in total [Itai, Konheim, Rodeh, 1981], and it
was shown recently that this is asymptotically optimal [Bul\'anek, Kouck\'y,
Saks, 2012]. For the case of m={\Theta}(n^C) for C&gt;1, algorithms are known that
use O(n log n) relabelings. A matching lower bound was claimed in [Dietz,
Seiferas, Zhang, 2004]. That proof involved two distinct steps: a lower bound
for a problem they call prefix bucketing and a reduction from prefix bucketing
to online labeling. The reduction seems to be incorrect, leaving a (seemingly
significant) gap in the proof. In this paper we close the gap by presenting a
correct reduction to prefix bucketing. Furthermore we give a simplified and
improved analysis of the prefix bucketing lower bound. This improvement allows
us to extend the lower bounds for online labeling to the case where the number
m of labels is superpolynomial in n. In particular, for superpolynomial m we
get an asymptotically optimal lower bound {\Omega}((n log n) / (log log m - log
log n)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3210</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3210</id><created>2012-10-11</created><updated>2013-01-24</updated><authors><author><keyname>Crossley</keyname><forenames>Matthew</forenames></author><author><keyname>Nisbet</keyname><forenames>Andy</forenames></author><author><keyname>Amos</keyname><forenames>Martyn</forenames></author></authors><title>Fitness Landscape-Based Characterisation of Nature-Inspired Algorithms</title><categories>cs.NE</categories><comments>10 pages, 1 figure, submitted to the 11th International Conference on
  Adaptive and Natural Computing Algorithms</comments><doi>10.1007/978-3-642-37213-1_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant challenge in nature-inspired algorithmics is the identification
of specific characteristics of problems that make them harder (or easier) to
solve using specific methods. The hope is that, by identifying these
characteristics, we may more easily predict which algorithms are best-suited to
problems sharing certain features. Here, we approach this problem using fitness
landscape analysis. Techniques already exist for measuring the &quot;difficulty&quot; of
specific landscapes, but these are often designed solely with evolutionary
algorithms in mind, and are generally specific to discrete optimisation. In
this paper we develop an approach for comparing a wide range of continuous
optimisation algorithms. Using a fitness landscape generation technique, we
compare six different nature-inspired algorithms and identify which methods
perform best on landscapes exhibiting specific features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3211</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3211</id><created>2012-10-11</created><updated>2012-12-23</updated><authors><author><keyname>van Iersel</keyname><forenames>Leo</forenames></author><author><keyname>Kelk</keyname><forenames>Steven</forenames></author><author><keyname>Leki&#x107;</keyname><forenames>Nela</forenames></author><author><keyname>Stougie</keyname><forenames>Leen</forenames></author></authors><title>Approximation algorithms for nonbinary agreement forests</title><categories>math.CO cs.DM q-bio.PE</categories><comments>Note that this version contains significantly more results than the
  previous versions. Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two rooted phylogenetic trees on the same set of taxa X, the Maximum
Agreement Forest problem (MAF) asks to find a forest that is, in a certain
sense, common to both trees and has a minimum number of components. The Maximum
Acyclic Agreement Forest problem (MAAF) has the additional restriction that the
components of the forest cannot have conflicting ancestral relations in the
input trees. There has been considerable interest in the special cases of these
problems in which the input trees are required to be binary. However, in
practice, phylogenetic trees are rarely binary, due to uncertainty about the
precise order of speciation events. Here, we show that the general, nonbinary
version of MAF has a polynomial-time 4-approximation and a fixed-parameter
tractable (exact) algorithm that runs in O(4^k poly(n)) time, where n = |X| and
k is the number of components of the agreement forest minus one. Moreover, we
show that a c-approximation algorithm for nonbinary MAF and a d-approximation
algorithm for the classical problem Directed Feedback Vertex Set (DFVS) can be
combined to yield a d(c+3)-approximation for nonbinary MAAF. The algorithms for
MAF have been implemented and made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3234</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3234</id><created>2012-10-11</created><authors><author><keyname>Akcora</keyname><forenames>Cuneyt Gurcan</forenames></author><author><keyname>Carminati</keyname><forenames>Barbara</forenames></author><author><keyname>Ferrari</keyname><forenames>Elena</forenames></author></authors><title>Risks of Friendships on Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 8 figures, 3 tables. To Appear in the 2012 IEEE
  International Conference on Data Mining (ICDM)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we explore the risks of friends in social networks caused by
their friendship patterns, by using real life social network data and starting
from a previously defined risk model. Particularly, we observe that risks of
friendships can be mined by analyzing users' attitude towards friends of
friends. This allows us to give new insights into friendship and risk dynamics
on social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3241</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3241</id><created>2012-10-11</created><authors><author><keyname>Novacek</keyname><forenames>Vit</forenames></author></authors><title>Distributional Framework for Emergent Knowledge Acquisition and its
  Application to Automated Document Annotation</title><categories>cs.AI cs.IR</categories><acm-class>I.2.6; I.2.7; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces a framework for representation and acquisition of
knowledge emerging from large samples of textual data. We utilise a
tensor-based, distributional representation of simple statements extracted from
text, and show how one can use the representation to infer emergent knowledge
patterns from the textual data in an unsupervised manner. Examples of the
patterns we investigate in the paper are implicit term relationships or
conjunctive IF-THEN rules. To evaluate the practical relevance of our approach,
we apply it to annotation of life science articles with terms from MeSH (a
controlled biomedical vocabulary and thesaurus).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3252</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3252</id><created>2012-10-10</created><authors><author><keyname>Esmalifalak</keyname><forenames>Mohammad</forenames></author><author><keyname>Shi</keyname><forenames>Ge</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author></authors><title>Bad Data Injection Attack and Defense in Electricity Market using Game
  Theory Study</title><categories>cs.CR cs.GT stat.AP</categories><comments>To appear in IEEE Transactions on Smart Grid, Special Issue on Cyber,
  Physical, and System Security for Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications of cyber technologies improve the quality of monitoring and
decision making in smart grid. These cyber technologies are vulnerable to
malicious attacks, and compromising them can have serious technical and
economical problems. This paper specifies the effect of compromising each
measurement on the price of electricity, so that the attacker is able to change
the prices in the desired direction (increasing or decreasing). Attacking and
defending all measurements are impossible for the attacker and defender,
respectively. This situation is modeled as a zero sum game between the attacker
and defender. The game defines the proportion of times that the attacker and
defender like to attack and defend different measurements, respectively. From
the simulation results based on the PJM 5 Bus test system, we can show the
effectiveness and properties of the studied game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3265</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3265</id><created>2012-10-11</created><authors><author><keyname>Gebser</keyname><forenames>Martin</forenames></author><author><keyname>Kaufmann</keyname><forenames>Benjamin</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>Multi-threaded ASP Solving with clasp</title><categories>cs.LO cs.AI cs.DC</categories><comments>19 pages, 5 figures, to appear in Theory and Practice of Logic
  Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the new multi-threaded version of the state-of-the-art answer set
solver clasp. We detail its component and communication architecture and
illustrate how they support the principal functionalities of clasp. Also, we
provide some insights into the data representation used for different
constraint types handled by clasp. All this is accompanied by an extensive
experimental analysis of the major features related to multi-threading in
clasp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3266</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3266</id><created>2012-10-11</created><authors><author><keyname>Pellegrini</keyname><forenames>Marco</forenames></author><author><keyname>Geraci</keyname><forenames>Filippo</forenames></author><author><keyname>Baglioni</keyname><forenames>Miriam</forenames></author></authors><title>Detecting dense communities in large social and information networks
  with the Core &amp; Peel algorithm</title><categories>cs.SI cs.DS physics.soc-ph</categories><msc-class>F.2.2 Nonnumerical Algorithms and Problems</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting and characterizing dense subgraphs (tight communities) in social
and information networks is an important exploratory tool in social network
analysis. Several approaches have been proposed that either (i) partition the
whole network into clusters, even in low density region, or (ii) are aimed at
finding a single densest community (and need to be iterated to find the next
one). As social networks grow larger both approaches (i) and (ii) result in
algorithms too slow to be practical, in particular when speed in analyzing the
data is required. In this paper we propose an approach that aims at balancing
efficiency of computation and expressiveness and manageability of the output
community representation. We define the notion of a partial dense cover (PDC)
of a graph. Intuitively a PDC of a graph is a collection of sets of nodes that
(a) each set forms a disjoint dense induced subgraphs and (b) its removal
leaves the residual graph without dense regions. Exact computation of PDC is an
NP-complete problem, thus, we propose an efficient heuristic algorithms for
computing a PDC which we christen Core and Peel. Moreover we propose a novel
benchmarking technique that allows us to evaluate algorithms for computing PDC
using the classical IR concepts of precision and recall even without a golden
standard. Tests on 25 social and technological networks from the Stanford Large
Network Dataset Collection confirm that Core and Peel is efficient and attains
very high precison and recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3269</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3269</id><created>2012-10-11</created><updated>2012-10-12</updated><authors><author><keyname>Picciolo</keyname><forenames>Francesco</forenames></author><author><keyname>Squartini</keyname><forenames>Tiziano</forenames></author><author><keyname>Ruzzenenti</keyname><forenames>Franco</forenames></author><author><keyname>Basosi</keyname><forenames>Riccardo</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author></authors><title>The role of distances in the World Trade Web</title><categories>physics.soc-ph cs.SI q-fin.GN</categories><comments>Preprint, accepted for SITIS 2012 (http://www.sitis-conf.org/). Final
  version to be published by IEEE Computer Society as conference proceedings</comments><journal-ref>in Proceedings of the Eighth International Conference on
  Signal-Image Technology &amp; Internet-Based Systems (SITIS 2012), pp. 784-792
  (edited by IEEE) (2013)</journal-ref><doi>10.1109/SITIS.2012.118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the economic literature, geographic distances are considered fundamental
factors to be included in any theoretical model whose aim is the quantification
of the trade between countries. Quantitatively, distances enter into the
so-called gravity models that successfully predict the weight of non-zero trade
flows. However, it has been recently shown that gravity models fail to
reproduce the binary topology of the World Trade Web. In this paper a different
approach is presented: the formalism of exponential random graphs is used and
the distances are treated as constraints, to be imposed on a previously chosen
ensemble of graphs. Then, the information encoded in the geographical distances
is used to explain the binary structure of the World Trade Web, by testing it
on the degree-degree correlations and the reciprocity structure. This leads to
the definition of a novel null model that combines spatial and non-spatial
effects. The effectiveness of spatial constraints is compared to that of
nonspatial ones by means of the Akaike Information Criterion and the Bayesian
Information Criterion. Even if it is commonly believed that the World Trade Web
is strongly dependent on the distances, what emerges from our analysis is that
distances do not play a crucial role in shaping the World Trade Web binary
structure and that the information encoded into the reciprocity is far more
useful in explaining the observed patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3271</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3271</id><created>2012-10-11</created><authors><author><keyname>Nabrzyski</keyname><forenames>Jarek</forenames></author><author><keyname>Kurowski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Merzky</keyname><forenames>Andre</forenames></author></authors><title>Grid Computing: The Next Decade -- Report and Summary</title><categories>cs.DC cs.CY</categories><comments>17 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The evolution of the global scientific cyberinfrastructure (CI) has, over the
last 10+ years, led to a large diversity of CI instances. While specialized,
competing and alternative CI building blocks are inherent to a healthy
ecosystem, it also becomes apparent that the increasing degree of fragmentation
is hindering interoperation, and thus limiting collaboration, which is
essential for modern science communities often spanning international groups
and multiple disciplines (but even 'small sciences', with smaller and localized
communities, are often embedded into the larger scientific ecosystem, and are
increasingly dependent on the availability of CI.)
  There are different reasons why fragmentation occurs, on technical and social
level. But also, it is apparent that the current funding model for creating CI
components largely fails to aid the transition from research to production, by
mixing CS research and IT engineering challenges into the same funding
strategies.
  The 10th anniversary of the EU funded project 'Grid Lab' (which was an early
and ambitious attempt on providing a consolidated and science oriented
cyberinfrastructure software stack to a specific science community) was taken
as an opportunity to invite international leaders and early stage researchers
in grid computing and e-Science from Europe, America and Asia, and, together
with representatives of the EU and US funding agencies, to discuss the
fundamental aspects of CI evolution, and to contemplate the options for a more
coherent, more coordinated approach to the global evolution of CI.
  This open document represents the results of that workshop - including a
draft of a mission statement and a proposal for a blueprint process - to inform
the wider community as well as to encourage external experts to provide their
feedback and comments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3277</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3277</id><created>2012-10-11</created><updated>2014-08-27</updated><authors><author><keyname>Casteigts</keyname><forenames>Arnaud</forenames></author><author><keyname>Flocchini</keyname><forenames>Paola</forenames></author><author><keyname>Mans</keyname><forenames>Bernard</forenames></author><author><keyname>Santoro</keyname><forenames>Nicola</forenames></author></authors><title>Shortest, Fastest, and Foremost Broadcast in Dynamic Networks</title><categories>cs.DC cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly dynamic networks rarely offer end-to-end connectivity at a given time.
Yet, connectivity in these networks can be established over time and space,
based on temporal analogues of multi-hop paths (also called {\em journeys}).
Attempting to optimize the selection of the journeys in these networks
naturally leads to the study of three cases: shortest (minimum hop), fastest
(minimum duration), and foremost (earliest arrival) journeys. Efficient
centralized algorithms exists to compute all cases, when the full knowledge of
the network evolution is given.
  In this paper, we study the {\em distributed} counterparts of these problems,
i.e. shortest, fastest, and foremost broadcast with termination detection
(TDB), with minimal knowledge on the topology.
  We show that the feasibility of each of these problems requires distinct
features on the evolution, through identifying three classes of dynamic graphs
wherein the problems become gradually feasible: graphs in which the
re-appearance of edges is {\em recurrent} (class R), {\em bounded-recurrent}
(B), or {\em periodic} (P), together with specific knowledge that are
respectively $n$ (the number of nodes), $\Delta$ (a bound on the recurrence
time), and $p$ (the period). In these classes it is not required that all pairs
of nodes get in contact -- only that the overall {\em footprint} of the graph
is connected over time.
  Our results, together with the strict inclusion between $P$, $B$, and $R$,
implies a feasibility order among the three variants of the problem, i.e.
TDB[foremost] requires weaker assumptions on the topology dynamics than
TDB[shortest], which itself requires less than TDB[fastest]. Reversely, these
differences in feasibility imply that the computational powers of $R_n$,
$B_\Delta$, and $P_p$ also form a strict hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3279</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3279</id><created>2012-10-11</created><updated>2012-12-16</updated><authors><author><keyname>Belovs</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Rosmanis</keyname><forenames>Ansis</forenames></author></authors><title>On the Power of Non-Adaptive Learning Graphs</title><categories>quant-ph cs.CC</categories><comments>16 pages, 1.5 figures v2: the main result generalised for all
  certificate structures, a bug in the proof of Proposition 17 fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a notion of the quantum query complexity of a certificate
structure. This is a formalisation of a well-known observation that many
quantum query algorithms only require the knowledge of the disposition of
possible certificates in the input string, not the precise values therein.
  Next, we derive a dual formulation of the complexity of a non-adaptive
learning graph, and use it to show that non-adaptive learning graphs are tight
for all certificate structures. By this, we mean that there exists a function
possessing the certificate structure and such that a learning graph gives an
optimal quantum query algorithm for it.
  For a special case of certificate structures generated by certificates of
bounded size, we construct a relatively general class of functions having this
property. The construction is based on orthogonal arrays, and generalizes the
quantum query lower bound for the $k$-sum problem derived recently in
arXiv:1206.6528.
  Finally, we use these results to show that the learning graph for the
triangle problem from arXiv:1210.1014 is almost optimal in these settings. This
also gives a quantum query lower bound for the triangle-sum problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3283</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3283</id><created>2012-10-11</created><updated>2014-06-13</updated><authors><author><keyname>Weeraddana</keyname><forenames>Pradeep Chathuranga</forenames></author><author><keyname>Athanasiou</keyname><forenames>George</forenames></author><author><keyname>Jakobsson</keyname><forenames>Martin</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Baras</keyname><forenames>John S.</forenames></author></authors><title>On the Privacy of Optimization Approaches</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensuring privacy of sensitive data is essential in many contexts, such as
healthcare data, banks, e-commerce, wireless sensor networks, and social
networks. It is common that different entities coordinate or want to rely on a
third party to solve a specific problem. At the same time, no entity wants to
publish its problem data during the solution procedure unless there is a
privacy guarantee. Unlike cryptography and differential privacy based
approaches, the methods based on optimization lack a quantification of the
privacy they can provide. The main contribution of this paper is to provide a
mechanism to quantify the privacy of a broad class of optimization approaches.
In particular, we formally define a one-to-many relation, which relates a given
adversarial observed message to an uncertainty set of the problem data. This
relation quantifies the potential ambiguity on problem data due to the employed
optimization approaches. The privacy definitions are then formalized based on
the uncertainty sets. The properties of the proposed privacy measure is
analyzed. The key ideas are illustrated with examples, including localization,
average consensus, among others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3288</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3288</id><created>2012-10-11</created><authors><author><keyname>Neiswanger</keyname><forenames>Willie</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Unsupervised Detection and Tracking of Arbitrary Objects with Dependent
  Dirichlet Process Mixtures</title><categories>stat.ML cs.CV cs.LG</categories><comments>21 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a technique for the unsupervised detection and tracking
of arbitrary objects in videos. It is intended to reduce the need for detection
and localization methods tailored to specific object types and serve as a
general framework applicable to videos with varied objects, backgrounds, and
image qualities. The technique uses a dependent Dirichlet process mixture
(DDPM) known as the Generalized Polya Urn (GPUDDPM) to model image pixel data
that can be easily and efficiently extracted from the regions in a video that
represent objects. This paper describes a specific implementation of the model
using spatial and color pixel data extracted via frame differencing and gives
two algorithms for performing inference in the model to accomplish detection
and tracking. This technique is demonstrated on multiple synthetic and
benchmark video datasets that illustrate its ability to, without modification,
detect and track objects with diverse physical characteristics moving over
non-uniform backgrounds and through occlusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3292</identifier>
 <datestamp>2013-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3292</id><created>2012-10-11</created><updated>2013-09-05</updated><authors><author><keyname>Keshtkarjahromi</keyname><forenames>Yasaman</forenames></author><author><keyname>Ansari</keyname><forenames>Rashid</forenames></author><author><keyname>Khokhar</keyname><forenames>Ashfaq</forenames></author></authors><title>Energy Efficient Decentralized Detection Based on Bit-optimal Multi-hop
  Transmission in One-dimensional Wireless Sensor Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing information theoretic work in decentralized detection is largely
focused on parallel configuration of Wireless Sensor Networks (WSNs), where an
individual hard or soft decision is computed at each sensor node and then
transmitted directly to the fusion node. Such an approach is not efficient for
large networks, where communication structure is likely to comprise of multiple
hops. On the other hand, decentralized detection problem investigated for
multi-hop networks is mainly concerned with reducing number and/or size of
messages by using compression and fusion of information at intermediate nodes.
In this paper an energy efficient multi-hop configuration of WSNs is proposed
to solve the detection problem in large networks with two objectives:
maximizing network lifetime and minimizing probability of error in the fusion
node. This optimization problem is considered under the constraint of total
consumed energy. The two objectives mentioned are achieved simultaneously in
the multi-hop configuration by exploring tradeoffs between different path
lengths and number of bits allocated to each node for quantization. Simulation
results show significant improvement in the proposed multi-hop configuration
compared with the parallel configuration in terms of energy efficiency and
detection accuracy for different size networks, especially in larger networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3304</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3304</id><created>2012-10-11</created><updated>2013-07-03</updated><authors><author><keyname>Nayebi</keyname><forenames>Aran</forenames></author></authors><title>Practical intractability: a critique of the hypercomputation movement</title><categories>math.LO cs.ET quant-ph</categories><comments>To appear in Minds and Machines</comments><journal-ref>Minds and Machines, 24(3): 275-305, August 2014</journal-ref><doi>10.1007/s11023-013-9317-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For over a decade, the hypercomputation movement has produced computational
models that in theory solve the algorithmically unsolvable, but they are not
physically realizable according to currently accepted physical theories. While
opponents to the hypercomputation movement provide arguments against the
physical realizability of specific models in order to demonstrate this, these
arguments lack the generality to be a satisfactory justification against the
construction of \emph{any} information-processing machine that computes beyond
the universal Turing machine. To this end, I present a more mathematically
concrete challenge to hypercomputability, and will show that one is immediately
led into physical impossibilities, thereby demonstrating the infeasibility of
hypercomputers more generally. This gives impetus to propose and justify a more
plausible starting point for an extension to the classical paradigm that is
physically possible, at least in principle. Instead of attempting to rely on
infinities such as idealized limits of infinite time or numerical precision, or
some other physically unattainable source, one should focus on extending the
classical paradigm to better encapsulate modern computational problems that are
not well-expressed/modeled by the closed-system paradigm of the Turing machine.
I present the first steps toward this goal by considering contemporary
computational problems dealing with intractability and issues surrounding
cyber-physical systems, and argue that a reasonable extension to the classical
paradigm should focus on these issues in order to be practically viable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3307</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3307</id><created>2012-10-11</created><authors><author><keyname>Pashazadeh</keyname><forenames>Saeid</forenames></author><author><keyname>Pashazadeh</keyname><forenames>Maryam</forenames></author></authors><title>Modelling an Automatic Proof Generator for Functional Dependency Rules
  Using Colored Petri Net</title><categories>cs.DB cs.FL cs.SE</categories><comments>17 pages, 4 figures</comments><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST) 2 (2012) 31-47</journal-ref><doi>10.5121/ijfcst.2012.2504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database administrators need to compute closure of functional dependencies
(FDs) for normalization of database systems and enforcing integrity rules.
Colored Petri net (CPN) is a powerful formal method for modelling and
verification of various systems. In this paper, we modelled Armstrong's axioms
for automatic proof generation of a new FD rule from initial FD rules using
CPN. For this purpose, a CPN model of Armstrong's axioms presents and initial
FDs considered in the model as initial color set. Then we search required FD in
the state space of the model via model checking. If it exists in the state
space, then a recursive ML code extracts the proof of this FD rule using
further searches in the state space of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3312</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3312</id><created>2012-10-11</created><authors><author><keyname>Torres-Moreno</keyname><forenames>Juan-Manuel</forenames></author></authors><title>Artex is AnotheR TEXt summarizer</title><categories>cs.IR cs.AI cs.CL</categories><comments>11 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1209.3126</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes Artex, another algorithm for Automatic Text
Summarization. In order to rank sentences, a simple inner product is calculated
between each sentence, a document vector (text topic) and a lexical vector
(vocabulary used by a sentence). Summaries are then generated by assembling the
highest ranked sentences. No ruled-based linguistic post-processing is
necessary in order to obtain summaries. Tests over several datasets (coming
from Document Understanding Conferences (DUC), Text Analysis Conferences (TAC),
evaluation campaigns, etc.) in French, English and Spanish have shown that
summarizer achieves interesting results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3319</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3319</id><created>2012-10-11</created><authors><author><keyname>Joseph</keyname><forenames>Shaun N.</forenames></author><author><keyname>DiPippo</keyname><forenames>Lisa C.</forenames></author></authors><title>Pseudo-scheduling: A New Approach to the Broadcast Scheduling Problem</title><categories>cs.DS cs.NI math.CO</categories><comments>8th International Symposium on Algorithms for Sensor Systems,
  Wireless Ad Hoc Networks and Autonomous Mobile Entities (ALGOSENSORS 2012),
  13-14 September 2012, Ljubljana, Slovenia. 12 pages</comments><msc-class>05C15</msc-class><acm-class>G.2.2; F.2.2; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broadcast scheduling problem asks how a multihop network of broadcast
transceivers operating on a shared medium may share the medium in such a way
that communication over the entire network is possible. This can be naturally
modeled as a graph coloring problem via distance-2 coloring (L(1,1)-labeling,
strict scheduling). This coloring is difficult to compute and may require a
number of colors quadratic in the graph degree. This paper introduces
pseudo-scheduling, a relaxation of distance-2 coloring. Centralized and
decentralized algorithms that compute pseudo-schedules with colors linear in
the graph degree are given and proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3320</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3320</id><created>2012-10-11</created><authors><author><keyname>Mohebbi</keyname><forenames>Keyvan</forenames></author><author><keyname>Ibrahim</keyname><forenames>Suhaimi</forenames></author><author><keyname>Idris</keyname><forenames>Norbik Bashah</forenames></author></authors><title>Contemporary Semantic Web Service Frameworks: An Overview and
  Comparisons</title><categories>cs.SE</categories><comments>12 pages</comments><journal-ref>International Journal on Web Service Computing (IJWSC), Vol.3,
  No.3, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing proliferation of distributed information systems, allows
organizations to offer their business processes to a worldwide audience through
Web services. Semantic Web services have emerged as a means to achieve the
vision of automatic discovery, selection, composition, and invocation of Web
services by encoding the specifications of these software components in an
unambiguous and machine-interpretable form. Several frameworks have been
devised as enabling technologies for Semantic Web services. In this paper, we
survey the prominent Semantic Web service frameworks. In addition, a set of
criteria is identified and the discussed frameworks are evaluated and compared
with respect to these criteria. Knowing the strengths and weaknesses of the
Semantic Web service frameworks can help researchers to utilize the most
appropriate one according to their needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3321</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3321</id><created>2012-10-11</created><updated>2014-09-03</updated><authors><author><keyname>Ebbing</keyname><forenames>Johannes</forenames><affiliation>Leibniz Universit&#xe4;t Hannover</affiliation></author><author><keyname>Kontinen</keyname><forenames>Juha</forenames><affiliation>University of Helsinki</affiliation></author><author><keyname>M&#xfc;ller</keyname><forenames>Julian-Steffen</forenames><affiliation>Leibniz Universit&#xe4;t Hannover</affiliation></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames><affiliation>Leibniz Universit&#xe4;t Hannover</affiliation></author></authors><title>A Fragment of Dependence Logic Capturing Polynomial Time</title><categories>cs.LO cs.CC</categories><proxy>Logical Methods In Computer Science</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  15, 2014) lmcs:795</journal-ref><doi>10.2168/LMCS-10(3:3)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the expressive power of Horn-formulae in dependence
logic and show that they can express NP-complete problems. Therefore we define
an even smaller fragment D-Horn* and show that over finite successor structures
it captures the complexity class P of all sets decidable in polynomial time.
Furthermore we study the question which of our results can ge generalized to
the case of open formulae of D-Horn* and so-called downwards monotone
polynomial time properties of teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3323</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3323</id><created>2012-10-11</created><updated>2015-02-10</updated><authors><author><keyname>Fournier</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Ismail</keyname><forenames>Anas</forenames></author><author><keyname>Vigneron</keyname><forenames>Antoine</forenames></author></authors><title>Computing the Gromov hyperbolicity of a discrete metric space</title><categories>cs.CG math.MG</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give exact and approximation algorithms for computing the Gromov
hyperbolicity of an n-point discrete metric space. We observe that computing
the Gromov hyperbolicity from a fixed base-point reduces to a (max,min) matrix
product. Hence, using the (max,min) matrix product algorithm by Duan and
Pettie, the fixed base-point hyperbolicity can be determined in O(n^2.69) time.
It follows that the Gromov hyperbolicity can be computed in O(n^3.69) time, and
a 2-approximation can be found in O(n^2.69) time. We also give a (2 log_2
n)-approximation algorithm that runs in O(n^2) time, based on a tree-metric
embedding by Gromov. We also show that hyperbolicity at a fixed base-point
cannot be computed in O(n^2.05) time, unless there exists a faster algorithm
for (max,min) matrix multiplication than currently known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3325</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3325</id><created>2012-10-11</created><updated>2013-03-22</updated><authors><author><keyname>B&#xfc;rger</keyname><forenames>Kai</forenames></author><author><keyname>Treib</keyname><forenames>Marc</forenames></author><author><keyname>Westermann</keyname><forenames>R&#xfc;diger</forenames></author><author><keyname>Werner</keyname><forenames>Suzanne</forenames></author><author><keyname>Lalescu</keyname><forenames>Cristian C</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander</forenames></author><author><keyname>Meneveau</keyname><forenames>Charles</forenames></author><author><keyname>Eyink</keyname><forenames>Gregory L</forenames></author></authors><title>Vortices within vortices: hierarchical nature of vortex tubes in
  turbulence</title><categories>physics.flu-dyn cs.GR</categories><comments>2 pages, 1 low quality video, 1 high quality video</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The JHU turbulence database [1] can be used with a state of the art
visualisation tool [2] to generate high quality fluid dynamics videos. In this
work we investigate the classical idea that smaller structures in turbulent
flows, while engaged in their own internal dynamics, are advected by the larger
structures. They are not advected undistorted, however. We see instead that the
small scale structures are sheared and twisted by the larger scales. This
illuminates the basic mechanisms of the turbulent cascade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3326</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3326</id><created>2012-10-11</created><authors><author><keyname>Verpillat</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LKB - Lhomond</affiliation></author><author><keyname>Joud</keyname><forenames>Fadwa</forenames><affiliation>LKB - Lhomond</affiliation></author><author><keyname>Desbiolles</keyname><forenames>Pierre</forenames><affiliation>LKB - Lhomond</affiliation></author><author><keyname>Gross</keyname><forenames>Michel</forenames><affiliation>L2C</affiliation></author></authors><title>Three dimensional tracking of gold nanoparticles using digital
  holographic microscopy</title><categories>physics.optics cs.CV</categories><proxy>ccsd</proxy><journal-ref>Novel Biophotonic Techniques and Applications, Munich : Germany
  (2011)</journal-ref><doi>10.1117/12.896523</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a digital holographic microscope to track gold
colloids in three dimensions. We report observations of 100nm gold particles in
motion in water. The expected signal and the chosen method of reconstruction
are described. We also discuss about how to implement the numerical calculation
to reach real-time 3D tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3344</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3344</id><created>2012-10-11</created><authors><author><keyname>Bulatov</keyname><forenames>Andrei A.</forenames></author><author><keyname>Hedayaty</keyname><forenames>Amir</forenames></author></authors><title>Galois correspondence for counting quantifiers</title><categories>cs.LO cs.CC</categories><comments>28 pages, 2 figures</comments><acm-class>F.2.2; F.4.1; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new type of closure operator on the set of relations,
max-implementation, and its weaker analog max-quantification. Then we show that
approximation preserving reductions between counting constraint satisfaction
problems (#CSPs) are preserved by these two types of closure operators.
Together with some previous results this means that the approximation
complexity of counting CSPs is determined by partial clones of relations that
additionally closed under these new types of closure operators. Galois
correspondence of various kind have proved to be quite helpful in the study of
the complexity of the CSP. While we were unable to identify a Galois
correspondence for partial clones closed under max-implementation and
max-quantification, we obtain such results for slightly different type of
closure operators, k-existential quantification. This type of quantifiers are
known as counting quantifiers in model theory, and often used to enhance first
order logic languages. We characterize partial clones of relations closed under
k-existential quantification as sets of relations invariant under a set of
partial functions that satisfy the condition of k-subset surjectivity. Finally,
we give a description of Boolean max-co-clones, that is, sets of relations on
{0,1} closed under max-implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3350</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3350</id><created>2012-10-11</created><authors><author><keyname>Estellers</keyname><forenames>Virginia</forenames></author><author><keyname>Thiran</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Bresson</keyname><forenames>Xavier</forenames></author></authors><title>Enhanced Compressed Sensing Recovery with Level Set Normals</title><categories>cs.CV</categories><doi>10.1109/TIP.2013.2253484</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a compressive sensing algorithm that exploits geometric properties
of images to recover images of high quality from few measurements. The image
reconstruction is done by iterating the two following steps: 1) estimation of
normal vectors of the image level curves and 2) reconstruction of an image
fitting the normal vectors, the compressed sensing measurements and the
sparsity constraint. The proposed technique can naturally extend to non local
operators and graphs to exploit the repetitive nature of textured images in
order to recover fine detail structures. In both cases, the problem is reduced
to a series of convex minimization problems that can be efficiently solved with
a combination of variable splitting and augmented Lagrangian methods, leading
to fast and easy-to-code algorithms. Extended experiments show a clear
improvement over related state-of-the-art algorithms in the quality of the
reconstructed images and the robustness of the proposed method to noise,
different kind of images and reduced measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3354</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3354</id><created>2012-10-11</created><authors><author><keyname>Chen</keyname><forenames>Xiaojie</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Averting group failures in collective-risk social dilemmas</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>6 two-column pages, 6 figures; accepted for publication in
  Europhysics Letters</comments><journal-ref>EPL 99 (2012) 68003</journal-ref><doi>10.1209/0295-5075/99/68003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Free-riding on a joint venture bears the risk of losing personal endowment as
the group may fail to reach the collective target due to insufficient
contributions. A collective-risk social dilemma emerges, which we here study in
the realm of the spatial public goods game with group-performance-dependent
risk levels. Instead of using an overall fixed value, we update the risk level
in each group based on the difference between the actual contributions and the
declared target. A single parameter interpolates between a step-like risk
function and virtual irrelevance of the group's performance in averting the
failure, thus bridging the two extremes constituting maximal and minimal
feedback. We show that stronger feedback between group performance and risk
level is in general more favorable for the successful evolution of public
cooperation, yet only if the collective target to be reached is moderate.
Paradoxically, if the goals are overambitious, intermediate feedback strengths
yield optimal conditions for cooperation. This can be explained by the
propagation of players that employ identical strategies but experience
different individual success while trying to cope with the collective-risk
social dilemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1210.3368</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1210.3368</id><created>2012-10-11</created><authors><author><keyname>Bieniusa</keyname><forenames>Annette</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Zawirski</keyname><forenames>Marek</forenames><affiliation>INRIA Rocquencourt, LIP6</affiliation></author><author><keyname>Pregui&#xe7;a</keyname><forenames>Nuno</forenames><affiliation>CITI</affiliation></author><author><keyname>Shapiro</keyname><forenames>Marc</forenames><affiliation>INRIA Rocquencourt, LIP6</affiliation></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames><affiliation>Universidade do Minho Departamento de Inform&#xe1;tica</affiliation></author><author><keyname>Balegas</keyname><forenames>Valter</forenames><affiliation>CITI</affiliation></author><author><keyname>Duarte</keyname><forenames>S&#xe9;rgio</forenames><affiliation>CITI</affiliation></author></authors><title>An optimized conflict-free replicated set</title><categories>cs.DC cs.DS</categories><comments>No. RR-8083 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eventual consistency of replicated data supports concurrent updates, reduces
latency and improves fault tolerance, but forgoes strong consistency.
Accordingly, several cloud computing platforms implement eventually-consistent
data types. The set is a widespread and useful abstraction, and many replicated
set designs have been proposed. We present a reasoning abstraction, permutation
equivalence, that systematizes the characterization of the expected concurrency
semantics of concurrent types. Under this framework we present one of the
existing conflict-free replicated data types, Observed-Remove Set. Furthermore,
in order to decrease the size of meta-data, we propose a new optimization to
avoid tombstones. This approach that can be transposed to other data types,
such as maps, graphs or sequences.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="36000" completeListSize="102538">1122234|37001</resumptionToken>
</ListRecords>
</OAI-PMH>
