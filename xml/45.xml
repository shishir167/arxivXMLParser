<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:11:26Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|44001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1590</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1590</id><created>2013-04-04</created><updated>2013-04-07</updated><authors><author><keyname>Chen</keyname><forenames>Jian-Jia</forenames></author><author><keyname>Kao</keyname><forenames>Mong-Jen</forenames></author><author><keyname>Lee</keyname><forenames>D. T.</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Online Power-Managing Strategy with Hard Real-Time Guarantees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of online dynamic power management that provides hard
real-time guarantees. In this problem, each of the given jobs is associated
with an arrival time, a deadline, and an execution time, and the objective is
to decide a schedule of the jobs as well as a sequence of state transitions on
the processors so as to minimize the total energy consumption.
  In this paper, we examine the problem complexity and provide online
strategies to achieve energy-efficiency. First, we show that the competitive
factor of any online algorithm for this problem is at least 2.06. Then we
present an online algorithm which gives a 4-competitive schedule. When the
execution times of the jobs are unit, we show that the competitive factor
improves to 3.59. At the end, the algorithm is generalized to allow a trade-off
between the number of processors we use and the energy-efficiency of the
resulting schedule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1608</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1608</id><created>2013-04-04</created><authors><author><keyname>Mulansky</keyname><forenames>Mario</forenames></author></authors><title>Simulating DNLS models</title><categories>physics.comp-ph cs.CE cs.NA</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present different techniques to numerically solve the equations of motion
for the widely studied Discrete Nonlinear Schroedinger equation (DNLS). Being a
Hamiltonian system, the DNLS requires symplectic routines for an efficient
numerical treatment. Here, we introduce different such schemes in detail and
compare their performance and accuracy by extensive numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1609</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1609</id><created>2013-04-04</created><authors><author><keyname>Tendurus</keyname><forenames>Melek</forenames></author><author><keyname>Baydin</keyname><forenames>Atilim Gunes</forenames></author><author><keyname>Eleveld</keyname><forenames>Marieke A.</forenames></author><author><keyname>Gilbert</keyname><forenames>Alison J.</forenames></author></authors><title>City versus wetland: Predicting urban growth in the Vecht area with a
  cellular automaton model</title><categories>nlin.CG cs.CE</categories><comments>22 pages, 7 figures</comments><msc-class>91D10, 68U20, 68Q80, 91B72, 91B70</msc-class><acm-class>I.6.3; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many studies dealing with the protection or restoration of wetlands
and the sustainable economic growth of cities as separate subjects. This study
investigates the conflict between the two in an area where city growth is
threatening a protected wetland area. We develop a stochastic cellular
automaton model for urban growth and apply it to the Vecht area surrounding the
city of Hilversum in the Netherlands, using topographic maps covering the past
150 years. We investigate the dependence of the urban growth pattern on the
values associated with the protected wetland and other types of landscape
surrounding the city. The conflict between city growth and wetland protection
is projected to occur before 2035, assuming full protection of the wetland. Our
results also show that a milder protection policy, allowing some of the wetland
to be sacrificed, could be beneficial for maintaining other valuable
landscapes. This insight would be difficult to achieve by other analytical
means. We conclude that even slight changes in usage priorities of landscapes
can significantly affect the landscape distribution in near future. Our results
also point to the importance of a protection policy to take the value of
surrounding landscapes and the dynamic nature of urban areas into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1625</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1625</id><created>2013-04-05</created><authors><author><keyname>Pavlova</keyname><forenames>Natalia V.</forenames></author><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author><author><keyname>Vasilyeva</keyname><forenames>Maria V.</forenames></author></authors><title>Mathematical modeling of thermal stabilization of vertical wells on high
  performance computing systems</title><categories>cs.CE</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temperature stabilization of oil and gas wells is used to ensure stability
and prevent deformation of a subgrade estuary zone. In this work, we consider
the numerical simulation of thermal stabilization using vertical seasonal
freezing columns.
  A mathematical model of such problems is described by a time-dependent
temperature equation with phase transitions from water to ice. The resulting
equation is a standard nonlinear parabolic equation.
  Numerical implementation is based on the finite element method using the
package Fenics. After standard purely implicit approximation in time and simple
linearization, we obtain a system of linear algebraic equations. Because the
size of freezing columns are substantially less than the size of the modeled
area, we obtain mesh refinement near columns. Due to this, we get a large
system of equations which are solved using high performance computing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1627</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1627</id><created>2013-04-05</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Chen</keyname><forenames>Hsiao-Hwa</forenames></author></authors><title>Interference-Aware Resource Control in Multi-Antenna Cognitive Ad Hoc
  Networks with Heterogeneous Delay Constraints</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures</comments><journal-ref>IEEE Communications Letters, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider a multi-antenna cognitive ad hoc network (CAHNet)
with heterogeneous delay requirements. To fulfill the interference and delay
constraints simultaneously, we propose to perform adaptive zero-forcing
beamforming (ZFBF) at cognitive transmitters according to interference channel
state information (CSI). To assist the CAHNet to obtain the interference CSI,
we use a win-win inter-network cooperation strategy, namely quantized
interference CSI feedback from the primary network to CAHNet through a feedback
link, under the condition that the CAHNet pays a proper price for it.
Considering the scarcity of feedback and power resources, we focus on the
minimization of the overall resource cost subject to both interference and
delay constraints. To solve the problem, we derive a joint feedback and power
control algorithm amongst multiple links of CAHNet. Finally, simulation results
validate the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1628</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1628</id><created>2013-04-05</created><authors><author><keyname>Berthier</keyname><forenames>Denis</forenames><affiliation>DSI</affiliation></author></authors><title>Pattern-Based Constraint Satisfaction and Logic Puzzles</title><categories>cs.AI math.LO</categories><proxy>ccsd</proxy><journal-ref>Pattern-Based Constraint Satisfaction and Logic Puzzles (2012) 484</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern-Based Constraint Satisfaction and Logic Puzzles develops a pure
logic, pattern-based perspective of solving the finite Constraint Satisfaction
Problem (CSP), with emphasis on finding the &quot;simplest&quot; solution. Different ways
of reasoning with the constraints are formalised by various families of
&quot;resolution rules&quot;, each of them carrying its own notion of simplicity. A large
part of the book illustrates the power of the approach by applying it to
various popular logic puzzles. It provides a unified view of how to model and
solve them, even though they involve very different types of constraints:
obvious symmetric ones in Sudoku, non-symmetric but transitive ones
(inequalities) in Futoshiki, topological and geometric ones in Map colouring,
Numbrix and Hidato, and even much more complex non-binary arithmetic ones in
Kakuro (or Cross Sums). It also shows that the most familiar techniques for
these puzzles can indeed be understood as mere application-specific
presentations of the general rules. Sudoku is used as the main example
throughout the book, making it also an advanced level sequel to &quot;The Hidden
Logic of Sudoku&quot; (another book by the same author), with: many examples of
relationships among different rules and of exceptional situations; comparisons
of the resolution potential of various families of rules; detailed statistics
of puzzles hardness; analysis of extreme instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1636</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1636</id><created>2013-04-05</created><authors><author><keyname>Haslhofer</keyname><forenames>Bernhard</forenames></author><author><keyname>Robitza</keyname><forenames>Werner</forenames></author><author><keyname>Lagoze</keyname><forenames>Carl</forenames></author><author><keyname>Guimbretiere</keyname><forenames>Francois</forenames></author></authors><title>Semantic Tagging on Historical Maps</title><categories>cs.DL cs.HC</categories><comments>10 pages</comments><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tags assigned by users to shared content can be ambiguous. As a possible
solution, we propose semantic tagging as a collaborative process in which a
user selects and associates Web resources drawn from a knowledge context. We
applied this general technique in the specific context of online historical
maps and allowed users to annotate and tag them. To study the effects of
semantic tagging on tag production, the types and categories of obtained tags,
and user task load, we conducted an in-lab within-subject experiment with 24
participants who annotated and tagged two distinct maps. We found that the
semantic tagging implementation does not affect these parameters, while
providing tagging relationships to well-defined concept definitions. Compared
to label-based tagging, our technique also gathers positive and negative
tagging relationships. We believe that our findings carry implications for
designers who want to adopt semantic tagging in other contexts and systems on
the Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1637</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1637</id><created>2013-04-05</created><authors><author><keyname>Charlier</keyname><forenames>&#xc9;milie</forenames></author><author><keyname>Honkala</keyname><forenames>Juha</forenames></author></authors><title>The freeness problem over matrix semigroups and bounded languages</title><categories>cs.DM math.CO</categories><comments>17 pages</comments><msc-class>G.2.0, F.4.3</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the freeness problem for matrix semigroups. We show that the
freeness problem is decidable for upper-triangular $2\times 2$ matrices with
rational entries when the products are restricted to certain bounded languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1647</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1647</id><created>2013-04-05</created><authors><author><keyname>Schroeder</keyname><forenames>Matthias</forenames></author><author><keyname>Selivanov</keyname><forenames>Victor</forenames></author></authors><title>Some Hierarchies of QCB_0-Spaces</title><categories>cs.LO math.LO</categories><comments>24 pages</comments><msc-class>03D55, 03D78, 03D65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define and study hierarchies of topological spaces induced by the
classical Borel and Luzin hierarchies of sets. Our hierarchies are divided into
two classes: hierarchies of countably based spaces induced by their embeddings
into the domain P\omega, and hierarchies of spaces (not necessarily countably
based) induced by their admissible representations. We concentrate on the
non-collapse property of the hierarchies and on the relationships between
hierarchies in the two classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1649</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1649</id><created>2013-04-05</created><authors><author><keyname>Gupta</keyname><forenames>Ruchir</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Trust Estimation in Peer-to-Peer Network Using BLUE</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1210.4301</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In peer-to-peer networks, free riding is a major problem. Reputation
management systems can be used to overcome this problem. Reputation estimation
methods generally do not consider the uncertainties in the inputs. We propose a
reputation estimation method using BLUE (Best Linear Unbiased estimator)
estimator that consider uncertainties in the input variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1658</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1658</id><created>2013-04-05</created><authors><author><keyname>Cremene</keyname><forenames>Ligia</forenames></author><author><keyname>Dumitrescu</keyname><forenames>D.</forenames></author></authors><title>Beyond Nash Equilibrium in Open Spectrum Sharing: Lorenz Equilibrium in
  Discrete Games</title><categories>nlin.AO cs.GT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new game theoretical solution concept for open spectrum sharing in
cognitive radio (CR) environments is presented, the Lorenz equilibrium (LE).
Both Nash and Pareto solution concepts have limitations when applied to real
world problems. Nash equilibrium (NE) rarely ensures maximal payoff and it is
frequently Pareto inefficient. The Pareto set is usually a large set of
solutions, often too hard to process. The Lorenz equilibrium is a subset of
Pareto efficient solutions that are equitable for all players and ensures a
higher payoff than the Nash equilibrium. LE induces a selection criterion of
NE, when several are present in a game (e.g. many-player discrete games) and
when fairness is an issue. Besides being an effective NE selection criterion,
the LE is an interesting game theoretical situation per se, useful for CR
interaction analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1672</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1672</id><created>2013-04-05</created><updated>2013-04-29</updated><authors><author><keyname>Loiacono</keyname><forenames>Daniele</forenames></author><author><keyname>Cardamone</keyname><forenames>Luigi</forenames></author><author><keyname>Lanzi</keyname><forenames>Pier Luca</forenames></author></authors><title>Simulated Car Racing Championship: Competition Software Manual</title><categories>cs.AI cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manual describes the competition software for the Simulated Car Racing
Championship, an international competition held at major conferences in the
field of Evolutionary Computation and in the field of Computational
Intelligence and Games. It provides an overview of the architecture, the
instructions to install the software and to run the simple drivers provided in
the package, the description of the sensors and the actuators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1675</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1675</id><created>2013-04-05</created><authors><author><keyname>Pershin</keyname><forenames>Yuriy V.</forenames></author><author><keyname>Di Ventra</keyname><forenames>Massimiliano</forenames></author></authors><title>Self-organization and solution of shortest-path optimization problems
  with memristive networks</title><categories>cs.ET cond-mat.dis-nn physics.comp-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1211.4487</comments><doi>10.1103/PhysRevE.88.013305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that memristive networks-namely networks of resistors with memory-can
efficiently solve shortest-path optimization problems. Indeed, the presence of
memory (time non-locality) promotes self organization of the network into the
shortest possible path(s). We introduce a network entropy function to
characterize the self-organized evolution, show the solution of the
shortest-path problem and demonstrate the healing property of the solution
path. Finally, we provide an algorithm to solve the traveling salesman problem.
Similar considerations apply to networks of memcapacitors and meminductors, and
networks with memory in various dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1676</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1676</id><created>2013-04-05</created><authors><author><keyname>Reddy</keyname><forenames>A Anji</forenames></author><author><keyname>Kamath</keyname><forenames>S Sowmya</forenames></author></authors><title>Research on Potential Semantic Web Service Discovery Mechanisms</title><categories>cs.SE cs.CY cs.DC</categories><comments>6 pages, International Conference on Recent Trends in Computer
  Science and Engineering (ICRTCSE' 2012) May 3 - 4, 2012 Chennai, INDIA ISBN:
  978-81-9089-807-2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of Web services is an important paradigm in distributed application
development. Currently, many businesses are seeking to convert their
applications into web services because of its ability to promote
inter-operability among applications. As a number of web services increase, the
process of discovering appropriate web services for consumption from user's
perspective gains importance. In this paper, we present a study of potential
ways of discovering web services and issues related to each of them. In
addition, we discuss ontology concepts and related technologies, which
incorporate semantic meaning and hence give domain knowledge about a web
service to improve the discovery mechanism. The paper also presents an overview
of related research work, identifying metrics useful in filtering web service
search mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1677</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1677</id><created>2013-04-05</created><authors><author><keyname>Dommati</keyname><forenames>Sunil Joy</forenames></author><author><keyname>Agrawal</keyname><forenames>Ruchi</forenames></author><author><keyname>G.</keyname><forenames>Ram Mohana Reddy</forenames></author><author><keyname>Kamath</keyname><forenames>S. Sowmya</forenames></author></authors><title>Bug Classification: Feature Extraction and Comparison of Event Model
  using Na\&quot;ive Bayes Approach</title><categories>cs.SE cs.IR cs.LG</categories><comments>5 pages, International Conference on Recent Trends in Computer and
  Information Engineering (ICRTCIE'2012) April 13-15, 2012 Pattaya,
  http://psrcentre.org/images/extraimages/412138.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In software industries, individuals at different levels from customer to an
engineer apply diverse mechanisms to detect to which class a particular bug
should be allocated. Sometimes while a simple search in Internet might help, in
many other cases a lot of effort is spent in analyzing the bug report to
classify the bug. So there is a great need of a structured mining algorithm -
where given a crash log, the existing bug database could be mined to find out
the class to which the bug should be allocated. This would involve Mining
patterns and applying different classification algorithms. This paper focuses
on the feature extraction, noise reduction in data and classification of
network bugs using probabilistic Na\&quot;ive Bayes approach. Different event models
like Bernoulli and Multinomial are applied on the extracted features. When new,
unseen bugs are given as input to the algorithms, the performance comparison of
different algorithms is done on the basis of accuracy and recall parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1679</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1679</id><created>2013-04-05</created><updated>2013-04-10</updated><authors><author><keyname>Meunier</keyname><forenames>Pierre-&#xc9;tienne</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author><author><keyname>Theyssier</keyname><forenames>Guillaume</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author><author><keyname>Woods</keyname><forenames>Damien</forenames></author></authors><title>Intrinsic universality in tile self-assembly requires cooperation</title><categories>cs.CC</categories><comments>Added references. Improved presentation of definitions and proofs.
  This article uses definitions from arXiv:1212.4756. arXiv admin note: text
  overlap with arXiv:1006.2897 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a negative result on the power of a model of algorithmic
self-assembly for which it has been notoriously difficult to find general
techniques and results. Specifically, we prove that Winfree's abstract Tile
Assembly Model, when restricted to use noncooperative tile binding, is not
intrinsically universal. This stands in stark contrast to the recent result
that, via cooperative binding, the abstract Tile Assembly Model is indeed
intrinsically universal. Noncooperative self-assembly, also known as
&quot;temperature 1&quot;, is where tiles bind to each other if they match on one or more
sides, whereas cooperative binding requires binding on multiple sides. Our
result shows that the change from single- to multi-sided binding qualitatively
improves the kinds of dynamics and behavior that these models of nanoscale
self-assembly are capable of. Our lower bound on simulation power holds in both
two and three dimensions; the latter being quite surprising given that
three-dimensional noncooperative tile assembly systems simulate Turing
machines. On the positive side, we exhibit a three-dimensional noncooperative
self-assembly tile set capable of simulating any two-dimensional noncooperative
self-assembly system.
  Our negative result can be interpreted to mean that Turing universal
algorithmic behavior in self-assembly does not imply the ability to simulate
arbitrary algorithmic self-assembly processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1682</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1682</id><created>2013-04-05</created><updated>2014-01-23</updated><authors><author><keyname>Brenguier</keyname><forenames>Romain</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Sassolas</keyname><forenames>Mathieu</forenames></author></authors><title>The Complexity of Admissibility in Omega-Regular Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterated admissibility is a well-known and important concept in classical
game theory, e.g. to determine rational behaviors in multi-player matrix games.
As recently shown by Berwanger, this concept can be soundly extended to
infinite games played on graphs with omega-regular objectives. In this paper,
we study the algorithmic properties of this concept for such games. We settle
the exact complexity of natural decision problems on the set of strategies that
survive iterated elimination of dominated strategies. As a byproduct of our
construction, we obtain automata which recognize all the possible outcomes of
such strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1683</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1683</id><created>2013-04-05</created><authors><author><keyname>Sinha</keyname><forenames>Sipendra</forenames></author><author><keyname>Gaikwad</keyname><forenames>Amol</forenames></author><author><keyname>Kumar</keyname><forenames>Deepak</forenames></author><author><keyname>Darade</keyname><forenames>Snehal</forenames></author><author><keyname>Singh</keyname><forenames>Rohit</forenames></author><author><keyname>Ganjewar</keyname><forenames>Mr. Pramod D.</forenames></author></authors><title>Data Hiding in Binary Image using Block Parity</title><categories>cs.CR</categories><comments>4 Pages,Implemented in Java</comments><acm-class>D.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secret data hiding in binary images is more difficult than other formats
since binary images require only one bit representation to indicate black and
white. This study proposes a new method for data hiding in binary images using
optimized bit position to replace a secret bit. This method manipulates blocks,
which are sub-divided. The parity bit for a specified block decides whether to
change or not, to embed a secret bit. By finding the best position to insert a
secret bit for each divided block, the image quality of the resulting
stego-image can be improved, while maintaining low computational complexity.The
experimental results show that the proposed method has an improvement with
respect to a previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1684</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1684</id><created>2013-04-05</created><authors><author><keyname>Saad</keyname><forenames>Emad</forenames></author></authors><title>Probability Aggregates in Probability Answer Set Programming</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Probability answer set programming is a declarative programming that has been
shown effective for representing and reasoning about a variety of probability
reasoning tasks. However, the lack of probability aggregates, e.g. {\em
expected values}, in the language of disjunctive hybrid probability logic
programs (DHPP) disallows the natural and concise representation of many
interesting problems. In this paper, we extend DHPP to allow arbitrary
probability aggregates. We introduce two types of probability aggregates; a
type that computes the expected value of a classical aggregate, e.g., the
expected value of the minimum, and a type that computes the probability of a
classical aggregate, e.g, the probability of sum of values. In addition, we
define a probability answer set semantics for DHPP with arbitrary probability
aggregates including monotone, antimonotone, and nonmonotone probability
aggregates. We show that the proposed probability answer set semantics of DHPP
subsumes both the original probability answer set semantics of DHPP and the
classical answer set semantics of classical disjunctive logic programs with
classical aggregates, and consequently subsumes the classical answer set
semantics of the original disjunctive logic programs. We show that the proposed
probability answer sets of DHPP with probability aggregates are minimal
probability models and hence incomparable, which is an important property for
nonmonotonic probability reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1692</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1692</id><created>2013-04-05</created><updated>2013-08-09</updated><authors><author><keyname>Hou</keyname><forenames>Jie</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Short Message Noisy Network Coding with a Decode-Forward Option</title><categories>cs.IT math.IT</categories><comments>The new version adds important and missing references. The paper was
  shortened to better emphasize the new insights, especially that short
  messages enable decode-forward</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Short message noisy network coding (SNNC) differs from long message noisy
network coding (LNNC) in that one transmits many short messages in blocks
rather than using one long message with repetitive encoding. Several properties
of SNNC are developed. First, SNNC with backward decoding achieves the same
rates as SNNC with offset encoding and sliding window decoding for memoryless
networks where each node transmits a multicast message. The rates are the same
as LNNC with joint decoding. Second, SNNC enables early decoding if the channel
quality happens to be good. This leads to mixed strategies that unify the
advantages of decode-forward and noisy network coding. Third, the best decoders
sometimes treat other nodes' signals as noise and an iterative method is given
to find the set of nodes that a given node should treat as noise sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1697</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1697</id><created>2013-04-05</created><authors><author><keyname>Solomakhin</keyname><forenames>Dmitry</forenames></author><author><keyname>Montali</keyname><forenames>Marco</forenames></author><author><keyname>Tessaris</keyname><forenames>Sergio</forenames></author><author><keyname>De Masellis</keyname><forenames>Riccardo</forenames></author></authors><title>Verification of Artifact-Centric Systems: Decidability and Modeling
  Issues</title><categories>cs.SE cs.LO</categories><comments>artifact-centric systems, guard-stage-milestone, formal verification</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artifact-centric business processes have recently emerged as an approach in
which processes are centred around the evolution of business entities, called
artifacts, giving equal importance to control-flow and data. The recent
Guard-State-Milestone (GSM) approach provides means for specifying business
artifacts lifecycles in a declarative manner, using constructs that match how
executive-level stakeholders think about their business. However, it turns out
that formal verification of GSM is undecidable even for very simple
propositional temporal properties. We attack this challenging problem by
translating GSM into a well-studied formal framework. We exploit this
translation to isolate an interesting class of state-bounded GSM models for
which verification of sophisticated temporal properties is decidable. We then
introduce some guidelines to turn an arbitrary GSM model into a state-bounded,
verifiable model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1705</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1705</id><created>2013-04-05</created><authors><author><keyname>Lei</keyname><forenames>Wang</forenames></author><author><keyname>Yuwang</keyname><forenames>Yang</forenames></author><author><keyname>Wei</keyname><forenames>Zhao</forenames></author><author><keyname>Wei</keyname><forenames>Lu</forenames></author></authors><title>Network Coding for Energy-Efficient Distributed Storage System in
  Wireless Sensor Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>19 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A network coding-based scheme is proposed to improve the energy efficiency of
distributed storage systems in WSNs (wireless sensor networks), which mainly
focuses on two problems: firstly, consideration is given to effective
distributed storage technology in WSNs; secondly, we address how to repair the
data in failed storage nodes with less resource. For the first problem, we
propose a method to obtain a sparse generator matrix to construct network
codes, and this sparse generator matrix is proven to be the sparsest.
Benefiting from the sparse generator matrix, the energy consumption required to
implement distributed storage is reduced. For the second problem, we designed a
network coding-based iterative repair method, which adequately utilizes the
idea of re-encoding at intermediate nodes from network coding theory.
Benefiting from the re-encoding, the energy consumption required by data repair
is significantly reduced. Moreover, we provide an explicit lower bound of field
size required by this scheme, which implies that this scheme can work over a
very small field and the required computation overhead of coding is very low.
The simulation result verifies that by using our scheme, the total energy
consumption required to implement distributed storage system in WSNs can be
reduced on the one hand, and on the other hand, this method can also balance
energy consumption of the networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1712</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1712</id><created>2013-04-05</created><authors><author><keyname>Coscia</keyname><forenames>Michele</forenames></author></authors><title>Competition and Success in the Meme Pool: a Case Study on Quickmeme.com</title><categories>physics.soc-ph cs.SI</categories><journal-ref>International Conference of Weblogs and Social Media, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of social media has provided data and insights about how people
relate to information and culture. While information is composed by bits and
its fundamental building bricks are relatively well understood, the same cannot
be said for culture. The fundamental cultural unit has been defined as a
&quot;meme&quot;. Memes are defined in literature as specific fundamental cultural
traits, that are floating in their environment together. Just like genes
carried by bodies, memes are carried by cultural manifestations like songs,
buildings or pictures. Memes are studied in their competition for being
successfully passed from one generation of minds to another, in different ways.
In this paper we choose an empirical approach to the study of memes. We
downloaded data about memes from a well-known website hosting hundreds of
different memes and thousands of their implementations. From this data, we
empirically describe the behavior of these memes. We statistically describe
meme occurrences in our dataset and we delineate their fundamental traits,
along with those traits that make them more or less apt to be successful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1718</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1718</id><created>2013-04-05</created><authors><author><keyname>Bousquet</keyname><forenames>Pierre Aboulker 'and' Nicolas</forenames></author></authors><title>Excluding cycles with a fixed number of chords</title><categories>cs.DM math.CO</categories><comments>30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trotignon and Vuskovic completely characterized graphs that do not contain
cycles with exactly one chord. In particular, they show that such a graph G has
chromatic number at most max(3,w(G)). We generalize this result to the class of
graphs that do not contain cycles with exactly two chords and the class of
graphs that do not contain cycles with exactly three chords. More precisely we
prove that graphs with no cycle with exactly two chords have chromatic number
at most 6. And a graph G with no cycle with exactly three chords have chromatic
number at most max(96,w(G)+1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1731</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1731</id><created>2013-04-05</created><updated>2013-04-21</updated><authors><author><keyname>Poinsot</keyname><forenames>Laurent</forenames><affiliation>LIPN</affiliation></author></authors><title>Harmonic analysis and a bentness-like notion in certain finite Abelian
  groups over some finite fields</title><categories>cs.CR cs.DM math.CO</categories><comments>23 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that degree two finite field extensions can be equipped with
a Hermitian-like structure similar to the extension of the complex field over
the reals. In this contribution, using this structure, we develop a modular
character theory and the appropriate Fourier transform for some particular kind
of finite Abelian groups. Moreover we introduce the notion of bent functions
for finite field valued functions rather than usual complex-valued functions,
and we study several of their properties. In particular we prove that this
bentness notion is a consequence of that of Logachev, Salnikov and Yashchenko,
introduced in &quot;Bent functions on a finite Abelian group&quot; (1997). In addition
this new bentness notion is also generalized to a vectorial setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1757</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1757</id><created>2013-04-05</created><authors><author><keyname>Lee</keyname><forenames>Soomin</forenames></author><author><keyname>Nedich</keyname><forenames>Angelia</forenames></author></authors><title>Asynchronous Gossip-Based Random Projection Algorithms Over Networks</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a fully distributed constrained convex optimization problem over
a multi-agent (no central coordinator) network. We propose an asynchronous
gossip-based random projection (GRP) algorithm that solves the distributed
problem using only local communications and computations. We analyze the
convergence properties of the algorithm for an uncoordinated diminishing
stepsize and a constant stepsize. For a diminishing stepsize, we prove that the
iterates of all agents converge to the same optimal point with probability 1.
For a constant stepsize, we establish an error bound on the expected distance
from the iterates of the algorithm to the optimal point. We also provide
simulation results on a distributed robust model predictive control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1760</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1760</id><created>2013-04-05</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>Note: interpreting iterative methods convergence with diffusion point of
  view</title><categories>cs.NA math.NA</categories><comments>4 pages</comments><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explain the convergence speed of different iteration
schemes with the fluid diffusion view when solving a linear fixed point
problem. This interpretation allows one to better understand why power
iteration or Jacobi iteration may converge faster or slower than Gauss-Seidel
iteration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1768</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1768</id><created>2013-04-05</created><updated>2013-10-12</updated><authors><author><keyname>Funke</keyname><forenames>S. W.</forenames></author><author><keyname>Farrell</keyname><forenames>P. E.</forenames></author><author><keyname>Piggott</keyname><forenames>M. D.</forenames></author></authors><title>Tidal turbine array optimisation using the adjoint approach</title><categories>math.OC cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oceanic tides have the potential to yield a vast amount of renewable energy.
Tidal stream generators are one of the key technologies for extracting and
harnessing this potential. In order to extract an economically useful amount of
power, hundreds of tidal turbines must typically be deployed in an array. This
naturally leads to the question of how these turbines should be configured to
extract the maximum possible power: the positioning and the individual tuning
of the turbines could significantly influence the extracted power, and hence is
of major economic interest. However, manual optimisation is difficult due to
legal site constraints, nonlinear interactions of the turbine wakes, and the
cubic dependence of the power on the flow speed. The novel contribution of this
paper is the formulation of this problem as an optimisation problem constrained
by a physical model, which is then solved using an efficient gradient-based
optimisation algorithm. In each optimisation iteration, a two-dimensional
finite element shallow water model predicts the flow and the performance of the
current array configuration. The gradient of the power extracted with respect
to the turbine positions and their tuning parameters is then computed in a
fraction of the time taken for a flow solution by solving the associated
adjoint equations. These equations propagate causality backwards through the
computation, from the power extracted back to the turbine positions and the
tuning parameters. This yields the gradient at a cost almost independent of the
number of turbines, which is crucial for any practical application. The utility
of the approach is demonstrated by optimising turbine arrays in four idealised
scenarios and a more realistic case with up to 256 turbines in the Inner Sound
of the Pentland Firth, Scotland.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1785</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1785</id><created>2013-03-27</created><authors><author><keyname>Hessar</keyname><forenames>Farzad</forenames></author><author><keyname>Roy</keyname><forenames>Sumit</forenames></author></authors><title>Capacity Considerations for Secondary Networks in TV White Space</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The so-called `TV white spaces' (TVWS) - representing unused TV channels in
any given location as the result of the transition to digital broadcasting -
designated by U.S. Federal Communications Commission (FCC) for unlicensed use
presents significant new opportunities within the context of emerging 4G
networks for developing new wireless access technologies that meet the goals of
the US National Broadband Plan (notably true broadband access for an increasing
fraction of the population). There are multiple challenges in realizing this
goal; the most fundamental being the fact that the available WS capacity is
currently not accurately known, since it depends on a multiplicity of factors -
including system parameters of existing incumbents (broadcasters), propagation
characteristics of local terrain as well as FCC rules. In this paper, we
explore the capacity of white space networks by developing a detailed model
that includes all the major variables, and is cognizant of FCC regulations that
provide constraints on incumbent protection. Real terrain information and
propagation models for the primary broadcaster and adjacent channel
interference from TV transmitters are included to estimate their impact on
achievable WS capacity. The model is later used to explore various trade-offs
between network capacity and system parameters and suggest possible amendments
to FCC's incumbent protection rules in the favor of furthering white space
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1790</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1790</id><created>2013-04-05</created><updated>2013-05-03</updated><authors><author><keyname>Ghayoori</keyname><forenames>Arash</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>Upgraded Approximation of Non-Binary Alphabets for Polar Code
  Construction</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm is presented for approximating a single-user channel with a
prime input alphabet size. The result is an upgraded version of the channel
with a reduced output alphabet size. It is shown that this algorithm can be
used to reduce the output alphabet size to the input alphabet size in most
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1796</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1796</id><created>2013-04-05</created><updated>2013-04-09</updated><authors><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author><author><keyname>Yaroslavtsev</keyname><forenames>Grigory</forenames></author></authors><title>The Round Complexity of Small Set Intersection</title><categories>cs.CC</categories><comments>There is an error in the statement and proof of Lemma A.1, so we have
  decided to withdraw the current manuscript. For the round / communication
  tradeoff for small set disjointness, we refer the reader to the independent
  work: http://arxiv.org/pdf/1304.1217.pdf The other results concerning
  OR-Index and Augmented-OR-Index are not affected and will appear in a later
  manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The set disjointness problem is one of the most fundamental and well-studied
problems in communication complexity. In this problem Alice and Bob hold sets
$S, T \subseteq [n]$, respectively, and the goal is to decide if $S \cap T =
\emptyset$. Reductions from set disjointness are a canonical way of proving
lower bounds in data stream algorithms, data structures, and distributed
computation. In these applications, often the set sizes $|S|$ and $|T|$ are
bounded by a value $k$ which is much smaller than $n$. This is referred to as
small set disjointness. A major restriction in the above applications is the
number of rounds that the protocol can make, which, e.g., translates to the
number of passes in streaming applications. A fundamental question is thus in
understanding the round complexity of the small set disjointness problem. For
an essentially equivalent problem, called OR-Equality, Brody et. al showed that
with $r$ rounds of communication, the randomized communication complexity is
$\Omega(k \ilog^r k)$, where$\ilog^r k$ denotes the $r$-th iterated logarithm
function. Unfortunately their result requires the error probability of the
protocol to be $1/k^{\Theta(1)}$. Since na\&quot;ive amplification of the success
probability of a protocol from constant to $1-1/k^{\Theta(1)}$ blows up the
communication by a $\Theta(\log k)$ factor, this destroys their improvements
over the well-known lower bound of $\Omega(k)$ which holds for any number of
rounds. They pose it as an open question to achieve the same $\Omega(k \ilog^r
k)$ lower bound for protocols with constant error probability. We answer this
open question by showing that the $r$-round randomized communication complexity
of ${\sf OREQ}_{n,k}$, and thus also of small set disjointness, with {\it
constant error probability} is $\Omega(k \ilog^r k)$, asymptotically matching
known upper bounds for ${\sf OREQ}_{n,k}$ and small set disjointness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1810</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1810</id><created>2013-04-05</created><updated>2013-05-10</updated><authors><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author></authors><title>A near-optimal approximation algorithm for Asymmetric TSP on embedded
  graphs</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a near-optimal polynomial-time approximation algorithm for the
asymmetric traveling salesman problem for graphs of bounded orientable or
non-orientable genus. Our algorithm achieves an approximation factor of O(f(g))
on graphs with genus g, where f(n) is the best approximation factor achievable
in polynomial time on arbitrary n-vertex graphs. In particular, the
O(log(n)/loglog(n))-approximation algorithm for general graphs by Asadpour et
al. [SODA 2010] immediately implies an O(log(g)/loglog(g))-approximation
algorithm for genus-g graphs. Our result improves the
O(sqrt(g)*log(g))-approximation algorithm of Oveis Gharan and Saberi [SODA
2011], which applies only to graphs with orientable genus g; ours is the first
approximation algorithm for graphs with bounded non-orientable genus.
  Moreover, using recent progress on approximating the genus of a graph, our
O(log(g) / loglog(g))-approximation can be implemented even without an
embedding when the input graph has bounded degree. In contrast, the
O(sqrt(g)*log(g))-approximation algorithm of Oveis Gharan and Saberi requires a
genus-g embedding as part of the input.
  Finally, our techniques lead to a O(1)-approximation algorithm for ATSP on
graphs of genus g, with running time 2^O(g)*n^O(1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1819</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1819</id><created>2013-04-05</created><authors><author><keyname>Lison</keyname><forenames>Pierre</forenames></author></authors><title>Model-based Bayesian Reinforcement Learning for Dialogue Management</title><categories>cs.AI</categories><acm-class>I.2.7; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning methods are increasingly used to optimise dialogue
policies from experience. Most current techniques are model-free: they directly
estimate the utility of various actions, without explicit model of the
interaction dynamics. In this paper, we investigate an alternative strategy
grounded in model-based Bayesian reinforcement learning. Bayesian inference is
used to maintain a posterior distribution over the model parameters, reflecting
the model uncertainty. This parameter distribution is gradually refined as more
data is collected and simultaneously used to plan the agent's actions. Within
this learning framework, we carried out experiments with two alternative
formalisations of the transition model, one encoded with standard multinomial
distributions, and one structured with probabilistic rules. We demonstrate the
potential of our approach with empirical results on a user simulator
constructed from Wizard-of-Oz data in a human-robot interaction scenario. The
results illustrate in particular the benefits of capturing prior domain
knowledge with high-level rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1827</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1827</id><created>2013-04-05</created><authors><author><keyname>Saad</keyname><forenames>Emad</forenames></author></authors><title>Fuzzy Aggregates in Fuzzy Answer Set Programming</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.1684</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Fuzzy answer set programming is a declarative framework for representing and
reasoning about knowledge in fuzzy environments. However, the unavailability of
fuzzy aggregates in disjunctive fuzzy logic programs, DFLP, with fuzzy answer
set semantics prohibits the natural and concise representation of many
interesting problems. In this paper, we extend DFLP to allow arbitrary fuzzy
aggregates. We define fuzzy answer set semantics for DFLP with arbitrary fuzzy
aggregates including monotone, antimonotone, and nonmonotone fuzzy aggregates.
We show that the proposed fuzzy answer set semantics subsumes both the original
fuzzy answer set semantics of DFLP and the classical answer set semantics of
classical disjunctive logic programs with classical aggregates, and
consequently subsumes the classical answer set semantics of classical
disjunctive logic programs. We show that the proposed fuzzy answer sets of DFLP
with fuzzy aggregates are minimal fuzzy models and hence incomparable, which is
an important property for nonmonotonic fuzzy reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1828</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1828</id><created>2013-04-05</created><authors><author><keyname>Asnani</keyname><forenames>Himanshu</forenames></author><author><keyname>Shomorony</keyname><forenames>Ilan</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Network Compression: Worst-Case Analysis</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of communicating a distributed correlated memoryless
source over a memoryless network, from source nodes to destination nodes, under
quadratic distortion constraints. We establish the following two complementary
results: (a) for an arbitrary memoryless network, among all distributed
memoryless sources of a given correlation, Gaussian sources are least
compressible, that is, they admit the smallest set of achievable distortion
tuples, and (b) for any memoryless source to be communicated over a memoryless
additive-noise network, among all noise processes of a given correlation,
Gaussian noise admits the smallest achievable set of distortion tuples. We
establish these results constructively by showing how schemes for the
corresponding Gaussian problems can be applied to achieve similar performance
for (source or noise) distributions that are not necessarily Gaussian but have
the same covariance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1831</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1831</id><created>2013-04-05</created><authors><author><keyname>Gamarnik</keyname><forenames>David</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Limits of local algorithms over sparse random graphs</title><categories>math.PR cs.CC cs.DC math.CO</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local algorithms on graphs are algorithms that run in parallel on the nodes
of a graph to compute some global structural feature of the graph. Such
algorithms use only local information available at nodes to determine local
aspects of the global structure, while also potentially using some randomness.
Recent research has shown that such algorithms show significant promise in
computing structures like large independent sets in graphs locally. Indeed the
promise led to a conjecture by Hatami, \Lovasz and Szegedy
\cite{HatamiLovaszSzegedy} that local algorithms may be able to compute maximum
independent sets in (sparse) random $d$-regular graphs. In this paper we refute
this conjecture and show that every independent set produced by local
algorithms is multiplicative factor $1/2+1/(2\sqrt{2})$ smaller than the
largest, asymptotically as $d\rightarrow\infty$.
  Our result is based on an important clustering phenomena predicted first in
the literature on spin glasses, and recently proved rigorously for a variety of
constraint satisfaction problems on random graphs. Such properties suggest that
the geometry of the solution space can be quite intricate. The specific
clustering property, that we prove and apply in this paper shows that typically
every two large independent sets in a random graph either have a significant
intersection, or have a nearly empty intersection. As a result, large
independent sets are clustered according to the proximity to each other. While
the clustering property was postulated earlier as an obstruction for the
success of local algorithms, such as for example, the Belief Propagation
algorithm, our result is the first one where the clustering property is used to
formally prove limits on local algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1835</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1835</id><created>2013-04-05</created><authors><author><keyname>Hielscher</keyname><forenames>Eric</forenames></author><author><keyname>Rubinsteyn</keyname><forenames>Alex</forenames></author><author><keyname>Shasha</keyname><forenames>Dennis</forenames></author></authors><title>Locality Optimization for Data Parallel Programs</title><categories>cs.PL</categories><report-no>NYU CS TR2013-955</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Productivity languages such as NumPy and Matlab make it much easier to
implement data-intensive numerical algorithms. However, these languages can be
intolerably slow for programs that don't map well to their built-in primitives.
In this paper, we discuss locality optimizations for our system Parakeet, a
just-in-time compiler and runtime system for an array-oriented subset of
Python. Parakeet dynamically compiles whole user functions to high performance
multi-threaded native code. Parakeet makes extensive use of the classic data
parallel operators Map, Reduce, and Scan. We introduce a new set of data
parallel operators,TiledMap, TiledReduce, and TiledScan, that break up their
computations into local pieces of bounded size so as better to make use of
small fast memories. We introduce a novel tiling transformation to generate
tiled operators automatically. Applying this transformation once tiles the
program for cache, and applying it again enables tiling for registers. The
sizes for cache tiles are left unspecified until runtime, when an autotuning
search is performed. Finally, we evaluate our optimizations on benchmarks and
show significant speedups on programs that exhibit data locality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1836</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1836</id><created>2013-04-05</created><updated>2013-04-11</updated><authors><author><keyname>Sun</keyname><forenames>Tairen</forenames></author></authors><title>A Simulation and Modeling of Access Points with Definition Language</title><categories>cs.OH</categories><comments>Withdrawn by arXiv admins</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This submission has been withdrawn by arXiv administrators because it
contains fictitious content and was submitted under a pseudonym, which is
against arXiv policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1838</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1838</id><created>2013-04-05</created><updated>2013-06-27</updated><authors><author><keyname>LeFevre</keyname><forenames>Jeff</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Jagan</forenames></author><author><keyname>Hacigumus</keyname><forenames>Hakan</forenames></author><author><keyname>Tatemura</keyname><forenames>Junichi</forenames></author><author><keyname>Polyzotis</keyname><forenames>Neoklis</forenames></author></authors><title>Towards a Workload for Evolutionary Analytics</title><categories>cs.DB cs.DC cs.PF</categories><comments>10 pages</comments><journal-ref>DanaC: Workshop on Data analytics in the Cloud, June 2013, New
  York, NY</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Emerging data analysis involves the ingestion and exploration of new data
sets, application of complex functions, and frequent query revisions based on
observing prior query answers. We call this new type of analysis evolutionary
analytics and identify its properties. This type of analysis is not well
represented by current benchmark workloads. In this paper, we present a
workload and identify several metrics to test system support for evolutionary
analytics. Along with our metrics, we present methodologies for running the
workload that capture this analytical scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1839</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1839</id><created>2013-04-05</created><updated>2015-03-05</updated><authors><author><keyname>Balan</keyname><forenames>Radu</forenames></author></authors><title>Reconstruction of Signals from Magnitudes of Redundant Representations:
  The Complex Case</title><categories>math.FA cs.IT math.IT stat.AP</categories><comments>updated 6 Apr. 2013 version arXiv:1304.1839: to appear in Foundations
  of Computational Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the question of reconstructing a vector in a
finite-dimensional complex Hilbert space when only the magnitudes of the
coefficients of the vector under a redundant linear map are known. We present
new invertibility results as well an iterative algorithm that finds the
least-square solution and is robust in the presence of noise. We analyze its
numerical performance by comparing it to the Cramer-Rao lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1842</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1842</id><created>2013-04-05</created><updated>2013-07-31</updated><authors><author><keyname>Fernandez</keyname><forenames>Nelson</forenames></author><author><keyname>Maldonado</keyname><forenames>Carlos</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Information Measures of Complexity, Emergence, Self-organization,
  Homeostasis, and Autopoiesis</title><categories>nlin.AO cs.IT math.IT q-bio.OT</categories><comments>35 pages, 12 figures, to be published in Prokopenko, M., editor,
  Guided Self-Organization: Inception. Springer. In Press</comments><report-no>C3 Report 2013.02</report-no><acm-class>H.1.1; F.1.3; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter reviews measures of emergence, self-organization, complexity,
homeostasis, and autopoiesis based on information theory. These measures are
derived from proposed axioms and tested in two case studies: random Boolean
networks and an Arctic lake ecosystem.
  Emergence is defined as the information a system or process produces.
Self-organization is defined as the opposite of emergence, while complexity is
defined as the balance between emergence and self-organization. Homeostasis
reflects the stability of a system. Autopoiesis is defined as the ratio between
the complexity of a system and the complexity of its environment. The proposed
measures can be applied at different scales, which can be studied with
multi-scale profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1845</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1845</id><created>2013-04-05</created><authors><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author></authors><title>Potential Networks, Contagious Communities, and Understanding Social
  Network Structure</title><categories>cs.SI physics.soc-ph</categories><comments>To Appear in Proceedings of the 22nd International World Wide Web
  Conference(WWW 2013)</comments><acm-class>G.2.2; G.3.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study how the network of agents adopting a particular
technology relates to the structure of the underlying network over which the
technology adoption spreads. We develop a model and show that the network of
agents adopting a particular technology may have characteristics that differ
significantly from the social network of agents over which the technology
spreads. For example, the network induced by a cascade may have a heavy-tailed
degree distribution even if the original network does not.
  This provides evidence that online social networks created by technology
adoption over an underlying social network may look fundamentally different
from social networks and indicates that using data from many online social
networks may mislead us if we try to use it to directly infer the structure of
social networks. Our results provide an alternate explanation for certain
properties repeatedly observed in data sets, for example: heavy-tailed degree
distribution, network densification, shrinking diameter, and network community
profile. These properties could be caused by a sort of `sampling bias' rather
than by attributes of the underlying social structure. By generating networks
using cascades over traditional network models that do not themselves contain
these properties, we can nevertheless reliably produce networks that contain
all these properties.
  An opportunity for interesting future research is developing new methods that
correctly infer underlying network structure from data about a network that is
generated via a cascade spread over the underlying network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1851</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1851</id><created>2013-04-05</created><updated>2013-11-14</updated><authors><author><keyname>Zhong</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>Managing Interference Correlation Through Random Medium Access</title><categories>cs.NI cs.IT math.IT</categories><comments>This is the final version, which was accepted in IEEE Transactions on
  Wireless Communications</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The capacity of wireless networks is fundamentally limited by interference.
However, little research has focused on the interference correlation, which may
greatly increase the local delay (namely the number of time slots required for
a node to successfully transmit a packet). This paper focuses on the question
that whether increasing randomness in the MAC, such as frequency-hopping
multiple access (FHMA) and ALOHA, helps to reduce the effect of interference
correlation. We derive closed-form results for the mean and variance of the
local delay for the two MAC protocols and evaluate the optimal parameters that
minimize the mean local delay. Based on the optimal parameters, we propose the
definitions of two operation regimes: correlation-limited regime and
bandwidth-limited regime. Our results reveal that while the mean local delays
for FHMA with N sub-bands and for ALOHA with transmit probability p are the
same when p=1/N with thermal noise ignored, significant difference exists
between the variances. At last, we evaluate the mean delay-jitter tradeoff and
the bounds on the tail probability of the local delay, which shed key insights
into the system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1858</identifier>
 <datestamp>2013-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1858</id><created>2013-04-06</created><updated>2013-05-06</updated><authors><author><keyname>Karag&#xf6;z</keyname><forenames>Batuhan</forenames></author><author><keyname>Yavuz</keyname><forenames>Semih</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author></authors><title>Multi-Resolution Video Streaming in Peer-to-peer Networks</title><categories>cs.IT cs.MM cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multi-resolution streaming in fully-connected peer-to-peer
networks, where transmission rates are constrained by arbitrarily specified
upload capacities of the source and peers. We fully characterize the capacity
region of rate vectors achievable with arbitrary coding, where an achievable
rate vector describes a vector of throughputs of the different resolutions that
can be supported by the network. We then prove that all rate vectors in the
capacity region can be achieved using pure routing strategies. This shows that
coding has no capacity advantage over routing in this scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1863</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1863</id><created>2013-04-06</created><authors><author><keyname>Li</keyname><forenames>Yongkun</forenames></author><author><keyname>Lee</keyname><forenames>Patrick P. C.</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Stochastic Analysis on RAID Reliability for Solid-State Drives</title><categories>cs.PF cs.DC</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solid-state drives (SSDs) have been widely deployed in desktops and data
centers. However, SSDs suffer from bit errors, and the bit error rate is time
dependent since it increases as an SSD wears down. Traditional storage systems
mainly use parity-based RAID to provide reliability guarantees by striping
redundancy across multiple devices, but the effectiveness of RAID in SSDs
remains debatable as parity updates aggravate the wearing and bit error rates
of SSDs. In particular, an open problem is that how different parity
distributions over multiple devices, such as the even distribution suggested by
conventional wisdom, or uneven distributions proposed in recent RAID schemes
for SSDs, may influence the reliability of an SSD RAID array. To address this
fundamental problem, we propose the first analytical model to quantify the
reliability dynamics of an SSD RAID array. Specifically, we develop a
&quot;non-homogeneous&quot; continuous time Markov chain model, and derive the transient
reliability solution. We validate our model via trace-driven simulations and
conduct numerical analysis to provide insights into the reliability dynamics of
SSD RAID arrays under different parity distributions and subject to different
bit error rates and array configurations. Designers can use our model to decide
the appropriate parity distribution based on their reliability requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1864</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1864</id><created>2013-04-06</created><updated>2013-06-22</updated><authors><author><keyname>Petschow</keyname><forenames>Matthias</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Quintana-Orti</keyname><forenames>Enrique</forenames><affiliation>Universidad Jaume I</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>Improved Accuracy and Parallelism for MRRR-based Eigensolvers -- A Mixed
  Precision Approach</title><categories>cs.NA cs.MS</categories><report-no>AICES-2013/04-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The real symmetric tridiagonal eigenproblem is of outstanding importance in
numerical computations; it arises frequently as part of eigensolvers for
standard and generalized dense Hermitian eigenproblems that are based on a
reduction to tridiagonal form. For its solution, the algorithm of Multiple
Relatively Robust Representations (MRRR) is among the fastest methods. Although
fast, the solvers based on MRRR do not deliver the same accuracy as competing
methods like Divide &amp; Conquer or the QR algorithm. In this paper, we
demonstrate that the use of mixed precisions leads to improved accuracy of
MRRR-based eigensolvers with limited or no performance penalty. As a result, we
obtain eigensolvers that are not only equally or more accurate than the best
available methods, but also -in most circumstances- faster and more scalable
than the competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1876</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1876</id><created>2013-04-06</created><updated>2013-05-28</updated><authors><author><keyname>Piater</keyname><forenames>Justus</forenames></author><author><keyname>Rodr&#xed;guez-S&#xe1;nchez</keyname><forenames>Antonio</forenames></author></authors><title>Proceedings of the 37th Annual Workshop of the Austrian Association for
  Pattern Recognition (\&quot;OAGM/AAPR), 2013</title><categories>cs.CV</categories><comments>Contributed papers presented at \&quot;OAGM/AAPR 2013</comments><proxy>Justus Piater</proxy><acm-class>I.4; I.5; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume represents the proceedings of the 37th Annual Workshop of the
Austrian Association for Pattern Recognition (\&quot;OAGM/AAPR), held May 23-24,
2013, in Innsbruck, Austria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1877</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1877</id><created>2013-04-06</created><authors><author><keyname>Pasierb</keyname><forenames>Katarzyna</forenames></author><author><keyname>Kajdanowicz</keyname><forenames>Tomasz</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemyslaw</forenames></author></authors><title>Privacy-preserving Data Mining, Sharing and Publishing</title><categories>cs.DB cs.CR</categories><journal-ref>Journal of Medical Informatics &amp; Technologies, Vol. 18, pp. 69-76,
  2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The goal of the paper is to present different approaches to
privacy-preserving data sharing and publishing in the context of e-health care
systems. In particular, the literature review on technical issues in privacy
assurance and current real-life high complexity implementation of medical
system that assumes proper data sharing mechanisms are presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1881</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1881</id><created>2013-04-06</created><updated>2014-11-12</updated><authors><author><keyname>Bodini</keyname><forenames>Olivier</forenames></author><author><keyname>Lumbroso</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Rolin</keyname><forenames>Nicolas</forenames></author></authors><title>Analytic Samplers and the Combinatorial Rejection Method</title><categories>cs.DM cs.DS math.CO math.PR</categories><comments>accepted at ANALCO 2015, 11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boltzmann samplers, introduced by Duchon et al. in 2001, make it possible to
uniformly draw approximate size objects from any class which can be specified
through the symbolic method. This, through by evaluating the associated
generating functions to obtain the correct branching probabilities.
  But these samplers require generating functions, in particular in the
neighborhood of their sunglarity, which is a complex problem; they also require
picking an appropriate tuning value to best control the size of generated
objects. Although Pivoteau~\etal have brought a sweeping question to the first
question, with the introduction of their Newton oracle, questions remain.
  By adapting the rejection method, a classical tool from the random, we show
how to obtain a variant of the Boltzmann sampler framework, which is tolerant
of approximation, even large ones. Our goal for this is twofold: this allows
for exact sampling with approximate values; but this also allows much more
flexibility in tuning samplers. For the class of simple trees, we will try to
show how this could be used to more easily calibrate samplers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1888</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1888</id><created>2013-04-06</created><updated>2014-12-17</updated><authors><author><keyname>Hansen</keyname><forenames>Thomas Dueholm</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author></authors><title>The complexity of interior point methods for solving discounted
  turn-based stochastic games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of solving discounted, two player, turn based,
stochastic games (2TBSGs). Jurdzinski and Savani showed that 2TBSGs with
deterministic transitions can be reduced to solving $P$-matrix linear
complementarity problems (LCPs). We show that the same reduction works for
general 2TBSGs. This implies that a number of interior point methods for
solving $P$-matrix LCPs can be used to solve 2TBSGs. We consider two such
algorithms. First, we consider the unified interior point method of Kojima,
Megiddo, Noma, and Yoshise, which runs in time $O((1+\kappa)n^{3.5}L)$, where
$\kappa$ is a parameter that depends on the $n \times n$ matrix $M$ defining
the LCP, and $L$ is the number of bits in the representation of $M$. Second, we
consider the interior point potential reduction algorithm of Kojima, Megiddo,
and Ye, which runs in time $O(\frac{-\delta}{\theta}n^4\log \epsilon^{-1})$,
where $\delta$ and $\theta$ are parameters that depend on $M$, and $\epsilon$
describes the quality of the solution. For 2TBSGs with $n$ states and discount
factor $\gamma$ we prove that in the worst case $\kappa =
\Theta(n/(1-\gamma)^2)$, $-\delta = \Theta(\sqrt{n}/(1-\gamma))$, and $1/\theta
= \Theta(n/(1-\gamma)^2)$. The lower bounds for $\kappa$, $-\delta$, and
$1/\theta$ are obtained using the same family of deterministic games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1898</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1898</id><created>2013-04-06</created><authors><author><keyname>Ferscha</keyname><forenames>A.</forenames></author><author><keyname>Farrahi</keyname><forenames>K.</forenames></author><author><keyname>van denHoven</keyname><forenames>J.</forenames></author><author><keyname>Hales</keyname><forenames>D.</forenames></author><author><keyname>Nowak</keyname><forenames>A.</forenames></author><author><keyname>Lukowicz</keyname><forenames>P.</forenames></author><author><keyname>Helbing</keyname><forenames>D.</forenames></author></authors><title>Socio-inspired ICT - Towards a socially grounded society-ICT symbiosis</title><categories>physics.soc-ph cs.SI physics.comp-ph</categories><journal-ref>Eur. Phys. J. Special Topics vol. 214, pp 401-434 (2012)</journal-ref><doi>10.1140/epjst/e2012-01700-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern ICT (Information and Communication Technology) has developed a vision
where the &quot;computer&quot; is no longer associated with the concept of a single
device or a network of devices, but rather the entirety of situated services
originating in a digital world, which are perceived through the physical world.
It is observed that services with explicit user input and output are becoming
to be replaced by a computing landscape sensing the physical world via a huge
variety of sensors, and controlling it via a plethora of actuators. The nature
and appearance of computing devices is changing to be hidden in the fabric of
everyday life, invisibly networked, and omnipresent, with applications greatly
being based on the notions of context and knowledge. Interaction with such
globe spanning, modern ICT systems will presumably be more implicit, at the
periphery of human attention, rather than explicit, i.e. at the focus of human
attention. Socio-inspired ICT assumes that future, globe scale ICT systems
should be viewed as social systems. Such a view challenges research to identify
and formalize the principles of interaction and adaptation in social systems,
so as to be able to ground future ICT systems on those principles. This
position paper therefore is concerned with the intersection of social behaviour
and modern ICT, creating or recreating social conventions and social contexts
through the use of pervasive, globe-spanning, omnipresent and participative
ICT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1900</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1900</id><created>2013-04-06</created><authors><author><keyname>Usatyuk</keyname><forenames>Vasily</forenames></author></authors><title>The implementation of the parallel shortest vector enumerate in the
  block Korkin-Zolotarev method</title><categories>cs.DM</categories><comments>3 pages, in Russian, Will appear in SIBECRYPT'13 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article present a parallel CPU implementation of Kannan algorithm for
solving shortest vector problem in Block Korkin-Zolotarev lattice reduction
method. Implementation based on Native POSIX Thread Library and show linear
decrease of runtime from number of threads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1902</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1902</id><created>2013-04-06</created><authors><author><keyname>Deni&#xe9;lou</keyname><forenames>Pierre-Malo</forenames></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames></author></authors><title>Multiparty Compatibility in Communicating Automata: Characterisation and
  Synthesis of Global Session Types</title><categories>cs.FL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiparty session types are a type system that can ensure the safety and
liveness of distributed peers via the global specification of their
interactions. To construct a global specification from a set of distributed
uncontrolled behaviours, this paper explores the problem of fully
characterising multiparty session types in terms of communicating automata. We
equip global and local session types with labelled transition systems (LTSs)
that faithfully represent asynchronous communications through unbounded
buffered channels. Using the equivalence between the two LTSs, we identify a
class of communicating automata that exactly correspond to the projected local
types. We exhibit an algorithm to synthesise a global type from a collection of
communicating automata. The key property of our findings is the notion of
multiparty compatibility which non-trivially extends the duality condition for
binary session types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1903</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1903</id><created>2013-04-06</created><authors><author><keyname>Paolucci</keyname><forenames>M.</forenames></author><author><keyname>Kossman</keyname><forenames>D.</forenames></author><author><keyname>Conte</keyname><forenames>R.</forenames></author><author><keyname>Lukowicz</keyname><forenames>P.</forenames></author><author><keyname>Argyrakis</keyname><forenames>P.</forenames></author><author><keyname>Blandford</keyname><forenames>A.</forenames></author><author><keyname>Bonelli</keyname><forenames>G.</forenames></author><author><keyname>Anderson</keyname><forenames>S.</forenames></author><author><keyname>de Freitas</keyname><forenames>S.</forenames></author><author><keyname>Edmonds</keyname><forenames>B.</forenames></author><author><keyname>Gilbert</keyname><forenames>N.</forenames></author><author><keyname>Gross</keyname><forenames>M.</forenames></author><author><keyname>Kohlhammer</keyname><forenames>J.</forenames></author><author><keyname>Koumoutsakos</keyname><forenames>P.</forenames></author><author><keyname>Krause</keyname><forenames>A.</forenames></author><author><keyname>Linn&#xe9;r</keyname><forenames>B. -O.</forenames></author><author><keyname>Slusallek</keyname><forenames>P.</forenames></author><author><keyname>Sorkine</keyname><forenames>O.</forenames></author><author><keyname>Sumner</keyname><forenames>R. W.</forenames></author><author><keyname>Helbing</keyname><forenames>D.</forenames></author></authors><title>Towards a living earth simulator</title><categories>physics.comp-ph cs.SI physics.soc-ph</categories><journal-ref>Eur. Phys. J. Special Topics vol. 214, pp. 77-108 (2012)</journal-ref><doi>10.1140/epjst/e2012-01689-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Living Earth Simulator (LES) is one of the core components of the
FuturICT architecture. It will work as a federation of methods, tools,
techniques and facilities supporting all of the FuturICT simulation-related
activities to allow and encourage interactive exploration and understanding of
societal issues. Society-relevant problems will be targeted by leaning on
approaches based on complex systems theories and data science in tight
interaction with the other components of FuturICT. The LES will evaluate and
provide answers to real-world questions by taking into account multiple
scenarios. It will build on present approaches such as agent-based simulation
and modeling, multiscale modelling, statistical inference, and data mining,
moving beyond disciplinary borders to achieve a new perspective on complex
social systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1909</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1909</id><created>2013-04-06</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Dannies</keyname><forenames>Kai</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Doell</keyname><forenames>Christoph</forenames></author><author><keyname>Grimm</keyname><forenames>Carsten</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Schirra</keyname><forenames>Stefan</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Network Farthest-Point Diagrams</title><categories>cs.CG</categories><comments>A preliminary version of this work was presented at the 24th Canadian
  Conference on Computational Geometry</comments><journal-ref>Journal of Computational Geometry, Vol 4 No 1 (2013), 182--211</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the continuum of points along the edges of a network, i.e., an
undirected graph with positive edge weights. We measure distance between these
points in terms of the shortest path distance along the network, known as the
network distance. Within this metric space, we study farthest points.
  We introduce network farthest-point diagrams, which capture how the farthest
points---and the distance to them---change as we traverse the network. We
preprocess a network G such that, when given a query point q on G, we can
quickly determine the farthest point(s) from q in G as well as the farthest
distance from q in G. Furthermore, we introduce a data structure supporting
queries for the parts of the network that are farther away from q than some
threshold R &gt; 0, where R is part of the query.
  We also introduce the minimum eccentricity feed-link problem defined as
follows. Given a network G with geometric edge weights and a point p that is
not on G, connect p to a point q on G with a straight line segment pq, called a
feed-link, such that the largest network distance from p to any point in the
resulting network is minimized. We solve the minimum eccentricity feed-link
problem using eccentricity diagrams. In addition, we provide a data structure
for the query version, where the network G is fixed and a query consists of the
point p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1913</identifier>
 <datestamp>2013-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1913</id><created>2013-04-06</created><updated>2013-05-07</updated><authors><author><keyname>Spaccasassi</keyname><forenames>Carlo</forenames></author><author><keyname>Koutavas</keyname><forenames>Vasileios</forenames></author></authors><title>Towards Efficient Abstractions for Concurrent Consensus</title><categories>cs.PL cs.DC</categories><comments>15 pages, 5 figures, symposium: TFP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consensus is an often occurring problem in concurrent and distributed
programming. We present a programming language with simple semantics and
build-in support for consensus in the form of communicating transactions. We
motivate the need for such a construct with a characteristic example of
generalized consensus which can be naturally encoded in our language. We then
focus on the challenges in achieving an implementation that can efficiently run
such programs. We setup an architecture to evaluate different implementation
alternatives and use it to experimentally evaluate runtime heuristics. This is
the basis for a research project on realistic programming language support for
consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1914</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1914</id><created>2013-04-06</created><authors><author><keyname>Angulo</keyname><forenames>L. D.</forenames></author><author><keyname>Alvarez</keyname><forenames>J.</forenames></author><author><keyname>Teixeira</keyname><forenames>F.</forenames></author><author><keyname>Bretones</keyname><forenames>A. R.</forenames></author><author><keyname>Garcia</keyname><forenames>S. G.</forenames></author></authors><title>Causal--Path Local Time--Stepping in the Discontinuous Galerkin Method
  for Maxwell's equations</title><categories>physics.comp-ph cs.CE math-ph math.MP</categories><doi>10.1016/j.jcp.2013.09.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel local time-stepping technique for marching-in-time
algorithms. The technique is denoted as Causal-Path Local Time-Stepping (CPLTS)
and it is applied for two time integration techniques: fourth order
low--storage explicit Runge--Kutta (LSERK4) and second order Leapfrog (LF2).
The CPLTS method is applied to evolve Maxwell's curl equations using a
Discontinuous Galerkin (DG) scheme for the spatial discretization. Numerical
results for LF2 and LSERK4 are compared with analytical solutions and the
Montseny's LF2 technique. The results show that the CPLTS technique improves
the dispersive and dissipative properties of LF2-LTS scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1916</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1916</id><created>2013-04-06</created><authors><author><keyname>Lumbroso</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author></authors><title>Optimal Discrete Uniform Generation from Coin Flips, and Applications</title><categories>cs.DS math.PR physics.comp-ph physics.data-an</categories><comments>first draft, 22 pages, 5 figures, C code implementation of algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces an algorithm to draw random discrete uniform
variables within a given range of size n from a source of random bits. The
algorithm aims to be simple to implement and optimal both with regards to the
amount of random bits consumed, and from a computational perspective---allowing
for faster and more efficient Monte-Carlo simulations in computational physics
and biology. I also provide a detailed analysis of the number of bits that are
spent per variate, and offer some extensions and applications, in particular to
the optimal random generation of permutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1924</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1924</id><created>2013-04-06</created><authors><author><keyname>Han</keyname><forenames>Shuguang</forenames></author><author><keyname>Yue</keyname><forenames>Zhen</forenames></author><author><keyname>He</keyname><forenames>Daqing</forenames></author></authors><title>Automatic Detection of Search Tactic in Individual Information Seeking:
  A Hidden Markov Model Approach</title><categories>cs.IR</categories><comments>5 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information seeking process is an important topic in information seeking
behavior research. Both qualitative and empirical methods have been adopted in
analyzing information seeking processes, with major focus on uncovering the
latent search tactics behind user behaviors. Most of the existing works require
defining search tactics in advance and coding data manually. Among the few
works that can recognize search tactics automatically, they missed making sense
of those tactics. In this paper, we proposed using an automatic technique, i.e.
the Hidden Markov Model (HMM), to explicitly model the search tactics. HMM
results show that the identified search tactics of individual information
seeking behaviors are consistent with Marchioninis Information seeking process
model. With the advantages of showing the connections between search tactics
and search actions and the transitions among search tactics, we argue that HMM
is a useful tool to investigate information seeking process, or at least it
provides a feasible way to analyze large scale dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1926</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1926</id><created>2013-04-06</created><authors><author><keyname>Peng</keyname><forenames>T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Schmeink</keyname><forenames>A.</forenames></author></authors><title>Distributed Space-Time Coding Based on Adjustable Code Matrices for
  Cooperative MIMO Relaying Systems</title><categories>cs.IT math.IT</categories><comments>6 figures</comments><journal-ref>IEEE Transactions on Communications, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive distributed space-time coding (DSTC) scheme is proposed for
two-hop cooperative MIMO networks. Linear minimum mean square error (MMSE)
receive filters and adjustable code matrices are considered subject to a power
constraint with an amplify-and-forward (AF) cooperation strategy. In the
proposed adaptive DSTC scheme, an adjustable code matrix obtained by a feedback
channel is employed to transform the space-time coded matrix at the relay node.
The effects of the limited feedback and the feedback errors are assessed.
Linear MMSE expressions are devised to compute the parameters of the adjustable
code matrix and the linear receive filters. Stochastic gradient (SG) and
least-squares (LS) algorithms are also developed with reduced computational
complexity. An upper bound on the pairwise error probability analysis is
derived and indicates the advantage of employing the adjustable code matrices
at the relay nodes. An alternative optimization algorithm for the adaptive DSTC
scheme is also derived in order to eliminate the need for the feedback. The
algorithm provides a fully distributed scheme for the adaptive DSTC at the
relay node based on the minimization of the error probability. Simulation
results show that the proposed algorithms obtain significant performance gains
as compared to existing DSTC schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1928</identifier>
 <datestamp>2013-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1928</id><created>2013-04-06</created><updated>2013-09-19</updated><authors><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Tsigaridas</keyname><forenames>Elias</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author></authors><title>A probabilistic algorithm to compute the real dimension of a
  semi-algebraic set</title><categories>cs.SC</categories><comments>Several typos fixed in Sections 4 and 5. There is an error in Section
  5 and thus the complexity result stated does not hold</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\RR$ be a real closed field (e.g. the field of real numbers) and
$\mathscr{S} \subset \RR^n$ be a semi-algebraic set defined as the set of
points in $\RR^n$ satisfying a system of $s$ equalities and inequalities of
multivariate polynomials in $n$ variables, of degree at most $D$, with
coefficients in an ordered ring $\ZZ$ contained in $\RR$. We consider the
problem of computing the {\em real dimension}, $d$, of $\mathscr{S}$. The real
dimension is the first topological invariant of interest; it measures the
number of degrees of freedom available to move in the set. Thus, computing the
real dimension is one of the most important and fundamental problems in
computational real algebraic geometry. The problem is ${\rm
NP}_{\mathbb{R}}$-complete in the Blum-Shub-Smale model of computation. The
current algorithms (probabilistic or deterministic) for computing the real
dimension have complexity $(s \, D)^{O(d(n-d))}$, that becomes $(s \,
D)^{O(n^2)}$ in the worst-case. The existence of a probabilistic or
deterministic algorithm for computing the real dimension with single
exponential complexity with a factor better than ${O(n^2)}$ in the exponent in
the worst-case, is a longstanding open problem. We provide a positive answer to
this problem by introducing a probabilistic algorithm for computing the real
dimension of a semi-algebraic set with complexity $(s\, D)^{O(n)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1930</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1930</id><created>2013-04-06</created><authors><author><keyname>Santosh</keyname><forenames>K. C.</forenames><affiliation>LORIA</affiliation></author><author><keyname>Bela&#xef;d</keyname><forenames>Abdel</forenames><affiliation>LORIA</affiliation></author></authors><title>Client-Driven Content Extraction Associated with Table</title><categories>cs.CV cs.IR</categories><proxy>ccsd</proxy><journal-ref>Machine Vision Applications (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of the project is to extract content within table in document images
based on learnt patterns. Real-world users i.e., clients first provide a set of
key fields within the table which they think are important. These are first
used to represent the graph where nodes are labelled with semantics including
other features and edges are attributed with relations. Attributed relational
graph (ARG) is then employed to mine similar graphs from a document image. Each
mined graph will represent an item within the table, and hence a set of such
graphs will compose a table. We have validated the concept by using a
real-world industrial problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1932</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1932</id><created>2013-04-06</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Generalized Reduced-Rank Decompositions Using Switching and Adaptive
  Algorithms for Space-Time Adaptive Processing</title><categories>cs.IT math.IT</categories><comments>4 figures</comments><journal-ref>ICASSP 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents generalized low-rank signal decompositions with the aid of
switching techniques and adaptive algorithms, which do not require
eigen-decompositions, for space-time adaptive processing. A generalized scheme
is proposed to compute low-rank signal decompositions by imposing suitable
constraints on the filtering and by performing iterations between the computed
subspace and the low-rank filter. An alternating optimization strategy based on
recursive least squares algorithms is presented along with switching and
iterations to cost-effectively compute the bases of the decomposition and the
low-rank filter. An application to space-time interference suppression in
DS-CDMA systems is considered. Simulations show that the proposed scheme and
algorithms obtain significant gains in performance over previously reported
low-rank schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1935</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1935</id><created>2013-04-06</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Interference Suppression and Group-Based Power Adjustment via
  Alternating Optimization for DS-CDMA Networks with Multihop Relaying</title><categories>cs.IT math.IT</categories><comments>2 figures. arXiv admin note: substantial text overlap with
  arXiv:1301.5912, arXiv:1301.0094</comments><journal-ref>European Wireless 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents joint interference suppression and power allocation
algorithms for DS-CDMA networks with multiple hops and decode-and-forward (DF)
protocols. A scheme for joint allocation of power levels across the relays
subject to group-based power constraints and the design of linear receivers for
interference suppression is proposed. A constrained minimum mean-squared error
(MMSE) design for the receive filters and the power allocation vectors is
devised along with an MMSE channel estimator. In order to solve the proposed
optimization efficiently, a method to form an effective group of users and an
alternating optimization strategy are devised with recursive alternating least
squares (RALS) algorithms for estimating the parameters of the receiver, the
power allocation and the channels. Simulations show that the proposed
algorithms obtain significant gains in capacity and performance over existing
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1942</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1942</id><created>2013-04-06</created><updated>2014-06-12</updated><authors><author><keyname>Kesidis</keyname><forenames>George</forenames></author></authors><title>Variation of &quot;The effect of caching on a model of content and access
  provider revenues in information-centric networks&quot;</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a variation of the two-sided market model of [10]: Demand D is
concave in \tilde{D} in (16) of [10] So, in (5) of [10] and after Theorem 2,
take the parametric case 0 &lt; a &lt;1. Thus, demand D is both decreasing and
concave in price p, and so the utilities (U=pD) are also concave in price.
Also, herein a simpler illustrative demand-response model is used in Appendix A
and B.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1944</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1944</id><created>2013-04-06</created><authors><author><keyname>Lomonaco</keyname><forenames>Samuel J.</forenames></author></authors><title>Symbolic Arithmetic and Integer Factorization</title><categories>math.NT cs.SC quant-ph</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we create a systematic and automatic procedure for
transforming the integer factorization problem into the problem of solving a
system of Boolean equations. Surprisingly, the resulting system of Boolean
equations takes on a &quot;life of its own&quot; and becomes a new type of integer, which
we call a generic integer.
  We then proceed to use the newly found algebraic structure of the ring of
generic integers to create two new integer factoring algorithms, called
respectively the Boolean factoring (BF) algorithm, and the multiplicative
Boolean factoring (MBF) algorithm. Although these two algorithms are not
competitive with current classical integer factoring algorithms, it is hoped
that they will become stepping stones to creating much faster and more
competitive algorithms, and perhaps be precursors of a new quantum algorithm
for integer factoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1962</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1962</id><created>2013-04-07</created><authors><author><keyname>Yoon</keyname><forenames>Seokhyun</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Low Complexity MIMO Detection based on Belief Propagation over Pair-wise
  Graphs</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1011.4792</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers belief propagation algorithm over pair-wise graphical
models to develop low complexity, iterative multiple-input multiple-output
(MIMO) detectors. The pair-wise graphical model is a bipartite graph where a
pair of variable nodes are related by an observation node represented by the
bivariate Gaussian function obtained by marginalizing the posterior joint
probability density under the Gaussian input assumption. Specifically, we
consider two types of pair-wise models, the fully connected and ring-type. The
pair-wise graphs are sparse, compared to the conventional graphical model in
[18], insofar as the number of edges connected to an observation node (edge
degree) is only two. Consequently the computations are much easier than those
of maximum likelihood (ML) detection, which are similar to the belief
propagation (BP) that is run over the fully connected bipartite graph. The link
level performance for non-Gaussian input is evaluated via simulations, and the
results show the validity of the proposed algorithms. We also customize the
algorithm with Gaussian input assumption to obtain the Gaussian BP run over the
two pair-wise graphical models and, for the ring-type, we prove its convergence
in mean to the linear minimum mean square error (MMSE) estimates. Since the
maximum a posterior (MAP) estimator for Gaussian input is equivalent to the
linear MMSE estimator, it shows the optimality, in mean, of the scheme for
Gaussian input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1963</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1963</id><created>2013-04-07</created><authors><author><keyname>Jiang</keyname><forenames>Haibo</forenames></author><author><keyname>Ma</keyname><forenames>YaoFei</forenames></author><author><keyname>Hong</keyname><forenames>Dongsheng</forenames></author><author><keyname>Li</keyname><forenames>Zhen</forenames></author></authors><title>A new metric for routing in military wireless network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Ad-hoc network is generally employed in military and emergencies due
to its flexibility and easy-to-use. It's suitable for military wireless network
that has the characteristics of mobility and works effectively under severe
environment and electromagnetic interfering conditions. However, military
network cannot benefit from existing routing protocol directly; there exists
quite many features which are only typical for military network. This paper
presents a new metric for routing, which is employed in A* algorithm. The goal
of the metric is to choose a route of less distance and less transmission delay
between a source and a destination. Our metric is a function of the distance
between the ends and the bandwidth over the link. Moreover, we take frequency
selection into account since a node can work on multi-frequencies. This paper
proposed the new metric, and experimented it based on A* algorithm. The
simulation results show that this metric can find the optimal route which has
less transmission delay compared to the shortest path routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1966</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1966</id><created>2013-04-07</created><authors><author><keyname>Capuzzo-Dolcetta</keyname><forenames>Roberto</forenames><affiliation>Dep. of Physics, Sapienza, Universit&#xe0; di Roma, Italy</affiliation></author><author><keyname>Spera</keyname><forenames>Mario</forenames><affiliation>Dep. of Physics, Sapienza, Universit&#xe0; di Roma, Italy</affiliation></author></authors><title>A Performance Comparison of Different Graphics Processing Units Running
  Direct N-Body Simulations</title><categories>astro-ph.IM cs.DC cs.PF</categories><comments>This paper has been submitted for publication to Computer Physics
  Communications It consists of 26 pages, with 6 tables and 10 figures</comments><doi>10.1016/j.cpc.2013.07.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid computational architectures based on the joint power of Central
Processing Units and Graphic Processing Units (GPUs) are becoming popular and
powerful hardware tools for a wide range of simulations in biology, chemistry,
engineering, physics, etc..
  In this paper we present a comparison of performance of various GPUs
available on market when applied to the numerical integration of the classic,
gravitational, N-body problem. To do this, we developed an OpenCL version of
the parallel code (HiGPUs) to use for these tests, because this version is the
only apt to work on GPUs of different makes.
  The main general result is that we confirm the reliability, speed and
cheapness of GPUs when applied to the examined kind of problems (i.e. when the
forces to evaluate are dependent on the mutual distances, as it happens in
gravitational physics and molecular dynamics). More specifically, we find that
also the cheap GPUs built to be employed just for gaming applications are very
performant in terms of computing speed also in scientific applications and,
although with some limitations in central memory and in bandwidth, can be a
good choice to implement a machine for scientific use at a very good
performance to cost ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1969</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1969</id><created>2013-04-07</created><updated>2013-05-18</updated><authors><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Shen</keyname><forenames>Yanning</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author></authors><title>One-Bit Quantization Design and Adaptive Methods for Compressed Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been a number of studies on sparse signal recovery from one-bit
quantized measurements. Nevertheless, little attention has been paid to the
choice of the quantization thresholds and its impact on the signal recovery
performance. This paper examines the problem of one-bit quantizer design for
sparse signal recovery. Our analysis shows that the magnitude ambiguity that
ever plagues conventional one-bit compressed sensing methods can be resolved,
and an arbitrarily small reconstruction error can be achieved by setting the
quantization thresholds close enough to the original data samples without being
quantized. Note that unquantized data samples are unaccessible in practice. To
overcome this difficulty, we propose an adaptive quantization method that
adaptively adjusts the quantization thresholds in a way such that the
thresholds converges to the optimal thresholds. Numerical results are
illustrated to collaborate our theoretical results and the effectiveness of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1972</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1972</id><created>2013-04-07</created><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>Facial transformations of ancient portraits: the face of Caesar</title><categories>cs.CV</categories><comments>Image processing, Facial transformation, Morphing, Portraits, Julius
  Caesar, Arles bust, Tusculum bust</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some software solutions used to obtain the facial transformations can help
investigating the artistic metamorphosis of the ancient portraits of the same
person. An analysis with a freely available software of portraitures of Julius
Caesar is proposed, showing his several &quot;morphs&quot;. The software helps enhancing
the mood the artist added to a portrait.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1978</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1978</id><created>2013-04-07</created><updated>2013-10-07</updated><authors><author><keyname>Doerr</keyname><forenames>Carola</forenames></author><author><keyname>De Rainville</keyname><forenames>Francois-Michel</forenames></author></authors><title>Constructing Low Star Discrepancy Point Sets with Genetic Algorithms</title><categories>cs.NE cs.NA</categories><comments>Extended abstract appeared at GECCO 2013. v2: corrected 3 numbers in
  table 4</comments><acm-class>F.2.1; I.2.8</acm-class><doi>10.1145/2463372.2463469</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometric discrepancies are standard measures to quantify the irregularity of
distributions. They are an important notion in numerical integration. One of
the most important discrepancy notions is the so-called \emph{star
discrepancy}. Roughly speaking, a point set of low star discrepancy value
allows for a small approximation error in quasi-Monte Carlo integration. It is
thus the most studied discrepancy notion.
  In this work we present a new algorithm to compute point sets of low star
discrepancy. The two components of the algorithm (for the optimization and the
evaluation, respectively) are based on evolutionary principles. Our algorithm
clearly outperforms existing approaches. To the best of our knowledge, it is
also the first algorithm which can be adapted easily to optimize inverse star
discrepancies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1979</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1979</id><created>2013-04-07</created><authors><author><keyname>Miritello</keyname><forenames>Giovanna</forenames></author><author><keyname>Lara</keyname><forenames>Rub&#xe9;n</forenames></author><author><keyname>Cebri&#xe1;n</keyname><forenames>Manuel</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author></authors><title>Limited communication capacity unveils strategies for human interaction</title><categories>physics.soc-ph cs.SI</categories><comments>Main Text: 8 pages, 5 figures. Supplementary info: 8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social connectivity is the key process that characterizes the structural
properties of social networks and in turn processes such as navigation,
influence or information diffusion. Since time, attention and cognition are
inelastic resources, humans should have a predefined strategy to manage their
social interactions over time. However, the limited observational length of
existing human interaction datasets, together with the bursty nature of dyadic
communications have hampered the observation of tie dynamics in social
networks. Here we develop a method for the detection of tie
activation/deactivation, and apply it to a large longitudinal, cross-sectional
communication dataset ($\approx$ 19 months, $\approx$ 20 million people).
Contrary to the perception of ever-growing connectivity, we observe that
individuals exhibit a finite communication capacity, which limits the number of
ties they can maintain active. In particular we find that men have an overall
higher communication capacity than women and that this capacity decreases
gradually for both sexes over the lifespan of individuals (16-70 years). We are
then able to separate communication capacity from communication activity,
revealing a diverse range of tie activation patterns, from stable to
exploratory. We find that, in simulation, individuals exhibiting exploratory
strategies display longer time to receive information spreading in the network
those individuals with stable strategies. Our principled method to determine
the communication capacity of an individual allows us to quantify how
strategies for human interaction shape the dynamical evolution of social
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1981</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1981</id><created>2013-04-07</created><authors><author><keyname>Athanasiou</keyname><forenames>George</forenames></author><author><keyname>Weeraddana</keyname><forenames>Pradeep Chathuranga</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>Auction-based Resource Allocation in MillimeterWave Wireless Access
  Networks</title><categories>cs.NI cs.GT</categories><comments>arXiv admin note: text overlap with arXiv:1301.2723</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The resource allocation problem of optimal assignment of the clients to the
available access points in 60 GHz millimeterWave wireless access networks is
investigated. The problem is posed as a multiassignment optimisation problem.
The proposed solution method converts the initial problem to a minimum cost
flow problem and allows to design an efficient algorithm by a combination of
auction algorithms. The solution algorithm exploits the network optimization
structure of the problem, and thus is much more powerful than computationally
intensive general-purpose solvers. Theoretical and numerical results evince
numerous properties, such as optimality, convergence, and scalability in
comparison to existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1984</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1984</id><created>2013-04-07</created><authors><author><keyname>Blake</keyname><forenames>Samuel T.</forenames></author></authors><title>A Multi-Dimensional Block-Circulant Perfect Array Construction</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a $N$-dimensional generalization of the two-dimensional
block-circulant perfect array construction by \cite{Blake2013}. As in
\cite{Blake2013}, the families of $N$-dimensional arrays possess pairwise
\textit{good} zero correlation zone (ZCZ) cross-correlation. Both constructions
use a perfect autocorrelation sequence with the array orthogonality property
(AOP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1986</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1986</id><created>2013-04-07</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>On growing connected beta-skeletons</title><categories>cs.CG nlin.PS</categories><journal-ref>Computational Geometry, 46 (2013) 6, 805--816</journal-ref><doi>10.1016/j.comgeo.2012.11.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $\beta$-skeleton, $\beta \geq 1$, is a planar proximity undirected graph of
an Euclidean points set, where nodes are connected by an edge if their
lune-based neighbourhood contains no other points of the given set. Parameter
$\beta$ determines the size and shape of the lune-based neighbourhood. A
$\beta$-skeleton of a random planar set is usually a disconnected graph for
$\beta&gt;2$. With the increase of $\beta$, the number of edges in the
$\beta$-skeleton of a random graph decreases. We show how to grow stable
$\beta$-skeletons, which are connected for any given value of $\beta$ and
characterise morphological transformations of the skeletons governed by $\beta$
and a degree of approximation. We speculate how the results obtained can be
applied in biology and chemistry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1995</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1995</id><created>2013-04-07</created><updated>2013-04-09</updated><authors><author><keyname>Liang</keyname><forenames>Liu</forenames></author></authors><title>Image Retrieval using Histogram Factorization and Contextual Similarity
  Learning</title><categories>cs.CV cs.DB cs.LG</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image retrieval has been a top topic in the field of both computer vision and
machine learning for a long time. Content based image retrieval, which tries to
retrieve images from a database visually similar to a query image, has
attracted much attention. Two most important issues of image retrieval are the
representation and ranking of the images. Recently, bag-of-words based method
has shown its power as a representation method. Moreover, nonnegative matrix
factorization is also a popular way to represent the data samples. In addition,
contextual similarity learning has also been studied and proven to be an
effective method for the ranking problem. However, these technologies have
never been used together. In this paper, we developed an effective image
retrieval system by representing each image using the bag-of-words method as
histograms, and then apply the nonnegative matrix factorization to factorize
the histograms, and finally learn the ranking score using the contextual
similarity learning method. The proposed novel system is evaluated on a large
scale image database and the effectiveness is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1996</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1996</id><created>2013-04-07</created><authors><author><keyname>Kanj</keyname><forenames>Iyad</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>On the Subexponential Time Complexity of CSP</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A CSP with n variables ranging over a domain of d values can be solved by
brute-force in d^n steps (omitting a polynomial factor). With a more careful
approach, this trivial upper bound can be improved for certain natural
restrictions of the CSP. In this paper we establish theoretical limits to such
improvements, and draw a detailed landscape of the subexponential-time
complexity of CSP.
  We first establish relations between the subexponential-time complexity of
CSP and that of other problems, including CNF-Sat. We exploit this connection
to provide tight characterizations of the subexponential-time complexity of CSP
under common assumptions in complexity theory. For several natural CSP
parameters, we obtain threshold functions that precisely dictate the
subexponential-time complexity of CSP with respect to the parameters under
consideration.
  Our analysis provides fundamental results indicating whether and when one can
significantly improve on the brute-force search approach for solving CSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1998</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1998</id><created>2013-04-07</created><updated>2013-11-13</updated><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author></authors><title>Convex conditions for robust stability analysis and stabilization of
  linear aperiodic impulsive and sampled-data systems under dwell-time
  constraints</title><categories>math.OC cs.SY math.CA math.DS</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stability analysis and control of linear impulsive systems is addressed in a
hybrid framework, through the use of continuous-time time-varying discontinuous
Lyapunov functions. Necessary and sufficient conditions for stability of
impulsive systems with periodic impulses are first provided in order to set up
the main ideas. Extensions to stability of aperiodic systems under minimum,
maximum and ranged dwell-times are then derived. By exploiting further the
particular structure of the stability conditions, the results are
non-conservatively extended to quadratic stability analysis of linear uncertain
impulsive systems. These stability criteria are, in turn, losslessly extended
to stabilization using a particular, yet broad enough, class of state-feedback
controllers, providing then a convex solution to the open problem of robust
dwell-time stabilization of impulsive systems using hybrid stability criteria.
Relying finally on the representability of sampled-data systems as impulsive
systems, the problems of robust stability analysis and robust stabilization of
periodic and aperiodic uncertain sampled-data systems are straightforwardly
solved using the same ideas. Several examples are discussed in order to show
the effectiveness and reduced complexity of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2014</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2014</id><created>2013-04-07</created><authors><author><keyname>Yun</keyname><forenames>Chol-Hui</forenames></author><author><keyname>Metzler</keyname><forenames>W.</forenames></author><author><keyname>Barski</keyname><forenames>M.</forenames></author></authors><title>Image Compression predicated on Recurrent Iterated Function Systems</title><categories>math.DS cs.CV math.GT</categories><comments>11 pages, presented at 2nd International Conference on Mathematics &amp;
  Statistics, 16-19 June, 2008, Athens, Greece</comments><report-no>KISU-MATH-2008-E-C-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent iterated function systems (RIFSs) are improvements of iterated
function systems (IFSs) using elements of the theory of Marcovian stochastic
processes which can produce more natural looking images. We construct new RIFSs
consisting substantially of a vertical contraction factor function and
nonlinear transformations. These RIFSs are applied to image compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2015</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2015</id><created>2013-04-07</created><authors><author><keyname>Aydin</keyname><forenames>Mehmet E.</forenames></author><author><keyname>Taylan</keyname><forenames>Osman</forenames></author></authors><title>Scheduling Cutting Process for Large Paper Rolls</title><categories>cs.OH</categories><comments>Academic Platform Journal of Engineering and Science, 2013</comments><doi>10.5505/apjes.2013.83997</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paper cutting is a simple process of slicing large rolls of paper,
jumbo-reels, into various sub-rolls with variable widths based on demands risen
by customers. Since the variability is high due to collected various orders
into a pool, the process turns to be production scheduling problem, which
requires optimisation so as to minimise the final remaining amount of paper
wasted. The problem holds characteristics similar one-dimensional bin-packing
problem to some extends and differs with some respects. This paper introduces a
modelling attempt as a scheduling problem with an integer programming approach
for optimisation purposes. Then, a constructive heuristic algorithm revising
one of well-known approaches, called Best-fit algorithm, is introduced to solve
the problem. The illustrative examples provided shows the near optimum solution
provided with very low complexity .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2024</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2024</id><created>2013-04-07</created><updated>2014-03-16</updated><authors><author><keyname>Hoang</keyname><forenames>Trong Nghia</forenames></author><author><keyname>Low</keyname><forenames>Kian Hsiang</forenames></author></authors><title>A General Framework for Interacting Bayes-Optimally with Self-Interested
  Agents using Arbitrary Parametric Model and Model Prior</title><categories>cs.LG cs.AI cs.MA stat.ML</categories><comments>23rd International Joint Conference on Artificial Intelligence (IJCAI
  2013), Extended version with proofs, 10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in Bayesian reinforcement learning (BRL) have shown that
Bayes-optimality is theoretically achievable by modeling the environment's
latent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In
self-interested multi-agent environments, the transition dynamics are mainly
controlled by the other agent's stochastic behavior for which FDM's
independence and modeling assumptions do not hold. As a result, FDM does not
allow the other agent's behavior to be generalized across different states nor
specified using prior domain knowledge. To overcome these practical limitations
of FDM, we propose a generalization of BRL to integrate the general class of
parametric models and model priors, thus allowing practitioners' domain
knowledge to be exploited to produce a fine-grained and compact representation
of the other agent's behavior. Empirical evaluation shows that our approach
outperforms existing multi-agent reinforcement learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2026</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2026</id><created>2013-04-07</created><authors><author><keyname>Kobayashi</keyname><forenames>Koji</forenames></author></authors><title>Resolution structure in HornSAT and CNFSAT</title><categories>cs.CC</categories><comments>6 pages, English and Japanese (see Other formats - Source)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes about the difference of resolution structure and size
between HornSAT and CNFSAT. We can compute HornSAT by using clauses causality.
Therefore we can compute proof diagram by using Log space reduction. But we
must compute CNFSAT by using clauses correlation. Therefore we cannot compute
proof diagram by using Log space reduction, and reduction of CNFSAT is not
P-Complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2031</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2031</id><created>2013-04-07</created><authors><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Quattrone</keyname><forenames>Giovanni</forenames></author><author><keyname>Mashhadi</keyname><forenames>Afra</forenames></author></authors><title>Temporal Analysis of Activity Patterns of Editors in Collaborative
  Mapping Project of OpenStreetMap</title><categories>cs.CY cs.HC cs.SI physics.data-an physics.soc-ph</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years Wikis have become an attractive platform for social
studies of the human behaviour. Containing millions records of edits across the
globe, collaborative systems such as Wikipedia have allowed researchers to gain
a better understanding of editors participation and their activity patterns.
However, contributions made to Geo-wikis_wiki-based collaborative mapping
projects_ differ from systems such as Wikipedia in a fundamental way due to
spatial dimension of the content that limits the contributors to a set of those
who posses local knowledge about a specific area and therefore cross-platform
studies and comparisons are required to build a comprehensive image of online
open collaboration phenomena. In this work, we study the temporal behavioural
pattern of OpenStreetMap editors, a successful example of geo-wiki, for two
European capital cities. We categorise different type of temporal patterns and
report on the historical trend within a period of 7 years of the project age.
We also draw a comparison with the previously observed editing activity
patterns of Wikipedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2032</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2032</id><created>2013-04-07</created><authors><author><keyname>Cabezas-Clavijo</keyname><forenames>Alvaro</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Google Scholar and the h-index in biomedicine: the popularization of
  bibliometric asessment</title><categories>cs.DL</categories><comments>23 pages, 4 figures, 4 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The aim of this paper is to review the features, benefits and limitations of
the new scientific evaluation products derived from Google Scholar; Google
Scholar Metrics and Google Scholar Citations, as well as the h-index which is
the standard bibliometric indicator adopted by these services. It also outlines
the potential of this new database as a source for studies in Biomedicine and
compares the h-index obtained by the most relevant journals and researchers in
the field of Intensive Care Medicine, by means of data extracted from Web of
Science, Scopus and Google Scholar. Results show that, although average h-index
values in Google Scholar are almost 30% higher than those obtained in Web of
Science and about 15% higher than those collected by Scopus, there are no
substantive changes in the rankings generated from either data source. Despite
some technical problems, it is concluded that Google Scholar is a valid tool
for researchers in Health Sciences, both for purposes of information retrieval
and computation of bibliometric indicators
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2033</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2033</id><created>2013-04-07</created><authors><author><keyname>Bouchti</keyname><forenames>Abdelali El</forenames></author><author><keyname>Kafhali</keyname><forenames>Said El</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author></authors><title>Performance Modelling and Analysis of Connection Admission Control in
  OFDMA based WiMAX System with MMPP Queuing</title><categories>cs.NI</categories><comments>9 pages. arXiv admin note: substantial text overlap with
  arXiv:1203.4339</comments><journal-ref>World of Computer Science and Information Technology Journal
  (WCSIT)2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a problem of queuing theoretic performance modeling and
analysis of Orthogonal Frequency Division Multiple Access (OFDMA) under
broad-band wireless networks. We consider a single-cell WiMAX environment in
which the base station allocates sub channels to the subscriber stations in its
coverage area. The sub channels allocated to a subscriber station are shared by
multiple connections at that subscriber station. To ensure the Quality of
Service (QoS) performances, two Connection Admission Control (CAC) schemes,
namely, threshold-based and queue-aware CAC schemes are considered at a
subscriber station. A queuing analytical framework for these admission control
schemes is presented considering OFDMA-based transmission at the physical
layer. Then, based on the queuing model, both the connection-level and the
packetlevel performances are studied and compared with their analogues in the
case without CAC. The connection arrival is modeled by a Poisson process and
the packet arrival for a connection by a Markov Modulated Poisson Process
(MMPP). We determine analytically and numerically different performance
parameters, such as connection blocking probability, average number of ongoing
connections, average queue length, packet dropping probability, queue
throughput and average packet delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2035</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2035</id><created>2013-04-07</created><authors><author><keyname>Saadi</keyname><forenames>Youssef</forenames></author><author><keyname>Kafhali</keyname><forenames>Said El</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author><author><keyname>Nassereddine</keyname><forenames>Bouchaib</forenames></author></authors><title>Simulation Analysis of Routing Protocols using Manhattan Grid Mobility
  Model in MANET</title><categories>cs.NI</categories><comments>7 pages</comments><journal-ref>International Journal of Computer Applications May 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Mobile Ad-hoc Network (MANET) is a self-configuring infrastructure less
network of mobile devices connected by wireless links. In this network
technology, simulative analysis is a significant method to understand the
performance of routing protocols. In this paper three protocols AODV, DSDV and
DSR were simulated using Manhattan Grid Mobility Model. The reactive (AODV,
DSR) and proactive (DSDV) protocols internal mechanism leads to considerable
performance difference. The performance differentials are analyzed using NS-2
which is the main network simulator, NAM (Network Animator), AWK (post
processing script) and were compared in terms of Packet Delivery Fraction
(PDF), Average end-to-end Delay and Throughput, in different environments
specified by varying network load, mobility rate and number of nodes. Our
results presented in this research work demonstrate the performance analysis of
AODV, DSDV and DSR routing protocols. It has been observed that, under
Manhattan Grid mobility model, AODV and DSR performs better than DSDV in terms
of PDF and Throughput. However in term of Average end-to-end Delay, DSDV
appears to be the best one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2050</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2050</id><created>2013-04-07</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Armstrong</keyname><forenames>Rachel</forenames></author><author><keyname>Jones</keyname><forenames>Jeff</forenames></author><author><keyname>Gunji</keyname><forenames>Yukio-Pegio</forenames></author></authors><title>On Creativity of Slime Mould</title><categories>cs.ET nlin.AO</categories><journal-ref>International Journal of General Systems 42 (2013) 5, 441-457</journal-ref><doi>10.1080/03081079.2013.776206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slime mould Physarum polycephalum is large single cell with intriguingly
smart behaviour. The slime mould shows outstanding abilities to adapt its
protoplasmic network to varying environmental conditions. The slime mould can
solve tasks of computational geometry, image processing, logics and arithmetics
when data are represented by configurations of attractants and repellents. We
attempt to map behavioural patterns of slime onto the cognitive control versus
schizotypy spectrum phase space and thus interpret slime mould's activity in
terms of creativity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2058</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2058</id><created>2013-04-07</created><authors><author><keyname>Andrecut</keyname><forenames>M.</forenames></author></authors><title>Stochastic Recovery Of Sparse Signals From Random Measurements</title><categories>physics.data-an cs.IT math.IT</categories><comments>6 pages, 3 figures</comments><journal-ref>Engineering Letters, 19:1, EL_19_1_01 (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse signal recovery from a small number of random measurements is a well
known NP-hard to solve combinatorial optimization problem, with important
applications in signal and image processing. The standard approach to the
sparse signal recovery problem is based on the basis pursuit method. This
approach requires the solution of a large convex optimization problem, and
therefore suffers from high computational complexity. Here, we discuss a
stochastic optimization method, as a low-complexity alternative to the basis
pursuit approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2060</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2060</id><created>2013-04-07</created><updated>2013-04-11</updated><authors><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Improved ARV Rounding in Small-set Expanders and Graphs of Bounded
  Threshold Rank</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a structure theorem for the feasible solutions of the
Arora-Rao-Vazirani SDP relaxation on low threshold rank graphs and on small-set
expanders. We show that if G is a graph of bounded threshold rank or a
small-set expander, then an optimal solution of the Arora-Rao-Vazirani
relaxation (or of any stronger version of it) can be almost entirely covered by
a small number of balls of bounded radius.
  Then, we show that, if k is the number of balls, a solution of this form can
be rounded with an approximation factor of O(sqrt {log k}) in the case of the
Arora-Rao-Vazirani relaxation, and with a constant-factor approximation in the
case of the k-th round of the Sherali-Adams hierarchy starting at the
Arora-Rao-Vazirani relaxation.
  The structure theorem and the rounding scheme combine to prove the following
result, where G=(V,E) is a graph of expansion \phi(G), \lambda_k is the k-th
smallest eigenvalue of the normalized Laplacian of G, and \phi_k(G) =
\min_{disjoint S_1,...,S_k} \max_{1 &lt;= i &lt;= k} \phi(S_i) is the largest
expansion of any k disjoint subsets of V: if either \lambda_k &gt;&gt; log^{2.5} k
\cdot phi(G) or \phi_{k} (G) &gt;&gt; log k \cdot sqrt{log n}\cdot loglog n\cdot
\phi(G), then the Arora-Rao-Vazirani relaxation can be rounded in polynomial
time with an approximation ratio O(sqrt{log k}).
  Stronger approximation guarantees are achievable in time exponential in k via
relaxations in the Lasserre hierarchy. Guruswami and Sinop [GS13] and Arora, Ge
and Sinop [AGS13] prove that 1+eps approximation is achievable in time 2^{O(k)}
poly(n) if either \lambda_k &gt; \phi(G)/ poly(eps), or if SSE_{n/k} &gt; sqrt{log k
log n} \cdot \phi(G)/ poly(eps), where SSE_s is the minimal expansion of sets
of size at most s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2077</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2077</id><created>2013-04-07</created><authors><author><keyname>Sherman</keyname><forenames>Jonah</forenames></author></authors><title>Nearly Maximum Flows in Nearly Linear Time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to the maximum flow problem in undirected,
capacitated graphs using $\alpha$-\emph{congestion-approximators}:
easy-to-compute functions that approximate the congestion required to route
single-commodity demands in a graph to within a factor of $\alpha$. Our
algorithm maintains an arbitrary flow that may have some residual excess and
deficits, while taking steps to minimize a potential function measuring the
congestion of the current flow plus an over-estimate of the congestion required
to route the residual demand. Since the residual term over-estimates, the
descent process gradually moves the contribution to our potential function from
the residual term to the congestion term, eventually achieving a flow routing
the desired demands with nearly minimal congestion after
$\tilde{O}(\alpha\eps^{-2}\log^2 n)$ iterations. Our approach is similar in
spirit to that used by Spielman and Teng (STOC 2004) for solving Laplacian
systems, and we summarize our approach as trying to do for $\ell_\infty$-flows
what they do for $\ell_2$-flows.
  Together with a nearly linear time construction of a
$n^{o(1)}$-congestion-approximator, we obtain $1+\eps$-optimal single-commodity
flows undirected graphs in time $m^{1+o(1)}\eps^{-2}$, yielding the fastest
known algorithm for that problem. Our requirements of a congestion-approximator
are quite low, suggesting even faster and simpler algorithms for certain
classes of graphs. For example, an $\alpha$-competitive oblivious routing tree
meets our definition, \emph{even without knowing how to route the tree back in
the graph}. For graphs of conductance $\phi$, a trivial
$\phi^{-1}$-congestion-approximator gives an extremely simple algorithm for
finding $1+\eps$-optimal-flows in time $\tilde{O}(m\phi^{-1})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2079</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2079</id><created>2013-04-07</created><updated>2014-05-27</updated><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Kothari</keyname><forenames>Pravesh</forenames></author></authors><title>Learning Coverage Functions and Private Release of Marginals</title><categories>cs.LG cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of approximating and learning coverage functions. A
function $c: 2^{[n]} \rightarrow \mathbf{R}^{+}$ is a coverage function, if
there exists a universe $U$ with non-negative weights $w(u)$ for each $u \in U$
and subsets $A_1, A_2, \ldots, A_n$ of $U$ such that $c(S) = \sum_{u \in
\cup_{i \in S} A_i} w(u)$. Alternatively, coverage functions can be described
as non-negative linear combinations of monotone disjunctions. They are a
natural subclass of submodular functions and arise in a number of applications.
  We give an algorithm that for any $\gamma,\delta&gt;0$, given random and uniform
examples of an unknown coverage function $c$, finds a function $h$ that
approximates $c$ within factor $1+\gamma$ on all but $\delta$-fraction of the
points in time $poly(n,1/\gamma,1/\delta)$. This is the first fully-polynomial
algorithm for learning an interesting class of functions in the demanding PMAC
model of Balcan and Harvey (2011). Our algorithms are based on several new
structural properties of coverage functions. Using the results in (Feldman and
Kothari, 2014), we also show that coverage functions are learnable agnostically
with excess $\ell_1$-error $\epsilon$ over all product and symmetric
distributions in time $n^{\log(1/\epsilon)}$. In contrast, we show that,
without assumptions on the distribution, learning coverage functions is at
least as hard as learning polynomial-size disjoint DNF formulas, a class of
functions for which the best known algorithm runs in time
$2^{\tilde{O}(n^{1/3})}$ (Klivans and Servedio, 2004).
  As an application of our learning results, we give simple
differentially-private algorithms for releasing monotone conjunction counting
queries with low average error. In particular, for any $k \leq n$, we obtain
private release of $k$-way marginals with average error $\bar{\alpha}$ in time
$n^{O(\log(1/\bar{\alpha}))}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2080</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2080</id><created>2013-04-07</created><authors><author><keyname>Chemaa</keyname><forenames>Sofiane</forenames></author><author><keyname>Elmansouri</keyname><forenames>Raida</forenames></author><author><keyname>Chaoui</keyname><forenames>Allaoua</forenames></author></authors><title>Web Services Modeling and Composition Approach using Object-Oriented
  Petri Nets</title><categories>cs.SE</categories><comments>16 pages, 14 figures</comments><msc-class>97Pxx, 97Rxx, 81T80</msc-class><journal-ref>International Journal of Computer Science Issues (IJCSI), volume
  9, issue 4, No 3, July 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, with the emergence and the evolution of new technologies, such as
e-business, a large number of companies are connected to Internet, and have
proposed web services to trade. Web services as presented, are conceptually
limited components to relatively simple functionalities. Generally, a single
service does not satisfy the users needs that are more and more complex.
Therefore, services must be made able to be composed to offer added value
services. In this paper, a web services composition approach, modelled by
Objects-Oriented Petri nets, is presented. In his context, an expressive
algebra, which successfully solves the web services complex composition
problem, is proposed. A java tool that allows automating this approach; based
on a definite algebra and a G-nets meta-model, proposed by us, is developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2094</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2094</id><created>2013-04-07</created><authors><author><keyname>Hosseini</keyname><forenames>Hossein</forenames></author><author><keyname>Bahrak</keyname><forenames>Behnam</forenames></author><author><keyname>Hessar</keyname><forenames>Farzad</forenames></author></authors><title>A GOST-like Blind Signature Scheme Based on Elliptic Curve Discrete
  Logarithm Problem</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a blind signature scheme and three practical educed
schemes based on elliptic curve discrete logarithm problem. The proposed
schemes impart the GOST signature structure and utilize the inherent advantage
of elliptic curve cryptosystems in terms of smaller key size and lower
computational overhead to its counterpart public key cryptosystems such as RSA
and ElGamal. The proposed schemes are proved to be secure and have less time
complexity in comparison with the existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2097</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2097</id><created>2013-04-08</created><authors><author><keyname>Jamali</keyname><forenames>R. M. Jalal Uddin</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author><author><keyname>Hasan</keyname><forenames>M. Mahfuz</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Bazlar</forenames></author></authors><title>Solving Linear Equations by Classical Jacobi-SR Based Hybrid
  Evolutionary Algorithm with Uniform Adaptation Technique</title><categories>cs.NE cs.NA</categories><comments>14 Pages, 5 Figures and 7 Tables</comments><journal-ref>Journal of Engineering Science, Vol. 1, No. 2, pp. 11-24, [ISSN:
  2075-4914](2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving a set of simultaneous linear equations is probably the most important
topic in numerical methods. For solving linear equations, iterative methods are
preferred over the direct methods especially when the coefficient matrix is
sparse. The rate of convergence of iteration method is increased by using
Successive Relaxation (SR) technique. But SR technique is very much sensitive
to relaxation factor, {\omega}. Recently, hybridization of classical
Gauss-Seidel based successive relaxation technique with evolutionary
computation techniques have successfully been used to solve large set of linear
equations in which relaxation factors are self-adapted. In this paper, a new
hybrid algorithm is proposed in which uniform adaptive evolutionary computation
techniques and classical Jacobi based SR technique are used instead of
classical Gauss-Seidel based SR technique. The proposed Jacobi-SR based uniform
adaptive hybrid algorithm, inherently, can be implemented in parallel
processing environment efficiently. Whereas Gauss-Seidel-SR based hybrid
algorithms cannot be implemented in parallel computing environment efficiently.
The convergence theorem and adaptation theorem of the proposed algorithm are
proved theoretically. And the performance of the proposed Jacobi-SR based
uniform adaptive hybrid evolutionary algorithm is compared with Gauss-Seidel-SR
based uniform adaptive hybrid evolutionary algorithm as well as with both
classical Jacobi-SR method and Gauss-Seidel-SR method in the experimental
domain. The proposed Jacobi-SR based hybrid algorithm outperforms the
Gauss-Seidel-SR based hybrid algorithm as well as both classical Jacobi-SR
method and Gauss-Seidel-SR method in terms of convergence speed and
effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2103</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2103</id><created>2013-04-08</created><authors><author><keyname>Lu</keyname><forenames>Hao</forenames></author><author><keyname>Hong</keyname><forenames>Peilin</forenames></author><author><keyname>Xue</keyname><forenames>Kaiping</forenames></author></authors><title>High-Throughput Cooperative Communication with Interference Cancellation
  for Two-Path Relay in Multi-source System</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relay-based cooperative communication has become a research focus in recent
years because it can achieve diversity gain in wireless networks. In existing
works, network coding and two-path relay are adopted to deal with the increase
of network size and the half-duplex nature of relay, respectively. To further
improve bandwidth efficiency, we propose a novel cooperative transmission
scheme which combines network coding and two-path relay together in a
multi-source system. Due to the utilization of two-path relay, our proposed
scheme achieves full-rate transmission. Adopting complex field network coding
(CFNC) at both sources and relays ensures that symbols from different sources
are allowed to be broadcast in the same time slot. We also adopt physical-layer
network coding (PNC) at relay nodes to deal with the inter-relay interference
caused by the two-path relay. With careful process design, the ideal throughput
of our scheme achieves by 1 symbol per source per time slot (sym/S/TS).
Furthermore, the theoretical analysis provides a method to estimate the symbol
error probability (SEP) and throughput in additive complex white Gaussian noise
(AWGN) and Rayleigh fading channels. The simulation results verify the
improvement achieved by the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2109</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2109</id><created>2013-04-08</created><authors><author><keyname>Mohsen</keyname><forenames>S. M.</forenames></author><author><keyname>Farhan</keyname><forenames>S. M. Zamshed</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>Automatic Fingerprint Recognition Using Minutiae Matching Technique for
  the Large Fingerprint Database</title><categories>cs.CV</categories><journal-ref>Procs. of the 3rd International Conference on Electrical and
  Computer Engineering (ICECE 2004), pp. 116-119, Dhaka, Bangladesh, December
  28-30, (2004)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting minutiae from fingerprint images is one of the most important
steps in automatic fingerprint identification system. Because minutiae matching
are certainly the most well-known and widely used method for fingerprint
matching, minutiae are local discontinuities in the fingerprint pattern. In
this paper a fingerprint matching algorithm is proposed using some specific
feature of the minutiae points, also the acquired fingerprint image is
considered by minimizing its size by generating a corresponding fingerprint
template for a large fingerprint database. The results achieved are compared
with those obtained through some other methods also shows some improvement in
the minutiae detection process in terms of memory and time required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2110</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2110</id><created>2013-04-08</created><authors><author><keyname>Rahman</keyname><forenames>Md. Mizanur</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Shahadat</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Rakib</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>An Improved GEF Fast Addition Algorithm</title><categories>cs.DS cs.AR</categories><journal-ref>Procs. of the 3rd International Conference on Electrical and
  Computer Engineering (ICECE 2004), pp. 613-616, Dhaka, Bangladesh, December
  28-30, (2004)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an improved GEF fast addition algorithm is proposed. The
proposed algorithm reduces time and memory space. In this algorithm, carry is
calculated on the basis of arrival timing of the operand's bits without
overhead of sorting. Intermediate terms are generated from the most significant
bit and the carry is generated from the least significant bit using the
functions of efficient operators. This algorithm shows better performance for
use in the fastest computational devices of the near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2112</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2112</id><created>2013-04-08</created><authors><author><keyname>Islam</keyname><forenames>Md. Nazrul</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author><author><keyname>Rahman</keyname><forenames>A. M. Moshiur</forenames></author></authors><title>A Probabilistic Algorithm for Reducing Broadcast Redundancy in Ad Hoc
  Wireless Networks</title><categories>cs.NI cs.DS</categories><journal-ref>Procs. of the 8th International Conference on Computer &amp;
  Information Technology (ICCIT 2005), pp. 752-757, Dhaka, Bangladesh, December
  28-30, (2005)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a wired network, a packet can be transmitted to a specified destination
only, no broadcasting required. But in ad hoc wireless network a packet
transmitted by a node can reach all neighbors due to broadcasting. This
broadcasting introduces unnecessary retransmissions of same message. Therefore,
the total number of transmissions (forward nodes) is generally used as the cost
criterion for broadcasting. The problem of finding the minimum number of
forward nodes is NP-complete. In this paper, the goal is to reduce the number
of forward nodes which will reduce redundant transmission as a result. Thus
some of approximation approaches are analyzed, especially dominant pruning and
total dominant pruning which use 2-hop neighborhood information and a new
approach: Probability based algorithm is proposed with a view to minimizing
number of forward nodes. Simulation results of applying this algorithm shows
performance improvements with compared to dominant pruning and total dominant
pruning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2126</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2126</id><created>2013-04-08</created><authors><author><keyname>Toyota</keyname><forenames>Norihito</forenames></author></authors><title>Braess like Paradox in a Small World Network</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages,5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Braess \cite{1} has been studied about a traffic flow on a diamond type
network and found that introducing new edges to the networks always does not
achieve the efficiency. Some researchers studied the Braess' paradox in similar
type networks by introducing various types of cost functions. But whether such
paradox occurs or not is not scarcely studied in complex networks. In this
article, I analytically and numerically study whether Braess like paradox
occurs or not on Dorogovtsev-Mendes network\cite{2}, which is a sort of small
world networks. The cost function needed to go along an edge is postulated to
be equally identified with the length between two nodes, independently of an
amount of traffic on the edge. It is also assumed the it takes a certain cost
$c$ to pass through the center node in Dorogovtsev-Mendes network. If $c$ is
small, then bypasses have the function to provide short cuts. As result of
numerical and theoretical analyses, while I find that any Braess' like paradox
will not occur when the network size becomes infinite, I can show that a
paradoxical phenomenon appears at finite size of network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2132</identifier>
 <datestamp>2013-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2132</id><created>2013-04-08</created><updated>2013-06-13</updated><authors><author><keyname>Morbidi</keyname><forenames>Fabio</forenames></author></authors><title>The Deformed Consensus Protocol: Extended Version</title><categories>cs.SY</categories><comments>17 pages, 9 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a generalization of the standard continuous-time consensus
protocol, obtained by replacing the Laplacian matrix of the communication graph
with the so-called deformed Laplacian. The deformed Laplacian is a
second-degree matrix polynomial in the real variable 's' which reduces to the
standard Laplacian for 's' equal to unity. The stability properties of the
ensuing deformed consensus protocol are studied in terms of parameter 's' for
some special families of undirected and directed graphs, and for arbitrary
graph topologies by leveraging the spectral theory of quadratic eigenvalue
problems. Examples and simulation results are provided to illustrate our
theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2133</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2133</id><created>2013-04-08</created><authors><author><keyname>Wong</keyname><forenames>Yongkang</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Mau</keyname><forenames>Sandra</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Dynamic Amelioration of Resolution Mismatches for Local Feature Based
  Identity Inference</title><categories>cs.CV cs.IR</categories><acm-class>I.5.4; I.4</acm-class><journal-ref>International Conference on Pattern Recognition (ICPR), pp.
  1200-1203, 2010</journal-ref><doi>10.1109/ICPR.2010.299</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While existing face recognition systems based on local features are robust to
issues such as misalignment, they can exhibit accuracy degradation when
comparing images of differing resolutions. This is common in surveillance
environments where a gallery of high resolution mugshots is compared to low
resolution CCTV probe images, or where the size of a given image is not a
reliable indicator of the underlying resolution (eg. poor optics). To alleviate
this degradation, we propose a compensation framework which dynamically chooses
the most appropriate face recognition system for a given pair of image
resolutions. This framework applies a novel resolution detection method which
does not rely on the size of the input images, but instead exploits the
sensitivity of local features to resolution using a probabilistic multi-region
histogram approach. Experiments on a resolution-modified version of the
&quot;Labeled Faces in the Wild&quot; dataset show that the proposed resolution detector
frontend obtains a 99% average accuracy in selecting the most appropriate face
recognition system, resulting in higher overall face discrimination accuracy
(across several resolutions) compared to the individual baseline face
recognition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2144</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2144</id><created>2013-04-08</created><authors><author><keyname>Huang</keyname><forenames>Jianbin</forenames></author><author><keyname>Huangfu</keyname><forenames>Xuejun</forenames></author><author><keyname>Sun</keyname><forenames>Heli</forenames></author><author><keyname>Cheng</keyname><forenames>Hong</forenames></author><author><keyname>Song</keyname><forenames>Qinbao</forenames></author></authors><title>Backward Path Growth for Efficient Mobile Sequential Recommendation</title><categories>cs.DS</categories><comments>28 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of mobile sequential recommendation is presented to suggest a
route connecting some pick-up points for a taxi driver so that he/she is more
likely to get passengers with less travel cost. Essentially, a key challenge of
this problem is its high computational complexity. In this paper, we propose a
dynamical programming based method to solve this problem. Our method consists
of two separate stages: an offline pre-processing stage and an online search
stage. The offline stage pre-computes optimal potential sequence candidates
from a set of pick-up points, and the online stage selects the optimal driving
route based on the pre-computed sequences with the current position of an empty
taxi. Specifically, for the offline pre-computation, a backward incremental
sequence generation algorithm is proposed based on the iterative property of
the cost function. Simultaneously, an incremental pruning policy is adopted in
the process of sequence generation to reduce the search space of the potential
sequences effectively. In addition, a batch pruning algorithm can also be
applied to the generated potential sequences to remove the non-optimal ones of
a certain length. Since the pruning effect continuously increases with the
increase of the sequence length, our method can search the optimal driving
route efficiently in the remaining potential sequence candidates. Experimental
results on real and synthetic data sets show that the pruning percentage of our
method is significantly improved compared to the state-of-the-art methods,
which makes our method can be used to handle the problem of mobile sequential
recommendation with more pick-up points and to search the optimal driving
routes in arbitrary length ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2161</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2161</id><created>2013-04-08</created><updated>2013-11-18</updated><authors><author><keyname>Goranko</keyname><forenames>Valentin</forenames></author><author><keyname>Turrini</keyname><forenames>Paolo</forenames></author></authors><title>Two-player preplay negotiation games with conditional offers</title><categories>cs.GT</categories><comments>Withdrawn. The paper is subsumed by arXiv:1208.1718</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an extension of strategic normal form games with a phase before
the actual play of the game, where players can make binding offers for transfer
of utilities to other players after the play of the game, contingent on the
recipient playing the strategy indicated in the offer. Such offers transform
the payoff matrix of the original game but preserve its non-cooperative nature.
The type of offers we focus on here are conditional on a suggested 'matching
offer' of the same kind made in return by the receiver. Players can exchange a
series of such offers, thus engaging in a bargaining process before a strategic
normal form game is played.
  In this paper we study and analyze solution concepts for two-player normal
form games with such preplay negotiation phase, under several assumptions for
the bargaining power of the players, such as the possibility of withdrawing
previously made offers and opting out from the negotiation process, as well as
the value of time for the players in such negotiations. We obtain results
describing the possible solutions of such bargaining games and analyze the
degrees of efficiency and fairness that can be achieved in such negotiation
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2170</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2170</id><created>2013-04-08</created><authors><author><keyname>Miklos</keyname><forenames>Istvan</forenames></author><author><keyname>Kiss</keyname><forenames>Sandor Z.</forenames></author><author><keyname>Tannier</keyname><forenames>Eric</forenames></author></authors><title>On sampling SCJ rearrangement scenarios</title><categories>cs.CE</categories><msc-class>F.2.2: Computations on discrete structures, G.2.1: Counting problems</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Single Cut or Join (SCJ) operation on genomes, generalizing chromosome
evolution by fusions and fissions, is the computationally simplest known model
of genome rearrangement. While most genome rearrangement problems are already
hard when comparing three genomes, it is possible to compute in polynomial time
a most parsimonious SCJ scenario for an arbitrary number of genomes related by
a binary phylogenetic tree.
  Here we consider the problems of sampling and counting the most parsimonious
SCJ scenarios. We show that both the sampling and counting problems are easy
for two genomes, and we relate SCJ scenarios to alternating permutations.
However, for an arbitrary number of genomes related by a binary phylogenetic
tree, the counting and sampling problems become hard. We prove that if a Fully
Polynomial Randomized Approximation Scheme or a Fully Polynomial Almost Uniform
Sampler exist for the most parsimonious SCJ scenario, then RP = NP.
  The proof has a wider scope than genome rearrangements: the same result holds
for parsimonious evolutionary scenarios on any set of discrete characters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2172</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2172</id><created>2013-04-08</created><authors><author><keyname>Barni</keyname><forenames>Mauro</forenames></author><author><keyname>Tondi</keyname><forenames>Benedetta</forenames></author></authors><title>Binary Hypothesis Testing Game with Training Data</title><categories>cs.IT cs.GT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a game-theoretic framework to study the hypothesis testing
problem, in the presence of an adversary aiming at preventing a correct
decision. Specifically, the paper considers a scenario in which an analyst has
to decide whether a test sequence has been drawn according to a probability
mass function (pmf) P_X or not. In turn, the goal of the adversary is to take a
sequence generated according to a different pmf and modify it in such a way to
induce a decision error. P_X is known only through one or more training
sequences. We derive the asymptotic equilibrium of the game under the
assumption that the analyst relies only on first order statistics of the test
sequence, and compute the asymptotic payoff of the game when the length of the
test sequence tends to infinity. We introduce the concept of
indistinguishability region, as the set of pmf's that can not be distinguished
reliably from P_X in the presence of attacks. Two different scenarios are
considered: in the first one the analyst and the adversary share the same
training sequence, in the second scenario, they rely on independent sequences.
The obtained results are compared to a version of the game in which the pmf P_X
is perfectly known to the analyst and the adversary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2184</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2184</id><created>2013-04-08</created><authors><author><keyname>Grigoriev</keyname><forenames>Evgeniy</forenames></author></authors><title>Object-Oriented Translation for Programmable Relational System (DRAFT)</title><categories>cs.DB</categories><comments>25 pages, 2 figures</comments><acm-class>F.1.1; D.3.3; H.2.3; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces the principles of object-oriented translation for target
machine which provides executing the sequences of elementary operations on
persistent data presented as a set of relations (programmable relational
system). The language of this target machine bases on formal operations of
relational data model. An approach is given to convert both the description of
complex object-oriented data structures and operations on these data, into a
description of relational structures and operations on them. The proposed
approach makes possible to extend the target relational language with commands
allowing data be described as a set of complex persistent objects of different
classes. Object views are introduced which allow relational operations be
applied to the data of complex objects. It is shown that any operation and
method can be executed on any group of the objects without explicit and
implicit iterators. Binding of both attributes and methods with their
polymorphic implementations are discussed. Classes can be co-used with
relations as scalar domains, in referential integrity constraints and in data
query operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2204</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2204</id><created>2013-04-08</created><updated>2014-08-12</updated><authors><author><keyname>Foniok</keyname><forenames>Jan</forenames></author><author><keyname>Tardif</keyname><forenames>Claude</forenames></author></authors><title>Digraph functors which admit both left and right adjoints</title><categories>math.CO cs.DM math.CT</categories><comments>16 pages, 1 figure</comments><msc-class>05C20, 18A40, 05C60</msc-class><journal-ref>Discrete Mathematics 338 (4): 527-535, 2015</journal-ref><doi>10.1016/j.disc.2014.10.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For our purposes, two functors {\Lambda} and {\Gamma} are said to be
respectively left and right adjoints of each other if for any digraphs G and H,
there exists a homomorphism of {\Lambda}(G) to H if and only if there exists a
homomorphism of G to {\Gamma}(H). We investigate the right adjoints
characterised by Pultr in [A. Pultr, The right adjoints into the categories of
relational systems, in Reports of the Midwest Category Seminar, IV, volume 137
of Lecture Notes in Mathematics, pages 100-113, Berlin, 1970]. We find
necessary conditions for these functors to admit right adjoints themselves. We
give many examples where these necessary conditions are satisfied, and the
right adjoint indeed exists. Finally, we discuss a connection between these
right adjoints and homomorphism dualities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2213</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2213</id><created>2013-04-08</created><updated>2013-04-18</updated><authors><author><keyname>Sharad</keyname><forenames>Mrigank</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Spintronic Switches for Ultra Low Energy On-Chip and Inter-Chip
  Current-Mode Interconnects</title><categories>cond-mat.mtrl-sci cs.ET</categories><doi>10.1109/LED.2013.2268152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy-efficiency and design-complexity of high-speed on-chip and inter-chip
data-interconnects has emerged as the major bottleneck for high-performance
computing-systems. As a solution, we propose an ultra-low energy interconnect
design-scheme using nano-scale spintorque switches. In the proposed method,
data is transmitted in the form of current-pulses, with amplitude of the order
of few micro-amperes that flows across a small terminal-voltage of less than
50mV. Sub-nanosecond spintorque switching of scaled nano-magnets can be used to
receive and convert such high-speed current-mode signal into binary
voltage-levels using magnetic-tunnel-junction (MTJ), with the help of simple
CMOS inverter. As a result of low-voltage, low-current signaling and minimal
signal-conversion overhead, the proposed technique can facilitate highly
compact and simplified designs for multi-gigahertz inter-chip and on-chip
data-communication links. Such links can achieve more than ~100x higher
energy-efficiency, as compared to state of the art CMOS interconnects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2215</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2215</id><created>2013-04-08</created><authors><author><keyname>Foniok</keyname><forenames>Jan</forenames></author><author><keyname>Tardif</keyname><forenames>Claude</forenames></author></authors><title>Adjoint functors in graph theory</title><categories>math.CO cs.DM math.CT</categories><comments>14 pages</comments><msc-class>05C15, 18B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey some uses of adjoint functors in graph theory pertaining to
colourings, complexity reductions, multiplicativity, circular colourings and
tree duality. The exposition of these applications through adjoint functors
unifies the presentation to some extent, and also raises interesting questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2222</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2222</id><created>2013-04-08</created><updated>2015-09-27</updated><authors><author><keyname>Chamanbaz</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Dabbene</keyname><forenames>Fabrizio</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author><author><keyname>Venkataramanan</keyname><forenames>Venkatakrishnan</forenames></author><author><keyname>Wang</keyname><forenames>Qing-Guo</forenames></author></authors><title>Sequential Randomized Algorithms for Convex Optimization in the Presence
  of Uncertainty</title><categories>cs.SY math.OC</categories><comments>18 pages, Submitted for publication to IEEE Transactions on Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose new sequential randomized algorithms for convex
optimization problems in the presence of uncertainty. A rigorous analysis of
the theoretical properties of the solutions obtained by these algorithms, for
full constraint satisfaction and partial constraint satisfaction, respectively,
is given. The proposed methods allow to enlarge the applicability of the
existing randomized methods to real-world applications involving a large number
of design variables. Since the proposed approach does not provide a priori
bounds on the sample complexity, extensive numerical simulations, dealing with
an application to hard-disk drive servo design, are provided. These simulations
testify the goodness of the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2233</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2233</id><created>2013-04-08</created><authors><author><keyname>Erhard</keyname><forenames>Michael</forenames></author><author><keyname>Strauch</keyname><forenames>Hans</forenames></author></authors><title>Sensors and Navigation Algorithms for Flight Control of Tethered Kites</title><categories>cs.SY</categories><comments>6 pages, 9 figures, submitted to European Control Conference (ECC)
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the sensor setup and the basic navigation algorithm used for the
flight control of the SkySails towing kite system. Starting with brief
summaries on system setup and equations of motion of the tethered kite system,
we subsequently give an overview of the sensor setup, present the navigation
task and discuss challenges which have to be mastered. In the second part we
introduce in detail the inertial navigation algorithm which has been used for
operational flights for years. The functional capability of this algorithm is
illustrated by experimental flight data. Finally we suggest a modification of
the algorithms as further development step in order to overcome certain
limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2234</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2234</id><created>2013-04-08</created><updated>2014-01-22</updated><authors><author><keyname>Torrisi</keyname><forenames>Giovanni Luca</forenames></author><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author></authors><title>Large deviations of the interference in the Ginibre network model</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under different assumptions on the distribution of the fading random
variables, we derive large deviation estimates for the tail of the interference
in a wireless network model whose nodes are placed, over a bounded region of
the plane, according to the $\beta$-Ginibre process, $0&lt;\beta\leq 1$. The
family of $\beta$-Ginibre processes is formed by determinantal point processes,
with different degree of repulsiveness, which converge in law to a homogeneous
Poisson process, as $\beta \to 0$. In this sense the Poisson network model may
be considered as the limiting uncorrelated case of the $\beta$-Ginibre network
model. Our results indicate the existence of two different regimes. When the
fading random variables are bounded or Weibull superexponential, large values
of the interference are typically originated by the sum of several equivalent
interfering contributions due to nodes in the vicinity of the receiver.
  In this case, the tail of the interference has, on the log-scale, the same
asymptotic behavior for any value of $0&lt;\beta\le 1$, but it differs (again on a
log-scale) from the asymptotic behavior of the tail of the interference in the
Poisson network model.
  When the fading random variables are exponential or subexponential, instead,
large values of the interference are typically originated by a single
dominating interferer node and, on the log-scale, the asymptotic behavior of
the tail of the interference is essentially insensitive to the distribution of
the nodes. As a consequence, on the log-scale, the asymptotic behavior of the
tail of the interference in any $\beta$-Ginibre network model, $0&lt;\beta\le 1$,
is the same as in the Poisson network model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2244</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2244</id><created>2013-04-08</created><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>Combinatorial Walrasian Equilibrium</title><categories>cs.GT</categories><comments>Accepted to STOC'2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a combinatorial market design problem, where a collection of
indivisible objects is to be priced and sold to potential buyers subject to
equilibrium constraints.The classic solution concept for such problems is
Walrasian Equilibrium (WE), which provides a simple and transparent pricing
structure that achieves optimal social welfare. The main weakness of the WE
notion is that it exists only in very restrictive cases. To overcome this
limitation, we introduce the notion of a Combinatorial Walrasian equilibium
(CWE), a natural relaxation of WE. The difference between a CWE and a
(non-combinatorial) WE is that the seller can package the items into
indivisible bundles prior to sale, and the market does not necessarily clear.
  We show that every valuation profile admits a CWE that obtains at least half
of the optimal (unconstrained) social welfare. Moreover, we devise a poly-time
algorithm that, given an arbitrary allocation X, computes a CWE that achieves
at least half of the welfare of X. Thus, the economic problem of finding a CWE
with high social welfare reduces to the algorithmic problem of social-welfare
approximation. In addition, we show that every valuation profile admits a CWE
that extracts a logarithmic fraction of the optimal welfare as revenue.
Finally, these results are complemented by strong lower bounds when the seller
is restricted to using item prices only, which motivates the use of bundles.
The strength of our results derives partly from their generality - our results
hold for arbitrary valuations that may exhibit complex combinations of
substitutes and complements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2263</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2263</id><created>2013-04-08</created><updated>2013-12-16</updated><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Felix</keyname><forenames>Luciano V.</forenames></author><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author><author><keyname>Spreafico</keyname><forenames>Marcos V. P.</forenames></author></authors><title>Linear codes on posets with extension property</title><categories>cs.IT math.IT</categories><comments>Final version; 18pp</comments><journal-ref>Discrete Mathematics, vol. 317, 2014, pp. 1-13</journal-ref><doi>10.1016/j.disc.2013.11.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate linear and additive codes in partially ordered Hamming-like
spaces that satisfy the extension property, meaning that automorphisms of
ideals extend to automorphisms of the poset. The codes are naturally described
in terms of translation association schemes that originate from the groups of
linear isometries of the space. We address questions of duality and invariants
of codes, establishing a connection between the dual association scheme and the
scheme defined on the dual poset (they are isomorphic if and only if the poset
is self-dual). We further discuss invariants that play the role of weight
enumerators of codes in the poset case. In the case of regular rooted trees
such invariants are linked to the classical problem of tree isomorphism. We
also study the question of whether these invariants are preserved under
standard operations on posets such as the ordinal sum and the like.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2266</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2266</id><created>2013-04-08</created><authors><author><keyname>Rowan</keyname><forenames>Mark</forenames></author><author><keyname>Neymotin</keyname><forenames>Samuel</forenames></author></authors><title>Synaptic Scaling Balances Learning in a Spiking Model of Neocortex</title><categories>q-bio.NC cs.NE</categories><comments>10 pages</comments><journal-ref>M. Rowan and S. Neymotin. Synaptic scaling balances learning in a
  spiking model of neocortex. In M. Tomassini et al., eds, 11th Int. Conf.
  Adaptive and Natural Comp. Algorithms (ICANNGA), LNCS vol. 7824, pp. 20-29,
  Lausanne, 2013. Springer</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning in the brain requires complementary mechanisms: potentiation and
activity-dependent homeostatic scaling. We introduce synaptic scaling to a
biologically-realistic spiking model of neocortex which can learn changes in
oscillatory rhythms using STDP, and show that scaling is necessary to balance
both positive and negative changes in input from potentiation and atrophy. We
discuss some of the issues that arise when considering synaptic scaling in such
a model, and show that scaling regulates activity whilst allowing learning to
remain unaltered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2268</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2268</id><created>2013-04-08</created><authors><author><keyname>Frasca</keyname><forenames>Paolo</forenames></author><author><keyname>Ravazzi</keyname><forenames>Chiara</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author></authors><title>Gossips and Prejudices: Ergodic Randomized Dynamics in Social Networks</title><categories>cs.SY cs.SI math.OC</categories><comments>submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a novel model of opinion dynamics in social networks,
which has two main features. First, agents asynchronously interact in pairs,
and these pairs are chosen according to a random process. We refer to this
communication model as &quot;gossiping&quot;. Second, agents are not completely
open-minded, but instead take into account their initial opinions, which may be
thought of as their &quot;prejudices&quot;. In the literature, such agents are often
called &quot;stubborn&quot;. We show that the opinions of the agents fail to converge,
but persistently undergo ergodic oscillations, which asymptotically concentrate
around a mean distribution of opinions. This mean value is exactly the limit of
the synchronous dynamics of the expected opinions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2269</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2269</id><created>2013-04-08</created><updated>2013-08-09</updated><authors><author><keyname>Cierny</keyname><forenames>Michal</forenames></author><author><keyname>Wang</keyname><forenames>Haining</forenames></author><author><keyname>Wichman</keyname><forenames>Risto</forenames></author><author><keyname>Ding</keyname><forenames>Zhi</forenames></author><author><keyname>Wijting</keyname><forenames>Carl</forenames></author></authors><title>On Number of Almost Blank Subframes in Heterogeneous Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In heterogeneous cellular scenarios with macrocells, femtocells or picocells
users may suffer from significant co-channel cross-tier interference. To manage
this interference 3GPP introduced almost blank subframe (ABSF), a subframe in
which the interferer tier is not allowed to transmit data. Vulnerable users
thus get a chance to be scheduled in ABSFs with reduced cross-tier
interference. We analyze downlink scenarios using stochastic geometry and
formulate a condition for the required number of ABSFs based on base station
placement statistics and user throughput requirement. The result is a
semi-analytical formula that serves as a good initial estimate and offers an
easy way to analyze impact of network parameters. We show that while in
macro/femto scenario the residue ABSF interference can be well managed, in
macro/pico scenario it affects the number of required ABSFs strongly. The
effect of ABSFs is subsequently demonstrated via user throughput simulations.
Especially in the macro/pico scenario, we find that using ABSFs is advantageous
for the system since victim users no longer suffer from poor performance for
the price of relatively small drop in higher throughput percentiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2272</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2272</id><created>2013-04-08</created><authors><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Fabregat</keyname><forenames>Diego</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Aulchenko</keyname><forenames>Yurii</forenames><affiliation>Institute of Cytology and Genetics, Novosibirsk</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>Algorithms for Large-scale Whole Genome Association Analysis</title><categories>cs.CE cs.MS q-bio.GN</categories><report-no>AICES-2013/04-2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to associate complex traits with genetic polymorphisms, genome-wide
association studies process huge datasets involving tens of thousands of
individuals genotyped for millions of polymorphisms. When handling these
datasets, which exceed the main memory of contemporary computers, one faces two
distinct challenges: 1) Millions of polymorphisms come at the cost of hundreds
of Gigabytes of genotype data, which can only be kept in secondary storage; 2)
the relatedness of the test population is represented by a covariance matrix,
which, for large populations, can only fit in the combined main memory of a
distributed architecture. In this paper, we present solutions for both
challenges: The genotype data is streamed from and to secondary storage using a
double buffering technique, while the covariance matrix is kept across the main
memory of a distributed memory system. We show that these methods sustain
high-performance and allow the analysis of enormous dataset
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2276</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2276</id><created>2013-04-08</created><authors><author><keyname>Cardone</keyname><forenames>Angelamaria</forenames></author><author><keyname>Jackiewicz</keyname><forenames>Zdzislaw</forenames></author><author><keyname>Zhang</keyname><forenames>Hong</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Extrapolation-based implicit-explicit general linear methods</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many systems of differential equations modeling problems in science and
engineering, there are natural splittings of the right hand side into two
parts, one non-stiff or mildly stiff, and the other one stiff. For such systems
implicit-explicit (IMEX) integration combines an explicit scheme for the
non-stiff part with an implicit scheme for the stiff part.
  In a recent series of papers two of the authors (Sandu and Zhang) have
developed IMEX GLMs, a family of implicit-explicit schemes based on general
linear methods. It has been shown that, due to their high stage order, IMEX
GLMs require no additional coupling order conditions, and are not marred by
order reduction.
  This work develops a new extrapolation-based approach to construct practical
IMEX GLM pairs of high order. We look for methods with large absolute stability
region, assuming that the implicit part of the method is A- or L-stable. We
provide examples of IMEX GLMs with optimal stability properties. Their
application to a two dimensional test problem confirms the theoretical
findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2281</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2281</id><created>2013-04-08</created><authors><author><keyname>Sharad</keyname><forenames>Mrigank</forenames></author><author><keyname>Fan</keyname><forenames>Deliang</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Ultra Low Power Associative Computing with Spin Neurons and Resistive
  Crossbar Memory</title><categories>cond-mat.mtrl-sci cs.ET</categories><journal-ref>In Proceedings of the 50th Annual Design Automation Conference (p.
  107). ACM., 2012</journal-ref><doi>10.1145/2463209.2488866</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging resistive-crossbar memory (RCM) technology can be promising for
computationally-expensive analog pattern-matching tasks. However, the use of
CMOS analog-circuits with RCM would result in large power-consumption and poor
scalability, thereby eschewing the benefits of RCM-based computation. We
propose the use of low-voltage, fast-switching, magneto-metallic spin-neurons
for ultra low-power non-Boolean computing with RCM. We present the design of
analog associative memory for face recognition using RCM, where, substituting
conventional analog circuits with spin-neurons can achieve ~100x lower power.
This makes the proposed design ~1000x more energy-efficient than a 45nm-CMOS
digital ASIC, thereby significantly enhancing the prospects of RCM based
computational hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2290</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2290</id><created>2013-04-08</created><updated>2015-08-07</updated><authors><author><keyname>Salikhmetov</keyname><forenames>Anton</forenames></author></authors><title>Macro Lambda Calculus</title><categories>cs.LO cs.FL</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of our Macro Lambda Calculus project (MLC) is to encode lambda terms
into interaction nets. Its software implementation will accept input in the
notation similar to lambda calculus allowing macro definitions. Output is
similar to interaction calculus and is suitable for our Interaction Nets
Compiler program (INC). In this paper, we describe the interaction system for
call-by-need evaluation and the mechanism of encoding lambda terms into this
system which MLC is based on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2295</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2295</id><created>2013-04-08</created><updated>2014-03-20</updated><authors><author><keyname>Gillibert</keyname><forenames>Pierre</forenames><affiliation>LIAFA, CAUL</affiliation></author></authors><title>The finiteness problem for automaton semigroups is undecidable</title><categories>cs.FL cs.DM</categories><proxy>ccsd</proxy><journal-ref>International Journal of Algebra and Computation 24, 1 (2014) 1-9</journal-ref><doi>10.1142/S0218196714500015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The finiteness problem for automaton groups and semigroups has been widely
studied, several partial positive results are known. However we prove that, in
the most general case, the problem is undecidable. We study the case of
automaton semigroups. Given a NW-deterministic Wang tile set, we construct an
Mealy automaton, such that the plane admit a valid Wang tiling if and only if
the Mealy automaton generates a finite semigroup. The construction is similar
to a construction by Kari for proving that the nilpotency problem for cellular
automata is unsolvable. Moreover Kari proves that the tiling of the plane is
undecidable for NW-deterministic Wang tile set. It follows that the finiteness
problem for automaton semigroup is undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2300</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2300</id><created>2013-04-08</created><authors><author><keyname>Ranjan</keyname><forenames>Gyan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhi-Li</forenames></author><author><keyname>Boley</keyname><forenames>Daniel</forenames></author></authors><title>Incremental Computation of Pseudo-Inverse of Laplacian: Theory and
  Applications</title><categories>cs.DM cs.SI</categories><comments>25 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A divide-and-conquer based approach for computing the Moore-Penrose
pseudo-inverse of the combinatorial Laplacian matrix $(\bb L^+)$ of a simple,
undirected graph is proposed. % The nature of the underlying sub-problems is
studied in detail by means of an elegant interplay between $\bb L^+$ and the
effective resistance distance $(\Omega)$. Closed forms are provided for a novel
{\em two-stage} process that helps compute the pseudo-inverse incrementally.
Analogous scalar forms are obtained for the converse case, that of structural
regress, which entails the breaking up of a graph into disjoint components
through successive edge deletions. The scalar forms in both cases, show
absolute element-wise independence at all stages, thus suggesting potential
parallelizability. Analytical and experimental results are presented for
dynamic (time-evolving) graphs as well as large graphs in general (representing
real-world networks). An order of magnitude reduction in computational time is
achieved for dynamic graphs; while in the general case, our approach performs
better in practice than the standard methods, even though the worst case
theoretical complexities may remain the same: an important contribution with
consequences to the study of online social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2302</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2302</id><created>2013-04-08</created><authors><author><keyname>Lovell</keyname><forenames>Dan</forenames></author><author><keyname>Malmaud</keyname><forenames>Jonathan</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author><author><keyname>Mansinghka</keyname><forenames>Vikash K.</forenames></author></authors><title>ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process
  Mixtures</title><categories>stat.ML cs.DC cs.LG</categories><comments>12 pages, 10 figures. Submitted to ICML 2013 during third submission
  cycle</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian
nonparametric modeling, and is widely used in tasks such as density estimation,
natural language processing, and time series modeling. Although MCMC inference
methods for the DP often provide a gold standard in terms asymptotic accuracy,
they can be computationally expensive and are not obviously parallelizable. We
propose a reparameterization of the Dirichlet process that induces conditional
independencies between the atoms that form the random measure. This conditional
independence enables many of the Markov chain transition operators for DP
inference to be simulated in parallel across multiple cores. Applied to mixture
modeling, our approach enables the Dirichlet process to simultaneously learn
clusters that describe the data and superclusters that define the granularity
of parallelization. Unlike previous approaches, our technique does not require
alteration of the model and leaves the true posterior distribution invariant.
It also naturally lends itself to a distributed software implementation in
terms of Map-Reduce, which we test in cluster configurations of over 50
machines and 100 cores. We present experiments exploring the parallel
efficiency and convergence properties of our approach on both synthetic and
real-world data, including runs on 1MM data vectors in 256 dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2310</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2310</id><created>2013-03-09</created><authors><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author><author><keyname>Maji</keyname><forenames>Prasenjit</forenames></author><author><keyname>Das</keyname><forenames>Poulami</forenames></author><author><keyname>Biswas</keyname><forenames>Shouvik</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Sheli Sinha</forenames></author></authors><title>Embedding of Blink Frequency in Electrooculography Signal using
  Difference Expansion based Reversible Watermarking Technique</title><categories>cs.CV cs.IR</categories><comments>6 Pages, 3 Figures, 4 Tables</comments><journal-ref>Scientific Bulletin of the Politehnica University of Timisoara -
  Transactions on Electronics and Communications p-ISSN 1583-3380, vol. 57(71),
  no. 2, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years, like other fields, rapid expansion of digitization and
globalization has influenced the medical field as well. For progress of
diagnostic results most of the reputed hospitals and diagnostic centres all
over the world have started exchanging medical information. In this proposed
method, the calculated diagnostic parametric values of the original
Electrooculography (EOG) signal are embedded as a watermark by using Difference
Expansion (DE) algorithm based reversible watermarking technique. The extracted
watermark provides the required parametric values at the recipient end without
any post computation of the recovered EOG signal. By computing the parametric
values from the recovered signal, the integrity of the extracted watermark can
be validated. The time domain features of EOG signal are calculated for the
generation of watermark. In the current work, various features are studied and
two major features related to blink frequency are used to generate the
watermark. The high Signal to Noise Ratio (SNR) and the Bit Error Rate (BER)
claim the robustness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2313</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2313</id><created>2013-04-04</created><authors><author><keyname>Ny</keyname><forenames>Jerome Le</forenames></author></authors><title>On Differentially Private Filtering for Event Streams</title><categories>cs.DB cs.CR cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1207.4305</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rigorous privacy mechanisms that can cope with dynamic data are required to
encourage a wider adoption of large-scale monitoring and decision systems
relying on end-user information. A promising approach to develop these
mechanisms is to specify quantitative privacy requirements at design time
rather than as an afterthought, and to rely on signal processing techniques to
achieve satisfying trade-offs between privacy and performance specifications.
This paper discusses, from the signal processing point of view, an event stream
analysis problem introduced in the database and cryptography literature. A
discrete-valued input signal describes the occurrence of events contributed by
end-users, and a system is supposed to provide some output signal based on this
information, while preserving the privacy of the participants. The notion of
privacy adopted here is that of event-level differential privacy, which
provides strong privacy guarantees and has important operational advantages.
Several mechanisms are described to provide differentially private output
signals while minimizing the impact on performance. These mechanisms
demonstrate the benefits of leveraging system theoretic techniques to provide
privacy guarantees for dynamic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2326</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2326</id><created>2013-04-08</created><authors><author><keyname>Kouki</keyname><forenames>Pigi</forenames></author></authors><title>Sharing of Semantically Enhanced Information for the Adaptive Execution
  of Business Processes</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated from the Context Aware Computing, and more particularly from the
Data-Driven Process Adaptation approach, we propose the Semantic Context Space
(SCS) Engine which aims to facilitate the provision of adaptable business
processes. The SCS Engine provides a space which stores semantically annotated
data and it is open to other processes, systems, and external sources for
information exchange. The specified implementation is inspired from the
Semantic TupleSpace and uses the JavaSpace Service of the Jini Framework
(changed to Apache River lately) as an underlying basis. The SCS Engine
supplies an interface where a client can execute the following operations: (i)
write: which inserts in the space available information along with its
respective meta-information, (ii) read: which retrieves from the space
information which meets specific meta-information constrains, and (iii) take:
which retrieves and simultaneously deletes from the space information which
meets specific meta-information constrains. In terms of this thesis the
available types of meta-information are based on ontologies described in RDFS
or WSML. The applicability of the SCS Engine implementation in the context of
data-driven process adaptation has been ensured by an experimental evaluation
of the provided operations. Ultimately, we refer to open issues which could be
addressed to enrich the proposed Engine with additional features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2331</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2331</id><created>2013-04-08</created><authors><author><keyname>Brummer</keyname><forenames>Niko</forenames></author><author><keyname>Preez</keyname><forenames>Johan du</forenames></author></authors><title>The PAV algorithm optimizes binary proper scoring rules</title><categories>stat.AP cs.LG stat.ML</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been much recent interest in application of the
pool-adjacent-violators (PAV) algorithm for the purpose of calibrating the
probabilistic outputs of automatic pattern recognition and machine learning
algorithms. Special cost functions, known as proper scoring rules form natural
objective functions to judge the goodness of such calibration. We show that for
binary pattern classifiers, the non-parametric optimization of calibration,
subject to a monotonicity constraint, can be solved by PAV and that this
solution is optimal for all regular binary proper scoring rules. This extends
previous results which were limited to convex binary proper scoring rules. We
further show that this result holds not only for calibration of probabilities,
but also for calibration of log-likelihood-ratios, in which case optimality
holds independently of the prior probabilities of the pattern classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2333</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2333</id><created>2013-04-08</created><updated>2013-10-07</updated><authors><author><keyname>Effenberger</keyname><forenames>Felix</forenames></author></authors><title>A primer on information theory, with applications to neuroscience</title><categories>cs.IT math.IT q-bio.NC</categories><comments>60 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the constant rise in quantity and quality of data obtained from neural
systems on many scales ranging from molecular to systems',
information-theoretic analyses became increasingly necessary during the past
few decades in the neurosciences. Such analyses can provide deep insights into
the functionality of such systems, as well as a rigid mathematical theory and
quantitative measures of information processing in both healthy and diseased
states of neural systems. This chapter will present a short introduction to the
fundamentals of information theory, especially suited for people having a less
firm background in mathematics and probability theory. To begin, the
fundamentals of probability theory such as the notion of probability,
probability distributions, and random variables will be reviewed. Then, the
concepts of information and entropy (in the sense of Shannon), mutual
information, and transfer entropy (sometimes also referred to as conditional
mutual information) will be outlined. As these quantities cannot be computed
exactly from measured data in practice, estimation techniques for
information-theoretic quantities will be presented. The chapter will conclude
with the applications of information theory in the field of neuroscience,
including questions of possible medical applications and a short review of
software packages that can be used for information-theoretic analyses of neural
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2336</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2336</id><created>2013-04-08</created><updated>2013-09-17</updated><authors><author><keyname>Datta</keyname><forenames>Nilanjana</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>One-shot lossy quantum data compression</title><categories>quant-ph cs.IT math.IT</categories><comments>36 pages</comments><journal-ref>IEEE Transactions on Information Theory vol. 59, no. 12, pages
  8057-8076 (December 2013)</journal-ref><doi>10.1109/TIT.2013.2283723</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a framework for one-shot quantum rate distortion coding, in which
the goal is to determine the minimum number of qubits required to compress
quantum information as a function of the probability that the distortion
incurred upon decompression exceeds some specified level. We obtain a one-shot
characterization of the minimum qubit compression size for an
entanglement-assisted quantum rate-distortion code in terms of the smooth
max-information, a quantity previously employed in the one-shot quantum reverse
Shannon theorem. Next, we show how this characterization converges to the known
expression for the entanglement-assisted quantum rate distortion function for
asymptotically many copies of a memoryless quantum information source. Finally,
we give a tight, finite blocklength characterization for the
entanglement-assisted minimum qubit compression size of a memoryless isotropic
qubit source subject to an average symbol-wise distortion constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2338</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2338</id><created>2013-04-08</created><updated>2013-09-23</updated><authors><author><keyname>Kelner</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>An Almost-Linear-Time Algorithm for Approximate Max Flow in Undirected
  Graphs, and its Multicommodity Generalizations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new framework for approximately solving flow
problems in capacitated, undirected graphs and apply it to provide
asymptotically faster algorithms for the maximum $s$-$t$ flow and maximum
concurrent multicommodity flow problems. For graphs with $n$ vertices and $m$
edges, it allows us to find an $\epsilon$-approximate maximum $s$-$t$ flow in
time $O(m^{1+o(1)}\epsilon^{-2})$, improving on the previous best bound of
$\tilde{O}(mn^{1/3} poly(1/\epsilon))$. Applying the same framework in the
multicommodity setting solves a maximum concurrent multicommodity flow problem
with $k$ commodities in $O(m^{1+o(1)}\epsilon^{-2}k^2)$ time, improving on the
existing bound of $\tilde{O}(m^{4/3} poly(k,\epsilon^{-1})$.
  Our algorithms utilize several new technical tools that we believe may be of
independent interest:
  - We give a non-Euclidean generalization of gradient descent and provide
bounds on its performance. Using this, we show how to reduce approximate
maximum flow and maximum concurrent flow to the efficient construction of
oblivious routings with a low competitive ratio.
  - We define and provide an efficient construction of a new type of flow
sparsifier. In addition to providing the standard properties of a cut
sparsifier our construction allows for flows in the sparse graph to be routed
(very efficiently) in the original graph with low congestion.
  - We give the first almost-linear-time construction of an
$O(m^{o(1)})$-competitive oblivious routing scheme. No previous such algorithm
ran in time better than $\tilde{{\Omega}}(mn)$.
  We also note that independently Jonah Sherman produced an almost linear time
algorithm for maximum flow and we thank him for coordinating submissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2339</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2339</id><created>2013-03-27</created><authors><author><keyname>Agosta</keyname><forenames>John Mark</forenames></author></authors><title>The structure of Bayes nets for vision recognition</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-1-7</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is part of a study whose goal is to show the effciency of using
Bayes networks to carry out model based vision calculations. [Binford et al.
1987] Recognition proceeds by drawing up a network model from the object's
geometric and functional description that predicts the appearance of an object.
Then this network is used to find the object within a photographic image. Many
existing and proposed techniques for vision recognition resemble the
uncertainty calculations of a Bayes net. In contrast, though, they lack a
derivation from first principles, and tend to rely on arbitrary parameters that
we hope to avoid by a network model. The connectedness of the network depends
on what independence considerations can be identified in the vision problem.
Greater independence leads to easier calculations, at the expense of the net's
expressiveness. Once this trade-off is made and the structure of the network is
determined, it should be possible to tailor a solution technique for it. This
paper explores the use of a network with multiply connected paths, drawing on
both techniques of belief networks [Pearl 86] and influence diagrams. We then
demonstrate how one formulation of a multiply connected network can be solved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2340</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2340</id><created>2013-03-27</created><authors><author><keyname>Aleliunas</keyname><forenames>Romas</forenames></author></authors><title>Summary of A New Normative Theory of Probabilistic Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-8-14</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By probabilistic logic I mean a normative theory of belief that explains how
a body of evidence affects one's degree of belief in a possible hypothesis. A
new axiomatization of such a theory is presented which avoids a finite
additivity axiom, yet which retains many useful inference rules. Many of the
examples of this theory--its models do not use numerical probabilities. Put
another way, this article gives sharper answers to the two questions: 1.What
kinds of sets can used as the range of a probability function? 2.Under what
conditions is the range set of a probability function isomorphic to the set of
real numbers in the interval 10,1/ with the usual arithmetical operations?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2341</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2341</id><created>2013-03-27</created><authors><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author></authors><title>Probability Distributions Over Possible Worlds</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-15-21</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Probabilistic Logic Nilsson uses the device of a probability distribution
over a set of possible worlds to assign probabilities to the sentences of a
logical language. In his paper Nilsson concentrated on inference and associated
computational issues. This paper, on the other hand, examines the probabilistic
semantics in more detail, particularly for the case of first-order languages,
and attempts to explain some of the features and limitations of this form of
probability logic. It is pointed out that the device of assigning probabilities
to logical sentences has certain expressive limitations. In particular,
statistical assertions are not easily expressed by such a device. This leads to
certain difficulties with attempts to give probabilistic semantics to default
reasoning using probabilities assigned to logical sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2342</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2342</id><created>2013-03-27</created><authors><author><keyname>Black</keyname><forenames>Paul K.</forenames></author><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>Hierarchical Evidence and Belief Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-22-29</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dempster/Shafer (D/S) theory has been advocated as a way of representing
incompleteness of evidence in a system's knowledge base. Methods now exist for
propagating beliefs through chains of inference. This paper discusses how rules
with attached beliefs, a common representation for knowledge in automated
reasoning systems, can be transformed into the joint belief functions required
by propagation algorithms. A rule is taken as defining a conditional belief
function on the consequent given the antecedents. It is demonstrated by example
that different joint belief functions may be consistent with a given set of
rules. Moreover, different representations of the same rules may yield
different beliefs on the consequent hypotheses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2343</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2343</id><created>2013-03-27</created><authors><author><keyname>Breese</keyname><forenames>John S.</forenames></author><author><keyname>Fehling</keyname><forenames>Michael R.</forenames></author></authors><title>Decision-Theoretic Control of Problem Solving: Principles and
  Architecture</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-30-37</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach to the design of autonomous, real-time
systems operating in uncertain environments. We address issues of problem
solving and reflective control of reasoning under uncertainty in terms of two
fundamental elements: l) a set of decision-theoretic models for selecting among
alternative problem-solving methods and 2) a general computational architecture
for resource-bounded problem solving. The decisiontheoretic models provide a
set of principles for choosing among alternative problem-solving methods based
on their relative costs and benefits, where benefits are characterized in terms
of the value of information provided by the output of a reasoning activity. The
output may be an estimate of some uncertain quantity or a recommendation for
action. The computational architecture, called Schemer-ll, provides for
interleaving of and communication among various problem-solving subsystems.
These subsystems provide alternative approaches to information gathering,
belief refinement, solution construction, and solution execution. In
particular, the architecture provides a mechanism for interrupting the
subsystems in response to critical events. We provide a decision theoretic
account for scheduling problem-solving elements and for critical-event-driven
interruption of activities in an architecture such as Schemer-II.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2344</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2344</id><created>2013-03-27</created><authors><author><keyname>Cecile</keyname><forenames>M.</forenames></author><author><keyname>McLeish</keyname><forenames>Mary</forenames></author><author><keyname>Pascoe</keyname><forenames>P.</forenames></author><author><keyname>Taylor</keyname><forenames>W.</forenames></author></authors><title>Induction and Uncertainty Management Techniques Applied to Veterinary
  Medical Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-38-48</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a project undertaken between the Departments of
Computing Science, Statistics, and the College of Veterinary Medicine to design
a medical diagnostic system. On-line medical data has been collected in the
hospital database system for several years. A number of induction methods are
being used to extract knowledge from the data in an attempt to improve upon
simple diagnostic charts used by the clinicians. They also enhance the results
of classical statistical methods - finding many more significant variables. The
second part of the paper describes an essentially Bayesian method of evidence
combination using fuzzy events at an initial step. Results are presented and
comparisons are made with other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2345</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2345</id><created>2013-03-27</created><authors><author><keyname>Chavez</keyname><forenames>R. Martin</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>KNET: Integrating Hypermedia and Bayesian Modeling</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-49-54</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  KNET is a general-purpose shell for constructing expert systems based on
belief networks and decision networks. Such networks serve as graphical
representations for decision models, in which the knowledge engineer must
define clearly the alternatives, states, preferences, and relationships that
constitute a decision basis. KNET contains a knowledge-engineering core written
in Object Pascal and an interface that tightly integrates HyperCard, a
hypertext authoring tool for the Apple Macintosh computer, into a novel
expert-system architecture. Hypertext and hypermedia have become increasingly
important in the storage management, and retrieval of information. In broad
terms, hypermedia deliver heterogeneous bits of information in dynamic,
extensively cross-referenced packages. The resulting KNET system features a
coherent probabilistic scheme for managing uncertainty, an objectoriented
graphics editor for drawing and manipulating decision networks, and HyperCard's
potential for quickly constructing flexible and friendly user interfaces. We
envision KNET as a useful prototyping tool for our ongoing research on a
variety of Bayesian reasoning problems, including tractable representation,
inference, and explanation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2346</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2346</id><created>2013-03-27</created><authors><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>A Method for Using Belief Networks as Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-55-63</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper demonstrates a method for using belief-network algorithms to solve
influence diagram problems. In particular, both exact and approximation
belief-network algorithms may be applied to solve influence-diagram problems.
More generally, knowing the relationship between belief-network and
influence-diagram problems may be useful in the design and development of more
efficient influence diagram algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2347</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2347</id><created>2013-03-27</created><authors><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author></authors><title>Process, Structure, and Modularity in Reasoning with Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-64-72</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational mechanisms for uncertainty management must support interactive
and incremental problem formulation, inference, hypothesis testing, and
decision making. However, most current uncertainty inference systems
concentrate primarily on inference, and provide no support for the larger
issues. We present a computational approach to uncertainty management which
provides direct support for the dynamic, incremental aspect of this task, while
at the same time permitting direct representation of the structure of
evidential relationships. At the same time, we show that this approach responds
to the modularity concerns of Heckerman and Horvitz [Heck87]. This paper
emphasizes examples of the capabilities of this approach. Another paper
[D'Am89] details the representations and algorithms involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2348</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2348</id><created>2013-03-27</created><authors><author><keyname>Dean</keyname><forenames>Thomas L.</forenames></author><author><keyname>Kanazawa</keyname><forenames>Keiji</forenames></author></authors><title>Probabilistic Causal Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-73-80</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the future is an important component of decision making. In most
situations, however, there is not enough information to make accurate
predictions. In this paper, we develop a theory of causal reasoning for
predictive inference under uncertainty. We emphasize a common type of
prediction that involves reasoning about persistence: whether or not a
proposition once made true remains true at some later time. We provide a
decision procedure with a polynomial-time algorithm for determining the
probability of the possible consequences of a set events and initial
conditions. The integration of simple probability theory with temporal
projection enables us to circumvent problems that nonmonotonic temporal
reasoning schemes have in dealing with persistence. The ideas in this paper
have been implemented in a prototype system that refines a database of causal
rules in the course of applying those rules to construct and carry out plans in
a manufacturing domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2349</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2349</id><created>2013-03-27</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Modeling uncertain and vague knowledge in possibility and evidence
  theories</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-81-89</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper advocates the usefulness of new theories of uncertainty for the
purpose of modeling some facets of uncertain knowledge, especially vagueness,
in AI. It can be viewed as a partial reply to Cheeseman's (among others)
defense of probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2350</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2350</id><created>2013-03-27</created><authors><author><keyname>Dutta</keyname><forenames>Soumitra</forenames></author></authors><title>A Temporal Logic for Uncertain Events and An Outline of A Possible
  Implementation in An Extension of PROLOG</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-90-97</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is uncertainty associated with the occurrence of many events in real
life. In this paper we develop a temporal logic to deal with such uncertain
events and outline a possible implementation in an extension of PROLOG. Events
are represented as fuzzy sets with the membership function giving the
possibility of occurrence of the event in a given interval of time. The
developed temporal logic is simple but powerful. It can determine effectively
the various temporal relations between uncertain events or their combinations.
PROLOG provides a uniform substrate on which to effectively implement such a
temporal logic for uncertain events
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2351</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2351</id><created>2013-03-27</created><authors><author><keyname>Eick</keyname><forenames>Christoph F.</forenames></author></authors><title>Uncertainty Management for Fuzzy Decision Support Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-98-108</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach for uncertainty management for fuzzy, rule based decision
support systems is proposed: The domain expert's knowledge is expressed by a
set of rules that frequently refer to vague and uncertain propositions. The
certainty of propositions is represented using intervals [a, b] expressing that
the proposition's probability is at least a and at most b. Methods and
techniques for computing the overall certainty of fuzzy compound propositions
that have been defined by using logical connectives 'and', 'or' and 'not' are
introduced. Different inference schemas for applying fuzzy rules by using modus
ponens are discussed. Different algorithms for combining evidence that has been
received from different rules for the same proposition are provided. The
relationship of the approach to other approaches is analyzed and its problems
of knowledge acquisition and knowledge representation are discussed in some
detail. The basic concepts of a rule-based programming language called PICASSO,
for which the approach is a theoretical foundation, are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2352</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2352</id><created>2013-03-27</created><authors><author><keyname>Frisch</keyname><forenames>Alan M.</forenames></author><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author></authors><title>Probability as a Modal Operator</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-109-118</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper argues for a modal view of probability. The syntax and semantics
of one particularly strong probability logic are discussed and some examples of
the use of the logic are provided. We show that it is both natural and useful
to think of probability as a modal operator. Contrary to popular belief in AI,
a probability ranging between 0 and 1 represents a continuum between
impossibility and necessity, not between simple falsity and truth. The present
work provides a clear semantics for quantification into the scope of the
probability operator and for higher-order probabilities. Probability logic is a
language for expressing both probabilistic and logical concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2353</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2353</id><created>2013-03-27</created><authors><author><keyname>Fu</keyname><forenames>Li-Min</forenames></author></authors><title>Truth Maintenance Under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-119-126</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of resolving errors under uncertainty in a
rule-based system. A new approach has been developed that reformulates this
problem as a neural-network learning problem. The strength and the fundamental
limitations of this approach are explored and discussed. The main result is
that neural heuristics can be applied to solve some but not all problems in
rule-based systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2354</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2354</id><created>2013-03-27</created><authors><author><keyname>Gallant</keyname><forenames>Stephen I.</forenames></author></authors><title>Bayesian Assessment of a Connectionist Model for Fault Detection</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-127-135</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A previous paper [2] showed how to generate a linear discriminant network
(LDN) that computes likely faults for a noisy fault detection problem by using
a modification of the perceptron learning algorithm called the pocket
algorithm. Here we compare the performance of this connectionist model with
performance of the optimal Bayesian decision rule for the example that was
previously described. We find that for this particular problem the
connectionist model performs about 97% as well as the optimal Bayesian
procedure. We then define a more general class of noisy single-pattern boolean
(NSB) fault detection problems where each fault corresponds to a single
:pattern of boolean instrument readings and instruments are independently
noisy. This is equivalent to specifying that instrument readings are
probabilistic but conditionally independent given any particular fault. We
prove:
  1. The optimal Bayesian decision rule for every NSB fault detection problem
is representable by an LDN containing no intermediate nodes. (This slightly
extends a result first published by Minsky &amp; Selfridge.) 2. Given an NSB fault
detection problem, then with arbitrarily high probability after sufficient
iterations the pocket algorithm will generate an LDN that computes an optimal
Bayesian decision rule for that problem. In practice we find that a reasonable
number of iterations of the pocket algorithm produces a network with good, but
not optimal, performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2355</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2355</id><created>2013-03-27</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>On the Logic of Causal Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-136-147</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the role of Directed Acyclic Graphs (DAGs) as a
representation of conditional independence relationships. We show that DAGs
offer polynomially sound and complete inference mechanisms for inferring
conditional independence relationships from a given causal set of such
relationships. As a consequence, d-separation, a graphical criterion for
identifying independencies in a DAG, is shown to uncover more valid
independencies then any other criterion. In addition, we employ the Armstrong
property of conditional independence to show that the dependence relationships
displayed by a DAG are inherently consistent, i.e. for every DAG D there exists
some probability distribution P that embodies all the conditional
independencies displayed in D and none other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2356</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2356</id><created>2013-03-27</created><authors><author><keyname>Hansson</keyname><forenames>Othar</forenames></author><author><keyname>Mayer</keyname><forenames>Andy</forenames></author></authors><title>The Optimality of Satisficing Solutions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-148-157</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a prevailing assumption in single-agent heuristic search
theory- that problem-solving algorithms should guarantee shortest-path
solutions, which are typically called optimal. Optimality implies a metric for
judging solution quality, where the optimal solution is the solution with the
highest quality. When path-length is the metric, we will distinguish such
solutions as p-optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2357</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2357</id><created>2013-03-27</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>An Empirical Comparison of Three Inference Methods</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1988-PG-158-169</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an empirical evaluation of three inference methods for
uncertain reasoning is presented in the context of Pathfinder, a large expert
system for the diagnosis of lymph-node pathology. The inference procedures
evaluated are (1) Bayes' theorem, assuming evidence is conditionally
independent given each hypothesis; (2) odds-likelihood updating, assuming
evidence is conditionally independent given each hypothesis and given the
negation of each hypothesis; and (3) a inference method related to the
Dempster-Shafer theory of belief. Both expert-rating and decision-theoretic
metrics are used to compare the diagnostic accuracy of the inference methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2358</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2358</id><created>2013-03-27</created><authors><author><keyname>Hunter</keyname><forenames>Daniel</forenames></author></authors><title>Parallel Belief Revision</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-170-177</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a formal system of belief revision developed by Wolfgang
Spohn and shows that this system has a parallel implementation that can be
derived from an influence diagram in a manner similar to that in which Bayesian
networks are derived. The proof rests upon completeness results for an
axiomatization of the notion of conditional independence, with the Spohn system
being used as a semantics for the relation of conditional independence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2359</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2359</id><created>2013-03-27</created><authors><author><keyname>Jain</keyname><forenames>Pramod</forenames></author><author><keyname>Agogino</keyname><forenames>Alice M.</forenames></author></authors><title>Stochastic Sensitivity Analysis Using Fuzzy Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-178-188</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The practice of stochastic sensitivity analysis described in the decision
analysis literature is a testimonial to the need for considering deviations
from precise point estimates of uncertainty. We propose the use of Bayesian
fuzzy probabilities within an influence diagram computational scheme for
performing sensitivity analysis during the solution of probabilistic inference
and decision problems. Unlike other parametric approaches, the proposed scheme
does not require resolving the problem for the varying probability point
estimates. We claim that the solution to fuzzy influence diagrams provides as
much information as the classical point estimate approach plus additional
information concerning stochastic sensitivity. An example based on diagnostic
decision making in microcomputer assembly is used to illustrate this idea. We
claim that the solution to fuzzy influence diagrams provides as much
information as the classical point estimate approach plus additional interval
information that is useful for stochastic sensitivity analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2360</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2360</id><created>2013-03-27</created><authors><author><keyname>Jimison</keyname><forenames>Holly B.</forenames></author></authors><title>A Representation of Uncertainty to Aid Insight into Decision Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-189-196</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world models can be characterized as weak, meaning that there is
significant uncertainty in both the data input and inferences. This lack of
determinism makes it especially difficult for users of computer decision aids
to understand and have confidence in the models. This paper presents a
representation for uncertainty and utilities that serves as a framework for
graphical summary and computer-generated explanation of decision models. The
application described that tests the methodology is a computer decision aid
designed to enhance the clinician-patient consultation process for patients
with angina (chest pain due to lack of blood flow to the heart muscle). The
angina model is represented as a Bayesian decision network. Additionally, the
probabilities and utilities are treated as random variables with probability
distributions on their range of possible values. The initial distributions
represent information on all patients with anginal symptoms, and the approach
allows for rapid tailoring to more patientspecific distributions. This
framework provides a metric for judging the importance of each variable in the
model dynamically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2361</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2361</id><created>2013-03-27</created><authors><author><keyname>Kadie</keyname><forenames>Carl</forenames></author></authors><title>Rational Nonmonotonic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-197-204</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonmonotonic reasoning is a pattern of reasoning that allows an agent to make
and retract (tentative) conclusions from inconclusive evidence. This paper
gives a possible-worlds interpretation of the nonmonotonic reasoning problem
based on standard decision theory and the emerging probability logic. The
system's central principle is that a tentative conclusion is a decision to make
a bet, not an assertion of fact. The system is rational, and as sound as the
proof theory of its underlying probability log.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2362</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2362</id><created>2013-03-27</created><authors><author><keyname>Kalagnanam</keyname><forenames>Jayant</forenames></author><author><keyname>Henrion</keyname><forenames>Max</forenames></author></authors><title>A Comparison of Decision Analysis and Expert Rules for Sequential
  Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-205-212</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has long been debate about the relative merits of decision theoretic
methods and heuristic rule-based approaches for reasoning under uncertainty. We
report an experimental comparison of the performance of the two approaches to
troubleshooting, specifically to test selection for fault diagnosis. We use as
experimental testbed the problem of diagnosing motorcycle engines. The first
approach employs heuristic test selection rules obtained from expert mechanics.
We compare it with the optimal decision analytic algorithm for test selection
which employs estimated component failure probabilities and test costs. The
decision analytic algorithm was found to reduce the expected cost (i.e. time)
to arrive at a diagnosis by an average of 14% relative to the expert rules.
Sensitivity analysis shows the results are quite robust to inaccuracy in the
probability and cost estimates. This difference suggests some interesting
implications for knowledge acquisition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2363</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2363</id><created>2013-03-27</created><authors><author><keyname>Kwok</keyname><forenames>Suk Wah</forenames></author><author><keyname>Carter</keyname><forenames>Chris</forenames></author></authors><title>Multiple decision trees</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-213-220</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes experiments, on two domains, to investigate the effect
of averaging over predictions of multiple decision trees, instead of using a
single tree. Other authors have pointed out theoretical and commonsense reasons
for preferring the multiple tree approach. Ideally, we would like to consider
predictions from all trees, weighted by their probability. However, there is a
vast number of different trees, and it is difficult to estimate the probability
of each tree. We sidestep the estimation problem by using a modified version of
the ID3 algorithm to build good trees, and average over only these trees. Our
results are encouraging. For each domain, we managed to produce a small number
of good trees. We find that it is best to average across sets of trees with
different structure; this usually gives better performance than any of the
constituent trees, including the ID3 tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2364</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2364</id><created>2013-03-27</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Probabilistic Inference and Probabilistic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-221-228</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncertainty enters into human reasoning and inference in at least two ways.
It is reasonable to suppose that there will be roles for these distinct uses of
uncertainty also in automated reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2365</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2365</id><created>2013-03-27</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Probabilistic and Non-Monotonic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-229-236</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  (l) I have enough evidence to render the sentence S probable. (la) So,
relative to what I know, it is rational of me to believe S. (2) Now that I have
more evidence, S may no longer be probable. (2a) So now, relative to what I
know, it is not rational of me to believe S. These seem a perfectly ordinary,
common sense, pair of situations. Generally and vaguely, I take them to embody
what I shall call probabilistic inference. This form of inference is clearly
non-monotonic. Relatively few people have taken this form of inference, based
on high probability, to serve as a foundation for non-monotonic logic or for a
logical or defeasible inference. There are exceptions: Jane Nutter [16] thinks
that sometimes probability has something to do with non-monotonic reasoning.
Judea Pearl [ 17] has recently been exploring the possibility. There are any
number of people whom one might call probability enthusiasts who feel that
probability provides all the answers by itself, with no need of help from
logic. Cheeseman [1], Henrion [5] and others think it useful to look at a
distribution of probabilities over a whole algebra of statements, to update
that distribution in the light of new evidence, and to use the latest updated
distribution of probability over the algebra as a basis for planning and
decision making. A slightly weaker form of this approach is captured by Nilsson
[15], where one assumes certain probabilities for certain statements, and
infers the probabilities, or constraints on the probabilities of other
statement. None of this corresponds to what I call probabilistic inference. All
of the inference that is taking place, either in Bayesian updating, or in
probabilistic logic, is strictly deductive. Deductive inference, particularly
that concerned with the distribution of classical probabilities or chances, is
of great importance. But this is not to say that there is no important role for
what earlier logicians have called ?ampliative? or ?inductive? or ?scientific?
inference, in which the conclusion goes beyond the premises, asserts more than
do the premises. This depends on what David Israel [6] has called ?real rules
of inference?. It is characteristic of any such logic or inference procedure
that it can go wrong: that statements accepted at one point may be rejected at
a later point. Research underlying the results reported here has been partially
supported by the Signals Warfare Center of the United States Army.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2366</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2366</id><created>2013-03-27</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Epistemological Relevance and Statistical Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-237-244</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many years, at least since McCarthy and Hayes (1969), writers have
lamented, and attempted to compensate for, the alleged fact that we often do
not have adequate statistical knowledge for governing the uncertainty of
belief, for making uncertain inferences, and the like. It is hardly ever
spelled out what &quot;adequate statistical knowledge&quot; would be, if we had it, and
how adequate statistical knowledge could be used to control and regulate
epistemic uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2367</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2367</id><created>2013-03-27</created><authors><author><keyname>Levitt</keyname><forenames>Tod S.</forenames></author><author><keyname>Binford</keyname><forenames>Thomas O.</forenames></author><author><keyname>Ettinger</keyname><forenames>Gil J.</forenames></author><author><keyname>Gelband</keyname><forenames>Patrice</forenames></author></authors><title>Utility-Based Control for Computer Vision</title><categories>cs.CV cs.AI cs.SY</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-245-256</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several key issues arise in implementing computer vision recognition of world
objects in terms of Bayesian networks. Computational efficiency is a driving
force. Perceptual networks are very deep, typically fifteen levels of
structure. Images are wide, e.g., an unspecified-number of edges may appear
anywhere in an image 512 x 512 pixels or larger. For efficiency, we dynamically
instantiate hypotheses of observed objects. The network is not fixed, but is
created incrementally at runtime. Generation of hypotheses of world objects and
indexing of models for recognition are important, but they are not considered
here [4,11]. This work is aimed at near-term implementation with parallel
computation in a radar surveillance system, ADRIES [5, 15], and a system for
industrial part recognition, SUCCESSOR [2]. For many applications, vision must
be faster to be practical and so efficiently controlling the machine vision
process is critical. Perceptual operators may scan megapixels and may require
minutes of computation time. It is necessary to avoid unnecessary sensor
actions and computation. Parallel computation is available at several levels of
processor capability. The potential for parallel, distributed computation for
high-level vision means distributing non-homogeneous computations. This paper
addresses the problem of task control in machine vision systems based on
Bayesian probability models. We separate control and inference to extend the
previous work [3] to maximize utility instead of probability. Maximizing
utility allows adopting perceptual strategies for efficient information
gathering with sensors and analysis of sensor data. Results of controlling
machine vision via utility to recognize military situations are presented in
this paper. Future work extends this to industrial part recognition for
SUCCESSOR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2368</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2368</id><created>2013-03-27</created><authors><author><keyname>Loui</keyname><forenames>Ronald P.</forenames></author></authors><title>Evidential Reasoning in a Network Usage Prediction Testbed</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-257-265</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on empirical work aimed at comparing evidential reasoning
techniques. While there is prima facie evidence for some conclusions, this i6
work in progress; the present focus is methodology, with the goal that
subsequent results be meaningful. The domain is a network of UNIX* cycle
servers, and the task is to predict properties of the state of the network from
partial descriptions of the state. Actual data from the network are taken and
used for blindfold testing in a betting game that allows abstention. The focal
technique has been Kyburg's method for reasoning with data of varying relevance
to a particular query, though the aim is to be able eventually to compare
various uncertainty calculi. The conclusions are not novel, but are
instructive. 1. All of the calculi performed better than human subjects, so
unbiased access to sample experience is apparently of value. 2. Performance
depends on metric: (a) when trials are repeated, net = gains - losses favors
methods that place many bets, if the probability of placing a correct bet is
sufficiently high; that is, it favors point-valued formalisms; (b) yield =
gains/(gains + lossee) favors methods that bet only when sure to bet correctly;
that is, it favors interval-valued formalisms. 3. Among the calculi, there were
no clear winners or losers. Methods are identified for eliminating the bias of
the net as a performance criterion and for separating the calculi effectively:
in both cases by posting odds for the betting game in the appropriate way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2369</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2369</id><created>2013-03-27</created><authors><author><keyname>Neapolitan</keyname><forenames>Richard E.</forenames></author><author><keyname>Kenevan</keyname><forenames>James</forenames></author></authors><title>Justifying the Principle of Interval Constraints</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-266-274</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When knowledge is obtained from a database, it is only possible to deduce
confidence intervals for probability values. With confidence intervals
replacing point values, the results in the set covering model include interval
constraints for the probabilities of mutually exclusive and exhaustive
explanations. The Principle of Interval Constraints ranks these explanations by
determining the expected values of the probabilities based on distributions
determined from the interval, constraints. This principle was developed using
the Classical Approach to probability. This paper justifies the Principle of
Interval Constraints with a more rigorous statement of the Classical Approach
and by defending the concept of probabilities of probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2370</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2370</id><created>2013-03-27</created><authors><author><keyname>Neufeld</keyname><forenames>Eric</forenames></author><author><keyname>Poole</keyname><forenames>David L</forenames></author></authors><title>Probabilistic Semantics and Defaults</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-275-282</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is much interest in providing probabilistic semantics for defaults but
most approaches seem to suffer from one of two problems: either they require
numbers, a problem defaults were intended to avoid, or they generate peculiar
side effects. Rather than provide semantics for defaults, we address the
problem defaults were intended to solve: that of reasoning under uncertainty
where numeric probability distributions are not available. We describe a
non-numeric formalism called an inference graph based on standard probability
theory, conditional independence and sentences of favouring where a favours b -
favours(a, b) - p(a|b) &gt; p(a). The formalism seems to handle the examples from
the nonmonotonic literature. Most importantly, the sentences of our system can
be verified by performing an appropriate experiment in the semantic domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2371</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2371</id><created>2013-03-27</created><authors><author><keyname>Pittarelli</keyname><forenames>Michael</forenames></author></authors><title>Decision Making with Linear Constraints on Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-283-290</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Techniques for decision making with knowledge of linear constraints on
condition probabilities are examined. These constraints arise naturally in many
situations: upper and lower condition probabilities are known; an ordering
among the probabilities is determined; marginal probabilities or bounds on such
probabilities are known, e.g., data are available in the form of a
probabilistic database (Cavallo and Pittarelli, 1987a); etc. Standard
situations of decision making under risk and uncertainty may also be
characterized by linear constraints. Each of these types of information may be
represented by a convex polyhedron of numerically determinate condition
probabilities. A uniform approach to decision making under risk, uncertainty,
and partial uncertainty based on a generalized version of a criterion of
Hurwicz is proposed, Methods for processing marginal probabilities to improve
decision making using any of the criteria discussed are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2372</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2372</id><created>2013-03-27</created><authors><author><keyname>Reid</keyname><forenames>Thomas F.</forenames></author><author><keyname>Parnell</keyname><forenames>Gregory S.</forenames></author></authors><title>Maintenance in Probabilistic Knowledge-Based Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-291-298</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments using directed acyclical graphs (i.e., influence diagrams
and Bayesian networks) for knowledge representation have lessened the problems
of using probability in knowledge-based systems (KBS). Most current research
involves the efficient propagation of new evidence, but little has been done
concerning the maintenance of domain-specific knowledge, which includes the
probabilistic information about the problem domain. By making use of
conditional independencies represented in she graphs, however, probability
assessments are required only for certain variables when the knowledge base is
updated. The purpose of this study was to investigate, for those variables
which require probability assessments, ways to reduce the amount of new
knowledge required from the expert when updating probabilistic information in a
probabilistic knowledge-based system. Three special cases (ignored outcome,
split outcome, and assumed constraint outcome) were identified under which many
of the original probabilities (those already in the knowledge-base) do not need
to be reassessed when maintenance is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2373</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2373</id><created>2013-03-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>A Linear Approximation Method for Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-299-306</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approximation method is presented for probabilistic inference with
continuous random variables. These problems can arise in many practical
problems, in particular where there are &quot;second order&quot; probabilities. The
approximation, based on the Gaussian influence diagram, iterates over linear
approximations to the inference problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2374</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2374</id><created>2013-03-27</created><authors><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author><author><keyname>Shafer</keyname><forenames>Glenn</forenames></author></authors><title>An Axiomatic Framework for Bayesian and Belief-function Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-307-314</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe an abstract framework and axioms under which exact
local computation of marginals is possible. The primitive objects of the
framework are variables and valuations. The primitive operators of the
framework are combination and marginalization. These operate on valuations. We
state three axioms for these operators and we derive the possibility of local
computation from the axioms. Next, we describe a propagation scheme for
computing marginals of a valuation when we have a factorization of the
valuation on a hypertree. Finally we show how the problem of computing
marginals of joint probability distributions and joint belief functions fits
the general framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2375</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2375</id><created>2013-03-27</created><authors><author><keyname>Spohn</keyname><forenames>Wolfgang</forenames></author></authors><title>A General Non-Probabilistic Theory of Inductive Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-315-322</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probability theory, epistemically interpreted, provides an excellent, if not
the best available account of inductive reasoning. This is so because there are
general and definite rules for the change of subjective probabilities through
information or experience; induction and belief change are one and same topic,
after all. The most basic of these rules is simply to conditionalize with
respect to the information received; and there are similar and more general
rules. 1 Hence, a fundamental reason for the epistemological success of
probability theory is that there at all exists a well-behaved concept of
conditional probability. Still, people have, and have reasons for, various
concerns over probability theory. One of these is my starting point:
Intuitively, we have the notion of plain belief; we believe propositions2 to be
true (or to be false or neither). Probability theory, however, offers no formal
counterpart to this notion. Believing A is not the same as having probability 1
for A, because probability 1 is incorrigible3; but plain belief is clearly
corrigible. And believing A is not the same as giving A a probability larger
than some 1 - c, because believing A and believing B is usually taken to be
equivalent to believing A &amp; B.4 Thus, it seems that the formal representation
of plain belief has to take a non-probabilistic route. Indeed, representing
plain belief seems easy enough: simply represent an epistemic state by the set
of all propositions believed true in it or, since I make the common assumption
that plain belief is deductively closed, by the conjunction of all propositions
believed true in it. But this does not yet provide a theory of induction, i.e.
an answer to the question how epistemic states so represented are changed
tbrough information or experience. There is a convincing partial answer: if the
new information is compatible with the old epistemic state, then the new
epistemic state is simply represented by the conjunction of the new information
and the old beliefs. This answer is partial because it does not cover the quite
common case where the new information is incompatible with the old beliefs. It
is, however, important to complete the answer and to cover this case, too;
otherwise, we would not represent plain belief as conigible. The crucial
problem is that there is no good completion. When epistemic states are
represented simply by the conjunction of all propositions believed true in it,
the answer cannot be completed; and though there is a lot of fruitful work, no
other representation of epistemic states has been proposed, as far as I know,
which provides a complete solution to this problem. In this paper, I want to
suggest such a solution. In [4], I have more fully argued that this is the only
solution, if certain plausible desiderata are to be satisfied. Here, in section
2, I will be content with formally defining and intuitively explaining my
proposal. I will compare my proposal with probability theory in section 3. It
will turn out that the theory I am proposing is structurally homomorphic to
probability theory in important respects and that it is thus equally easily
implementable, but moreover computationally simpler. Section 4 contains a very
brief comparison with various kinds of logics, in particular conditional logic,
with Shackle's functions of potential surprise and related theories, and with
the Dempster - Shafer theory of belief functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2376</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2376</id><created>2013-03-27</created><authors><author><keyname>Star</keyname><forenames>Spencer</forenames></author></authors><title>Generating Decision Structures and Causal Explanations for Decision
  Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-323-334</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines two related problems that are central to developing an
autonomous decision-making agent, such as a robot. Both problems require
generating structured representafions from a database of unstructured
declarative knowledge that includes many facts and rules that are irrelevant in
the problem context. The first problem is how to generate a well structured
decision problem from such a database. The second problem is how to generate,
from the same database, a well-structured explanation of why some possible
world occurred. In this paper it is shown that the problem of generating the
appropriate decision structure or explanation is intractable without
introducing further constraints on the knowledge in the database. The paper
proposes that the problem search space can be constrained by adding knowledge
to the database about causal relafions between events. In order to determine
the causal knowledge that would be most useful, causal theories for
deterministic and indeterministic universes are proposed. A program that uses
some of these causal constraints has been used to generate explanations about
faulty plans. The program shows the expected increase in efficiency as the
causal constraints are introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2377</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2377</id><created>2013-03-27</created><authors><author><keyname>Suermondt</keyname><forenames>Jaap</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>Updating Probabilities in Multiply-Connected Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-335-343</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on probability updates in multiply-connected belief
networks. Pearl has designed the method of conditioning, which enables us to
apply his algorithm for belief updates in singly-connected networks to
multiply-connected belief networks by selecting a loop-cutset for the network
and instantiating these loop-cutset nodes. We discuss conditions that need to
be satisfied by the selected nodes. We present a heuristic algorithm for
finding a loop-cutset that satisfies these conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2378</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2378</id><created>2013-03-27</created><authors><author><keyname>Tessem</keyname><forenames>Bjornar</forenames></author><author><keyname>Ersland</keyname><forenames>Lars Johan</forenames></author></authors><title>Handling uncertainty in a system for text-symbol context analysis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-344-351</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In pattern analysis, information regarding an object can often be drawn from
its surroundings. This paper presents a method for handling uncertainty when
using context of symbols and texts for analyzing technical drawings. The method
is based on Dempster-Shafer theory and possibility theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2379</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2379</id><created>2013-03-27</created><authors><author><keyname>Verma</keyname><forenames>Tom S.</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Causal Networks: Semantics and Expressiveness</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-352-359</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependency knowledge of the form &quot;x is independent of y once z is known&quot;
invariably obeys the four graphoid axioms, examples include probabilistic and
database dependencies. Often, such knowledge can be represented efficiently
with graphical structures such as undirected graphs and directed acyclic graphs
(DAGs). In this paper we show that the graphical criterion called d-separation
is a sound rule for reading independencies from any DAG based on a causal input
list drawn from a graphoid. The rule may be extended to cover DAGs that
represent functional dependencies as well as conditional dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2380</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2380</id><created>2013-03-27</created><authors><author><keyname>Wen</keyname><forenames>Wilson X.</forenames></author></authors><title>MCE Reasoning in Recursive Causal Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-360-367</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A probabilistic method of reasoning under uncertainty is proposed based on
the principle of Minimum Cross Entropy (MCE) and concept of Recursive Causal
Model (RCM). The dependency and correlations among the variables are described
in a special language BNDL (Belief Networks Description Language). Beliefs are
propagated among the clauses of the BNDL programs representing the underlying
probabilistic distributions. BNDL interpreters in both Prolog and C has been
developed and the performance of the method is compared with those of the
others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2381</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2381</id><created>2013-03-27</created><authors><author><keyname>Yager</keyname><forenames>Ronald R.</forenames></author></authors><title>Nonmonotonic Reasoning via Possibility Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-368-373</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the operation of possibility qualification and show how. this
modal-like operator can be used to represent &quot;typical&quot; or default knowledge in
a theory of nonmonotonic reasoning. We investigate the representational power
of this approach by looking at a number of prototypical problems from the
nonmonotonic reasoning literature. In particular we look at the so called Yale
shooting problem and its relation to priority in default reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2382</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2382</id><created>2013-03-27</created><authors><author><keyname>Yeh</keyname><forenames>Alexander</forenames></author></authors><title>Predicting the Likely Behaviors of Continuous Nonlinear Systems in
  Equilibrium</title><categories>cs.SY cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-374-381</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a method for predicting the likely behaviors of
continuous nonlinear systems in equilibrium in which the input values can vary.
The method uses a parameterized equation model and a lower bound on the input
joint density to bound the likelihood that some behavior will occur, such as a
state variable being inside a given numeric range. Using a bound on the density
instead of the density itself is desirable because often the input density's
parameters and shape are not exactly known. The new method is called SAB after
its basic operations: split the input value space into smaller regions, and
then bound those regions' possible behaviors and the probability of being in
them. SAB finds rough bounds at first, and then refines them as more time is
given. In contrast to other researchers' methods, SAB can (1) find all the
possible system behaviors, and indicate how likely they are, (2) does not
approximate the distribution of possible outcomes without some measure of the
error magnitude, (3) does not use discretized variable values, which limit the
events one can find probability bounds for, (4) can handle density bounds, and
(5) can handle such criteria as two state variables both being inside a numeric
range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2383</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2383</id><created>2013-03-27</created><authors><author><keyname>Yen</keyname><forenames>John</forenames></author></authors><title>Generalizing the Dempster-Shafer Theory to Fuzzy Sets</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)</comments><proxy>auai</proxy><report-no>UAI-P-1988-PG-382-391</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the desire to apply the Dempster-Shafer theory to complex real world
problems where the evidential strength is often imprecise and vague, several
attempts have been made to generalize the theory. However, the important
concept in the D-S theory that the belief and plausibility functions are lower
and upper probabilities is no longer preserved in these generalizations. In
this paper, we describe a generalized theory of evidence where the degree of
belief in a fuzzy set is obtained by minimizing the probability of the fuzzy
set under the constraints imposed by a basic probability assignment. To
formulate the probabilistic constraint of a fuzzy focal element, we decompose
it into a set of consonant non-fuzzy focal elements. By generalizing the
compatibility relation to a possibility theory, we are able to justify our
generalization to Dempster's rule based on possibility distribution. Our
generalization not only extends the application of the D-S theory but also
illustrates a way that probability theory and fuzzy set theory can be combined
to deal with different kinds of uncertain information in AI systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2384</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2384</id><created>2013-04-05</created><authors><author><keyname>Saad</keyname><forenames>Emad</forenames></author></authors><title>Logical Fuzzy Optimization</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a logical framework to represent and reason about fuzzy
optimization problems based on fuzzy answer set optimization programming. This
is accomplished by allowing fuzzy optimization aggregates, e.g., minimum and
maximum in the language of fuzzy answer set optimization programming to allow
minimization or maximization of some desired criteria under fuzzy environments.
We show the application of the proposed logical fuzzy optimization framework
under the fuzzy answer set optimization programming to the fuzzy water
allocation optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2387</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2387</id><created>2013-04-07</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Blind Interference Suppression and Power Adjustment with Alternating
  Optimization for Cooperative DS-CDMA Networks</title><categories>cs.IT math.IT</categories><comments>2 figures. arXiv admin note: substantial text overlap with
  arXiv:1301.5912, arXiv:1304.1935</comments><journal-ref>SSP 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents blind joint interference suppression and power allocation
algorithms for DS-CDMA networks with multiple relays and decode and forward
protocols. A scheme for joint allocation of power levels across the relays
subject to group-based power constraints and the design of linear receivers for
interference suppression is proposed. A code-constrained constant modulus (CCM)
design for the receive filters and the power allocation vectors is devised
along with a blind channel estimator. In order to solve the proposed
optimization efficiently, an alternating optimization strategy is presented
with recursive least squares (RLS)-type algorithms for estimating the
parameters of the receiver, the power allocation and the channels. Simulations
show that the proposed algorithms obtain significant gains in capacity and
performance over existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2388</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2388</id><created>2013-04-07</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Li</keyname><forenames>S.</forenames></author></authors><title>Joint Iterative Power Adjustment and Interference Suppression Algorithms
  for Cooperative DS-CDMA Networks</title><categories>cs.IT math.IT</categories><comments>2 figures. arXiv admin note: substantial text overlap with
  arXiv:1301.0094, arXiv:1301.5912, arXiv:1304.1935</comments><journal-ref>VTC 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents joint iterative power allocation and interference
suppression algorithms for DS-CDMA networks which employ multiple relays and
the amplify and forward cooperation strategy. We propose a joint constrained
optimization framework that considers the allocation of power levels across the
relays subject to individual and global power constraints and the design of
linear receivers for interference suppression. We derive constrained minimum
mean-squared error (MMSE) expressions for the parameter vectors that determine
the optimal power levels across the relays and the parameters of the linear
receivers. In order to solve the proposed optimization problems efficiently, we
develop recursive least squares (RLS) algorithms for adaptive joint iterative
power allocation, and receiver and channel parameter estimation. Simulation
results show that the proposed algorithms obtain significant gains in
performance and capacity over existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2401</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2401</id><created>2013-04-08</created><authors><author><keyname>Murnane</keyname><forenames>Elizabeth L.</forenames></author><author><keyname>Haslhofer</keyname><forenames>Bernhard</forenames></author><author><keyname>Lagoze</keyname><forenames>Carl</forenames></author></authors><title>RESLVE: Leveraging User Interest to Improve Entity Disambiguation on
  Short Text</title><categories>cs.IR cs.HC</categories><acm-class>H.3.3; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the Named Entity Disambiguation (NED) problem for short,
user-generated texts on the social Web. In such settings, the lack of
linguistic features and sparse lexical context result in a high degree of
ambiguity and sharp performance drops of nearly 50% in the accuracy of
conventional NED systems. We handle these challenges by developing a model of
user-interest with respect to a personal knowledge context; and Wikipedia, a
particularly well-established and reliable knowledge base, is used to
instantiate the procedure. We conduct systematic evaluations using individuals'
posts from Twitter, YouTube, and Flickr and demonstrate that our novel
technique is able to achieve substantial performance gains beyond
state-of-the-art NED methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2412</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2412</id><created>2013-04-08</created><authors><author><keyname>Cantone</keyname><forenames>Domenico</forenames></author><author><keyname>Asmundo</keyname><forenames>Marianna Nicolosi</forenames></author></authors><title>On the satisfiability problem for a 3-level quantified syllogistic</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1209.1943</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a collection of three-sorted set-theoretic formulae, denoted
TLQSR and which admits a restricted form of quantification over individual and
set variables, has a solvable satisfiability problem by proving that it enjoys
a small model property, i.e., any satisfiable TLQSR-formula psi has a finite
model whose size depends solely on the size of psi itself. We also introduce
the sublanguages (TLQSR)^h of TLQSR, whose formulae are characterized by having
quantifier prefixes of length bounded by h \geq 2 and some other syntactic
constraints, and we prove that each of them has the satisfiability problem
NP-complete. Then, we show that the modal logic S5 can be formalized in
(TLQSR)^3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2416</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2416</id><created>2013-04-08</created><updated>2013-11-01</updated><authors><author><keyname>Chekuri</keyname><forenames>Chandra</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author></authors><title>Approximation algorithms for Euler genus and related problems</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Euler genus of a graph is a fundamental and well-studied parameter in
graph theory and topology. Computing it has been shown to be NP-hard by
[Thomassen '89 &amp; '93], and it is known to be fixed-parameter tractable.
However, the approximability of the Euler genus is wide open. While the
existence of an O(1)-approximation is not ruled out, only an
O(sqrt(n))-approximation [Chen, Kanchi, Kanevsky '97] is known even in bounded
degree graphs. In this paper we give a polynomial-time algorithm which on input
a bounded-degree graph of Euler genus g, computes a drawing into a surface of
Euler genus poly(g, log(n)). Combined with the upper bound from [Chen, Kanchi,
Kanevsky '97], our result also implies a O(n^(1/2 - alpha))-approximation, for
some constant alpha&gt;0.
  Using our algorithm for approximating the Euler genus as a subroutine, we
obtain, in a unified fashion, algorithms with approximation ratios of the form
poly(OPT, log(n)) for several related problems on bounded degree graphs. These
include the problems of orientable genus, crossing number, and planar edge and
vertex deletion problems. Our algorithm and proof of correctness for the
crossing number problem is simpler compared to the long and difficult proof in
the recent breakthrough by [Chuzhoy 2011], while essentially obtaining a
qualitatively similar result. For planar edge and vertex deletion problems our
results are the first to obtain a bound of form poly(OPT, log(n)).
  We also highlight some further applications of our results in the design of
algorithms for graphs with small genus. Many such algorithms require that a
drawing of the graph is given as part of the input. Our results imply that in
several interesting cases, we can implement such algorithms even when the
drawing is unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2418</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2418</id><created>2013-03-31</created><authors><author><keyname>Rezgui</keyname><forenames>Hanene</forenames></author><author><keyname>Sassi-Hidri</keyname><forenames>Minyar</forenames></author></authors><title>Mod\`ele flou d'expression des pr\'ef\'erences bas\'e sur les CP-Nets</title><categories>cs.AI</categories><comments>2 pages, EGC 2013</comments><journal-ref>13\`eme Conf\'erence Francophone sur l'Extraction et la Gestion
  des Connaissances (EGC), pp. 27-28, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses the problem of expressing preferences in flexible
queries while basing on a combination of the fuzzy logic theory and Conditional
Preference Networks or CP-Nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2444</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2444</id><created>2013-04-08</created><authors><author><keyname>Tyagi</keyname><forenames>Himanshu</forenames></author></authors><title>Common Information and Secret Key Capacity</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the generation of a secret key of maximum rate by a pair of
terminals observing correlated sources and with the means to communicate over a
noiseless public com- munication channel. Our main result establishes a
structural equivalence between the generation of a maximum rate secret key and
the generation of a common randomness that renders the observations of the two
terminals conditionally independent. The minimum rate of such common
randomness, termed interactive common information, is related to Wyner's notion
of common information, and serves to characterize the minimum rate of
interactive public communication required to generate an optimum rate secret
key. This characterization yields a single-letter expression for the
aforementioned communication rate when the number of rounds of interaction are
bounded. An application of our results shows that interaction does not reduce
this rate for binary symmetric sources. Further, we provide an example for
which interaction does reduce the minimum rate of communication. Also, certain
invariance properties of common information quantities are established that may
be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2454</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2454</id><created>2013-04-09</created><updated>2013-10-26</updated><authors><author><keyname>Bunn</keyname><forenames>Paul</forenames></author><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author></authors><title>Secure End-to-End Communication with Optimal Throughput in Unreliable
  Networks</title><categories>cs.NI</categories><comments>20 pages</comments><journal-ref>Distributed Computing, Lecture Notes in Computer Science Volume
  8205, pp. 403-417. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the feasibility of end-to-end communication in highly
unreliable networks. Modeling a network as a graph with vertices representing
nodes and edges representing the links between them, we consider two forms of
unreliability: unpredictable edge-failures, and deliberate deviation from
protocol specifications by corrupt nodes.
  We present a robust routing protocol for end-to-end communication that is
simultaneously resilient to both forms of unreliability. In particular, we
prove rigorously that our protocol is SECURE against the actions of the corrupt
nodes, achieves correctness (Receiver gets ALL of the messages from Sender, in
order and without modification), and enjoys provably optimal throughput
performance, as measured using competitive analysis.
  Furthermore, our protocol does not incur any asymptotic memory overhead as
compared to other protocols that are unable to handle malicious interference of
corrupt nodes. In particular, our protocol requires O(n^2) memory per
processor, where n is the size of the network. This represents an O(n^2)
improvement over all existing protocols that have been designed for this
network model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2467</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2467</id><created>2013-04-09</created><authors><author><keyname>Eftekhar</keyname><forenames>S. M. Ashik</forenames></author><author><keyname>Habib</keyname><forenames>Sk. Mahbub</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>Evolutionary Design of Digital Circuits Using Genetic Programming</title><categories>cs.NE</categories><journal-ref>Procs. of the 3rd International Conference on Electrical,
  Electronics and Computer Engineering (ICEECE 2003), pp. 231-236, Dhaka,
  Bangladesh, December 22-24, (2003)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For simple digital circuits, conventional method of designing circuits can
easily be applied. But for complex digital circuits, the conventional method of
designing circuits is not fruitfully applicable because it is time-consuming.
On the contrary, Genetic Programming is used mostly for automatic program
generation. The modern approach for designing Arithmetic circuits, commonly
digital circuits, is based on Graphs. This graph-based evolutionary design of
arithmetic circuits is a method of optimized designing of arithmetic circuits.
In this paper, a new technique for evolutionary design of digital circuits is
proposed using Genetic Programming (GP) with Subtree Mutation in place of
Graph-based design. The results obtained using this technique demonstrates the
potential capability of genetic programming in digital circuit design with
limited computer algorithms. The proposed technique, helps to simplify and
speed up the process of designing digital circuits, discovers a variation in
the field of digital circuit design where optimized digital circuits can be
successfully and effectively designed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2475</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2475</id><created>2013-04-09</created><authors><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author><author><keyname>Kader</keyname><forenames>Md. Abdul</forenames></author><author><keyname>Sayed</keyname><forenames>Md. Abu</forenames></author></authors><title>Design and Development of a Heart Rate Measuring Device using Fingertip</title><categories>cs.OH</categories><journal-ref>Procs. of the IEEE International Conference on Computer and
  Communication Engineering (ICCCE10), pp. 197-201, Kuala Lumpur, Malaysia, May
  11-13, (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we presented the design and development of a new integrated
device for measuring heart rate using fingertip to improve estimating the heart
rate. As heart related diseases are increasing day by day, the need for an
accurate and affordable heart rate measuring device or heart monitor is
essential to ensure quality of health. However, most heart rate measuring tools
and environments are expensive and do not follow ergonomics. Our proposed Heart
Rate Measuring (HRM) device is economical and user friendly and uses optical
technology to detect the flow of blood through index finger. Three phases are
used to detect pulses on the fingertip that include pulse detection, signal
extraction, and pulse amplification. Qualitative and quantitative performance
evaluation of the device on real signals shows accuracy in heart rate
estimation, even under intense of physical activity. We compared the
performance of HRM device with Electrocardiogram reports and manual pulse
measurement of heartbeat of 90 human subjects of different ages. The results
showed that the error rate of the device is negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2476</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2476</id><created>2013-04-09</created><authors><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author><author><keyname>Hossain</keyname><forenames>Afrina</forenames></author><author><keyname>Akter</keyname><forenames>Suraiya Rumana</forenames></author><author><keyname>Gope</keyname><forenames>Monika</forenames></author></authors><title>Corpus-based Web Document Summarization using Statistical and Linguistic
  Approach</title><categories>cs.IR cs.CL</categories><journal-ref>Procs. of the IEEE International Conference on Computer and
  Communication Engineering (ICCCE10), pp. 115-120, Kuala Lumpur, Malaysia, May
  11-13, (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single document summarization generates summary by extracting the
representative sentences from the document. In this paper, we presented a novel
technique for summarization of domain-specific text from a single web document
that uses statistical and linguistic analysis on the text in a reference corpus
and the web document. The proposed summarizer uses the combinational function
of Sentence Weight (SW) and Subject Weight (SuW) to determine the rank of a
sentence, where SW is the function of number of terms (t_n) and number of words
(w_n) in a sentence, and term frequency (t_f) in the corpus and SuW is the
function of t_n and w_n in a subject, and t_f in the corpus. 30 percent of the
ranked sentences are considered to be the summary of the web document. We
generated three web document summaries using our technique and compared each of
them with the summaries developed manually from 16 different human subjects.
Results showed that 68 percent of the summaries produced by our approach
satisfy the manual summaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2490</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2490</id><created>2013-04-09</created><authors><author><keyname>Xiao</keyname><forenames>Yanhui</forenames></author><author><keyname>Zhu</keyname><forenames>Zhenfeng</forenames></author><author><keyname>Zhao</keyname><forenames>Yao</forenames></author></authors><title>Kernel Reconstruction ICA for Sparse Representation</title><categories>cs.CV cs.LG</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independent Component Analysis (ICA) is an effective unsupervised tool to
learn statistically independent representation. However, ICA is not only
sensitive to whitening but also difficult to learn an over-complete basis.
Consequently, ICA with soft Reconstruction cost(RICA) was presented to learn
sparse representations with over-complete basis even on unwhitened data.
Whereas RICA is infeasible to represent the data with nonlinear structure due
to its intrinsic linearity. In addition, RICA is essentially an unsupervised
method and can not utilize the class information. In this paper, we propose a
kernel ICA model with reconstruction constraint (kRICA) to capture the
nonlinear features. To bring in the class information, we further extend the
unsupervised kRICA to a supervised one by introducing a discrimination
constraint, namely d-kRICA. This constraint leads to learn a structured basis
consisted of basis vectors from different basis subsets corresponding to
different class labels. Then each subset will sparsely represent well for its
own class but not for the others. Furthermore, data samples belonging to the
same class will have similar representations, and thereby the learned sparse
representations can take more discriminative power. Experimental results
validate the effectiveness of kRICA and d-kRICA for image classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2503</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2503</id><created>2013-04-09</created><authors><author><keyname>P&#xf6;chacker</keyname><forenames>Manfred</forenames></author><author><keyname>Sobe</keyname><forenames>Anita</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>Simulating the Smart Grid</title><categories>cs.SY</categories><comments>6 pages, 2 figures, 2 tables, PowerTech 2013 Conference in Grenoble</comments><doi>10.1109/PTC.2013.6652259</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Major challenges for the transition of power systems do not only tackle power
electronics but also communication technology, power market economy and user
acceptance studies. Simulation is an important research method therein, as it
helps to avoid costly failures. A common smart grid simulation platform is
still missing. We introduce a conceptual model of agents in multiple flow
networks. Flow networks extend the depth of established power flow analysis
through use of networks of information flow and financial transactions. We use
this model as a basis for comparing different power system simulators.
Furthermore, a quantitative comparison of simulators is done to facilitate the
decision for a suitable tool in comprehensive smart grid simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2504</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2504</id><created>2013-04-09</created><updated>2013-12-16</updated><authors><author><keyname>Pang</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Yang</forenames></author></authors><title>A New Access Control Scheme for Facebook-style Social Networks</title><categories>cs.CR cs.SI</categories><comments>20 pages</comments><acm-class>D.4.6; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popularity of online social networks (OSNs) makes the protection of
users' private information an important but scientifically challenging problem.
In the literature, relationship-based access control schemes have been proposed
to address this problem. However, with the dynamic developments of OSNs, we
identify new access control requirements which cannot be fully captured by the
current schemes. In this paper, we focus on public information in OSNs and
treat it as a new dimension which users can use to regulate access to their
resources. We define a new OSN model containing users and their relationships
as well as public information. Based on this model, we introduce a variant of
hybrid logic for formulating access control policies. A type of category
relations among public information are exploited to further improve our logic
for its usage in practice. In the end, we propose a few solutions to address
the problem of information reliability in OSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2514</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2514</id><created>2013-04-09</created><authors><author><keyname>Kamala</keyname><forenames>B.</forenames></author><author><keyname>Nandhini</keyname><forenames>J. M.</forenames></author></authors><title>Automatic Structuring Of Semantic Web Services An Approach</title><categories>cs.IR</categories><comments>ICRTCT, 2013 Coimbatore, International Journal of Computer and
  Communication Technology, Jan 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontologies have become the effective modeling for various applications and
significantly in the semantic web. The difficulty of extracting information
from the web, which was created mainly for visualising information, has driven
the birth of the semantic web, which will contain much more resources than the
web and will attach machine-readable semantic information to these resources.
Ontological bootstrapping on a set of predefined sources, such as web services,
must address the problem of multiple, largely unrelated concepts. The web
services consist of basically two components, Web Services Description Language
(WSDL) descriptors and free text descriptors. The WSDL descriptor is evaluated
using two methods, namely Term Frequency/Inverse Document Frequency (TF/IDF)
and web context generation. The proposed bootstrapping ontological process
integrates TF/IDF and web context generation and applies validation using the
free text descriptor service, so that, it offers more accurate definition of
ontologies. This paper uses ranking adaption model which predicts the rank for
a collection of web service documents which leads to the automatic
construction, enrichment and adaptation of ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2523</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2523</id><created>2013-04-09</created><updated>2014-06-06</updated><authors><author><keyname>Feng</keyname><forenames>Chen</forenames></author><author><keyname>N&#xf3;brega</keyname><forenames>Roberto W.</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author><author><keyname>Silva</keyname><forenames>Danilo</forenames></author></authors><title>Communication over Finite-Chain-Ring Matrix Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, April 2013.
  Revised version submitted in Feb. 2014. Final version submitted in June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though network coding is traditionally performed over finite fields, recent
work on nested-lattice-based network coding suggests that, by allowing network
coding over certain finite rings, more efficient physical-layer network coding
schemes can be constructed. This paper considers the problem of communication
over a finite-ring matrix channel $Y = AX + BE$, where $X$ is the channel
input, $Y$ is the channel output, $E$ is random error, and $A$ and $B$ are
random transfer matrices. Tight capacity results are obtained and simple
polynomial-complexity capacity-achieving coding schemes are provided under the
assumption that $A$ is uniform over all full-rank matrices and $BE$ is uniform
over all rank-$t$ matrices, extending the work of Silva, Kschischang and
K\&quot;{o}tter (2010), who handled the case of finite fields. This extension is
based on several new results, which may be of independent interest, that
generalize concepts and methods from matrices over finite fields to matrices
over finite chain rings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2524</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2524</id><created>2013-04-09</created><authors><author><keyname>Jenab</keyname><forenames>Seyyed Mehdi Hosseini</forenames></author><author><keyname>Nejati</keyname><forenames>Ammar</forenames></author></authors><title>Relative Positions of Countries in the World of Science</title><categories>physics.soc-ph cs.DL</categories><comments>20 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A novel picture of the relative positions of countries in the world of
science is offered through application of a two-dimensional mapping method
which is based on quantity and quality indicators of the scientific production
as peer-reviewed articles. To obtain such indicators, different influential
effects such as the background global trends, temporal fluctuations,
disciplinary characteristics, and mainly, the effect of countries resources
have been taken into account. Fifty countries with the highest scientific
production are studied in twelve years (1996-2007). A common clustering
algorithm is used to detect groups of co-evolving countries in the
two-dimensional map, and thereby countries are classified into four major
clusters based on their relative positions in the two-dimensional map. The
final results are in contrast with common views on relative positions of
countries in the world of science, as demonstrated by considering some examples
like USA, China or New Zealand. The proposed method and results thereof might
influence the concept of 'scientific advancement' and the future scientific
orientations of countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2528</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2528</id><created>2013-04-09</created><updated>2013-08-02</updated><authors><author><keyname>Fleurquin</keyname><forenames>Pablo</forenames></author><author><keyname>Ramasco</keyname><forenames>Jos&#xe9; J.</forenames></author><author><keyname>Egu&#xed;luz</keyname><forenames>Victor M.</forenames></author></authors><title>Characterization of delay propagation in the US air transportation
  network</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 10 figures. Submitted to Proceedings of the 2012 Air
  Transport Research Society (ATRS) World Conference</comments><journal-ref>Transportation Journal 53, 330-344 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks provide a suitable framework to characterize air traffic.
Previous works described the world air transport network as a graph where
direct flights are edges and commercial airports are vertices. In this work, we
focus instead on the properties of flight delays in the US air transportation
network. We analyze flight performance data in 2010 and study the topological
structure of the network as well as the aircraft rotation. The properties of
flight delays, including the distribution of total delays, the dependence on
the day of the week and the hour-by-hour evolution within each day, are
characterized paying special attention to flights accumulating delays longer
than 12 hours. We find that the distributions are robust to changes in takeoff
or landing operations, different moments of the year or even different airports
in the contiguous states. However, airports in remote areas (Hawaii, Alaska,
Puerto Rico) can show peculiar distributions biased toward long delays.
Additionally, we show that long delayed flights have an important dependence on
the destination airport.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2538</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2538</id><created>2013-04-09</created><authors><author><keyname>Hossain</keyname><forenames>K. M. Motahar</forenames></author><author><keyname>Raihan</keyname><forenames>Zahir</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>On Appropriate Selection of Fuzzy Aggregation Operators in Medical
  Decision Support System</title><categories>cs.AI cs.IR</categories><journal-ref>Procs. of the 8th International Conference on Computer &amp;
  Information Technology (ICCIT 2005), pp. 563-568, Dhaka, Bangladesh, December
  28-30, (2005)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Decision Support System (DSS) contains more than one antecedent and the
degrees of strength of the antecedents need to be combined to determine the
overall strength of the rule consequent. The membership values of the
linguistic variables in Fuzzy have to be combined using an aggregation
operator. But it is not feasible to predefine the form of aggregation operators
in decision making. Instead, each rule should be found based on the feeling of
the experts and on their actual decision pattern over the set of typical
examples. Thus this work illustrates how the choice of aggregation operators is
intended to mimic human decision making and can be selected and adjusted to fit
empirical data, a series of test cases. Both parametrized and nonparametrized
aggregation operators are adapted to fit empirical data. Moreover, they
provided compensatory properties and, therefore, seemed to produce a better
decision support system. To solve the problem, a threshold point from the
output of the aggregation operators is chosen as the separation point between
two classes. The best achieved accuracy is chosen as the appropriate
aggregation operator. Thus a medical decision can be generated which is very
close to a practitioner's guideline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2543</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2543</id><created>2013-04-09</created><authors><author><keyname>Islam</keyname><forenames>Md. Asadul</forenames></author><author><keyname>Mashrur-E-Elahi</keyname><forenames>G. M.</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>A New Distributed Evolutionary Computation Technique for Multi-Objective
  Optimization</title><categories>cs.NE</categories><journal-ref>Procs. of the IEEE International Conference on Computer and
  Communication Engineering (ICCCE10), pp. 31-36, Kuala Lumpur, Malaysia, May
  11-13, (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now-a-days, it is important to find out solutions of Multi-Objective
Optimization Problems (MOPs). Evolutionary Strategy helps to solve such real
world problems efficiently and quickly. But sequential Evolutionary Algorithms
(EAs) require an enormous computation power to solve such problems and it takes
much time to solve large problems. To enhance the performance for solving this
type of problems, this paper presents a new Distributed Novel Evolutionary
Strategy Algorithm (DNESA) for Multi-Objective Optimization. The proposed DNESA
applies the divide-and-conquer approach to decompose population into smaller
sub-population and involves multiple solutions in the form of cooperative
sub-populations. In DNESA, the server distributes the total computation load to
all associate clients and simulation results show that the time for solving
large problems is much less than sequential EAs. Also DNESA shows better
performance in convergence test when compared with other three well-known EAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2545</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2545</id><created>2013-04-09</created><authors><author><keyname>Jamali</keyname><forenames>A. R. M. Jalal Uddin</forenames></author><author><keyname>Hossain</keyname><forenames>Mohammad Arif</forenames></author><author><keyname>Moniruzzaman</keyname><forenames>G. M.</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>For Solving Linear Equations Recombination is a Needless Operation in
  Time-Variant Adaptive Hybrid Algorithms</title><categories>cs.NE cs.NA</categories><journal-ref>Procs. of the 8th International Conference on Computer &amp;
  Information Technology (ICCIT 2005), pp. 23-28, Dhaka, Bangladesh, December
  28-30, (2005)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently hybrid evolutionary computation (EC) techniques are successfully
implemented for solving large sets of linear equations. All the recently
developed hybrid evolutionary algorithms, for solving linear equations, contain
both the recombination and the mutation operations. In this paper, two modified
hybrid evolutionary algorithms contained time-variant adaptive evolutionary
technique are proposed for solving linear equations in which recombination
operation is absent. The effectiveness of the recombination operator has been
studied for the time-variant adaptive hybrid algorithms for solving large set
of linear equations. Several experiments have been carried out using both the
proposed modified hybrid evolutionary algorithms (in which the recombination
operation is absent) and corresponding existing hybrid algorithms (in which the
recombination operation is present) to solve large set of linear equations. It
is found that the number of generations required by the existing hybrid
algorithms (i.e. the Gauss-Seidel-SR based time variant adaptive (GSBTVA)
hybrid algorithm and the Jacobi-SR based time variant adaptive (JBTVA) hybrid
algorithm) and modified hybrid algorithms (i.e. the modified Gauss-Seidel-SR
based time variant adaptive (MGSBTVA) hybrid algorithm and the modified
Jacobi-SR based time variant adaptive (MJBTVA) hybrid algorithm) are
comparable. Also the proposed modified algorithms require less amount of
computational time in comparison to the corresponding existing hybrid
algorithms. As the proposed modified hybrid algorithms do not contain
recombination operation, so they require less computational effort, and also
they are more efficient, effective and easy to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2550</identifier>
 <datestamp>2013-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2550</id><created>2013-04-09</created><updated>2013-06-13</updated><authors><author><keyname>Hargreaves</keyname><forenames>Felix P.</forenames></author><author><keyname>Merkle</keyname><forenames>Daniel</forenames></author></authors><title>FooPar: A Functional Object Oriented Parallel Framework in Scala</title><categories>cs.DC cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present FooPar, an extension for highly efficient Parallel Computing in
the multi-paradigm programming language Scala. Scala offers concise and clean
syntax and integrates functional programming features. Our framework FooPar
combines these features with parallel computing techniques. FooPar is designed
modular and supports easy access to different communication backends for
distributed memory architectures as well as high performance math libraries. In
this article we use it to parallelize matrix matrix multiplication and show its
scalability by a isoefficiency analysis. In addition, results based on a
empirical analysis on two supercomputers are given. We achieve close-to-optimal
performance wrt. theoretical peak performance. Based on this result we conclude
that FooPar allows to fully access Scala's design features without suffering
from performance drops when compared to implementations purely based on C and
MPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2554</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2554</id><created>2013-04-09</created><updated>2014-03-17</updated><authors><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author></authors><title>Throughput Optimal Scheduling Policies in Networks of Interacting Queues</title><categories>cs.PF math.OC</categories><acm-class>C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report considers a fairly general model of constrained queuing networks
that allows us to represent both MMBP (Markov Modulated Bernoulli Processes)
arrivals and time-varying service constraints. We derive a set of sufficient
conditions for throughput optimality of scheduling policies that encompass and
generalize all the previously obtained results in the field. This leads to the
definition of new classes of (non diagonal) throughput optimal scheduling
policies. We prove the stability of queues by extending the traditional
Lyapunov drift criteria methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2557</identifier>
 <datestamp>2013-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2557</id><created>2013-04-09</created><updated>2013-09-12</updated><authors><author><keyname>de Keijzer</keyname><forenames>Bart</forenames></author><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>The H-index can be easily manipulated</title><categories>cs.CC</categories><comments>7 pages</comments><journal-ref>Bulletin of EATCS No 110, pp. 79-85, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove two complexity results about the H-index concerned with the Google
scholar merge operation on one's scientific articles. The results show that,
although it is hard to merge one's articles in an optimal way, it is easy to
merge them in such a way that one's H-index increases. This suggests the need
for an alternative scientific performance measure that is resistant to this
type of manipulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2574</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2574</id><created>2013-04-09</created><authors><author><keyname>Sunny</keyname><forenames>Albert</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>An Analysis on the Inter-Cell Station Dependency Probability in an IEEE
  802.11 Infrastructure WLANs</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document, we are primarily interested in computing the probabilities
of various types of dependencies that can occur in a multi-cell infrastructure
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2576</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2576</id><created>2013-04-09</created><updated>2013-04-24</updated><authors><author><keyname>Zhu</keyname><forenames>Andy Diwen</forenames></author><author><keyname>Ma</keyname><forenames>Hui</forenames></author><author><keyname>Xiao</keyname><forenames>Xiaokui</forenames></author><author><keyname>Luo</keyname><forenames>Siqiang</forenames></author><author><keyname>Tang</keyname><forenames>Youze</forenames></author><author><keyname>Zhou</keyname><forenames>Shuigeng</forenames></author></authors><title>Shortest Path and Distance Queries on Road Networks: Towards Bridging
  Theory and Practice</title><categories>cs.DS cs.DB</categories><comments>to appear in SIGMOD 2013. Table 1 updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two locations $s$ and $t$ in a road network, a distance query returns
the minimum network distance from $s$ to $t$, while a shortest path query
computes the actual route that achieves the minimum distance. These two types
of queries find important applications in practice, and a plethora of solutions
have been proposed in past few decades. The existing solutions, however, are
optimized for either practical or asymptotic performance, but not both. In
particular, the techniques with enhanced practical efficiency are mostly
heuristic-based, and they offer unattractive worst-case guarantees in terms of
space and time. On the other hand, the methods that are worst-case efficient
often entail prohibitive preprocessing or space overheads, which render them
inapplicable for the large road networks (with millions of nodes) commonly used
in modern map applications.
  This paper presents {\em Arterial Hierarchy (AH)}, an index structure that
narrows the gap between theory and practice in answering shortest path and
distance queries on road networks. On the theoretical side, we show that, under
a realistic assumption, AH answers any distance query in $\tilde{O}(\log \r)$
time, where $\r = d_{max}/d_{min}$, and $d_{max}$ (resp.\ $d_{min}$) is the
largest (resp.\ smallest) $L_\infty$ distance between any two nodes in the road
network. In addition, any shortest path query can be answered in $\tilde{O}(k +
\log \r)$ time, where $k$ is the number of nodes on the shortest path. On the
practical side, we experimentally evaluate AH on a large set of real road
networks with up to twenty million nodes, and we demonstrate that (i) AH
outperforms the state of the art in terms of query time, and (ii) its space and
pre-computation overheads are moderate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2580</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2580</id><created>2013-04-09</created><authors><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Frolik</keyname><forenames>Jeff</forenames></author></authors><title>Active Consensus over Sensor Networks via Randomized Communication</title><categories>cs.NI</categories><comments>8 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed consensus has been widely studied for sensor network
applications. Whereas the asymptotic convergence rate has been extensively
explored in prior work, other important and practical issues, including energy
efficiency and link reliability, have received relatively little attention. In
this paper, we present a distributed consensus approach that can achieve a good
balance between convergence rate and energy efficiency.
  The approach selects a subset of links that significantly contribute to the
formation of consensus at each iteration, thus adapting the network's topology
dynamically to the changes of the sensor states. A global optimization problem
is formulated for optimal link selection, which is subsequently factorized into
sub-problems that can be solved locally, and practically via approximation. An
algorithm is derived to solve the approximation efficiently, using quadratic
programming (QP) relaxation and random sampling. Simulations on networks of
different types demonstrate that the proposed method reduces the communication
energy costs without significantly impacting the convergence rate and that the
approach is robust to link failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2581</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2581</id><created>2013-04-09</created><updated>2013-04-19</updated><authors><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Stability and performance of stochastic predictive control</title><categories>cs.SY math.OC</categories><comments>19 pages. Minor corrections and updated references</comments><doi>10.1109/TAC.2014.2335274</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is concerned with stability and performance of controlled
stochastic processes under receding horizon policies. We carry out a systematic
study of methods to guarantee stability under receding horizon policies via
appropriate selections of cost functions in the underlying finite-horizon
optimal control problem. We also obtain quantitative bounds on the performance
of the system under receding horizon policies as measured by the long-run
expected average cost. The results are illustrated with the help of several
simple examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2604</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2604</id><created>2013-04-09</created><authors><author><keyname>Souviron</keyname><forenames>Jean</forenames></author></authors><title>On the predictability of the number of convex vertices</title><categories>cs.CG</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex hulls are a fundamental geometric tool used in a number of algorithms.
As a side-effect of exhaustive tests for an algorithm for which a convex hull
computation was the first step, interesting experimental results were found and
are the sunject of this paper. They establish that the number of convex
vertices of natural datasets can be predicted, if not precisely at least within
a defined range. Namely it was found that the number of convex vertices of a
dataset of N points lies in the range 2.35 N^0.091 &lt;= h &lt;= 19.19 N^0.091. This
range obviously does not describe neither natural nor artificial worst-cases
but corresponds to the distributions of natural data. This can be used for
instance to define a starting size for pre-allocated arrays or to evaluate
output-sensitive algorithms. A further consequence of these results is that the
random models of data used to test convex hull algorithms should be bounded by
rectangles and not as they usually are by circles if they want to represent
accurately natural datasets
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2610</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2610</id><created>2013-02-27</created><authors><author><keyname>Herrouz</keyname><forenames>Abdelhakim</forenames></author><author><keyname>Khentout</keyname><forenames>Chabane</forenames></author><author><keyname>Djoudi</keyname><forenames>Mahieddine</forenames></author></authors><title>Overview of Visualization Tools for Web Browser History Data</title><categories>cs.HC</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol.9,
  Issue 6, No3, November 2012, pp. 92-98, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the Web has become one of the most widespread platforms for
information change and retrieval. As it becomes easier to publish documents, as
the number of users, and thus publishers, increases and as the number of
documents grows, searching for information is turning into a cumbersome and
time-consuming operation. Because of the loose interconnection between
documents, people have difficulty remembering where they have been and
returning to previously visited pages. Navigation through the web faces
problems of locating oneself with respect to space and time. The idea of
graphical assistance navigation is to help users to find their paths in
hyperspace by adapting the style of link presentation to the goals, knowledge
and other characteristics of an individual user. We first introduce the
concepts related to web navigation; we then present an overview of different
graphical navigation tools and techniques. We conclude by presenting a
comparative table of these tools based on some pertinent criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2612</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2612</id><created>2013-04-09</created><updated>2014-01-07</updated><authors><author><keyname>Zhang</keyname><forenames>Leo Yu</forenames></author><author><keyname>Hu</keyname><forenames>Xiaobo</forenames></author><author><keyname>Liu</keyname><forenames>Yuansheng</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-Wo</forenames></author></authors><title>A chaotic image encryption scheme owning temp-value feedback</title><categories>cs.CR</categories><comments>10 pages, 4 figures</comments><doi>10.1016/j.cnsns.2014.03.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel efficient chaotic image encryption scheme, in
which the temp-value feedback mechanism is introduced to the permutation and
diffusion procedures. Firstly, a simple trick is played to map the plain-image
pixels to the initial condition of the Logistic map. Then, a pseudorandom
number sequence (PRNS) is obtained from iterating the map. The permutation
procedure is carried out by a permutation sequence which is generated by
comparing the PRNS and its sorted version. The diffusion procedure is composed
of two reversely executed rounds. During each round, the current plain-image
pixel and the last cipher-image pixel are used to produce the current
cipher-image pixel with the help of the Logistic map and a pseudorandom number
generated by the Chen system. To enhance the efficiency, only expanded XOR
operation and modulo 256 addition are employed during diffusion. Experimental
results show that the new scheme owns a large key space and can resist the
differential attack. It is also efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2617</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2617</id><created>2013-04-09</created><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author></authors><title>Resilience of Dynamic Overlays through Local Interactions</title><categories>cs.DC</categories><comments>A version of this paper is published in the Proceedings of 5th
  International Workshop on Simplifying Complex Networks for Pratictioners
  (SIMPLEX 2013) - World Wide Web Conference (WWW 2013), ACM, Rio de Janeiro
  (Brazil), May 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a self-organizing protocol for dynamic (unstructured P2P)
overlay networks, which allows to react to the variability of node arrivals and
departures. Through local interactions, the protocol avoids that the departure
of nodes causes a partitioning of the overlay. We show that it is sufficient to
have knowledge about 1st and 2nd neighbours, plus a simple interaction P2P
protocol, to make unstructured networks resilient to node faults. A simulation
assessment over different kinds of overlay networks demonstrates the viability
of the proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2618</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2618</id><created>2013-04-09</created><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author></authors><title>Lexicographic identifying codes</title><categories>math.CO cs.DM cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An identifying code in a graph is a set of vertices which intersects all the
symmetric differences between pairs of neighbourhoods of vertices. Not all
graphs have identifying codes; those that do are referred to as twin-free. In
this paper, we design an algorithm that finds an identifying code in a
twin-free graph on n vertices in O(n^3) binary operations, and returns a
failure if the graph is not twin-free. We also determine an alternative for
sparse graphs with a running time of O(n^2d log n) binary operations, where d
is the maximum degree. We also prove that these algorithms can return any
identifying code with minimum cardinality, provided the vertices are correctly
sorted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2637</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2637</id><created>2013-04-09</created><updated>2013-06-19</updated><authors><author><keyname>Reutter</keyname><forenames>Juan L.</forenames></author></authors><title>Containment of Nested Regular Expressions</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nested regular expressions (NREs) have been proposed as a powerful formalism
for querying RDFS graphs, but research in a more general graph database context
has been scarce, and static analysis results are currently lacking. In this
paper we investigate the problem of containment of NREs, and show that it can
be solved in PSPACE, i.e., the same complexity as the problem of containment of
regular expressions or regular path queries (RPQs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2639</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2639</id><created>2013-04-01</created><authors><author><keyname>Fremont</keyname><forenames>Daniel</forenames></author></authors><title>The Reachability Problem for Affine Functions on the Integers</title><categories>cs.FL cs.DM</categories><comments>13 pages</comments><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of determining, given x, y in Z^k and a finite set F
of affine functions on Z^k, whether y is reachable from x by applying the
functions F. We also consider the analogous problem over N^k. These problems
are known to be undecidable for k &gt;= 2. We give 2-EXPTIME algorithms for both
problems in the remaining case k = 1. The exact complexities remain open,
although we show a simple NP lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2641</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2641</id><created>2013-04-09</created><authors><author><keyname>Jin</keyname><forenames>Yan</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author><author><keyname>Hamiez</keyname><forenames>Jean-Philippe</forenames></author></authors><title>A memetic algorithm for the minimum sum coloring problem</title><categories>cs.DM cs.DS</categories><comments>Submitted manuscript</comments><journal-ref>Computers &amp; Operations Research 43(3): 318-327, 2014</journal-ref><doi>10.1016/j.cor.2013.09.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph $G$, the Minimum Sum Coloring problem (MSCP) is to
find a legal assignment of colors (represented by natural numbers) to each
vertex of $G$ such that the total sum of the colors assigned to the vertices is
minimized. This paper presents a memetic algorithm for MSCP based on a tabu
search procedure with two neighborhoods and a multi-parent crossover operator.
Experiments on a set of 77 well-known DIMACS and COLOR 2002-2004 benchmark
instances show that the proposed algorithm achieves highly competitive results
in comparison with five state-of-the-art algorithms. In particular, the
proposed algorithm can improve the best known results for 17 instances. We also
provide upper bounds for 18 additional instances for the first time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2664</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2664</id><created>2013-04-09</created><authors><author><keyname>Sun</keyname><forenames>Qiyu</forenames></author></authors><title>Localized nonlinear functional equations and two sampling problems in
  signal processing</title><categories>math.FA cs.IT math.IT math.NA math.OA</categories><msc-class>47H07, 47J05, 65J15, 94A20, 94A12, 42C15, 46H99, 46T20, 47H05, 39B42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $1\le p\le \infty$. In this paper, we consider solving a nonlinear
functional equation $$f(x)=y,$$ where $x, y$ belong to $\ell^p$ and $f$ has
continuous bounded gradient in an inverse-closed subalgebra of ${\mathcal
B}(\ell^2)$, the Banach algebra of all bounded linear operators on the Hilbert
space $\ell^2$. We introduce strict monotonicity property for functions $f$ on
Banach spaces $\ell^p$ so that the above nonlinear functional equation is
solvable and the solution $x$ depends continuously on the given data $y$ in
$\ell^p$. We show that the Van-Cittert iteration converges in $\ell^p$ with
exponential rate and hence it could be used to locate the true solution of the
above nonlinear functional equation. We apply the above theory to handle two
problems in signal processing: nonlinear sampling termed with instantaneous
companding and subsequently average sampling; and local identification of
innovation positions and qualification of amplitudes of signals with finite
rate of innovation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2671</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2671</id><created>2013-04-09</created><authors><author><keyname>Gomes</keyname><forenames>Jorge</forenames></author><author><keyname>Silva</keyname><forenames>Fernando</forenames></author><author><keyname>Chambel</keyname><forenames>Teresa</forenames></author></authors><title>Genetic Soundtracks: Creative Matching of Audio to Video</title><categories>cs.MM</categories><comments>Artech'2012 Crossing Digital Boundaries, 6th International Conference
  on Digital Arts. Faro, Portugal. Nov 7-9, 2012</comments><journal-ref>6th International Conference on Digital Arts (Artech), pp.349-352,
  2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The matching of the soundtrack in a movie or a video can have an enormous
influence in the message being conveyed and its impact, in the sense of
involvement and engagement, and ultimately in their aesthetic and entertainment
qualities. Art is often associated with creativity, implying the presence of
inspiration, originality and appropriateness. Evolutionary systems provides us
with the novelty, showing us new and subtly different solutions in every
generation, possibly stimulating the creativity of the human using the system.
In this paper, we present Genetic Soundtracks, an evolutionary approach to the
creative matching of audio to a video. It analyzes both media to extract
features based on their content, and adopts genetic algorithms, with the
purpose of truncating, combining and adjusting audio clips, to align and match
them with the video scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2676</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2676</id><created>2013-04-09</created><authors><author><keyname>Souviron</keyname><forenames>Jean</forenames></author></authors><title>Convex hull: Incremental variations on the Akl-Toussaint heuristics
  Simple, optimal and space-saving convex hull algorithms</title><categories>cs.CG</categories><comments>15 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex hulls are a fundamental geometric tool used in a number of algorithms.
A famous paper by Akl and Toussaint in 1978 described a way to reduce the
number of points involved in the computation, which is since known as the
Akl-Toussaint heuristics. This paper first studies what this heurstics really
represents in terms of reduction of points and demonstrates that the optimum
selection is reached using an octogon as the remaining number of points is in
O(sqrt(N)) rather than the usual O(N). Then it focuses on optimising the
overall computational efficiency in a convex hull computation. Although the
heuristics is usually used as a first step in computations one can obtain the
convex hull directly from the heuristics's basis. First a simple incremental
implementation is described, and if the number of characteristic points of the
Akl-Toussaint heuristics p is taken as a parametre the convex hull is then
computed in a O(N(p+h/p)) average complexity or O(Nh) asymptotic complexity.
Given the relative constant factor of 1/p however experimental results show
that this algorithm should be considered linear in average. Worst-case
complexity is in O(N^2) and space complexity is O(h) but could be O(1) if the
required output is the array of convex vertices's indexes. Then a remark on why
the basic incremental method should be preferred for average cases is made.
Finally an optimal linear algorithm both in average and worst-case and using a
minimal space complexity in O(sqrt(N)) in average (or O(1) if in-place
computation is allowed) is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2681</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2681</id><created>2013-04-09</created><authors><author><keyname>Fried</keyname><forenames>Daniel</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author></authors><title>Maps of Computer Science</title><categories>cs.IR cs.DL physics.soc-ph</categories><comments>10 pages, 8 figures, live version and source code available at
  mocs.cs.arizona.edu</comments><acm-class>H.3.3; G.2.2; I.5.4; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a practical approach for visual exploration of research papers.
Specifically, we use the titles of papers from the DBLP database to create what
we call maps of computer science (MoCS). Words and phrases from the paper
titles are the cities in the map, and countries are created based on word and
phrase similarity, calculated using co-occurrence. With the help of heatmaps,
we can visualize the profile of a particular conference or journal over the
base map. Similarly, heatmap profiles can be made of individual researchers or
groups such as a department. The visualization system also makes it possible to
change the data used to generate the base map. For example, a specific journal
or conference can be used to generate the base map and then the heatmap
overlays can be used to show the evolution of research topics in the field over
the years. As before, individual researchers or research groups profiles can be
visualized using heatmap overlays but this time over the journal or conference
base map. Finally, research papers or abstracts easily generate visual
abstracts giving a visual representation of the distribution of topics in the
paper. We outline a modular and extensible system for term extraction using
natural language processing techniques, and show the applicability of methods
of information retrieval to calculation of term similarity and creation of a
topic map. The system is available at mocs.cs.arizona.edu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2683</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2683</id><created>2013-04-09</created><authors><author><keyname>Nan</keyname><forenames>Yao</forenames></author><author><keyname>Feng</keyname><forenames>Qian</forenames></author><author><keyname>Zuolei</keyname><forenames>Sun</forenames></author></authors><title>Image Classification by Feature Dimension Reduction and Graph based
  Ranking</title><categories>cs.CV</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimensionality reduction (DR) of image features plays an important role in
image retrieval and classification tasks. Recently, two types of methods have
been proposed to improve the both the accuracy and efficiency for the
dimensionality reduction problem. One uses Non-negative matrix factorization
(NMF) to describe the image distribution on the space of base matrix. Another
one for dimension reduction trains a subspace projection matrix to project
original data space into some low-dimensional subspaces which have deep
architecture, so that the low-dimensional codes would be learned. At the same
time, the graph based similarity learning algorithm which tries to exploit
contextual information for improving the effectiveness of image rankings is
also proposed for image class and retrieval problem. In this paper, after above
two methods mentioned are utilized to reduce the high-dimensional features of
images respectively, we learn the graph based similarity for the image
classification problem. This paper compares the proposed approach with other
approaches on an image database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2688</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2688</id><created>2013-04-09</created><authors><author><keyname>Ghaderi</keyname><forenames>Majid</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Orda</keyname><forenames>Ariel</forenames></author><author><keyname>Dehghan</keyname><forenames>Mostafa</forenames></author></authors><title>Efficient Wireless Security Through Jamming, Coding and Routing</title><categories>cs.NI cs.CR cs.IT math.IT</categories><report-no>CPSC Technical Report 2012-1027-10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a rich recent literature on how to assist secure communication
between a single transmitter and receiver at the physical layer of wireless
networks through techniques such as cooperative jamming. In this paper, we
consider how these single-hop physical layer security techniques can be
extended to multi-hop wireless networks and show how to augment physical layer
security techniques with higher layer network mechanisms such as coding and
routing. Specifically, we consider the secure minimum energy routing problem,
in which the objective is to compute a minimum energy path between two network
nodes subject to constraints on the end-to-end communication secrecy and
goodput over the path. This problem is formulated as a constrained optimization
of transmission power and link selection, which is proved to be NP-hard.
Nevertheless, we show that efficient algorithms exist to compute both exact and
approximate solutions for the problem. In particular, we develop an exact
solution of pseudo-polynomial complexity, as well as an epsilon-optimal
approximation of polynomial complexity. Simulation results are also provided to
show the utility of our algorithms and quantify their energy savings compared
to a combination of (standard) security-agnostic minimum energy routing and
physical layer security. In the simulated scenarios, we observe that, by
jointly optimizing link selection at the network layer and cooperative jamming
at the physical layer, our algorithms reduce the network energy consumption by
half.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2694</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2694</id><created>2013-04-09</created><authors><author><keyname>Niepert</keyname><forenames>Mathias</forenames></author></authors><title>Symmetry-Aware Marginal Density Estimation</title><categories>cs.AI</categories><comments>To appear in proceedings of AAAI 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rao-Blackwell theorem is utilized to analyze and improve the scalability
of inference in large probabilistic models that exhibit symmetries. A novel
marginal density estimator is introduced and shown both analytically and
empirically to outperform standard estimators by several orders of magnitude.
The developed theory and algorithms apply to a broad class of probabilistic
models including statistical relational models considered not susceptible to
lifted probabilistic inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2695</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2695</id><created>2013-04-09</created><authors><author><keyname>Cie&#x15b;li&#x144;ski</keyname><forenames>Jan L.</forenames></author></authors><title>Locally exact modifications of numerical schemes</title><categories>math.NA cs.NA</categories><comments>28 pages plus 6 figures, Computers &amp; Mathematics with Applications,
  2013. arXiv admin note: substantial text overlap with arXiv:1101.0578</comments><msc-class>65P10, 65L12, 34K28</msc-class><acm-class>G.1.7</acm-class><journal-ref>Computers &amp; Mathematics with Applications 65 (2013) 1920-1938</journal-ref><doi>10.1016/j.camwa.2013.04.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new class of exponential integrators for ordinary differential
equations: locally exact modifications of known numerical schemes. Local
exactness means that they preserve the linearization of the original system at
every point. In particular, locally exact integrators preserve all fixed points
and are A-stable. We apply this approach to popular schemes including Euler
schemes, implicit midpoint rule and trapezoidal rule. We found locally exact
modifications of discrete gradient schemes (for symmetric discrete gradients
and coordinate increment discrete gradients) preserving their main geometric
property: exact conservation of the energy integral (for arbitrary
multidimensional Hamiltonian systems in canonical coordinates). Numerical
experiments for a 2-dimensional anharmonic oscillator show that locally exact
schemes have very good accuracy in the neighbourhood of stable equilibrium,
much higher than suggested by the order of new schemes (locally exact
modification sometimes increases the order but in many cases leaves it
unchanged).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2698</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2698</id><created>2013-04-09</created><authors><author><keyname>Nejati</keyname><forenames>Ammar</forenames></author><author><keyname>Jenab</keyname><forenames>Seyyed Mehdi Hosseini</forenames></author></authors><title>A Two-Dimensional Approach to Evaluate the Scientific Production of
  Countries (Case Study: The Basic Sciences)</title><categories>physics.soc-ph cs.DL</categories><comments>6 pages, 5 figures</comments><journal-ref>Scientometrics, vol. 84, iss. 2 (2010): pp. 357-364</journal-ref><doi>10.1007/s11192-009-0103-1</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The quantity and quality of scientific output of the topmost 50 countries in
the four basic sciences (agricultural and biological sciences, chemistry,
mathematics, and physics and astronomy) are studied in the period of the recent
12 years (1996-2007). In order to rank the countries, a novel two-dimensional
method is proposed, which is inspired by the H-index and other methods based on
quality and quantity measures. The countries data are represented in a
&quot;quantity-quality diagram&quot;, and partitioned by a conventional statistical
algorithm (k-means), into three clusters, members of which are rather the same
in all of the basic sciences. The results offer a new perspective on the global
positions of countries with regards to their scientific output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2707</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2707</id><created>2013-04-09</created><authors><author><keyname>Ciuonzo</keyname><forenames>D.</forenames></author><author><keyname>Willett</keyname><forenames>P. K.</forenames></author><author><keyname>Bar-Shalom</keyname><forenames>Y.</forenames></author></authors><title>Tracking the Tracker from its Passive Sonar ML-PDA Estimates</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Aerospace and Electronic Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Target motion analysis with wideband passive sonar has received much
attention. Maximum likelihood probabilistic data-association (ML-PDA)
represents an asymptotically efficient estimator for deterministic target
motion, and is especially well-suited for low-observable targets; the results
presented here apply to situations with higher signal to noise ratio as well,
including of course the situation of a deterministic target observed via clean
measurements without false alarms or missed detections. Here we study the
inverse problem, namely, how to identify the observing platform (following a
two-leg motion model) from the results of the target estimation process, i.e.
the estimated target state and the Fisher information matrix, quantities we
assume an eavesdropper might intercept. We tackle the problem and we present
observability properties, with supporting simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2711</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2711</id><created>2013-03-27</created><authors><author><keyname>Black</keyname><forenames>Paul K.</forenames></author></authors><title>Is Shafer General Bayes?</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-2-9</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the relationship between Shafer's belief functions and
convex sets of probability distributions. Kyburg's (1986) result showed that
belief function models form a subset of the class of closed convex probability
distributions. This paper emphasizes the importance of Kyburg's result by
looking at simple examples involving Bernoulli trials. Furthermore, it is shown
that many convex sets of probability distributions generate the same belief
function in the sense that they support the same lower and upper values. This
has implications for a decision theoretic extension. Dempster's rule of
combination is also compared with Bayes' rule of conditioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2712</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2712</id><created>2013-03-27</created><authors><author><keyname>Cohen</keyname><forenames>Paul</forenames></author><author><keyname>Shafer</keyname><forenames>Glenn</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>Modifiable Combining Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-10-21</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modifiable combining functions are a synthesis of two common approaches to
combining evidence. They offer many of the advantages of these approaches and
avoid some disadvantages. Because they facilitate the acquisition,
representation, explanation, and modification of knowledge about combinations
of evidence, they are proposed as a tool for knowledge engineers who build
systems that reason under uncertainty, not as a normative theory of evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2713</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2713</id><created>2013-03-27</created><authors><author><keyname>Hunter</keyname><forenames>Daniel</forenames></author></authors><title>Dempster-Shafer vs. Probabilistic Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-22-29</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The combination of evidence in Dempster-Shafer theory is compared with the
combination of evidence in probabilistic logic. Sufficient conditions are
stated for these two methods to agree. It is then shown that these conditions
are minimal in the sense that disagreement can occur when any one of them is
removed. An example is given in which the traditional assumption of conditional
independence of evidence on hypotheses holds and a uniform prior is assumed,
but probabilistic logic and Dempster's rule give radically different results
for the combination of two evidence events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2714</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2714</id><created>2013-03-27</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Higher Order Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-30-38</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of writers have supposed that for the full specification of belief,
higher order probabilities are required. Some have even supposed that there may
be an unending sequence of higher order probabilities of probabilities of
probabilities.... In the present paper we show that higher order probabilities
can always be replaced by the marginal distributions of joint probability
distributions. We consider both the case in which higher order probabilities
are of the same sort as lower order probabilities and that in which higher
order probabilities are distinct in character, as when lower order
probabilities are construed as frequencies and higher order probabilities are
construed as subjective degrees of belief. In neither case do higher order
probabilities appear to offer any advantages, either conceptually or
computationally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2715</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2715</id><created>2013-03-27</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>Belief in Belief Functions: An Examination of Shafer's Canonical
  Examples</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-39-46</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the canonical examples underlying Shafer-Dempster theory, beliefs over the
hypotheses of interest are derived from a probability model for a set of
auxiliary hypotheses. Beliefs are derived via a compatibility relation
connecting the auxiliary hypotheses to subsets of the primary hypotheses. A
belief function differs from a Bayesian probability model in that one does not
condition on those parts of the evidence for which no probabilities are
specified. The significance of this difference in conditioning assumptions is
illustrated with two examples giving rise to identical belief functions but
different Bayesian probability distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2716</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2716</id><created>2013-03-27</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Do We Need Higher-Order Probabilities and, If So, What Do They Mean?</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-47-60</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The apparent failure of individual probabilistic expressions to distinguish
uncertainty about truths from uncertainty about probabilistic assessments have
prompted researchers to seek formalisms where the two types of uncertainties
are given notational distinction. This paper demonstrates that the desired
distinction is already a built-in feature of classical probabilistic models,
thus, specialized notations are unnecessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2717</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2717</id><created>2013-03-27</created><authors><author><keyname>Self</keyname><forenames>Matthew</forenames></author><author><keyname>Cheeseman</keyname><forenames>Peter</forenames></author></authors><title>Bayesian Prediction for Artificial Intelligence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-61-69</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the common method used for making predictions under
uncertainty in A1 and science is in error. This method is to use currently
available data to select the best model from a given class of models-this
process is called abduction-and then to use this model to make predictions
about future data. The correct method requires averaging over all the models to
make a prediction-we call this method transduction. Using transduction, an AI
system will not give misleading results when basing predictions on small
amounts of data, when no model is clearly best. For common classes of models we
show that the optimal solution can be given in closed form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2718</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2718</id><created>2013-03-27</created><authors><author><keyname>Yen</keyname><forenames>John</forenames></author></authors><title>Can Evidence Be Combined in the Dempster-Shafer Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-70-76</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dempster's rule of combination has been the most controversial part of the
Dempster-Shafer (D-S) theory. In particular, Zadeh has reached a conjecture on
the noncombinability of evidence from a relational model of the D-S theory. In
this paper, we will describe another relational model where D-S masses are
represented as conditional granular distributions. By comparing it with Zadeh's
relational model, we will show how Zadeh's conjecture on combinability does not
affect the applicability of Dempster's rule in our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2719</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2719</id><created>2013-03-27</created><authors><author><keyname>Bacon</keyname><forenames>John B.</forenames></author></authors><title>An Interesting Uncertainty-Based Combinatoric Problem in Spare Parts
  Forecasting: The FRED System</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-78-85</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The domain of spare parts forecasting is examined, and is found to present
unique uncertainty based problems in the architectural design of a
knowledge-based system. A mixture of different uncertainty paradigms is
required for the solution, with an intriguing combinatoric problem arising from
an uncertain choice of inference engines. Thus, uncertainty in the system is
manifested in two different meta-levels. The different uncertainty paradigms
and meta-levels must be integrated into a functioning whole. FRED is an example
of a difficult real-world domain to which no existing uncertainty approach is
completely appropriate. This paper discusses the architecture of FRED,
highlighting: the points of uncertainty and other interesting features of the
domain, the specific implications of those features on the system design
(including the combinatoric explosions), their current implementation &amp; future
plans,and other problems and issues with the architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2720</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2720</id><created>2013-03-27</created><authors><author><keyname>Binford</keyname><forenames>Thomas O.</forenames></author><author><keyname>Levitt</keyname><forenames>Tod S.</forenames></author><author><keyname>Mann</keyname><forenames>Wallace B.</forenames></author></authors><title>Bayesian Inference in Model-Based Machine Vision</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-86-97</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a preliminary version of visual interpretation integrating multiple
sensors in SUCCESSOR, an intelligent, model-based vision system. We pursue a
thorough integration of hierarchical Bayesian inference with comprehensive
physical representation of objects and their relations in a system for
reasoning with geometry, surface materials and sensor models in machine vision.
Bayesian inference provides a framework for accruing_ probabilities to rank
order hypotheses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2721</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2721</id><created>2013-03-27</created><authors><author><keyname>Biswas</keyname><forenames>Gautam</forenames></author><author><keyname>Anand</keyname><forenames>Teywansh S.</forenames></author></authors><title>Using the Dempster-Shafer Scheme in a Diagnostic Expert System Shell</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-98-105</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses an expert system shell that integrates rule-based
reasoning and the Dempster-Shafer evidence combination scheme. Domain knowledge
is stored as rules with associated belief functions. The reasoning component
uses a combination of forward and backward inferencing mechanisms to allow
interaction with users in a mixed-initiative format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2722</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2722</id><created>2013-03-27</created><authors><author><keyname>Chin</keyname><forenames>Homer L.</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>Stochastic Simulation of Bayesian Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-106-113</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines Bayesian belief network inference using simulation as a
method for computing the posterior probabilities of network variables.
Specifically, it examines the use of a method described by Henrion, called
logic sampling, and a method described by Pearl, called stochastic simulation.
We first review the conditions under which logic sampling is computationally
infeasible. Such cases motivated the development of the Pearl's stochastic
simulation algorithm. We have found that this stochastic simulation algorithm,
when applied to certain networks, leads to much slower than expected
convergence to the true posterior probabilities. This behavior is a result of
the tendency for local areas in the network to become fixed through many
simulation cycles. The time required to obtain significant convergence can be
made arbitrarily long by strengthening the probabilistic dependency between
nodes. We propose the use of several forms of graph modification, such as graph
pruning, arc reversal, and node reduction, in order to convert some networks
into formats that are computationally more efficient for simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2723</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2723</id><created>2013-03-27</created><authors><author><keyname>Hanks</keyname><forenames>Steve</forenames></author></authors><title>Temporal Reasoning About Uncertain Worlds</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-114-122</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a program that manages a database of temporally scoped beliefs.
The basic functionality of the system includes maintaining a network of
constraints among time points, supporting a variety of fetches, mediating the
application of causal rules, monitoring intervals of time for the addition of
new facts, and managing data dependencies that keep the database consistent. At
this level the system operates independent of any measure of belief or belief
calculus. We provide an example of how an application program mi9ght use this
functionality to implement a belief calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2724</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2724</id><created>2013-03-27</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Jimison</keyname><forenames>Holly B.</forenames></author></authors><title>A Perspective on Confidence and Its Use in Focusing Attention During
  Knowledge Acquisition</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-123-131</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a representation of partial confidence in belief and preference
that is consistent with the tenets of decision-theory. The fundamental insight
underlying the representation is that if a person is not completely confident
in a probability or utility assessment, additional modeling of the assessment
may improve decisions to which it is relevant. We show how a traditional
decision-analytic approach can be used to balance the benefits of additional
modeling with associated costs. The approach can be used during knowledge
acquisition to focus the attention of a knowledge engineer or expert on parts
of a decision model that deserve additional refinement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2725</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2725</id><created>2013-03-27</created><authors><author><keyname>Henrion</keyname><forenames>Max</forenames></author></authors><title>Practical Issues in Constructing a Bayes' Belief Network</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-132-139</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayes belief networks and influence diagrams are tools for constructing
coherent probabilistic representations of uncertain knowledge. The process of
constructing such a network to represent an expert's knowledge is used to
illustrate a variety of techniques which can facilitate the process of
structuring and quantifying uncertain relationships. These include some
generalizations of the &quot;noisy OR gate&quot; concept. Sensitivity analysis of generic
elements of Bayes' networks provides insight into when rough probability
assessments are sufficient and when greater precision may be important.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2726</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2726</id><created>2013-03-27</created><authors><author><keyname>Higgins</keyname><forenames>Michael C.</forenames></author></authors><title>NAIVE: A Method for Representing Uncertainty and Temporal Relationships
  in an Automated Reasoner</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-140-147</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes NAIVE, a low-level knowledge representation language and
inferencing process. NAIVE has been designed for reasoning about
nondeterministic dynamic systems like those found in medicine. Knowledge is
represented in a graph structure consisting of nodes, which correspond to the
variables describing the system of interest, and arcs, which correspond to the
procedures used to infer the value of a variable from the values of other
variables. The value of a variable can be determined at an instant in time,
over a time interval or for a series of times. Information about the value of a
variable is expressed as a probability density function which quantifies the
likelihood of each possible value. The inferencing process uses these
probability density functions to propagate uncertainty. NAIVE has been used to
develop medical knowledge bases including over 100 variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2727</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2727</id><created>2013-03-27</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Objective Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-148-155</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distinction is sometimes made between &quot;statistical&quot; and &quot;subjective&quot;
probabilities. This is based on a distinction between &quot;unique&quot; events and
&quot;repeatable&quot; events. We argue that this distinction is untenable, since all
events are &quot;unique&quot; and all events belong to &quot;kinds&quot;, and offer a conception of
probability for A1 in which (1) all probabilities are based on -- possibly
vague -- statistical knowledge, and (2) every statement in the language has a
probability. This conception of probability can be applied to very rich
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2728</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2728</id><created>2013-03-27</created><authors><author><keyname>Ursic</keyname><forenames>Silvio</forenames></author></authors><title>Coefficients of Relations for Probabilistic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-156-162</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Definitions and notations with historical references are given for some
numerical coefficients commonly used to quantify relations among collections of
objects for the purpose of expressing approximate knowledge and probabilistic
reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2729</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2729</id><created>2013-03-27</created><authors><author><keyname>Wise</keyname><forenames>Ben P.</forenames></author></authors><title>Satisfaction of Assumptions is a Weak Predictor of Performance</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-163-169</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper demonstrates a methodology for examining the accuracy of uncertain
inference systems (UIS), after their parameters have been optimized, and does
so for several common UIS's. This methodology may be used to test the accuracy
when either the prior assumptions or updating formulae are not exactly
satisfied. Surprisingly, these UIS's were revealed to be no more accurate on
the average than a simple linear regression. Moreover, even on prior
distributions which were deliberately biased so as give very good accuracy,
they were less accurate than the simple probabilistic model which assumes
marginal independence between inputs. This demonstrates that the importance of
updating formulae can outweigh that of prior assumptions. Thus, when UIS's are
judged by their final accuracy after optimization, we get completely different
results than when they are judged by whether or not their prior assumptions are
perfectly satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2730</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2730</id><created>2013-03-27</created><authors><author><keyname>Xu</keyname><forenames>Lei</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Structuring Causal Tree Models with Continuous Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-170-179</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of invoking auxiliary, unobservable
variables to facilitate the structuring of causal tree models for a given set
of continuous variables. Paralleling the treatment of bi-valued variables in
[Pearl 1986], we show that if a collection of coupled variables are governed by
a joint normal distribution and a tree-structured representation exists, then
both the topology and all internal relationships of the tree can be uncovered
by observing pairwise dependencies among the observed variables (i.e., the
leaves of the tree). Furthermore, the conditions for normally distributed
variables are less restrictive than those governing bi-valued variables. The
result extends the applications of causal tree models which were found useful
in evidential reasoning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2731</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2731</id><created>2013-03-27</created><authors><author><keyname>Yen</keyname><forenames>John</forenames></author></authors><title>Implementing Evidential Reasoning in Expert Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-180-188</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Dempster-Shafer theory has been extended recently for its application to
expert systems. However, implementing the extended D-S reasoning model in
rule-based systems greatly complicates the task of generating informative
explanations. By implementing GERTIS, a prototype system for diagnosing
rheumatoid arthritis, we show that two kinds of knowledge are essential for
explanation generation: (l) taxonomic class relationships between hypotheses
and (2) pointers to the rules that significantly contribute to belief in the
hypothesis. As a result, the knowledge represented in GERTIS is richer and more
complex than that of conventional rule-based systems. GERTIS not only
demonstrates the feasibility of rule-based evidential-reasoning systems, but
also suggests ways to generate better explanations, and to explicitly represent
various useful relationships among hypotheses and rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2732</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2732</id><created>2013-03-27</created><authors><author><keyname>Buntine</keyname><forenames>Wray L.</forenames></author></authors><title>Decision Tree Induction Systems: A Bayesian Analysis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-190-197</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision tree induction systems are being used for knowledge acquisition in
noisy domains. This paper develops a subjective Bayesian interpretation of the
task tackled by these systems and the heuristic methods they use. It is argued
that decision tree systems implicitly incorporate a prior belief that the
simpler (in terms of decision tree complexity) of two hypotheses be preferred,
all else being equal, and that they perform a greedy search of the space of
decision rules to find one in which there is strong posterior belief. A number
of improvements to these systems are then suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2733</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2733</id><created>2013-03-27</created><authors><author><keyname>Caruana</keyname><forenames>Richard A.</forenames></author></authors><title>The Automatic Training of Rule Bases that Use Numerical Uncertainty
  Representations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-198-204</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of numerical uncertainty representations allows better modeling of
some aspects of human evidential reasoning. It also makes knowledge acquisition
and system development, test, and modification more difficult. We propose that
where possible, the assignment and/or refinement of rule weights should be
performed automatically. We present one approach to performing this training -
numerical optimization - and report on the results of some preliminary tests in
training rule bases. We also show that truth maintenance can be used to make
training more efficient and ask some epistemological questions raised by
training rule weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2734</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2734</id><created>2013-03-27</created><authors><author><keyname>Dalkey</keyname><forenames>Norman C.</forenames></author></authors><title>The Inductive Logic of Information Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-205-211</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An inductive logic can be formulated in which the elements are not
propositions or probability distributions, but information systems. The logic
is complete for information systems with binary hypotheses, i.e., it applies to
all such systems. It is not complete for information systems with more than two
hypotheses, but applies to a subset of such systems. The logic is inductive in
that conclusions are more informative than premises. Inferences using the
formalism have a strong justification in terms of the expected value of the
derived information system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2735</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2735</id><created>2013-03-27</created><authors><author><keyname>Gallant</keyname><forenames>Stephen I.</forenames></author></authors><title>Automated Generation of Connectionist Expert Systems for Problems
  Involving Noise and Redundancy</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-212-221</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When creating an expert system, the most difficult and expensive task is
constructing a knowledge base. This is particularly true if the problem
involves noisy data and redundant measurements. This paper shows how to modify
the MACIE process for generating connectionist expert systems from training
examples so that it can accommodate noisy and redundant data. The basic idea is
to dynamically generate appropriate training examples by constructing both a
'deep' model and a noise model for the underlying problem. The use of
winner-take-all groups of variables is also discussed. These techniques are
illustrated with a small example that would be very difficult for standard
expert system approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2736</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2736</id><created>2013-03-27</created><authors><author><keyname>Rebane</keyname><forenames>George</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>The Recovery of Causal Poly-Trees from Statistical Data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-222-228</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poly-trees are singly connected causal networks in which variables may arise
from multiple causes. This paper develops a method of recovering ply-trees from
empirically measured probability distributions of pairs of variables. The
method guarantees that, if the measured distributions are generated by a causal
process structured as a ply-tree then the topological structure of such tree
can be recovered precisely and, in addition, the causal directionality of the
branches can be determined up to the maximum extent possible. The method also
pinpoints the minimum (if any) external semantics required to determine the
causal relationships among the variables considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2737</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2737</id><created>2013-03-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Eddy</keyname><forenames>David M.</forenames></author><author><keyname>Hasselblad</keyname><forenames>Vic</forenames></author><author><keyname>Wolpert</keyname><forenames>Robert</forenames></author></authors><title>A Heuristic Bayesian Approach to Knowledge Acquisition: Application to
  Analysis of Tissue-Type Plasminogen Activator</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-229-236</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a heuristic Bayesian method for computing probability
distributions from experimental data, based upon the multivariate normal form
of the influence diagram. An example illustrates its use in medical technology
assessment. This approach facilitates the integration of results from different
studies, and permits a medical expert to make proper assessments without
considerable statistical training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2738</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2738</id><created>2013-03-27</created><authors><author><keyname>Star</keyname><forenames>Spencer</forenames></author></authors><title>Theory-Based Inductive Learning: An Integration of Symbolic and
  Quantitative Methods</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-237-248</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this paper is to propose a method that will generate a
causal explanation of observed events in an uncertain world and then make
decisions based on that explanation. Feedback can cause the explanation and
decisions to be modified. I call the method Theory-Based Inductive Learning
(T-BIL). T-BIL integrates deductive learning, based on a technique called
Explanation-Based Generalization (EBG) from the field of machine learning, with
inductive learning methods from Bayesian decision theory. T-BIL takes as inputs
(1) a decision problem involving a sequence of related decisions over time, (2)
a training example of a solution to the decision problem in one period, and (3)
the domain theory relevant to the decision problem. T-BIL uses these inputs to
construct a probabilistic explanation of why the training example is an
instance of a solution to one stage of the sequential decision problem. This
explanation is then generalized to cover a more general class of instances and
is used as the basis for making the next-stage decisions. As the outcomes of
each decision are observed, the explanation is revised, which in turn affects
the subsequent decisions. A detailed example is presented that uses T-BIL to
solve a very general stochastic adaptive control problem for an autonomous
mobile robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2739</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2739</id><created>2013-03-27</created><authors><author><keyname>Bonissone</keyname><forenames>Piero P.</forenames></author></authors><title>Using T-Norm Based Uncertainty Calculi in a Naval Situation Assessment
  Application</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-250-261</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RUM (Reasoning with Uncertainty Module), is an integrated software tool based
on a KEE, a frame system implemented in an object oriented language. RUM's
architecture is composed of three layers: representation, inference, and
control. The representation layer is based on frame-like data structures that
capture the uncertainty information used in the inference layer and the
uncertainty meta-information used in the control layer. The inference layer
provides a selection of five T-norm based uncertainty calculi with which to
perform the intersection, detachment, union, and pooling of information. The
control layer uses the meta-information to select the appropriate calculus for
each context and to resolve eventual ignorance or conflict in the information.
This layer also provides a context mechanism that allows the system to focus on
the relevant portion of the knowledge base, and an uncertain-belief revision
system that incrementally updates the certainty values of well-formed formulae
(wffs) in an acyclic directed deduction graph. RUM has been tested and
validated in a sequence of experiments in both naval and aerial situation
assessment (SA), consisting of correlating reports and tracks, locating and
classifying platforms, and identifying intents and threats. An example of naval
situation assessment is illustrated. The testbed environment for developing
these experiments has been provided by LOTTA, a symbolic simulator implemented
in Flavors. This simulator maintains time-varying situations in a multi-player
antagonistic game where players must make decisions in light of uncertain and
incomplete data. RUM has been used to assist one of the LOTTA players to
perform the SA task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2740</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2740</id><created>2013-03-27</created><authors><author><keyname>Cheng</keyname><forenames>Yizong</forenames></author><author><keyname>Kashyap</keyname><forenames>Rangasami L.</forenames></author></authors><title>A Study of Associative Evidential Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-262-269</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidential reasoning is cast as the problem of simplifying the
evidence-hypothesis relation and constructing combination formulas that possess
certain testable properties. Important classes of evidence as identifiers,
annihilators, and idempotents and their roles in determining binary operations
on intervals of reals are discussed. The appropriate way of constructing
formulas for combining evidence and their limitations, for instance, in
robustness, are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2741</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2741</id><created>2013-03-27</created><authors><author><keyname>Goodman</keyname><forenames>I. R.</forenames></author></authors><title>A Measure-Free Approach to Conditioning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-270-277</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an earlier paper, a new theory of measurefree &quot;conditional&quot; objects was
presented. In this paper, emphasis is placed upon the motivation of the theory.
The central part of this motivation is established through an example involving
a knowledge-based system. In order to evaluate combination of evidence for this
system, using observed data, auxiliary at tribute and diagnosis variables, and
inference rules connecting them, one must first choose an appropriate algebraic
logic description pair (ALDP): a formal language or syntax followed by a
compatible logic or semantic evaluation (or model). Three common choices- for
this highly non-unique choice - are briefly discussed, the logics being
Classical Logic, Fuzzy Logic, and Probability Logic. In all three,the key
operator representing implication for the inference rules is interpreted as the
often-used disjunction of a negation (b =&gt; a) = (b'v a), for any events a,b.
  However, another reasonable interpretation of the implication operator is
through the familiar form of probabilistic conditioning. But, it can be shown -
quite surprisingly - that the ALDP corresponding to Probability Logic cannot be
used as a rigorous basis for this interpretation! To fill this gap, a new ALDP
is constructed consisting of &quot;conditional objects&quot;, extending ordinary
Probability Logic, and compatible with the desired conditional probability
interpretation of inference rules. It is shown also that this choice of ALDP
leads to feasible computations for the combination of evidence evaluation in
the example. In addition, a number of basic properties of conditional objects
and the resulting Conditional Probability Logic are given, including a
characterization property and a developed calculus of relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2742</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2742</id><created>2013-03-27</created><authors><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author><author><keyname>Frisch</keyname><forenames>Alan M.</forenames></author></authors><title>Convergent Deduction for Probabilistic Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-278-286</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the semantics and proof theory of Nilsson's
probabilistic logic, outlining both the benefits of its well-defined model
theory and the drawbacks of its proof theory. Within Nilsson's semantic
framework, we derive a set of inference rules which are provably sound. The
resulting proof system, in contrast to Nilsson's approach, has the important
feature of convergence - that is, the inference process proceeds by computing
increasingly narrow probability intervals which converge from above and below
on the smallest entailed probability interval. Thus the procedure can be
stopped at any time to yield partial information concerning the smallest
entailed interval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2743</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2743</id><created>2013-03-27</created><authors><author><keyname>Li</keyname><forenames>Ze-Nian</forenames></author></authors><title>Comparisons of Reasoning Mechanisms for Computer Vision</title><categories>cs.CV cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-287-294</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An evidential reasoning mechanism based on the Dempster-Shafer theory of
evidence is introduced. Its performance in real-world image analysis is
compared with other mechanisms based on the Bayesian formalism and a simple
weight combination method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2744</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2744</id><created>2013-03-27</created><authors><author><keyname>Mitchell</keyname><forenames>Donald H.</forenames></author><author><keyname>Harp</keyname><forenames>Steven A.</forenames></author><author><keyname>Simkin</keyname><forenames>David K.</forenames></author></authors><title>A Knowledge Engineer's Comparison of Three Evidence Aggregation Methods</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-297-304</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The comparisons of uncertainty calculi from the last two Uncertainty
Workshops have all used theoretical probabilistic accuracy as the sole metric.
While mathematical correctness is important, there are other factors which
should be considered when developing reasoning systems. These other factors
include, among other things, the error in uncertainty measures obtainable for
the problem and the effect of this error on the performance of the resulting
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2745</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2745</id><created>2013-03-27</created><authors><author><keyname>Neufeld</keyname><forenames>Eric</forenames></author><author><keyname>Poole</keyname><forenames>David L</forenames></author></authors><title>Towards Solving the Multiple Extension Problem: Combining Defaults and
  Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-305-312</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiple extension problem arises frequently in diagnostic and default
inference. That is, we can often use any of a number of sets of defaults or
possible hypotheses to explain observations or make Predictions. In default
inference, some extensions seem to be simply wrong and we use qualitative
techniques to weed out the unwanted ones. In the area of diagnosis, however,
the multiple explanations may all seem reasonable, however improbable. Choosing
among them is a matter of quantitative preference. Quantitative preference
works well in diagnosis when knowledge is modelled causally. Here we suggest a
framework that combines probabilities and defaults in a single unified
framework that retains the semantics of diagnosis as construction of
explanations from a fixed set of possible hypotheses. We can then compute
probabilities incrementally as we construct explanations. Here we describe a
branch and bound algorithm that maintains a set of all partial explanations
while exploring a most promising one first. A most probable explanation is
found first if explanations are partially ordered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2746</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2746</id><created>2013-03-27</created><authors><author><keyname>Tong</keyname><forenames>Richard M.</forenames></author><author><keyname>Appelbaum</keyname><forenames>Lee A.</forenames></author></authors><title>Problem Structure and Evidential Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-313-320</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our previous series of studies to investigate the role of evidential
reasoning in the RUBRIC system for full-text document retrieval (Tong et al.,
1985; Tong and Shapiro, 1985; Tong and Appelbaum, 1987), we identified the
important role that problem structure plays in the overall performance of the
system. In this paper, we focus on these structural elements (which we now call
&quot;semantic structure&quot;) and show how explicit consideration of their properties
reduces what previously were seen as difficult evidential reasoning problems to
more tractable questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2747</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2747</id><created>2013-03-27</created><authors><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>The Role of Calculi in Uncertain Inference Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-321-331</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of the controversy about methods for automated decision making has
focused on specific calculi for combining beliefs or propagating uncertainty.
We broaden the debate by (1) exploring the constellation of secondary tasks
surrounding any primary decision problem, and (2) identifying knowledge
engineering concerns that present additional representational tradeoffs. We
argue on pragmatic grounds that the attempt to support all of these tasks
within a single calculus is misguided. In the process, we note several
uncertain reasoning objectives that conflict with the Bayesian ideal of
complete specification of probabilities and utilities. In response, we advocate
treating the uncertainty calculus as an object language for reasoning
mechanisms that support the secondary tasks. Arguments against Bayesian
decision theory are weakened when the calculus is relegated to this role.
Architectures for uncertainty handling that take statements in the calculus as
objects to be reasoned about offer the prospect of retaining normative status
with respect to decision making while supporting the other tasks in uncertain
reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2748</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2748</id><created>2013-03-27</created><authors><author><keyname>Wise</keyname><forenames>Ben P.</forenames></author><author><keyname>Perrin</keyname><forenames>Bruce M.</forenames></author><author><keyname>Vaughan</keyname><forenames>David S.</forenames></author><author><keyname>Yadrick</keyname><forenames>Robert M.</forenames></author></authors><title>The Role of Tuning Uncertain Inference Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-332-339</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examined the effects of &quot;tuning&quot; the parameters of the incremental
function of MYCIN, the independent function of PROSPECTOR, a probability model
that assumes independence, and a simple additive linear equation. me parameters
of each of these models were optimized to provide solutions which most nearly
approximated those from a full probability model for a large set of simple
networks. Surprisingly, MYCIN, PROSPECTOR, and the linear equation performed
equivalently; the independence model was clearly more accurate on the networks
studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2749</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2749</id><created>2013-03-27</created><authors><author><keyname>Zhang</keyname><forenames>Minchuan</forenames></author><author><keyname>Chen</keyname><forenames>Su-shing</forenames></author></authors><title>Evidential Reasoning in Image Understanding</title><categories>cs.CV cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-340-346</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present some results of evidential reasoning in
understanding multispectral images of remote sensing systems. The
Dempster-Shafer approach of combination of evidences is pursued to yield
contextual classification results, which are compared with previous results of
the Bayesian context free classification, contextual classifications of dynamic
programming and stochastic relaxation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2750</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2750</id><created>2013-03-27</created><authors><author><keyname>Booker</keyname><forenames>Lashon B.</forenames></author><author><keyname>Hota</keyname><forenames>Naveen</forenames></author><author><keyname>Hemphill</keyname><forenames>Gavin</forenames></author></authors><title>Implementing a Bayesian Scheme for Revising Belief Commitments</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-348-354</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our previous work on classifying complex ship images [1,2] has evolved into
an effort to develop software tools for building and solving generic
classification problems. Managing the uncertainty associated with feature data
and other evidence is an important issue in this endeavor. Bayesian techniques
for managing uncertainty [7,12,13] have proven to be useful for managing
several of the belief maintenance requirements of classification problem
solving. One such requirement is the need to give qualitative explanations of
what is believed. Pearl [11] addresses this need by computing what he calls a
belief commitment-the most probable instantiation of all hypothesis variables
given the evidence available. Before belief commitments can be computed, the
straightforward implementation of Pearl's procedure involves finding an
analytical solution to some often difficult optimization problems. We describe
an efficient implementation of this procedure using tensor products that solves
these problems enumeratively and avoids the need for case by case analysis. The
procedure is thereby made more practical to use in the general case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2751</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2751</id><created>2013-03-27</created><authors><author><keyname>Breese</keyname><forenames>John S.</forenames></author><author><keyname>Tse</keyname><forenames>Edison</forenames></author></authors><title>Integrating Logical and Probabilistic Reasoning for Decision Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-355-362</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a representation and a set of inference methods that combine
logic programming techniques with probabilistic network representations for
uncertainty (influence diagrams). The techniques emphasize the dynamic
construction and solution of probabilistic and decision-theoretic models for
complex and uncertain domains. Given a query, a logical proof is produced if
possible; if not, an influence diagram based on the query and the knowledge of
the decision domain is produced and subsequently solved. A uniform declarative,
first-order, knowledge representation is combined with a set of integrated
inference procedures for logical, probabilistic, and decision-theoretic
reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2752</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2752</id><created>2013-03-27</created><authors><author><keyname>Chiu</keyname><forenames>Stephen</forenames></author><author><keyname>Togai</keyname><forenames>Masaki</forenames></author></authors><title>Compiling Fuzzy Logic Control Rules to Hardware Implementations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-363-371</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major aspect of human reasoning involves the use of approximations.
Particularly in situations where the decision-making process is under stringent
time constraints, decisions are based largely on approximate, qualitative
assessments of the situations. Our work is concerned with the application of
approximate reasoning to real-time control. Because of the stringent processing
speed requirements in such applications, hardware implementations of fuzzy
logic inferencing are being pursued. We describe a programming environment for
translating fuzzy control rules into hardware realizations. Two methods of
hardware realizations are possible. The First is based on a special purpose
chip for fuzzy inferencing. The second is based on a simple memory chip. The
ability to directly translate a set of decision rules into hardware
implementations is expected to make fuzzy control an increasingly practical
approach to the control of complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2753</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2753</id><created>2013-03-27</created><authors><author><keyname>Cohen</keyname><forenames>Paul</forenames></author></authors><title>Steps Towards Programs that Manage Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-372-379</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reasoning under uncertainty in Al hats come to mean assessing the credibility
of hypotheses inferred from evidence. But techniques for assessing credibility
do not tell a problem solver what to do when it is uncertain. This is the focus
of our current research. We have developed a medical expert system called MUM,
for Managing Uncertainty in Medicine, that plans diagnostic sequences of
questions, tests, and treatments. This paper describes the kinds of problems
that MUM was designed to solve and gives a brief description of its
architecture. More recently, we have built an empty version of MUM called MU,
and used it to reimplement MUM and a small diagnostic system for plant
pathology. The latter part of the paper describes the features of MU that make
it appropriate for building expert systems that manage uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2754</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2754</id><created>2013-03-27</created><authors><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>An Algorithm for Computing Probabilistic Propositions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-380-385</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for computing probabilistic propositions is presented. It assumes
the availability of a single external routine for computing the probability of
one instantiated variable, given a conjunction of other instantiated variables.
In particular, the method allows belief network algorithms to calculate general
probabilistic propositions over nodes in the network. Although in the worst
case the time complexity of the method is exponential in the size of a query,
it is polynomial in the size of a number of common types of queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2755</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2755</id><created>2013-03-27</created><authors><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author></authors><title>Combining Symbolic and Numeric Approaches to Uncertainty Management</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-386-393</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complete approach to reasoning under uncertainty requires support for
incremental and interactive formulation and revision of, as well as reasoning
with, models of the problem domain capable of representing our uncertainty. We
present a hybrid reasoning scheme which combines symbolic and numeric methods
for uncertainty management to provide efficient and effective support for each
of these tasks. The hybrid is based on symbolic techniques adapted from
Assumption-based Truth Maintenance systems (ATMS), combined with numeric
methods adapted from the Dempster/Shafer theory of evidence, as extended in
Baldwin's Support Logic Programming system. The hybridization is achieved by
viewing an ATMS as a symbolic algebra system for uncertainty calculations. This
technique has several major advantages over conventional methods for performing
inference with numeric certainty estimates in addition to the ability to
dynamically determine hypothesis spaces, including improved management of
dependent and partially independent evidence, faster run-time evaluation of
propositional certainties, the ability to query the certainty value of a
proposition from multiple perspectives, and the ability to incrementally extend
or revise domain models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2756</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2756</id><created>2013-03-27</created><authors><author><keyname>Elsaesser</keyname><forenames>Christopher</forenames></author></authors><title>Explanation of Probabilistic Inference for Decision Support Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-394-403</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An automated explanation facility for Bayesian conditioning aimed at
improving user acceptance of probability-based decision support systems has
been developed. The domain-independent facility is based on an information
processing perspective on reasoning about conditional evidence that accounts
both for biased and normative inferences. Experimental results indicate that
the facility is both acceptable to naive users and effective in improving
understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2757</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2757</id><created>2013-03-27</created><authors><author><keyname>Hager</keyname><forenames>Greg</forenames></author><author><keyname>Mintz</keyname><forenames>Max</forenames></author></authors><title>Estimation Procedures for Robust Sensor Control</title><categories>cs.SY cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-404-411</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many robotic sensor estimation problems can characterized in terms of
nonlinear measurement systems. These systems are contaminated with noise and
may be underdetermined from a single observation. In order to get reliable
estimation results, the system must choose views which result in an
overdetermined system. This is the sensor control problem. Accurate and
reliable sensor control requires an estimation procedure which yields both
estimates and measures of its own performance. In the case of nonlinear
measurement systems, computationally simple closed-form estimation solutions
may not exist. However, approximation techniques provide viable alternatives.
In this paper, we evaluate three estimation techniques: the extended Kalman
filter, a discrete Bayes approximation, and an iterative Bayes approximation.
We present mathematical results and simulation statistics illustrating
operating conditions where the extended Kalman filter is inappropriate for
sensor control, and discuss issues in the use of the discrete Bayes
approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2758</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2758</id><created>2013-03-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Bertrand</keyname><forenames>Leonard</forenames></author></authors><title>Efficient Inference on Generalized Fault Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-413-420</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized fault diagram, a data structure for failure analysis based on
the influence diagram, is defined. Unlike the fault tree, this structure allows
for dependence among the basic events and replicated logical elements. A
heuristic procedure is developed for efficient processing of these structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2759</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2759</id><created>2013-03-27</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>Reasoning About Beliefs and Actions Under Computational Resource
  Constraints</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-429-447</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although many investigators affirm a desire to build reasoning systems that
behave consistently with the axiomatic basis defined by probability theory and
utility theory, limited resources for engineering and computation can make a
complete normative analysis impossible. We attempt to move discussion beyond
the debate over the scope of problems that can be handled effectively to cases
where it is clear that there are insufficient computational resources to
perform an analysis deemed as complete. Under these conditions, we stress the
importance of considering the expected costs and benefits of applying
alternative approximation procedures and heuristics for computation and
knowledge acquisition. We discuss how knowledge about the structure of user
utility can be used to control value tradeoffs for tailoring inference to
alternative contexts. We address the notion of real-time rationality, focusing
on the application of knowledge about the expected timewise-refinement
abilities of reasoning strategies to balance the benefits of additional
computation with the costs of acting with a partial result. We discuss the
benefits of applying decision theory to control the solution of difficult
problems given limitations and uncertainty in reasoning resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2760</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2760</id><created>2013-03-28</created><authors><author><keyname>Slack</keyname><forenames>Thomas</forenames></author></authors><title>Advantages and a Limitation of Using LEG Nets in a Real-TIme Problem</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</comments><proxy>auai</proxy><report-no>UAI-P-1987-PG-421-428</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After experimenting with a number of non-probabilistic methods for dealing
with uncertainty many researchers reaffirm a preference for probability methods
[1] [2], although this remains controversial. The importance of being able to
form decisions from incomplete data in diagnostic problems has highlighted
probabilistic methods [5] which compute posterior probabilities from prior
distributions in a way similar to Bayes Rule, and thus are called Bayesian
methods. This paper documents the use of a Bayesian method in a real time
problem which is similar to medical diagnosis in that there is a need to form
decisions and take some action without complete knowledge of conditions in the
problem domain. This particular method has a limitation which is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2797</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2797</id><created>2013-04-05</created><authors><author><keyname>Saad</keyname><forenames>Emad</forenames></author></authors><title>Logical Fuzzy Preferences</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.2384</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a unified logical framework for representing and reasoning about
both quantitative and qualitative preferences in fuzzy answer set programming,
called fuzzy answer set optimization programs. The proposed framework is vital
to allow defining quantitative preferences over the possible outcomes of
qualitative preferences. We show the application of fuzzy answer set
optimization programs to the course scheduling with fuzzy preferences problem.
To the best of our knowledge, this development is the first to consider a
logical framework for reasoning about quantitative preferences, in general, and
reasoning about both quantitative and qualitative preferences in particular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2798</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2798</id><created>2013-04-09</created><authors><author><keyname>Motahari</keyname><forenames>Abolfazl</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author><author><keyname>Ma</keyname><forenames>Nan</forenames></author></authors><title>Optimal DNA shotgun sequencing: Noisy reads are as good as noiseless
  reads</title><categories>cs.IT math.IT q-bio.GN</categories><comments>Submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the fundamental limits of DNA shotgun sequencing under noisy
reads. We show a surprising result: for the i.i.d. DNA model, noisy reads are
as good as noiseless reads, provided that the noise level is below a certain
threshold which can be surprisingly high. As an example, for a uniformly
distributed DNA sequence and a symmetric substitution noisy read channel, the
threshold is as high as 19%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2799</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2799</id><created>2013-04-05</created><authors><author><keyname>Saad</keyname><forenames>Emad</forenames></author></authors><title>Nested Aggregates in Answer Sets: An Application to a Priori
  Optimization</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1304.2384</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We allow representing and reasoning in the presence of nested multiple
aggregates over multiple variables and nested multiple aggregates over
functions involving multiple variables in answer sets, precisely, in answer set
optimization programming and in answer set programming. We show the
applicability of the answer set optimization programming with nested multiple
aggregates and the answer set programming with nested multiple aggregates to
the Probabilistic Traveling Salesman Problem, a fundamental a priori
optimization problem in Operation Research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2809</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2809</id><created>2013-04-09</created><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Scheinberg</keyname><forenames>Katya</forenames></author><author><keyname>Vicente</keyname><forenames>Luis Nunes</forenames></author></authors><title>On partial sparse recovery</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a partially sparse solution of an
underdetermined system of linear equations by minimizing the $\ell_1$-norm of
the part of the solution vector which is known to be sparse. Such a problem is
closely related to a classical problem in Compressed Sensing where the
$\ell_1$-norm of the whole solution vector is minimized. We introduce analogues
of restricted isometry and null space properties for the recovery of partially
sparse vectors and show that these new properties are implied by their original
counterparts. We show also how to extend recovery under noisy measurements to
the partially sparse case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2814</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2814</id><created>2013-04-09</created><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Esti&#xe9;venart</keyname><forenames>Morgane</forenames></author><author><keyname>Geeraerts</keyname><forenames>Gilles</forenames></author></authors><title>On MITL and alternating timed automata</title><categories>cs.FL cs.LO</categories><comments>28 pages, 3 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One clock alternating timed automata OCATA have been recently introduced as
natural extension of (one clock) timed automata to express the semantics of MTL
(Ouaknine, Worrell 2005). We consider the application of OCATA to problem of
model-checking MITL formulas (a syntactic fragment of MTL) against timed
automata. We introduce a new semantics for OCATA where, intuitively, clock
valuations are intervals instead of single real values. Thanks to this new
semantics, we show that we can bound the number of clock copies that are
necessary to allow an OCATA to recognise the models of an MITL formula.
Equipped with this technique, we propose a new algorithm to translate an MITL
formula into a timed automaton, and we sketch several ideas to define new model
checking algorithms for MITL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2816</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2816</id><created>2013-04-09</created><updated>2015-08-20</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Villarreal-Zapata</keyname><forenames>Elena</forenames></author></authors><title>Asymptotic Behaviour and Ratios of Complexity in Cellular Automata</title><categories>nlin.CG cs.CC</categories><comments>22 pages, 13 figures. As appeared in the International Journal of
  Bifurcation and Chaos with corrections to the definition of Wolfram class</comments><journal-ref>International Journal of Bifurcation and Chaos, vol. 13, no. 9,
  2013</journal-ref><doi>10.1142/S0218127413501599</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the asymptotic behaviour of symbolic computing systems, notably
one-dimensional cellular automata (CA), in order to ascertain whether and at
what rate the number of complex versus simple rules dominate the rule space for
increasing neighbourhood range and number of symbols (or colours), and how
different behaviour is distributed in the spaces of different cellular automata
formalisms. Using two different measures, Shannon's block entropy and
Kolmogorov complexity, the latter approximated by two different methods
(lossless compressibility and block decomposition), we arrive at the same trend
of larger complex behavioural fractions. We also advance a notion of asymptotic
and limit behaviour for individual rules, both over initial conditions and
runtimes, and we provide a formalisation of Wolfram's classification as a limit
function in terms of Kolmogorov complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2840</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2840</id><created>2013-04-10</created><authors><author><keyname>Rodero</keyname><forenames>Ivan</forenames></author><author><keyname>Parashar</keyname><forenames>Manish</forenames></author></authors><title>Cross-layer Application-aware Power/Energy Management for Extreme Scale
  Science</title><categories>cs.DC</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High Performance Computing (HPC) has evolved over the past decades into
increasingly complex and powerful systems. Current HPC systems consume several
MWs of power, enough to power small towns, and are in fact soon approaching the
limits of the power available to them. Estimates are with the given current
technology, achieving exascale will require hundreds of MW, which is not
feasible from multiple perspectives. Architecture and technology researchers
are aggressively addressing this; however as past history is shown, innovation
at these levels are not sufficient and have to be accompanied with innovations
at higher levels (algorithms, programming, runtime, OS) to achieve the multiple
orders of magnitude reduction - i.e., a comprehensive cross-layer and
application-aware strategy is required. Furthermore, energy/power-efficiency
has to be addressed in combination with quality of solutions, performance and
reliability and other objectives and appropriate tradeoffs are required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2850</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2850</id><created>2013-04-10</created><updated>2013-08-08</updated><authors><author><keyname>Huang</keyname><forenames>Haiping</forenames></author><author><keyname>Wong</keyname><forenames>K. Y. Michael</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Entropy landscape of solutions in the binary perceptron problem</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.LG</categories><comments>21 pages, 6 figures, version accepted by Journal of Physics A:
  Mathematical and Theoretical</comments><journal-ref>J. Phys. A: Math. Theor. 46 (2013) 375002</journal-ref><doi>10.1088/1751-8113/46/37/375002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The statistical picture of the solution space for a binary perceptron is
studied. The binary perceptron learns a random classification of input random
patterns by a set of binary synaptic weights. The learning of this network is
difficult especially when the pattern (constraint) density is close to the
capacity, which is supposed to be intimately related to the structure of the
solution space. The geometrical organization is elucidated by the entropy
landscape from a reference configuration and of solution-pairs separated by a
given Hamming distance in the solution space. We evaluate the entropy at the
annealed level as well as replica symmetric level and the mean field result is
confirmed by the numerical simulations on single instances using the proposed
message passing algorithms. From the first landscape (a random configuration as
a reference), we see clearly how the solution space shrinks as more constraints
are added. From the second landscape of solution-pairs, we deduce the
coexistence of clustering and freezing in the solution space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2860</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2860</id><created>2013-04-10</created><authors><author><keyname>Pierrot</keyname><forenames>Adeline</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Rossin</keyname><forenames>Dominique</forenames><affiliation>LIX</affiliation></author></authors><title>2-Stack Sorting is polynomial</title><categories>math.CO cs.DM</categories><comments>23 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we give a polynomial algorithm to decide whether a given
permutation $\sigma$ is sortable with two stacks in series. This is indeed a
longstanding open problem which was first introduced by Knuth. He introduced
the stack sorting problem as well as permutation patterns which arises
naturally when characterizing permutations that can be sorted with one stack.
When several stacks in series are considered, few results are known. There are
two main different problems. The first one is the complexity of deciding if a
permutation is sortable or not, the second one being the characterization and
the enumeration of those sortable permutations. We hereby prove that the first
problem lies in P by giving a polynomial algorithm to solve it. This article
strongly relies on a previous article in which 2-stack pushall sorting is
defined and studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2865</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2865</id><created>2013-04-10</created><authors><author><keyname>Br&#xfc;mmer</keyname><forenames>Niko</forenames></author><author><keyname>de Villiers</keyname><forenames>Edward</forenames></author></authors><title>The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New
  DCF</title><categories>stat.AP cs.LG stat.ML</categories><comments>presented at: The NIST SRE'11 Analysis Workshop, Atlanta, December
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The change of two orders of magnitude in the 'new DCF' of NIST's SRE'10,
relative to the 'old DCF' evaluation criterion, posed a difficult challenge for
participants and evaluator alike. Initially, participants were at a loss as to
how to calibrate their systems, while the evaluator underestimated the required
number of evaluation trials. After the fact, it is now obvious that both
calibration and evaluation require very large sets of trials. This poses the
challenges of (i) how to decide what number of trials is enough, and (ii) how
to process such large data sets with reasonable memory and CPU requirements.
After SRE'10, at the BOSARIS Workshop, we built solutions to these problems
into the freely available BOSARIS Toolkit. This paper explains the principles
and algorithms behind this toolkit. The main contributions of the toolkit are:
1. The Normalized Bayes Error-Rate Plot, which analyses likelihood- ratio
calibration over a wide range of DCF operating points. These plots also help in
judging the adequacy of the sizes of calibration and evaluation databases. 2.
Efficient algorithms to compute DCF and minDCF for large score files, over the
range of operating points required by these plots. 3. A new score file format,
which facilitates working with very large trial lists. 4. A faster logistic
regression optimizer for fusion and calibration. 5. A principled way to define
EER (equal error rate), which is of practical interest when the absolute error
count is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2867</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2867</id><created>2013-04-10</created><authors><author><keyname>Sarker</keyname><forenames>Md. Mamun Ali</forenames></author><author><keyname>Khan</keyname><forenames>Md. Ashraf Hossain</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>Guidelines to the Problem of Location Management and Database
  Architecture for the Next Generation Mobile Networks</title><categories>cs.NI cs.DB</categories><journal-ref>Procs. of the International Conference on Computer and
  Communication Engineering (ICCCE 06), Vol. II, pp. 761-766, Kuala Lumpur,
  Malaysia, May 9-11, (2006)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In near future, anticipated large number of mobile users may introduce very
large centralized databases and increase end-to-end delays in location
registration and call delivery on HLR-VLR database and will become infeasible.
After observing several problems we propose some guidelines. Multitree
distributed database, high throughput index structure, memory oriented database
organization are used. Location management guidelines for moving user in
overlapping network, neighbor discovery protocol (NDP), and global roaming rule
are adopted. Analytic model and examples are presented to evaluate the
efficiency of proposed guidelines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2881</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2881</id><created>2013-04-10</created><authors><author><keyname>Demirel</keyname><forenames>&#xd6;mer</forenames></author><author><keyname>Sbalzarini</keyname><forenames>Ivo F.</forenames></author></authors><title>Balanced offline allocation of weighted balls into bins</title><categories>cs.DM cs.DS</categories><comments>8 pages, 4 figures</comments><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a sorting-based greedy algorithm called SortedGreedy[m] for
approximately solving the offline version of the d-choice weighted
balls-into-bins problem where the number of choices for each ball is equal to
the number of bins. We assume the ball weights to be non-negative. We compare
the performance of the sorting-based algorithm with a naive algorithm called
Greedy[m]. We show that by sorting the input data according to the weights we
are able to achieve an order of magnitude smaller gap (the weight difference
between the heaviest and the lightest bin) for small problems (&lt;= 4000 balls),
and at least two orders of magnitude smaller gap for larger problems. In
practice, SortedGreedy[m] runs almost as fast as Greedy[m]. This makes
sorting-based algorithms favorable for solving offline weighted balls-into-bins
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2888</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2888</id><created>2013-04-10</created><authors><author><keyname>Bredeche</keyname><forenames>Nicolas</forenames></author><author><keyname>Montanier</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Weel</keyname><forenames>Berend</forenames></author><author><keyname>Haasdijk</keyname><forenames>Evert</forenames></author></authors><title>Roborobo! a Fast Robot Simulator for Swarm and Collective Robotics</title><categories>cs.RO cs.AI cs.NE</categories><comments>2 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Roborobo! is a multi-platform, highly portable, robot simulator for
large-scale collective robotics experiments. Roborobo! is coded in C++, and
follows the KISS guideline (&quot;Keep it simple&quot;). Therefore, its external
dependency is solely limited to the widely available SDL library for fast 2D
Graphics. Roborobo! is based on a Khepera/ePuck model. It is targeted for fast
single and multi-robots simulation, and has already been used in more than a
dozen published research mainly concerned with evolutionary swarm robotics,
including environment-driven self-adaptation and distributed evolutionary
optimization, as well as online onboard embodied evolution and embodied
morphogenesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2889</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2889</id><created>2013-04-10</created><authors><author><keyname>Chung</keyname><forenames>David H. S.</forenames></author><author><keyname>Legg</keyname><forenames>Philip A.</forenames></author><author><keyname>Parry</keyname><forenames>Matthew L.</forenames></author><author><keyname>Bown</keyname><forenames>Rhodri</forenames></author><author><keyname>Griffiths</keyname><forenames>Iwan W.</forenames></author><author><keyname>Laramee</keyname><forenames>Robert S.</forenames></author><author><keyname>Chen</keyname><forenames>Min</forenames></author></authors><title>Glyph Sorting: Interactive Visualization for Multi-dimensional Data</title><categories>cs.GR</categories><comments>10 pages, 6 figures</comments><acm-class>I.3.6; I.3.8</acm-class><doi>10.1177/1473871613511959</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Glyph-based visualization is an effective tool for depicting multivariate
information. Since sorting is one of the most common analytical tasks performed
on individual attributes of a multi-dimensional data set, this motivates the
hypothesis that introducing glyph sorting would significantly enhance the
usability of glyph-based visualization. In this paper, we present a glyph-based
conceptual framework as part of a visualization process for interactive sorting
of multivariate data. We examine several technical aspects of glyph sorting and
provide design principles for developing effective, visually sortable glyphs.
Glyphs that are visually sortable provide two key benefits: 1) performing
comparative analysis of multiple attributes between glyphs and 2) to support
multi-dimensional visual search. We describe a system that incorporates focus
and context glyphs to control sorting in a visually intuitive manner and for
viewing sorted results in an Interactive, Multi-dimensional Glyph (IMG) plot
that enables users to perform high-dimensional sorting, analyse and examine
data trends in detail. To demonstrate the usability of glyph sorting, we
present a case study in rugby event analysis for comparing and analysing trends
within matches. This work is undertaken in conjunction with a national rugby
team. From using glyph sorting, analysts have reported the discovery of new
insight beyond traditional match analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2917</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2917</id><created>2013-04-09</created><authors><author><keyname>Marquitti</keyname><forenames>Flavia Maria Darcie</forenames></author><author><keyname>Guimaraes</keyname><forenames>Paulo Roberto</forenames><suffix>Jr.</suffix></author><author><keyname>Pires</keyname><forenames>Mathias Mistretta</forenames></author><author><keyname>Bittencourt</keyname><forenames>Luiz Fernando</forenames></author></authors><title>MODULAR: Software for the Autonomous Computation of Modularity in Large
  Network Sets</title><categories>q-bio.QM cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ecological systems can be seen as networks of interactions between
individual, species, or habitat patches. A key feature of many ecological
networks is their organization into modules, which are subsets of elements that
are more connected to each other than to the other elements in the network. We
introduce MODULAR to perform rapid and autonomous calculation of modularity in
sets of networks. MODULAR reads a set of files with matrices or edge lists that
represent unipartite or bipartite networks, and identify modules using two
different modularity metrics that have been previously used in studies of
ecological networks. To find the network partition that maximizes modularity,
the software offers five optimization methods to the user. We also included two
of the most common null models that are used in studies of ecological networks
to verify how the modularity found by the maximization of each metric differs
from a theoretical benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2920</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2920</id><created>2013-04-10</created><authors><author><keyname>Ustimenko</keyname><forenames>Vasyl</forenames></author><author><keyname>Wr&#xf3;blewska</keyname><forenames>Aneta</forenames></author></authors><title>On the key exchange with nonlinear polynomial maps of stable degree</title><categories>cs.CR math.CO</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We say that the sequence $g_n$, $n\ge 3$, $n \rightarrow \infty$ of
polynomial transformation bijective maps of free module $K^n$ over commutative
ring $K$ is a sequence of stable degree if the order of $g_n$ is growing with
$n$ and the degree of each nonidentical polynomial map of kind ${g_n}^k$ is an
independent constant $c$. A transformation $b={\tau}
  {g_n}^k {\tau}^{-1}$, where $\tau$ is affine bijection, $n$ is large and $k$
is relatively small, can be used as a base of group theoretical Diffie-Hellman
key exchange algorithm for the Cremona group $C(K^n)$ of all regular
automorphisms of $K^n$. The specific feature of this method is that the order
of the base may be unknown for the adversary because of the complexity of its
computation. The exchange can be implemented by tools of Computer Algebra
(symbolic computations). The adversary can not use the degree of righthandside
in $b^x=d$ to evaluate unknown $x$ in this form for the discrete logarithm
problem.
  In the paper we introduce the explicit constructions of sequences of elements
of stable degree for cases $c=3$ for each commutative ring $K$ containing at
least 3 regular elements and discuss the implementation of related key exchange
and public key algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2924</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2924</id><created>2013-04-10</created><updated>2015-01-27</updated><authors><author><keyname>Winkler</keyname><forenames>Marco</forenames></author><author><keyname>Reichardt</keyname><forenames>Joerg</forenames></author></authors><title>Motifs in Triadic Random Graphs based on Steiner Triple Systems</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an</categories><comments>13 pages, 10 figures</comments><journal-ref>Physical Review E 88.2 (2013): 022805</journal-ref><doi>10.1103/PhysRevE.88.022805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventionally, pairwise relationships between nodes are considered to be the
fundamental building blocks of complex networks. However, over the last decade
the overabundance of certain sub-network patterns, so called motifs, has
attracted high attention. It has been hypothesized, these motifs, instead of
links, serve as the building blocks of network structures.
  Although the relation between a network's topology and the general properties
of the system, such as its function, its robustness against perturbations, or
its efficiency in spreading information is the central theme of network
science, there is still a lack of sound generative models needed for testing
the functional role of subgraph motifs. Our work aims to overcome this
limitation.
  We employ the framework of exponential random graphs (ERGMs) to define novel
models based on triadic substructures. The fact that only a small portion of
triads can actually be set independently poses a challenge for the formulation
of such models. To overcome this obstacle we use Steiner Triple Systems (STS).
These are partitions of sets of nodes into pair-disjoint triads, which thus can
be specified independently. Combining the concepts of ERGMs and STS, we suggest
novel generative models capable of generating ensembles of networks with
non-trivial triadic Z-score profiles. Further, we discover inevitable
correlations between the abundance of triad patterns, which occur solely for
statistical reasons and need to be taken into account when discussing the
functional implications of motif statistics. Moreover, we calculate the degree
distributions of our triadic random graphs analytically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2936</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2936</id><created>2013-04-10</created><authors><author><keyname>Narayan</keyname><forenames>Chinmay</forenames></author><author><keyname>Guha</keyname><forenames>Shibashis</forenames></author><author><keyname>Arun-Kumar</keyname><forenames>S.</forenames></author></authors><title>Inferring Fences in a Concurrent Program Using SC proof of Correctness</title><categories>cs.LO cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most proof systems for concurrent programs assume the underlying memory model
to be sequentially consistent (SC), an assumption which does not hold for
modern multicore processors. These processors, for performance reasons,
implement relaxed memory models. As a result of this relaxation a program,
proved correct on the SC memory model, might execute incorrectly. To ensure its
correctness under relaxation, fence instructions are inserted in the code. In
this paper we show that the SC proof of correctness of an algorithm, carried
out in the proof system of [Sou84], identifies per-thread instruction orderings
sufficient for this SC proof. Further, to correctly execute this algorithm on
an underlying relaxed memory model it is sufficient to respect only these
orderings by inserting fence instructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2946</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2946</id><created>2013-04-10</created><authors><author><keyname>Zheng</keyname><forenames>Jia</forenames></author><author><keyname>Wu</keyname><forenames>Baofeng</forenames></author><author><keyname>Chen</keyname><forenames>Yufu</forenames></author><author><keyname>Liu</keyname><forenames>Zhuojun</forenames></author></authors><title>Constructing $2m$-variable Boolean functions with optimal algebraic
  immunity based on polar decomposition of $\mathbb{F}_{2^{2m}}^*$</title><categories>cs.CR</categories><comments>20 pages</comments><msc-class>94A60, 94C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructing $2m$-variable Boolean functions with optimal algebraic immunity
based on decomposition of additive group of the finite field
$\mathbb{F}_{2^{2m}}$ seems to be a promising approach since Tu and Deng's
work. In this paper, we consider the same problem in a new way. Based on polar
decomposition of the multiplicative group of $\mathbb{F}_{2^{2m}}$, we propose
a new construction of Boolean functions with optimal algebraic immunity. By a
slight modification of it, we obtain a class of balanced Boolean functions
achieving optimal algebraic immunity, which also have optimal algebraic degree
and high nonlinearity. Computer investigations imply that this class of
functions also behave well against fast algebraic attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2947</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2947</id><created>2013-04-10</created><updated>2015-05-06</updated><authors><author><keyname>Boissonnat</keyname><forenames>Jean-Daniel</forenames><affiliation>INRIA Sophia Antipolis / INRIA Saclay - Ile de France</affiliation></author><author><keyname>Dyer</keyname><forenames>Ramsay</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>The Stability of Delaunay Triangulations</title><categories>cs.CG</categories><comments>Displayed expressions in the statements of Thm 3.14 and Cor 4.18 are
  missing a factor of the dimension (m) in the denominator of the version
  published in IJCGA: it is corrected here</comments><proxy>ccsd</proxy><journal-ref>Int. J. Comput. Geom. Appl. 23, 303 (2013)</journal-ref><doi>10.1142/S0218195913600078</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a parametrized notion of genericity for Delaunay triangulations
which, in particular, implies that the Delaunay simplices of $\delta$-generic
point sets are thick. Equipped with this notion, we study the stability of
Delaunay triangulations under perturbations of the metric and of the vertex
positions. We quantify the magnitude of the perturbations under which the
Delaunay triangulation remains unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2948</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2948</id><created>2013-04-10</created><authors><author><keyname>Nabli</keyname><forenames>Faten</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Fages</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Martinez</keyname><forenames>Thierry</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Soliman</keyname><forenames>Sylvain</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Un mod\`ele bool\'een pour l'\'enum\'eration des siphons et des pi\`eges
  minimaux dans les r\'eseaux de Petri</title><categories>cs.CE cs.LO</categories><comments>JFPC 2012 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Petri-nets are a simple formalism for modeling concurrent computation.
Recently, they have emerged as a powerful tool for the modeling and analysis of
biochemical reaction networks, bridging the gap between purely qualitative and
quantitative models. These networks can be large and complex, which makes their
study difficult and computationally challenging. In this paper, we focus on two
structural properties of Petri-nets, siphons and traps, that bring us
information about the persistence of some molecular species. We present two
methods for enumerating all minimal siphons and traps of a Petri-net by
iterating the resolution of a boolean model interpreted as either a SAT or a
CLP(B) program. We compare the performance of these methods with a
state-of-the-art dedicated algorithm of the Petri-net community. We show that
the SAT and CLP(B) programs are both faster. We analyze why these programs
perform so well on the models of the repository of biological models
biomodels.net, and propose some hard instances for the problem of minimal
siphons enumeration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2959</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2959</id><created>2013-04-10</created><authors><author><keyname>Mousavi</keyname><forenames>Hamoon</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Shortest Repetition-Free Words Accepted by Automata</title><categories>cs.FL cs.DM math.CO</categories><comments>12 pages, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following problem: given that a finite automaton $M$ of $N$
states accepts at least one $k$-power-free (resp., overlap-free) word, what is
the length of the shortest such word accepted? We give upper and lower bounds
which, unfortunately, are widely separated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2974</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2974</id><created>2013-04-10</created><authors><author><keyname>Doerr</keyname><forenames>Christian</forenames></author><author><keyname>Blenn</keyname><forenames>Norbert</forenames></author><author><keyname>Tang</keyname><forenames>Siyu</forenames></author><author><keyname>Van Mieghem</keyname><forenames>Piet</forenames></author></authors><title>Are Friends Overrated? A Study for the Social News Aggregator Digg.com</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Computer Communications 35(7), pp. 796-809, 2012</journal-ref><doi>10.1016/j.comcom.2012.02.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key feature of online social networks (OSN) is the ability of users to
become active, make friends and interact via comments, videos or messages with
those around them. This social interaction is typically perceived as critical
to the proper functioning of these platforms; therefore, a significant share of
OSN research in the recent past has investigated the characteristics and
importance of these social links, studying the networks' friendship relations
through their topological properties, the structure of the resulting
communities and identifying the role and importance of individual members
within these networks.
  In this paper, we present results from a multi-year study of the online
social network Digg.com, indicating that the importance of friends and the
friend network in the propagation of information is less than originally
perceived. While we do note that users form and maintain a social structure
along which information is exchanged, the importance of these links and their
contribution is very low: Users with even a nearly identical overlap in
interests react on average only with a probability of 2% to information
propagated and received from friends. Furthermore, in only about 50% of stories
that became popular from the entire body of 10 million news we find evidence
that the social ties among users were a critical ingredient to the successful
spread. Our findings indicate the presence of previously unconsidered factors,
the temporal alignment between user activities and the existence of additional
logical relationships beyond the topology of the social graph, that are able to
drive and steer the dynamics of such OSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2981</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2981</id><created>2013-04-10</created><updated>2013-04-27</updated><authors><author><keyname>Ullah</keyname><forenames>Sultan</forenames></author><author><keyname>Xuefeng</keyname><forenames>Zheng</forenames></author></authors><title>Cloud Computing: a Prologue</title><categories>cs.DC</categories><comments>04 Pages</comments><journal-ref>International Journal of Advanced Research in Computer and
  Communication Engineering Vol. 1, No. 1, March, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An emerging internet based super computing model is represented by cloud
computing. Cloud computing is the convergence and evolution of several concepts
from virtualization, distributed storage, grid, and automation management to
enable a more flexible approach for deploying and scaling applications.
However, cloud computing moves the application software and databases to the
large data centers, where the management of the data and services may not be
fully trustworthy. The concept of cloud computing on the basis of the various
definitions available in the industry and the characteristics of cloud
computing are being analyzed in this paper. The paper also describes the main
cloud service providers and their products followed by primary cloud computing
operating systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2983</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2983</id><created>2013-04-10</created><authors><author><keyname>An</keyname><forenames>Hyung-Chan</forenames></author><author><keyname>Bhaskara</keyname><forenames>Aditya</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>Centrality of Trees for Capacitated k-Center</title><categories>cs.DS</categories><comments>21 pages, 2 figures</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a large discrepancy in our understanding of uncapacitated and
capacitated versions of network location problems. This is perhaps best
illustrated by the classical k-center problem: there is a simple tight
2-approximation algorithm for the uncapacitated version whereas the first
constant factor approximation algorithm for the general version with capacities
was only recently obtained by using an intricate rounding algorithm that
achieves an approximation guarantee in the hundreds.
  Our paper aims to bridge this discrepancy. For the capacitated k-center
problem, we give a simple algorithm with a clean analysis that allows us to
prove an approximation guarantee of 9. It uses the standard LP relaxation and
comes close to settling the integrality gap (after necessary preprocessing),
which is narrowed down to either 7, 8 or 9. The algorithm proceeds by first
reducing to special tree instances, and then solves such instances optimally.
Our concept of tree instances is quite versatile, and applies to natural
variants of the capacitated k-center problem for which we also obtain improved
algorithms. Finally, we give evidence to show that more powerful preprocessing
could lead to better algorithms, by giving an approximation algorithm that
beats the integrality gap for instances where all non-zero capacities are
uniform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2994</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2994</id><created>2013-04-10</created><updated>2014-07-13</updated><authors><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author></authors><title>A Generalized Online Mirror Descent with Applications to Classification
  and Regression</title><categories>cs.LG</categories><journal-ref>Machine Learning June 2015, Volume 99, Issue 3, pp 411-435</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning algorithms are fast, memory-efficient, easy to implement, and
applicable to many prediction problems, including classification, regression,
and ranking. Several online algorithms were proposed in the past few decades,
some based on additive updates, like the Perceptron, and some on multiplicative
updates, like Winnow. A unifying perspective on the design and the analysis of
online algorithms is provided by online mirror descent, a general prediction
strategy from which most first-order algorithms can be obtained as special
cases. We generalize online mirror descent to time-varying regularizers with
generic updates. Unlike standard mirror descent, our more general formulation
also captures second order algorithms, algorithms for composite losses and
algorithms for adaptive filtering. Moreover, we recover, and sometimes improve,
known regret bounds as special cases of our analysis using specific
regularizers. Finally, we show the power of our approach by deriving a new
second order algorithm with a regret bound invariant with respect to arbitrary
rescalings of individual features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2998</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2998</id><created>2013-04-10</created><updated>2014-06-10</updated><authors><author><keyname>Olhede</keyname><forenames>Sofia</forenames></author><author><keyname>Ram&#xed;rez</keyname><forenames>David</forenames></author><author><keyname>Schreier</keyname><forenames>Peter J.</forenames></author></authors><title>Detecting Directionality in Random Fields Using the Monogenic Signal</title><categories>cs.IT cs.CV math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting and analyzing directional structures in images is important in many
applications since one-dimensional patterns often correspond to important
features such as object contours or trajectories. Classifying a structure as
directional or non-directional requires a measure to quantify the degree of
directionality and a threshold, which needs to be chosen based on the
statistics of the image. In order to do this, we model the image as a random
field. So far, little research has been performed on analyzing directionality
in random fields. In this paper, we propose a measure to quantify the degree of
directionality based on the random monogenic signal, which enables a unique
decomposition of a 2D signal into local amplitude, local orientation, and local
phase. We investigate the second-order statistical properties of the monogenic
signal for isotropic, anisotropic, and unidirectional random fields. We analyze
our measure of directionality for finite-size sample images, and determine a
threshold to distinguish between unidirectional and non-unidirectional random
fields, which allows the automatic classification of images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.2999</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.2999</id><created>2013-04-10</created><updated>2014-01-07</updated><authors><author><keyname>Poling</keyname><forenames>Bryan</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>A New Approach To Two-View Motion Segmentation Using Global Dimension
  Minimization</title><categories>cs.CV</categories><journal-ref>International Journal of Computer Vision, 108 (2014), no. 3,
  165-185</journal-ref><doi>10.1007/s11263-013-0694-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to rigid-body motion segmentation from two views.
We use a previously developed nonlinear embedding of two-view point
correspondences into a 9-dimensional space and identify the different motions
by segmenting lower-dimensional subspaces. In order to overcome nonuniform
distributions along the subspaces, whose dimensions are unknown, we suggest the
novel concept of global dimension and its minimization for clustering subspaces
with some theoretical motivation. We propose a fast projected gradient
algorithm for minimizing global dimension and thus segmenting motions from
2-views. We develop an outlier detection framework around the proposed method,
and we present state-of-the-art results on outlier-free and outlier-corrupted
two-view data for segmenting motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3010</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3010</id><created>2013-04-10</created><updated>2013-10-13</updated><authors><author><keyname>Castillo</keyname><forenames>Carlos</forenames></author><author><keyname>El-Haddad</keyname><forenames>Mohammed</forenames></author><author><keyname>Pfeffer</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Stempeck</keyname><forenames>Matt</forenames></author></authors><title>Characterizing the Life Cycle of Online News Stories Using Social Media
  Reactions</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Computer Supported Cooperative Work and Social Computing (CSCW 2014)</comments><acm-class>H.4.m</acm-class><doi>10.1145/2531602.2531623</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a study of the life cycle of news articles posted online.
We describe the interplay between website visitation patterns and social media
reactions to news content. We show that we can use this hybrid observation
method to characterize distinct classes of articles. We also find that social
media reactions can help predict future visitation patterns early and
accurately.
  We validate our methods using qualitative analysis as well as quantitative
analysis on data from a large international news network, for a set of articles
generating more than 3,000,000 visits and 200,000 social media reactions. We
show that it is possible to model accurately the overall traffic articles will
ultimately receive by observing the first ten to twenty minutes of social media
reactions. Achieving the same prediction accuracy with visits alone would
require to wait for three hours of data. We also describe significant
improvements on the accuracy of the early prediction of shelf-life for news
stories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3013</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3013</id><created>2013-04-10</created><authors><author><keyname>Marcoux</keyname><forenames>Marianne</forenames></author><author><keyname>Lusseau</keyname><forenames>David</forenames></author></authors><title>The influence of repressive legislation on the structure of a social
  media network</title><categories>physics.soc-ph cs.SI</categories><comments>4 pages, 4 figures, submitted to EPL</comments><msc-class>91D30, 91F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media have been widely used to organize citizen movements. In 2012,
75% university and college students in Quebec, Canada, participated in mass
protests against an increase in tuition fees, mainly organized using social
media. To reduce public disruption, the government introduced special
legislation designed to impede protest organization. Here, we show that the
legislation changed the behaviour of social media users but not the overall
structure of their social network on Twitter. Thus, users were still able to
spread information to efficiently organize demonstrations using their social
network. This natural experiment shows the power of social media in political
mobilization, as well as behavioural flexibility in information flow over a
large number of individuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3016</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3016</id><created>2013-04-10</created><updated>2014-08-15</updated><authors><author><keyname>Kasparick</keyname><forenames>Martin</forenames></author><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author></authors><title>Autonomous Algorithms for Centralized and Distributed Interference
  Coordination: A Virtual Layer Based Approach</title><categories>cs.IT math.IT</categories><comments>revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference mitigation techniques are essential for improving the
performance of interference limited wireless networks. In this paper, we
introduce novel interference mitigation schemes for wireless cellular networks
with space division multiple access (SDMA). The schemes are based on a virtual
layer that captures and simplifies the complicated interference situation in
the network and that is used for power control. We show how optimization in
this virtual layer generates gradually adapting power control settings that
lead to autonomous interference minimization. Thereby, the granularity of
control ranges from controlling frequency sub-band power via controlling the
power on a per-beam basis, to a granularity of only enforcing average power
constraints per beam. In conjunction with suitable short-term scheduling, our
algorithms gradually steer the network towards a higher utility. We use
extensive system-level simulations to compare three distributed algorithms and
evaluate their applicability for different user mobility assumptions. In
particular, it turns out that larger gains can be achieved by imposing average
power constraints and allowing opportunistic scheduling instantaneously, rather
than controlling the power in a strict way. Furthermore, we introduce a
centralized algorithm, which directly solves the underlying optimization and
shows fast convergence, as a performance benchmark for the distributed
solutions. Moreover, we investigate the deviation from global optimality by
comparing to a branch-and-bound-based solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3056</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3056</id><created>2013-04-10</created><authors><author><keyname>Sadr</keyname><forenames>Sanam</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author></authors><title>Anticipatory Buffer Control and Resource Allocation for Wireless Video
  Streaming</title><categories>cs.MM cs.NI cs.SY</categories><comments>5 pages, submitted to IEEE Globecom 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new approach for allocating resources to video
streaming traffic. Assuming that the future channel state can be predicted for
a certain time, we minimize the fraction of the bandwidth consumed for smooth
streaming by jointly allocating wireless channel resources and play-out buffer
size. To formalize this idea, we introduce a new model to capture the dynamic
of a video streaming buffer and the allocated spectrum in an optimization
problem. The result is a Linear Program that allows to trade off buffer size
and allocated bandwidth. Based on this tractable model, our simulation results
show that anticipating poor channel states and pre-loading the buffer
accordingly allows to serve more users at perfect video quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3059</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3059</id><created>2013-04-10</created><authors><author><keyname>Abdulla</keyname><forenames>Mouhamed</forenames></author><author><keyname>Shayan</keyname><forenames>Yousef R.</forenames></author></authors><title>Simple and Generic Simulator Algorithm for Inhomogeneous Random Spatial
  Deployment</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conceptualized a straightforward and flexible approach for random spatial
inhomogeneity by proposing the area-specific deployment (ASD) algorithm, which
takes into account the clustering tendency of users. In fact, the ASD method
has the advantage of achieving a more realistic heterogeneous deployment based
on limited planning inputs, while still preserving the stochastic character of
users position. We then applied this technique to different circumstances, and
developed spatial-level network algorithms for controlled and uncontrolled
cellular network deployments. Overall, the derived simulator tools will
effectively and easily be useful for designers and deployment planners modeling
a host of multi-coverage and multi-scale wireless network situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3063</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3063</id><created>2013-04-10</created><authors><author><keyname>Tanaka</keyname><forenames>Takashi</forenames></author><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Langbort</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>A Faithful Distributed Implementation of Dual Decomposition and Average
  Consensus Algorithms</title><categories>math.OC cs.GT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider large scale cost allocation problems and consensus seeking
problems for multiple agents, in which agents are suggested to collaborate in a
distributed algorithm to find a solution. If agents are strategic to minimize
their own individual cost rather than the global social cost, they are endowed
with an incentive not to follow the intended algorithm, unless the tax/subsidy
mechanism is carefully designed. Inspired by the classical
Vickrey-Clarke-Groves mechanism and more recent algorithmic mechanism design
theory, we propose a tax mechanism that incentivises agents to faithfully
implement the intended algorithm. In particular, a new notion of asymptotic
incentive compatibility is introduced to characterize a desirable property of
such class of mechanisms. The proposed class of tax mechanisms provides a
sequence of mechanisms that gives agents a diminishing incentive to deviate
from suggested algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3071</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3071</id><created>2013-04-10</created><updated>2014-05-02</updated><authors><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>Minimal Controllability Problems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a linear system, we consider the problem of finding a small set of
variables to affect with an input so that the resulting system is controllable.
We show that this problem is NP-hard; indeed, we show that even approximating
the minimum number of variables that need to be affected within a
multiplicative factor of $c \log n$ is NP-hard for some positive $c$. On the
positive side, we show it is possible to find sets of variables matching this
inapproximability barrier in polynomial time. This can be done by a simple
greedy heuristic which sequentially picks variables to maximize the rank
increase of the controllability matrix. Experiments on Erdos-Renyi random
graphs demonstrate this heuristic almost always succeeds at findings the
minimum number of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3075</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3075</id><created>2013-03-27</created><authors><author><keyname>Abel</keyname><forenames>Shoshana</forenames></author></authors><title>Application of Evidential Reasoning to Helicopter Flight Path Control</title><categories>cs.AI cs.RO cs.SY</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-1-6</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a methodology for research and development of the
inferencing and knowledge representation aspects of an Expert System approach
for performing reasoning under uncertainty in support of a real time vehicle
guidance and navigation system. Such a system could be of major benefit for
non-terrain following low altitude flight systems operating in foreign hostile
environments such as might be experienced by NOE helicopter or similar mission
craft. An innovative extension of the evidential reasoning methodology, termed
the Sum-and-Lattice-Points Method, has been developed. The research and
development effort presented in this paper consists of a formal mathematical
development of the Sum-and-Lattice-Points Method, its formulation and
representation in a parallel environment, prototype software development of the
method within an expert system, and initial testing of the system within the
confines of the vehicle guidance system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3076</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3076</id><created>2013-03-27</created><authors><author><keyname>Barth</keyname><forenames>Stephen W.</forenames></author><author><keyname>Norton</keyname><forenames>Steven W.</forenames></author></authors><title>Knowledge Engineering Within A Generalized Bayesian Framework</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-7-16</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the ongoing debate over the representation of uncertainty in
Artificial Intelligence, Cheeseman, Lemmer, Pearl, and others have argued that
probability theory, and in particular the Bayesian theory, should be used as
the basis for the inference mechanisms of Expert Systems dealing with
uncertainty. In order to pursue the issue in a practical setting, sophisticated
tools for knowledge engineering are needed that allow flexible and
understandable interaction with the underlying knowledge representation
schemes. This paper describes a Generalized Bayesian framework for building
expert systems which function in uncertain domains, using algorithms proposed
by Lemmer. It is neither rule-based nor frame-based, and requires a new system
of knowledge engineering tools. The framework we describe provides a
knowledge-based system architecture with an inference engine, explanation
capability, and a unique aid for building consistent knowledge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3077</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3077</id><created>2013-03-27</created><authors><author><keyname>Ben-Bassat</keyname><forenames>Moshe</forenames></author></authors><title>Taxonomy, Structure, and Implementation of Evidential Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-17-28</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental elements of evidential reasoning problems are described,
followed by a discussion of the structure of various types of problems.
Bayesian inference networks and state space formalism are used as the tool for
problem representation.
  A human-oriented decision making cycle for solving evidential reasoning
problems is described and illustrated for a military situation assessment
problem. The implementation of this cycle may serve as the basis for an expert
system shell for evidential reasoning; i.e. a situation assessment processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3078</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3078</id><created>2013-03-27</created><authors><author><keyname>Booker</keyname><forenames>Lashon B.</forenames></author><author><keyname>Hota</keyname><forenames>Naveen</forenames></author></authors><title>Probabilistic Reasoning About Ship Images</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-29-36</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important aspects of current expert systems technology is the
ability to make causal inferences about the impact of new evidence. When the
domain knowledge and problem knowledge are uncertain and incomplete Bayesian
reasoning has proven to be an effective way of forming such inferences [3,4,8].
While several reasoning schemes have been developed based on Bayes Rule, there
has been very little work examining the comparative effectiveness of these
schemes in a real application. This paper describes a knowledge based system
for ship classification [1], originally developed using the PROSPECTOR updating
method [2], that has been reimplemented to use the inference procedure
developed by Pearl and Kim [4,5]. We discuss our reasons for making this
change, the implementation of the new inference engine, and the comparative
performance of the two versions of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3079</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3079</id><created>2013-03-27</created><authors><author><keyname>Chen</keyname><forenames>Kaihu</forenames></author></authors><title>Towards The Inductive Acquisition of Temporal Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-37-42</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to predict the future in a given domain can be acquired by
discovering empirically from experience certain temporal patterns that tend to
repeat unerringly. Previous works in time series analysis allow one to make
quantitative predictions on the likely values of certain linear variables.
Since certain types of knowledge are better expressed in symbolic forms, making
qualitative predictions based on symbolic representations require a different
approach. A domain independent methodology called TIM (Time based Inductive
Machine) for discovering potentially uncertain temporal patterns from real time
observations using the technique of inductive inference is described here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3080</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3080</id><created>2013-03-27</created><authors><author><keyname>Chen</keyname><forenames>Su-shing</forenames></author></authors><title>Some Extensions of Probabilistic Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-43-48</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [12], Nilsson proposed the probabilistic logic in which the truth values
of logical propositions are probability values between 0 and 1. It is
applicable to any logical system for which the consistency of a finite set of
propositions can be established. The probabilistic inference scheme reduces to
the ordinary logical inference when the probabilities of all propositions are
either 0 or 1. This logic has the same limitations of other probabilistic
reasoning systems of the Bayesian approach. For common sense reasoning,
consistency is not a very natural assumption. We have some well known examples:
{Dick is a Quaker, Quakers are pacifists, Republicans are not pacifists, Dick
is a Republican}and {Tweety is a bird, birds can fly, Tweety is a penguin}. In
this paper, we shall propose some extensions of the probabilistic logic. In the
second section, we shall consider the space of all interpretations, consistent
or not. In terms of frames of discernment, the basic probability assignment
(bpa) and belief function can be defined. Dempster's combination rule is
applicable. This extension of probabilistic logic is called the evidential
logic in [ 1]. For each proposition s, its belief function is represented by an
interval [Spt(s), Pls(s)]. When all such intervals collapse to single points,
the evidential logic reduces to probabilistic logic (in the generalized version
of not necessarily consistent interpretations). Certainly, we get Nilsson's
probabilistic logic by further restricting to consistent interpretations. In
the third section, we shall give a probabilistic interpretation of
probabilistic logic in terms of multi-dimensional random variables. This
interpretation brings the probabilistic logic into the framework of probability
theory. Let us consider a finite set S = {sl, s2, ..., Sn) of logical
propositions. Each proposition may have true or false values; and may be
considered as a random variable. We have a probability distribution for each
proposition. The e-dimensional random variable (sl,..., Sn) may take values in
the space of all interpretations of 2n binary vectors. We may compute absolute
(marginal), conditional and joint probability distributions. It turns out that
the permissible probabilistic interpretation vector of Nilsson [12] consists of
the joint probabilities of S. Inconsistent interpretations will not appear, by
setting their joint probabilities to be zeros. By summing appropriate joint
probabilities, we get probabilities of individual propositions or subsets of
propositions. Since the Bayes formula and other techniques are valid for
e-dimensional random variables, the probabilistic logic is actually very close
to the Bayesian inference schemes. In the last section, we shall consider a
relaxation scheme for probabilistic logic. In this system, not only new
evidences will update the belief measures of a collection of propositions, but
also constraint satisfaction among these propositions in the relational network
will revise these measures. This mechanism is similar to human reasoning which
is an evaluative process converging to the most satisfactory result. The main
idea arises from the consistent labeling problem in computer vision. This
method is originally applied to scene analysis of line drawings. Later, it is
applied to matching, constraint satisfaction and multi sensor fusion by several
authors [8], [16] (and see references cited there). Recently, this method is
used in knowledge aggregation by Landy and Hummel [9].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3081</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3081</id><created>2013-03-27</created><authors><author><keyname>Chi</keyname><forenames>Ping-Chung</forenames></author><author><keyname>Nau</keyname><forenames>Dana</forenames></author></authors><title>Predicting The Performance of Minimax and Product in Game-Tree</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-49-56</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discovery that the minimax decision rule performs poorly in some games
has sparked interest in possible alternatives to minimax. Until recently, the
only games in which minimax was known to perform poorly were games which were
mainly of theoretical interest. However, this paper reports results showing
poor performance of minimax in a more common game called kalah. For the kalah
games tested, a non-minimax decision rule called the product rule performs
significantly better than minimax.
  This paper also discusses a possible way to predict whether or not minimax
will perform well in a game when compared to product. A parameter called the
rate of heuristic flaw (rhf) has been found to correlate positively with the.
performance of product against minimax. Both analytical and experimental
results are given that appear to support the predictive power of rhf.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3082</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3082</id><created>2013-03-27</created><authors><author><keyname>Craddock</keyname><forenames>A. Julian</forenames></author><author><keyname>Browse</keyname><forenames>Roger A.</forenames></author></authors><title>Reasoning With Uncertain Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-57-62</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model of knowledge representation is described in which propositional facts
and the relationships among them can be supported by other facts. The set of
knowledge which can be supported is called the set of cognitive units, each
having associated descriptions of their explicit and implicit support
structures, summarizing belief and reliability of belief. This summary is
precise enough to be useful in a computational model while remaining
descriptive of the underlying symbolic support structure. When a fact supports
another supportive relationship between facts we call this meta-support. This
facilitates reasoning about both the propositional knowledge. and the support
structures underlying it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3083</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3083</id><created>2013-03-27</created><authors><author><keyname>Dalkey</keyname><forenames>Norman C.</forenames></author></authors><title>Models vs. Inductive Inference for Dealing With Probabilistic Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-63-70</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two different approaches to dealing with probabilistic knowledge are examined
-models and inductive inference. Examples of the first are: influence diagrams
[1], Bayesian networks [2], log-linear models [3, 4]. Examples of the second
are: games-against nature [5, 6] varieties of maximum-entropy methods [7, 8,
9], and the author's min-score induction [10]. In the modeling approach, the
basic issue is manageability, with respect to data elicitation and computation.
Thus, it is assumed that the pertinent set of users in some sense knows the
relevant probabilities, and the problem is to format that knowledge in a way
that is convenient to input and store and that allows computation of the
answers to current questions in an expeditious fashion. The basic issue for the
inductive approach appears at first sight to be very different. In this
approach it is presumed that the relevant probabilities are only partially
known, and the problem is to extend that incomplete information in a reasonable
way to answer current questions. Clearly, this approach requires that some form
of induction be invoked. Of course, manageability is an important additional
concern. Despite their seeming differences, the two approaches have a fair
amount in common, especially with respect to the structural framework they
employ. Roughly speaking, this framework involves identifying clusters of
variables which strongly interact, establishing marginal probability
distributions on the clusters, and extending the subdistributions to a more
complete distribution, usually via a product formalism. The product extension
is justified on the modeling approach in terms of assumed conditional
independence; in the inductive approach the product form arises from an
inductive rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3084</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3084</id><created>2013-03-27</created><authors><author><keyname>Falkenhainer</keyname><forenames>Brian</forenames></author></authors><title>Towards a General-Purpose Belief Maintenance System</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-71-76</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There currently exists a gap between the theories proposed by the probability
and uncertainty and the needs of Artificial Intelligence research. These
theories primarily address the needs of expert systems, using knowledge
structures which must be pre-compiled and remain static in structure during
runtime. Many Al systems require the ability to dynamically add and remove
parts of the current knowledge structure (e.g., in order to examine what the
world would be like for different causal theories). This requires more
flexibility than existing uncertainty systems display. In addition, many Al
researchers are only interested in using &quot;probabilities&quot; as a means of
obtaining an ordering, rather than attempting to derive an accurate
probabilistic account of a situation. This indicates the need for systems which
stress ease of use and don't require extensive probability information when one
cannot (or doesn't wish to) provide such information. This paper attempts to
help reconcile the gap between approaches to uncertainty and the needs of many
AI systems by examining the control issues which arise, independent of a
particular uncertainty calculus. when one tries to satisfy these needs. Truth
Maintenance Systems have been used extensively in problem solving tasks to help
organize a set of facts and detect inconsistencies in the believed state of the
world. These systems maintain a set of true/false propositions and their
associated dependencies. However, situations often arise in which we are unsure
of certain facts or in which the conclusions we can draw from available
information are somewhat uncertain. The non-monotonic TMS 12] was an attempt at
reasoning when all the facts are not known, but it fails to take into account
degrees of belief and how available evidence can combine to strengthen a
particular belief. This paper addresses the problem of probabilistic reasoning
as it applies to Truth Maintenance Systems. It describes a belief Maintenance
System that manages a current set of beliefs in much the same way that a TMS
manages a set of true/false propositions. If the system knows that belief in
fact is dependent in some way upon belief in fact2, then it automatically
modifies its belief in facts when new information causes a change in belief of
fact2. It models the behavior of a TMS, replacing its 3-valued logic (true,
false, unknown) with an infinite valued logic, in such a way as to reduce to a
standard TMS if all statements are given in absolute true/false terms. Belief
Maintenance Systems can, therefore, be thought of as a generalization of Truth
Maintenance Systems, whose possible reasoning tasks are a superset of those for
a TMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3085</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3085</id><created>2013-03-27</created><authors><author><keyname>Fox</keyname><forenames>B. R.</forenames></author><author><keyname>Kempf</keyname><forenames>Karl G.</forenames></author></authors><title>Planning, Scheduling, and Uncertainty in the Sequence of Future Events</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-77-84</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scheduling in the factory setting is compounded by computational complexity
and temporal uncertainty. Together, these two factors guarantee that the
process of constructing an optimal schedule will be costly and the chances of
executing that schedule will be slight. Temporal uncertainty in the task
execution time can be offset by several methods: eliminate uncertainty by
careful engineering, restore certainty whenever it is lost, reduce the
uncertainty by using more accurate sensors, and quantify and circumscribe the
remaining uncertainty. Unfortunately, these methods focus exclusively on the
sources of uncertainty and fail to apply knowledge of the tasks which are to be
scheduled. A complete solution must adapt the schedule of activities to be
performed according to the evolving state of the production world. The example
of vision-directed assembly is presented to illustrate that the principle of
least commitment, in the creation of a plan, in the representation of a
schedule, and in the execution of a schedule, enables a robot to operate
intelligently and efficiently, even in the presence of considerable uncertainty
in the sequence of future events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3086</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3086</id><created>2013-03-27</created><authors><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>Deriving And Combining Continuous Possibility Functions in the Framework
  of Evidential Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-85-90</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To develop an approach to utilizing continuous statistical information within
the Dempster- Shafer framework, we combine methods proposed by Strat and by
Shafero We first derive continuous possibility and mass functions from
probability-density functions. Then we propose a rule for combining such
evidence that is simpler and more efficiently computed than Dempster's rule. We
discuss the relationship between Dempster's rule and our proposed rule for
combining evidence over continuous frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3087</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3087</id><created>2013-03-27</created><authors><author><keyname>Grosof</keyname><forenames>Benjamin N.</forenames></author></authors><title>Non-Monotonicity in Probabilistic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-91-98</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We start by defining an approach to non-monotonic probabilistic reasoning in
terms of non-monotonic categorical (true-false) reasoning. We identify a type
of non-monotonic probabilistic reasoning, akin to default inheritance, that is
commonly found in practice, especially in &quot;evidential&quot; and &quot;Bayesian&quot;
reasoning. We formulate this in terms of the Maximization of Conditional
Independence (MCI), and identify a variety of applications for this sort of
default. We propose a formalization using Pointwise Circumscription. We compare
MCI to Maximum Entropy, another kind of non-monotonic principle, and conclude
by raising a number of open questions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3088</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3088</id><created>2013-03-27</created><authors><author><keyname>Hager</keyname><forenames>Greg</forenames></author><author><keyname>Durrant-Whyte</keyname><forenames>Hugh F.</forenames></author></authors><title>Information and Multi-Sensor Coordination</title><categories>cs.SY cs.AI cs.MA</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-99-108</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The control and integration of distributed, multi-sensor perceptual systems
is a complex and challenging problem. The observations or opinions of different
sensors are often disparate incomparable and are usually only partial views.
Sensor information is inherently uncertain and in addition the individual
sensors may themselves be in error with respect to the system as a whole. The
successful operation of a multi-sensor system must account for this uncertainty
and provide for the aggregation of disparate information in an intelligent and
robust manner. We consider the sensors of a multi-sensor system to be members
or agents of a team, able to offer opinions and bargain in group decisions. We
will analyze the coordination and control of this structure using a theory of
team decision-making. We present some new analytic results on multi-sensor
aggregation and detail a simulation which we use to investigate our ideas. This
simulation provides a basis for the analysis of complex agent structures
cooperating in the presence of uncertainty. The results of this study are
discussed with reference to multi-sensor robot systems, distributed Al and
decision making under uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3089</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3089</id><created>2013-03-27</created><authors><author><keyname>Hardt</keyname><forenames>Shohara L.</forenames></author></authors><title>Flexible Interpretations: A Computational Model for Dynamic Uncertainty
  Assessment</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-109-114</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The investigations reported in this paper center on the process of dynamic
uncertainty assessment during interpretation tasks in real domain. In
particular, we are interested here in the nature of the control structure of
computer programs that can support multiple interpretation and smooth
transitions between them, in real time. Each step of the processing involves
the interpretation of one input item and the appropriate re-establishment of
the system's confidence of the correctness of its interpretation(s).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3090</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3090</id><created>2013-03-27</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>The Myth of Modularity in Rule-Based Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-115-122</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine the concept of modularity, an often cited advantage
of the ruled-based representation methodology. We argue that the notion of
modularity consists of two distinct concepts which we call syntactic modularity
and semantic modularity. We argue that when reasoning under certainty, it is
reasonable to regard the rule-based approach as both syntactically and
semantically modular. However, we argue that in the case of plausible
reasoning, rules are syntactically modular but are rarely semantically modular.
To illustrate this point, we examine a particular approach for managing
uncertainty in rule-based systems called the MYCIN certainty factor model. We
formally define the concept of semantic modularity with respect to the
certainty factor model and discuss logical consequences of the definition. We
show that the assumption of semantic modularity imposes strong restrictions on
rules in a knowledge base. We argue that such restrictions are rarely valid in
practical applications. Finally, we suggest how the concept of semantic
modularity can be relaxed in a manner that makes it appropriate for plausible
reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3091</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3091</id><created>2013-03-27</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>An Axiomatic Framework for Belief Updates</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-123-128</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the 1940's, a physicist named Cox provided the first formal justification
for the axioms of probability based on the subjective or Bayesian
interpretation. He showed that if a measure of belief satisfies several
fundamental properties, then the measure must be some monotonic transformation
of a probability. In this paper, measures of change in belief or belief updates
are examined. In the spirit of Cox, properties for a measure of change in
belief are enumerated. It is shown that if a measure satisfies these
properties, it must satisfy other restrictive conditions. For example, it is
shown that belief updates in a probabilistic context must be equal to some
monotonic transformation of a likelihood ratio. It is hoped that this formal
explication of the belief update paradigm will facilitate critical discussion
and useful extensions of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3092</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3092</id><created>2013-03-27</created><authors><author><keyname>Henkind</keyname><forenames>Steven J.</forenames></author></authors><title>Imprecise Meanings as a Cause of Uncertainty in Medical Knowledge-Based
  Systems</title><categories>cs.AI cs.CL</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-129-134</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a considerable amount of work on uncertainty in
knowledge-based systems. This work has generally been concerned with
uncertainty arising from the strength of inferences and the weight of evidence.
In this paper we discuss another type of uncertainty: that which is due to
imprecision in the underlying primitives used to represent the knowledge of the
system. In particular, a given word may denote many similar but not identical
entities. Such words are said to be lexically imprecise. Lexical imprecision
has caused widespread problems in many areas. Unless this phenomenon is
recognized and appropriately handled, it can degrade the performance of
knowledge-based systems. In particular, it can lead to difficulties with the
user interface, and with the inferencing processes of these systems. Some
techniques are suggested for coping with this phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3093</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3093</id><created>2013-03-27</created><authors><author><keyname>Hummel</keyname><forenames>Robert</forenames></author><author><keyname>Landy</keyname><forenames>Michael</forenames></author></authors><title>Evidence as Opinions of Experts</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-135-144</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a viewpoint on the Dempster/Shafer 'Theory of Evidence', and
provide an interpretation which regards the combination formulas as statistics
of the opinions of &quot;experts&quot;. This is done by introducing spaces with binary
operations that are simpler to interpret or simpler to implement than the
standard combination formula, and showing that these spaces can be mapped
homomorphically onto the Dempster/Shafer theory of evidence space. The experts
in the space of &quot;opinions of experts&quot; combine information in a Bayesian
fashion. We present alternative spaces for the combination of evidence
suggested by this viewpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3094</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3094</id><created>2013-03-27</created><authors><author><keyname>Kalme</keyname><forenames>Charles I.</forenames></author></authors><title>Decision Under Uncertainty in Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-145-150</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the incorporation of uncertainty in diagnostic reasoning
based on the set covering model of Reggia et. al. extended to what in the
Artificial Intelligence dichotomy between deep and compiled (shallow, surface)
knowledge based diagnosis may be viewed as the generic form at the compiled end
of the spectrum. A major undercurrent in this is advocating the need for a
strong underlying model and an integrated set of support tools for carrying
such a model in order to deal with uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3095</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3095</id><created>2013-03-27</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Knowledge and Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-151-158</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One purpose -- quite a few thinkers would say the main purpose -- of seeking
knowledge about the world is to enhance our ability to make good decisions. An
item of knowledge that can make no conceivable difference with regard to
anything we might do would strike many as frivolous. Whether or not we want to
be philosophical pragmatists in this strong sense with regard to everything we
might want to enquire about, it seems a perfectly appropriate attitude to adopt
toward artificial knowledge systems. If is granted that we are ultimately
concerned with decisions, then some constraints are imposed on our measures of
uncertainty at the level of decision making. If our measure of uncertainty is
real-valued, then it isn't hard to show that it must satisfy the classical
probability axioms. For example, if an act has a real-valued utility U(E) if
the event E obtains, and the same real-valued utility if the denial of E
obtains, so that U(E) = U(-E), then the expected utility of that act must be
U(E), and that must be the same as the uncertainty-weighted average of the
returns of the act, p-U(E) + q-U('E), where p and q represent the uncertainty
of E and-E respectively. But then we must have p + q = 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3096</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3096</id><created>2013-03-27</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author><author><keyname>Cohen</keyname><forenames>Marvin S.</forenames></author></authors><title>An Application of Non-Monotonic Probabilistic Reasoning to Air Force
  Threat Correlation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-159-166</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current approaches to expert systems' reasoning under uncertainty fail to
capture the iterative revision process characteristic of intelligent human
reasoning. This paper reports on a system, called the Non-monotonic
Probabilist, or NMP (Cohen, et al., 1985). When its inferences result in
substantial conflict, NMP examines and revises the assumptions underlying the
inferences until conflict is reduced to acceptable levels. NMP has been
implemented in a demonstration computer-based system, described below, which
supports threat correlation and in-flight route replanning by Air Force pilots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3097</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3097</id><created>2013-03-27</created><authors><author><keyname>Levitt</keyname><forenames>Tod S.</forenames></author></authors><title>Bayesian Inference for Radar Imagery Based Surveillance</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-167-174</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in creating an automated or semi-automated system with the
capability of taking a set of radar imagery, collection parameters and a priori
map and other tactical data, and producing likely interpretations of the
possible military situations given the available evidence. This paper is
concerned with the problem of the interpretation and computation of certainty
or belief in the conclusions reached by such a system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3098</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3098</id><created>2013-03-27</created><authors><author><keyname>Li</keyname><forenames>Ze-Nian</forenames></author><author><keyname>Uhr</keyname><forenames>Leonard</forenames></author></authors><title>Evidential Reasoning in Parallel Hierarchical Vision Programs</title><categories>cs.AI cs.CV</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-175-182</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an efficient adaptation and application of the
Dempster-Shafer theory of evidence, one that can be used effectively in a
massively parallel hierarchical system for visual pattern perception. It
describes the techniques used, and shows in an extended example how they serve
to improve the system's performance as it applies a multiple-level set of
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3099</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3099</id><created>2013-03-27</created><authors><author><keyname>Loui</keyname><forenames>Ronald P.</forenames></author></authors><title>Computing Reference Classes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-183-188</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any system with limited statistical knowledge, the combination of
evidence and the interpretation of sampling information require the
determination of the right reference class (or of an adequate one). The present
note (1) discusses the use of reference classes in evidential reasoning, and
(2) discusses implementations of Kyburg's rules for reference classes. This
paper contributes the first frank discussion of how much of Kyburg's system is
needed to be powerful, how much can be computed effectively, and how much is
philosophical fat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3100</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3100</id><created>2013-03-27</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Uttam</forenames></author></authors><title>An Uncertainty Management Calculus for Ordering Searches in Distributed
  Dynamic Databases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-189-192</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MINDS is a distributed system of cooperating query engines that customize,
document retrieval for each user in a dynamic environment. It improves its
performance and adapts to changing patterns of document distribution by
observing system-user interactions and modifying the appropriate certainty
factors, which act as search control parameters. It argued here that the
uncertainty management calculus must account for temporal precedence,
reliability of evidence, degree of support for a proposition, and saturation
effects. The calculus presented here possesses these features. Some results
obtained with this scheme are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3101</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3101</id><created>2013-03-27</created><authors><author><keyname>Norton</keyname><forenames>Steven W.</forenames></author></authors><title>An Explanation Mechanism for Bayesian Inferencing Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-193-200</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explanation facilities are a particularly important feature of expert system
frameworks. It is an area in which traditional rule-based expert system
frameworks have had mixed results. While explanations about control are well
handled, facilities are needed for generating better explanations concerning
knowledge base content. This paper approaches the explanation problem by
examining the effect an event has on a variable of interest within a symmetric
Bayesian inferencing system. We argue that any effect measure operating in this
context must satisfy certain properties. Such a measure is proposed. It forms
the basis for an explanation facility which allows the user of the Generalized
Bayesian Inferencing System to question the meaning of the knowledge base. That
facility is described in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3102</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3102</id><created>2013-03-27</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Distributed Revision of Belief Commitment in Multi-Hypothesis
  Interpretations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-201-210</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the applications of belief-networks to include the
revision of belief commitments, i.e., the categorical acceptance of a subset of
hypotheses which, together, constitute the most satisfactory explanation of the
evidence at hand. A coherent model of non-monotonic reasoning is established
and distributed algorithms for belief revision are presented. We show that, in
singly connected networks, the most satisfactory explanation can be found in
linear time by a message-passing algorithm similar to the one used in belief
updating. In multiply-connected networks, the problem may be exponentially hard
but, if the network is sparse, topological considerations can be used to render
the interpretation task tractable. In general, finding the most probable
combination of hypotheses is no more complex than computing the degree of
belief for any individual hypothesis. Applications to medical diagnosis are
illustrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3103</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3103</id><created>2013-03-27</created><authors><author><keyname>Roizer</keyname><forenames>Igor</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Learning Link-Probabilities in Causal Trees</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-211-214</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A learning algorithm is presented which given the structure of a causal tree,
will estimate its link probabilities by sequential measurements on the leaves
only. Internal nodes of the tree represent conceptual (hidden) variables
inaccessible to observation. The method described is incremental, local,
efficient, and remains robust to measurement imprecisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3104</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3104</id><created>2013-03-27</created><authors><author><keyname>Ruspini</keyname><forenames>Enrique H.</forenames></author></authors><title>Approximate Deduction in Single Evidential Bodies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-215-222</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Results on approximate deduction in the context of the calculus of evidence
of Dempster-Shafer and the theory of interval probabilities are reported.
Approximate conditional knowledge about the truth of conditional propositions
was assumed available and expressed as sets of possible values (actually
numeric intervals) of conditional probabilities. Under different
interpretations of this conditional knowledge, several formulas were produced
to integrate unconditioned estimates (assumed given as sets of possible values
of unconditioned probabilities) with conditional estimates. These formulas are
discussed together with the computational characteristics of the methods
derived from them. Of particular importance is one such evidence integration
formulation, produced under a belief oriented interpretation, which
incorporates both modus ponens and modus tollens inferential mechanisms, allows
integration of conditioned and unconditioned knowledge without resorting to
iterative or sequential approximations, and produces elementary mass
distributions as outputs using similar distributions as inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3105</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3105</id><created>2013-03-27</created><authors><author><keyname>Schocken</keyname><forenames>Shimon</forenames></author></authors><title>The Rational and Computational Scope of Probabilistic Rule-Based Expert
  Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-223-228</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief updating schemes in artificial intelligence may be viewed as three
dimensional languages, consisting of a syntax (e.g. probabilities or certainty
factors), a calculus (e.g. Bayesian or CF combination rules), and a semantics
(i.e. cognitive interpretations of competing formalisms). This paper studies
the rational scope of those languages on the syntax and calculus grounds. In
particular, the paper presents an endomorphism theorem which highlights the
limitations imposed by the conditional independence assumptions implicit in the
CF calculus. Implications of the theorem to the relationship between the CF and
the Bayesian languages and the Dempster-Shafer theory of evidence are
presented. The paper concludes with a discussion of some implications on
rule-based knowledge engineering in uncertain domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3106</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3106</id><created>2013-03-27</created><authors><author><keyname>Schwartz</keyname><forenames>Stanley M.</forenames></author><author><keyname>Baron</keyname><forenames>Jonathan</forenames></author><author><keyname>Clarke</keyname><forenames>John R.</forenames></author></authors><title>A Causal Bayesian Model for the Diagnosis of Appendicitis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-229-236</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The causal Bayesian approach is based on the assumption that effects (e.g.,
symptoms) that are not conditionally independent with respect to some causal
agent (e.g., a disease) are conditionally independent with respect to some
intermediate state caused by the agent, (e.g., a pathological condition). This
paper describes the development of a causal Bayesian model for the diagnosis of
appendicitis. The paper begins with a description of the standard Bayesian
approach to reasoning about uncertainty and the major critiques it faces. The
paper then lays the theoretical groundwork for the causal extension of the
Bayesian approach, and details specific improvements we have developed. The
paper then goes on to describe our knowledge engineering and implementation and
the results of a test of the system. The paper concludes with a discussion of
how the causal Bayesian approach deals with the criticisms of the standard
Bayesian model and why it is superior to alternative approaches to reasoning
about uncertainty popular in the Al community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3107</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3107</id><created>2013-03-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>A Backwards View for Assessment</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-237-242</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much artificial intelligence research focuses on the problem of deducing the
validity of unobservable propositions or hypotheses from observable evidence.!
Many of the knowledge representation techniques designed for this problem
encode the relationship between evidence and hypothesis in a directed manner.
Moreover, the direction in which evidence is stored is typically from evidence
to hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3108</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3108</id><created>2013-03-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>DAVID: Influence Diagram Processing System for the Macintosh</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-243-248</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence diagrams are a directed graph representation for uncertainties as
probabilities. The graph distinguishes between those variables which are under
the control of a decision maker (decisions, shown as rectangles) and those
which are not (chances, shown as ovals), as well as explicitly denoting a goal
for solution (value, shown as a rounded rectangle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3109</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3109</id><created>2013-03-27</created><authors><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author><author><keyname>Shafer</keyname><forenames>Glenn</forenames></author><author><keyname>Mellouli</keyname><forenames>Khaled</forenames></author></authors><title>Propagation of Belief Functions: A Distributed Approach</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-249-260</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a scheme for propagating belief functions in
certain kinds of trees using only local computations. This scheme generalizes
the computational scheme proposed by Shafer and Logan1 for diagnostic trees of
the type studied by Gordon and Shortliffe, and the slightly more general scheme
given by Shafer for hierarchical evidence. It also generalizes the scheme
proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer). Pearl's
causal trees and Gordon and Shortliffe's diagnostic trees are both ways of
breaking the evidence that bears on a large problem down into smaller items of
evidence that bear on smaller parts of the problem so that these smaller
problems can be dealt with one at a time. This localization of effort is often
essential in order to make the process of probability judgment feasible, both
for the person who is making probability judgments and for the machine that is
combining them. The basic structure for our scheme is a type of tree that
generalizes both Pearl's and Gordon and Shortliffe's trees. Trees of this
general type permit localized computation in Pearl's sense. They are based on
qualitative judgments of conditional independence. We believe that the scheme
we describe here will prove useful in expert systems. It is now clear that the
successful propagation of probabilities or certainty factors in expert systems
requires much more structure than can be provided in a pure production-system
framework. Bayesian schemes, on the other hand, often make unrealistic demands
for structure. The propagation of belief functions in trees and more general
networks stands on a middle ground where some sensible and useful things can be
done. We would like to emphasize that the basic idea of local computation for
propagating probabilities is due to Judea Pearl. It is a very innovative idea;
we do not believe that it can be found in the Bayesian literature prior to
Pearl's work. We see our contribution as extending the usefulness of Pearl's
idea by generalizing it from Bayesian probabilities to belief functions. In the
next section, we give a brief introduction to belief functions. The notions of
qualitative independence for partitions and a qualitative Markov tree are
introduced in Section III. Finally, in Section IV, we describe a scheme for
propagating belief functions in qualitative Markov trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3110</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3110</id><created>2013-03-27</created><authors><author><keyname>Sher</keyname><forenames>David</forenames></author></authors><title>Appropriate and Inappropriate Estimation Techniques</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-261-266</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mode {also called MAP} estimation, mean estimation and median estimation are
examined here to determine when they can be safely used to derive {posterior)
cost minimizing estimates. (These are all Bayes procedures, using the mode.
mean. or median of the posterior distribution). It is found that modal
estimation only returns cost minimizing estimates when the cost function is
0-t. If the cost function is a function of distance then mean estimation only
returns cost minimizing estimates when the cost function is squared distance
from the true value and median estimation only returns cost minimizing
estimates when the cost function ts the distance from the true value. Results
are presented on the goodness or modal estimation with non 0-t cost functions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3111</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3111</id><created>2013-03-27</created><authors><author><keyname>Smith</keyname><forenames>Randall</forenames></author><author><keyname>Self</keyname><forenames>Matthew</forenames></author><author><keyname>Cheeseman</keyname><forenames>Peter</forenames></author></authors><title>Estimating Uncertain Spatial Relationships in Robotics</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-267-288</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a representation for spatial information, called
the stochastic map, and associated procedures for building it, reading
information from it, and revising it incrementally as new information is
obtained. The map contains the estimates of relationships among objects in the
map, and their uncertainties, given all the available information. The
procedures provide a general solution to the problem of estimating uncertain
relative spatial relationships. The estimates are probabilistic in nature, an
advance over the previous, very conservative, worst-case approaches to the
problem. Finally, the procedures are developed in the context of
state-estimation and filtering theory, which provides a solid basis for
numerous extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3112</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3112</id><created>2013-03-27</created><authors><author><keyname>Togai</keyname><forenames>Masaki</forenames></author><author><keyname>Watanabe</keyname><forenames>Hiroyuki</forenames></author></authors><title>A VLSI Design and Implementation for a Real-Time Approximate Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-289-296</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role of inferencing with uncertainty is becoming more important in
rule-based expert systems (ES), since knowledge given by a human expert is
often uncertain or imprecise. We have succeeded in designing a VLSI chip which
can perform an entire inference process based on fuzzy logic. The design of the
VLSI fuzzy inference engine emphasizes simplicity, extensibility, and
efficiency (operational speed and layout area). It is fabricated in 2.5 um CMOS
technology. The inference engine consists of three major components; a rule set
memory, an inference processor, and a controller. In this implementation, a
rule set memory is realized by a read only memory (ROM). The controller
consists of two counters. In the inference processor, one data path is laid out
for each rule. The number of the inference rule can be increased adding more
data paths to the inference processor. All rules are executed in parallel, but
each rule is processed serially. The logical structure of fuzzy inference
proposed in the current paper maps nicely onto the VLSI structure. A two-phase
nonoverlapping clocking scheme is used. Timing tests indicate that the
inference engine can operate at approximately 20.8 MHz. This translates to an
execution speed of approximately 80,000 Fuzzy Logical Inferences Per Second
(FLIPS), and indicates that the inference engine is suitable for a demanding
real-time application. The potential applications include decision-making in
the area of command and control for intelligent robot systems, process control,
missile and aircraft guidance, and other high performance machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3113</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3113</id><created>2013-03-27</created><authors><author><keyname>Tong</keyname><forenames>Richard M.</forenames></author><author><keyname>Appelbaum</keyname><forenames>Lee A.</forenames></author><author><keyname>Shapiro</keyname><forenames>D. G.</forenames></author></authors><title>A General Purpose Inference Engine for Evidential Reasoning Research</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-297-302</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to report on the most recent developments in our
ongoing investigation of the representation and manipulation of uncertainty in
automated reasoning systems. In our earlier studies (Tong and Shapiro, 1985) we
described a series of experiments with RUBRIC (Tong et al., 1985), a system for
full-text document retrieval, that generated some interesting insights into the
effects of choosing among a class of scalar valued uncertainty calculi. [n
order to extend these results we have begun a new series of experiments with a
larger class of representations and calculi, and to help perform these
experiments we have developed a general purpose inference engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3114</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3114</id><created>2013-03-27</created><authors><author><keyname>Ursic</keyname><forenames>Silvio</forenames></author></authors><title>Generalizing Fuzzy Logic Probabilistic Inferences</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-303-310</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear representations for a subclass of boolean symmetric functions selected
by a parity condition are shown to constitute a generalization of the linear
constraints on probabilities introduced by Boole. These linear constraints are
necessary to compute probabilities of events with relations between the.
arbitrarily specified with propositional calculus boolean formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3115</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3115</id><created>2013-03-27</created><authors><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Qualitative Probabilistic Networks for Planning Under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-311-318</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks provide a probabilistic semantics for qualitative
assertions about likelihood. A qualitative reasoner based on an algebra over
these assertions can derive further conclusions about the influence of actions.
While the conclusions are much weaker than those computed from complete
probability distributions, they are still valuable for suggesting potential
actions, eliminating obviously inferior plans, identifying important tradeoffs,
and explaining probabilistic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3116</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3116</id><created>2013-03-27</created><authors><author><keyname>Wise</keyname><forenames>Ben P.</forenames></author></authors><title>Experimentally Comparing Uncertain Inference Systems to Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-319-332</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the biases and performance of several uncertain inference
systems: Mycin, a variant of Mycin. and a simplified version of probability
using conditional independence assumptions. We present axiomatic arguments for
using Minimum Cross Entropy inference as the best way to do uncertain
inference. For Mycin and its variant we found special situations where its
performance was very good, but also situations where performance was worse than
random guessing, or where data was interpreted as having the opposite of its
true import We have found that all three of these systems usually gave accurate
results, and that the conditional independence assumptions gave the most robust
results. We illustrate how the Importance of biases may be quantitatively
assessed and ranked. Considerations of robustness might be a critical factor is
selecting UlS's for a given application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3117</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3117</id><created>2013-03-27</created><authors><author><keyname>Yadrick</keyname><forenames>Robert M.</forenames></author><author><keyname>Perrin</keyname><forenames>Bruce M.</forenames></author><author><keyname>Vaughan</keyname><forenames>David S.</forenames></author><author><keyname>Holden</keyname><forenames>Peter D.</forenames></author><author><keyname>Kempf</keyname><forenames>Karl G.</forenames></author></authors><title>Evaluation of Uncertain Inference Models I: PROSPECTOR</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-333-338</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the accuracy of the PROSPECTOR model for uncertain
reasoning. PROSPECTOR's solutions for a large number of computer-generated
inference networks were compared to those obtained from probability theory and
minimum cross-entropy calculations. PROSPECTOR's answers were generally
accurate for a restricted subset of problems that are consistent with its
assumptions. However, even within this subset, we identified conditions under
which PROSPECTOR's performance deteriorates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3118</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3118</id><created>2013-03-27</created><authors><author><keyname>Yager</keyname><forenames>Ronald R.</forenames></author></authors><title>On Implementing Usual Values</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-339-346</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many cases commonsense knowledge consists of knowledge of what is usual.
In this paper we develop a system for reasoning with usual information. This
system is based upon the fact that these pieces of commonsense information
involve both a probabilistic aspect and a granular aspect. We implement this
system with the aid of possibility-probability granules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3119</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3119</id><created>2013-03-27</created><authors><author><keyname>Zadeh</keyname><forenames>Lotfi</forenames></author><author><keyname>Ralescu</keyname><forenames>Anca</forenames></author></authors><title>On the Combinality of Evidence in the Dempster-Shafer Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)</comments><proxy>auai</proxy><report-no>UAI-P-1986-PG-347-349</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current versions of the Dempster-Shafer theory, the only essential
restriction on the validity of the rule of combination is that the sources of
evidence must be statistically independent. Under this assumption, it is
permissible to apply the Dempster-Shafer rule to two or mere distinct
probability distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3120</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3120</id><created>2013-04-09</created><authors><author><keyname>Quaye-Ballard</keyname><forenames>J. A.</forenames></author><author><keyname>An</keyname><forenames>R.</forenames></author><author><keyname>Agyemang</keyname><forenames>A. B.</forenames></author><author><keyname>Oppong-Quayson</keyname><forenames>N. Y.</forenames></author><author><keyname>Ablade</keyname><forenames>J. E. N.</forenames></author></authors><title>GUI Database for the Equipment Store of the Department of Geomatic
  Engineering, KNUST</title><categories>cs.DB</categories><comments>6 pages, 9 figures, (IJACSA) International Journal of Advanced
  Computer Science and Applications, Vol. 3, No. 7, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The geospatial analyst is required to apply art, science, and technology to
measure relative positions of natural and man-made features above or beneath
the earths surface, and to present this information either graphically or
numerically. The reference positions for these measurements need to be well
archived and managed to effectively sustain the activities in the spatial
industry. The research herein described highlights the need for an information
system for the Land Surveyors Equipment Store. Such a system is a database
management system with a user friendly graphical interface. This paper
describes one such system that has been developed for the Equipment Store of
the Department of Geomatic Engineering, Kwame Nkrumah University of Science and
Technology, Ghana. The system facilitates efficient management and location of
instruments, as well as easy location of beacons together with their attribute
information, it provides multimedia information about instruments in an
Equipment Store. Digital camera was used capture the pictorial descriptions of
the beacons. Geographic Information System software was employed to visualize
the spatial location of beacons and to publish the various layers for the
Graphical User Interface. The aesthetics of the interface was developed with
user interface design tools and coded by programming. The developed Suite,
powered by a reliable and fully scalable database, provides an efficient way of
booking and analyzing transactions in an Equipment Store.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3121</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3121</id><created>2013-04-10</created><authors><author><keyname>Rathke</keyname><forenames>Julian</forenames></author><author><keyname>Sobocinski</keyname><forenames>Pawel</forenames></author><author><keyname>Stephens</keyname><forenames>Owen</forenames></author></authors><title>Decomposing Petri nets</title><categories>cs.LO</categories><acm-class>F.3.2; D.2.2; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work, the second and third authors introduced a technique for
reachability checking in 1-bounded Petri nets, based on wiring decompositions,
which are expressions in a fragment of the compositional algebra of nets with
boundaries. Here we extend the technique to the full algebra and introduce the
related structural property of decomposition width on directed hypergraphs.
Small decomposition width is necessary for the applicability of the
reachability checking algorithm. We give examples of families of nets with
constant decomposition width and develop the underlying theory of
decompositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3134</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3134</id><created>2013-04-10</created><authors><author><keyname>Ullah</keyname><forenames>Sultan</forenames></author><author><keyname>Xuefeng</keyname><forenames>Zheng</forenames></author><author><keyname>Feng</keyname><forenames>Zhou</forenames></author><author><keyname>Haichun</keyname><forenames>Zhao</forenames></author></authors><title>TCLOUD: Challenges and Best Practices for Cloud Computing</title><categories>cs.DC cs.CR cs.CY</categories><journal-ref>International Journal of Engineering Research and Technology Vol.
  1 (09), 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has achieved an unbelievable adoption response rate but still
its infancy stage is not over. It is an emerging paradigm and amazingly gaining
popularity. The size of the market shared of the applications provided by cloud
computing is still not much behind the expectations. It provides the
organizations with great potential to minimize the cost and maximizes the
overall operating effectiveness of computing required by an organization.
Despite its growing popularity, still it is faced with security, privacy, and
portability issues, which in one or the other way create hurdles in the fast
acceptance of this new technology for the computing community. This paper
provides a concise all around analysis of the challenges faced by cloud
computing community and also presents the solutions available to these
challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3135</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3135</id><created>2013-02-11</created><authors><author><keyname>Niu</keyname><forenames>Jinzhong</forenames></author><author><keyname>Parsons</keyname><forenames>Simon</forenames></author></authors><title>Maximizing Matching in Double-sided Auctions</title><categories>cs.GT q-fin.TR</categories><comments>16 pages, 4 figures, full-length version of an extended abstract
  published at the AAMAS 2013 conference</comments><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel, non-recursive, maximal matching
algorithm for double auctions, which aims to maximize the amount of commodities
to be traded. It differs from the usual equilibrium matching, which clears a
market at the equilibrium price. We compare the two algorithms through
experimental analyses, showing that the maximal matching algorithm is favored
in scenarios where trading volume is a priority and that it may possibly
improve allocative efficiency over equilibrium matching as well. A
parameterized algorithm that incorporates both maximal matching and equilibrium
matching as special cases is also presented to allow flexible control on how
much to trade in a double auction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3138</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3138</id><created>2013-04-10</created><authors><author><keyname>De Rainville</keyname><forenames>Fran&#xe7;ois-Michel</forenames></author><author><keyname>Sebag</keyname><forenames>Mich&#xe8;le</forenames></author><author><keyname>Gagn&#xe9;</keyname><forenames>Christian</forenames></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames></author><author><keyname>Laurendeau</keyname><forenames>Denis</forenames></author></authors><title>Sustainable Cooperative Coevolution with a Multi-Armed Bandit</title><categories>cs.NE</categories><comments>Accepted at GECCO 2013</comments><acm-class>I.2.8</acm-class><doi>10.1145/2463372.2463556</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a self-adaptation mechanism to manage the resources
allocated to the different species comprising a cooperative coevolutionary
algorithm. The proposed approach relies on a dynamic extension to the
well-known multi-armed bandit framework. At each iteration, the dynamic
multi-armed bandit makes a decision on which species to evolve for a
generation, using the history of progress made by the different species to
guide the decisions. We show experimentally, on a benchmark and a real-world
problem, that evolving the different populations at different paces allows not
only to identify solutions more rapidly, but also improves the capacity of
cooperative coevolution to solve more complex problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3139</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3139</id><created>2013-04-10</created><updated>2013-11-10</updated><authors><author><keyname>Louis</keyname><forenames>Anand</forenames></author><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>The Complexity of Approximating Vertex Expansion</title><categories>cs.CC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study the complexity of approximating the vertex expansion of graphs $G =
(V,E)$, defined as \[ \Phi^V := \min_{S \subset V} n \cdot \frac{|N(S)|}{|S| |V
\backslash S|}. \]
  We give a simple polynomial-time algorithm for finding a subset with vertex
expansion $O(\sqrt{OPT \log d})$ where $d$ is the maximum degree of the graph.
Our main result is an asymptotically matching lower bound: under the Small Set
Expansion (SSE) hypothesis, it is hard to find a subset with expansion less
than $C\sqrt{OPT \log d}$ for an absolute constant $C$. In particular, this
implies for all constant $\epsilon &gt; 0$, it is SSE-hard to distinguish whether
the vertex expansion $&lt; \epsilon$ or at least an absolute constant. The
analogous threshold for edge expansion is $\sqrt{OPT}$ with no dependence on
the degree; thus our results suggest that vertex expansion is harder to
approximate than edge expansion. In particular, while Cheeger's algorithm can
certify constant edge expansion, it is SSE-hard to certify constant vertex
expansion in graphs.
  Our proof is via a reduction from the {\it Unique Games} instance obtained
from the \SSE hypothesis to the vertex expansion problem. It involves the
definition of a smoother intermediate problem we call {\sf Analytic Vertex
Expansion} which is representative of both the vertex expansion and the
conductance of the graph. Both reductions (from the UGC instance to this
problem and from this problem to vertex expansion) use novel proof ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3140</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3140</id><created>2013-04-09</created><authors><author><keyname>Grigoriev</keyname><forenames>Evgeniy</forenames></author></authors><title>On PROGRESS Operation. How to Make Object-Oriented Programming System
  More Object-Oriented (DRAFT)</title><categories>cs.PL cs.SE</categories><comments>4 pages</comments><acm-class>F.1.1; D.3.3; H.2.3; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system, which implements persistent objects, has to provide different
opportunities to change the objects in arbitrary ways during their existence. A
traditional realization of OO paradigm in modern programming systems has
fundamental drawbacks which complicate an implementation of persistent
modifiable objects considerably. There is alternative realization that does not
have these drawbacks. In the article the PROGRESS operation is offered, which
modify existing object within an existing inheritance hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3144</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3144</id><created>2013-04-05</created><authors><author><keyname>Saad</keyname><forenames>Emad</forenames></author></authors><title>Logical Probability Preferences</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.2384,
  arXiv:1304.2797</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a unified logical framework for representing and reasoning about
both probability quantitative and qualitative preferences in probability answer
set programming, called probability answer set optimization programs. The
proposed framework is vital to allow defining probability quantitative
preferences over the possible outcomes of qualitative preferences. We show the
application of probability answer set optimization programs to a variant of the
well-known nurse restoring problem, called the nurse restoring with probability
preferences problem. To the best of our knowledge, this development is the
first to consider a logical framework for reasoning about probability
quantitative preferences, in general, and reasoning about both probability
quantitative and qualitative preferences in particular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3145</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3145</id><created>2013-04-10</created><authors><author><keyname>Yang</keyname><forenames>Yongjie</forenames></author><author><keyname>Guo</keyname><forenames>Jiong</forenames></author></authors><title>Exact Algorithms for Weighted and Unweighted Borda Manipulation Problems</title><categories>cs.DS cs.GT</categories><comments>9 pages, 3 figures, a short version appears in AAMAS 2013</comments><acm-class>F.2; G.2.1; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both weighted and unweighted Borda manipulation problems have been proved
$\mathcal{NP}$-hard. However, there is no exact combinatorial algorithm known
for these problems. In this paper, we initiate the study of exact combinatorial
algorithms for both weighted and unweighted Borda manipulation problems. More
precisely, we propose $O^*((m\cdot 2^m)^{t+1})$time and
$O^*(t^{2m})$time\footnote{$O^*()$ is the $O()$ notation with suppressed
factors polynomial in the size of the input.} combinatorial algorithms for
weighted and unweighted Borda manipulation problems, respectively, where $t$ is
the number of manipulators and $m$ is the number of candidates. Thus, for $t=2$
we solve one of the open problems posted by Betzler et al. [IJCAI 2011]. As a
byproduct of our results, we show that the {{unweighted Borda manipulation}}
problem admits an algorithm of running time $O^*(2^{9m^2\log{m}})$, based on an
integer linear programming technique. Finally, we study the {{unweighted Borda
manipulation}} problem under single-peaked elections and present
polynomial-time algorithms for the problem in the case of two manipulators, in
contrast to the $\mathcal{NP}$-hardness of this case in general settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3156</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3156</id><created>2013-04-10</created><updated>2013-04-11</updated><authors><author><keyname>Goparaju</keyname><forenames>Sreechakra</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Data Secrecy in Distributed Storage Systems under Exact Repair</title><categories>cs.IT math.IT</categories><comments>Submitted to Netcod 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of securing data against eavesdropping in distributed storage
systems is studied. The focus is on systems that use linear codes and implement
exact repair to recover from node failures.The maximum file size that can be
stored securely is determined for systems in which all the available nodes help
in repair (i.e., repair degree $d=n-1$, where $n$ is the total number of nodes)
and for any number of compromised nodes. Similar results in the literature are
restricted to the case of at most two compromised nodes. Moreover, new explicit
upper bounds are given on the maximum secure file size for systems with
$d&lt;n-1$. The key ingredients for the contribution of this paper are new results
on subspace intersection for the data downloaded during repair. The new bounds
imply the interesting fact that the maximum data that can be stored securely
decreases exponentially with the number of compromised nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3157</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3157</id><created>2013-04-10</created><authors><author><keyname>Alshehri</keyname><forenames>Mohammed</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author></authors><title>Analysis of Citizens Acceptance for E-government Services: Applying the
  UTAUT Model</title><categories>cs.CY</categories><comments>Presented to IADIS International Conferences Theory and Practice in
  Modern Computing and Internet Applications and Research 2012, pp. 69-76</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-government services aims to provide citizens with more accessible,
accurate, real-time and high quality services and information. Although the
public sectors in Kingdom of Saudi Arabia (KSA) have promoted their
e-Government services for many years, its uses and achievements are few.
Therefore, this paper explores the key factors of Saudi citizens acceptance
through a research survey and by gathering empirical evidence based on the
Unified Theory of Acceptance and the Use of Technology (UTAUT). Survey Data
collected from 400 respondents was examined using structural equation modelling
(SEM) technique and utilized AMOS tools. The study results explored the factors
that affect the acceptance of e-government services in KSA based on UTAUT
model. Moreover, as a result of this study an amended UTAUT model was proposed.
Such a model contributes to the discussion and development of adoption models
for technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3162</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3162</id><created>2013-04-10</created><updated>2014-06-29</updated><authors><author><keyname>Kannan</keyname><forenames>Ravindran</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author><author><keyname>Woodruff</keyname><forenames>David</forenames></author></authors><title>Principal Component Analysis and Higher Correlations for Distributed
  Data</title><categories>cs.DS cs.DC</categories><comments>rewritten with focus on two main results (distributed PCA,
  higher-order moments and correlations) in the arbitrary partition model</comments><msc-class>68Q25, 68Q05</msc-class><acm-class>F.1.1; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider algorithmic problems in the setting in which the input data has
been partitioned arbitrarily on many servers. The goal is to compute a function
of all the data, and the bottleneck is the communication used by the algorithm.
We present algorithms for two illustrative problems on massive data sets: (1)
computing a low-rank approximation of a matrix $A=A^1 + A^2 + \ldots + A^s$,
with matrix $A^t$ stored on server $t$ and (2) computing a function of a vector
$a_1 + a_2 + \ldots + a_s$, where server $t$ has the vector $a_t$; this
includes the well-studied special case of computing frequency moments and
separable functions, as well as higher-order correlations such as the number of
subgraphs of a specified type occurring in a graph. For both problems we give
algorithms with nearly optimal communication, and in particular the only
dependence on $n$, the size of the data, is in the number of bits needed to
represent indices and words ($O(\log n)$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3169</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3169</id><created>2013-04-10</created><updated>2013-04-23</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Brandt</keyname><forenames>Felix</forenames></author><author><keyname>Brill</keyname><forenames>Markus</forenames></author></authors><title>The Computational Complexity of Random Serial Dictatorship</title><categories>cs.GT cs.CC</categories><comments>11 pages</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><journal-ref>Economics Letters 121(3), 2013</journal-ref><doi>10.1016/j.econlet.2013.09.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social choice settings with linear preferences, random dictatorship is
known to be the only social decision scheme satisfying strategyproofness and ex
post efficiency. When also allowing indifferences, random serial dictatorship
(RSD) is a well-known generalization of random dictatorship that retains both
properties. RSD has been particularly successful in the special domain of
random assignment where indifferences are unavoidable. While executing RSD is
obviously feasible, we show that computing the resulting probabilities is
#P-complete and thus intractable, both in the context of voting and assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3172</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3172</id><created>2013-04-10</created><updated>2013-05-02</updated><authors><author><keyname>Itoh</keyname><forenames>Toshiya</forenames></author><author><keyname>Yoshimoto</keyname><forenames>Seiji</forenames></author></authors><title>Buffer Management of Multi-Queue QoS Switches with Class Segregation</title><categories>cs.DS</categories><comments>12 pages, 2 tables, 2 figures. arXiv admin note: substantial text
  overlap with arXiv:1109.6060</comments><doi>10.1016/j.tcs.2015.04.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on buffer management of multi-queue QoS switches in
which packets of different values are segregated in different queues. Our model
consists of $m$ queues and $m$ packet values $0 &lt; v_{1} &lt; v_{2} &lt; ... &lt; v_{m}$.
Recently, Al-Bawani and Souza [IPL 113(4), pp.145-150, 2013] presented an
online algorithm GREEDY for buffer management of multi-queue QoS switches with
class segregation and showed thatif $m$ queues have the same size, then the
competitive ratio of GREEDY is $1+r$, where $r=\max_{1 \leq i \leq m-1}
v_{i}/v_{i+1}$. In this paper, we precisely analyze the behavior of GREEDY and
show that it is $(1+r)$-competitive for the case that $m$ queues do not
necessarily have the same size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3177</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3177</id><created>2013-04-10</created><updated>2014-02-13</updated><authors><author><keyname>Mascarenhas</keyname><forenames>Fabio</forenames></author><author><keyname>Medeiros</keyname><forenames>S&#xe9;rgio</forenames></author><author><keyname>Ierusalimschy</keyname><forenames>Roberto</forenames></author></authors><title>On the Relation between Context-Free Grammars and Parsing Expression
  Grammars</title><categories>cs.FL</categories><doi>10.1016/j.scico.2014.01.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-Free Grammars (CFGs) and Parsing Expression Grammars (PEGs) have
several similarities and a few differences in both their syntax and semantics,
but they are usually presented through formalisms that hinder a proper
comparison. In this paper we present a new formalism for CFGs that highlights
the similarities and differences between them. The new formalism borrows from
PEGs the use of parsing expressions and the recognition-based semantics. We
show how one way of removing non-determinism from this formalism yields a
formalism with the semantics of PEGs. We also prove, based on these new
formalisms, how LL(1) grammars define the same language whether interpreted as
CFGs or as PEGs, and also show how strong-LL(k), right-linear, and LL-regular
grammars have simple language-preserving translations from CFGs to PEGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3179</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3179</id><created>2013-04-10</created><authors><author><keyname>Park</keyname><forenames>Seok-Hwan</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Sahin</keyname><forenames>Onur</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Joint Precoding and Multivariate Backhaul Compression for the Downlink
  of Cloud Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2280111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the joint design of precoding and backhaul compression
strategies for the downlink of cloud radio access networks. In these systems, a
central encoder is connected to multiple multi-antenna base stations (BSs) via
finite-capacity backhaul links. At the central encoder, precoding is followed
by compression in order to produce the rate-limited bit streams delivered to
each BS over the corresponding backhaul link. In current state-of-the-art
approaches, the signals intended for different BSs are compressed
independently. In contrast, this work proposes to leverage joint compression,
also referred to as multivariate compression, of the signals of different BSs
in order to better control the effect of the additive quantization noises at
the mobile stations (MSs). The problem of maximizing the weighted sum-rate with
respect to both the precoding matrix and the joint correlation matrix of the
quantization noises is formulated subject to power and backhaul capacity
constraints. An iterative algorithm is proposed that achieves a stationary
point of the problem. Moreover, in order to enable the practical implementation
of multivariate compression across BSs, a novel architecture is proposed based
on successive steps of minimum mean-squared error (MMSE) estimation and per-BS
compression. Robust design with respect to imperfect channel state information
is also discussed. From numerical results, it is confirmed that the proposed
joint precoding and compression strategy outperforms conventional approaches
based on the separate design of precoding and compression or independent
compression across the BSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3183</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3183</id><created>2013-04-10</created><authors><author><keyname>Gupta</keyname><forenames>Palash</forenames></author><author><keyname>Mohammed</keyname><forenames>Hussain</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>Characterization of Downlink Transmit Power Control during Soft Handover
  in WCDMA Systems</title><categories>cs.NI</categories><journal-ref>Procs. of the 9th International Conference on Computer and
  Information Technology (ICCIT 2006), pp. 448-452, Dhaka, Bangladesh, December
  21-23, (2006)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the characterization of power control in WCDMA systems.
We know that CDMA is an interference limited system. It is shown that the
unbalance scheme is reliable and successful for both 2-way and 3-way soft
handover. Unbalance scheme minimizes interference and speed up the soft
handover algorithm to support more users quickly. Furthermore it requires
minimum time to make decision for proper power control in soft handover status.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3192</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3192</id><created>2013-04-11</created><authors><author><keyname>Guo</keyname><forenames>Yulan</forenames></author><author><keyname>Sohel</keyname><forenames>Ferdous</forenames></author><author><keyname>Bennamoun</keyname><forenames>Mohammed</forenames></author><author><keyname>Lu</keyname><forenames>Min</forenames></author><author><keyname>Wan</keyname><forenames>Jianwei</forenames></author></authors><title>Rotational Projection Statistics for 3D Local Surface Description and
  Object Recognition</title><categories>cs.CV</categories><comments>The final publication is available at link.springer.com International
  Journal of Computer Vision 2013</comments><acm-class>I.4; I.5.4</acm-class><doi>10.1007/s11263-013-0627-y</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recognizing 3D objects in the presence of noise, varying mesh resolution,
occlusion and clutter is a very challenging task. This paper presents a novel
method named Rotational Projection Statistics (RoPS). It has three major
modules: Local Reference Frame (LRF) definition, RoPS feature description and
3D object recognition. We propose a novel technique to define the LRF by
calculating the scatter matrix of all points lying on the local surface. RoPS
feature descriptors are obtained by rotationally projecting the neighboring
points of a feature point onto 2D planes and calculating a set of statistics
(including low-order central moments and entropy) of the distribution of these
projected points. Using the proposed LRF and RoPS descriptor, we present a
hierarchical 3D object recognition algorithm. The performance of the proposed
LRF, RoPS descriptor and object recognition algorithm was rigorously tested on
a number of popular and publicly available datasets. Our proposed techniques
exhibited superior performance compared to existing techniques. We also showed
that our method is robust with respect to noise and varying mesh resolution.
Our RoPS based algorithm achieved recognition rates of 100%, 98.9%, 95.4% and
96.0% respectively when tested on the Bologna, UWA, Queen's and Ca' Foscari
Venezia Datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3200</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3200</id><created>2013-04-11</created><authors><author><keyname>Jamali</keyname><forenames>A. R. M. Jalal Uddin</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Bazlar</forenames></author></authors><title>An Approach to Solve Linear Equations Using a Time-Variant Adaptation
  Based Hybrid Evolutionary Algorithm</title><categories>cs.NE cs.NA</categories><comments>arXiv admin note: text overlap with arXiv:1304.2097</comments><journal-ref>Jahangirnagar University Journal of Science, Bangladesh, Vol. 27,
  pp. 277-289, (2004)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For small number of equations, systems of linear (and sometimes nonlinear)
equations can be solved by simple classical techniques. However, for large
number of systems of linear (or nonlinear) equations, solutions using classical
method become arduous. On the other hand evolutionary algorithms have mostly
been used to solve various optimization and learning problems. Recently,
hybridization of evolutionary algorithm with classical Gauss-Seidel based
Successive Over Relaxation (SOR) method has successfully been used to solve
large number of linear equations; where a uniform adaptation (UA) technique of
relaxation factor is used. In this paper, a new hybrid algorithm is proposed in
which a time-variant adaptation (TVA) technique of relaxation factor is used
instead of uniform adaptation technique to solve large number of linear
equations. The convergence theorems of the proposed algorithms are proved
theoretically. And the performance of the proposed TVA-based algorithm is
compared with the UA-based hybrid algorithm in the experimental domain. The
proposed algorithm outperforms the hybrid one in terms of efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3203</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3203</id><created>2013-04-11</created><authors><author><keyname>Ullah</keyname><forenames>Sultan</forenames></author><author><keyname>Xuefeng</keyname><forenames>Zheng</forenames></author></authors><title>Cloud Computing Research Challenges</title><categories>cs.DC</categories><comments>IEEE 5th International Conference on BioMedical Engineering and
  Informatics (BMEI 2012)</comments><journal-ref>IEEE 5th International Conference on BioMedical Engineering and
  Informatics (BMEI 2012), PP 1397 - 1401</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent times cloud computing has appeared as a new model for hosting and
conveying services over the Internet. This model is striking to business
vendors as it eradicates the requirement for users to plan in advance, and it
permits the organization to start from low level and then add more resources
only if there is an increase in the service demand. Even though cloud computing
presents greater opportunities not only to information technology industry, but
every organization involved in utilizing the computing in one way or the other,
it is still in infancy with many problems to be fixed. The paper discusses
research challenges in cloud computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3208</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3208</id><created>2013-04-11</created><authors><author><keyname>Berthier</keyname><forenames>Denis</forenames></author></authors><title>From Constraints to Resolution Rules, Part I: Conceptual Framework</title><categories>cs.AI</categories><comments>International Joint Conferences on Computer, Information, Systems
  Sciences and Engineering (CISSE 08), December 5-13, 2008, Springer. Also a
  chapter of the book &quot;Advanced Techniques in Computing Sciences and Software
  Engineering&quot;, Khaled Elleithy Editor, pp. 165-170, Springer, 2010, ISBN
  9789094136599</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world problems naturally appear as constraints satisfaction
problems (CSP), for which very efficient algorithms are known. Most of these
involve the combination of two techniques: some direct propagation of
constraints between variables (with the goal of reducing their sets of possible
values) and some kind of structured search (depth-first, breadth-first,...).
But when such blind search is not possible or not allowed or when one wants a
'constructive' or a 'pattern-based' solution, one must devise more complex
propagation rules instead. In this case, one can introduce the notion of a
candidate (a 'still possible' value for a variable). Here, we give this
intuitive notion a well defined logical status, from which we can define the
concepts of a resolution rule and a resolution theory. In order to keep our
analysis as concrete as possible, we illustrate each definition with the well
known Sudoku example. Part I proposes a general conceptual framework based on
first order logic; with the introduction of chains and braids, Part II will
give much deeper results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3209</identifier>
 <datestamp>2013-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3209</id><created>2013-04-11</created><authors><author><keyname>Akkoyun</keyname><forenames>Serkan</forenames></author><author><keyname>Bayram</keyname><forenames>Tuncay</forenames></author><author><keyname>Kara</keyname><forenames>S. Okan</forenames></author></authors><title>Improvement studies on neutron-gamma separation in HPGe detectors by
  using neural networks</title><categories>physics.ins-det cs.NE nucl-ex</categories><comments>10 pages, 4 figures, 1 table</comments><journal-ref>Cumhuriyet Science Journal, 34-1 (2013) 42</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The neutrons emitted in heavy-ion fusion-evaporation (HIFE) reactions
together with the gamma-rays cause unwanted backgrounds in gamma-ray spectra.
Especially in the nuclear reactions, where relativistic ion beams (RIBs) are
used, these neutrons are serious problem. They have to be rejected in order to
obtain clearer gamma-ray peaks. In this study, the radiation energy and three
criteria which were previously determined for separation between neutron and
gamma-rays in the HPGe detectors have been used in artificial neural network
(ANN) for improving of the decomposition power. According to the preliminary
results obtained from ANN method, the ratio of neutron rejection has been
improved by a factor of 1.27 and the ratio of the lost in gamma-rays has been
decreased by a factor of 0.50.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3210</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3210</id><created>2013-04-11</created><authors><author><keyname>Berthier</keyname><forenames>Denis</forenames></author></authors><title>From Constraints to Resolution Rules, Part II: chains, braids,
  confluence and T&amp;E</title><categories>cs.AI</categories><comments>International Joint Conferences on Computer, Information, Systems
  Sciences and Engineering (CISSE 08), December 5-13, 2008, Springer. Also a
  chapter of the book 'Advanced Techniques in Computing Sciences and Software
  Engineering', Khaled Elleithy Editor, pp. 171-176, Springer, 2010, ISBN
  9789094136599</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this Part II, we apply the general theory developed in Part I to a
detailed analysis of the Constraint Satisfaction Problem (CSP). We show how
specific types of resolution rules can be defined. In particular, we introduce
the general notions of a chain and a braid. As in Part I, these notions are
illustrated in detail with the Sudoku example - a problem known to be
NP-complete and which is therefore typical of a broad class of hard problems.
For Sudoku, we also show how far one can go in 'approximating' a CSP with a
resolution theory and we give an empirical statistical analysis of how the
various puzzles, corresponding to different sets of entries, can be classified
along a natural scale of complexity. For any CSP, we also prove the confluence
property of some Resolution Theories based on braids and we show how it can be
used to define different resolution strategies. Finally, we prove that, in any
CSP, braids have the same solving capacity as Trial-and-Error (T&amp;E) with no
guessing and we comment this result in the Sudoku case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3242</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3242</id><created>2013-04-11</created><authors><author><keyname>Zhou</keyname><forenames>Wenxiong</forenames></author><author><keyname>Wang</keyname><forenames>Yanyu</forenames></author><author><keyname>Nan</keyname><forenames>Gangyang</forenames></author><author><keyname>Zhang</keyname><forenames>Jianchuan</forenames></author></authors><title>The design of high-speed data transmission method for a small nuclear
  physics DAQ system</title><categories>physics.ins-det cs.OH</categories><comments>submited to Chinese Physics C</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of data need to be transmitted in high-speed between Field
Programmable Gate Array (FPGA) and Advanced RISC Machines 11 micro-controller
(ARM11) when we design a small data acquisition (DAQ) system for nuclear
experiments. However, it is a complex problem to beat the target. In this
paper, we will introduce a method which can realize the high-speed data
transmission. By this way, FPGA is designed to acquire massive data from
Front-end electronics (FEE) and send it to ARM11, which will transmit the data
to other computer through the TCP/IP protocol. This paper mainly introduces the
interface design of the high-speed transmission between FPGA and ARM11, the
transmission logic of FPGA and the driver program of ARM11. The research shows
that the maximal transmission speed between FPGA and ARM11 by this way can
reach 50MB/s theoretically, while in nuclear physics experiment, the system can
acquire data with the speed of 2.2MB/s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3249</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3249</id><created>2013-04-11</created><authors><author><keyname>Moyen</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Toldin</keyname><forenames>Paolo Parisen</forenames></author></authors><title>A polytime complexity analyser for Probabilistic Polynomial Time over
  imperative stack programs</title><categories>cs.LO cs.CC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present iSAPP (Imperative Static Analyser for Probabilistic Polynomial
Time), a complexity verifier tool that is sound and extensionally complete for
the Probabilistic Polynomial Time (PP) complexity class. iSAPP works on an
imperative programming language for stack machines. The certificate of
polynomiality can be built in polytime, with respect to the number of stacks
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3256</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3256</id><created>2013-04-11</created><authors><author><keyname>Kafhali</keyname><forenames>Said El</forenames></author><author><keyname>Hanini</keyname><forenames>Mohamed</forenames></author><author><keyname>Bouchti</keyname><forenames>Abdelali El</forenames></author><author><keyname>Haqtq</keyname><forenames>Abdelkrim</forenames></author></authors><title>Performances Evaluation of Enhanced Basic Time Space Priority combined
  with an AQM</title><categories>cs.NI</categories><comments>7 pages</comments><journal-ref>International Journal of Computer Science and Information
  Security,Vol. 9, No. 8, August 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active Queue Management(AQM) is an efficient tool in the network to avoid
saturation of the queue by warning the sender that the queue is almost full to
reduce its speed before the queue is full. The buffer management schemes focus
on space management, in the other hand scheduling priorities (focusing on time
management) attempt to guarantee acceptable delay boundaries to applications
for which it is important that delay is bounded. Combined mechanisms (time and
space management) are possible and enable networks to improve the perceived
quality for multimedia traffic at the end users. The key idea in this paper is
to study the performance of a mechanism combining an AQM with a time-space
priority scheme applied to multimedia flows transmitted to an end user in HSDPA
network. The studied queue is shared by Real Time and Non Real Time packets. We
propose a mathematical model using Poisson and MMPP processes to model the
arrival of packets in the system. The performance parameters are analytically
deducted for the Combined EB-TSP and compared to the case of Simple EB-TSP.
Numerical results obtained show the positive impact of the AQM added to the
EB-TSP on the performance parameters of NRT packets compared to the Simple
EB-TSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3258</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3258</id><created>2013-04-11</created><authors><author><keyname>Hanini</keyname><forenames>Mohamed</forenames></author><author><keyname>Kafhali</keyname><forenames>Said El</forenames></author><author><keyname>Haqtq</keyname><forenames>Abdelkrim</forenames></author><author><keyname>Berqta</keyname><forenames>Amine</forenames></author></authors><title>Effect of the Feedback Function on the QoS in a Time Space Priority with
  Active Queue Management</title><categories>cs.NI</categories><comments>5 pages</comments><journal-ref>International Journal of Computer Science and Telecommunications
  [Volume 2, Issue 4, July 2011]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic idea behind an active queue management (AQM) is to sense the
congestion level within the network and inform the packet sources about, so
that they reduce their sending rate. In literature a lot off mechanisms of AQM
are studied. But there are not used in the context of the DiffServ architecture
where different types of packet with different requirements of QoS share the
same link. In this paper, we study an access control mechanism for RT and NRT
packets arriving in a buffer implemented at an end user in HSDPA. The mechanism
uses thresholds to mange access in the buffer and gives access priority to RT
packets. In order to control the arrival rate of the NRT packets in the buffer
an active queue management is used. We study the effect of the feedback
function on the QoS parameters for both kinds of packets .Mathematical
description and analytical results are given, and numerical results show that
the proposed function achieves higher QoS for the NRT packets in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3259</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3259</id><created>2013-04-11</created><authors><author><keyname>Kafhali</keyname><forenames>Said El</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author></authors><title>Effect of Mobility and Traffic Models on the Energy Consumption in MANET
  Routing Protocols</title><categories>cs.NI</categories><comments>8 pages. arXiv admin note: substantial text overlap with
  arXiv:1304.2035</comments><journal-ref>International Journal of Soft Computing and Engineering (IJSCE),
  Volume-3, Issue-1, March 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Mobile Ad hoc Network (MANET) is a group of mobile nodes that can be set up
randomly and formed without the need of any existing network infrastructure or
centralized administration. In this network the mobile devices are dependent on
battery power, it is important to minimize their energy consumption. Also
storage capacity and power are severely limited. In situations such as
emergency rescue, military actions, and scientific field missions, energy
conservation plays an even more important role which is critical to the success
of the tasks performed by the network. Therefore, energy conservation should be
considered carefully when designing or evaluating ad hoc routing protocols. In
this paper we concentrated on the energy consumption issues of existing routing
protocols in MANET under various mobility models and whose connections
communicate in a particular traffic model (CBR, Exponential, and Pareto). This
paper describes a performance comparison of the AODV, DSR and DSDV routing
protocols in term of energy consumed due to packet type (routing/MAC) during
transmission and reception of control packets. The mobility models used in this
work are Random Waypoint, Manhattan Grid and Reference Point Group. Simulations
have been carried out using NS-2 and its associated tools for animation and
analysis of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3260</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3260</id><created>2013-04-11</created><authors><author><keyname>Collins</keyname><forenames>John</forenames></author><author><keyname>Farrimond</keyname><forenames>Brian</forenames></author><author><keyname>Flower</keyname><forenames>David</forenames></author><author><keyname>Anderson</keyname><forenames>Mark</forenames></author><author><keyname>Gill</keyname><forenames>David</forenames></author></authors><title>The Removal of Numerical Drift from Scientific Models</title><categories>cs.SE cs.PL</categories><comments>12 pages</comments><msc-class>68U01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer programs often behave differently under different compilers or in
different computing environments. Relative debugging is a collection of
techniques by which these differences are analysed. Differences may arise
because of different interpretations of errors in the code, because of bugs in
the compilers or because of numerical drift, and all of these were observed in
the present study. Numerical drift arises when small and acceptable differences
in values computed by different systems are integrated, so that the results
drift apart. This is well understood and need not degrade the validity of the
program results. Coding errors and compiler bugs may degrade the results and
should be removed. This paper describes a technique for the comparison of two
program runs which removes numerical drift and therefore exposes coding and
compiler errors. The procedure is highly automated and requires very little
intervention by the user. The technique is applied to the Weather Research and
Forecasting model, the most widely used weather and climate modelling code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3265</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3265</id><created>2013-04-11</created><authors><author><keyname>Jebali</keyname><forenames>Maher</forenames></author><author><keyname>Dalle</keyname><forenames>Patrice</forenames></author><author><keyname>Jemni</keyname><forenames>Mohamed</forenames></author></authors><title>Extension of hidden markov model for recognizing large vocabulary of
  sign language</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computers still have a long way to go before they can interact with users in
a truly natural fashion. From a users perspective, the most natural way to
interact with a computer would be through a speech and gesture interface.
Although speech recognition has made significant advances in the past ten
years, gesture recognition has been lagging behind. Sign Languages (SL) are the
most accomplished forms of gestural communication. Therefore, their automatic
analysis is a real challenge, which is interestingly implied to their lexical
and syntactic organization levels. Statements dealing with sign language occupy
a significant interest in the Automatic Natural Language Processing (ANLP)
domain. In this work, we are dealing with sign language recognition, in
particular of French Sign Language (FSL). FSL has its own specificities, such
as the simultaneity of several parameters, the important role of the facial
expression or movement and the use of space for the proper utterance
organization. Unlike speech recognition, Frensh sign language (FSL) events
occur both sequentially and simultaneously. Thus, the computational processing
of FSL is too complex than the spoken languages. We present a novel approach
based on HMM to reduce the recognition complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3268</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3268</id><created>2013-04-11</created><authors><author><keyname>Aznag</keyname><forenames>Mustapha</forenames></author><author><keyname>Quafafou</keyname><forenames>Mohamed</forenames></author><author><keyname>Durand</keyname><forenames>Nicolas</forenames></author><author><keyname>Jarir</keyname><forenames>Zahi</forenames></author></authors><title>Web Services Discovery and Recommendation Based on Information
  Extraction and Symbolic Reputation</title><categories>cs.IR</categories><journal-ref>International Journal on Web Service Computing (IJWSC), Vol.4,
  No.1, March 2013</journal-ref><doi>10.5121/ijwsc.2013.4101</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper shows that the problem of web services representation is crucial
and analyzes the various factors that influence on it. It presents the
traditional representation of web services considering traditional textual
descriptions based on the information contained in WSDL files. Unfortunately,
textual web services descriptions are dirty and need significant cleaning to
keep only useful information. To deal with this problem, we introduce rules
based text tagging method, which allows filtering web service description to
keep only significant information. A new representation based on such filtered
data is then introduced. Many web services have empty descriptions. Also, we
consider web services representations based on the WSDL file structure (types,
attributes, etc.). Alternatively, we introduce a new representation called
symbolic reputation, which is computed from relationships between web services.
The impact of the use of these representations on web service discovery and
recommendation is studied and discussed in the experimentation using real world
web services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3271</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3271</id><created>2013-04-11</created><authors><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Google as God? Opportunities and Risks of the Information Age</title><categories>physics.soc-ph cs.CY</categories><comments>For more information see http://www.soms.ethz.ch and
  http://www.futurict.eu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If God did not exist - people would invent one. The development of human
civilization requires mechanisms promoting cooperation and social order. One of
these mechanisms is based on the idea that everything we do is seen and judged
by God - bad deeds will be punished, while good ones will be rewarded. The
Information Age has now fueled the dream that God-like omniscience and
omnipotence can be created by man.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3273</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3273</id><created>2013-04-11</created><authors><author><keyname>Hossain</keyname><forenames>Md. Kamal</forenames></author><author><keyname>El-Saleh</keyname><forenames>Ayman Abd</forenames></author></authors><title>Cognitive Radio Engine Model Utilizing Soft Fusion Based Genetic
  Algorithm For Cooperative Spectrum Optimization</title><categories>cs.NI</categories><comments>15 pages</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC),ISSN : 0974 - 9322[Online]; 0975 - 2293, 2013 [Print]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio (CR) is to detect the presence of primary users (PUs)
reliably in order to reduce the interference to licensed communications.
Genetic algorithms (GAs) are well suited for CR optimization problems to
increase efficiency of bandwidth utilization by manipulating its unused
portions of the apparent spectrum. In this paper, a binary genetic algorithm
(BGA)-based soft fusion (SF) scheme for cooperative spectrum sensing in
cognitive radio network is proposed to improve detection performance and
bandwidth utilization. The BGA-based optimization method is implemented at the
fusion centre of a linear SF scheme to optimize the weighting coefficients
vector to maximize global probability of detection performance. Simulation
results and analyses confirm that the proposed scheme meets real time
requirements of cognitive radio spectrum sensing and it outperforms
conventional natural deflection coefficient- (NDC-), modified deflection
coefficient- (MDC-), maximal ratio combining- (MRC-) and equal gain combining-
(EGC-) based SDF schemes as well as the OR-rule based hard decision fusion
(HDF). The propose BGA scheme also converges fast and achieves the optimum
performance, which means that BGA-based method is efficient and quite stable
also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3280</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3280</id><created>2013-04-11</created><authors><author><keyname>Shirazi</keyname><forenames>Avihay</forenames></author><author><keyname>Basher</keyname><forenames>Uria</forenames></author><author><keyname>Permuter</keyname><forenames>Haim</forenames></author></authors><title>Channel Coding and Source Coding with Increased Partial Side Information</title><categories>cs.IT math.IT</categories><acm-class>E.4; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let (S1,i, S2,i), distributed according to i.i.d p(s1, s2), i = 1, 2, . . .
be a memoryless, correlated partial side information sequence. In this work we
study channel coding and source coding problems where the partial side
information (S1, S2) is available at the encoder and the decoder, respectively,
and, additionally, either the encoder's or the decoder's side information is
increased by a limited-rate description of the other's partial side
information. We derive six special cases of channel coding and source coding
problems and we characterize the capacity and the rate-distortion functions for
the different cases. We present a duality between the channel capacity and the
rate-distortion cases we study. In order to find numerical solutions for our
channel capacity and rate-distortion problems, we use the Blahut-Arimoto
algorithm and convex optimization tools. As a byproduct of our work, we found a
tight lower bound on the Wyner-Ziv solution by formulating its Lagrange dual as
a geometric program. Previous results in the literature provide a geometric
programming formulation that is only a lower bound, but not necessarily tight.
Finally, we provide several examples corresponding to the channel capacity and
the rate-distortion cases we presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3285</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3285</id><created>2013-04-11</created><updated>2013-07-24</updated><authors><author><keyname>Reed</keyname><forenames>Colorado</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Scaling the Indian Buffet Process via Submodular Maximization</title><categories>stat.ML cs.LG</categories><comments>13 pages, 8 figures</comments><journal-ref>In ICML 2013: JMLR W&amp;CP 28 (3): 1013-1021, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference for latent feature models is inherently difficult as the inference
space grows exponentially with the size of the input data and number of latent
features. In this work, we use Kurihara &amp; Welling (2008)'s
maximization-expectation framework to perform approximate MAP inference for
linear-Gaussian latent feature models with an Indian Buffet Process (IBP)
prior. This formulation yields a submodular function of the features that
corresponds to a lower bound on the model evidence. By adding a constant to
this function, we obtain a nonnegative submodular function that can be
maximized via a greedy algorithm that obtains at least a one-third
approximation to the optimal solution. Our inference method scales linearly
with the size of the input data, and we show the efficacy of our method on the
largest datasets currently analyzed using an IBP model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3307</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3307</id><created>2013-04-11</created><authors><author><keyname>Gusev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Maslennikova</keyname><forenames>Marina I.</forenames></author><author><keyname>Pribavkina</keyname><forenames>Elena V.</forenames></author></authors><title>Principal ideal languages and synchronizing automata</title><categories>cs.FL</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study ideal languages generated by a single word. We provide an algorithm
to construct a strongly connected synchronizing automaton for which such a
language serves as the language of synchronizing words. Also we present a
compact formula to calculate the syntactic complexity of this language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3309</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3309</id><created>2013-04-11</created><authors><author><keyname>Vidakovic</keyname><forenames>Dragan</forenames></author><author><keyname>Parezanovic</keyname><forenames>Dusko</forenames></author><author><keyname>Nikolic</keyname><forenames>Olivera</forenames></author><author><keyname>Kaljevic</keyname><forenames>Jelena</forenames></author></authors><title>RSA Signature: Behind the Scenes</title><categories>cs.CR</categories><comments>13 pages</comments><msc-class>d.4.6</msc-class><acm-class>D.3.2</acm-class><journal-ref>Advanced Computing: An International Journal ( ACIJ ), Vol.4,
  No.2, March 2013</journal-ref><doi>10.5121/acij.2013.4203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a complete digital signature message stream, just
the way the RSA digital signature scheme does it. We will focus on the
operations with large numbers due to the fact that operating with large numbers
is the essence of RSA that cannot be understood by the usual illustrative
examples with small numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3313</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3313</id><created>2013-04-11</created><authors><author><keyname>S</keyname><forenames>Hemalatha</forenames></author><author><keyname>Acharya</keyname><forenames>U Dinesh</forenames></author><author><keyname>A</keyname><forenames>Renuka</forenames></author><author><keyname>Kamath</keyname><forenames>Priya R.</forenames></author></authors><title>A Secure Color Image Steganography In Transform Domain</title><categories>cs.CR</categories><comments>8 pages</comments><journal-ref>International Journal on Cryptography and Information Security
  (IJCIS), Vol.3, No.1, March 2013</journal-ref><doi>10.5121/ijcis.2013.3103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is the art and science of covert communication. The secret
information can be concealed in content such as image, audio, or video. This
paper provides a novel image steganography technique to hide both image and key
in color cover image using Discrete Wavelet Transform (DWT) and Integer Wavelet
Transform (IWT). There is no visual difference between the stego image and the
cover image. The extracted image is also similar to the secret image. This is
proved by the high PSNR (Peak Signal to Noise Ratio), value for both stego and
extracted secret image. The results are compared with the results of similar
techniques and it is found that the proposed technique is simple and gives
better PSNR values than others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3345</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3345</id><created>2013-04-11</created><authors><author><keyname>Parandehgheibi</keyname><forenames>Marzieh</forenames></author></authors><title>Probabilistic Classification using Fuzzy Support Vector Machines</title><categories>cs.LG math.ST stat.TH</categories><comments>6 pages, Proceedings of the 6th INFORMS Workshop on Data Mining and
  Health Informatics (DM-HI 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In medical applications such as recognizing the type of a tumor as Malignant
or Benign, a wrong diagnosis can be devastating. Methods like Fuzzy Support
Vector Machines (FSVM) try to reduce the effect of misplaced training points by
assigning a lower weight to the outliers. However, there are still uncertain
points which are similar to both classes and assigning a class by the given
information will cause errors. In this paper, we propose a two-phase
classification method which probabilistically assigns the uncertain points to
each of the classes. The proposed method is applied to the Breast Cancer
Wisconsin (Diagnostic) Dataset which consists of 569 instances in 2 classes of
Malignant and Benign. This method assigns certain instances to their
appropriate classes with probability of one, and the uncertain instances to
each of the classes with associated probabilities. Therefore, based on the
degree of uncertainty, doctors can suggest further examinations before making
the final diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3357</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3357</id><created>2013-04-11</created><authors><author><keyname>Khairnar</keyname><forenames>Vaishali D.</forenames></author><author><keyname>Kotecha</keyname><forenames>Ketan</forenames></author></authors><title>Performance of Vehicle-to-Vehicle Communication using IEEE 802.11p in
  Vehicular Ad-hoc Network Environment</title><categories>cs.NI</categories><comments>28 pages, 18 figures, International Journal of Network Security &amp; Its
  Applications, International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.5, No.2, March 2013</comments><doi>10.5121/ijnsa.2013.5212</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic safety applications using vehicle-to-vehicle (V2V) communication is
an emerging and promising area within the ITS environment. Many of these
applications require real-time communication with high reliability. To meet a
real-time deadline, timely and predictable access to the channel is paramount.
The medium access method used in 802.11p, CSMA with collision avoidance, does
not guarantee channel access before a finite deadline. The well-known property
of CSMA is undesirable for critical communications scenarios. The simulation
results reveal that a specific vehicle is forced to drop over 80% of its
packets because no channel access was possible before the next message was
generated. To overcome this problem, we propose to use STDMA for real-time data
traffic between vehicles. The real- time properties of STDMA are investigated
by means of the highway road simulation scenario, with promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3362</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3362</id><created>2013-04-11</created><authors><author><keyname>Gomes</keyname><forenames>Jorge</forenames></author><author><keyname>Urbano</keyname><forenames>Paulo</forenames></author><author><keyname>Christensen</keyname><forenames>Anders Lyhne</forenames></author></authors><title>Evolution of Swarm Robotics Systems with Novelty Search</title><categories>cs.NE</categories><comments>To appear in Swarm Intelligence (2013), ANTS Special Issue. The final
  publication will be available at link.springer.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novelty search is a recent artificial evolution technique that challenges
traditional evolutionary approaches. In novelty search, solutions are rewarded
based on their novelty, rather than their quality with respect to a predefined
objective. The lack of a predefined objective precludes premature convergence
caused by a deceptive fitness function. In this paper, we apply novelty search
combined with NEAT to the evolution of neural controllers for homogeneous
swarms of robots. Our empirical study is conducted in simulation, and we use a
common swarm robotics task - aggregation, and a more challenging task - sharing
of an energy recharging station. Our results show that novelty search is
unaffected by deception, is notably effective in bootstrapping the evolution,
can find solutions with lower complexity than fitness-based evolution, and can
find a broad diversity of solutions for the same task. Even in non-deceptive
setups, novelty search achieves solution qualities similar to those obtained in
traditional fitness-based evolution. Our study also encompasses variants of
novelty search that work in concert with fitness-based evolution to combine the
exploratory character of novelty search with the exploitatory character of
objective-based evolution. We show that these variants can further improve the
performance of novelty search. Overall, our study shows that novelty search is
a promising alternative for the evolution of controllers for robotic swarms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3365</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3365</id><created>2013-04-11</created><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Sinop</keyname><forenames>Ali Kemal</forenames></author></authors><title>Towards a better approximation for sparsest cut?</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new $(1+\epsilon)$-approximation for sparsest cut problem on graphs
where small sets expand significantly more than the sparsest cut (sets of size
$n/r$ expand by a factor $\sqrt{\log n\log r}$ bigger, for some small $r$; this
condition holds for many natural graph families). We give two different
algorithms. One involves Guruswami-Sinop rounding on the level-$r$ Lasserre
relaxation. The other is combinatorial and involves a new notion called {\em
Small Set Expander Flows} (inspired by the {\em expander flows} of ARV) which
we show exists in the input graph. Both algorithms run in time $2^{O(r)}
\mathrm{poly}(n)$. We also show similar approximation algorithms in graphs with
genus $g$ with an analogous local expansion condition. This is the first
algorithm we know of that achieves $(1+\epsilon)$-approximation on such general
family of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3367</identifier>
 <datestamp>2013-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3367</id><created>2013-04-11</created><authors><author><keyname>Elkouss</keyname><forenames>David</forenames></author><author><keyname>Martinez-Mateo</keyname><forenames>Jesus</forenames></author><author><keyname>Martin</keyname><forenames>Vicente</forenames></author></authors><title>Analysis of a rate-adaptive reconciliation protocol and the effect of
  the leakage on the secret key rate</title><categories>quant-ph cs.IT math.IT</categories><comments>8 pages, 2 figures, accepted for publication in Physical Review A.
  This submission supersedes arXiv:1007.0904</comments><journal-ref>Physical Review A 87, 042334 (2013)</journal-ref><doi>10.1103/PhysRevA.87.042334</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum key distribution performs the trick of growing a secret key in two
distant places connected by a quantum channel. The main reason is that the
legitimate users can bound the information gathered by the eavesdropper. In
practical systems, whether because of finite resources or external conditions,
the quantum channel is subject to fluctuations. A rate adaptive information
reconciliation protocol, that adapts to the changes in the communication
channel, is then required to minimize the leakage of information in the
classical postprocessing.
  We consider here the leakage of a rate-adaptive information reconciliation
protocol. The length of the exchanged messages is larger than that of an
optimal protocol; however, we prove that the min-entropy reduction is limited.
The simulation results, both on the asymptotic and in the finite-length regime,
show that this protocol allows to increase the amount of distillable secret
key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3375</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3375</id><created>2013-04-11</created><authors><author><keyname>Rudolf</keyname><forenames>Boris</forenames></author><author><keyname>Marko\vsov&#xe1;</keyname><forenames>M&#xe1;ria</forenames></author><author><keyname>\vCaj&#xe1;gi</keyname><forenames>Martin</forenames></author><author><keyname>Ti\vno</keyname><forenames>Peter</forenames></author></authors><title>Degree distribution and scaling in the Connecting Nearest Neighbors
  model</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>21 pages, 3 figures</comments><journal-ref>Physical Review E 85(2012)026144</journal-ref><doi>10.1103/PhysRevE.85.026114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a detailed analysis of the Connecting Nearest Neighbors (CNN)
model by V\'azquez. We show that the degree distribution follows a power law,
but the scaling exponent can vary with the parameter setting. Moreover, the
correspondence of the growing version of the Connecting Nearest Neighbors
(GCNN) model to the particular random walk model (PRW model) and recursive
search model (RS model) is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3390</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3390</id><created>2013-04-11</created><authors><author><keyname>Green</keyname><forenames>Alexander S.</forenames></author><author><keyname>Lumsdaine</keyname><forenames>Peter LeFanu</forenames></author><author><keyname>Ross</keyname><forenames>Neil J.</forenames></author><author><keyname>Selinger</keyname><forenames>Peter</forenames></author><author><keyname>Valiron</keyname><forenames>Beno&#xee;t</forenames></author></authors><title>Quipper: A Scalable Quantum Programming Language</title><categories>cs.PL cs.ET quant-ph</categories><comments>10 pages, PLDI 2013</comments><acm-class>D.3.1</acm-class><journal-ref>ACM SIGPLAN Notices 48(6):333-342, 2013</journal-ref><doi>10.1145/2499370.2462177</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of quantum algorithms is vibrant. Still, there is currently a lack
of programming languages for describing quantum computation on a practical
scale, i.e., not just at the level of toy problems. We address this issue by
introducing Quipper, a scalable, expressive, functional, higher-order quantum
programming language. Quipper has been used to program a diverse set of
non-trivial quantum algorithms, and can generate quantum gate representations
using trillions of gates. It is geared towards a model of computation that uses
a classical computer to control a quantum device, but is not dependent on any
particular model of quantum hardware. Quipper has proven effective and easy to
use, and opens the door towards using formal methods to analyze quantum
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3393</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3393</id><created>2013-04-11</created><authors><author><keyname>Gomes</keyname><forenames>Jorge</forenames></author><author><keyname>Christensen</keyname><forenames>Anders Lyhne</forenames></author></authors><title>Generic Behaviour Similarity Measures for Evolutionary Swarm Robotics</title><categories>cs.NE</categories><comments>Initial submission. Final version to appear in GECCO 2013 and
  dl.acm.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novelty search has shown to be a promising approach for the evolution of
controllers for swarm robotics. In existing studies, however, the experimenter
had to craft a domain dependent behaviour similarity measure to use novelty
search in swarm robotics applications. The reliance on hand-crafted similarity
measures places an additional burden to the experimenter and introduces a bias
in the evolutionary process. In this paper, we propose and compare two
task-independent, generic behaviour similarity measures: combined state count
and sampled average state. The proposed measures use the values of sensors and
effectors recorded for each individual robot of the swarm. The characterisation
of the group-level behaviour is then obtained by combining the sensor-effector
values from all the robots. We evaluate the proposed measures in an aggregation
task and in a resource sharing task. We show that the generic measures match
the performance of domain dependent measures in terms of solution quality. Our
results indicate that the proposed generic measures operate as effective
behaviour similarity measures, and that it is possible to leverage the benefits
of novelty search without having to craft domain specific similarity measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3396</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3396</id><created>2013-04-11</created><authors><author><keyname>Mundra</keyname><forenames>Ankit</forenames></author><author><keyname>Gupta</keyname><forenames>Bhagvan K.</forenames></author><author><keyname>Rathee</keyname><forenames>Geetanjali</forenames></author><author><keyname>Chawla</keyname><forenames>Meenu</forenames></author><author><keyname>Rakesh</keyname><forenames>Nitin</forenames></author><author><keyname>Tyagi</keyname><forenames>Vipin</forenames></author></authors><title>Validated Real Time Middle Ware For Distributed Cyber Physical Systems
  Using HMM</title><categories>cs.DC</categories><comments>11 Pages, 4 figures</comments><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.4, No.2, March 2013</journal-ref><doi>10.5121/ijdps.2013.4204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Cyber Physical Systems designed for different scenario must be
capable enough to perform in an efficient manner in every situation. Earlier
approaches, such as CORBA, has performed but with different time constraints.
Therefore, there was the need to design reconfigurable, robust, validated and
consistent real time middle ware systems with end-to-end timing. In the
DCPS-HMM we have proposed the processor efficiency and data validation which
may proof crucial in implementing various distributed systems such as credit
card systems or file transfer through network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3402</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3402</id><created>2013-04-11</created><authors><author><keyname>Morin</keyname><forenames>Pat</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>On the Average Number of Edges in Theta Graphs</title><categories>cs.CG math.CO</categories><comments>20 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theta graphs are important geometric graphs that have many applications,
including wireless networking, motion planning, real-time animation, and
minimum-spanning tree construction. We give closed form expressions for the
average degree of theta graphs of a homogeneous Poisson point process over the
plane. We then show that essentially the same bounds---with vanishing error
terms---hold for theta graphs of finite sets of points that are uniformly
distributed in a square. Finally, we show that the number of edges in a theta
graph of points uniformly distributed in a square is concentrated around its
expected value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3405</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3405</id><created>2013-04-11</created><authors><author><keyname>Sharma</keyname><forenames>Amit</forenames></author><author><keyname>Cosley</keyname><forenames>Dan</forenames></author></authors><title>Do Social Explanations Work? Studying and Modeling the Effects of Social
  Explanations in Recommender Systems</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>11 pages, WWW 2013</comments><acm-class>H.1.2; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems associated with social networks often use social
explanations (e.g. &quot;X, Y and 2 friends like this&quot;) to support the
recommendations. We present a study of the effects of these social explanations
in a music recommendation context. We start with an experiment with 237 users,
in which we show explanations with varying levels of social information and
analyze their effect on users' decisions. We distinguish between two key
decisions: the likelihood of checking out the recommended artist, and the
actual rating of the artist based on listening to several songs. We find that
while the explanations do have some influence on the likelihood, there is
little correlation between the likelihood and actual (listening) rating for the
same artist. Based on these insights, we present a generative probabilistic
model that explains the interplay between explanations and background
information on music preferences, and how that leads to a final likelihood
rating for an artist. Acknowledging the impact of explanations, we discuss a
general recommendation framework that models external informational elements in
the recommendation interface, in addition to inherent preferences of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3406</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3406</id><created>2013-04-11</created><authors><author><keyname>Alemohammad</keyname><forenames>Seyed Hamed</forenames></author><author><keyname>Entekhabi</keyname><forenames>Dara</forenames></author></authors><title>Merging Satellite Measurements of Rainfall Using Multi-scale Imagery
  Technique</title><categories>cs.CV cs.IR</categories><comments>6 pages, 10 Figures, WCRP Open Science Conference, 2011</comments><msc-class>94A12 (Primary), 62P12</msc-class><acm-class>I.4; I.4.5; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several passive microwave satellites orbit the Earth and measure rainfall.
These measurements have the advantage of almost full global coverage when
compared to surface rain gauges. However, these satellites have low temporal
revisit and missing data over some regions. Image fusion is a useful technique
to fill in the gaps of one image (one satellite measurement) using another one.
The proposed algorithm uses an iterative fusion scheme to integrate information
from two satellite measurements. The algorithm is implemented on two datasets
for 7 years of half-hourly data. The results show significant improvements in
rain detection and rain intensity in the merged measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3418</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3418</id><created>2013-03-27</created><authors><author><keyname>Grosof</keyname><forenames>Benjamin N.</forenames></author></authors><title>An Inequality Paradigm for Probabilistic Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-1-8</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an inequality paradigm for probabilistic reasoning based on a
logic of upper and lower bounds on conditional probabilities. We investigate a
family of probabilistic logics, generalizing the work of Nilsson [14]. We
develop a variety of logical notions for probabilistic reasoning, including
soundness, completeness justification; and convergence: reduction of a theory
to a simpler logical class. We argue that a bound view is especially useful for
describing the semantics of probabilistic knowledge representation and for
describing intermediate states of probabilistic inference and updating. We show
that the Dempster-Shafer theory of evidence is formally identical to a special
case of our generalized probabilistic logic. Our paradigm thus incorporates
both Bayesian &quot;rule-based&quot; approaches and avowedly non-Bayesian &quot;evidential&quot;
approaches such as MYCIN and DempsterShafer. We suggest how to integrate the
two &quot;schools&quot;, and explore some possibilities for novel synthesis of a variety
of ideas in probabilistic reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3419</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3419</id><created>2013-03-27</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Probabilistic Interpretations for MYCIN's Certainty Factors</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-9-20</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the quantities used by MYCIN to reason with uncertainty,
called certainty factors. It is shown that the original definition of certainty
factors is inconsistent with the functions used in MYCIN to combine the
quantities. This inconsistency is used to argue for a redefinition of certainty
factors in terms of the intuitively appealing desiderata associated with the
combining functions. It is shown that this redefinition accommodates an
unlimited number of probabilistic interpretations. These interpretations are
shown to be monotonic transformations of the likelihood ratio p(EIH)/p(El H).
The construction of these interpretations provides insight into the assumptions
implicit in the certainty factor model. In particular, it is shown that if
uncertainty is to be propagated through an inference network in accordance with
the desiderata, evidence must be conditionally independent given the hypothesis
and its negation and the inference network must have a tree structure. It is
emphasized that assumptions implicit in the model are rarely true in practical
applications. Methods for relaxing the assumptions are suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3420</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3420</id><created>2013-03-27</created><authors><author><keyname>Hunter</keyname><forenames>Daniel</forenames></author></authors><title>Uncertain Reasoning Using Maximum Entropy Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-21-27</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of maximum entropy inference in reasoning with uncertain information
is commonly justified by an information-theoretic argument. This paper
discusses a possible objection to this information-theoretic justification and
shows how it can be met. I then compare maximum entropy inference with certain
other currently popular methods for uncertain reasoning. In making such a
comparison, one must distinguish between static and dynamic theories of degrees
of belief: a static theory concerns the consistency conditions for degrees of
belief at a given time; whereas a dynamic theory concerns how one's degrees of
belief should change in the light of new information. It is argued that maximum
entropy is a dynamic theory and that a complete theory of uncertain reasoning
can be gotten by combining maximum entropy inference with probability theory,
which is a static theory. This total theory, I argue, is much better grounded
than are other theories of uncertain reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3421</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3421</id><created>2013-03-27</created><authors><author><keyname>Johnson</keyname><forenames>Rodney W.</forenames></author></authors><title>Independence and Bayesian Updating Methods</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-28-30</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Duda, Hart, and Nilsson have set forth a method for rule-based inference
systems to use in updating the probabilities of hypotheses on the basis of
multiple items of new evidence. Pednault, Zucker, and Muresan claimed to give
conditions under which independence assumptions made by Duda et al. preclude
updating-that is, prevent the evidence from altering the probabilities of the
hypotheses. Glymour refutes Pednault et al.'s claim with a counterexample of a
rather special form (one item of evidence is incompatible with all but one of
the hypotheses); he raises, but leaves open, the question whether their result
would be true with an added assumption to rule out such special cases. We show
that their result does not hold even with the added assumption, but that it can
nevertheless be largely salvaged. Namely, under the conditions assumed by
Pednault et al., at most one of the items of evidence can alter the probability
of any given hypothesis; thus, although updating is possible, multiple updating
for any of the hypotheses is precluded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3422</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3422</id><created>2013-03-27</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>A Constraint Propagation Approach to Probabilistic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-31-42</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper demonstrates that strict adherence to probability theory does not
preclude the use of concurrent, self-activated constraint-propagation
mechanisms for managing uncertainty. Maintaining local records of
sources-of-belief allows both predictive and diagnostic inferences to be
activated simultaneously and propagate harmoniously towards a stable
equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3423</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3423</id><created>2013-03-27</created><authors><author><keyname>Shore</keyname><forenames>John E.</forenames></author></authors><title>Relative Entropy, Probabilistic Inference and AI</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-43-47</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various properties of relative entropy have led to its widespread use in
information theory. These properties suggest that relative entropy has a role
to play in systems that attempt to perform inference in terms of probability
distributions. In this paper, I will review some basic properties of relative
entropy as well as its role in probabilistic inference. I will also mention
briefly a few existing and potential applications of relative entropy to
so-called artificial intelligence (AI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3424</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3424</id><created>2013-03-27</created><authors><author><keyname>Solomonoff</keyname><forenames>Ray</forenames></author></authors><title>Foundations of Probability Theory for AI - The Application of
  Algorithmic Probability to Problems in Artificial Intelligence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-48-56</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper covers two topics: first an introduction to Algorithmic Complexity
Theory: how it defines probability, some of its characteristic properties and
past successful applications. Second, we apply it to problems in A.I. - where
it promises to give near optimum search procedures for two very broad classes
of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3425</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3425</id><created>2013-03-27</created><authors><author><keyname>Bonissone</keyname><forenames>Piero P.</forenames></author><author><keyname>Decker</keyname><forenames>Keith S.</forenames></author></authors><title>Selecting Uncertainty Calculi and Granularity: An Experiment in
  Trading-Off Precision and Complexity</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-57-66</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The management of uncertainty in expert systems has usually been left to ad
hoc representations and rules of combinations lacking either a sound theory or
clear semantics. The objective of this paper is to establish a theoretical
basis for defining the syntax and semantics of a small subset of calculi of
uncertainty operating on a given term set of linguistic statements of
likelihood. Each calculus is defined by specifying a negation, a conjunction
and a disjunction operator. Families of Triangular norms and conorms constitute
the most general representations of conjunction and disjunction operators.
These families provide us with a formalism for defining an infinite number of
different calculi of uncertainty. The term set will define the uncertainty
granularity, i.e. the finest level of distinction among different
quantifications of uncertainty. This granularity will limit the ability to
differentiate between two similar operators. Therefore, only a small finite
subset of the infinite number of calculi will produce notably different
results. This result is illustrated by two experiments where nine and eleven
different calculi of uncertainty are used with three term sets containing five,
nine, and thirteen elements, respectively. Finally, the use of context
dependent rule set is proposed to select the most appropriate calculus for any
given situation. Such a rule set will be relatively small since it must only
describe the selection policies for a small number of calculi (resulting from
the analyzed trade-off between complexity and precision).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3426</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3426</id><created>2013-03-27</created><authors><author><keyname>Cohen</keyname><forenames>Marvin S.</forenames></author></authors><title>A Framework for Non-Monotonic Reasoning About Probabilistic Assumptions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-67-75</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attempts to replicate probabilistic reasoning in expert systems have
typically overlooked a critical ingredient of that process. Probabilistic
analysis typically requires extensive judgments regarding interdependencies
among hypotheses and data, and regarding the appropriateness of various
alternative models. The application of such models is often an iterative
process, in which the plausibility of the results confirms or disconfirms the
validity of assumptions made in building the model. In current expert systems,
by contrast, probabilistic information is encapsulated within modular rules
(involving, for example, &quot;certainty factors&quot;), and there is no mechanism for
reviewing the overall form of the probability argument or the validity of the
judgments entering into it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3427</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3427</id><created>2013-03-27</created><authors><author><keyname>Fung</keyname><forenames>Robert</forenames></author><author><keyname>Chong</keyname><forenames>Chee Yee</forenames></author></authors><title>Metaprobability and Dempster-Shafer in Evidential Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-76-83</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidential reasoning in expert systems has often used ad-hoc uncertainty
calculi. Although it is generally accepted that probability theory provides a
firm theoretical foundation, researchers have found some problems with its use
as a workable uncertainty calculus. Among these problems are representation of
ignorance, consistency of probabilistic judgements, and adjustment of a priori
judgements with experience. The application of metaprobability theory to
evidential reasoning is a new approach to solving these problems.
Metaprobability theory can be viewed as a way to provide soft or hard
constraints on beliefs in much the same manner as the Dempster-Shafer theory
provides constraints on probability masses on subsets of the state space. Thus,
we use the Dempster-Shafer theory, an alternative theory of evidential
reasoning to illuminate metaprobability theory as a theory of evidential
reasoning. The goal of this paper is to compare how metaprobability theory and
Dempster-Shafer theory handle the adjustment of beliefs with evidence with
respect to a particular thought experiment. Sections 2 and 3 give brief
descriptions of the metaprobability and Dempster-Shafer theories.
Metaprobability theory deals with higher order probabilities applied to
evidential reasoning. Dempster-Shafer theory is a generalization of probability
theory which has evolved from a theory of upper and lower probabilities.
Section 4 describes a thought experiment and the metaprobability and
DempsterShafer analysis of the experiment. The thought experiment focuses on
forming beliefs about a population with 6 types of members {1, 2, 3, 4, 5, 6}.
A type is uniquely defined by the values of three features: A, B, C. That is,
if the three features of one member of the population were known then its type
could be ascertained. Each of the three features has two possible values, (e.g.
A can be either &quot;a0&quot; or &quot;al&quot;). Beliefs are formed from evidence accrued from
two sensors: sensor A, and sensor B. Each sensor senses the corresponding
defining feature. Sensor A reports that half of its observations are &quot;a0&quot; and
half the observations are 'al'. Sensor B reports that half of its observations
are ``b0,' and half are &quot;bl&quot;. Based on these two pieces of evidence, what
should be the beliefs on the distribution of types in the population? Note that
the third feature is not observed by any sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3428</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3428</id><created>2013-03-27</created><authors><author><keyname>Ginsberg</keyname><forenames>Matthew L.</forenames></author></authors><title>Implementing Probabilistic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-84-90</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  General problems in analyzing information in a probabilistic database are
considered. The practical difficulties (and occasional advantages) of storing
uncertain data, of using it conventional forward- or backward-chaining
inference engines, and of working with a probabilistic version of resolution
are discussed. The background for this paper is the incorporation of uncertain
reasoning facilities in MRS, a general-purpose expert system building tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3429</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3429</id><created>2013-03-27</created><authors><author><keyname>Shafer</keyname><forenames>Glenn</forenames></author></authors><title>Probability Judgement in Artificial Intelligence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-91-98</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with two theories of probability judgment: the
Bayesian theory and the theory of belief functions. It illustrates these
theories with some simple examples and discusses some of the issues that arise
when we try to implement them in expert systems. The Bayesian theory is well
known; its main ideas go back to the work of Thomas Bayes (1702-1761). The
theory of belief functions, often called the Dempster-Shafer theory in the
artificial intelligence community, is less well known, but it has even older
antecedents; belief-function arguments appear in the work of George Hooper
(16401723) and James Bernoulli (1654-1705). For elementary expositions of the
theory of belief functions, see Shafer (1976, 1985).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3430</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3430</id><created>2013-03-27</created><authors><author><keyname>Wise</keyname><forenames>Ben P.</forenames></author><author><keyname>Henrion</keyname><forenames>Max</forenames></author></authors><title>A Framework for Comparing Uncertain Inference Systems to Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-99-108</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several different uncertain inference systems (UISs) have been developed for
representing uncertainty in rule-based expert systems. Some of these, such as
Mycin's Certainty Factors, Prospector, and Bayes' Networks were designed as
approximations to probability, and others, such as Fuzzy Set Theory and
DempsterShafer Belief Functions were not. How different are these UISs in
practice, and does it matter which you use? When combining and propagating
uncertain information, each UIS must, at least by implication, make certain
assumptions about correlations not explicily specified. The maximum entropy
principle with minimum cross-entropy updating, provides a way of making
assumptions about the missing specification that minimizes the additional
information assumed, and thus offers a standard against which the other UISs
can be compared. We describe a framework for the experimental comparison of the
performance of different UISs, and provide some illustrative results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3431</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3431</id><created>2013-03-27</created><authors><author><keyname>Dalkey</keyname><forenames>Norman C.</forenames></author></authors><title>Inductive Inference and the Representation of Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-109-116</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The form and justification of inductive inference rules depend strongly on
the representation of uncertainty. This paper examines one generic
representation, namely, incomplete information. The notion can be formalized by
presuming that the relevant probabilities in a decision problem are known only
to the extent that they belong to a class K of probability distributions. The
concept is a generalization of a frequent suggestion that uncertainty be
represented by intervals or ranges on probabilities. To make the representation
useful for decision making, an inductive rule can be formulated which
determines, in a well-defined manner, a best approximation to the unknown
probability, given the set K. In addition, the knowledge set notion entails a
natural procedure for updating -- modifying the set K given new evidence.
Several non-intuitive consequences of updating emphasize the differences
between inference with complete and inference with incomplete information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3432</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3432</id><created>2013-03-27</created><authors><author><keyname>Hanson</keyname><forenames>Stephen Jose</forenames></author><author><keyname>Bauer</keyname><forenames>Malcolm</forenames></author></authors><title>Machine Learning, Clustering, and Polymorphy</title><categories>cs.AI cs.CL cs.LG</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-117-128</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a machine induction program (WITT) that attempts to
model human categorization. Properties of categories to which human subjects
are sensitive includes best or prototypical members, relative contrasts between
putative categories, and polymorphy (neither necessary or sufficient features).
This approach represents an alternative to usual Artificial Intelligence
approaches to generalization and conceptual clustering which tend to focus on
necessary and sufficient feature rules, equivalence classes, and simple search
and match schemes. WITT is shown to be more consistent with human
categorization while potentially including results produced by more traditional
clustering schemes. Applications of this approach in the domains of expert
systems and information retrieval are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3433</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3433</id><created>2013-03-27</created><authors><author><keyname>Rendell</keyname><forenames>Larry</forenames></author></authors><title>Induction, of and by Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-129-134</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines some methods and ideas underlying the author's successful
probabilistic learning systems(PLS), which have proven uniquely effective and
efficient in generalization learning or induction. While the emerging
principles are generally applicable, this paper illustrates them in heuristic
search, which demands noise management and incremental learning. In our
approach, both task performance and learning are guided by probability.
Probabilities are incrementally normalized and revised, and their errors are
located and corrected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3434</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3434</id><created>2013-03-27</created><authors><author><keyname>Vaughan</keyname><forenames>David S.</forenames></author><author><keyname>Perrin</keyname><forenames>Bruce M.</forenames></author><author><keyname>Yadrick</keyname><forenames>Robert M.</forenames></author><author><keyname>Holden</keyname><forenames>Peter D.</forenames></author><author><keyname>Kempf</keyname><forenames>Karl G.</forenames></author></authors><title>An Odds Ratio Based Inference Engine</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-135-142</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expert systems applications that involve uncertain inference can be
represented by a multidimensional contingency table. These tables offer a
general approach to inferring with uncertain evidence, because they can embody
any form of association between any number of pieces of evidence and
conclusions. (Simpler models may be required, however, if the number of pieces
of evidence bearing on a conclusion is large.) This paper presents a method of
using these tables to make uncertain inferences without assumptions of
conditional independence among pieces of evidence or heuristic combining rules.
As evidence is accumulated, new joint probabilities are calculated so as to
maintain any dependencies among the pieces of evidence that are found in the
contingency table. The new conditional probability of the conclusion is then
calculated directly from these new joint probabilities and the conditional
probabilities in the contingency table.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3435</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3435</id><created>2013-03-27</created><authors><author><keyname>Ben-Bassat</keyname><forenames>Moshe</forenames></author><author><keyname>Maler</keyname><forenames>Oded</forenames></author></authors><title>A Framework for Control Strategies in Uncertain Inference Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-143-151</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control Strategies for hierarchical tree-like probabilistic inference
networks are formulated and investigated. Strategies that utilize staged
look-ahead and temporary focus on subgoals are formalized and refined using the
Depth Vector concept that serves as a tool for defining the 'virtual tree'
regarded by the control strategy. The concept is illustrated by four types of
control strategies for three-level trees that are characterized according to
their Depth Vector, and according to the way they consider intermediate nodes
and the role that they let these nodes play. INFERENTI is a computerized
inference system written in Prolog, which provides tools for exercising a
variety of control strategies. The system also provides tools for simulating
test data and for comparing the relative average performance under different
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3436</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3436</id><created>2013-03-27</created><authors><author><keyname>Hamburger</keyname><forenames>Henry</forenames></author></authors><title>Combining Uncertain Estimates</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-152-159</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a real expert system, one may have unreliable, unconfident, conflicting
estimates of the value for a particular parameter. It is important for decision
making that the information present in this aggregate somehow find its way into
use. We cast the problem of representing and combining uncertain estimates as
selection of two kinds of functions, one to determine an estimate, the other
its uncertainty. The paper includes a long list of properties that such
functions should satisfy, and it presents one method that satisfies them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3437</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3437</id><created>2013-03-27</created><authors><author><keyname>Lemmer</keyname><forenames>John F.</forenames></author></authors><title>Confidence Factors, Empiricism and the Dempster-Shafer Theory of
  Evidence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-160-176</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issue of confidence factors in Knowledge Based Systems has become
increasingly important and Dempster-Shafer (DS) theory has become increasingly
popular as a basis for these factors. This paper discusses the need for an
empirical lnterpretatlon of any theory of confidence factors applied to
Knowledge Based Systems and describes an empirical lnterpretatlon of DS theory
suggesting that the theory has been extensively misinterpreted. For the
essentially syntactic DS theory, a model is developed based on sample spaces,
the traditional semantic model of probability theory. This model is used to
show that, if belief functions are based on reasonably accurate sampling or
observation of a sample space, then the beliefs and upper probabilities as
computed according to DS theory cannot be interpreted as frequency ratios.
Since many proposed applications of DS theory use belief functions in
situations with statistically derived evidence (Wesley [1]) and seem to appeal
to statistical intuition to provide an lnterpretatlon of the results as has
Garvey [2], it may be argued that DS theory has often been misapplied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3438</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3438</id><created>2013-03-27</created><authors><author><keyname>Bundy</keyname><forenames>Alan</forenames></author></authors><title>Incidence Calculus: A Mechanism for Probabilistic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-177-184</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mechanisms for the automation of uncertainty are required for expert systems.
Sometimes these mechanisms need to obey the properties of probabilistic
reasoning. A purely numeric mechanism, like those proposed so far, cannot
provide a probabilistic logic with truth functional connectives. We propose an
alternative mechanism, Incidence Calculus, which is based on a representation
of uncertainty using sets of points, which might represent situations, models
or possible worlds. Incidence Calculus does provide a probabilistic logic with
truth functional connectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3439</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3439</id><created>2013-03-27</created><authors><author><keyname>Grosof</keyname><forenames>Benjamin N.</forenames></author></authors><title>Evidential Confirmation as Transformed Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-185-192</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A considerable body of work in AI has been concerned with aggregating
measures of confirmatory and disconfirmatory evidence for a common set of
propositions. Claiming classical probability to be inadequate or inappropriate,
several researchers have gone so far as to invent new formalisms and methods.
We show how to represent two major such alternative approaches to evidential
confirmation not only in terms of transformed (Bayesian) probability, but also
in terms of each other. This unifies two of the leading approaches to
confirmation theory, by showing that a revised MYCIN Certainty Factor method
[12] is equivalent to a special case of Dempster-Shafer theory. It yields a
well-understood axiomatic basis, i.e. conditional independence, to interpret
previous work on quantitative confirmation theory. It substantially resolves
the &quot;taxe-them-or-leave-them&quot; problem of priors: MYCIN had to leave them out,
while PROSPECTOR had to have them in. It recasts some of confirmation theory's
advantages in terms of the psychological accessibility of probabilistic
information in different (transformed) formats. Finally, it helps to unify the
representation of uncertain reasoning (see also [11]).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3440</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3440</id><created>2013-03-27</created><authors><author><keyname>Loui</keyname><forenames>Ronald P.</forenames></author></authors><title>Interval-Based Decisions for Reasoning Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-193-200</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This essay looks at decision-making with interval-valued probability
measures. Existing decision methods have either supplemented expected utility
methods with additional criteria of optimality, or have attempted to supplement
the interval-valued measures. We advocate a new approach, which makes the
following questions moot: 1. which additional criteria to use, and 2. how wide
intervals should be. In order to implement the approach, we need more
epistemological information. Such information can be generated by a rule of
acceptance with a parameter that allows various attitudes toward error, or can
simply be declared. In sketch, the argument is: 1. probability intervals are
useful and natural in All. systems; 2. wide intervals avoid error, but are
useless in some risk sensitive decision-making; 3. one may obtain narrower
intervals if one is less cautious; 4. if bodies of knowledge can be ordered by
their caution, one should perform the decision analysis with the acceptable
body of knowledge that is the most cautious, of those that are useful. The
resulting behavior differs from that of a behavioral probabilist (a Bayesian)
because in the proposal, 5. intervals based on successive bodies of knowledge
are not always nested; 6. if the agent uses a probability for a particular
decision, she need not commit to that probability for credence or future
decision; and 7. there may be no acceptable body of knowledge that is useful;
hence, sometimes no decision is mandated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3441</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3441</id><created>2013-03-27</created><authors><author><keyname>Corter</keyname><forenames>James E.</forenames></author><author><keyname>Gluck</keyname><forenames>Mark A.</forenames></author></authors><title>Machine Generalization and Human Categorization: An
  Information-Theoretic View</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-201-207</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In designing an intelligent system that must be able to explain its reasoning
to a human user, or to provide generalizations that the human user finds
reasonable, it may be useful to take into consideration psychological data on
what types of concepts and categories people naturally use. The psychological
literature on concept learning and categorization provides strong evidence that
certain categories are more easily learned, recalled, and recognized than
others. We show here how a measure of the informational value of a category
predicts the results of several important categorization experiments better
than standard alternative explanations. This suggests that information-based
approaches to machine generalization may prove particularly useful and natural
for human users of the systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3442</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3442</id><created>2013-03-27</created><authors><author><keyname>Holtzman</keyname><forenames>Samuel</forenames></author><author><keyname>Breese</keyname><forenames>John S.</forenames></author></authors><title>Exact Reasoning Under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-208-216</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on designing expert systems to support decision making in
complex, uncertain environments. In this context, our research indicates that
strictly probabilistic representations, which enable the use of
decision-theoretic reasoning, are highly preferable to recently proposed
alternatives (e.g., fuzzy set theory and Dempster-Shafer theory). Furthermore,
we discuss the language of influence diagrams and a corresponding methodology
-decision analysis -- that allows decision theory to be used effectively and
efficiently as a decision-making aid. Finally, we use RACHEL, a system that
helps infertile couples select medical treatments, to illustrate the
methodology of decision analysis as basis for expert decision systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3443</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3443</id><created>2013-03-27</created><authors><author><keyname>Zimmer</keyname><forenames>Alf C.</forenames></author></authors><title>The Estimation of Subjective Probabilities via Categorical Judgments of
  Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-217-224</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theoretically as well as experimentally it is investigated how people
represent their knowledge in order to make decisions or to share their
knowledge with others. Experiment 1 probes into the ways how people 6ather
information about the frequencies of events and how the requested response
mode, that is, numerical vs. verbal estimates interferes with this knowledge.
The least interference occurs if the subjects are allowed to give verbal
responses. From this it is concluded that processing knowledge about
uncertainty categorically, that is, by means of verbal expressions, imposes
less mental work load on the decision matter than numerical processing.
Possibility theory is used as a framework for modeling the individual usage of
verbal categories for grades of uncertainty. The 'elastic' constraints on the
verbal expressions for every sing1e subject are determined in Experiment 2 by
means of sequential calibration. In further experiments it is shown that the
superiority of the verbal processing of knowledge about uncertainty guise
generally reduces persistent biases reported in the literature: conservatism
(Experiment 3) and neg1igence of regression (Experiment 4). The reanalysis of
Hormann's data reveal that in verbal Judgments people exhibit sensitivity for
base rates and are not prone to the conjunction fallacy. In a final experiment
(5) about predictions in a real-life situation it turns out that in a numerical
forecasting task subjects restricted themselves to those parts of their
knowledge which are numerical. On the other hand subjects in a verbal
forecasting task accessed verbally as well as numerically stated knowledge.
Forecasting is structurally related to the estimation of probabilities for rare
events insofar as supporting and contradicting arguments have to be evaluated
and the choice of the final Judgment has to be Justified according to the
evidence brought forward. In order to assist people in such choice situations a
formal model for the interactive checking of arguments has been developed. The
model transforms the normal-language quantifiers used in the arguments into
fuzzy numbers and evaluates the given train of arguments by means of fuzzy
numerica1 operations. Ambiguities in the meanings of quantifiers are resolved
interactively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3444</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3444</id><created>2013-03-27</created><authors><author><keyname>Abramson</keyname><forenames>Bruce</forenames></author></authors><title>A Cure for Pathological Behavior in Games that Use Minimax</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-225-231</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The traditional approach to choosing moves in game-playing programs is the
minimax procedure. The general belief underlying its use is that increasing
search depth improves play. Recent research has shown that given certain
simplifying assumptions about a game tree's structure, this belief is
erroneous: searching deeper decreases the probability of making a correct move.
This phenomenon is called game tree pathology. Among these simplifying
assumptions is uniform depth of win/loss (terminal) nodes, a condition which is
not true for most real games. Analytic studies in [10] have shown that if every
node in a pathological game tree is made terminal with probability exceeding a
certain threshold, the resulting tree is nonpathological. This paper considers
a new evaluation function which recognizes increasing densities of forced wins
at deeper levels in the tree. This property raises two points that strengthen
the hypothesis that uniform win depth causes pathology. First, it proves
mathematically that as search deepens, an evaluation function that does not
explicitly check for certain forced win patterns becomes decreasingly likely to
force wins. This failing predicts the pathological behavior of the original
evaluation function. Second, it shows empirically that despite recognizing
fewer mid-game wins than the theoretically predicted minimum, the new function
is nonpathological.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3445</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3445</id><created>2013-03-27</created><authors><author><keyname>Nau</keyname><forenames>Dana</forenames></author><author><keyname>Purdom</keyname><forenames>Paul</forenames></author><author><keyname>Tzeng</keyname><forenames>Chun-Hung</forenames></author></authors><title>An Evaluation of Two Alternatives to Minimax</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-232-236</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of Artificial Intelligence, traditional approaches to choosing
moves in games involve the we of the minimax algorithm. However, recent
research results indicate that minimizing may not always be the best approach.
In this paper we summarize the results of some measurements on several model
games with several different evaluation functions. These measurements, which
are presented in detail in [NPT], show that there are some new algorithms that
can make significantly better use of evaluation function values than the
minimax algorithm does.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3446</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3446</id><created>2013-03-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Intelligent Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-237-244</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of practical probabilistic models on the computer demands a
convenient representation for the available knowledge and an efficient
algorithm to perform inference. An appealing representation is the influence
diagram, a network that makes explicit the random variables in a model and
their probabilistic dependencies. Recent advances have developed solution
procedures based on the influence diagram. In this paper, we examine the
fundamental properties that underlie those techniques, and the information
about the probabilistic structure that is available in the influence diagram
representation. The influence diagram is a convenient representation for
computer processing while also being clear and non-mathematical. It displays
probabilistic dependence precisely, in a way that is intuitive for decision
makers and experts to understand and communicate. As a result, the same
influence diagram can be used to build, assess and analyze a model,
facilitating changes in the formulation and feedback from sensitivity analysis.
The goal in this paper is to determine arbitrary conditional probability
distributions from a given probabilistic model. Given qualitative information
about the dependence of the random variables in the model we can, for a
specific conditional expression, specify precisely what quantitative
information we need to be able to determine the desired conditional probability
distribution. It is also shown how we can find that probability distribution by
performing operations locally, that is, over subspaces of the joint
distribution. In this way, we can exploit the conditional independence present
in the model to avoid having to construct or manipulate the full joint
distribution. These results are extended to include maximal processing when the
information available is incomplete, and optimal decision making in an
uncertain environment. Influence diagrams as a computer-aided modeling tool
were developed by Miller, Merkofer, and Howard [5] and extended by Howard and
Matheson [2]. Good descriptions of how to use them in modeling are in Owen [7]
and Howard and Matheson [2]. The notion of solving a decision problem through
influence diagrams was examined by Olmsted [6] and such an algorithm was
developed by Shachter [8]. The latter paper also shows how influence diagrams
can be used to perform a variety of sensitivity analyses. This paper extends
those results by developing a theory of the properties of the diagram that are
used by the algorithm, and the information needed to solve arbitrary
probability inference problems. Section 2 develops the notation and the
framework for the paper and the relationship between influence diagrams and
joint probability distributions. The general probabilistic inference problem is
posed in Section 3. In Section 4 the transformations on the diagram are
developed and then put together into a solution procedure in Section 5. In
Section 6, this procedure is used to calculate the information requirement to
solve an inference problem and the maximal processing that can be performed
with incomplete information. Section 7 contains a summary of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3447</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3447</id><created>2013-03-27</created><authors><author><keyname>Sher</keyname><forenames>David</forenames></author></authors><title>Developing and Analyzing Boundary Detection Operators Using
  Probabilistic Models</title><categories>cs.CV</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-245-252</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most feature detectors such as edge detectors or circle finders are
statistical, in the sense that they decide at each point in an image about the
presence of a feature, this paper describes the use of Bayesian feature
detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3448</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3448</id><created>2013-03-27</created><authors><author><keyname>Fox</keyname><forenames>John</forenames></author></authors><title>Strong &amp; Weak Methods: A Logical View of Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-253-257</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last few years has seen a growing debate about techniques for managing
uncertainty in AI systems. Unfortunately this debate has been cast as a rivalry
between AI methods and classical probability based ones. Three arguments for
extending the probability framework of uncertainty are presented, none of which
imply a challenge to classical methods. These are (1) explicit representation
of several types of uncertainty, specifically possibility and plausibility, as
well as probability, (2) the use of weak methods for uncertainty management in
problems which are poorly defined, and (3) symbolic representation of different
uncertainty calculi and methods for choosing between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3449</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3449</id><created>2013-03-27</created><authors><author><keyname>Ingber</keyname><forenames>Lester</forenames></author></authors><title>Statistical Mechanics Algorithm for Response to Targets (SMART)</title><categories>cs.CE cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-258-264</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proposed to apply modern methods of nonlinear nonequilibrium
statistical mechanics to develop software algorithms that will optimally
respond to targets within short response times with minimal computer resources.
This Statistical Mechanics Algorithm for Response to Targets (SMART) can be
developed with a view towards its future implementation into a hardwired
Statistical Algorithm Multiprocessor (SAM) to enhance the efficiency and speed
of response to targets (SMART_SAM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3450</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3450</id><created>2013-03-27</created><authors><author><keyname>Levitt</keyname><forenames>Tod S.</forenames></author></authors><title>Probabilistic Conflict Resolution in Hierarchical Hypothesis Spaces</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-265-272</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial intelligence applications such as industrial robotics, military
surveillance, and hazardous environment clean-up, require situation
understanding based on partial, uncertain, and ambiguous or erroneous evidence.
It is necessary to evaluate the relative likelihood of multiple possible
hypotheses of the (current) situation faced by the decision making program.
Often, the evidence and hypotheses are hierarchical in nature. In image
understanding tasks, for example, evidence begins with raw imagery, from which
ambiguous features are extracted which have multiple possible aggregations
providing evidential support for the presence of multiple hypothesis of objects
and terrain, which in turn aggregate in multiple ways to provide partial
evidence for different interpretations of the ambient scene. Information fusion
for military situation understanding has a similar evidence/hypothesis
hierarchy from multiple sensor through message level interpretations, and also
provides evidence at multiple levels of the doctrinal hierarchy of military
forces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3451</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3451</id><created>2013-03-27</created><authors><author><keyname>Liu</keyname><forenames>Gerald Shao-Hung</forenames></author></authors><title>Knowledge Structures and Evidential Reasoning in Decision Analysis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-273-282</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The roles played by decision factors in making complex subject are decisions
are characterized by how these factors affect the overall decision. Evidence
that partially matches a factor is evaluated, and then effective computational
rules are applied to these roles to form an appropriate aggregation of the
evidence. The use of this technique supports the expression of deeper levels of
causality, and may also preserve the cognitive structure of the decision maker
better than the usual weighting methods, certainty-factor or other
probabilistic models can.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3477</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3477</id><created>2013-04-11</created><authors><author><keyname>Kamalapurkar</keyname><forenames>Rushikesh</forenames></author><author><keyname>Walters</keyname><forenames>Patrick</forenames></author><author><keyname>Dixon</keyname><forenames>Warren</forenames></author></authors><title>Concurrent learning-based approximate optimal regulation</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In deterministic systems, reinforcement learning-based online approximate
optimal control methods typically require a restrictive persistence of
excitation (PE) condition for convergence. This paper presents a concurrent
learning-based solution to the online approximate optimal regulation problem
that eliminates the need for PE. The development is based on the observation
that given a model of the system, the Bellman error, which quantifies the
deviation of the system Hamiltonian from the optimal Hamiltonian, can be
evaluated at any point in the state space. Further, a concurrent learning-based
parameter identifier is developed to compensate for parametric uncertainty in
the plant dynamics. Uniformly ultimately bounded (UUB) convergence of the
system states to the origin, and UUB convergence of the developed policy to the
optimal policy are established using a Lyapunov-based analysis, and simulations
are performed to demonstrate the performance of the developed controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3478</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3478</id><created>2013-04-11</created><authors><author><keyname>Belabbas</keyname><forenames>M. -A .</forenames></author></authors><title>Sparse Stable Matrices</title><categories>math.OC cs.SY</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the design of decentralized networked systems, it is useful to know
whether a given network topology can sustain stable dynamics. We consider a
basic version of this problem here: given a vector space of sparse real
matrices, does it contain a stable (Hurwitz) matrix? Said differently, is a
feedback channel (corresponding to a non-zero entry) necessary for
stabilization or can it be done without. We provide in this paper a set of
necessary and a set of sufficient conditions for the existence of stable
matrices in a vector space of sparse matrices. We further prove some properties
of the set of sparse matrix spaces that contain Hurwitz matrices. The
conditions we exhibit are most easily stated in the language of graph theory,
which we thus adopt in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3479</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3479</id><created>2013-04-11</created><authors><author><keyname>Kamalapurkar</keyname><forenames>Rushikesh</forenames></author><author><keyname>Dinh</keyname><forenames>Huyen</forenames></author><author><keyname>Walters</keyname><forenames>Patrick</forenames></author><author><keyname>Dixon</keyname><forenames>Warren</forenames></author></authors><title>Approximate optimal cooperative decentralized control for consensus in a
  topological network of agents with uncertain nonlinear dynamics</title><categories>cs.SY math.OC</categories><journal-ref>Proc., American Control Conf., 2013, pp.1320,1325</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efforts in this paper seek to combine graph theory with adaptive dynamic
programming (ADP) as a reinforcement learning (RL) framework to determine
forward-in-time, real-time, approximate optimal controllers for distributed
multi-agent systems with uncertain nonlinear dynamics. A decentralized
continuous time-varying control strategy is proposed, using only local
communication feedback from two-hop neighbors on a communication topology that
has a spanning tree. An actor-critic-identifier architecture is proposed that
employs a nonlinear state derivative estimator to estimate the unknown dynamics
online and uses the estimate thus obtained for value function approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3480</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3480</id><created>2013-04-11</created><authors><author><keyname>Hodas</keyname><forenames>Nathan O.</forenames></author><author><keyname>Kooti</keyname><forenames>Farshad</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Friendship Paradox Redux: Your Friends Are More Interesting Than You</title><categories>cs.SI cs.CY nlin.AO physics.soc-ph stat.AP</categories><comments>Accepted to ICWSM 2013</comments><acm-class>J.4; G.3; G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feld's friendship paradox states that &quot;your friends have more friends than
you, on average.&quot; This paradox arises because extremely popular people, despite
being rare, are overrepresented when averaging over friends. Using a sample of
the Twitter firehose, we confirm that the friendship paradox holds for &gt;98% of
Twitter users. Because of the directed nature of the follower graph on Twitter,
we are further able to confirm more detailed forms of the friendship paradox:
everyone you follow or who follows you has more friends and followers than you.
This is likely caused by a correlation we demonstrate between Twitter activity,
number of friends, and number of followers. In addition, we discover two new
paradoxes: the virality paradox that states &quot;your friends receive more viral
content than you, on average,&quot; and the activity paradox, which states &quot;your
friends are more active than you, on average.&quot; The latter paradox is important
in regulating online communication. It may result in users having difficulty
maintaining optimal incoming information rates, because following additional
users causes the volume of incoming tweets to increase super-linearly. While
users may compensate for increased information flow by increasing their own
activity, users become information overloaded when they receive more
information than they are able or willing to process. We compare the average
size of cascades that are sent and received by overloaded and underloaded
users. And we show that overloaded users post and receive larger cascades and
they are poor detector of small cascades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3483</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3483</id><created>2013-04-11</created><updated>2014-01-22</updated><authors><author><keyname>Arnold</keyname><forenames>Andrew</forenames></author><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author><author><keyname>Roche</keyname><forenames>Daniel S.</forenames></author></authors><title>Faster sparse interpolation of straight-line programs</title><categories>cs.SC</categories><comments>15 pages, 1 table, 4 procedures, version appeared at Computer Algebra
  in Scientific Computing (CASC) 2013</comments><msc-class>68W20</msc-class><acm-class>F.2.1; G.1.1; G.4; I.1.2</acm-class><journal-ref>Proc. CASC 2013, Lecture Notes in Computer Science, Volume 8136,
  2013, pp 61-74</journal-ref><doi>10.1007/978-3-319-02297-0_5</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We give a new probabilistic algorithm for interpolating a &quot;sparse&quot; polynomial
f given by a straight-line program. Our algorithm constructs an approximation
f* of f, such that their difference probably has at most half the number of
terms of f, then recurses on their difference. Our approach builds on previous
work by Garg and Schost (2009), and Giesbrecht and Roche (2011), and is
asymptotically more efficient in terms of the total cost of the probes required
than previous methods, in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3487</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3487</id><created>2013-04-11</created><updated>2014-05-29</updated><authors><author><keyname>Costa</keyname><forenames>Alfredo</forenames></author><author><keyname>Steinberg</keyname><forenames>Benjamin</forenames></author></authors><title>A categorical invariant of flow equivalence of shifts</title><categories>math.DS cs.FL math.CT math.GR</categories><journal-ref>Ergod. Th. Dynam. Sys. 36 (2014) 470-513</journal-ref><doi>10.1017/etds.2014.74</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the Karoubi envelope of a shift --- defined as the Karoubi
envelope of the syntactic semigroup of the language of blocks of the shift ---
is, up to natural equivalence of categories, an invariant of flow equivalence.
More precisely, we show that the action of the Karoubi envelope on the Krieger
cover of the shift is a flow invariant. An analogous result concerning the
Fischer cover of a synchronizing shift is also obtained. From these main
results, several flow equivalence invariants --- some new and some old --- are
obtained. We also show that the Karoubi envelope is, in a natural sense, the
best possible syntactic invariant of flow equivalence of sofic shifts. Another
application concerns the classification of Markov-Dyck and Markov-Motzkin
shifts: it is shown that, under mild conditions, two graphs define flow
equivalent shifts if and only if they are isomorphic. Shifts with property (A)
and their associated semigroups, introduced by Wolfgang Krieger, are
interpreted in terms of the Karoubi envelope, yielding a proof of the flow
invariance of the associated semigroups in the cases usually considered (a
result recently announced by Krieger), and also a proof that property (A) is
decidable for sofic shifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3489</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3489</id><created>2013-04-06</created><authors><author><keyname>Saad</keyname><forenames>Emad</forenames></author></authors><title>Logical Stochastic Optimization</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.2384,
  arXiv:1304.2797, arXiv:1304.1684, arXiv:1304.3144</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a logical framework to represent and reason about stochastic
optimization problems based on probability answer set programming. This is
established by allowing probability optimization aggregates, e.g., minimum and
maximum in the language of probability answer set programming to allow
minimization or maximization of some desired criteria under the probabilistic
environments. We show the application of the proposed logical stochastic
optimization framework under the probability answer set programming to two
stages stochastic optimization problems with recourse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3507</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3507</id><created>2013-04-11</created><authors><author><keyname>Farah</keyname><forenames>Salim</forenames></author><author><keyname>Bayoumi</keyname><forenames>Magdy</forenames></author></authors><title>Hardware Acceleration of the Gipps Model for Real-Time Traffic
  Simulation</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic simulation software is becoming increasingly popular as more cities
worldwide use it to better manage their crowded traffic networks. An important
requirement for such software is the ability to produce accurate results in
real time, requiring great computation resources. This work proposes an
ASIC-based hardware accelerated approach for the AIMSUN traffic simulator,
taking advantage of repetitive tasks in the algorithm. Different system
configurations using this accelerator are also discussed. Compared with the
traditional software simulator, it has been found to improve the performance by
as much as 9x when using a single processing element approach, or more
depending on the chosen hardware configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3513</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3513</id><created>2013-04-11</created><authors><author><keyname>Carbunar</keyname><forenames>Bogdan</forenames></author><author><keyname>Rahman</keyname><forenames>Mahmudur</forenames></author><author><keyname>Ballesteros</keyname><forenames>Jaime</forenames></author><author><keyname>Rishe</keyname><forenames>Naphtali</forenames></author></authors><title>Eat the Cake and Have It Too: Privacy Preserving Location Aggregates in
  Geosocial Networks</title><categories>cs.CR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geosocial networks are online social networks centered on the locations of
subscribers and businesses. Providing input to targeted advertising, profiling
social network users becomes an important source of revenue. Its natural
reliance on personal information introduces a trade-off between user privacy
and incentives of participation for businesses and geosocial network providers.
In this paper we introduce location centric profiles (LCPs), aggregates built
over the profiles of users present at a given location. We introduce PROFILR, a
suite of mechanisms that construct LCPs in a private and correct manner. We
introduce iSafe, a novel, context aware public safety application built on
PROFILR . Our Android and browser plugin implementations show that PROFILR is
efficient: the end-to-end overhead is small even under strong correctness
assurances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3518</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3518</id><created>2013-04-11</created><authors><author><keyname>Martins</keyname><forenames>Andr&#xe9; C. R.</forenames></author></authors><title>Trust in the CODA model: Opinion Dynamics and the reliability of other
  agents</title><categories>physics.soc-ph cs.MA cs.SI</categories><comments>15 pages, 14 figures</comments><doi>10.1016/j.physleta.2013.07.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model for the joint evolution of opinions and how much the agents trust
each other is presented. The model is built using the framework of the
Continuous Opinions and Discrete Actions (CODA) model. Instead of a fixed
probability that the other agents will decide in the favor of the best choice,
each agent considers that other agents might be one one of two types:
trustworthy or useless. Trustworthy agents are considered more likely to be
right than wrong, while the opposite holds for useless ones. Together with the
opinion about the discussed issue, each agent also updates that probability for
each one of the other agents it interacts withe probability each one it
interacts with is of one type or the other. The dynamics of opinions and the
evolution of the trust between the agents are studied. Clear evidences of the
existence of two phases, one where strong polarization is observed and the
other where a clear division is permanent and reinforced are observed. The
transition seems signs of being a first-order transition, with a location
dependent on both the parameters of the model and the initial conditions. This
happens despite the fact that the trust network evolves much slower than the
opinion on the central issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3519</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3519</id><created>2013-04-11</created><updated>2013-07-21</updated><authors><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Zhang</keyname><forenames>Fa</forenames></author><author><keyname>Aroca</keyname><forenames>Jordi Arjona</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author><author><keyname>Zheng</keyname><forenames>Kai</forenames></author><author><keyname>Hou</keyname><forenames>Chenying</forenames></author><author><keyname>Li</keyname><forenames>Dan</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyong</forenames></author></authors><title>GreenDCN: a General Framework for Achieving Energy Efficiency in Data
  Center Networks</title><categories>cs.NI</categories><comments>14 pages, accepted by IEEE JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popularization of cloud computing has raised concerns over the energy
consumption that takes place in data centers. In addition to the energy
consumed by servers, the energy consumed by large numbers of network devices
emerges as a significant problem. Existing work on energy-efficient data center
networking primarily focuses on traffic engineering, which is usually adapted
from traditional networks. We propose a new framework to embrace the new
opportunities brought by combining some special features of data centers with
traffic engineering. Based on this framework, we characterize the problem of
achieving energy efficiency with a time-aware model, and we prove its
NP-hardness with a solution that has two steps. First, we solve the problem of
assigning virtual machines (VM) to servers to reduce the amount of traffic and
to generate favorable conditions for traffic engineering. The solution reached
for this problem is based on three essential principles that we propose.
Second, we reduce the number of active switches and balance traffic flows,
depending on the relation between power consumption and routing, to achieve
energy conservation. Experimental results confirm that, by using this
framework, we can achieve up to 50 percent energy savings. We also provide a
comprehensive discussion on the scalability and practicability of the
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3521</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3521</id><created>2013-04-11</created><updated>2013-12-29</updated><authors><author><keyname>Bucksch</keyname><forenames>Alexander</forenames></author><author><keyname>Turk</keyname><forenames>Greg</forenames></author><author><keyname>Weitz</keyname><forenames>Joshua S.</forenames></author></authors><title>The Fiber Walk: A Model of Tip-Driven Growth with Lateral Expansion</title><categories>physics.bio-ph cs.CG</categories><comments>Plos One (in press)</comments><doi>10.1371/journal.pone.0085585</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tip-driven growth processes underlie the development of many plants. To date,
tip-driven growth processes have been modelled as an elongating path or series
of segments without taking into account lateral expansion during elongation.
Instead, models of growth often introduce an explicit thickness by expanding
the area around the completed elongated path. Modelling expansion in this way
can lead to contradictions in the physical plausibility of the resulting
surface and to uncertainty about how the object reached certain regions of
space. Here, we introduce &quot;fiber walks&quot; as a self-avoiding random walk model
for tip-driven growth processes that includes lateral expansion. In 2D, the
fiber walk takes place on a square lattice and the space occupied by the fiber
is modelled as a lateral contraction of the lattice. This contraction
influences the possible follow-up steps of the fiber walk. The boundary of the
area consumed by the contraction is derived as the dual of the lattice faces
adjacent to the fiber. We show that fiber walks generate fibers that have
well-defined curvatures, enable the identification of the process underlying
the occupancy of physical space. Hence, fiber walks provide a base from which
to model both the extension and expansion of physical biological objects with
finite thickness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3531</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3531</id><created>2013-04-12</created><authors><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Zhang</keyname><forenames>Anru</forenames></author></authors><title>Compressed Sensing and Affine Rank Minimization under Restricted
  Isometry</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>to appear in IEEE Transactions on Signal Processing</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper establishes new restricted isometry conditions for compressed
sensing and affine rank minimization. It is shown for compressed sensing that
$\delta_{k}^A+\theta_{k,k}^A &lt; 1$ guarantees the exact recovery of all $k$
sparse signals in the noiseless case through the constrained $\ell_1$
minimization. Furthermore, the upper bound 1 is sharp in the sense that for any
$\epsilon &gt; 0$, the condition $\delta_k^A + \theta_{k, k}^A &lt; 1+\epsilon$ is
not sufficient to guarantee such exact recovery using any recovery method.
Similarly, for affine rank minimization, if
$\delta_{r}^\mathcal{M}+\theta_{r,r}^\mathcal{M}&lt; 1$ then all matrices with
rank at most $r$ can be reconstructed exactly in the noiseless case via the
constrained nuclear norm minimization; and for any $\epsilon &gt; 0$,
$\delta_r^\mathcal{M} +\theta_{r,r}^\mathcal{M} &lt; 1+\epsilon$ does not ensure
such exact recovery using any method. Moreover, in the noisy case the
conditions $\delta_{k}^A+\theta_{k,k}^A &lt; 1$ and
$\delta_{r}^\mathcal{M}+\theta_{r,r}^\mathcal{M}&lt; 1$ are also sufficient for
the stable recovery of sparse signals and low-rank matrices respectively.
Applications and extensions are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3535</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3535</id><created>2013-04-12</created><authors><author><keyname>Hassan</keyname><forenames>Mostaque Md. Morshedur</forenames></author></authors><title>Current Studies On Intrusion Detection System, Genetic Algorithm And
  Fuzzy Logic</title><categories>cs.CR</categories><comments>Total number of pages: 13 Total figures used: 2 Total tables used: 1
  Total keywords: 4. arXiv admin note: text overlap with arXiv:1204.1336 by
  other authors</comments><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.4, No.2, March 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays Intrusion Detection System (IDS) which is increasingly a key element
of system security is used to identify the malicious activities in a computer
system or network. There are different approaches being employed in intrusion
detection systems, but unluckily each of the technique so far is not entirely
ideal. The prediction process may produce false alarms in many anomaly based
intrusion detection systems. With the concept of fuzzy logic, the false alarm
rate in establishing intrusive activities can be reduced. A set of efficient
fuzzy rules can be used to define the normal and abnormal behaviors in a
computer network. Therefore some strategy is needed for best promising security
to monitor the anomalous behavior in computer network. In this paper I present
a few research papers regarding the foundations of intrusion detection systems,
the methodologies and good fuzzy classifiers using genetic algorithm which are
the focus of current development efforts and the solution of the problem of
Intrusion Detection System to offer a realworld view of intrusion detection.
Ultimately, a discussion of the upcoming technologies and various methodologies
which promise to improve the capability of computer systems to detect
intrusions is offered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3541</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3541</id><created>2013-04-12</created><authors><author><keyname>Maazallahi</keyname><forenames>Ramin</forenames></author><author><keyname>Niknafs</keyname><forenames>Aliakbar</forenames></author></authors><title>A modified dna computing approach to tackle the exponential solution
  space of the graph coloring problem</title><categories>cs.CC cs.DS</categories><comments>7 pages, 3 figures, 1 table, International Journal in Foundations of
  Computer Science &amp; Technology (IJFCST), Vol. 3, No.2, March 2013</comments><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol. 3, No.2, March 2013</journal-ref><doi>10.5121/ijfcst.2013.3201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although it has been evidenced that DNA computing is able to solve the graph
coloring problem in a polynomial time complexity, but the exponential solution
space is still a restrictive factor in applying this technique for solving
really large problems. In this paper a modified DNA computing approach based on
Adleman-Lipton model is proposed which tackles the mentioned restriction by
coloring the vertices one by one. In each step, it expands the DNA strands
encoding promising solutions and discards those which encode infeasible ones. A
sample graph is colored by simulating the proposed approach and shows a notable
reduction in the number of DNA strands used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3548</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3548</id><created>2013-04-12</created><updated>2014-02-22</updated><authors><author><keyname>Naroditskiy</keyname><forenames>Victor</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author></authors><title>Crowdsourcing Dilemma</title><categories>cs.SI cs.GT physics.soc-ph</categories><comments>Press embargo in place until publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing offers unprecedented potential for solving tasks efficiently by
tapping into the skills of large groups of people. A salient feature of
crowdsourcing---its openness of entry---makes it vulnerable to malicious
behavior. Such behavior took place in a number of recent popular crowdsourcing
competitions. We provide game-theoretic analysis of a fundamental tradeoff
between the potential for increased productivity and the possibility of being
set back by malicious behavior. Our results show that in crowdsourcing
competitions malicious behavior is the norm, not the anomaly---a result
contrary to the conventional wisdom in the area. Counterintuitively, making the
attacks more costly does not deter them but leads to a less desirable outcome.
These findings have cautionary implications for the design of crowdsourcing
competitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3550</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3550</id><created>2013-04-12</created><authors><author><keyname>Dua</keyname><forenames>Gagan</forenames></author><author><keyname>Gautam</keyname><forenames>Nitin</forenames></author><author><keyname>Sharma</keyname><forenames>Dharmendar</forenames></author><author><keyname>Arora</keyname><forenames>Ankit</forenames></author></authors><title>Replay Attack Prevention in Kerberos Authentication Protocol Using
  Triple Password</title><categories>cs.CR cs.DC</categories><comments>12 pages, 2 Figures, 2 Tables</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.5, No.2, March 2013</journal-ref><doi>10.5121/ijcnc.2013.5205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replay attack and password attacks are serious issues in the Kerberos
authentication protocol. Many ideas have been proposed to prevent these attacks
but they increase complexity of the total Kerberos environment. In this paper
we present an improved method which prevents replay attacks and password
attacks by using Triple password scheme. Three passwords are stored on
Authentication Server and Authentication Server sends two passwords to Ticket
Granting Server (one for Application Server) by encrypting with the secret key
shared between Authentication server and Ticket Granting server.
Similarly,Ticket Granting Server sends one password to Application Server by
encrypting with the secret key shared between TGS and application server.
Meanwhile, Service-Granting-Ticket is transferred to users by encrypting it
with the password that TGS just received from AS. It helps to prevent Replay
attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3553</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3553</id><created>2013-04-12</created><updated>2014-12-22</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>On the Reliability Function of the Discrete Memoryless Relay Channel</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Information Theory; Presented
  in part at the 2013 ISIT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounds on the reliability function for the discrete memoryless relay channel
are derived using the method of types. Two achievable error exponents are
derived based on partial decode-forward and compress-forward which are
well-known superposition block-Markov coding schemes. The derivations require
combinations of the techniques involved in the proofs of
Csisz\'ar-K\&quot;orner-Marton's packing lemma for the error exponent of channel
coding and Marton's type covering lemma for the error exponent of source coding
with a fidelity criterion. The decode-forward error exponent is evaluated on
Sato's relay channel. From this example, it is noted that to obtain the fastest
possible decay in the error probability for a fixed effective coding rate, one
ought to optimize the number of blocks in the block-Markov coding scheme
assuming the blocklength within each block is large. An upper bound on the
reliability function is also derived using ideas from Haroutunian's lower bound
on the error probability for point-to-point channel coding with feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3554</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3554</id><created>2013-04-12</created><authors><author><keyname>Murthy</keyname><forenames>Dr. G. Rama</forenames></author><author><keyname>Srikanth</keyname><forenames>M.</forenames></author><author><keyname>Viswanadh</keyname><forenames>K.</forenames></author></authors><title>Global cognitive radio based communication systems: Space-time
  communications</title><categories>cs.ET cs.NI</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum Scarcity is a global problem across the world. This paper emphasizes
on the fact that a global problem has to be dealt on global basis, not just
locally by applying the principle of global cognitive radio,Global
Opportunistic Remote Spectrum Access. The Future Internet and Internet of
Things literally scare the communication system designer regarding the
available bandwidth and spectrum resources. There is absolutely no scope to
waste or under utilize the available resources. Hence the proposed idea of
Global Cognitive Radio Concept can undoubtedly solve the resource problems in
next Generation Communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3557</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3557</id><created>2013-04-12</created><authors><author><keyname>Ameen</keyname><forenames>Radhwan Y</forenames></author><author><keyname>Hamo</keyname><forenames>Asmaa Y.</forenames></author></authors><title>Survey of Server Virtualization</title><categories>cs.OS</categories><comments>10 pages 14 figures. arXiv admin note: text overlap with
  arXiv:1010.3233 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization is a term that refers to the abstraction of computer
resources. The purpose of virtual computing environment is to improve resource
utilization by providing a unified integrated operating platform for users and
applications based on aggregation of heterogeneous and autonomous resources.
More recently, virtualization at all levels (system, storage, and network)
became important again as a way to improve system security, reliability and
availability, reduce costs, and provide greater flexibility. Virtualization has
rapidly become a go-to technology for increasing efficiency in the data center.
With virtualization technologies providing tremendous flexibility, even
disparate architectures may be deployed on a single machine without
interference This paper explains the basics of server virtualization and
addresses pros and cons of virtualization
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3560</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3560</id><created>2013-04-12</created><authors><author><keyname>Vasanthi</keyname><forenames>V.</forenames></author><author><keyname>Hemalatha</keyname><forenames>M.</forenames></author></authors><title>Simulation of Obstruction Avoidance Generously Mobility (OAGM) Model
  using Graph-theory Technique</title><categories>cs.NI</categories><comments>9 pages</comments><journal-ref>Research Journal of Applied Sciences, Engineering and Technology
  5(7): 2799-2808, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Obstruction Avoidance Generously Mobility (OAGM) model has been introduced
for controlling ad-hoc sensor networks and thereby operating emerging fields
like military and healthcare services. According to this model, the ability to
send a message to a group of users simultaneously, based solely on their
geographic location, is desirable by using Mission Critical Mobility model that
assumes the obstacle shapes like rectangle or square in the simulation terrain.
The OAGM model is developed by grasping the critical situations of military and
healthcare services by incorporating the node movement model, hierarchical node
organization, placement of obstacle that affect the movement of nodes and also
signal propagation. Graph theory technique is used to find the shortest path of
the node movement process. The varying number of parameter sets with DSR
protocol is analyzed for MCM and OAGM mobility model. The results show OAGM
performance is better than MCM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3563</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3563</id><created>2013-04-12</created><authors><author><keyname>Al-Azmi</keyname><forenames>Abdul-Aziz Rashid</forenames></author></authors><title>Data, text and web mining for business intelligence: a survey</title><categories>cs.IR</categories><comments>21 page, journal paper</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.2, March 2013</journal-ref><doi>10.5121/ijdkp.2013.3201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Information and Communication Technologies revolution brought a digital
world with huge amounts of data available. Enterprises use mining technologies
to search vast amounts of data for vital insight and knowledge. Mining tools
such as data mining, text mining, and web mining are used to find hidden
knowledge in large databases or the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3568</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3568</id><created>2013-04-12</created><authors><author><keyname>Chainais</keyname><forenames>Pierre</forenames></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>Distributed dictionary learning over a sensor network</title><categories>stat.ML cs.LG stat.AP</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed dictionary learning, where a set of
nodes is required to collectively learn a common dictionary from noisy
measurements. This approach may be useful in several contexts including sensor
networks. Diffusion cooperation schemes have been proposed to solve the
distributed linear regression problem. In this work we focus on a
diffusion-based adaptive dictionary learning strategy: each node records
observations and cooperates with its neighbors by sharing its local dictionary.
The resulting algorithm corresponds to a distributed block coordinate descent
(alternate optimization). Beyond dictionary learning, this strategy could be
adapted to many matrix factorization problems and generalized to various
settings. This article presents our approach and illustrates its efficiency on
some numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3573</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3573</id><created>2013-04-12</created><authors><author><keyname>Beckouche</keyname><forenames>Simon</forenames></author><author><keyname>Starck</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Fadili</keyname><forenames>Jalal</forenames></author></authors><title>Astronomical Image Denoising Using Dictionary Learning</title><categories>astro-ph.IM cs.CV</categories><doi>10.1051/0004-6361/201220752</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomical images suffer a constant presence of multiple defects that are
consequences of the intrinsic properties of the acquisition equipments, and
atmospheric conditions. One of the most frequent defects in astronomical
imaging is the presence of additive noise which makes a denoising step
mandatory before processing data. During the last decade, a particular modeling
scheme, based on sparse representations, has drawn the attention of an ever
growing community of researchers. Sparse representations offer a promising
framework to many image and signal processing tasks, especially denoising and
restoration applications. At first, the harmonics, wavelets, and similar bases
and overcomplete representations have been considered as candidate domains to
seek the sparsest representation. A new generation of algorithms, based on
data-driven dictionaries, evolved rapidly and compete now with the
off-the-shelf fixed dictionaries. While designing a dictionary beforehand leans
on a guess of the most appropriate representative elementary forms and
functions, the dictionary learning framework offers to construct the dictionary
upon the data themselves, which provides us with a more flexible setup to
sparse modeling and allows to build more sophisticated dictionaries. In this
paper, we introduce the Centered Dictionary Learning (CDL) method and we study
its performances for astronomical image denoising. We show how CDL outperforms
wavelet or classic dictionary learning denoising techniques on astronomical
images, and we give a comparison of the effect of these different algorithms on
the photometry of the denoised images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3596</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3596</id><created>2013-04-12</created><authors><author><keyname>Blazy</keyname><forenames>Sandrine</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Laporte</keyname><forenames>Vincent</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Maroneze</keyname><forenames>Andr&#xe9;</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Pichardie</keyname><forenames>David</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Formal Verification of a C Value Analysis Based on Abstract
  Interpretation</title><categories>cs.PL</categories><proxy>ccsd</proxy><journal-ref>SAS - 20th Static Analysis Symposium Lecture Notes in Computer
  Science (2013) 324-344</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static analyzers based on abstract interpretation are complex pieces of
software implementing delicate algorithms. Even if static analysis techniques
are well understood, their implementation on real languages is still
error-prone. This paper presents a formal verification using the Coq proof
assistant: a formalization of a value analysis (based on abstract
interpretation), and a soundness proof of the value analysis. The formalization
relies on generic interfaces. The mechanized proof is facilitated by a
translation validation of a Bourdoncle fixpoint iterator. The work has been
integrated into the CompCert verified C-compiler. Our verified analysis
directly operates over an intermediate language of the compiler having the same
expressiveness as C. The automatic extraction of our value analysis into OCaml
yields a program with competitive results, obtained from experiments on a
number of benchmarks and comparisons with the Frama-C tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3600</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3600</id><created>2013-04-12</created><authors><author><keyname>Banerji</keyname><forenames>Christopher R. S.</forenames></author><author><keyname>Mansour</keyname><forenames>Toufik</forenames></author><author><keyname>Severini</keyname><forenames>Simone</forenames></author></authors><title>A notion of graph likelihood and an infinite monkey theorem</title><categories>cs.DM math.CO physics.soc-ph</categories><comments>6 pages, 1 EPS figure</comments><doi>10.6084/m9.figshare.679855</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We play with a graph-theoretic analogue of the folklore infinite monkey
theorem. We define a notion of graph likelihood as the probability that a given
graph is constructed by a monkey in a number of time steps equal to the number
of vertices. We present an algorithm to compute this graph invariant and closed
formulas for some infinite classes. We have to leave the computational
complexity of the likelihood as an open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3602</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3602</id><created>2013-04-12</created><updated>2014-11-26</updated><authors><author><keyname>Mercure</keyname><forenames>J. -F.</forenames></author></authors><title>An age structured demographic theory of technological change</title><categories>physics.soc-ph cs.SI math.DS q-fin.GN</categories><comments>24 pages, 5 figures, 2014</comments><journal-ref>Journal of Evolutionary Economics (2015) 25:787-820</journal-ref><doi>10.1007/s00191-015-0413-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the heart of technology transitions lie complex processes of social and
industrial dynamics. The quantitative study of sustainability transitions
requires modelling work, which necessitates a theory of technology
substitution. Many, if not most, contemporary modelling approaches for future
technology pathways overlook most aspects of transitions theory, for instance
dimensions of heterogenous investor choices, dynamic rates of diffusion and the
profile of transitions. A significant body of literature however exists that
demonstrates how transitions follow S-shaped diffusion curves or Lotka-Volterra
systems of equations. This framework is used ex-post since timescales can only
be reliably obtained in cases where the transitions have already occurred,
precluding its use for studying cases of interest where nascent innovations in
protective niches await favourable conditions for their diffusion. In
principle, scaling parameters of transitions can, however, be derived from
knowledge of industrial dynamics, technology turnover rates and technology
characteristics. In this context, this paper presents a theory framework for
evaluating the parameterisation of S-shaped diffusion curves for use in
simulation models of technology transitions without the involvement of
historical data fitting, making use of standard demography theory applied to
technology at the unit level. The classic Lotka-Volterra competition system
emerges from first principles from demography theory, its timescales explained
in terms of technology lifetimes and industrial dynamics. The theory is placed
in the context of the multi-level perspective on technology transitions, where
innovation and the diffusion of new socio-technical regimes take a prominent
place, as well as discrete choice theory, the primary theoretical framework for
introducing agent diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3603</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3603</id><created>2013-04-12</created><authors><author><keyname>Jahirabadkar</keyname><forenames>Sunita</forenames></author><author><keyname>Kulkarni</keyname><forenames>Parag</forenames></author></authors><title>SCAF An effective approach to Classify Subspace Clustering algorithms</title><categories>cs.DB</categories><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.2, March 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering discovers the clusters embedded in multiple, overlapping
subspaces of high dimensional data. Many significant subspace clustering
algorithms exist, each having different characteristics caused by the use of
different techniques, assumptions, heuristics used etc. A comprehensive
classification scheme is essential which will consider all such characteristics
to divide subspace clustering approaches in various families. The algorithms
belonging to same family will satisfy common characteristics. Such a
categorization will help future developers to better understand the quality
criteria to be used and similar algorithms to be used to compare results with
their proposed clustering algorithms. In this paper, we first proposed the
concept of SCAF (Subspace Clustering Algorithms Family). Characteristics of
SCAF will be based on the classes such as cluster orientation, overlap of
dimensions etc. As an illustration, we further provided a comprehensive,
systematic description and comparison of few significant algorithms belonging
to 'Axis parallel, overlapping, density based' SCAF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3604</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3604</id><created>2013-04-12</created><updated>2014-04-25</updated><authors><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author></authors><title>On Model-Based RIP-1 Matrices</title><categories>cs.DS cs.IT math.IT math.NA</categories><comments>Version 3 corrects a few errors present in the earlier version. In
  particular, it states and proves correct upper and lower bounds for the
  number of rows in RIP-1 matrices for the block-sparse model. The bounds are
  of the form k log_b n, not k log_k n as stated in the earlier version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Restricted Isometry Property (RIP) is a fundamental property of a matrix
enabling sparse recovery. Informally, an m x n matrix satisfies RIP of order k
in the l_p norm if ||Ax||_p \approx ||x||_p for any vector x that is k-sparse,
i.e., that has at most k non-zeros. The minimal number of rows m necessary for
the property to hold has been extensively investigated, and tight bounds are
known. Motivated by signal processing models, a recent work of Baraniuk et al
has generalized this notion to the case where the support of x must belong to a
given model, i.e., a given family of supports. This more general notion is much
less understood, especially for norms other than l_2. In this paper we present
tight bounds for the model-based RIP property in the l_1 norm. Our bounds hold
for the two most frequently investigated models: tree-sparsity and
block-sparsity. We also show implications of our results to sparse recovery
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3610</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3610</id><created>2013-04-12</created><authors><author><keyname>Parekh</keyname><forenames>Hardik M.</forenames></author><author><keyname>Dabhi</keyname><forenames>Vipul K.</forenames></author></authors><title>Modified Soft Brood Crossover in Genetic Programming</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Premature convergence is one of the important issues while using Genetic
Programming for data modeling. It can be avoided by improving population
diversity. Intelligent genetic operators can help to improve the population
diversity. Crossover is an important operator in Genetic Programming. So, we
have analyzed number of intelligent crossover operators and proposed an
algorithm with the modification of soft brood crossover operator. It will help
to improve the population diversity and reduce the premature convergence. We
have performed experiments on three different symbolic regression problems.
Then we made the performance comparison of our proposed crossover (Modified
Soft Brood Crossover) with the existing soft brood crossover and subtree
crossover operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3612</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3612</id><created>2013-04-12</created><authors><author><keyname>Ravibabu</keyname><forenames>V.</forenames></author></authors><title>A Novel Metaheuristics To Solve Mixed Shop Scheduling Problems</title><categories>cs.NE</categories><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology, March2013, Volume 3, Number 2, ISSN : 1839-7662</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper represents the metaheuristics proposed for solving a class of Shop
Scheduling problem. The Bacterial Foraging Optimization algorithm is featured
with Ant Colony Optimization algorithm and proposed as a natural inspired
computing approach to solve the Mixed Shop Scheduling problem. The Mixed Shop
is the combination of Job Shop, Flow Shop and Open Shop scheduling problems.
The sample instances for all mentioned Shop problems are used as test data and
Mixed Shop survive its computational complexity to minimize the makespan. The
computational results show that the proposed algorithm is gentler to solve and
performs better than the existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3623</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3623</id><created>2013-04-12</created><updated>2013-10-02</updated><authors><author><keyname>Tomasello</keyname><forenames>Mario Vincenzo</forenames></author><author><keyname>Napoletano</keyname><forenames>Mauro</forenames></author><author><keyname>Garas</keyname><forenames>Antonios</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>The Rise and Fall of R&amp;D Networks</title><categories>physics.soc-ph cs.SI</categories><comments>34 pages, 5 figures, 10 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawing on a large database of publicly announced R&amp;D alliances, we track the
evolution of R&amp;D networks in a large number of economic sectors over a long
time period (1986-2009). Our main goal is to evaluate temporal and sectoral
robustness of the main statistical properties of empirical R&amp;D networks. We
study a large set of indicators, thus providing a more complete description of
R&amp;D networks with respect to the existing literature. We find that most network
properties are invariant across sectors. In addition, they do not change when
alliances are considered independently of the sectors to which partners belong.
Moreover, we find that many properties of R&amp;D networks are characterized by a
rise-and-fall dynamics with a peak in the mid-nineties. Finally, we show that
the properties of empirical R&amp;D networks support predictions of the recent
theoretical literature on R&amp;D network formation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3629</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3629</id><created>2013-04-11</created><authors><author><keyname>S</keyname><forenames>Hemalatha</forenames></author><author><keyname>Acharya</keyname><forenames>U Dinesh</forenames></author><author><keyname>A</keyname><forenames>Renuka</forenames></author><author><keyname>Kamath</keyname><forenames>Priya R.</forenames></author></authors><title>A Secure And High Capacity Image Steganography Technique</title><categories>cs.MM</categories><comments>7 pages, Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.4, No.1, February 2013. arXiv admin note: substantial text overlap with
  arXiv:1304.3313</comments><doi>10.5121/sipij.2013.4108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is the science of invisible communication. The purpose of
Steganography is to maintain secret communication between two parties. The
secret information can be concealed in content such as image, audio, or video.
This paper provides a novel image steganography technique to hide multiple
secret images and keys in color cover image using Integer Wavelet Transform
(IWT). There is no visual difference between the stego image and the cover
image. The extracted secret images are also similar to the original secret
images. Very good PSNR (Peak Signal to Noise Ratio) values are obtained for
both stego and extracted secret images. The results are compared with the
results of other techniques, where single image is hidden and it is found that
the proposed technique is simple and gives better PSNR values than others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3640</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3640</id><created>2013-04-12</created><updated>2013-06-29</updated><authors><author><keyname>Lyu</keyname><forenames>Jiangbin</forenames></author><author><keyname>Chew</keyname><forenames>Yong Huat</forenames></author><author><keyname>Wong</keyname><forenames>Wai-Choong</forenames></author></authors><title>Aloha Games with Spatial Reuse</title><categories>cs.GT cs.IT math.IT</categories><comments>26 pages, 9 figures, 1 table, accepted by IEEE Transactions on
  Wireless Communications, June 15, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aloha games study the transmission probabilities of a group of
non-cooperative users which share a channel to transmit via the slotted Aloha
protocol. This paper extends the Aloha games to spatial reuse scenarios, and
studies the system equilibrium and performance. Specifically, fixed point
theory and order theory are used to prove the existence of a least fixed point
as the unique Nash equilibrium (NE) of the game and the optimal choice of all
players. The Krasovskii's method is used to construct a Lyapunov function and
obtain the conditions to examine the stability of the NE. Simulations show that
the theories derived are applicable to large-scale distributed systems of
complicated network topologies. An empirical relationship between the network
connectivity and the achievable total throughput is finally obtained through
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3646</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3646</id><created>2013-04-12</created><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Coon</keyname><forenames>Justin</forenames></author></authors><title>Network connectivity through small openings</title><categories>cond-mat.dis-nn cs.IT math.IT</categories><comments>6 pages, 4 figures</comments><journal-ref>ISWCS 2013, 602-606</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network connectivity is usually addressed for convex domains where a direct
line of sight exists between any two transmitting/receiving nodes. Here, we
develop a general theory for the network connectivity properties across a small
opening, rendering the domain essentially non-convex. Our analytic approach can
go only so far as we encounter what is referred to in statistical physics as
quenched disorder making the problem non-trivial. We confirm our theory through
computer simulations, obtain leading order approximations and discuss possible
extensions and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3653</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3653</id><created>2013-04-12</created><authors><author><keyname>Kanj</keyname><forenames>Iyad</forenames></author><author><keyname>Lin</keyname><forenames>Guohui</forenames></author><author><keyname>Liu</keyname><forenames>Tian</forenames></author><author><keyname>Tong</keyname><forenames>Weitian</forenames></author><author><keyname>Xia</keyname><forenames>Ge</forenames></author><author><keyname>Xu</keyname><forenames>Jinhui</forenames></author><author><keyname>Yang</keyname><forenames>Boting</forenames></author><author><keyname>Zhang</keyname><forenames>Fenghui</forenames></author><author><keyname>Zhang</keyname><forenames>Peng</forenames></author><author><keyname>Zhu</keyname><forenames>Binhai</forenames></author></authors><title>Algorithms for Cut Problems on Trees</title><categories>cs.DS</categories><msc-class>68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the {\sc multicut on trees} and the {\sc generalized multiway Cut on
trees} problems. For the {\sc multicut on trees} problem, we present a
parameterized algorithm that runs in time $O^{*}(\rho^k)$, where $\rho =
\sqrt{\sqrt{2} + 1} \approx 1.555$ is the positive root of the polynomial
$x^4-2x^2-1$. This improves the current-best algorithm of Chen et al. that runs
in time $O^{*}(1.619^k)$. For the {\sc generalized multiway cut on trees}
problem, we show that this problem is solvable in polynomial time if the number
of terminal sets is fixed; this answers an open question posed in a recent
paper by Liu and Zhang. By reducing the {\sc generalized multiway cut on trees}
problem to the {\sc multicut on trees} problem, our results give a
parameterized algorithm that solves the {\sc generalized multiway cut on trees}
problem in time $O^{*}(\rho^k)$, where $\rho = \sqrt{\sqrt{2} + 1} \approx
1.555$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3658</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3658</id><created>2013-04-12</created><authors><author><keyname>Sutter</keyname><forenames>David</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author></authors><title>Efficient One-Way Secret-Key Agreement and Private Channel Coding via
  Polarization</title><categories>cs.IT cs.CR math.IT</categories><comments>18.1 pages, 2 figures, 2 tables</comments><journal-ref>ASIACRYPT 2013, p. 194-213</journal-ref><doi>10.1007/978-3-642-42033-7_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce explicit schemes based on the polarization phenomenon for the
tasks of one-way secret key agreement from common randomness and private
channel coding. For the former task, we show how to use common randomness and
insecure one-way communication to obtain a strongly secure key such that the
key construction has a complexity essentially linear in the blocklength and the
rate at which the key is produced is optimal, i.e., equal to the one-way
secret-key rate. For the latter task, we present a private channel coding
scheme that achieves the secrecy capacity using the condition of strong secrecy
and whose encoding and decoding complexity are again essentially linear in the
blocklength.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3663</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3663</id><created>2013-04-12</created><updated>2013-11-21</updated><authors><author><keyname>Nilsson</keyname><forenames>John-Olof</forenames></author><author><keyname>Zachariah</keyname><forenames>Dave</forenames></author><author><keyname>Skog</keyname><forenames>Isaac</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>Cooperative localization by dual foot-mounted inertial sensors and
  inter-agent ranging</title><categories>cs.RO cs.MA cs.SY</categories><comments>14 pages</comments><acm-class>I.2.9; I.2.11</acm-class><journal-ref>EURASIP Journal on Advances in Signal Processing 2013, 2013:164</journal-ref><doi>10.1186/1687-6180-2013-164</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation challenges of cooperative localization by dual
foot-mounted inertial sensors and inter-agent ranging are discussed and work on
the subject is reviewed. System architecture and sensor fusion are identified
as key challenges. A partially decentralized system architecture based on
step-wise inertial navigation and step-wise dead reckoning is presented. This
architecture is argued to reduce the computational cost and required
communication bandwidth by around two orders of magnitude while only giving
negligible information loss in comparison with a naive centralized
implementation. This makes a joint global state estimation feasible for up to a
platoon-sized group of agents. Furthermore, robust and low-cost sensor fusion
for the considered setup, based on state space transformation and
marginalization, is presented. The transformation and marginalization are used
to give the necessary flexibility for presented sampling based updates for the
inter-agent ranging and ranging free fusion of the two feet of an individual
agent. Finally, characteristics of the suggested implementation are
demonstrated with simulations and a real-time system implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3666</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3666</id><created>2013-04-12</created><authors><author><keyname>Tan</keyname><forenames>Shuo</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Sets Represented as the Length-n Factors of a Word</title><categories>cs.FL cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the following problems: how many different subsets
of Sigma^n can occur as set of all length-n factors of a finite word? If a
subset is representable, how long a word do we need to represent it? How many
such subsets are represented by words of length t? For the first problem, we
give upper and lower bounds of the form alpha^(2^n) in the binary case. For the
second problem, we give a weak upper bound and some experimental data. For the
third problem, we give a closed-form formula in the case where n &lt;= t &lt; 2n.
Algorithmic variants of these problems have previously been studied under the
name &quot;shortest common superstring&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3671</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3671</id><created>2013-04-12</created><authors><author><keyname>Rubin</keyname><forenames>Natan</forenames></author></authors><title>On topological changes in the Delaunay triangulation of moving points</title><categories>cs.CG</categories><comments>To appear in Discrete and Computational Geometry. A preliminary
  version has appeared in SoCG 2012. A stronger result has been submitted to a
  conference</comments><msc-class>52C45</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a collection of $n$ points moving along pseudo-algebraic
trajectories in the plane. One of the hardest open problems in combinatorial
and computational geometry is to obtain a nearly quadratic upper bound, or at
least a subcubic bound, on the maximum number of discrete changes that the
Delaunay triangulation $\DT(P)$ of $P$ experiences during the motion of the
points of $P$.
  In this paper we obtain an upper bound of $O(n^{2+\eps})$, for any $\eps&gt;0$,
under the assumptions that (i) any four points can be co-circular at most
twice, and (ii) either no triple of points can be collinear more than twice, or
no ordered triple of points can be collinear more than once.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3674</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3674</id><created>2013-04-12</created><authors><author><keyname>Gent</keyname><forenames>Ian P.</forenames></author></authors><title>The Recomputation Manifesto</title><categories>cs.GL cs.DL</categories><comments>Unpublished position paper, Version 1.9479, http://recomputation.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replication of scientific experiments is critical to the advance of science.
Unfortunately, the discipline of Computer Science has never treated replication
seriously, even though computers are very good at doing the same thing over and
over again. Not only are experiments rarely replicated, they are rarely even
replicable in a meaningful way. Scientists are being encouraged to make their
source code available, but this is only a small step. Even in the happy event
that source code can be built and run successfully, running code is a long way
away from being able to replicate the experiment that code was used for. I
propose that the discipline of Computer Science must embrace replication of
experiments as standard practice. I propose that the only credible technique to
make experiments truly replicable is to provide copies of virtual machines in
which the experiments are validated to run. I propose that tools and
repositories should be made available to make this happen. I propose to be one
of those who makes it happen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3680</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3680</id><created>2013-04-12</created><updated>2015-11-13</updated><authors><author><keyname>Attali</keyname><forenames>Dominique</forenames></author><author><keyname>Lieutier</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Geometry-driven collapses for converting a Cech complex into a
  triangulation of a nicely triangulable shape</title><categories>cs.CG math.GT</categories><comments>24 pages, 9 figures</comments><journal-ref>Discrete &amp; Computational Geometry: Volume 54, Issue 4 (2015), Page
  798-825</journal-ref><doi>10.1007/s00454-015-9733-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of points that sample a shape, the Rips complex of the data
points is often used in machine-learning to provide an approximation of the
shape easily-computed. It has been proved recently that the Rips complex
captures the homotopy type of the shape assuming the vertices of the complex
meet some mild sampling conditions. Unfortunately, the Rips complex is
generally high-dimensional. To remedy this problem, it is tempting to simplify
it through a sequence of collapses. Ideally, we would like to end up with a
triangulation of the shape. Experiments suggest that, as we simplify the
complex by iteratively collapsing faces, it should indeed be possible to avoid
entering a dead end such as the famous Bing's house with two rooms. This paper
provides a theoretical justification for this empirical observation.
  We demonstrate that the Rips complex of a point-cloud (for a well-chosen
scale parameter) can always be turned into a simplicial complex homeomorphic to
the shape by a sequence of collapses, assuming the shape is nicely triangulable
and well-sampled (two concepts we will explain in the paper). To establish our
result, we rely on a recent work which gives conditions under which the Rips
complex can be converted into a Cech complex by a sequence of collapses. We
proceed in two phases. Starting from the Cech complex, we first produce a
sequence of collapses that arrives to the Cech complex, restricted by the
shape. We then apply a sequence of collapses that transforms the result into
the nerve of some robust covering of the shape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3700</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3700</id><created>2013-04-02</created><authors><author><keyname>Giannotti</keyname><forenames>Fosca</forenames><affiliation>Sandy</affiliation></author><author><keyname>Pedreschi</keyname><forenames>Dino</forenames><affiliation>Sandy</affiliation></author><author><keyname>Alex</keyname><affiliation>Sandy</affiliation></author><author><keyname>Pentland</keyname></author><author><keyname>Lukowicz</keyname><forenames>Paul</forenames></author><author><keyname>Kossmann</keyname><forenames>Donald</forenames></author><author><keyname>Crowley</keyname><forenames>James</forenames></author><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>A planetary nervous system for social mining and collective awareness</title><categories>cs.CY cs.SI physics.soc-ph</categories><journal-ref>Eur. Phys. J. Special Topics vol. 214, pp. 49-75 (2012)</journal-ref><doi>10.1140/epjst/e2012-01688-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a research roadmap of a Planetary Nervous System (PNS), capable of
sensing and mining the digital breadcrumbs of human activities and unveiling
the knowledge hidden in the big data for addressing the big questions about
social complexity. We envision the PNS as a globally distributed,
self-organizing, techno-social system for answering analytical questions about
the status of world-wide society, based on three pillars: social sensing,
social mining, and the idea of trust networks and privacy-aware social mining.
We discuss the ingredients of a science and a technology necessary to build the
PNS upon the three mentioned pillars, beyond the limitations of their
respective state-of-art. Social sensing is aimed at developing better methods
for harvesting the big data from the techno-social ecosystem and make them
available for mining, learning and analysis at a properly high abstraction
level.Social mining is the problem of discovering patterns and models of human
behaviour from the sensed data across the various social dimensions by data
mining, machine learning and social network analysis. Trusted networks and
privacy-aware social mining is aimed at creating a new deal around the
questions of privacy and data ownership empowering individual persons with full
awareness and control on own personal data, so that users may allow access and
use of their data for their own good and the common good. The PNS will provide
a goal-oriented knowledge discovery framework, made of technology and people,
able to configure itself to the aim of answering questions about the pulse of
global society. Given an analytical request, the PNS activates a process
composed by a variety of interconnected tasks exploiting the social sensing and
mining methods within the transparent ecosystem provided by the trusted
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3708</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3708</id><created>2013-04-12</created><authors><author><keyname>Seldin</keyname><forenames>Yevgeny</forenames></author><author><keyname>Bartlett</keyname><forenames>Peter</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author></authors><title>Advice-Efficient Prediction with Expert Advice</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advice-efficient prediction with expert advice (in analogy to label-efficient
prediction) is a variant of prediction with expert advice game, where on each
round of the game we are allowed to ask for advice of a limited number $M$ out
of $N$ experts. This setting is especially interesting when asking for advice
of every expert on every round is expensive. We present an algorithm for
advice-efficient prediction with expert advice that achieves
$O(\sqrt{\frac{N}{M}T\ln N})$ regret on $T$ rounds of the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3716</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3716</id><created>2013-04-12</created><authors><author><keyname>Blaskovic</keyname><forenames>Bruno</forenames></author><author><keyname>Randic</keyname><forenames>Mirko</forenames></author></authors><title>From Declarative Model to Solution: Scheduling Scenario Synthesis</title><categories>cs.SE</categories><comments>9 pages, 4 figures, full version, in official proceedings about half
  pages are missing</comments><journal-ref>9th International International Conference on Telecommunications,
  2007, pp. 139-142</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents deductive programming for scheduling scenario generation.
Modeling for solution is achieved through program transformations. First,
declarative model for scheduling problem domain is introduced. After that model
is interpreted as scheduling domain language and as predicate transition Petri
net. Generated reachability tree presents search space with solutions. At the
end results are discussed and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3733</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3733</id><created>2013-04-12</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>General Quantum Hilbert Space Modeling Scheme for Entanglement</title><categories>quant-ph cs.AI</categories><comments>11 pages. arXiv admin note: text overlap with arXiv:1304.0100</comments><journal-ref>Proceedings of the Seventh International Conference on Quantum,
  Nano and Micro Technologies (pp. 25-31), Eds. V. Ovchinnikov and P. Dini,
  IARIA, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We work out a classification scheme for quantum modeling in Hilbert space of
any kind of composite entity violating Bell's inequalities and exhibiting
entanglement. Our theoretical framework includes situations with entangled
states and product measurements ('customary quantum situation'), and also
situations with both entangled states and entangled measurements ('nonlocal box
situation', 'nonlocal non-marginal box situation'). We show that entanglement
is structurally a joint property of states and measurements. Furthermore,
entangled measurements enable quantum modeling of situations that are usually
believed to be 'beyond quantum'. Our results are also extended from pure states
to quantum mixtures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3742</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3742</id><created>2013-04-12</created><authors><author><keyname>West</keyname><forenames>Robert</forenames></author><author><keyname>White</keyname><forenames>Ryen W.</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric</forenames></author></authors><title>From Cookies to Cooks: Insights on Dietary Patterns via Analysis of Web
  Usage Logs</title><categories>cs.CY cs.IR physics.soc-ph</categories><comments>WWW 2013, 11 pages, 11 figures</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nutrition is a key factor in people's overall health. Hence, understanding
the nature and dynamics of population-wide dietary preferences over time and
space can be valuable in public health. To date, studies have leveraged small
samples of participants via food intake logs or treatment data. We propose a
complementary source of population data on nutrition obtained via Web logs. Our
main contribution is a spatiotemporal analysis of population-wide dietary
preferences through the lens of logs gathered by a widely distributed
Web-browser add-on, using the access volume of recipes that users seek via
search as a proxy for actual food consumption. We discover that variation in
dietary preferences as expressed via recipe access has two main periodic
components, one yearly and the other weekly, and that there exist
characteristic regional differences in terms of diet within the United States.
In a second study, we identify users who show evidence of having made an acute
decision to lose weight. We characterize the shifts in interests that they
express in their search queries and focus on changes in their recipe queries in
particular. Last, we correlate nutritional time series obtained from recipe
queries with time-aligned data on hospital admissions, aimed at understanding
how behavioral data captured in Web logs might be harnessed to identify
potential relationships between diet and acute health problems. In this
preliminary study, we focus on patterns of sodium identified in recipes over
time and patterns of admission for congestive heart failure, a chronic illness
that can be exacerbated by increases in sodium intake.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3745</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3745</id><created>2013-04-12</created><authors><author><keyname>Ghanem</keyname><forenames>Khadoudja</forenames></author></authors><title>Towards more accurate clustering method by using dynamic time warping</title><categories>cs.LG stat.ML</categories><comments>12 pages, 1 figure, 2 tables, journal. arXiv admin note: text overlap
  with arXiv:1206.3509 by other authors</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.2, March 2013</journal-ref><doi>10.5121/ijdkp.2013.3207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An intrinsic problem of classifiers based on machine learning (ML) methods is
that their learning time grows as the size and complexity of the training
dataset increases. For this reason, it is important to have efficient
computational methods and algorithms that can be applied on large datasets,
such that it is still possible to complete the machine learning tasks in
reasonable time. In this context, we present in this paper a more accurate
simple process to speed up ML methods. An unsupervised clustering algorithm is
combined with Expectation, Maximization (EM) algorithm to develop an efficient
Hidden Markov Model (HMM) training. The idea of the proposed process consists
of two steps. In the first step, training instances with similar inputs are
clustered and a weight factor which represents the frequency of these instances
is assigned to each representative cluster. Dynamic Time Warping technique is
used as a dissimilarity function to cluster similar examples. In the second
step, all formulas in the classical HMM training algorithm (EM) associated with
the number of training instances are modified to include the weight factor in
appropriate terms. This process significantly accelerates HMM training while
maintaining the same initial, transition and emission probabilities matrixes as
those obtained with the classical HMM training algorithm. Accordingly, the
classification accuracy is preserved. Depending on the size of the training
set, speedups of up to 2200 times is possible when the size is about 100.000
instances. The proposed approach is not limited to training HMMs, but it can be
employed for a large variety of MLs methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3747</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3747</id><created>2013-04-12</created><authors><author><keyname>Bodnar</keyname><forenames>Todd J</forenames></author><author><keyname>Salath&#xe9;</keyname><forenames>Marcel</forenames></author></authors><title>The Social Maintenance of Cooperation through Hypocrisy</title><categories>cs.SI physics.soc-ph q-bio.OT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation is widespread in human societies, but its maintenance at the
group level remains puzzling if individuals benefit from not cooperating.
Explanations of the maintenance of cooperation generally assume that
cooperative and non-cooperative behavior in others can be assessed and copied
accurately. However, humans have a well known capacity to deceive and thus to
manipulate how others assess their behavior. Here, we show that hypocrisy -
claiming to be acting cooperatively while acting selfishly - can maintain
social cooperation because it prevents the spread of selfish behavior. We
demonstrate this effect both theoretically and experimentally. Hypocrisy allows
the cooperative strategy to spread by taking credit for the success of the
non-cooperative strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3754</identifier>
 <datestamp>2013-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3754</id><created>2013-04-12</created><updated>2013-09-02</updated><authors><author><keyname>Chandrasekaran</keyname><forenames>Karthekeyan</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author><author><keyname>Wan</keyname><forenames>Andrew</forenames></author></authors><title>Faster Private Release of Marginals on Small Databases</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of answering \emph{$k$-way marginal} queries on a
database $D \in (\{0,1\}^d)^n$, while preserving differential privacy. The
answer to a $k$-way marginal query is the fraction of the database's records $x
\in \{0,1\}^d$ with a given value in each of a given set of up to $k$ columns.
Marginal queries enable a rich class of statistical analyses on a dataset, and
designing efficient algorithms for privately answering marginal queries has
been identified as an important open problem in private data analysis.
  For any $k$, we give a differentially private online algorithm that runs in
time $$ \min{\exp(d^{1-\Omega(1/\sqrt{k})}), \exp(d / \log^{.99} d)\} $$ per
query and answers any (possibly superpolynomially long and adaptively chosen)
sequence of $k$-way marginal queries up to error at most $\pm .01$ on every
query, provided $n \gtrsim d^{.51} $. To the best of our knowledge, this is the
first algorithm capable of privately answering marginal queries with a
non-trivial worst-case accuracy guarantee on a database of size $\poly(d, k)$
in time $\exp(o(d))$.
  Our algorithms are a variant of the private multiplicative weights algorithm
(Hardt and Rothblum, FOCS '10), but using a different low-weight representation
of the database. We derive our low-weight representation using approximations
to the OR function by low-degree polynomials with coefficients of bounded
$L_1$-norm. We also prove a strong limitation on our approach that is of
independent approximation-theoretic interest. Specifically, we show that for
any $k = o(\log d)$, any polynomial with coefficients of $L_1$-norm $poly(d)$
that pointwise approximates the $d$-variate OR function on all inputs of
Hamming weight at most $k$ must have degree $d^{1-O(1/\sqrt{k})}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3758</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3758</id><created>2013-04-12</created><authors><author><keyname>Pande</keyname><forenames>Gaurav</forenames></author></authors><title>Metrics for Video Quality Assessment in Mobile Scenarios</title><categories>cs.MM cs.NI</categories><comments>Submitted to International Journal of Digital Multimedia Broadcasting</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With exponential increase in the volumes of video traffic in cellular
net-works, there is an increasing need for optimizing the quality of video
delivery. 4G networks (Long Term Evolution Advanced or LTE A) are being
introduced in many countries worldwide, which allow a downlink speed of upto 1
Gbps and uplink of 100 Mbps over a single base station. This makes a strong
push towards video broadcasting over LTE networks, characterizing its
performance and developing metrics which can be deployed to provide user
feedback of video quality and feed-back them to network operators to fine tune
the network. In this paper, we characterize the performance of video
transmission over LTE A physical layer using popular video quality metrics such
as SSIM, Blocking, Blurring, NIQE and BRISQUE. We conduct experiments to find a
suitable no-reference metrics for mobile scenario and find that Blocking
Metrics is most promising in case of channel or modulation variations but it
does not perform well to quantize variations in compression ratios. The metrics
BRISQUE is very efficient in quantizing this distortion and performs well in
case of network variations also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3760</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3760</id><created>2013-04-12</created><updated>2013-10-12</updated><authors><author><keyname>Gaynor</keyname><forenames>Sheila</forenames></author><author><keyname>Bair</keyname><forenames>Eric</forenames></author></authors><title>Identification of biologically relevant subtypes via preweighted sparse
  clustering</title><categories>stat.ME cs.LG q-bio.QM stat.AP stat.ML</categories><comments>Version 2: 33 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cluster analysis methods are used to identify homogeneous subgroups in a data
set. Frequently one applies cluster analysis in order to identify biologically
interesting subgroups. In particular, one may wish to identify subgroups that
are associated with a particular outcome of interest. Conventional clustering
methods often fail to identify such subgroups, particularly when there are a
large number of high-variance features in the data set. Conventional methods
may identify clusters associated with these high-variance features when one
wishes to obtain secondary clusters that are more interesting biologically or
more strongly associated with a particular outcome of interest. We describe a
modification of the sparse clustering method of Witten and Tibshirani (2010)
that can be used to identify such secondary clusters or clusters associated
with an outcome of interest. We show that this method can correctly identify
such clusters of interest in several simulation scenarios. The method is also
applied to a large case-control study of TMD and a leukemia microarray data
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3762</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3762</id><created>2013-04-12</created><authors><author><keyname>Burgin</keyname><forenames>Mark</forenames></author><author><keyname>Eberbach</keyname><forenames>Eugene</forenames></author></authors><title>Evolutionary Turing in the Context of Evolutionary Machines</title><categories>cs.AI</categories><msc-class>68Q05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the roots of evolutionary computation was the idea of Turing about
unorganized machines. The goal of this work is the development of foundations
for evolutionary computations, connecting Turing's ideas and the contemporary
state of art in evolutionary computations. To achieve this goal, we develop a
general approach to evolutionary processes in the computational context,
building mathematical models of computational systems, functioning of which is
based on evolutionary processes, and studying properties of such systems.
Operations with evolutionary machines are described and it is explored when
definite classes of evolutionary machines are closed with respect to basic
operations with these machines. We also study such properties as linguistic and
functional equivalence of evolutionary machines and their classes, as well as
computational power of evolutionary machines and their classes, comparing of
evolutionary machines to conventional automata, such as finite automata or
Turing machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3763</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3763</id><created>2013-04-12</created><authors><author><keyname>Hassan</keyname><forenames>Md. Rakib</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Kamrul</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>An Improved ACS Algorithm for the Solutions of Larger TSP Problems</title><categories>cs.AI cs.DS cs.NE</categories><journal-ref>Procs. of the 3rd International Conference on Electrical,
  Electronics and Computer Engineering (ICEECE 2003), pp. 201-206, Dhaka,
  Bangladesh, December 22-24, (2003)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving large traveling salesman problem (TSP) in an efficient way is a
challenging area for the researchers of computer science. This paper presents a
modified version of the ant colony system (ACS) algorithm called Red-Black Ant
Colony System (RB-ACS) for the solutions of TSP which is the most prominent
member of the combinatorial optimization problem. RB-ACS uses the concept of
ant colony system together with the parallel search of genetic algorithm for
obtaining the optimal solutions quickly. In this paper, it is shown that the
proposed RB-ACS algorithm yields significantly better performance than the
existing best-known algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3767</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3767</id><created>2013-04-12</created><authors><author><keyname>Rotithor</keyname><forenames>Hemant</forenames></author></authors><title>A Taxonomy of Performance Assurance Methodologies and its Application in
  High Performance Computer Architectures</title><categories>cs.PF</categories><comments>17 pages, 5 figures, 3 tables</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.4, No.2, March 2013</journal-ref><doi>10.5121/ijsea.2013.4201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a systematic approach to the complex problem of high
confidence performance assurance of high performance architectures based on
methods used over several generations of industrial microprocessors. A taxonomy
is presented for performance assurance through three key stages of a product
life cycle-high level performance, RTL performance, and silicon performance.
The proposed taxonomy includes two components-independent performance assurance
space for each stage and a correlation performance assurance space between
stages. It provides a detailed insight into the performance assurance space in
terms of coverage provided taking into account capabilities and limitations of
tools and methodologies used at each stage. An application of the taxonomy to
cases described in the literature and to high performance Intel architectures
is shown. The proposed work should be of interest to manufacturers of high
performance microprocessor/chipset architectures and has not been discussed in
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3771</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3771</id><created>2013-04-13</created><authors><author><keyname>Sani</keyname><forenames>Ardalan Amiri</forenames></author><author><keyname>Nair</keyname><forenames>Sreekumar</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author><author><keyname>Jacobson</keyname><forenames>Quinn</forenames></author></authors><title>Making I/O Virtualization Easy with Device Files</title><categories>cs.OS</categories><report-no>Rice University ECE Technical Report 2013-04-13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personal computers have diverse and fast-evolving I/O devices, making their
I/O virtualization different from that of servers and data centers. In this
paper, we present our recent endeavors in simplifying I/O virtualization for
personal computers. Our key insight is that many operating systems, including
Unix-like ones, abstract I/O devices as device files. There is a small and
stable set of operations on device files, therefore, I/O virtualization at the
device file boundary requires a one-time effort to support various I/O devices.
  We present devirtualization, our design of I/O virtualization at the device
file boundary and its implementation for Linux/x86 systems. We are able to
virtualize various GPUs, input devices, cameras, and audio devices with fewer
than 4900 LoC, of which only about 300 are specific to I/O device classes. Our
measurements show that devirtualized devices achieve interactive performance
indistinguishable from native ones by human users, even when running 3D HD
games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3778</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3778</id><created>2013-04-13</created><authors><author><keyname>Ren</keyname><forenames>Jimmy SJ.</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Liao</keyname><forenames>Stephen Shaoyi</forenames></author></authors><title>Optimal Control Theory in Intelligent Transportation Systems Research -
  A Review</title><categories>cs.SY cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous motorization and urbanization around the globe leads to an
expansion of population in major cities. Therefore, ever-growing pressure
imposed on the existing mass transit systems calls for a better technology,
Intelligent Transportation Systems (ITS), to solve many new and demanding
management issues. Many studies in the extant ITS literature attempted to
address these issues within which various research methodologies were adopted.
However, there is very few paper summarized what does optimal control theory
(OCT), one of the sharpest tools to tackle management issues in engineering, do
in solving these issues. It{\textquoteright}s both important and interesting to
answer the following two questions. (1) How does OCT contribute to ITS research
objectives? (2) What are the research gaps and possible future research
directions? We searched 11 top transportation and control journals and reviewed
41 research articles in ITS area in which OCT was used as the main research
methodology. We categorized the articles by four different ways to address our
research questions. We can conclude from the review that OCT is widely used to
address various aspects of management issues in ITS within which a large
portion of the studies aimed to reduce traffic congestion. We also critically
discussed these studies and pointed out some possible future research
directions towards which OCT can be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3779</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3779</id><created>2013-04-13</created><authors><author><keyname>Naik</keyname><forenames>Tejashvi R.</forenames></author><author><keyname>Dabhi</keyname><forenames>Vipul K.</forenames></author></authors><title>Improving Generalization Ability of Genetic Programming: Comparative
  Study</title><categories>cs.NE</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of empirical modeling using Genetic Programming (GP), it is
important to evolve solution with good generalization ability. Generalization
ability of GP solutions get affected by two important issues: bloat and
over-fitting. Bloat is uncontrolled growth of code without any gain in fitness
and important issue in GP. We surveyed and classified existing literature
related to different techniques used by GP research community to deal with the
issue of bloat. Moreover, the classifications of different bloat control
approaches and measures for bloat are discussed. Next, we tested four bloat
control methods: Tarpeian, double tournament, lexicographic parsimony pressure
with direct bucketing and ratio bucketing on six different problems and
identified where each bloat control method performs well on per problem basis.
Based on the analysis of each method, we combined two methods: double
tournament (selection method) and Tarpeian method (works before evaluation) to
avoid bloated solutions and compared with the results obtained from individual
performance of double tournament method. It was found that the results were
improved with this combination of two methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3780</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3780</id><created>2013-04-13</created><updated>2014-09-18</updated><authors><author><keyname>Alekseyev</keyname><forenames>Max A.</forenames></author><author><keyname>Berger</keyname><forenames>Toby</forenames></author></authors><title>Solving the Tower of Hanoi with Random Moves</title><categories>math.CO cs.DM math.PR</categories><journal-ref>In: The Mathematics of Various Entertaining Subjects: Research in
  Recreational Math, Princeton University Press, 2016, pp. 65-79. ISBN
  978-0-691-16403-8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the exact formulae for the expected number of moves to solve several
variants of the Tower of Hanoi puzzle with 3 pegs and n disks, when each move
is chosen uniformly randomly from the set of all valid moves. We further
present an alternative proof for one of the formulae that couples a theorem
about expected commute times of random walks on graphs with the delta-to-wye
transformation used in the analysis of three-phase AC systems for electrical
power distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3792</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3792</id><created>2013-04-13</created><authors><author><keyname>Jamali</keyname><forenames>A. R. M. Jalal Uddin</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Bazlar</forenames></author></authors><title>Solving Linear Equations Using a Jacobi Based Time-Variant Adaptive
  Hybrid Evolutionary Algorithm</title><categories>cs.NE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.3200,
  arXiv:1304.2097</comments><journal-ref>Procs. of the 7th International Conference on Computer &amp;
  Information Technology (ICCIT 2004), pp. 688-693, Dhaka, Bangladesh, December
  26-28, (2004)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large set of linear equations, especially for sparse and structured
coefficient (matrix) equations, solutions using classical methods become
arduous. And evolutionary algorithms have mostly been used to solve various
optimization and learning problems. Recently, hybridization of classical
methods (Jacobi method and Gauss-Seidel method) with evolutionary computation
techniques have successfully been applied in linear equation solving. In the
both above hybrid evolutionary methods, uniform adaptation (UA) techniques are
used to adapt relaxation factor. In this paper, a new Jacobi Based Time-Variant
Adaptive (JBTVA) hybrid evolutionary algorithm is proposed. In this algorithm,
a Time-Variant Adaptive (TVA) technique of relaxation factor is introduced
aiming at both improving the fine local tuning and reducing the disadvantage of
uniform adaptation of relaxation factors. This algorithm integrates the Jacobi
based SR method with time variant adaptive evolutionary algorithm. The
convergence theorems of the proposed algorithm are proved theoretically. And
the performance of the proposed algorithm is compared with JBUA hybrid
evolutionary algorithm and classical methods in the experimental domain. The
proposed algorithm outperforms both the JBUA hybrid algorithm and classical
methods in terms of convergence speed and effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3795</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3795</id><created>2013-04-13</created><authors><author><keyname>Ariananda</keyname><forenames>Dyonisius Dony</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Madan Kumar</forenames></author><author><keyname>Nikookar</keyname><forenames>Homayoun</forenames></author></authors><title>An Investigation of Wavelet Packet Transform for Spectrum Estimation</title><categories>cs.IT math.IT math.SP</categories><comments>Proceeding of The 12th International Symposium on Wireless Personal
  Multimedia Communications (WPMC 2009, Sendai, Japan)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we investigate the application of wavelet packet transform
as a novel spectrum sensing approach. The main attraction for wavelet packets
is the tradeoffs they offer in terms of satisfying various performance metrics
such as frequency resolution, variance of the estimated power spectral density
(PSD) and complexity. The results of the experiments show that the wavelet
based approach offers great flexibility, reconfigure ability and adaptability
apart from its performances which are comparable and at times even better than
Fourier based estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3796</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3796</id><created>2013-04-13</created><updated>2013-06-30</updated><authors><author><keyname>Simko</keyname><forenames>Gabor I.</forenames></author><author><keyname>Csermely</keyname><forenames>Peter</forenames></author></authors><title>Nodes having a major influence to break cooperation define a novel
  centrality measure: game centrality</title><categories>q-bio.MN cs.GT cs.SI nlin.AO physics.soc-ph</categories><comments>18 pages, 2 figures, 3 Tables + a supplement containing 8 pages, 1
  figure, 2 Tables and the pseudo-code of the algorithm, the NetworGame
  algorithm is downloadable from here: http://www.NetworGame.linkgroup.hu</comments><journal-ref>PLoS ONE (2013) 8: e67159</journal-ref><doi>10.1371/journal.pone.0067159</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation played a significant role in the self-organization and evolution
of living organisms. Both network topology and the initial position of
cooperators heavily affect the cooperation of social dilemma games. We
developed a novel simulation program package, called 'NetworGame', which is
able to simulate any type of social dilemma games on any model, or real world
networks with any assignment of initial cooperation or defection strategies to
network nodes. The ability of initially defecting single nodes to break overall
cooperation was called as 'game centrality'. The efficiency of this measure was
verified on well-known social networks, and was extended to 'protein games',
i.e. the simulation of cooperation between proteins, or their amino acids. Hubs
and in particular, party hubs of yeast protein-protein interaction networks had
a large influence to convert the cooperation of other nodes to defection.
Simulations on methionyl-tRNA synthetase protein structure network indicated an
increased influence of nodes belonging to intra-protein signaling pathways on
breaking cooperation. The efficiency of single, initially defecting nodes to
convert the cooperation of other nodes to defection in social dilemma games may
be an important measure to predict the importance of nodes in the integration
and regulation of complex systems. Game centrality may help to design more
efficient interventions to cellular networks (in forms of drugs), to ecosystems
and social networks. The NetworGame algorithm is downloadable from here:
www.NetworGame.linkgroup.hu
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3804</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3804</id><created>2013-04-13</created><authors><author><keyname>Coppa</keyname><forenames>Emilio</forenames></author><author><keyname>Demetrescu</keyname><forenames>Camil</forenames></author><author><keyname>Finocchi</keyname><forenames>Irene</forenames></author><author><keyname>Marotta</keyname><forenames>Romolo</forenames></author></authors><title>Multithreaded Input-Sensitive Profiling</title><categories>cs.PF cs.PL</categories><msc-class>68N30</msc-class><acm-class>C.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Input-sensitive profiling is a recent performance analysis technique that
makes it possible to estimate the empirical cost function of individual
routines of a program, helping developers understand how performance scales to
larger inputs and pinpoint asymptotic bottlenecks in the code. A current
limitation of input-sensitive profilers is that they specifically target
sequential computations, ignoring any communication between threads. In this
paper we show how to overcome this limitation, extending the range of
applicability of the original approach to multithreaded applications and to
applications that operate on I/O streams. We develop new metrics for
automatically estimating the size of the input given to each routine
activation, addressing input produced by non-deterministic memory stores
performed by other threads as well as by the OS kernel (e.g., in response to
I/O or network operations). We provide real case studies, showing that our
extension allows it to characterize the behavior of complex applications more
precisely than previous approaches. An extensive experimental investigation on
a variety of benchmark suites (including the SPEC OMP2012 and the PARSEC
benchmarks) shows that our Valgrind-based input-sensitive profiler incurs an
overhead comparable to other prominent heavyweight analysis tools, while
collecting significantly more performance points from each profiling session
and correctly characterizing both thread-induced and external input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3812</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3812</id><created>2013-04-13</created><updated>2013-07-27</updated><authors><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Time-Optimal Interactive Proofs for Circuit Evaluation</title><categories>cs.CR cs.CC cs.DS</categories><comments>This is the full version of the CRYPTO 2013 paper of the same title.
  This version corrects typos from prior versions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, researchers have been working toward the development of practical
general-purpose protocols for verifiable computation. These protocols enable a
computationally weak verifier to offload computations to a powerful but
untrusted prover, while providing the verifier with a guarantee that the prover
performed the computations correctly. Despite substantial progress, existing
implementations are not yet practical. The main bottleneck is typically the
extra effort required by the prover to return an answer with a guarantee of
correctness, compared to returning an answer with no guarantee.
  We describe a refinement of a powerful interactive proof protocol originally
due to Goldwasser, Kalai, and Rothblum. Cormode, Mitzenmacher, and Thaler show
how to implement the prover in this protocol in time O(S log S), where S is the
size of an arithmetic circuit computing the function of interest. Our
refinements apply to circuits whose wiring pattern is sufficiently &quot;regular&quot;;
for these circuits, we bring the runtime of the prover down to O(S). That is,
our prover can evaluate the circuit with a guarantee of correctness, with only
a constant-factor blowup in work compared to evaluating the circuit with no
guarantee.
  We argue that our refinements capture a large class of circuits, and prove
some theorems formalizing this. Experimentally, our refinements yield a 200x
speedup for the prover over the implementation of Cormode et al., and our
prover is less than 10x slower than a C++ program that simply evaluates the
circuit. Along the way, we describe a special-purpose protocol for matrix
multiplication that is of interest in its own right.
  Our final contribution is a protocol targeted at general data parallel
computation. Compared to prior work, this protocol can more efficiently verify
complicated computations as long as that computation is applied independently
to many pieces of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3816</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3816</id><created>2013-04-13</created><authors><author><keyname>Chakrabarti</keyname><forenames>Amit</forenames></author><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Goyal</keyname><forenames>Navin</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Annotations for Sparse Data Streams</title><categories>cs.CC cs.DS</categories><comments>29 pages, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by cloud computing, a number of recent works have studied annotated
data streams and variants thereof. In this setting, a computationally weak
verifier (cloud user), lacking the resources to store and manipulate his
massive input locally, accesses a powerful but untrusted prover (cloud
service). The verifier must work within the restrictive data streaming
paradigm. The prover, who can annotate the data stream as it is read, must not
just supply the answer but also convince the verifier of its correctness.
Ideally, both the amount of annotation and the space used by the verifier
should be sublinear in the relevant input size parameters.
  A rich theory of such algorithms -- which we call schemes -- has emerged.
Prior work has shown how to leverage the prover's power to efficiently solve
problems that have no non-trivial standard data stream algorithms. However,
while optimal schemes are now known for several basic problems, such optimality
holds only for streams whose length is commensurate with the size of the data
universe. In contrast, many real-world datasets are relatively sparse,
including graphs that contain only O(n^2) edges, and IP traffic streams that
contain much fewer than the total number of possible IP addresses, 2^128 in
IPv6.
  We design the first schemes that allow both the annotation and the space
usage to be sublinear in the total number of stream updates rather than the
size of the data universe. We solve significant problems, including variations
of INDEX, SET-DISJOINTNESS, and FREQUENCY-MOMENTS, plus several natural
problems on graphs. On the other hand, we give a new lower bound that, for the
first time, rules out smooth tradeoffs between annotation and space usage for a
specific problem. Our technique brings out new nuances in Merlin-Arthur
communication complexity models, and provides a separation between online
versions of the MA and AMA models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3819</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3819</id><created>2013-04-13</created><authors><author><keyname>Cao</keyname><forenames>Qiang</forenames></author><author><keyname>Yang</keyname><forenames>Xiaowei</forenames></author></authors><title>SybilFence: Improving Social-Graph-Based Sybil Defenses with User
  Negative Feedback</title><categories>cs.SI physics.soc-ph</categories><comments>Submitted to WOSN 2012 on March 14, 2012</comments><report-no>Duke CS Technical Report: CS-TR-2012-05, March 2012</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting and suspending fake accounts (Sybils) in online social networking
(OSN) services protects both OSN operators and OSN users from illegal
exploitation. Existing social-graph-based defense schemes effectively bound the
accepted Sybils to the total number of social connections between Sybils and
non-Sybil users. However, Sybils may still evade the defenses by soliciting
many social connections to real users. We propose SybilFence, a system that
improves over social-graph-based Sybil defenses to further thwart Sybils.
SybilFence is based on the observation that even well-maintained fake accounts
inevitably receive a significant number of user negative feedback, such as the
rejections to their friend requests. Our key idea is to discount the social
edges on users that have received negative feedback, thereby limiting the
impact of Sybils' social edges. The preliminary simulation results show that
our proposal is more resilient to attacks where fake accounts continuously
solicit social connections over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3826</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3826</id><created>2013-04-13</created><authors><author><keyname>Park</keyname><forenames>Seok-Hwan</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Sahin</keyname><forenames>Onur</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Multi-Layer Transmission and Hybrid Relaying for Relay Channels with
  Multiple Out-of-Band Relays</title><categories>cs.IT math.IT</categories><comments>Submitted to Transactions on Emerging Telecommunications Technologies
  (ETT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a relay channel is studied in which a source encoder
communicates with a destination decoder through a number of out-of-band relays
that are connected to the decoder through capacity-constrained digital backhaul
links. This model is motivated by the uplink of cloud radio access networks. In
this scenario, a novel transmission and relaying strategies are proposed in
which multi-layer transmission is used, on the one hand, to adaptively leverage
the different decoding capabilities of the relays and, on the other hand, to
enable hybrid decode-and-forward (DF) and compress-and-forward (CF) relaying.
The hybrid relaying strategy allows each relay to forward part of the decoded
messages and a compressed version of the received signal to the decoder. The
problem of optimizing the power allocation across the layers and the
compression test channels is formulated. Albeit non-convex, the derived problem
is found to belong to the class of so called complementary geometric programs
(CGPs). Using this observation, an iterative algorithm based on the homotopy
method is proposed that achieves a stationary point of the original problem by
solving a sequence of geometric programming (GP), and thus convex, problems.
Numerical results are provided that show the effectiveness of the proposed
multi-layer hybrid scheme in achieving performance close to a theoretical
(cutset) upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3840</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3840</id><created>2013-04-13</created><authors><author><keyname>Meftahi</keyname><forenames>Badreddine</forenames></author><author><keyname>Saidi</keyname><forenames>Ourida Ben Boubaker</forenames></author></authors><title>A New Homogeneity Inter-Clusters Measure in SemiSupervised Clustering</title><categories>cs.LG</categories><comments>9 pages, 27 figures, International Journal of Computer Applications
  see http://www.ijcaonline.org/</comments><msc-class>62H30, 91C20, 68P15, 68T05</msc-class><acm-class>H.2.8; H.3.3; I.2; I.5.3</acm-class><journal-ref>journal = {International Journal of Computer Applications}, year =
  {2013}, volume = {66}, number = {24}, pages = {37-45}, month = {March}, note
  = {Published by Foundation of Computer Science, New York, USA}</journal-ref><doi>10.5120/11267-6526</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many studies in data mining have proposed a new learning called
semi-Supervised. Such type of learning combines unlabeled and labeled data
which are hard to obtain. However, in unsupervised methods, the only unlabeled
data are used. The problem of significance and the effectiveness of
semi-supervised clustering results is becoming of main importance. This paper
pursues the thesis that muchgreater accuracy can be achieved in such clustering
by improving the similarity computing. Hence, we introduce a new approach of
semisupervised clustering using an innovative new homogeneity measure of
generated clusters. Our experimental results demonstrate significantly improved
accuracy as a result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3841</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3841</id><created>2013-04-13</created><updated>2014-09-25</updated><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author><author><keyname>Liu</keyname><forenames>Haitao</forenames></author></authors><title>The risks of mixing dependency lengths from sequences of different
  length</title><categories>cs.CL physics.data-an</categories><comments>Laguage and referencing has been improved; Eqs. 7, 11, B7 and B8 have
  been corrected</comments><journal-ref>Glottotheory 5 (2), 143-155 (2014)</journal-ref><doi>10.1515/glot-2014-0014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixing dependency lengths from sequences of different length is a common
practice in language research. However, the empirical distribution of
dependency lengths of sentences of the same length differs from that of
sentences of varying length and the distribution of dependency lengths depends
on sentence length for real sentences and also under the null hypothesis that
dependencies connect vertices located in random positions of the sequence. This
suggests that certain results, such as the distribution of syntactic dependency
lengths mixing dependencies from sentences of varying length, could be a mere
consequence of that mixing. Furthermore, differences in the global averages of
dependency length (mixing lengths from sentences of varying length) for two
different languages do not simply imply a priori that one language optimizes
dependency lengths better than the other because those differences could be due
to differences in the distribution of sentence lengths and other factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3842</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3842</id><created>2013-04-13</created><updated>2014-08-28</updated><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author></authors><title>Proceedings of the Sixteenth Conference on Uncertainty in Artificial
  Intelligence (2000)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2000</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Sixteenth Conference on Uncertainty in
Artificial Intelligence, which was held in San Francisco, CA, June 30 - July 3,
2000
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3843</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3843</id><created>2013-04-13</created><updated>2014-08-28</updated><authors><author><keyname>Laskey</keyname><forenames>Kathryn</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Proceedings of the Fifteenth Conference on Uncertainty in Artificial
  Intelligence (1999)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1999</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Fifteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Stockholm Sweden, July 30 - August
1, 1999
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3844</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3844</id><created>2013-04-13</created><updated>2014-08-28</updated><authors><author><keyname>Cooper</keyname><forenames>Gregory</forenames></author><author><keyname>Moral</keyname><forenames>Serafin</forenames></author></authors><title>Proceedings of the Fourteenth Conference on Uncertainty in Artificial
  Intelligence (1998)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1998</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Fourteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Madison, WI, July 24-26, 1998
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3845</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3845</id><created>2013-04-13</created><updated>2014-03-30</updated><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>The Impact of Situation Clustering in Contextual-Bandit Algorithm for
  Context-Aware Recommender Systems</title><categories>cs.IR</categories><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing approaches in Context-Aware Recommender Systems (CRS) focus on
recommending relevant items to users taking into account contextual
information, such as time, location, or social aspects. However, few of them
have considered the problem of user's content dynamicity. We introduce in this
paper an algorithm that tackles the user's content dynamicity by modeling the
CRS as a contextual bandit algorithm and by including a situation clustering
algorithm to improve the precision of the CRS. Within a deliberately designed
offline simulation framework, we conduct evaluations with real online event log
data. The experimental results and detailed analysis reveal several important
discoveries in context aware recommender system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3846</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3846</id><created>2013-04-13</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash</forenames></author></authors><title>Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1997</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3847</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3847</id><created>2013-04-13</created><updated>2014-08-28</updated><authors><author><keyname>Horvitz</keyname><forenames>Eric</forenames></author><author><keyname>Jensen</keyname><forenames>Finn</forenames></author></authors><title>Proceedings of the Twelfth Conference on Uncertainty in Artificial
  Intelligence (1996)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1996</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twelfth Conference on Uncertainty in
Artificial Intelligence, which was held in Portland, OR, August 1-4, 1996
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3848</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3848</id><created>2013-04-13</created><updated>2014-08-28</updated><authors><author><keyname>Besnard</keyname><forenames>Philippe</forenames></author><author><keyname>Hanks</keyname><forenames>Steve</forenames></author></authors><title>Proceedings of the Eleventh Conference on Uncertainty in Artificial
  Intelligence (1995)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1995</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Eleventh Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QU, August 18-20, 1995
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3849</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3849</id><created>2013-04-13</created><authors><author><keyname>de Mantaras</keyname><forenames>Ramon Lopez</forenames></author><author><keyname>Poole</keyname><forenames>David</forenames></author></authors><title>Proceedings of the Tenth Conference on Uncertainty in Artificial
  Intelligence (1994)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1994</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Tenth Conference on Uncertainty in Artificial
Intelligence, which was held in Seattle, WA, July 29-31, 1994
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3850</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3850</id><created>2013-04-13</created><authors><author><keyname>Si</keyname><forenames>Hongbo</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Polar Coding for Fading Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polar coding scheme for fading channels is proposed in this paper. More
specifically, the focus is Gaussian fading channel with a BPSK modulation
technique, where the equivalent channel could be modeled as a binary symmetric
channel with varying cross-over probabilities. To deal with variable channel
states, a coding scheme of hierarchically utilizing polar codes is proposed. In
particular, by observing the polarization of different binary symmetric
channels over different fading blocks, each channel use corresponding to a
different polarization is modeled as a binary erasure channel such that polar
codes could be adopted to encode over blocks. It is shown that the proposed
coding scheme, without instantaneous channel state information at the
transmitter, achieves the capacity of the corresponding fading binary symmetric
channel, which is constructed from the underlying fading AWGN channel through
the modulation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3851</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3851</id><created>2013-04-13</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Mamdani</keyname><forenames>E.</forenames></author></authors><title>Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1993</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3852</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3852</id><created>2013-04-13</created><authors><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Smets</keyname><forenames>Philippe</forenames></author><author><keyname>Wellman</keyname><forenames>Michael</forenames></author></authors><title>Proceedings of the Eighth Conference on Uncertainty in Artificial
  Intelligence (1992)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1992</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Eighth Conference on Uncertainty in Artificial
Intelligence, which was held in Stanford, CA, July 17-19, 1992
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3853</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3853</id><created>2013-04-13</created><authors><author><keyname>Bonissone</keyname><forenames>Piero</forenames></author><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Proceedings of the Seventh Conference on Uncertainty in Artificial
  Intelligence (1991)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1991</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Los Angeles, CA, July 13-15, 1991
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3854</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3854</id><created>2013-04-13</created><updated>2014-08-28</updated><authors><author><keyname>Bonissone</keyname><forenames>Piero</forenames></author><author><keyname>Henrion</keyname><forenames>Max</forenames></author><author><keyname>Kanal</keyname><forenames>Laveen</forenames></author><author><keyname>Lemmer</keyname><forenames>John</forenames></author></authors><title>Proceedings of the Sixth Conference on Uncertainty in Artificial
  Intelligence (1990)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1990</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Sixth Conference on Uncertainty in Artificial
Intelligence, which was held in Cambridge, MA, Jul 27 - Jul 29, 1990
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3855</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3855</id><created>2013-04-13</created><authors><author><keyname>Henrion</keyname><forenames>Max</forenames></author><author><keyname>Kanal</keyname><forenames>Laveen</forenames></author><author><keyname>Lemmer</keyname><forenames>John</forenames></author><author><keyname>Shachter</keyname><forenames>Ross</forenames></author></authors><title>Proceedings of the Fifth Conference on Uncertainty in Artificial
  Intelligence (1989)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1989</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Fifth Conference on Uncertainty in Artificial
Intelligence, which was held in Windsor, ON, August 18-20, 1989
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3856</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3856</id><created>2013-04-13</created><authors><author><keyname>Kanal</keyname><forenames>Laveen</forenames></author><author><keyname>Lemmer</keyname><forenames>John</forenames></author><author><keyname>Levitt</keyname><forenames>Tod</forenames></author><author><keyname>Shachter</keyname><forenames>Ross</forenames></author></authors><title>Proceedings of the Fourth Conference on Uncertainty in Artificial
  Intelligence (1988)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1988</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Fourth Conference on Uncertainty in Artificial
Intelligence, which was held in Minneapolis, MN, July 10-12, 1988
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3857</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3857</id><created>2013-04-13</created><authors><author><keyname>Kanal</keyname><forenames>Laveen</forenames></author><author><keyname>Lemmer</keyname><forenames>John</forenames></author><author><keyname>Levitt</keyname><forenames>Tod</forenames></author></authors><title>Proceedings of the Third Conference on Uncertainty in Artificial
  Intelligence (1987)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1987</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Third Conference on Uncertainty in Artificial
Intelligence, which was held in Seattle, WA, July 10-12, 1987
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3859</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3859</id><created>2013-04-13</created><authors><author><keyname>Kanal</keyname><forenames>Laveen</forenames></author><author><keyname>Lemmer</keyname><forenames>John</forenames></author></authors><title>Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1986</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3860</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3860</id><created>2013-04-13</created><authors><author><keyname>Letia</keyname><forenames>Ioan Alfred</forenames></author><author><keyname>Groza</keyname><forenames>Adrian</forenames></author></authors><title>Justificatory and Explanatory Argumentation for Committing Agents</title><categories>cs.AI</categories><journal-ref>ARGMAS 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the interaction between agents we can have an explicative discourse, when
communicating preferences or intentions, and a normative discourse, when
considering normative knowledge. For justifying their actions our agents are
endowed with a Justification and Explanation Logic (JEL), capable to cover both
the justification for their commitments and explanations why they had to act in
that way, due to the current situation in the environment. Social commitments
are used to formalise justificatory and explanatory patterns. The combination
of ex- planation, justification, and commitments
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3865</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3865</id><created>2013-04-13</created><authors><author><keyname>Nekouei</keyname><forenames>Ehsan</forenames></author><author><keyname>Inaltekin</keyname><forenames>Hazer</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author></authors><title>Distributed Cognitive Multiple Access Networks: Power Control,
  Scheduling and Multiuser Diversity</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1209.1426</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies optimal distributed power allocation and scheduling
policies (DPASPs) for distributed total power and interference limited (DTPIL)
cognitive multiple access networks in which secondary users (SU) independently
perform power allocation and scheduling tasks using their local knowledge of
secondary transmitter secondary base-station (STSB) and secondary transmitter
primary base-station (STPB) channel gains. In such networks, transmission
powers of SUs are limited by an average total transmission power constraint and
by a constraint on the average interference power that SUs cause to the primary
base-station. We first establish the joint optimality of water-filling power
allocation and threshold-based scheduling policies for DTPIL networks. We then
show that the secondary network throughput under the optimal DPASP scales
according to $\frac{1}{\e{}n_h}\log\logp{N}$, where $n_h$ is a parameter
obtained from the distribution of STSB channel power gains and $N$ is the total
number of SUs. From a practical point of view, our results signify the fact
that distributed cognitive multiple access networks are capable of harvesting
multiuser diversity gains without employing centralized schedulers and feedback
links as well as without disrupting primary's quality-of-service (QoS)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3866</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3866</id><created>2013-04-13</created><authors><author><keyname>Billey</keyname><forenames>Sara C.</forenames></author><author><keyname>Tenner</keyname><forenames>Bridget E.</forenames></author></authors><title>Fingerprint databases for theorems</title><categories>math.HO cs.DL</categories><comments>to appear in Notices of the AMS</comments><msc-class>00-02, 00A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the advantages of searchable, collaborative, language-independent
databases of mathematical results, indexed by &quot;fingerprints&quot; of small and
canonical data. Our motivating example is Neil Sloane's massively influential
On-Line Encyclopedia of Integer Sequences. We hope to encourage the greater
mathematical community to search for the appropriate fingerprints within each
discipline, and to compile fingerprint databases of results wherever possible.
The benefits of these databases are broad - advancing the state of knowledge,
enhancing experimental mathematics, enabling researchers to discover unexpected
connections between areas, and even improving the refereeing process for
journal publication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3868</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3868</id><created>2013-04-13</created><updated>2013-07-29</updated><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Chawla</keyname><forenames>Shuchi</forenames></author><author><keyname>Umboh</keyname><forenames>Seeun</forenames></author></authors><title>Network Design with Coverage Costs</title><categories>cs.DS</categories><comments>Updated version with additional results</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study network design with a cost structure motivated by redundancy in data
traffic. We are given a graph, g groups of terminals, and a universe of data
packets. Each group of terminals desires a subset of the packets from its
respective source. The cost of routing traffic on any edge in the network is
proportional to the total size of the distinct packets that the edge carries.
Our goal is to find a minimum cost routing. We focus on two settings. In the
first, the collection of packet sets desired by source-sink pairs is laminar.
For this setting, we present a primal-dual based 2-approximation, improving
upon a logarithmic approximation due to Barman and Chawla (2012). In the second
setting, packet sets can have non-trivial intersection. We focus on the case
where each packet is desired by either a single terminal group or by all of the
groups, and the graph is unweighted. For this setting we present an O(log
g)-approximation.
  Our approximation for the second setting is based on a novel spanner-type
construction in unweighted graphs that, given a collection of g vertex subsets,
finds a subgraph of cost only a constant factor more than the minimum spanning
tree of the graph, such that every subset in the collection has a Steiner tree
in the subgraph of cost at most O(log g) that of its minimum Steiner tree in
the original graph. We call such a subgraph a group spanner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3872</identifier>
 <datestamp>2013-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3872</id><created>2013-04-13</created><updated>2013-05-10</updated><authors><author><keyname>Epstein</keyname><forenames>Samuel</forenames></author></authors><title>All Sampling Methods Produce Outliers</title><categories>cs.CC</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a computable probability measure P over natural numbers or infinite
binary sequences, there is no method that can produce an arbitrarily large
sample such that all its members are typical of P. This paper also contains
upper bounds on the minimal encoding length of a predicate (over the set of
natural numbers) consistent with another predicate over a finite domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3874</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3874</id><created>2013-04-13</created><authors><author><keyname>Yang</keyname><forenames>Z.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Sparsity-Aware STAP Algorithms Using $L_1$-norm Regularization For Radar
  Systems</title><categories>cs.IT math.IT</categories><comments>6 figures</comments><journal-ref>IET Signal Processing 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes novel sparsity-aware space-time adaptive processing
(SA-STAP) algorithms with $l_1$-norm regularization for airborne phased-array
radar applications. The proposed SA-STAP algorithms suppose that a number of
samples of the full-rank STAP data cube are not meaningful for processing and
the optimal full-rank STAP filter weight vector is sparse, or nearly sparse.
The core idea of the proposed method is imposing a sparse regularization
($l_1$-norm type) to the minimum variance (MV) STAP cost function. Under some
reasonable assumptions, we firstly propose a $l_1$-based sample matrix
inversion (SMI) to compute the optimal filter weight vector. However, it is
impractical due to its matrix inversion, which requires a high computational
cost when in a large phased-array antenna. Then, we devise lower complexity
algorithms based on conjugate gradient (CG) techniques. A computational
complexity comparison with the existing algorithms and an analysis of the
proposed algorithms are conducted. Simulation results with both simulated and
the Mountain Top data demonstrate that fast
signal-to-interference-plus-noise-ratio (SINR) convergence and good performance
of the proposed algorithms are achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3875</identifier>
 <datestamp>2013-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3875</id><created>2013-04-13</created><updated>2013-08-27</updated><authors><author><keyname>Yu</keyname><forenames>Seung Min</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Game-theoretic Understanding of Price Dynamics in Mobile Communication
  Services</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the mobile communication services, users wish to subscribe to high quality
service with a low price level, which leads to competition between mobile
network operators (MNOs). The MNOs compete with each other by service prices
after deciding the extent of investment to improve quality of service (QoS).
Unfortunately, the theoretic backgrounds of price dynamics are not known to us,
and as a result, effective network planning and regulative actions are hard to
make in the competitive market. To explain this competition more detail, we
formulate and solve an optimization problem applying the two-stage Cournot and
Bertrand competition model. Consequently, we derive a price dynamics that the
MNOs increase and decrease their service prices periodically, which completely
explains the subsidy dynamics in the real world. Moving forward, to avoid this
instability and inefficiency, we suggest a simple regulation rule which leads
to a Pareto-optimal equilibrium point. Moreover, we suggest regulator's optimal
actions corresponding to user welfare and the regulator's revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3876</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3876</id><created>2013-04-14</created><updated>2015-05-02</updated><authors><author><keyname>Zheng</keyname><forenames>Shenggen</forenames></author><author><keyname>Qiu</keyname><forenames>Daowen</forenames></author><author><keyname>Gruska</keyname><forenames>Jozef</forenames></author></authors><title>Power of the interactive proof systems with verifiers modeled by
  semi-quantum two-way finite automata</title><categories>cs.CC cs.CR quant-ph</categories><comments>26 pages, 5 figures, some references have been added, and comments
  are welcome</comments><acm-class>F.1.1</acm-class><journal-ref>Information and Computation 241(2015) 197-214</journal-ref><doi>10.1016/j.ic.2015.02.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the power of AM for the case that verifiers are {\em
two-way finite automata with quantum and classical states} (2QCFA)--introduced
by Ambainis and Watrous in 2002--and the communications are classical. It is of
interest to consider AM with such &quot;semi-quantum&quot; verifiers because they use
only limited quantum resources. Our main result is that such Quantum
Arthur-Merlin proof systems (QAM(2QCFA)) with polynomial expected running time
are more powerful than in the case verifiers are two-way probabilistic finite
automata (AM(2PFA)) with polynomial expected running time. Moreover, we prove
that there is a language which can be recognized by an exponential expected
running time QAM(2QCFA), but can not be recognized by any AM(2PFA), and that
the NP-complete language $L_{knapsack}$ can also be recognized by a QAM(2QCFA)
working only on quantum pure states using unitary operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3877</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3877</id><created>2013-04-14</created><authors><author><keyname>Ning</keyname><forenames>Lipeng</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon T.</forenames></author><author><keyname>Tannenbaum</keyname><forenames>Allen</forenames></author><author><keyname>Boyd</keyname><forenames>Stephen P.</forenames></author></authors><title>Linear models based on noisy data and the Frisch scheme</title><categories>cs.SY math.OC math.ST stat.TH</categories><comments>26 pages</comments><msc-class>46N10, 62Jxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of identifying linear relations among variables based
on noisy measurements. This is, of course, a central question in problems
involving &quot;Big Data.&quot; Often a key assumption is that measurement errors in each
variable are independent. This precise formulation has its roots in the work of
Charles Spearman in 1904 and of Ragnar Frisch in the 1930's. Various topics
such as errors-in-variables, factor analysis, and instrumental variables, all
refer to alternative formulations of the problem of how to account for the
anticipated way that noise enters in the data. In the present paper we begin by
describing the basic theory and provide alternative modern proofs to some key
results. We then go on to consider certain generalizations of the theory as
well applying certain novel numerical techniques to the problem. A central role
is played by the Frisch-Kalman dictum which aims at a noise contribution that
allows a maximal set of simultaneous linear relations among the noise-free
variables --a rank minimization problem. In the years since Frisch's original
formulation, there have been several insights including trace minimization as a
convenient heuristic to replace rank minimization. We discuss convex
relaxations and certificates guaranteeing global optimality. A complementary
point of view to the Frisch-Kalman dictum is introduced in which models lead to
a min-max quadratic estimation error for the error-free variables. Points of
contact between the two formalisms are discussed and various alternative
regularization schemes are indicated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3879</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3879</id><created>2013-04-14</created><authors><author><keyname>Dufour-Lussier</keyname><forenames>Valmi</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Ber</keyname><forenames>Florence Le</forenames><affiliation>ICube</affiliation></author><author><keyname>Lieber</keyname><forenames>Jean</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Nauer</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Automatic case acquisition from texts for process-oriented case-based
  reasoning</title><categories>cs.AI cs.CL</categories><comments>Sous presse, publication pr\'evue en 2013</comments><proxy>ccsd</proxy><journal-ref>Information Systems (2012)</journal-ref><doi>10.1016/j.is.2012.11.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a method for the automatic acquisition of a rich case
representation from free text for process-oriented case-based reasoning. Case
engineering is among the most complicated and costly tasks in implementing a
case-based reasoning system. This is especially so for process-oriented
case-based reasoning, where more expressive case representations are generally
used and, in our opinion, actually required for satisfactory case adaptation.
In this context, the ability to acquire cases automatically from procedural
texts is a major step forward in order to reason on processes. We therefore
detail a methodology that makes case acquisition from processes described as
free text possible, with special attention given to assembly instruction texts.
This methodology extends the techniques we used to extract actions from cooking
recipes. We argue that techniques taken from natural language processing are
required for this task, and that they give satisfactory results. An evaluation
based on our implemented prototype extracting workflows from recipe texts is
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3886</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3886</id><created>2013-04-14</created><authors><author><keyname>Jung</keyname><forenames>Alexander</forenames></author><author><keyname>Schmutzhard</keyname><forenames>Sebastian</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author><author><keyname>Ben-Haim</keyname><forenames>Zvika</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Minimum Variance Estimation of a Sparse Vector within the Linear
  Gaussian Model: An RKHS Approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider minimum variance estimation within the sparse linear Gaussian
model (SLGM). A sparse vector is to be estimated from a linearly transformed
version embedded in Gaussian noise. Our analysis is based on the theory of
reproducing kernel Hilbert spaces (RKHS). After a characterization of the RKHS
associated with the SLGM, we derive novel lower bounds on the minimum variance
achievable by estimators with a prescribed bias function. This includes the
important case of unbiased estimation. The variance bounds are obtained via an
orthogonal projection of the prescribed mean function onto a subspace of the
RKHS associated with the SLGM. Furthermore, we specialize our bounds to
compressed sensing measurement matrices and express them in terms of the
restricted isometry and coherence parameters. For the special case of the SLGM
given by the sparse signal in noise model (SSNM), we derive closed-form
expressions of the minimum achievable variance (Barankin bound) and the
corresponding locally minimum variance estimator. We also analyze the effects
of exact and approximate sparsity information and show that the minimum
achievable variance for exact sparsity is not a limiting case of that for
approximate sparsity. Finally, we compare our bounds with the variance of three
well-known estimators, namely, the maximum-likelihood estimator, the
hard-thresholding estimator, and compressive reconstruction using the
orthogonal matching pursuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3892</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3892</id><created>2013-04-14</created><authors><author><keyname>Saeed</keyname><forenames>Muhammad Omer Bin</forenames></author><author><keyname>Sohail</keyname><forenames>Muhammad Saqib</forenames></author><author><keyname>Rizvi</keyname><forenames>Syed Zeeshan</forenames></author><author><keyname>Shoaib</keyname><forenames>Mobien</forenames></author><author><keyname>Sheikh</keyname><forenames>Asrar Ul Haq</forenames></author></authors><title>An accelerated CLPSO algorithm</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The particle swarm approach provides a low complexity solution to the
optimization problem among various existing heuristic algorithms. Recent
advances in the algorithm resulted in improved performance at the cost of
increased computational complexity, which is undesirable. Literature shows that
the particle swarm optimization algorithm based on comprehensive learning
provides the best complexity-performance trade-off. We show how to reduce the
complexity of this algorithm further, with a slight but acceptable performance
loss. This enhancement allows the application of the algorithm in time critical
applications, such as, real-time tracking, equalization etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3898</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3898</id><created>2013-04-14</created><updated>2013-10-23</updated><authors><author><keyname>Guan</keyname><forenames>Wanqiu</forenames></author><author><keyname>Gao</keyname><forenames>Haoyu</forenames></author><author><keyname>Yang</keyname><forenames>Mingmin</forenames></author><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Ma</keyname><forenames>Haixin</forenames></author><author><keyname>Qian</keyname><forenames>Weining</forenames></author><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author><author><keyname>Yang</keyname><forenames>Xiaoguang</forenames></author></authors><title>Analyzing user behavior of the micro-blogging website Sinaweibo during
  hot social events</title><categories>cs.SI physics.soc-ph</categories><comments>Physica A, Oct. 2013</comments><doi>10.1016/j.physa.2013.09.059</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread and resonance of users' opinions on SinaWeibo, the most popular
micro-blogging website in China, are tremendously influential, having
significantly affected the processes of many real-world hot social events. We
select 21 hot events that were widely discussed on SinaWeibo in 2011, and do
some statistical analyses. Our main findings are that (i) male users are more
likely to be involved, (ii) messages that contain pictures and those posted by
verified users are more likely to be reposted, while those with URLs are less
likely, (iii) gender factor, for most events, presents no significant
difference in reposting likelihood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3904</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3904</id><created>2013-04-14</created><authors><author><keyname>Mamechaoui</keyname><forenames>Sarra</forenames></author><author><keyname>Didi</keyname><forenames>Fedoua</forenames></author><author><keyname>Pujolle</keyname><forenames>Guy</forenames></author></authors><title>A survey on energy efficiency for wireless mesh network</title><categories>cs.NI</categories><comments>International Journal of Computer Networks &amp; Communications (IJCNC)
  Vol.5, No.2, March 2013</comments><doi>10.5121/ijcnc.2013.5209</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reducing CO2 emissions is an important global environmental issue. Over the
recent years, wireless and mobile communications have increasingly become
popular with consumers. An increasingly popular type of wireless access is the
so-called Wireless Mesh Networks (WMNs) that provide wireless connectivity
through much cheaper and more flexible backhaul infrastructure compared with
wired solutions. Wireless Mesh Network (WMN) is an emerging new technology
which is being adopted as the wireless internetworking solution for the near
future. Due to increased energy consumption in the information and
communication technology (ICT) industries, and its consequent environmental
effects, energy efficiency has become a key factor to evaluate the performance
of a communication network. This paper mainly focuses on classification layer
of the largest existing approaches dedicated to energy conservation. It is also
discussing the most interesting works on energy saving in WMNs networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3911</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3911</id><created>2013-04-14</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Mehbodniya</keyname><forenames>Abolfazl</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Least Mean Square/Fourth Algorithm with Application to Sparse Channel
  Estimation</title><categories>cs.IT math.IT</categories><comments>5pages, 9figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Broadband signal transmission over frequency-selective fading channel often
requires accurate channel state information at receiver. One of the most
attracting adaptive channel estimation methods is least mean square (LMS)
algorithm. However, LMS-based method is often degraded by random scaling of
input training signal. To improve the estimation performance, in this paper we
apply the standard least mean square/fourth (LMS/F) algorithm to adaptive
channel estimation (ACE). Since the broadband channel is often described by
sparse channel model, such sparsity could be exploited as prior information.
First, we propose an adaptive sparse channel estimation (ASCE) method using
zero-attracting LMS/F (ZA-LMS/F) algorithm. To exploit the sparsity
effectively, an improved channel estimation method is also proposed, using
reweighted zero-attracting LMS/F (RZA-LMS/F) algorithm. We explain the reason
why sparse LMS/F algorithms using l_1-norm sparse constraint function can
improve the estimation performance by virtual of geometrical interpretation. In
addition, for different channel sparsity, we propose a Monte Carlo method to
select a regularization parameter for RA-LMS/F and RZA-LMS/F to achieve
approximate optimal estimation performance. Finally, simulation results show
that the proposed ASCE methods achieve better estimation performance than the
conventional one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3912</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3912</id><created>2013-04-14</created><authors><author><keyname>Bhumula</keyname><forenames>Mahendra Reddy</forenames></author></authors><title>Comparative Study and Analysis of Variability Tools</title><categories>cs.SE</categories><comments>35 pages; An earlier draft of the dissertation submitted in partial
  fulfillment of the requirements of the degree of Master of Science (MSc) at
  UEL, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dissertation provides a comparative analysis of a number of variability
tools currently in use. It serves as a catalogue for practitioners interested
in the topic. We compare a range of modelling, configuring, and management
tools for product line engineering. The tools surveyed are compared against the
following criteria: functional, non-functional, governance issues and Technical
aspects. The outcome of the analysis is provided in tabular format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3915</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3915</id><created>2013-04-14</created><authors><author><keyname>Hassner</keyname><forenames>Tal</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author></authors><title>Single View Depth Estimation from Examples</title><categories>cs.CV</categories><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a non-parametric, &quot;example-based&quot; method for estimating the depth
of an object, viewed in a single photo. Our method consults a database of
example 3D geometries, searching for those which look similar to the object in
the photo. The known depths of the selected database objects act as shape
priors which constrain the process of estimating the object's depth. We show
how this process can be performed by optimizing a well defined target
likelihood function, via a hard-EM procedure. We address the problem of
representing the (possibly infinite) variability of viewing conditions with a
finite (and often very small) example set, by proposing an on-the-fly example
update scheme. We further demonstrate the importance of non-stationarity in
avoiding misleading examples when estimating structured shapes. We evaluate our
method and present both qualitative as well as quantitative results for
challenging object classes. Finally, we show how this same technique may be
readily applied to a number of related problems. These include the novel task
of estimating the occluded depth of an object's backside and the task of
tailoring custom fitting image-maps for input depths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3919</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3919</id><created>2013-04-14</created><authors><author><keyname>El-Mistikawy</keyname><forenames>Tarek M. A.</forenames></author></authors><title>Modular Analysis of Almost Block Diagonal Systems of Equations</title><categories>math.NA cs.NA</categories><comments>The article is in 40 pages; containing 1 figure, 1 table, 13
  references, and 3 appendices</comments><msc-class>65F05, 65F50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost block diagonal linear systems of equations can be exemplified by two
modules. This makes it possible to construct all sequential forms of band
and/or block elimination methods, six old and fourteen new. It allows easy
assessment of the methods on the basis of their operation counts, storage
needs, and admissibility of partial pivoting. It unveils a robust partial
pivoting strategy- local pivoting. Extension of modular analysis to bordered
systems is also included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3924</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3924</id><created>2013-04-14</created><authors><author><keyname>Garc&#xed;a</keyname><forenames>J. A.</forenames></author><author><keyname>Rodriguez-S&#xe1;nchez</keyname><forenames>Rosa</forenames></author><author><keyname>Fdez-Valdivia</keyname><forenames>Joaqu&#xed;n</forenames></author><author><keyname>Robinson-Garcia</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author></authors><title>Best-in-class and Strategic Benchmarking of Scientific Subject
  Categories of Web of Science in 2010</title><categories>cs.DL</categories><journal-ref>Garc\'ia, J.A. et al. Best-in-class and Strategic Benchmarking of
  Scientific Subject Categories of Web of Science in 2010. Scientometrics.
  Online first. doi:10.1007/s11192-013-1000-1</journal-ref><doi>10.1007/s11192-013-1000-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we show a novel technique for comparing subject categories, where the
prestige of academic journals in each category is represented statistically by
an impact-factor histogram. For each subject category we compute the
probability of occurrence of scholarly journals with impact factor in different
intervals. Here impact factor is measured with Thomson Reuters Impact Factor,
Eigenfactor Score, and Immediacy Index. Assuming the probabilities associated
with a pair of subject categories our objective is to measure the degree of
dissimilarity between them. To do so, we use an axiomatic characterization for
predicting dissimilarity between subject categories. The scientific subject
categories of Web of Science in 2010 were used to test the proposed approach
for benchmarking Cell Biology and Computer Science Information Systems with the
rest as two case studies. The former is best-in-class benchmarking that
involves studying the leading competitor category; the latter is strategic
benchmarking that involves observing how other scientific subject categories
compete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3931</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3931</id><created>2013-04-14</created><authors><author><keyname>Ning</keyname><forenames>Lipeng</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon T.</forenames></author><author><keyname>Tannenbaum</keyname><forenames>Allen</forenames></author></authors><title>Matrix-valued Monge-Kantorovich Optimal Mass Transport</title><categories>cs.SY math.DS math.FA math.OC</categories><comments>11 pages</comments><msc-class>37M10, 47N10, 49Q10, 46L54, 90C08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate an optimal transport problem for matrix-valued density
functions. This is pertinent in the spectral analysis of multivariable
time-series. The &quot;mass&quot; represents energy at various frequencies whereas, in
addition to a usual transportation cost across frequencies, a cost of rotation
is also taken into account. We show that it is natural to seek the
transportation plan in the tensor product of the spaces for the two
matrix-valued marginals. In contrast to the classical Monge-Kantorovich
setting, the transportation plan is no longer supported on a thin zero-measure
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3935</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3935</id><created>2013-04-14</created><updated>2013-05-16</updated><authors><author><keyname>Rosenbaum</keyname><forenames>David J.</forenames></author></authors><title>Bidirectional Collision Detection and Faster Deterministic Isomorphism
  Testing</title><categories>cs.DS cs.CC</categories><comments>18 pages. v1 shows the results. v2 makes minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we introduce bidirectional collision detection --- a new
algorithmic tool that applies to the collision problems that arise in many
isomorphism problems. For the group isomorphism problem, we show that
bidirectional collision detection yields a deterministic n^((1 / 2) log n +
O(1)) time algorithm whereas previously the n^(log n + O(1))
generator-enumeration algorithm was the best result for several decades. For
the hard special case of solvable groups, we combine bidirectional collision
detection with methods from the author's previous work to obtain a
deterministic square-root speedup over the best previous algorithm. We also
show a deterministic square-root speedup over the best previous algorithm for
testing isomorphism of rings. We can even apply bidirectional collision
detection to the graph isomorphism problem to obtain a deterministic T^(1 /
sqrt(2)) speedup over the best previous deterministic algorithm. Although the
space requirements for our algorithms are greater than those for previous
deterministic isomorphism tests, we show time-space tradeoffs that interpolate
between the resource requirements of our algorithms and previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3940</identifier>
 <datestamp>2013-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3940</id><created>2013-04-14</created><updated>2013-05-09</updated><authors><author><keyname>Lieto</keyname><forenames>Antonio</forenames></author><author><keyname>Vernero</keyname><forenames>Fabiana</forenames></author></authors><title>Unveiling the link between logical fallacies and web persuasion</title><categories>cs.HC cs.AI</categories><comments>6 pages, 3 figures, in proceedings of the WebSci'13 Conference,
  Paris, 2013</comments><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade Human-Computer Interaction (HCI) has started to focus
attention on forms of persuasive interaction where computer technologies have
the goal of changing users behavior and attitudes according to a predefined
direction. In this work, we hypothesize a strong connection between logical
fallacies (forms of reasoning which are logically invalid but cognitively
effective) and some common persuasion strategies adopted within web
technologies. With the aim of empirically evaluating our hypothesis, we carried
out a pilot study on a sample of 150 e-commerce websites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3944</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3944</id><created>2013-04-14</created><authors><author><keyname>Sobe</keyname><forenames>Anita</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>Smart Microgrids: Overview and Outlook</title><categories>cs.ET cs.CY cs.SY</categories><comments>presented at the GI Informatik 2012, Braunschweig Germany, Smart Grid
  Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of changing our energy system from a hierarchical design into a set
of nearly independent microgrids becomes feasible with the availability of
small renewable energy generators. The smart microgrid concept comes with
several challenges in research and engineering targeting load balancing,
pricing, consumer integration and home automation. In this paper we first
provide an overview on these challenges and present approaches that target the
problems identified. While there exist promising algorithms for the particular
field, we see a missing integration which specifically targets smart
microgrids. Therefore, we propose an architecture that integrates the presented
approaches and defines interfaces between the identified components such as
generators, storage, smart and \dq{dumb} devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3946</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3946</id><created>2013-04-14</created><updated>2013-04-16</updated><authors><author><keyname>Chandramouli</keyname><forenames>Shyam S</forenames></author><author><keyname>Sethuraman</keyname><forenames>Jay</forenames></author></authors><title>Strategyproof and Consistent Rules for Bipartite Flow Problems</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the study of Bochet et al. and Moulin and Sethuraman on fair
allocation in bipartite networks. In these models, there is a moneyless market,
in which a non-storable, homogeneous commodity is reallocated between agents
with single-peaked preferences. Agents are either suppliers or demanders. While
the egalitarian rule of Bochet et al. satisfies pareto optimality, no envy and
strategyproof, it is not consistent. On the other hand, the work of Moulin and
Sethuraman is related to consistent allocations and rules that are extensions
of the uniform rule. We bridge the two streams of work by introducing the edge
fair mechanism which is both consistent and groupstrategyproof. On the way, we
explore the &quot;price of consistency&quot; i.e. how the notion of consistency is
fundamentally incompatible with certain notions of fairness like Lorenz
Dominance and No-Envy. The current work also introduces the idea of strong
invariance as desideratum for groupstrategyproofness and generalizes the proof
of Chandramouli and Sethuraman to a more broader class of mechanisms. Finally,
we conclude with the study of the edge fair mechanism in a transshipment model
where the strategic agents are on the links connecting different supply/demand
locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3949</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3949</id><created>2013-04-14</created><updated>2013-05-12</updated><authors><author><keyname>Pfrommer</keyname><forenames>Julius</forenames></author><author><keyname>Warrington</keyname><forenames>Joseph</forenames></author><author><keyname>Schildbach</keyname><forenames>Georg</forenames></author><author><keyname>Morari</keyname><forenames>Manfred</forenames></author></authors><title>Dynamic vehicle redistribution and online price incentives in shared
  mobility systems</title><categories>cs.SY</categories><doi>10.1109/TITS.2014.2303986</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a combination of intelligent repositioning decisions and
dynamic pricing for the improved operation of shared mobility systems. The
approach is applied to London's Barclays Cycle Hire scheme, which the authors
have simulated based on historical data. Using model-based predictive control
principles, dynamically varying rewards are computed and offered to customers
carrying out journeys. The aim is to encourage them to park bicycles at nearby
under-used stations, thereby reducing the expected cost of repositioning them
using dedicated staff. In parallel, the routes that repositioning staff should
take are periodically recomputed using a model-based heuristic. It is shown
that a trade-off between reward payouts to customers and the cost of hiring
repositioning staff could be made, in order to minimize operating costs for a
given desired service level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3962</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3962</id><created>2013-04-14</created><updated>2013-08-01</updated><authors><author><keyname>Pantazis</keyname><forenames>Yannis</forenames></author><author><keyname>Katsoulakis</keyname><forenames>Markos A.</forenames></author><author><keyname>Vlachos</keyname><forenames>Dionisios G.</forenames></author></authors><title>Parametric Sensitivity Analysis for Biochemical Reaction Networks based
  on Pathwise Information Theory</title><categories>cs.IT math.IT q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic modeling and simulation provide powerful predictive methods for
the intrinsic understanding of fundamental mechanisms in complex biochemical
networks. Typically, such mathematical models involve networks of coupled jump
stochastic processes with a large number of parameters that need to be suitably
calibrated against experimental data. In this direction, the parameter
sensitivity analysis of reaction networks is an essential mathematical and
computational tool, yielding information regarding the robustness and the
identifiability of model parameters. However, existing sensitivity analysis
approaches such as variants of the finite difference method can have an
overwhelming computational cost in models with a high-dimensional parameter
space. We develop a sensitivity analysis methodology suitable for complex
stochastic reaction networks with a large number of parameters. The proposed
approach is based on Information Theory methods and relies on the
quantification of information loss due to parameter perturbations between
time-series distributions. For this reason, we need to work on path-space,
i.e., the set consisting of all stochastic trajectories, hence the proposed
approach is referred to as &quot;pathwise&quot;. The pathwise sensitivity analysis method
is realized by employing the rigorously-derived Relative Entropy Rate (RER),
which is directly computable from the propensity functions. A key aspect of the
method is that an associated pathwise Fisher Information Matrix (FIM) is
defined, which in turn constitutes a gradient-free approach to quantifying
parameter sensitivities. The structure of the FIM turns out to be
block-diagonal, revealing hidden parameter dependencies and sensitivities in
reaction networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3963</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3963</id><created>2013-04-14</created><updated>2014-04-25</updated><authors><author><keyname>Akdeniz</keyname><forenames>Mustafa Riza</forenames></author><author><keyname>Liu</keyname><forenames>Yuanpeng</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Millimeter Wave Picocellular System Evaluation for Urban Deployments</title><categories>cs.NI</categories><comments>This paper is replaced by arXiv:1312.4921</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the severe spectrum shortage in conventional cellular bands, millimeter
wave (mmW) frequencies between 30 and 300 GHz have been attracting growing
attention as a possible candidate for next-generation micro- and picocellular
wireless networks. The mmW bands offer orders of magnitude greater spectrum
than current cellular allocations and enable very high-dimensional antenna
arrays for further gains via spatial multiplexing. However, the propagation of
mmW signals in outdoor non line-of-sight (NLOS) links remains challenging and
the feasibility of wide-area mmW cellular networks is far from clear. This
paper uses recent real-world measurements at 28 GHz in New York City to provide
a realistic assessment of mmW picocellular networks in a dense urban
deployment. It is found that, even under conservative propagation assumptions,
mmW systems with cell radii of 100m can offer an order of magnitude increase in
capacity over current state-of-the-art 4G cellular networks with similar cell
density. However, it is also shown that such mmW networks may operate in a
largely power-limited regime where the full spatial and bandwidth degrees of
freedom are not fully utilized. This power-limited regime contrasts
significantly with current bandwidth-limited cellular systems, requiring
alternate technologies for mmW systems that may unlock further gains that mmW
frequency bands offer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3972</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3972</id><created>2013-04-14</created><authors><author><keyname>Cheng</keyname><forenames>Long</forenames></author><author><keyname>Hou</keyname><forenames>Zeng-Guang</forenames></author><author><keyname>Tan</keyname><forenames>Min</forenames></author></authors><title>Reaching a Consensus in Networks of High-Order Integral Agents under
  Switching Directed Topology</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consensus problem of high-order integral multi-agent systems under switching
directed topology is considered in this study. Depending on whether the agent's
full state is available or not, two distributed protocols are proposed to
ensure that states of all agents can be convergent to a same stationary value.
In the proposed protocols, the gain vector associated with the agent's
(estimated) state and the gain vector associated with the relative (estimated)
states between agents are designed in a sophisticated way. By this particular
design, the high-order integral multi-agent system can be transformed into a
first-order integral multi-agent system. And the convergence of the transformed
first-order integral agent's state indicates the convergence of the original
high-order integral agent's state if and only if all roots of the polynomial,
whose coefficients are the entries of the gain vector associated with the
relative (estimated) states between agents, are in the open left-half complex
plane. Therefore, many analysis techniques in the first-order integral
multi-agent system can be directly borrowed to solve the problems in the
high-order integral multi-agent system. Due to this property, it is proved that
to reach a consensus, the switching directed topology of multi-agent system is
only required to be &quot;uniformly jointly quasi-strongly connected&quot;, which seems
the mildest connectivity condition in the literature. In addition, the
consensus problem of discrete-time high-order integral multi-agent systems is
studied. The corresponding consensus protocol and performance analysis are
presented. Finally, three simulation examples are provided to show the
effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3977</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3977</id><created>2013-04-15</created><authors><author><keyname>Kim</keyname><forenames>Changkyu</forenames></author><author><keyname>Ford</keyname><forenames>Russell</forenames></author><author><keyname>Qi</keyname><forenames>Yanjia</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author></authors><title>Joint Interference and User Association Optimization in Cellular
  Wireless Networks</title><categories>cs.NI</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cellular wireless networks, user association refers to the problem of
assigning mobile users to base station cells -- a critical, but challenging,
problem in many emerging small cell and heterogeneous networks. This paper
considers a general class of utility maximization problems for joint
optimization of mobile user associations and bandwidth and power allocations.
The formulation can incorporate a large class of network topologies,
interference models, SNR-to-rate mappings and network constraints. In addition,
the model can applied in carrier aggregation scenarios where mobiles can be
served by multiple cells simultaneously. While the problem is non-convex, our
main contribution shows that the optimization admits a separable dual
decomposition. This property enables fast computation of upper bounds on the
utility as well as an efficient, distributed implementation for approximate
local optimization via augmented Lagrangian techniques. Simulations are
presented in heterogeneous networks with mixtures of macro and picocells. We
demonstrate significant value of the proposed methods in scenarios with
variable backhaul capacity in the femtocell links and in cases where the user
density is sufficiently low that lightly-used cells can reduce power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3978</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3978</id><created>2013-04-15</created><authors><author><keyname>Shah</keyname><forenames>Manan D.</forenames></author><author><keyname>Prajapati</keyname><forenames>Harshad B.</forenames></author></authors><title>Reallocation and Allocation of Virtual Machines in Cloud Computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has given the new face to the distributed field. Two main
issues are discussed in this paper, (I) the process of finding the efficient
virtual machine by using the concept of load balancing algorithm. (II)
Reallocation of the Virtual Machines i.e. migration of the Virtual Machines
when cloud provider is not available with the required Virtual Machines. We
have discussed about the different load balancing algorithms which are used for
deciding the efficient Virtual Machine for the allocation to the client on
demand. While in the second issue is concern we have discuss about different
modules available for the migration of Virtual Machines from one source machine
to the other target machine. At last discussion about the different simulators
available for the cloud are carried out in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3980</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3980</id><created>2013-04-15</created><authors><author><keyname>vegda</keyname><forenames>Deepak. c.</forenames></author><author><keyname>Prajapati</keyname><forenames>Harshad. B.</forenames></author></authors><title>Scheduling of Dependent Tasks Application using Random Search Technique</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since beginning of Grid computing, scheduling of dependent tasks application
has attracted attention of researchers due to NP-Complete nature of the
problem. In Grid environment, scheduling is deciding about assignment of tasks
to available resources. Scheduling in Grid is challenging when the tasks have
dependencies and resources are heterogeneous. The main objective in scheduling
of dependent tasks is minimizing make-span. Due to NP-complete nature of
scheduling problem, exact solutions cannot generate schedule efficiently.
Therefore, researchers apply heuristic or random search techniques to get
optimal or near to optimal solution of such problems. In this paper, we show
how Genetic Algorithm can be used to solve dependent task scheduling problem.
We describe how initial population can be generated using random assignment and
height based approaches. We also present design of crossover and mutation
operators to enable scheduling of dependent tasks application without violating
dependency constraints. For implementation of GA based scheduling, we explore
and analyze SimGrid and GridSim simulation toolkits. From results, we found
that SimGrid is suitable, as it has support of SimDag API for DAG applications.
We found that GA based approach can generate schedule for dependent tasks
application in reasonable time while trying to minimize make-span.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3992</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3992</id><created>2013-04-15</created><authors><author><keyname>Tejaswi</keyname><forenames>K. Phani</forenames></author><author><keyname>Rao</keyname><forenames>D. Shanmukha</forenames></author><author><keyname>Nair</keyname><forenames>Thara</forenames></author><author><keyname>Prasad</keyname><forenames>A. V. V.</forenames></author></authors><title>GPU Acclerated Automated Feature Extraction from Satellite Images</title><categories>cs.DC cs.CV</categories><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.4, No.2, March 2013</journal-ref><doi>10.5121/ijdps.2013.4201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of large volumes of remote sensing data insists on higher
degree of automation in feature extraction, making it a need of the hour.The
huge quantum of data that needs to be processed entails accelerated processing
to be enabled.GPUs, which were originally designed to provide efficient
visualization, are being massively employed for computation intensive parallel
processing environments. Image processing in general and hence automated
feature extraction, is highly computation intensive, where performance
improvements have a direct impact on societal needs. In this context, an
algorithm has been formulated for automated feature extraction from a
panchromatic or multispectral image based on image processing techniques. Two
Laplacian of Guassian (LoG) masks were applied on the image individually
followed by detection of zero crossing points and extracting the pixels based
on their standard deviation with the surrounding pixels. The two extracted
images with different LoG masks were combined together which resulted in an
image with the extracted features and edges. Finally the user is at liberty to
apply the image smoothing step depending on the noise content in the extracted
image. The image is passed through a hybrid median filter to remove the salt
and pepper noise from the image. This paper discusses the aforesaid algorithm
for automated feature extraction, necessity of deployment of GPUs for the same;
system-level challenges and quantifies the benefits of integrating GPUs in such
environment. The results demonstrate that substantial enhancement in
performance margin can be achieved with the best utilization of GPU resources
and an efficient parallelization strategy. Performance results in comparison
with the conventional computing scenario have provided a speedup of 20x, on
realization of this parallelizing strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3994</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3994</id><created>2013-04-15</created><updated>2013-06-14</updated><authors><author><keyname>Jung</keyname><forenames>Sang Yeob</forenames></author><author><keyname>Lee</keyname><forenames>Hyun-kwan</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Worst-case User Analysis in Poisson Voronoi Cells</title><categories>cs.IT math.IT</categories><comments>Accepted, IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we focus on the performance of a worst-case mobile user (MU)
in the downlink cellular network. We derive the coverage probability and the
spectral efficiency of the worst-case MU using stochastic geometry. Through
analytical and numerical results, we draw out interesting insights that the
coverage probability and the spectral efficiency of the worst-case MU decrease
down to 23% and 19% of those of a typical MU, respectively. By applying a
coordinated scheduling (CS) scheme, we also investigate how much the
performance of the worst-case MU is improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3996</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3996</id><created>2013-04-15</created><authors><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Bent</keyname><forenames>Russell</forenames></author><author><keyname>Bono</keyname><forenames>James</forenames></author><author><keyname>Lee</keyname><forenames>Ritchie</forenames></author><author><keyname>Tracey</keyname><forenames>Brendan</forenames></author><author><keyname>Wolpert</keyname><forenames>David</forenames></author><author><keyname>Xie</keyname><forenames>Dongping</forenames></author><author><keyname>Yildiz</keyname><forenames>Yildiray</forenames></author></authors><title>Cyber-Physical Security: A Game Theory Model of Humans Interacting over
  Control Systems</title><categories>cs.GT cs.CR cs.CY cs.SY</categories><comments>8 pages, 7 figures, IEEE Transactions on Smart Grids pending</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Recent years have seen increased interest in the design and deployment of
smart grid devices and control algorithms. Each of these smart communicating
devices represents a potential access point for an intruder spurring research
into intruder prevention and detection. However, no security measures are
complete, and intruding attackers will compromise smart grid devices leading to
the attacker and the system operator interacting via the grid and its control
systems. The outcome of these machine-mediated human-human interactions will
depend on the design of the physical and control systems mediating the
interactions. If these outcomes can be predicted via simulation, they can be
used as a tool for designing attack-resilient grids and control systems.
However, accurate predictions require good models of not just the physical and
control systems, but also of the human decision making. In this manuscript, we
present an approach to develop such tools, i.e. models of the decisions of the
cyber-physical intruder who is attacking the systems and the system operator
who is defending it, and demonstrate its usefulness for design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3997</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3997</id><created>2013-04-15</created><updated>2013-06-25</updated><authors><author><keyname>Cong</keyname><forenames>Shuang</forenames></author><author><keyname>Meng</keyname><forenames>Fangfang</forenames></author></authors><title>A Survey of Quantum Lyapunov Control Methods</title><categories>math-ph cs.SY math.MP</categories><comments>14</comments><journal-ref>The Scientific World Journal,Volume 2013, Article ID 967529</journal-ref><doi>10.1155/2013/967529</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The condition of a quantum Lyapunov-based control which can be well used in a
closed quantum system is that the method can make the system convergent but not
just stable. In the convergence study of the quantum Lyapunov control, two
situations are classified: non-degenerate cases and degenerate cases. In this
paper, for these two situations, respectively, the target state is divided into
four categories: eigenstate, the mixed state which commutes with the internal
Hamiltonian, the superposition state, and the mixed state which does not
commute with the internal Hamiltonian state. For these four categories, the
quantum Lyapunov control methods for the closed quantum systems are summarized
and analyzed. Especially, the convergence of the control system to the
different target states is reviewed, and how to make the convergence conditions
be satisfied is summarized and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3998</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3998</id><created>2013-04-15</created><authors><author><keyname>Hanif</keyname><forenames>Muhammad Fainan</forenames></author><author><keyname>Tran</keyname><forenames>Le-Nam</forenames></author><author><keyname>T&#xf6;lli</keyname><forenames>Antti</forenames></author><author><keyname>Juntti</keyname><forenames>Markku</forenames></author></authors><title>Computationally Efficient Robust Beamforming for SINR Balancing in
  Multicell Downlink</title><categories>cs.IT math.IT</categories><comments>26 pages, 5 figures. Submitted for possible publication</comments><journal-ref>IEEE Transactions on Communications, vol.62, no.6, pp.1908,1920,
  June 2014</journal-ref><doi>10.1109/TCOMM.2014.2320913</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of downlink beamformer design for
signal-to-interference-plus-noise ratio (SINR) balancing in a multiuser
multicell environment with imperfectly estimated channels at base stations
(BSs). We first present a semidefinite program (SDP) based approximate solution
to the problem. Then, as our main contribution, by exploiting some properties
of the robust counterpart of the optimization problem, we arrive at a
second-order cone program (SOCP) based approximation of the balancing problem.
The advantages of the proposed SOCP-based design are twofold. First, it greatly
reduces the computational complexity compared to the SDP-based method. Second,
it applies to a wide range of uncertainty models. As a case study, we
investigate the performance of proposed formulations when the base station is
equipped with a massive antenna array. Numerical experiments are carried out to
confirm that the proposed robust designs achieve favorable results in scenarios
of practical interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.3999</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.3999</id><created>2013-04-15</created><authors><author><keyname>Geist</keyname><forenames>Matthieu</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Scherrer</keyname><forenames>Bruno</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Off-policy Learning with Eligibility Traces: A Survey</title><categories>cs.AI cs.RO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of Markov Decision Processes, off-policy learning, that is
the problem of learning a linear approximation of the value function of some
fixed policy from one trajectory possibly generated by some other policy. We
briefly review on-policy learning algorithms of the literature (gradient-based
and least-squares-based), adopting a unified algorithmic view. Then, we
highlight a systematic approach for adapting them to off-policy learning with
eligibility traces. This leads to some known algorithms - off-policy
LSTD(\lambda), LSPE(\lambda), TD(\lambda), TDC/GQ(\lambda) - and suggests new
extensions - off-policy FPKF(\lambda), BRM(\lambda), gBRM(\lambda),
GTD2(\lambda). We describe a comprehensive algorithmic derivation of all
algorithms in a recursive and memory-efficent form, discuss their known
convergence properties and illustrate their relative empirical behavior on
Garnet problems. Our experiments suggest that the most standard algorithms on
and off-policy LSTD(\lambda)/LSPE(\lambda) - and TD(\lambda) if the feature
space dimension is too large for a least-squares approach - perform the best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4002</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4002</id><created>2013-04-15</created><authors><author><keyname>Thakker</keyname><forenames>Harsh N</forenames></author><author><keyname>Saha</keyname><forenames>Mayank</forenames></author><author><keyname>Das</keyname><forenames>Manik Lal</forenames></author></authors><title>Reputation Algebra for Cloud-based Anonymous Data Storage Systems</title><categories>cs.DC cs.CR</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Given a cloud-based anonymous data storage system, there are two ways for
managing the nodes involved in file transfers. One of them is using reputations
and the other uses a micropayment system. In reputation-based approach, each
node has a reputation associated with it, which is used as a currency or
feedback collection for file exchange operations. There have been several
attempts over the years to develop a strong and efficient reputation system
that provides credibility, fairness, and accountability. One such attempt was
the Free Haven Project that provides a strong foundation for cloud-based
anonymous data storage systems. The work proposed in this paper is motivated by
the Free Haven Project aimed at developing a reputation system that facilitates
dynamic operations such as adding servers, removing servers and changing role
of authorities. The proposed system also provides algorithm for scoring and
maintaining reputations of the servers in order to achieve credibility,
accountability and fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4003</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4003</id><created>2013-04-15</created><authors><author><keyname>Heydari</keyname><forenames>Seyed Javad</forenames></author><author><keyname>Naeiny</keyname><forenames>Mahmoud Ferdosizade</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>Iterative Detection with Soft Decision in Spectrally Efficient FDM
  Systems</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Spectrally Efficient Frequency Division Multiplexing systems the input
data stream is divided into several adjacent subchannels where the distance of
the subchannels is less than that of Orthogonal Frequency Division
Multiplexing(OFDM)systems. Since the subcarriers are not orthogonal in SEFDM
systems, they lead to interference at the receiver side. In this paper, an
iterative method is proposed for interference compensation for SEFDM systems.
In this method a soft mapping technique is used after each iteration block to
improve its performance. The performance of the proposed method is comparable
to that of Sphere Detection(SD)which is a nearly optimal detection method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4028</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4028</id><created>2013-04-15</created><authors><author><keyname>Nafi</keyname><forenames>Kawser Wazed</forenames></author><author><keyname>Kar</keyname><forenames>Tonny Shekha</forenames></author><author><keyname>Hossain</keyname><forenames>Amjad</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A</forenames></author></authors><title>A Fuzzy Logic Based Certain Trust Model for E-Commerce</title><categories>cs.AI cs.CR</categories><comments>Accepted for the Procs. of the IEEE 2013 International Conference on
  Informatics, Electronics and Vision (ICIEV 2013), pp.XX-XX, Dhaka,
  Bangladesh, May 17-18, (2013)</comments><journal-ref>Procs. of the IEEE 2013 International Conference on Informatics,
  Electronics and Vision (ICIEV 2013), pp.XX-XX, Dhaka, Bangladesh, May 17-18,
  (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trustworthiness especially for service oriented system is very important
topic now a day in IT field of the whole world. There are many successful
E-commerce organizations presently run in the whole world, but E-commerce has
not reached its full potential. The main reason behind this is lack of Trust of
people in e-commerce. Again, proper models are still absent for calculating
trust of different e-commerce organizations. Most of the present trust models
are subjective and have failed to account vagueness and ambiguity of different
domain. In this paper we have proposed a new fuzzy logic based Certain Trust
model which considers these ambiguity and vagueness of different domain. Fuzzy
Based Certain Trust Model depends on some certain values given by experts and
developers. can be applied in a system like cloud computing, internet, website,
e-commerce, etc. to ensure trustworthiness of these platforms. In this paper we
show, although fuzzy works with uncertainties, proposed model works with some
certain values. Some experimental results and validation of the model with
linguistics terms are shown at the last part of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4041</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4041</id><created>2013-04-15</created><authors><author><keyname>Irshad</keyname><forenames>H.</forenames></author><author><keyname>Gouaillard</keyname><forenames>A.</forenames></author><author><keyname>Roux</keyname><forenames>L.</forenames></author><author><keyname>Racoceanu</keyname><forenames>D.</forenames></author></authors><title>Multispectral Spatial Characterization: Application to Mitosis Detection
  in Breast Cancer Histopathology</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate detection of mitosis plays a critical role in breast cancer
histopathology. Manual detection and counting of mitosis is tedious and subject
to considerable inter- and intra-reader variations. Multispectral imaging is a
recent medical imaging technology, proven successful in increasing the
segmentation accuracy in other fields. This study aims at improving the
accuracy of mitosis detection by developing a specific solution using
multispectral and multifocal imaging of breast cancer histopathological data.
We propose to enable clinical routine-compliant quality of mitosis
discrimination from other objects. The proposed framework includes
comprehensive analysis of spectral bands and z-stack focus planes, detection of
expected mitotic regions (candidates) in selected focus planes and spectral
bands, computation of multispectral spatial features for each candidate,
selection of multispectral spatial features and a study of different
state-of-the-art classification methods for candidates classification as
mitotic or non mitotic figures. This framework has been evaluated on MITOS
multispectral medical dataset and achieved 60% detection rate and 57%
F-Measure. Our results indicate that multispectral spatial features have more
information for mitosis classification in comparison with white spectral band
features, being therefore a very promising exploration area to improve the
quality of the diagnosis assistance in histopathology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4045</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4045</id><created>2013-04-15</created><authors><author><keyname>Ghadirli</keyname><forenames>Hossein Movafegh</forenames></author><author><keyname>Rastgarpour</keyname><forenames>Maryam</forenames></author></authors><title>A Model for an Intelligent and Adaptive Tutor based on Web by Jackson's
  Learning Styles Profiler and Expert Systems</title><categories>cs.CY</categories><comments>5 pages, 3 figures, Proceedings of the International MultiConference
  of Engineers and Computer Scientists, Volume 1</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Todays, Intelligent and web-based E-Learning is one of the important area in
E-Learning. This paper integrates an intelligent and web-based E-Learning with
expert system technology to be able to model the learning styles of the
learners using Jackson's model. It is intelligent because it can interact with
the learners and offer them some subjects in Pedagogy view. Learning process of
this system is in the following. First it determines learner's individual
characteristics and learning styles based on a questionnaire in Jackson's
learning styles profiler. Learning styles profiler is a modern measure of
individual differences in learning style. Then learner's model is obtained and
an Expert system simulator plans a &quot;pre-test&quot; and rates him. The concept would
be presented if the learner scores enough. Subsequently, the system evaluates
him by a &quot;post-test&quot;. Finally the learner's model would be updated by the
modeler based on try-and-error. The proposed system can be available Every Time
and Every Where (ETEW) through the web. It improves the learning performance
and has some important advantages such as high speed, simplicity of learning,
low cost and be available ETEW.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4047</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4047</id><created>2013-04-15</created><authors><author><keyname>Ghadirli</keyname><forenames>Hossein Movafegh</forenames></author><author><keyname>Rastgarpour</keyname><forenames>Maryam</forenames></author></authors><title>A Paradigm for the Application of Cloud Computing in Mobile Intelligent
  Tutoring Systems</title><categories>cs.CY</categories><comments>11 pages, 4 figures, International Journal of Software Engineering &amp;
  Applications (IJSEA)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Nowadays, with the rapid growth of cloud computing, many industries are going
to move their computing activities to clouds. Researchers of virtual learning
are also looking for the ways to use clouds through mobile platforms. This
paper offers a model to accompany the benefits of &quot;Mobile Intelligent Learning&quot;
technology and &quot;Cloud Computing&quot;. The architecture of purposed system is based
on multi-layer architecture of Mobile Cloud Computing. Despite the existing
challenges, the system has increased the life of mobile device battery. It will
raise working memory capacity and processing capacity of the educational system
in addition to the greater advantage of the educational system. The proposed
system allows the users to enjoy an intelligent learning every-time and
every-where, reduces training costs and hardware dependency, and increases
consistency, efficiency, and data reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4048</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4048</id><created>2013-04-15</created><authors><author><keyname>Elkouss</keyname><forenames>David</forenames></author><author><keyname>Martinez-Mateo</keyname><forenames>Jesus</forenames></author><author><keyname>Ciurana</keyname><forenames>Alex</forenames></author><author><keyname>Martin</keyname><forenames>Vicente</forenames></author></authors><title>Secure Optical Networks Based on Quantum Key Distribution and Weakly
  Trusted Repeaters</title><categories>quant-ph cs.CR cs.NI</categories><comments>11 pages, 13 figures</comments><journal-ref>Journal of Optical Communications and Networking 5 (2013) 316-328</journal-ref><doi>10.1364/JOCN.5.000316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore how recent technologies can improve the security of
optical networks. In particular, we study how to use quantum key distribution
(QKD) in common optical network infrastructures and propose a method to
overcome its distance limitations. QKD is the first technology offering
information theoretic secret-key distribution that relies only on the
fundamental principles of quantum physics. Point-to-point QKD devices have
reached a mature industrial state; however, these devices are severely limited
in distance, since signals at the quantum level (e.g. single photons) are
highly affected by the losses in the communication channel and intermediate
devices. To overcome this limitation, intermediate nodes (i.e. repeaters) are
used. Both, quantum-regime and trusted, classical, repeaters have been proposed
in the QKD literature, but only the latter can be implemented in practice. As a
novelty, we propose here a new QKD network model based on the use of not fully
trusted intermediate nodes, referred as weakly trusted repeaters. This approach
forces the attacker to simultaneously break several paths to get access to the
exchanged key, thus improving significantly the security of the network. We
formalize the model using network codes and provide real scenarios that allow
users to exchange secure keys over metropolitan optical networks using only
passive components. Moreover, the theoretical framework allows to extend these
scenarios not only to accommodate more complex trust constraints, but also to
consider robustness and resiliency constraints on the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4051</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4051</id><created>2013-04-15</created><authors><author><keyname>Aydin</keyname><forenames>Mehmet Emin</forenames></author></authors><title>Coordinating metaheuristic agents with swarm intelligence</title><categories>cs.MA cs.NE</categories><journal-ref>Journal of Intelligent Manufacturing, 23 (4), pp:991-999, 2012</journal-ref><doi>10.1007/s10845-010-0435-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordination of multi agent systems remains as a problem since there is no
prominent method to completely solve this problem. Metaheuristic agents are
specific implementations of multi-agent systems, which imposes working together
to solve optimisation problems with metaheuristic algorithms. The idea borrowed
from swarm intelligence seems working much better than those implementations
suggested before. This paper reports the performance of swarms of simulated
annealing agents collaborating with particle swarm optimization algorithm. The
proposed approach is implemented for multidimensional knapsack problem and has
resulted much better than some other works published before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4055</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4055</id><created>2013-04-15</created><updated>2013-04-18</updated><authors><author><keyname>Shroff</keyname><forenames>Siddharth</forenames></author><author><keyname>Dabhi</keyname><forenames>Vipul</forenames></author></authors><title>Multiobjective optimization in Gene Expression Programming for Dew Point</title><categories>cs.NE</categories><comments>This paper has been withdrawn due to changes in file</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The processes occurring in climatic change evolution and their variations
play a major role in environmental engineering. Different techniques are used
to model the relationship between temperatures, dew point and relative
humidity. Gene expression programming is capable of modelling complex realities
with great accuracy, allowing, at the same time, the extraction of knowledge
from the evolved models compared to other learning algorithms. This research
aims to use Gene Expression Programming for modelling of dew point. Generally,
accuracy of the model is the only objective used by selection mechanism of GEP.
This will evolve large size models with low training error. To avoid this
situation, use of multiple objectives, like accuracy and size of the model are
preferred by Genetic Programming practitioners. Multi-objective problem finds a
set of solutions satisfying the objectives given by decision maker.
Multiobjective based GEP will be used to evolve simple models. Various
algorithms widely used for multi objective optimization like NSGA II and SPEA 2
are tested for different test cases. The results obtained thereafter gives idea
that SPEA 2 is better algorithm compared to NSGA II based on the features like
execution time, number of solutions obtained and convergence rate. Thus
compared to models obtained by GEP, multi-objective algorithms fetch better
solutions considering the dual objectives of fitness and size of the equation.
These simple models can be used to predict dew point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4058</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4058</id><created>2013-04-15</created><authors><author><keyname>Lee</keyname><forenames>Conrad</forenames></author><author><keyname>Nick</keyname><forenames>Bobo</forenames></author><author><keyname>Brandes</keyname><forenames>Ulrik</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Link Prediction with Social Vector Clocks</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art link prediction utilizes combinations of complex features
derived from network panel data. We here show that computationally less
expensive features can achieve the same performance in the common scenario in
which the data is available as a sequence of interactions. Our features are
based on social vector clocks, an adaptation of the vector-clock concept
introduced in distributed computing to social interaction networks. In fact,
our experiments suggest that by taking into account the order and spacing of
interactions, social vector clocks exploit different aspects of link formation
so that their combination with previous approaches yields the most accurate
predictor to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4071</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4071</id><created>2013-04-15</created><updated>2013-10-02</updated><authors><author><keyname>Lu</keyname><forenames>Weizhi</forenames></author><author><keyname>Li</keyname><forenames>Weiyu</forenames></author><author><keyname>Kpalma</keyname><forenames>Kidiyo</forenames></author><author><keyname>Ronsin</keyname><forenames>Joseph</forenames></author></authors><title>Near-optimal Binary Compressed Sensing Matrix</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Compressed sensing is a promising technique that attempts to faithfully
recover sparse signal with as few linear and nonadaptive measurements as
possible. Its performance is largely determined by the characteristic of
sensing matrix. Recently several zero-one binary sensing matrices have been
deterministically constructed for their relative low complexity and competitive
performance. Considering the complexity of implementation, it is of great
practical interest if one could further improve the sparsity of binary matrix
without performance loss. Based on the study of restricted isometry property
(RIP), this paper proposes the near-optimal binary sensing matrix, which
guarantees nearly the best performance with as sparse distribution as possible.
The proposed near-optimal binary matrix can be deterministically constructed
with progressive edge-growth (PEG) algorithm. Its performance is confirmed with
extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4073</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4073</id><created>2013-04-15</created><authors><author><keyname>Wan</keyname><forenames>Long</forenames></author></authors><title>Simultaneous approximation for scheduling problems</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Motivated by the problem to approximate all feasible schedules by one
schedule in a given scheduling environment, we introduce in this paper the
concepts of strong simultaneous approximation ratio (SAR) and weak simultaneous
approximation ratio (WAR). Then we study the two parameters under various
scheduling environments, such as, non-preemptive, preemptive or fractional
scheduling on identical, related or unrelated machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4077</identifier>
 <datestamp>2013-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4077</id><created>2013-04-15</created><updated>2013-05-31</updated><authors><author><keyname>Agarwal</keyname><forenames>Reshu</forenames></author><author><keyname>Ranjan</keyname><forenames>Pritam</forenames></author><author><keyname>Chipman</keyname><forenames>Hugh</forenames></author></authors><title>A new Bayesian ensemble of trees classifier for identifying multi-class
  labels in satellite images</title><categories>stat.ME cs.CV cs.LG</categories><comments>31 pages, 6 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification of satellite images is a key component of many remote sensing
applications. One of the most important products of a raw satellite image is
the classified map which labels the image pixels into meaningful classes.
Though several parametric and non-parametric classifiers have been developed
thus far, accurate labeling of the pixels still remains a challenge. In this
paper, we propose a new reliable multiclass-classifier for identifying class
labels of a satellite image in remote sensing applications. The proposed
multiclass-classifier is a generalization of a binary classifier based on the
flexible ensemble of regression trees model called Bayesian Additive Regression
Trees (BART). We used three small areas from the LANDSAT 5 TM image, acquired
on August 15, 2009 (path/row: 08/29, L1T product, UTM map projection) over
Kings County, Nova Scotia, Canada to classify the land-use. Several prediction
accuracy and uncertainty measures have been used to compare the reliability of
the proposed classifier with the state-of-the-art classifiers in remote
sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4086</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4086</id><created>2013-04-15</created><updated>2013-05-15</updated><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>Hubiness, length, crossings and their relationships in dependency trees</title><categories>cs.CL cs.DM cs.SI physics.soc-ph</categories><comments>The upper bound for for the number of crossings has been improved</comments><journal-ref>Ferrer-i-Cancho, R. (2013). Hubiness, length, crossings and their
  relationships in dependency trees. Glottometrics 25, 1-21</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here tree dependency structures are studied from three different
perspectives: their degree variance (hubiness), the mean dependency length and
the number of dependency crossings. Bounds that reveal pairwise dependencies
among these three metrics are derived. Hubiness (the variance of degrees) plays
a central role: the mean dependency length is bounded below by hubiness while
the number of crossings is bounded above by hubiness. Our findings suggest that
the online memory cost of a sentence might be determined not just by the
ordering of words but also by the hubiness of the underlying structure. The 2nd
moment of degree plays a crucial role that is reminiscent of its role in large
complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4091</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4091</id><created>2013-04-15</created><authors><author><keyname>Birolo</keyname><forenames>Giovanni</forenames></author></authors><title>Interactive Realizability, Monads and Witness Extraction</title><categories>cs.LO</categories><comments>This is a draft of the author's Ph.D. dissertation</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this dissertation we collect some results about &quot;interactive
realizability&quot;, a realizability semantics that extends the
Brouwer-Heyting-Kolmogorov interpretation to (sub-)classical logic, more
precisely to first-order intuitionistic arithmetic (Heyting Arithmetic, HA)
extended by the law of the excluded middle restricted to simply existential
formulas formulas (EM1), a system motivated by its interest in proof mining.
  We describe the interactive interpretation of a classical proof involving
real numbers. The statement we prove is a simple but non-trivial fact about
points in the real plane. The proof employs EM1 to deduce properties of the
ordering on the real numbers, which is undecidable and thus problematic from a
constructive point of view.
  We present a new set of reductions for derivations in natural deduction that
can extract witnesses from closed derivations of simply existential formulas in
HA + EM1. The reduction we present are inspired by the informal idea of
learning by making falsifiable hypothesis and checking them, and by the
interactive realizability interpretation. We extract the witnesses directly
from derivations in HA + EM1 by reduction, without encoding derivations by a
realizability interpretation.
  We give a new presentation of interactive realizability with a more explicit
syntax. We express interactive realizers by means of an abstract framework that
applies the monadic approach used in functional programming to modified
realizability, in order to obtain less strict notions of realizability that are
suitable to classical logic. In particular we use a combination of the state
and exception monads in order to capture the learning-from-mistakes nature of
interactive realizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4104</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4104</id><created>2013-04-15</created><updated>2014-06-15</updated><authors><author><keyname>Hofman</keyname><forenames>Piotr</forenames></author><author><keyname>Mayr</keyname><forenames>Richard</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>Decidability of Weak Simulation on One-counter Nets</title><categories>cs.FL</categories><comments>24 pages</comments><report-no>EDI-INF-RR-1415</report-no><msc-class>68Q45</msc-class><acm-class>F.1.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-counter nets (OCN) are Petri nets with exactly one unbounded place. They
are equivalent to a subclass of one-counter automata with only a weak test for
zero. We show that weak simulation preorder is decidable for OCN and that weak
simulation approximants do not converge at level omega, but only at omega^2. In
contrast, other semantic relations like weak bisimulation are undecidable for
OCN, and so are weak (and strong) trace inclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4112</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4112</id><created>2013-04-15</created><authors><author><keyname>Abrams</keyname><forenames>Austin</forenames></author><author><keyname>Hawley</keyname><forenames>Chris</forenames></author><author><keyname>Miskell</keyname><forenames>Kylia</forenames></author><author><keyname>Stoica</keyname><forenames>Adina</forenames></author><author><keyname>Jacobs</keyname><forenames>Nathan</forenames></author><author><keyname>Pless</keyname><forenames>Robert</forenames></author></authors><title>Shadow Estimation Method for &quot;The Episolar Constraint: Monocular Shape
  from Shadow Correspondence&quot;</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovering shadows is an important step for many vision algorithms. Current
approaches that work with time-lapse sequences are limited to simple
thresholding heuristics. We show these approaches only work with very careful
tuning of parameters, and do not work well for long-term time-lapse sequences
taken over the span of many months. We introduce a parameter-free expectation
maximization approach which simultaneously estimates shadows, albedo, surface
normals, and skylight. This approach is more accurate than previous methods,
works over both very short and very long sequences, and is robust to the
effects of nonlinear camera response. Finally, we demonstrate that the shadow
masks derived through this algorithm substantially improve the performance of
sun-based photometric stereo compared to earlier shadow mask estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4119</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4119</id><created>2013-04-15</created><authors><author><keyname>van Hoek</keyname><forenames>Wilko</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Assessing Visualization Techniques for the Search Process in Digital
  Libraries</title><categories>cs.DL cs.IR</categories><comments>23 pages, 14 figures, pre-print to appear in &quot;Wissensorganisation mit
  digitalen Technologien&quot; (deGruyter)</comments><msc-class>68-00</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an overview of several visualization techniques to
support the search process in Digital Libraries (DLs). The search process
typically can be separated into three major phases: query formulation and
refinement, browsing through result lists and viewing and interacting with
documents and their properties. We discuss a selection of popular visualization
techniques that have been developed for the different phases to support the
user during the search process. Along prototypes based on the different
techniques we show how the approaches have been implemented. Although various
visualizations have been developed in prototypical systems very few of these
approaches have been adapted into today's DLs. We conclude that this is most
likely due to the fact that most systems are not evaluated intensely in
real-life scenarios with real information seekers and that results of the
interesting visualization techniques are often not comparable. We can say that
many of the assessed systems did not properly address the information need of
cur-rent users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4134</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4134</id><created>2013-04-15</created><authors><author><keyname>Schneider</keyname><forenames>Carsten</forenames></author></authors><title>Simplifying Multiple Sums in Difference Fields</title><categories>cs.SC math-ph math.CO math.MP</categories><comments>Uses svmult.cls, to appear as contribution in the book &quot;Computer
  Algebra in Quantum Field Theory: Integration, Summation and Special
  Functions&quot; (www.Springer.com)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this survey article we present difference field algorithms for symbolic
summation. Special emphasize is put on new aspects in how the summation
problems are rephrased in terms of difference fields, how the problems are
solved there, and how the derived results in the given difference field can be
reinterpreted as solutions of the input problem. The algorithms are illustrated
with the Mathematica package \SigmaP\ by discovering and proving new harmonic
number identities extending those from (Paule and Schneider, 2003). In
addition, the newly developed package \texttt{EvaluateMultiSums} is introduced
that combines the presented tools. In this way, large scale summation problems
for the evaluation of Feynman diagrams in QCD (Quantum ChromoDynamics) can be
solved completely automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4137</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4137</id><created>2013-04-15</created><authors><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Saganowski</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Group Evolution Discovery in Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Brodka, P.; Saganowski, S.; Kazienko, P., &quot;Group Evolution Discovery
  in Social Networks,&quot; Advances in Social Networks Analysis and Mining
  (ASONAM), 2011 International Conference on, vol., no., pp.247,253, 25-27 July
  2011 doi: 10.1109/ASONAM.2011.69</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group extraction and their evolution are among the topics which arouse the
greatest interest in the domain of social network analysis. However, while the
grouping methods in social networks are developed very dynamically, the methods
of group evolution discovery and analysis are still uncharted territory on the
social network analysis map. Therefore the new method for the group evolution
discovery called GED is proposed in this paper. Additionally, the results of
the first experiments on the email based social network together with
comparison with two other methods of group evolution discovery are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4150</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4150</id><created>2013-04-15</created><updated>2013-07-02</updated><authors><author><keyname>Barcelo</keyname><forenames>Pablo</forenames></author><author><keyname>Figueira</keyname><forenames>Diego</forenames></author><author><keyname>Libkin</keyname><forenames>Leonid</forenames></author></authors><title>Graph Logics with Rational Relations</title><categories>cs.FL</categories><proxy>Logical Methods In Computer Science</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (April 15,
  2013) lmcs:664</journal-ref><doi>10.2168/LMCS-9(3:01)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate some basic questions about the interaction of regular and
rational relations on words. The primary motivation comes from the study of
logics for querying graph topology, which have recently found numerous
applications. Such logics use conditions on paths expressed by regular
languages and relations, but they often need to be extended by rational
relations such as subword or subsequence. Evaluating formulae in such extended
graph logics boils down to checking nonemptiness of the intersection of
rational relations with regular or recognizable relations (or, more generally,
to the generalized intersection problem, asking whether some projections of a
regular relation have a nonempty intersection with a given rational relation).
  We prove that for several basic and commonly used rational relations, the
intersection problem with regular relations is either undecidable (e.g., for
subword or su?x, and some generalizations), or decidable with
non-primitive-recursive complexity (e.g., for subsequence and its
generalizations). These results are used to rule out many classes of graph
logics that freely combine regular and rational relations, as well as to
provide the simplest problem related to verifying lossy channel systems that
has non-primitive-recursive complexity. We then prove a dichotomy result for
logics combining regular conditions on individual paths and rational relations
on paths, by showing that the syntactic form of formulae classi?es them into
either e?ciently checkable or undecidable cases. We also give examples of
rational relations for which such logics are decidable even without syntactic
restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4151</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4151</id><created>2013-04-15</created><updated>2014-04-08</updated><authors><author><keyname>Bi</keyname><forenames>Suzhi</forenames><affiliation>Angela</affiliation></author><author><keyname>Jun</keyname><forenames>Ying</forenames><affiliation>Angela</affiliation></author><author><keyname>Zhang</keyname></author></authors><title>Graphical Methods for Defense Against False-data Injection Attacks on
  Power System State Estimation</title><categories>cs.OH</categories><comments>Accepted for publication by IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The normal operation of power system relies on accurate state estimation that
faithfully reflects the physical aspects of the electrical power grids.
However, recent research shows that carefully synthesized false-data injection
attacks can bypass the security system and introduce arbitrary errors to state
estimates. In this paper, we use graphical methods to study defending
mechanisms against false-data injection attacks on power system state
estimation. By securing carefully selected meter measurements, no false data
injection attack can be launched to compromise any set of state variables. We
characterize the optimal protection problem, which protects the state variables
with minimum number of measurements, as a variant Steiner tree problem in a
graph. Based on the graphical characterization, we propose both exact and
reduced-complexity approximation algorithms. In particular, we show that the
proposed tree-pruning based approximation algorithm significantly reduces
computational complexity, while yielding negligible performance degradation
compared with the optimal algorithms. The advantageous performance of the
proposed defending mechanisms is verified in IEEE standard power system
testcases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4156</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4156</id><created>2013-04-15</created><updated>2013-11-19</updated><authors><author><keyname>Fallani</keyname><forenames>Fabrizio De Vico</forenames></author><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author><author><keyname>Chavez</keyname><forenames>Mario</forenames></author></authors><title>Non-parametric resampling of random walks for spectral network
  clustering</title><categories>physics.soc-ph cs.SI stat.AP</categories><comments>5 pages, 2 figures</comments><journal-ref>Phys. Rev. E 89, 012802 (2014)</journal-ref><doi>10.1103/PhysRevE.89.012802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parametric resampling schemes have been recently introduced in complex
network analysis with the aim of assessing the statistical significance of
graph clustering and the robustness of community partitions. We propose here a
method to replicate structural features of complex networks based on the
non-parametric resampling of the transition matrix associated with an unbiased
random walk on the graph. We test this bootstrapping technique on synthetic and
real-world modular networks and we show that the ensemble of replicates
obtained through resampling can be used to improve the performance of standard
spectral algorithms for community detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4159</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4159</id><created>2013-04-15</created><authors><author><keyname>Fredriksson</keyname><forenames>Olle</forenames></author><author><keyname>Ghica</keyname><forenames>Dan R.</forenames></author></authors><title>Abstract machines for game semantics, revisited</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define new abstract machines for game semantics which correspond to
networks of conventional computers, and can be used as an intermediate
representation for compilation targeting distributed systems. This is achieved
in two steps. First we introduce the HRAM, a Heap and Register Abstract
Machine, an abstraction of a conventional computer, which can be structured
into HRAM nets, an abstract point-to-point network model. HRAMs are
multi-threaded and subsume communication by tokens (cf. IAM) or jumps. Game
Abstract Machines (GAM), are HRAMs with additional structure at the interface
level, but no special operational capabilities. We show that GAMs cannot be
naively composed, but composition must be mediated using appropriate HRAM
combinators. HRAMs are flexible enough to allow the representation of game
models for languages with state (non-innocent games) or concurrency
(non-alternating games). We illustrate the potential of this technique by
implementing a toy distributed compiler for ICA, a higher-order programming
language with shared state concurrency, thus significantly extending our
previous distributed PCF compiler. We show that compilation is sound and
memory-safe, i.e. no (distributed or local) garbage collection is necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4161</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4161</id><created>2013-04-15</created><updated>2014-02-07</updated><authors><author><keyname>Lu</keyname><forenames>Weizhi</forenames></author><author><keyname>Kpalma</keyname><forenames>Kidiyo</forenames></author><author><keyname>Ronsin</keyname><forenames>Joseph</forenames></author></authors><title>Compressed Sensing Matrices: Binary vs. Ternary</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors. The proof is irrigorous
  so that the conclusion is not completely right</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Binary matrix and ternary matrix are two types of popular sensing matrices in
compressed sensing for their competitive performance and low computation.
However, to the best of our knowledge, there seems no literature aiming at
evaluating their performances if they hold the same sparisty, though it is of
practical importance. Based on both RIP analysis and numerical simulations,
this paper, for the first time, discloses that {0, 1} binary matrix holds
better overall performance over {0, +1, -1} ternary matrix, if they share the
same distribution on nonzero positions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4162</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4162</id><created>2013-04-15</created><authors><author><keyname>Petukhov</keyname><forenames>Alexander</forenames></author><author><keyname>Kozlov</keyname><forenames>Inna</forenames></author></authors><title>Greedy Approach for Low-Rank Matrix Recovery</title><categories>math.NA cs.IT cs.NA math.IT</categories><msc-class>65F30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the Simple Greedy Matrix Completion Algorithm providing an
efficient method for restoration of low-rank matrices from incomplete corrupted
entries.
  We provide numerical evidences that, even in the simplest implementation, the
greedy approach may increase the recovery capability of existing algorithms
significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4164</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4164</id><created>2013-04-15</created><updated>2015-10-13</updated><authors><author><keyname>Hannula</keyname><forenames>Miika</forenames></author></authors><title>Axiomatizing first-order consequences in independence logic</title><categories>math.LO cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1208.0176 by other authors</comments><msc-class>03C80</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independence logic cannot be effectively axiomatized. However, first-order
consequences of independence logic sentences can be axiomatized. In this
article we give an explicit axiomatization and prove that it is complete in
this sense. The proof is a generalization of the similar result for dependence
logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4181</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4181</id><created>2013-04-15</created><updated>2013-12-29</updated><authors><author><keyname>Song</keyname><forenames>Eva C.</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Guan</keyname><forenames>Kyle</forenames></author></authors><title>Rate-Distortion-Based Physical Layer Secrecy with Applications to
  Multimode Fiber</title><categories>cs.CR cs.IT math.IT</categories><comments>30 pages, 5 figures, accepted to IEEE Transactions on Communications</comments><journal-ref>IEEE Trans. on Communications, 62(3):1080-90, March, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical networks are vulnerable to physical layer attacks; wiretappers can
improperly receive messages intended for legitimate recipients. Our work
considers an aspect of this security problem within the domain of multimode
fiber (MMF) transmission. MMF transmission can be modeled via a broadcast
channel in which both the legitimate receiver's and wiretapper's channels are
multiple-input-multiple-output complex Gaussian channels. Source-channel coding
analyses based on the use of distortion as the metric for secrecy are
developed. Alice has a source sequence to be encoded and transmitted over this
broadcast channel so that the legitimate user Bob can reliably decode while
forcing the distortion of wiretapper, or eavesdropper, Eve's estimate as high
as possible. Tradeoffs between transmission rate and distortion under two
extreme scenarios are examined: the best case where Eve has only her channel
output and the worst case where she also knows the past realization of the
source. It is shown that under the best case, an operationally separate
source-channel coding scheme guarantees maximum distortion at the same rate as
needed for reliable transmission. Theoretical bounds are given, and
particularized for MMF. Numerical results showing the rate distortion tradeoff
are presented and compared with corresponding results for the perfect secrecy
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4182</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4182</id><created>2013-04-15</created><updated>2014-08-27</updated><authors><author><keyname>Kanal</keyname><forenames>Laveen</forenames></author><author><keyname>Lemmer</keyname><forenames>John</forenames></author></authors><title>Proceedings of the First Conference on Uncertainty in Artificial
  Intelligence (1985)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI1985</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the First Conference on Uncertainty in Artificial
Intelligence, which was held in Los Angeles, CA, July 10-12, 1985
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4184</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4184</id><created>2013-04-15</created><authors><author><keyname>Srikantaiah</keyname><forenames>K. C.</forenames></author><author><keyname>Kumar</keyname><forenames>N. Krishna</forenames></author><author><keyname>Venugopal</keyname><forenames>K. R.</forenames></author><author><keyname>Patnaik</keyname><forenames>L. M.</forenames></author></authors><title>Bidirectional Growth based Mining and Cyclic Behaviour Analysis of Web
  Sequential Patterns</title><categories>cs.DB</categories><comments>19 pages</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.2, March 2013</journal-ref><doi>10.5121/ijdkp.2013.3204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web sequential patterns are important for analyzing and understanding users
behaviour to improve the quality of service offered by the World Wide Web. Web
Prefetching is one such technique that utilizes prefetching rules derived
through Cyclic Model Analysis of the mined Web sequential patterns. The more
accurate the prediction and more satisfying the results of prefetching if we
use a highly efficient and scalable mining technique such as the Bidirectional
Growth based Directed Acyclic Graph. In this paper, we propose a novel
algorithm called Bidirectional Growth based mining Cyclic behavior Analysis of
web sequential Patterns (BGCAP) that effectively combines these strategies to
generate prefetching rules in the form of 2-sequence patterns with Periodicity
and threshold of Cyclic Behaviour that can be utilized to effectively prefetch
Web pages, thus reducing the users perceived latency. As BGCAP is based on
Bidirectional pattern growth, it performs only (log n+1) levels of recursion
for mining n Web sequential patterns. Our experimental results show that
prefetching rules generated using BGCAP is 5-10 percent faster for different
data sizes and 10-15% faster for a fixed data size than TD-Mine. In addition,
BGCAP generates about 5-15 percent more prefetching rules than TD-Mine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4187</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4187</id><created>2013-04-15</created><authors><author><keyname>Abiteboul</keyname><forenames>Serge</forenames><affiliation>LSV</affiliation></author><author><keyname>Antoine</keyname><forenames>&#xc9;milien</forenames><affiliation>LSV</affiliation></author><author><keyname>Stoyanovich</keyname><forenames>Julia</forenames></author></authors><title>The Webdamlog System Managing Distributed Knowledge on the Web</title><categories>cs.DB</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the use of WebdamLog, a declarative high-level lan- guage in the
style of datalog, to support the distribution of both data and knowledge (i.e.,
programs) over a network of au- tonomous peers. The main novelty of WebdamLog
compared to datalog is its use of delegation, that is, the ability for a peer
to communicate a program to another peer. We present results of a user study,
showing that users can write WebdamLog programs quickly and correctly, and with
a minimal amount of training. We present an implementation of the WebdamLog
inference engine relying on the Bud dat- alog engine. We describe an
experimental evaluation of the WebdamLog engine, demonstrating that WebdamLog
can be im- plemented efficiently. We conclude with a discussion of ongoing and
future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4191</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4191</id><created>2013-04-15</created><authors><author><keyname>Petukhov</keyname><forenames>Alexander</forenames></author><author><keyname>Kozlov</keyname><forenames>Inna</forenames></author></authors><title>Correcting Errors in Linear Measurements and Compressed Sensing of
  Multiple Sources</title><categories>math.NA cs.IT math.IT</categories><msc-class>65F99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for finding sparse solutions of the system of linear
equations $\Phi\mathbf{x}=\mathbf{y}$ with rectangular matrices $\Phi$ of size
$n\times N$, where $n&lt;N$, when measurement vector $\mathbf{y}$ is corrupted by
a sparse vector of errors $\mathbf e$. We call our algorithm the
$\ell^1$-greedy-generous (LGGA) since it combines both greedy and generous
strategies in decoding. Main advantage of LGGA over traditional error
correcting methods consists in its ability to work efficiently directly on
linear data measurements. It uses the natural residual redundancy of the
measurements and does not require any additional redundant channel encoding. We
show how to use this algorithm for encoding-decoding multichannel sources. This
algorithm has a significant advantage over existing straightforward decoders
when the encoded sources have different density/sparsity of the information
content. That nice property can be used for very efficient blockwise encoding
of the sets of data with a non-uniform distribution of the information. The
images are the most typical example of such sources. The important feature of
LGGA is its separation from the encoder. The decoder does not need any
additional side information from the encoder except for linear measurements and
the knowledge that those measurements created as a linear combination of
different sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4199</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4199</id><created>2013-04-15</created><authors><author><keyname>Treust</keyname><forenames>Mael Le</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author><author><keyname>He</keyname><forenames>Gaoning</forenames></author></authors><title>Green Power Control in Cognitive Wireless Networks</title><categories>cs.IT cs.GT math.IT</categories><comments>13 pages,5 figures, Journal Paper</comments><journal-ref>IEEE Transaction on Vehicular Technology, 2012</journal-ref><doi>10.1109/TVT.2012.2227858</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A decentralized network of cognitive and non-cognitive transmitters where
each transmitter aims at maximizing his energy-efficiency is considered. The
cognitive transmitters are assumed to be able to sense the transmit power of
their non-cognitive counterparts and the former have a cost for sensing. The
Stackelberg equilibrium analysis of this $2-$level hierarchical game is
conducted, which allows us to better understand the effects of cognition on
energy-efficiency. In particular, it is proven that the network
energy-efficiency is maximized when only a given fraction of terminals are
cognitive. Then, we study a sensing game where all the transmitters are assumed
to take the decision whether to sense (namely to be cognitive) or not. This
game is shown to be a weighted potential game and its set of equilibria is
studied. Playing the sensing game in a first phase (e.g., of a time-slot) and
then playing the power control game is shown to be more efficient individually
for all transmitters than playing a game where a transmitter would jointly
optimize whether to sense and his power level, showing the existence of a kind
of Braess paradox. The derived results are illustrated by numerical results and
provide some insights on how to deploy cognitive radios in heterogeneous
networks in terms of sensing capabilities. Keywords: Power Control, Stackelberg
Equilibrium, Energy-Efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4207</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4207</id><created>2013-04-15</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>The planar directed k-Vertex-Disjoint Paths problem is fixed-parameter
  tractable</title><categories>cs.DM cs.DS math.CO</categories><comments>110 pages, 43 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph G and k pairs of vertices (s_1,t_1), ..., (s_k,t_k), the
k-Vertex-Disjoint Paths problem asks for pairwise vertex-disjoint paths P_1,
..., P_k such that P_i goes from s_i to t_i. Schrijver [SICOMP'94] proved that
the k-Vertex-Disjoint Paths problem on planar directed graphs can be solved in
time n^{O(k)}. We give an algorithm with running time 2^{2^{O(k^2)}} n^{O(1)}
for the problem, that is, we show the fixed-parameter tractability of the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4222</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4222</id><created>2013-04-15</created><authors><author><keyname>Ghadirli</keyname><forenames>Hossein Movafegh</forenames></author><author><keyname>Rastgarpour</keyname><forenames>Maryam</forenames></author></authors><title>A Web-based Adaptive and Intelligent Tutor by Expert Systems</title><categories>cs.CY</categories><comments>10 pages, 3 figures, The Second International Conference on Advances
  in Computing and Information Technology (ACITY 2012). arXiv admin note:
  substantial text overlap with arXiv:1304.4045</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Todays, Intelligent and web-based E-learning is one of regarded topics. So
researchers are trying to optimize and expand its application in the field of
education. The aim of this paper is developing of E-learning software which is
customizable, dynamic, intelligent and adaptive with Pedagogy view for learners
in intelligent schools. This system is an integration of adaptive web-based
E-learning with expert systems as well. Learning process in this system is as
follows. First intelligent tutor determines learning style and characteristics
of learner by a questionnaire and then makes his model. After that the expert
system simulator plans a pre-test and then calculates his score. If the learner
gets the required score, the concept will be trained. Finally the learner will
be evaluated by a post-test. The proposed system can improves the education
efficiency highly as well as de-creases the costs and problems of an expert
tutor. As a result, every time and eve-rywhere (ETEW) learning would be
provided via web in this system. Moreover the learners can enjoy a cheap remote
learning even at home in a virtual simulated physical class. So they can learn
thousands courses very simple and fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4223</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4223</id><created>2013-04-15</created><authors><author><keyname>Ghadirli</keyname><forenames>Hossein Movafegh</forenames></author><author><keyname>Rastgarpour</keyname><forenames>Maryam</forenames></author></authors><title>A Web-based Multilingual Intelligent Tutor System based on Jackson's
  Learning Styles Profiler and Expert Systems</title><categories>cs.CY</categories><comments>12 pages, 2 figures, IAENG Transactions on Electrical Engineering
  Volume 1 - Special Issue of the International MultiConference of Engineers
  and Computer Scientists 2012. arXiv admin note: substantial text overlap with
  arXiv:1304.4045</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Nowadays, Intelligent Tutoring Systems (ITSs) are so regarded in order to
improve education quality via new technologies in this area. One of the
problems is that the language of ITSs is different from the learner's. It
forces the learners to learn the system language. This paper tries to remove
this necessity by using an Automatic Translator Component in system structure
like Google Translate API. This system carry out a pre-test and post-test by
using Expert System and Jackson Model before and after of training a concept.
It constantly updates learner model to save all changes in learning process. So
this paper offers an E-Learning system which is web-based, intelligent,
adaptive, multilingual and remotely accessible where tutors and learners can
have non-identical language. It is also applicable Every Time and Every Where
(ETEW). Furthermore, it trains the concepts in the best method with any
language and low cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4243</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4243</id><created>2013-04-15</created><authors><author><keyname>Abdullah</keyname><forenames>Amirali</forenames></author><author><keyname>Daruki</keyname><forenames>Samira</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author></authors><title>Range Counting Coresets for Uncertain Data</title><categories>cs.CG cs.DS</categories><comments>17 pages, 3 figures, 29th Annual ACM Symposium on Computational
  Geometry (SoCG)- June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study coresets for various types of range counting queries on uncertain
data. In our model each uncertain point has a probability density describing
its location, sometimes defined as k distinct locations. Our goal is to
construct a subset of the uncertain points, including their locational
uncertainty, so that range counting queries can be answered by just examining
this subset. We study three distinct types of queries. RE queries return the
expected number of points in a query range. RC queries return the number of
points in the range with probability at least a threshold. RQ queries returns
the probability that fewer than some threshold fraction of the points are in
the range. In both RC and RQ coresets the threshold is provided as part of the
query. And for each type of query we provide coreset constructions with
approximation-size tradeoffs. We show that random sampling can be used to
construct each type of coreset, and we also provide significantly improved
bounds using discrepancy-based approaches on axis-aligned range queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4267</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4267</id><created>2013-04-15</created><updated>2013-04-30</updated><authors><author><keyname>Galliani</keyname><forenames>Pietro</forenames></author><author><keyname>Hella</keyname><forenames>Lauri</forenames></author></authors><title>Inclusion Logic and Fixed Point Logic</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the properties of Inclusion Logic, that is, First Order Logic
with Team Semantics extended with inclusion dependencies. We prove that
Inclusion Logic is equivalent to Greatest Fixed Point Logic, and we prove that
all union-closed first-order definable properties of relations are definable in
it. We also provide an Ehrenfeucht-Fra\&quot;iss\'e game for Inclusion Logic, and
give an example illustrating its use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4273</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4273</id><created>2013-04-15</created><authors><author><keyname>Talati</keyname><forenames>Gunjan</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>The Quartic Public Key Transformation</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the quartic public key transformation which can be used
for public key applications if side information is also used. This extends an
earlier work where the cubic transformation was similarly used. Such a
transformation can be used in multiparty communications protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4280</identifier>
 <datestamp>2013-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4280</id><created>2013-04-15</created><updated>2013-05-07</updated><authors><author><keyname>Singh</keyname><forenames>Rishi Ranjan</forenames></author><author><keyname>Balakuntala</keyname><forenames>Shreyas</forenames></author><author><keyname>Iyengar</keyname><forenames>Sudarshan</forenames></author></authors><title>Navigability on Networks: A Graph Theoretic Perspective</title><categories>cs.DS cs.SI</categories><comments>This paper has been withdrawn by the author due to an error in
  equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human navigation has been of interest to psychologists and cognitive
scientists since the past few decades. It was in the recent past that a study
of human navigational strategies was initiated with a network analytic
approach, instigated mainly by Milgrams small world experiment. We brief the
work in this direction and provide answers to the algorithmic questions raised
by the previous study. It is noted that humans have a tendency to navigate
using centers of the network - such paths are called the
center-strategic-paths. We show that the problem of finding a
center-strategic-path is an easy one. We provide a polynomial time algorithm to
find a center-strategic-path between a given pair of nodes. We apply our
finding in empirically checking the navigability on synthetic networks and
analyze few special types of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4285</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4285</id><created>2013-04-15</created><authors><author><keyname>Yu</keyname><forenames>Seung Min</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Cost-Effective Broadcast in Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, video-related services such as YouTube and Netflix have
generated huge amounts of traffic and the network neutrality debate has emerged
as a major issue. In this paper, we consider feasibility of using a hybrid of
unicast and broadcast in cellular networks for high-traffic services (e.g.,
video streaming), from the perspective of cost effectiveness. To reflect
spatial characteristics of base stations (BSs) and mobile users (MUs), we use
the stochastic geometry approach where BSs and MUs are modeled as independent
homogeneous Poisson point processes (PPPs). With these assumptions and results,
we show how to cope with the trade-off between broadcast and unicast for
providing the service with affordable cost levels and reduced network load.
Moreover, we propose the so called periodic broadcasting service, where popular
video contents are periodically broadcast over cellular networks. This service
will make a positive impact on the network neutrality debate by stimulating
cooperation between mobile network operators (MNOs) and content providers
(CPs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4287</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4287</id><created>2013-04-15</created><updated>2014-10-29</updated><authors><author><keyname>Dantchev</keyname><forenames>Stefan</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author></authors><title>Relativisation makes contradictions harder for Resolution</title><categories>cs.LO</categories><journal-ref>Ann. Pure Appl. Logic 165(3): 837-857 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a number of simplified and improved separations between pairs of
Resolution-with-bounded-conjunction refutation systems, Res(d), as well as
their tree-like versions, Res*(d). The contradictions we use are natural
combinatorial principles: the Least number principle, LNP_n and an ordered
variant thereof, the Induction principle, IP_n.
  LNP_n is known to be easy for Resolution. We prove that its relativisation is
hard for Resolution, and more generally, the relativisation of LNP_n iterated d
times provides a separation between Res(d) and Res(d+1). We prove the same
result for the iterated relativisation of IP_n, where the tree-like variant
Res*(d) is considered instead of Res(d).
  We go on to provide separations between the parameterized versions of Res(1)
and Res(2). Here we are able again to use the relativisation of the LNP_n, but
the classical proof breaks down and we are forced to use an alternative.
Finally, we separate the parameterized versions of Res*(1) and Res*(2). Here,
the relativisation of IP_n will not work as it is, and so we make a vectorising
amendment to it in order to address this shortcoming
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4292</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4292</id><created>2013-04-15</created><authors><author><keyname>Lowery</keyname><forenames>Bradley R.</forenames></author></authors><title>Relative error due to a single bit-flip in floating-point arithmetic</title><categories>cs.NA</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the error due to a single bit-flip in a floating point number. We
assume IEEE 754 double precision arithmetic, which encodes binary floating
point numbers in a 64-bit word. We assume that the bit-flip happens randomly so
it has equi-probability (1/64) to hit any of the 64 bits. Since we want to
mitigate the assumption on our initial floating-point number, we assume that it
is uniformly picked among all normalized number. With this framework, we can
summarize our findings as follows. The probability for a single bit flip to
cause a relative error less than 10^-11 in a normalized floating-point number
is above 25%; The probability for a single bit flip to cause a relative error
less than 10^-6 in a normalized floating-point number is above 50%; Etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4303</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4303</id><created>2013-04-15</created><authors><author><keyname>Abouzied</keyname><forenames>Azza</forenames></author><author><keyname>Angluin</keyname><forenames>Dana</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Hellerstein</keyname><forenames>Joseph M.</forenames></author><author><keyname>Silberschatz</keyname><forenames>Avi</forenames></author></authors><title>Learning and Verifying Quantified Boolean Queries by Example</title><categories>cs.DB</categories><comments>Extended Version of PODS 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To help a user specify and verify quantified queries --- a class of database
queries known to be very challenging for all but the most expert users --- one
can question the user on whether certain data objects are answers or
non-answers to her intended query. In this paper, we analyze the number of
questions needed to learn or verify qhorn queries, a special class of Boolean
quantified queries whose underlying form is conjunctions of quantified Horn
expressions. We provide optimal polynomial-question and polynomial-time
learning and verification algorithms for two subclasses of the class qhorn with
upper constant limits on a query's causal density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4320</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4320</id><created>2013-04-15</created><updated>2014-05-01</updated><authors><author><keyname>Bishnu</keyname><forenames>Arijit</forenames></author><author><keyname>Ghosh</keyname><forenames>Subir Kumar</forenames></author><author><keyname>Goswami</keyname><forenames>Partha Pratim</forenames></author><author><keyname>Pal</keyname><forenames>Sudebkumar Prasant</forenames></author><author><keyname>Sarvattomananda</keyname><forenames>Swami</forenames></author></authors><title>An Algorithm for Computing Constrained Reflection Paths in Simple
  Polygon</title><categories>cs.CG</categories><msc-class>68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $s$ be a source point and $t$ be a destination point inside an $n$-vertex
simple polygon $P$. Euclidean shortest paths and minimum-link paths between $s$
and $t$ inside $P$ have been well studied. Both these kinds of paths are simple
and piecewise-convex. However, computing optimal paths in the context of
diffuse or specular reflections does not seem to be an easy task. A path from a
light source $s$ to $t$ inside $P$ is called a diffuse reflection path if the
turning points of the path lie in the interiors of the boundary edges of $P$. A
diffuse reflection path is said to be optimal if it has the minimum number of
turning points amongst all diffuse reflection paths between $s$ and $t$. The
minimum diffuse reflection path may not be simple. The problem of computing the
minimum diffuse reflection path in low degree polynomial time has remained
open.
  In our quest for understanding the geometric structure of the minimum diffuse
reflection paths vis-a-vis shortest paths and minimum link paths, we define a
new kind of diffuse reflection path called a constrained diffuse reflection
path where (i) the path is simple, (ii) it intersects only the eaves of the
Euclidean shortest path between $s$ and $t$, and (iii) it intersects each eave
exactly once. For computing a minimum constrained diffuse reflection path from
$s$ to $t$, we present an $O(n(n+\beta))$ time algorithm, where $\beta =\Theta
(n^2)$ in the worst case. Here, $\beta$ depends on the shape of the polygon. We
also establish some properties relating minimum constrained diffuse reflection
paths and minimum diffuse reflection paths. Constrained diffuse reflection
paths introduced in this paper provide new geometric insights into the hitherto
unknown structures and shapes of optimal reflection paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4321</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4321</id><created>2013-04-15</created><updated>2013-11-18</updated><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Xia</keyname><forenames>Patrick</forenames></author></authors><title>Polar Codes: Speed of polarization and polynomial gap to capacity</title><categories>cs.IT cs.DS math.IT math.PR</categories><comments>26 pages; Submitted to IEEE Transactions on Information Theory (Full
  version of conference paper appearing in FOCS'13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that, for all binary-input symmetric memoryless channels, polar
codes enable reliable communication at rates within $\epsilon &gt; 0$ of the
Shannon capacity with a block length, construction complexity, and decoding
complexity all bounded by a {\em polynomial} in $1/\epsilon$. Polar coding
gives the {\em first known explicit construction} with rigorous proofs of all
these properties; previous constructions were not known to achieve capacity
with less than $\exp(1/\epsilon)$ decoding complexity except for erasure
channels.
  We establish the capacity-achieving property of polar codes via a direct
analysis of the underlying martingale of conditional entropies, without relying
on the martingale convergence theorem. This step gives rough polarization
(noise levels $\approx \epsilon$ for the &quot;good&quot; channels), which can then be
adequately amplified by tracking the decay of the channel Bhattacharyya
parameters. Our effective bounds imply that polar codes can have block length
(and encoding/decoding complexity) bounded by a polynomial in $1/\epsilon$. The
generator matrix of such polar codes can be constructed in polynomial time by
algorithmically computing an adequate approximation of the polarization
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4324</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4324</id><created>2013-04-15</created><authors><author><keyname>Bao</keyname><forenames>Peng</forenames></author><author><keyname>Shen</keyname><forenames>Hua-Wei</forenames></author><author><keyname>Huang</keyname><forenames>Junming</forenames></author><author><keyname>Cheng</keyname><forenames>Xueqi</forenames></author></authors><title>Popularity Prediction in Microblogging Network: A Case Study on Sina
  Weibo</title><categories>cs.SI physics.soc-ph</categories><comments>22nd International World Wide Web Conference (WWW'13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the popularity of content is important for both the host and users
of social media sites. The challenge of this problem comes from the inequality
of the popularity of con- tent. Existing methods for popularity prediction are
mainly based on the quality of content, the interface of social media site to
highlight contents, and the collective behavior of user- s. However, little
attention is paid to the structural charac- teristics of the networks spanned
by early adopters, i.e., the users who view or forward the content in the early
stage of content dissemination. In this paper, taking the Sina Weibo as a case,
we empirically study whether structural character- istics can provide clues for
the popularity of short messages. We find that the popularity of content is
well reflected by the structural diversity of the early adopters. Experimental
results demonstrate that the prediction accuracy is signif- icantly improved by
incorporating the factor of structural diversity into existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4326</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4326</id><created>2013-04-15</created><updated>2013-06-04</updated><authors><author><keyname>Chauhan</keyname><forenames>Himanshu</forenames></author><author><keyname>Garg</keyname><forenames>Vijay K.</forenames></author><author><keyname>Natarajan</keyname><forenames>Aravind</forenames></author><author><keyname>Mittal</keyname><forenames>Neeraj</forenames></author></authors><title>Distributed Abstraction Algorithm for Online Predicate Detection</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing a distributed computation is a hard problem in general due to the
combinatorial explosion in the size of the state-space with the number of
processes in the system. By abstracting the computation, unnecessary
explorations can be avoided. Computation slicing is an approach for abstracting
dis- tributed computations with respect to a given predicate. We focus on
regular predicates, a family of predicates that covers a large number of
commonly used predicates for runtime verification. The existing algorithms for
computation slicing are centralized in nature in which a single process is
responsible for computing the slice in either offline or online manner. In this
paper, we present a distributed online algorithm for computing the slice of a
distributed computation with respect to a regular predicate. Our algorithm
distributes the work and storage requirements across the system, thus reducing
the space and computation complexities per process. In addition, for
conjunctive predicates, our algorithm also reduces the message load per
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4327</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4327</id><created>2013-04-16</created><authors><author><keyname>Curtin</keyname><forenames>Ryan R.</forenames></author><author><keyname>March</keyname><forenames>William B.</forenames></author><author><keyname>Ram</keyname><forenames>Parikshit</forenames></author><author><keyname>Anderson</keyname><forenames>David V.</forenames></author><author><keyname>Gray</keyname><forenames>Alexander G.</forenames></author><author><keyname>Isbell</keyname><forenames>Charles L.</forenames><suffix>Jr</suffix></author></authors><title>Tree-Independent Dual-Tree Algorithms</title><categories>cs.DS</categories><comments>accepted in ICML 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dual-tree algorithms are a widely used class of branch-and-bound algorithms.
Unfortunately, developing dual-tree algorithms for use with different trees and
problems is often complex and burdensome. We introduce a four-part logical
split: the tree, the traversal, the point-to-point base case, and the pruning
rule. We provide a meta-algorithm which allows development of dual-tree
algorithms in a tree-independent manner and easy extension to entirely new
types of trees. Representations are provided for five common algorithms; for
k-nearest neighbor search, this leads to a novel, tighter pruning bound. The
meta-algorithm also allows straightforward extensions to massively parallel
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4329</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4329</id><created>2013-04-16</created><authors><author><keyname>Rajesh</keyname><forenames>Pasupuleti</forenames></author><author><keyname>Narsimha</keyname><forenames>Gugulothu</forenames></author></authors><title>Privacy Preserving Data Mining by Using Implicit Function Theorem</title><categories>cs.CR cs.DB</categories><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining has made broad significant multidisciplinary field used in vast
application domains and extracts knowledge by identifying structural
relationship among the objects in large data bases. Privacy preserving data
mining is a new area of data mining research for providing privacy of sensitive
knowledge of information extracted from data mining system to be shared by the
intended persons not to everyone to access. In this paper, we proposed a new
approach of privacy preserving data mining by using implicit function theorem
for secure transformation of sensitive data obtained from data mining system.
we proposed two way enhanced security approach. First transforming original
values of sensitive data into different partial derivatives of functional
values for perturbation of data. secondly generating symmetric key value by
Eigen values of jacobian matrix for secure computation. we given an example of
academic sensitive data converting into vector valued functions to explain
about our proposed concept and presented implementation based results of new
proposed of approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4344</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4344</id><created>2013-04-16</created><authors><author><keyname>Harandi</keyname><forenames>Mehrtash T.</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Sparse Coding and Dictionary Learning for Symmetric Positive Definite
  Matrices: A Kernel Approach</title><categories>cs.LG cs.CV stat.ML</categories><acm-class>I.2.6; I.5.1; I.5.4; I.2.10</acm-class><journal-ref>European Conference on Computer Vision, Lecture Notes in Computer
  Science (LNCS), Vol. 7573, pp. 216-229, 2012</journal-ref><doi>10.1007/978-3-642-33709-3_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances suggest that a wide range of computer vision problems can be
addressed more appropriately by considering non-Euclidean geometry. This paper
tackles the problem of sparse coding and dictionary learning in the space of
symmetric positive definite matrices, which form a Riemannian manifold. With
the aid of the recently introduced Stein kernel (related to a symmetric version
of Bregman matrix divergence), we propose to perform sparse coding by embedding
Riemannian manifolds into reproducing kernel Hilbert spaces. This leads to a
convex and kernel version of the Lasso problem, which can be solved
efficiently. We furthermore propose an algorithm for learning a Riemannian
dictionary (used for sparse coding), closely tied to the Stein kernel.
Experiments on several classification tasks (face recognition, texture
classification, person re-identification) show that the proposed sparse coding
approach achieves notable improvements in discrimination accuracy, in
comparison to state-of-the-art methods such as tensor sparse coding, Riemannian
locality preserving projection, and symmetry-driven accumulation of local
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4350</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4350</id><created>2013-04-16</created><authors><author><keyname>Castellano</keyname><forenames>Alice</forenames></author><author><keyname>Cuomo</keyname><forenames>Francesca</forenames></author></authors><title>Analysis of urban traffic data sets for VANETs simulations</title><categories>cs.NI</categories><comments>Abstract presented at INFOCOM 2013 N2WOMEN workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad-hoc Networks (VANET) are self-organized, distributed
communication networks built up from moving vehicles where each node is
characterized by variable speed, strict limits of freedom in movement patterns
and a variety of traffic dynamics. In the last few years vehicular traffic is
attracting a growing attention from both industry and research, due to the
importance of the related applications, ranging from traffic control to road
safety. However, less attention has been paid to the modeling of realistic user
mobility through real empirical data that would allow to develop much more
effective communication and networking schemes. The aim of this study is then
twofold: on one hand we derived real mobility patters to be used in a VANET
simulator and on the other hand we simulated the VANET data dissemination
achieved with different broadcast protocols in our real traffic setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4363</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4363</id><created>2013-04-16</created><authors><author><keyname>Kochkarev</keyname><forenames>B. S.</forenames></author></authors><title>Typical property of one class of combinatory objects and estimation from
  above corresponding combinatory numbers</title><categories>cs.DM math.CO</categories><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate properties of families $F$ of subsets of a finite set in a
situation where subsets are incomparable by the binary inclusion relation and
a) for any $A\notin F$, there is such set $A'\in F$ that either $A\subset A'$
or $A'\subset A$; b) for any $A\in F$, $\mid A\mid\in \{k,k+1\}$. For these
families we introduce one parametre and we show that for almost all families
$F$ the value of this parametre is ${n-1\choose k}$. We show that families with
the minimum value of the entered parametre have certain structure and we find
also number of such families. At last, we find an estimation from above for
combinatory numbers of considered combinatory objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4371</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4371</id><created>2013-04-16</created><authors><author><keyname>Lang</keyname><forenames>Joel</forenames></author><author><keyname>Henderson</keyname><forenames>James</forenames></author></authors><title>Efficient Computation of Mean Truncated Hitting Times on Very Large
  Graphs</title><categories>cs.DS cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work has shown the effectiveness of random walk hitting times as a
measure of dissimilarity in a variety of graph-based learning problems such as
collaborative filtering, query suggestion or finding paraphrases. However,
application of hitting times has been limited to small datasets because of
computational restrictions. This paper develops a new approximation algorithm
with which hitting times can be computed on very large, disk-resident graphs,
making their application possible to problems which were previously out of
reach. This will potentially benefit a range of large-scale problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4373</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4373</id><created>2013-04-16</created><updated>2014-06-02</updated><authors><author><keyname>Storath</keyname><forenames>Martin</forenames></author><author><keyname>Weinmann</keyname><forenames>Andreas</forenames></author><author><keyname>Demaret</keyname><forenames>Laurent</forenames></author></authors><title>Jump-sparse and sparse recovery using Potts functionals</title><categories>math.NA cs.NA math.OC</categories><msc-class>65J22, 65K10, 90C39, 90C26,</msc-class><doi>10.1109/TSP.2014.2329263</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recover jump-sparse and sparse signals from blurred incomplete data
corrupted by (possibly non-Gaussian) noise using inverse Potts energy
functionals. We obtain analytical results (existence of minimizers, complexity)
on inverse Potts functionals and provide relations to sparsity problems. We
then propose a new optimization method for these functionals which is based on
dynamic programming and the alternating direction method of multipliers (ADMM).
A series of experiments shows that the proposed method yields very satisfactory
jump-sparse and sparse reconstructions, respectively. We highlight the
capability of the method by comparing it with classical and recent approaches
such as TV minimization (jump-sparse signals), orthogonal matching pursuit,
iterative hard thresholding, and iteratively reweighted $\ell^1$ minimization
(sparse signals).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4379</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4379</id><created>2013-04-16</created><updated>2013-04-30</updated><authors><author><keyname>Noessner</keyname><forenames>Jan</forenames></author><author><keyname>Niepert</keyname><forenames>Mathias</forenames></author><author><keyname>Stuckenschmidt</keyname><forenames>Heiner</forenames></author></authors><title>RockIt: Exploiting Parallelism and Symmetry for MAP Inference in
  Statistical Relational Models</title><categories>cs.AI</categories><comments>To appear in proceedings of AAAI 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RockIt is a maximum a-posteriori (MAP) query engine for statistical
relational models. MAP inference in graphical models is an optimization problem
which can be compiled to integer linear programs (ILPs). We describe several
advances in translating MAP queries to ILP instances and present the novel
meta-algorithm cutting plane aggregation (CPA). CPA exploits local
context-specific symmetries and bundles up sets of linear constraints. The
resulting counting constraints lead to more compact ILPs and make the symmetry
of the ground model more explicit to state-of-the-art ILP solvers. Moreover,
RockIt parallelizes most parts of the MAP inference pipeline taking advantage
of ubiquitous shared-memory multi-core architectures.
  We report on extensive experiments with Markov logic network (MLN) benchmarks
showing that RockIt outperforms the state-of-the-art systems Alchemy, Markov
TheBeast, and Tuffy both in terms of efficiency and quality of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4383</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4383</id><created>2013-04-16</created><updated>2013-09-01</updated><authors><author><keyname>Karbalay-Ghareh</keyname><forenames>Alireza</forenames></author><author><keyname>Nasiri-Kenari</keyname><forenames>Masoumeh</forenames></author><author><keyname>Hejazi</keyname><forenames>Mohsen</forenames></author></authors><title>Convolutional Network-Coded Cooperation in Multi-Source Networks with a
  Multi-Antenna Relay</title><categories>cs.NI cs.IT math.IT</categories><comments>23 pages, 9 figures, 1 table, submitted to IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel cooperative transmission scheme called &quot;Convolutional
Network-Coded Cooperation&quot; (CNCC) for a network including N sources, one
M-antenna relay, and one common destination. The source-relay (S-R) channels
are assumed to be Nakagami-m fading, while the source-destination (S-D) and the
relay-destination (R-D) channels are considered Rayleigh fading. The CNCC
scheme exploits the generator matrix of a good (N+M', N, v) systematic
convolutional code, with the free distance of d_free designed over GF(2), as
the network coding matrix which is run by the network's nodes, such that the
systematic symbols are directly transmitted from the sources, and the parity
symbols are sent by the best antenna of the relay. An upper bound on the BER of
the sources, and consequently, the achieved diversity orders are obtained. The
numerical results indicate that the CNCC scheme outperforms the other
cooperative schemes considered, in terms of the diversity order and the network
throughput. The simulation results confirm the accuracy of the theoretical
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4391</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4391</id><created>2013-04-16</created><authors><author><keyname>Galliani</keyname><forenames>Pietro</forenames></author><author><keyname>Hannula</keyname><forenames>Miika</forenames></author><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author></authors><title>Hierarchies in independence logic</title><categories>math.LO cs.LO</categories><msc-class>03C80</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the expressive power of fragments of inclusion and independence
logic defined either by restricting the number of universal quantifiers or the
arity of inclusion and independence atoms in formulas. Assuming the so-called
lax semantics for these logics, we relate these fragments of inclusion and
independence logic to familiar sublogics of existential second-order logic. We
also show that, with respect to the stronger strict semantics, inclusion logic
is equivalent to existential second-order logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4407</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4407</id><created>2013-04-16</created><updated>2013-05-21</updated><authors><author><keyname>Fadili</keyname><forenames>M. J.</forenames></author><author><keyname>Peyr&#xe9;</keyname><forenames>G.</forenames></author><author><keyname>Vaiter</keyname><forenames>S.</forenames></author><author><keyname>Deledalle</keyname><forenames>C.</forenames></author><author><keyname>Salmon</keyname><forenames>J.</forenames></author></authors><title>Stable Recovery with Analysis Decomposable Priors</title><categories>cs.IT math.FA math.IT math.OC</categories><comments>4 pages, to appear in SAMPTA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate in a unified way the structural properties of
solutions to inverse problems. These solutions are regularized by the generic
class of semi-norms defined as a decomposable norm composed with a linear
operator, the so-called analysis type decomposable prior. This encompasses
several well-known analysis-type regularizations such as the discrete total
variation (in any dimension), analysis group-Lasso or the nuclear norm. Our
main results establish sufficient conditions under which uniqueness and
stability to a bounded noise of the regularized solution are guaranteed. Along
the way, we also provide a strong sufficient uniqueness result that is of
independent interest and goes beyond the case of decomposable norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4415</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4415</id><created>2013-04-16</created><authors><author><keyname>Jabbour</keyname><forenames>Said</forenames></author><author><keyname>Sais</keyname><forenames>Lakhdar</forenames></author><author><keyname>Salhi</keyname><forenames>Yakoub</forenames></author></authors><title>Mining to Compact CNF Propositional Formulae</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a first application of data mining techniques to
propositional satisfiability. Our proposed Mining4SAT approach aims to discover
and to exploit hidden structural knowledge for reducing the size of
propositional formulae in conjunctive normal form (CNF). Mining4SAT combines
both frequent itemset mining techniques and Tseitin's encoding for a compact
representation of CNF formulae. The experiments of our Mining4SAT approach show
interesting reductions of the sizes of many application instances taken from
the last SAT competitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4428</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4428</id><created>2013-04-16</created><updated>2013-04-24</updated><authors><author><keyname>Hejazi</keyname><forenames>Mohsen</forenames></author><author><keyname>Nasiri-Kenari</keyname><forenames>Masoumeh</forenames></author></authors><title>Simplified Compute-and-Forward and Its Performance Analysis</title><categories>cs.IT math.IT</categories><comments>Submitted to IET Communications, 29 pages, 7 figures, 1 table, latex,
  The authors are with the Wireless Research Laboratory (WRL), Department of
  Electrical Engineering, Sharif University of Technology, Tehran, Iran</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The compute-and-forward (CMF) method has shown a great promise as an
innovative approach to exploit interference toward achieving higher network
throughput. The CMF was primarily introduced by means of information theory
tools. While there have been some recent works discussing different aspects of
efficient and practical implementation of CMF, there are still some issues that
are not covered. In this paper, we first introduce a method to decrease the
implementation complexity of the CMF method. We then evaluate the exact outage
probability of our proposed simplified CMF scheme, and hereby provide an upper
bound on the outage probability of the optimum CMF in all SNR values, and a
close approximation of its outage probability in low SNR regimes. We also
evaluate the effect of the channel estimation error (CEE) on the performance of
both optimum and our proposed simplified CMF by simulations. Our simulation
results indicate that the proposed method is more robust against CEE than the
optimum CMF method for the examples considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4453</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4453</id><created>2013-04-16</created><updated>2015-02-02</updated><authors><author><keyname>Staudt</keyname><forenames>Christian L.</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author></authors><title>Engineering Parallel Algorithms for Community Detection in Massive
  Networks</title><categories>cs.DC cs.SI</categories><comments>14 pages, first presented at the 2013 International Conference on
  Parallel Processing, revised accepted for IEEE Transactions on Parallel and
  Distributed Systems (TPDS)</comments><doi>10.1109/TPDS.2015.2390633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of graph-structured data has recently experienced an enormous
growth in many applications. To transform such data into useful information,
fast analytics algorithms and software tools are necessary. One common graph
analytics kernel is disjoint community detection (or graph clustering). Despite
extensive research on heuristic solvers for this task, only few parallel codes
exist, although parallelism will be necessary to scale to the data volume of
real-world applications. We address the deficit in computing capability by a
flexible and extensible community detection framework with shared-memory
parallelism. Within this framework we design and implement efficient parallel
community detection heuristics: A parallel label propagation scheme; the first
large-scale parallelization of the well-known Louvain method, as well as an
extension of the method adding refinement; and an ensemble scheme combining the
above. In extensive experiments driven by the algorithm engineering paradigm,
we identify the most successful parameters and combinations of these
algorithms. We also compare our implementations with state-of-the-art
competitors. The processing rate of our fastest algorithm often reaches 50M
edges/second. We recommend the parallel Louvain method and our variant with
refinement as both qualitatively strong and fast. Our methods are suitable for
massive data sets with billions of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4464</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4464</id><created>2013-04-16</created><authors><author><keyname>Zewail</keyname><forenames>Ahmed A.</forenames></author><author><keyname>Nafie</keyname><forenames>M.</forenames></author><author><keyname>Mohasseb</keyname><forenames>Y.</forenames></author><author><keyname>Gamal</keyname><forenames>H. El</forenames></author></authors><title>The Deterministic Multicast Capacity of 4-Node Relay Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, accepted at ISIT'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we completely characterize the deterministic capacity region
of a four-node relay network with no direct links between the nodes, where each
node communicates with the three other nodes via a relay. Towards this end, we
develop an upper bound on the deterministic capacity region, based on the
notion of a one-sided genie. To establish achievability, we use the detour
schemes that achieve the upper bound by routing specific bits via indirect
paths instead of sending them directly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4471</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4471</id><created>2013-04-16</created><updated>2013-06-29</updated><authors><author><keyname>Yang</keyname><forenames>Yongjie</forenames></author><author><keyname>Guo</keyname><forenames>Jiong</forenames></author></authors><title>Complexity of Control Behaviors in k-Peaked Elections for a Variant of
  Approval Voting</title><categories>cs.GT</categories><msc-class>03D15</msc-class><acm-class>F.2; G.2.1; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single-peaked elections have been attracting much attention recently. It
turned out that many NP-hard voting problems become polynomial-time solvable
when restricted to single-peaked elections. A natural generalization of the
single-peaked elections is the k-peaked elections, where at most k peaks are
allowed in each vote in the election. In this paper, we mainly aim at
establishing a complexity dichotomy of controlling behaviors of a variant of
the sincere-strategy preference-based approval voting in k-peaked elections for
different values of k. It turns out that most NP-hardness results in the
general case also hold in k-peaked elections, even for k=2,3. On the other
hand, we derive polynomial-time algorithms for certain sincere-strategy
preference-based approval voting control problems for k=2. In addition, we also
study the sincere-strategy preference-based approval control problems from the
viewpoint of parameterized complexity and prove some FPT and W-hardness
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4472</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4472</id><created>2013-04-16</created><updated>2013-04-28</updated><authors><author><keyname>Romdhani</keyname><forenames>Lamia</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>An analytic study of a distributed EDCA-based QoS mapping for layered
  video delivery in WLAN</title><categories>cs.NI</categories><comments>in proceedings of the PEMWN 2012, Tunisia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key challenges in multimedia networks is video delivery over
wireless channels. MRC (Multi-Resolution Coding) Layered video, divides video
into a base layer and multiple enhancement layers. In this paper, we aim to
improve video quality, impacted by high channel contention, through mapping
individual video layers to EDCA (Enhanced Distributed Channel Access) access
categories in order to maximize the average number of reconstructed video
layers. We propose an adaptive cross layer video layers mapping technique that
optimally enhances the QoS of wireless video transmission over IEEE 802.11e
EDCA priority queues. The optimization is based on a dynamic program that takes
into account the EDCA parameters and the layered dependency nature of layered
video delivery. Our proposed technique makes use of a channel delay estimation
model and an estimation of average video useful layers delivered. The optimal
mapping strategies are selected by an optimization module based on the
information from the analytical model. The accuracy of our optimized mapping
technique performance is verified through extensive simulations. The obtained
results illustrate significant trade-off between complexity and delivered video
quality for canonical mapping schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4499</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4499</id><created>2013-04-16</created><updated>2013-10-03</updated><authors><author><keyname>Alberti</keyname><forenames>Francesco</forenames></author><author><keyname>Ghilardi</keyname><forenames>Silvio</forenames></author><author><keyname>Sharygina</keyname><forenames>Natasha</forenames></author></authors><title>Abstraction and Acceleration in SMT-based Model-Checking for Array
  Programs</title><categories>cs.LO</categories><comments>Published in the proceedings of the 9th International Symposium on
  Frontiers of Combining Systems (FroCoS) with the title &quot;Definability of
  Accelerated Relations in a Theory of Arrays and its Applications&quot; (available
  at http://www.springerlink.com)</comments><doi>10.1007/978-3-642-40885-4_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstraction (in its various forms) is a powerful established technique in
model-checking; still, when unbounded data-structures are concerned, it cannot
always cope with divergence phenomena in a satisfactory way. Acceleration is an
approach which is widely used to avoid divergence, but it has been applied
mostly to integer programs. This paper addresses the problem of accelerating
transition relations for unbounded arrays with the ultimate goal of avoiding
divergence during reachability analysis of abstract programs. For this, we
first design a format to compute accelerations in this domain; then we show how
to adapt the so-called 'monotonic abstraction' technique to efficiently handle
complex formulas with nested quantifiers generated by the acceleration
preprocessing. Notably, our technique can be easily plugged-in into
abstraction/refinement loops, and strongly contributes to avoid divergence:
experiments conducted with the MCMT model checker attest the effectiveness of
our approach on programs with unbounded arrays, where acceleration and
abstraction/refinement technologies fail if applied alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4517</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4517</id><created>2013-04-16</created><updated>2013-09-01</updated><authors><author><keyname>Christlieb</keyname><forenames>Andrew</forenames></author><author><keyname>Lawlor</keyname><forenames>David</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author></authors><title>A Multiscale Sub-linear Time Fourier Algorithm for Noisy Data</title><categories>math.NA cs.DS</categories><comments>28 pages, 6 figures</comments><msc-class>65T50, 68W25</msc-class><acm-class>F.2.1; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the recent sparse Fourier transform algorithm of (Lawlor,
Christlieb, and Wang, 2013) to the noisy setting, in which a signal of
bandwidth N is given as a superposition of k &lt;&lt; N frequencies and additive
noise. We present two such extensions, the second of which exhibits a novel
form of error-correction in its frequency estimation not unlike that of the
beta-encoders in analog-to-digital conversion (Daubechies et al, 2006). The
algorithm runs in time O(k log(k) log(N/k)) on average, provided the noise is
not overwhelming. The error-correction property allows the algorithm to
outperform FFTW, a highly optimized software package for computing the full
discrete Fourier transform, over a wide range of sparsity and noise values, and
is to the best of our knowledge novel in the sparse Fourier transform context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4519</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4519</id><created>2013-04-16</created><authors><author><keyname>Doty</keyname><forenames>David</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>Monir</forenames></author></authors><title>Leaderless deterministic chemical reaction networks</title><categories>cs.CC cs.DC cs.DS q-bio.MN</categories><comments>arXiv admin note: substantial text overlap with arXiv:1204.4176</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper answers an open question of Chen, Doty, and Soloveichik [1], who
showed that a function f:N^k --&gt; N^l is deterministically computable by a
stochastic chemical reaction network (CRN) if and only if the graph of f is a
semilinear subset of N^{k+l}. That construction crucially used &quot;leaders&quot;: the
ability to start in an initial configuration with constant but non-zero counts
of species other than the k species X_1,...,X_k representing the input to the
function f. The authors asked whether deterministic CRNs without a leader
retain the same power.
  We answer this question affirmatively, showing that every semilinear function
is deterministically computable by a CRN whose initial configuration contains
only the input species X_1,...,X_k, and zero counts of every other species. We
show that this CRN completes in expected time O(n), where n is the total number
of input molecules. This time bound is slower than the O(log^5 n) achieved in
[1], but faster than the O(n log n) achieved by the direct construction of [1]
(Theorem 4.1 in the latest online version of [1]), since the fast construction
of that paper (Theorem 4.4) relied heavily on the use of a fast, error-prone
CRN that computes arbitrary computable functions, and which crucially uses a
leader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4520</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4520</id><created>2013-04-16</created><authors><author><keyname>Mukherjee</keyname><forenames>Subhabrata</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Pushpak</forenames></author></authors><title>Sentiment Analysis : A Literature Survey</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our day-to-day life has always been influenced by what people think. Ideas
and opinions of others have always affected our own opinions. The explosion of
Web 2.0 has led to increased activity in Podcasting, Blogging, Tagging,
Contributing to RSS, Social Bookmarking, and Social Networking. As a result
there has been an eruption of interest in people to mine these vast resources
of data for opinions. Sentiment Analysis or Opinion Mining is the computational
treatment of opinions, sentiments and subjectivity of text. In this report, we
take a look at the various challenges and applications of Sentiment Analysis.
We will discuss in details various approaches to perform a computational
treatment of sentiments and opinions. Various supervised or data-driven
techniques to SA like Na\&quot;ive Byes, Maximum Entropy, SVM, and Voted Perceptrons
will be discussed and their strengths and drawbacks will be touched upon. We
will also see a new dimension of analyzing sentiments by Cognitive Psychology
mainly through the work of Janyce Wiebe, where we will see ways to detect
subjectivity, perspective in narrative and understanding the discourse
structure. We will also study some specific topics in Sentiment Analysis and
the contemporary works in those areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4523</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4523</id><created>2013-04-16</created><authors><author><keyname>Muchnik</keyname><forenames>Lev</forenames></author><author><keyname>Pei</keyname><forenames>Sen</forenames></author><author><keyname>Parra</keyname><forenames>Lucas C.</forenames></author><author><keyname>Reis</keyname><forenames>Saulo D. S.</forenames></author><author><keyname>Andrade,</keyname><forenames>Jose S.</forenames><suffix>Jr.</suffix></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Makse</keyname><forenames>Hernan A.</forenames></author></authors><title>Origins of power-law degree distribution in the heterogeneity of human
  activity in social networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>23 pages, 5 figures</comments><journal-ref>Scientific Reports, 3, 1783 (2013)</journal-ref><doi>10.1038/srep01783</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probability distribution of number of ties of an individual in a social
network follows a scale-free power-law. However, how this distribution arises
has not been conclusively demonstrated in direct analyses of people's actions
in social networks. Here, we perform a causal inference analysis and find an
underlying cause for this phenomenon. Our analysis indicates that heavy-tailed
degree distribution is causally determined by similarly skewed distribution of
human activity. Specifically, the degree of an individual is entirely random -
following a &quot;maximum entropy attachment&quot; model - except for its mean value
which depends deterministically on the volume of the users' activity. This
relation cannot be explained by interactive models, like preferential
attachment, since the observed actions are not likely to be caused by
interactions with other people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4524</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4524</id><created>2013-04-16</created><authors><author><keyname>Bhanage</keyname><forenames>Gautam</forenames></author><author><keyname>Kaul</keyname><forenames>Sanjit</forenames></author></authors><title>Investigating Randomly Generated Adjacency Matrices For Their Use In
  Modeling Wireless Topologies</title><categories>cs.NI cs.PF</categories><comments>3 pages, 4 figures</comments><acm-class>D.4.8; C.2.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Generation of realistic topologies plays an important role in determining the
accuracy and validity of simulation studies. This study presents a discussion
to justify why, and how often randomly generated adjacency matrices may not not
conform to wireless topologies in the physical world. Specifically, it shows
through analysis and random trials that, more than 90% of times, a randomly
generated adjacency matrix will not conform to a valid wireless topology, when
it has more than 3 nodes. By showing that node triplets in the adjacency graph
need to adhere to rules of a geometric vector space, the study shows that the
number of randomly chosen node triplets failing consistency checks grow at the
order of O(base^3), where base is the granularity of the distance metric.
Further, the study models and presents a probability estimate with which any
randomly generated adjacency matrix would fail realization. This information
could be used to design simpler algorithms for generating k-connected wireless
topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4535</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4535</id><created>2013-04-16</created><authors><author><keyname>da Silva</keyname><forenames>N&#xfa;bia Rosa</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Heterogeneous patterns enhancing static and dynamic texture
  classification</title><categories>cs.CV</categories><comments>6 pages, 5 figures</comments><journal-ref>N\'ubia Rosa da Silva and Odemir Martinez Bruno 2013 J. Phys.:
  Conf. Ser. 410 012033</journal-ref><doi>10.1088/1742-6596/410/1/012033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some mixtures, such as colloids like milk, blood, and gelatin, have
homogeneous appearance when viewed with the naked eye, however, to observe them
at the nanoscale is possible to understand the heterogeneity of its components.
The same phenomenon can occur in pattern recognition in which it is possible to
see heterogeneous patterns in texture images. However, current methods of
texture analysis can not adequately describe such heterogeneous patterns.
Common methods used by researchers analyse the image information in a global
way, taking all its features in an integrated manner. Furthermore, multi-scale
analysis verifies the patterns at different scales, but still preserving the
homogeneous analysis. On the other hand various methods use textons to
represent the texture, breaking texture down into its smallest unit. To tackle
this problem, we propose a method to identify texture patterns not small as
textons at distinct scales enhancing the separability among different types of
texture. We find sub patterns of texture according to the scale and then group
similar patterns for a more refined analysis. Tests were performed in four
static texture databases and one dynamic one. Results show that our method
provides better classification rate compared with conventional approaches both
in static and in dynamic texture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4539</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4539</id><created>2013-04-16</created><authors><author><keyname>Pai</keyname><forenames>Ganesh J.</forenames></author></authors><title>A Survey of Software Reliability Models</title><categories>cs.SE</categories><comments>12 pages, 1 table, 20 Equations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software reliability analysis is performed at various stages during the
process of engineering software as an attempt to evaluate if the software
reliability requirements have been (or might be) met. In this report, I present
a summary of some fundamental black-box and white-box software reliability
models. I also present some general shortcomings of these models and suggest
avenues for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4548</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4548</id><created>2013-04-16</created><authors><author><keyname>ElAarag</keyname><forenames>Hala</forenames></author><author><keyname>Bauschlicher</keyname><forenames>David</forenames></author><author><keyname>Bauschlicher</keyname><forenames>Steven</forenames></author></authors><title>System Architecture of HatterHealthConnect: An Integration of Body
  Sensor Networks and Social Networks to Improve Health Awareness</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Networks and Communications
  (IJCNC), Vol.5, No.2, March 2013, pp.1-22, DOI : 10.5121/ijcnc.2013.5201 1
  http://airccse.org/journal/ijc2013.html</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade, the demand for efficient healthcare monitoring has
increased and forced the health and wellness industry to embrace modern
technological advances. Body Sensor Networks, or BSNs, can remotely collect
users data and upload vital statistics to servers over the Internet. Advances
in wireless technologies such as cellular devices and Bluetooth increase the
mobility users experience while wearing a body sensor network. When connected
by the proper framework, BSNs can efficiently monitor and record data while
minimizing the energy expenditure of nodes in the BSN. Social networking sites
play a large role in the aggregation and sharing of data between many users.
Connecting a BSN to a social network creates the unique ability to share health
related data with other users through social interaction. In this research, we
present an integration of BSNs and social networks to establish a community
promoting well being and great social awareness. We present the system
architecture; both hardware and software, of a prototype implementation using
Zephyr HxM heart monitor, Intel-Shimmer EMG senor and a Samsung Captivate smart
phone. We provide implementation details for the design on the base station,
the database server and the Facebook application. We illustrate how the Android
application was designed with both functionality and user perspective in mind
that resulted in an easy to use system. This prototype can be used in multiple
health related applications based on the type of sensors used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4553</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4553</id><created>2013-04-16</created><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author></authors><title>A New Perspective on Vertex Connectivity</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge connectivity and vertex connectivity are two fundamental concepts in
graph theory. Although by now there is a good understanding of the structure of
graphs based on their edge connectivity, our knowledge in the case of vertex
connectivity is much more limited. An essential tool in capturing edge
connectivity are edge-disjoint spanning trees. The famous results of Tutte and
Nash-Williams show that a graph with edge connectivity $\lambda$ contains
$\floor{\lambda/2}$ edge-disjoint spanning trees.
  We present connected dominating set (CDS) partition and packing as tools that
are analogous to edge-disjoint spanning trees and that help us to better grasp
the structure of graphs based on their vertex connectivity. The objective of
the CDS partition problem is to partition the nodes of a graph into as many
connected dominating sets as possible. The CDS packing problem is the
corresponding fractional relaxation, where CDSs are allowed to overlap as long
as this is compensated by assigning appropriate weights. CDS partition and CDS
packing can be viewed as the counterparts of the well-studied edge-disjoint
spanning trees, focusing on vertex disjointedness rather than edge
disjointness.
  We constructively show that every $k$-vertex-connected graph with $n$ nodes
has a CDS packing of size $\Omega(k/\log n)$ and a CDS partition of size
$\Omega(k/\log^5 n)$. We prove that the $\Omega(k/\log n)$ CDS packing bound is
existentially optimal.
  Using CDS packing, we show that if vertices of a $k$-vertex-connected graph
are independently sampled with probability $p$, then the graph induced by the
sampled vertices has vertex connectivity $\tilde{\Omega}(kp^2)$. Moreover,
using our $\Omega(k/\log n)$ CDS packing, we get a store-and-forward broadcast
algorithm with optimal throughput in the networking model where in each round,
each node can send one bounded-size message to all its neighbors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4557</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4557</id><created>2013-04-16</created><authors><author><keyname>Rieg</keyname><forenames>Lionel</forenames><affiliation>LIP</affiliation></author></authors><title>Extracting Herbrand trees from Coq</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software certification aims at proving the correctness of programs but in
many cases, the use of external libraries allows only a conditional proof: it
depends on the assumption that the libraries meet their specifications. In
particular, a bug in these libraries might still impact the certified program.
In this case, the difficulty that arises is to isolate the defective library
function and provide a counter-example. In this paper, we show that this
problem can be logically formalized as the construction of a Herbrand tree for
a contradictory universal theory and address it. The solution we propose is
based on a proof of Herbrand's theorem in the proof assistant Coq. Classical
program extraction using Krivine's classical realizability then translates this
proof into a certified program that computes Herbrand trees. Using this tree
and calls to the library functions, we are able to determine which function is
defective and explicitly produce a counter-example to its specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4567</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4567</id><created>2013-04-16</created><updated>2014-02-17</updated><authors><author><keyname>Zamanighomi</keyname><forenames>Mahdi</forenames></author><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author></authors><title>Multiple-Antenna Interference Network with Receive Antenna Joint
  Processing and Real Interference Alignment</title><categories>cs.IT math.IT</categories><comments>27 pages, 2 figures. This paper was accepted in part to the ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the degrees of freedom (DoF) regions of constant coefficient
multiple antenna interference channels are investigated. First, we consider a
$K$-user Gaussian interference channel with $M_k$ antennas at transmitter $k$,
$1\le k\le K$, and $N_j$ antennas at receiver $j$, $1\le j\le K$, denoted as a
$(K,[M_k],[N_j])$ channel. Relying on a result of simultaneous Diophantine
approximation, a real interference alignment scheme with joint receive antenna
processing is developed. The scheme is used to obtain an achievable DoF region.
The proposed DoF region includes two previously known results as special cases,
namely 1) the total DoF of a $K$-user interference channel with $N$ antennas at
each node, $(K, [N], [N])$ channel, is $NK/2$; and 2) the total DoF of a $(K,
[M], [N])$ channel is at least $KMN/(M+N)$. We next explore
constant-coefficient interference networks with $K$ transmitters and $J$
receivers, all having $N$ antennas. Each transmitter emits an independent
message and each receiver requests an arbitrary subset of the messages.
Employing the novel joint receive antenna processing, the DoF region for this
set-up is obtained. We finally consider wireless X networks where each node is
allowed to have an arbitrary number of antennas. It is shown that the joint
receive antenna processing can be used to establish an achievable DoF region,
which is larger than what is possible with antenna splitting. As a special case
of the derived achievable DoF region for constant coefficient X network, the
total DoF of wireless X networks with the same number of antennas at all nodes
and with joint antenna processing is tight while the best inner bound based on
antenna splitting cannot meet the outer bound. Finally, we obtain a DoF region
outer bound based on the technique of transmitter grouping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4572</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4572</id><created>2013-04-16</created><authors><author><keyname>Vidakovic</keyname><forenames>Dragan</forenames></author><author><keyname>Nikolic</keyname><forenames>Olivera</forenames></author><author><keyname>Parezanovic</keyname><forenames>Dusko</forenames></author><author><keyname>Kaljevic</keyname><forenames>Jelena</forenames></author></authors><title>Joint operation in public key cryptography</title><categories>cs.CR</categories><comments>5 pages</comments><acm-class>D.4.6</acm-class><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 4, No.3, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We believe that there is no real data protection without our own tools.
Therefore, our permanent aim is to have more of our own codes. In order to
achieve that, it is necessary that a lot of young researchers become interested
in cryptography. We believe that the encoding of cryptographic algorithms is an
important step in that direction, and it is the main reason why in this paper
we present a software implementation of finding the inverse element, the
operation which is essentially related to both ECC (Elliptic Curve
Cryptography) and the RSA schemes of digital signature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4577</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4577</id><created>2013-04-16</created><updated>2014-10-22</updated><authors><author><keyname>Swenson</keyname><forenames>Brian</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Xavier</keyname><forenames>Joao</forenames></author></authors><title>Empirical Centroid Fictitious Play: An Approach For Distributed Learning
  In Multi-Agent Games</title><categories>math.OC cs.GT cs.SY</categories><comments>Submitted to the IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is concerned with distributed learning in large-scale games. The
well-known fictitious play (FP) algorithm is addressed, which, despite
theoretical convergence results, might be impractical to implement in
large-scale settings due to intense computation and communication requirements.
An adaptation of the FP algorithm, designated as the empirical centroid
fictitious play (ECFP), is presented. In ECFP players respond to the centroid
of all players' actions rather than track and respond to the individual actions
of every player. Convergence of the ECFP algorithm in terms of average
empirical frequency (a notion made precise in the paper) to a subset of the
Nash equilibria is proven under the assumption that the game is a potential
game with permutation invariant potential function. A more general formulation
of ECFP is then given (which subsumes FP as a special case) and convergence
results are given for the class of potential games. Furthermore, a distributed
formulation of the ECFP algorithm is presented, in which, players endowed with
a (possibly sparse) preassigned communication graph, engage in local,
non-strategic information exchange to eventually agree on a common equilibrium.
Convergence results are proven for the distributed ECFP algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4578</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4578</id><created>2013-04-16</created><updated>2013-10-30</updated><authors><author><keyname>Rossi</keyname><forenames>Marco</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander M.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Spatial Compressive Sensing for MIMO Radar</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2289875</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study compressive sensing in the spatial domain to achieve target
localization, specifically direction of arrival (DOA), using multiple-input
multiple-output (MIMO) radar. A sparse localization framework is proposed for a
MIMO array in which transmit and receive elements are placed at random. This
allows for a dramatic reduction in the number of elements needed, while still
attaining performance comparable to that of a filled (Nyquist) array. By
leveraging properties of structured random matrices, we develop a bound on the
coherence of the resulting measurement matrix, and obtain conditions under
which the measurement matrix satisfies the so-called isotropy property. The
coherence and isotropy concepts are used to establish uniform and non-uniform
recovery guarantees within the proposed spatial compressive sensing framework.
In particular, we show that non-uniform recovery is guaranteed if the product
of the number of transmit and receive elements, MN (which is also the number of
degrees of freedom), scales with K(log(G))^2, where K is the number of targets
and G is proportional to the array aperture and determines the angle
resolution. In contrast with a filled virtual MIMO array where the product MN
scales linearly with G, the logarithmic dependence on G in the proposed
framework supports the high-resolution provided by the virtual array aperture
while using a small number of MIMO radar elements. In the numerical results we
show that, in the proposed framework, compressive sensing recovery algorithms
are capable of better performance than classical methods, such as beamforming
and MUSIC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4602</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4602</id><created>2013-04-16</created><authors><author><keyname>Backstrom</keyname><forenames>Lars</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author></authors><title>Characterizing and curating conversation threads: Expansion, focus,
  volume, re-entry</title><categories>cs.SI physics.soc-ph</categories><acm-class>H.2.8</acm-class><journal-ref>Proceedings of WSDM 2013, pp. 13-22</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion threads form a central part of the experience on many Web sites,
including social networking sites such as Facebook and Google Plus and
knowledge creation sites such as Wikipedia. To help users manage the challenge
of allocating their attention among the discussions that are relevant to them,
there has been a growing need for the algorithmic curation of on-line
conversations --- the development of automated methods to select a subset of
discussions to present to a user.
  Here we consider two key sub-problems inherent in conversational curation:
length prediction --- predicting the number of comments a discussion thread
will receive --- and the novel task of re-entry prediction --- predicting
whether a user who has participated in a thread will later contribute another
comment to it. The first of these sub-problems arises in estimating how
interesting a thread is, in the sense of generating a lot of conversation; the
second can help determine whether users should be kept notified of the progress
of a thread to which they have already contributed. We develop and evaluate a
range of approaches for these tasks, based on an analysis of the network
structure and arrival pattern among the participants, as well as a novel
dichotomy in the structure of long threads. We find that for both tasks,
learning-based approaches using these sources of information yield improvements
for all the performance metrics we used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4610</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4610</id><created>2013-04-16</created><updated>2013-04-30</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author></authors><title>Spectral Compressed Sensing via Structured Matrix Completion</title><categories>cs.IT cs.LG math.IT math.NA stat.ML</categories><comments>accepted to International Conference on Machine Learning (ICML 2013)</comments><journal-ref>Journal of Machine Learning Research, W&amp;CP 28 (3) :414-422, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies the problem of recovering a spectrally sparse object from a
small number of time domain samples. Specifically, the object of interest with
ambient dimension $n$ is assumed to be a mixture of $r$ complex
multi-dimensional sinusoids, while the underlying frequencies can assume any
value in the unit disk. Conventional compressed sensing paradigms suffer from
the {\em basis mismatch} issue when imposing a discrete dictionary on the
Fourier representation. To address this problem, we develop a novel
nonparametric algorithm, called enhanced matrix completion (EMaC), based on
structured matrix completion. The algorithm starts by arranging the data into a
low-rank enhanced form with multi-fold Hankel structure, then attempts recovery
via nuclear norm minimization. Under mild incoherence conditions, EMaC allows
perfect recovery as soon as the number of samples exceeds the order of
$\mathcal{O}(r\log^{2} n)$. We also show that, in many instances, accurate
completion of a low-rank multi-fold Hankel matrix is possible when the number
of observed entries is proportional to the information theoretical limits
(except for a logarithmic gap). The robustness of EMaC against bounded noise
and its applicability to super resolution are further demonstrated by numerical
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4613</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4613</id><created>2013-04-16</created><authors><author><keyname>Lin</keyname><forenames>Bing-Rong</forenames></author><author><keyname>Wang</keyname><forenames>Ye</forenames></author><author><keyname>Rane</keyname><forenames>Shantanu</forenames></author></authors><title>On the Benefits of Sampling in Privacy Preserving Statistical Analysis
  on Distributed Databases</title><categories>cs.CR cs.DB cs.DS</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a problem where mutually untrusting curators possess portions of
a vertically partitioned database containing information about a set of
individuals. The goal is to enable an authorized party to obtain aggregate
(statistical) information from the database while protecting the privacy of the
individuals, which we formalize using Differential Privacy. This process can be
facilitated by an untrusted server that provides storage and processing
services but should not learn anything about the database. This work describes
a data release mechanism that employs Post Randomization (PRAM), encryption and
random sampling to maintain privacy, while allowing the authorized party to
conduct an accurate statistical analysis of the data. Encryption ensures that
the storage server obtains no information about the database, while PRAM and
sampling ensures individual privacy is maintained against the authorized party.
We characterize how much the composition of random sampling with PRAM increases
the differential privacy of system compared to using PRAM alone. We also
analyze the statistical utility of our system, by bounding the estimation error
- the expected l2-norm error between the true empirical distribution and the
estimated distribution - as a function of the number of samples, PRAM noise,
and other system parameters. Our analysis shows a tradeoff between increasing
PRAM noise versus decreasing the number of samples to maintain a desired level
of privacy, and we determine the optimal number of samples that balances this
tradeoff and maximizes the utility. In experimental simulations with the UCI
&quot;Adult Data Set&quot; and with synthetically generated data, we confirm that the
theoretically predicted optimal number of samples indeed achieves close to the
minimal empirical error, and that our analytical error bounds match well with
the empirical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4618</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4618</id><created>2013-04-16</created><authors><author><keyname>Shin</keyname><forenames>Junghwan</forenames></author><author><keyname>Kapoor</keyname><forenames>Sanjiv</forenames></author></authors><title>Auction Algorithm for Production Models</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show an auction-based algorithm to compute market equilibrium prices in a
production model, where consumers purchase items under separable nonlinear
utility concave functions which satisfy W.G.S(Weak Gross Substitutes);
producers produce items with multiple linear production constraints. Our
algorithm di?ers from previous approaches in that the prices are allowed to
both increase and decrease to handle changes in the production. This provides a
t^atonnement style algorithm which converges and provides a PTAS. The algorithm
can also be extended to arbitrary convex production regions and the
Arrow-Debreu model. The convergence is dependent on the behavior of the
marginal utility of the concave function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4619</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4619</id><created>2013-04-15</created><authors><author><keyname>Ghadirli</keyname><forenames>Hossein Movafegh</forenames></author><author><keyname>Rastgarpour</keyname><forenames>Maryam</forenames></author></authors><title>An Adaptive and Intelligent Tutor by Expert Systems for Mobile Devices</title><categories>cs.CY</categories><comments>8 pages, 3 figures, International Journal of Managing Public Sector
  Information and Communication Technologies(IJMPICT). arXiv admin note:
  substantial text overlap with arXiv:1304.4222</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Mobile Learning (M-Learning) is an emerging discipline in the area of
education and educational technology. So researchers are trying to optimize and
expanding its application in the field of education. The aim of this paper is
to investigate the role of mobile devices and expert systems in disseminating
and supporting the knowledge gained by intelligent tutors and to propose a
system based on integration of intelligent M-Learning with expert systems. It
acts as an intelligent tutor which can perform three processes - pre-test,
learning concept and post-test - according to characteristic of the learner.
The proposed system can improves the education efficiency highly as well as
decreases costs. As a result, every time and everywhere (ETEW) simple and cheap
learning would be provided via SMS, MMS and so on in this system. The global
intention of M-Learning is to make learning &quot;a way of being&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4621</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4621</id><created>2013-04-16</created><authors><author><keyname>Kaviani</keyname><forenames>Saeed</forenames></author><author><keyname>Krzymien</keyname><forenames>Witold A.</forenames></author></authors><title>Optimal Multiuser Zero-Forcing with Per-Antenna Power Constraints for
  Network MIMO Coordination</title><categories>cs.IT math.IT math.OC</categories><comments>14 pages, 8 figures</comments><journal-ref>published in EURASIP Journal on Wireless Communications and
  Networking, Volume 2011, Article ID 190461, 12 pages</journal-ref><doi>10.1155/2011/190461</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-cell multiple-input multiple-output (MIMO) coordinated
downlink transmission, also known as network MIMO, under per-antenna power
constraints. We investigate a simple multiuser zero-forcing (ZF) linear
precoding technique known as block diagonalization (BD) for network MIMO. The
optimal form of BD with per-antenna power constraints is proposed. It involves
a novel approach of optimizing the precoding matrices over the entire null
space of other users' transmissions.
  An iterative gradient descent method is derived by solving the dual of the
throughput maximization problem, which finds the optimal precoding matrices
globally and efficiently. The comprehensive simulations illustrate several
network MIMO coordination advantages when the optimal BD scheme is used. Its
achievable throughput is compared with the capacity region obtained through the
recently established duality concept under per-antenna power constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4624</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4624</id><created>2013-04-16</created><authors><author><keyname>Kaviani</keyname><forenames>Saeed</forenames></author><author><keyname>Krzymien</keyname><forenames>Witold A.</forenames></author></authors><title>Robust Joint Precoder and Equalizer Design in MIMO Communication Systems</title><categories>cs.IT math.IT math.OC</categories><comments>2 figures, 5 pages, conference</comments><journal-ref>Kaviani, S.; Krzymien, W.A., &quot;Robust joint precoder and equalizer
  design in MIMO communication systems,&quot; Wireless Communications and Networking
  Conference (WCNC), 2012 IEEE , vol., no., pp.277,282, 1-4 April 2012</journal-ref><doi>10.1109/WCNC.2012.6214273</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address joint design of robust precoder and equalizer in a MIMO
communication system using the minimization of weighted sum of mean square
errors. In addition to imperfect knowledge of channel state information, we
also account for inaccurate awareness of interference plus noise covariance
matrix and power shaping matrix. We follow the worst-case model for imperfect
knowledge of these matrices. First, we derive the worst-case values of these
matrices. Then, we transform the joint precoder and equalizer optimization
problem into a convex scalar optimization problem. Further, the solution to
this problem will be simplified to a depressed quartic equation, the
closed-form expressions for roots of which are known. Finally, we propose an
iterative algorithm to obtain the worst-case robust transceivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4626</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4626</id><created>2013-04-16</created><updated>2016-02-22</updated><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Panolan</keyname><forenames>Fahad</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Efficient Computation of Representative Sets with Applications in
  Parameterized and Exact Algorithms</title><categories>cs.DS cs.DM math.CO</categories><comments>61 pages</comments><msc-class>68R01</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give two algorithms computing representative families of linear and
uniform matroids and demonstrate how to use representative families for
designing single-exponential parameterized and exact exponential time
algorithms. The applications of our approach include
  - LONGEST DIRECTED CYCLE
  - MINIMUM EQUIVALENT GRAPH (MEG)
  - Algorithms on graphs of bounded treewidth
  -k-PATH, k-TREE, and more generally, k-SUBGRAPH ISOMORPHISM, where the
k-vertex pattern graph is of constant treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4627</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4627</id><created>2013-04-16</created><authors><author><keyname>Swindlehurst</keyname><forenames>Ali Fakoorian andA. Lee</forenames></author></authors><title>On the Optimality of Linear Precoding for Secrecy in the MIMO Broadcast
  Channel</title><categories>cs.IT math.IT</categories><comments>to appear IEEE JSAC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimality of linear precoding for the two-receiver
multiple-input multiple-output (MIMO) Gaussian broadcast channel (BC) with
confidential messages. Secret dirty-paper coding (SDPC) is optimal under an
input covariance constraint, but there is no computable secrecy capacity
expression for the general MIMO case under an average power constraint. In
principle, for this case, the secrecy capacity region could be found through an
exhaustive search over the set of all possible matrix power constraints.
Clearly, this search, coupled with the complexity of dirty-paper encoding and
decoding, motivates the consideration of low complexity linear precoding as an
alternative. We prove that for a two-user MIMO Gaussian BC under an input
covariance constraint, linear precoding is optimal and achieves the same
secrecy rate region as S-DPC if the input covariance constraint satisfies a
specific condition, and we characterize the corresponding optimal linear
precoders. We then use this result to derive a closed-form sub-optimal
algorithm based on linear precoding for an average power constraint. Numerical
results indicate that the secrecy rate region achieved by this algorithm is
close to that obtained by the optimal S-DPC approach with a search over all
suitable input covariance matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4633</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4633</id><created>2013-04-16</created><authors><author><keyname>Juba</keyname><forenames>Brendan</forenames></author></authors><title>PAC Quasi-automatizability of Resolution over Restricted Distributions</title><categories>cs.DS cs.LG cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider principled alternatives to unsupervised learning in data mining
by situating the learning task in the context of the subsequent analysis task.
Specifically, we consider a query-answering (hypothesis-testing) task: In the
combined task, we decide whether an input query formula is satisfied over a
background distribution by using input examples directly, rather than invoking
a two-stage process in which (i) rules over the distribution are learned by an
unsupervised learning algorithm and (ii) a reasoning algorithm decides whether
or not the query formula follows from the learned rules. In a previous work
(2013), we observed that the learning task could satisfy numerous desirable
criteria in this combined context -- effectively matching what could be
achieved by agnostic learning of CNFs from partial information -- that are not
known to be achievable directly. In this work, we show that likewise, there are
reasoning tasks that are achievable in such a combined context that are not
known to be achievable directly (and indeed, have been seriously conjectured to
be impossible, cf. (Alekhnovich and Razborov, 2008)). Namely, we test for a
resolution proof of the query formula of a given size in quasipolynomial time
(that is, &quot;quasi-automatizing&quot; resolution). The learning setting we consider is
a partial-information, restricted-distribution setting that generalizes
learning parities over the uniform distribution from partial information,
another task that is known not to be achievable directly in various models (cf.
(Ben-David and Dichterman, 1998) and (Michael, 2010)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4634</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4634</id><created>2013-04-16</created><authors><author><keyname>Torres</keyname><forenames>Leonardo</forenames></author><author><keyname>Sant'Anna</keyname><forenames>Sidnei J. S.</forenames></author><author><keyname>Freitas</keyname><forenames>Corina C.</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author></authors><title>Speckle Reduction in Polarimetric SAR Imagery with Stochastic Distances
  and Nonlocal Means</title><categories>cs.IT cs.CV cs.GR math.IT stat.AP stat.ML</categories><comments>Accepted for publication in Pattern Recognition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a technique for reducing speckle in Polarimetric
Synthetic Aperture Radar (PolSAR) imagery using Nonlocal Means and a
statistical test based on stochastic divergences. The main objective is to
select homogeneous pixels in the filtering area through statistical tests
between distributions. This proposal uses the complex Wishart model to describe
PolSAR data, but the technique can be extended to other models. The weights of
the location-variant linear filter are function of the p-values of tests which
verify the hypothesis that two samples come from the same distribution and,
therefore, can be used to compute a local mean. The test stems from the family
of (h-phi) divergences which originated in Information Theory. This novel
technique was compared with the Boxcar, Refined Lee and IDAN filters. Image
quality assessment methods on simulated and real data are employed to validate
the performance of this approach. We show that the proposed filter also
enhances the polarimetric entropy and preserves the scattering information of
the targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4636</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4636</id><created>2013-04-16</created><updated>2013-07-26</updated><authors><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author><author><keyname>Zhang</keyname><forenames>Qin</forenames></author></authors><title>When Distributed Computation is Communication Expensive</title><categories>cs.DS</categories><comments>A few minor modifications are made. The new version also has a more
  appropriate title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a number of fundamental statistical and graph problems in the
message-passing model, where we have $k$ machines (sites), each holding a piece
of data, and the machines want to jointly solve a problem defined on the union
of the $k$ data sets. The communication is point-to-point, and the goal is to
minimize the total communication among the $k$ machines. This model captures
all point-to-point distributed computational models with respect to minimizing
communication costs. Our analysis shows that exact computation of many
statistical and graph problems in this distributed setting requires a
prohibitively large amount of communication, and often one cannot improve upon
the communication of the simple protocol in which all machines send their data
to a centralized server. Thus, in order to obtain protocols that are
communication-efficient, one has to allow approximation, or investigate the
distribution or layout of the data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4642</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4642</id><created>2013-04-16</created><authors><author><keyname>Childs</keyname><forenames>Andrew M.</forenames></author><author><keyname>Kothari</keyname><forenames>Robin</forenames></author><author><keyname>Ozols</keyname><forenames>Maris</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author></authors><title>Easy and hard functions for the Boolean hidden shift problem</title><categories>quant-ph cs.CC cs.LG</categories><comments>29 pages, 2 figures</comments><journal-ref>Proceedings of TQC 2013, LIPIcs, vol. 22, pp. 50-79, ISBN
  978-3-939897-55-2 (2013)</journal-ref><doi>10.4230/LIPIcs.TQC.2013.50</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the quantum query complexity of the Boolean hidden shift problem.
Given oracle access to f(x+s) for a known Boolean function f, the task is to
determine the n-bit string s. The quantum query complexity of this problem
depends strongly on f. We demonstrate that the easiest instances of this
problem correspond to bent functions, in the sense that an exact one-query
algorithm exists if and only if the function is bent. We partially characterize
the hardest instances, which include delta functions. Moreover, we show that
the problem is easy for random functions, since two queries suffice. Our
algorithm for random functions is based on performing the pretty good
measurement on several copies of a certain state; its analysis relies on the
Fourier transform. We also use this approach to improve the quantum rejection
sampling approach to the Boolean hidden shift problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4648</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4648</id><created>2013-04-16</created><authors><author><keyname>Zhang</keyname><forenames>Guanghui</forenames></author><author><keyname>Chen</keyname><forenames>Bocong</forenames></author></authors><title>Construction of Self-dual Codes over $F_p+vF_p$</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><msc-class>94B05, 94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we determine all self-dual codes over $F_p+vF_p$ ($v^2=v$) in
terms of self-dual codes over the finite field $F_p$ and give an explicit
construction for self-dual codes over $F_p+vF_p$, where $p$ is a prime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4651</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4651</id><created>2013-04-16</created><authors><author><keyname>Talati</keyname><forenames>Gunjan</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Generalized Public Key Transformations with Side Information</title><categories>cs.CR</categories><comments>15 pages. arXiv admin note: text overlap with arXiv:1304.4273</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents results on generalized public key cryptography with
exponentials modulo primes and composite numbers where the mapping is not
one-to-one and the uniqueness is achieved by additional side information. Such
transformations may be used for oblivious transfer and generate events of
specific probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4652</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4652</id><created>2013-04-16</created><authors><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author><author><keyname>Raheja</keyname><forenames>Jagdish L.</forenames></author></authors><title>A Health Monitoring System for Elder and Sick Persons</title><categories>cs.CV cs.HC</categories><journal-ref>International Journal of Computer Theory and Engineering, Vol. 5,
  No. 3, June 2013</journal-ref><doi>10.7763/IJCTE.2013.V5.723</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a vision based health monitoring system which would be
very easy in use and deployment. Elder and sick people who are not able to talk
or walk they are dependent on other human beings for their daily needs and need
continuous monitoring. The developed system provides facility to the sick or
elder person to describe his or her need to their caretaker in lingual
description by showing particular hand gesture with the developed system. This
system uses fingertip detection technique for gesture extraction and artificial
neural network for gesture classification and recognition. The system is able
to work in different light conditions and can be connected to different devices
to announce users need on a distant location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4657</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4657</id><created>2013-04-16</created><authors><author><keyname>Koutra</keyname><forenames>Danai</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>DELTACON: A Principled Massive-Graph Similarity Function</title><categories>cs.SI physics.soc-ph</categories><comments>2013 SIAM International Conference in Data Mining (SDM)</comments><acm-class>E.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How much did a network change since yesterday? How different is the wiring
between Bob's brain (a left-handed male) and Alice's brain (a right-handed
female)? Graph similarity with known node correspondence, i.e. the detection of
changes in the connectivity of graphs, arises in numerous settings. In this
work, we formally state the axioms and desired properties of the graph
similarity functions, and evaluate when state-of-the-art methods fail to detect
crucial connectivity changes in graphs. We propose DeltaCon, a principled,
intuitive, and scalable algorithm that assesses the similarity between two
graphs on the same nodes (e.g. employees of a company, customers of a mobile
carrier). Experiments on various synthetic and real graphs showcase the
advantages of our method over existing similarity measures. Finally, we employ
DeltaCon to real applications: (a) we classify people to groups of high and low
creativity based on their brain connectivity graphs, and (b) do temporal
anomaly detection in the who-emails-whom Enron graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4658</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4658</id><created>2013-04-16</created><updated>2014-04-11</updated><authors><author><keyname>Lofgren</keyname><forenames>Peter</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author></authors><title>Personalized PageRank to a Target Node</title><categories>cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalalized PageRank uses random walks to determine the importance or
authority of nodes in a graph from the point of view of a given source node.
Much past work has considered how to compute personalized PageRank from a given
source node to other nodes. In this work we consider the problem of computing
personalized PageRanks to a given target node from all source nodes. This
problem can be interpreted as finding who supports the target or who is
interested in the target.
  We present an efficient algorithm for computing personalized PageRank to a
given target up to any given accuracy. We give a simple analysis of our
algorithm's running time in both the average case and the parameterized
worst-case. We show that for any graph with $n$ nodes and $m$ edges, if the
target node is randomly chosen and the teleport probability $\alpha$ is given,
the algorithm will compute a result with $\epsilon$ error in time
$O\left(\frac{1}{\alpha \epsilon} \left(\frac{m}{n} + \log(n)\right)\right)$.
This is much faster than the previously proposed method of computing
personalized PageRank separately from every source node, and it is comparable
to the cost of computing personalized PageRank from a single source. We present
results from experiments on the Twitter graph which show that the constant
factors in our running time analysis are small and our algorithm is efficient
in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4661</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4661</id><created>2013-04-16</created><authors><author><keyname>Akiba</keyname><forenames>Takuya</forenames></author><author><keyname>Iwata</keyname><forenames>Yoichi</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Fast Exact Shortest-Path Distance Queries on Large Networks by Pruned
  Landmark Labeling</title><categories>cs.DS cs.DB</categories><comments>To appear in SIGMOD 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new exact method for shortest-path distance queries on
large-scale networks. Our method precomputes distance labels for vertices by
performing a breadth-first search from every vertex. Seemingly too obvious and
too inefficient at first glance, the key ingredient introduced here is pruning
during breadth-first searches. While we can still answer the correct distance
for any pair of vertices from the labels, it surprisingly reduces the search
space and sizes of labels. Moreover, we show that we can perform 32 or 64
breadth-first searches simultaneously exploiting bitwise operations. We
experimentally demonstrate that the combination of these two techniques is
efficient and robust on various kinds of large-scale real-world networks. In
particular, our method can handle social networks and web graphs with hundreds
of millions of edges, which are two orders of magnitude larger than the limits
of previous exact methods, with comparable query time to those of previous
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4662</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4662</id><created>2013-04-16</created><authors><author><keyname>Raheja</keyname><forenames>J. L.</forenames></author><author><keyname>Chaudhary</keyname><forenames>A.</forenames></author><author><keyname>Singal</keyname><forenames>K</forenames></author></authors><title>Tracking of Fingertips and Centres of Palm using KINECT</title><categories>cs.CV</categories><comments>4 page</comments><journal-ref>In proceedings of the 3rd IEEE International Conference on
  Computational Intelligence, Modelling and Simulation, Malaysia, 20-22 Sep,
  2011, pp. 248-252</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Hand Gesture is a popular way to interact or control machines and it has been
implemented in many applications. The geometry of hand is such that it is hard
to construct in virtual environment and control the joints but the
functionality and DOF encourage researchers to make a hand like instrument.
This paper presents a novel method for fingertips detection and centres of
palms detection distinctly for both hands using MS KINECT in 3D from the input
image. KINECT facilitates us by providing the depth information of foreground
objects. The hands were segmented using the depth vector and centres of palms
were detected using distance transformation on inverse image. This result would
be used to feed the inputs to the robotic hands to emulate human hands
operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4664</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4664</id><created>2013-04-16</created><authors><author><keyname>Chaudhary</keyname><forenames>A.</forenames></author><author><keyname>Verma</keyname><forenames>B. K.</forenames></author><author><keyname>Raheja</keyname><forenames>J. L.</forenames></author></authors><title>Product line Development Architectural Model</title><categories>cs.SE</categories><comments>4 pages</comments><journal-ref>In proceedings of the 3rd IEEE International Conference on
  Computer Science and Information Technology, China, 9-11 July, 2010,
  pp.749-753</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Products with new features need to be introduced on the market in a rapid
pace and organizations need to speed up their development process. The ordinary
way to develop products, one at a time, is not time efficient enough and is
costly. Reuse has been suggested as a solution, but to achieve effective reuse
within an organization a planned and proactive effort must be used. Product
lines are the most promising technique and it increases productivity and
software quality and decreases time-to-market. This paper describes the
architecture of product line engineering process and also addresses what the
design issues of product line architecture are and how a UML profile looks like
for a product line by referring to the basic aspects of a case study,
CelsiusTech in its Naval Product Line, SS2000.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4666</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4666</id><created>2013-04-16</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Ruyet</keyname><forenames>D. Le</forenames></author></authors><title>Multi-Branch MMSE Decision Feedback Detection Algorithms with Error
  Propagation Mitigation for Multi-Antenna Systems</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><journal-ref>ICASSP 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose novel decision feedback (DF) detection algorithms
with error propagation mitigation capabilities for multi-input multi-output
(MIMO) spatial multiplexing systems based on multiple processing branches. The
novel strategies for detection exploit different patterns, orderings and
constraints for the design of the feedforward and feedback filters. We present
constrained minimum mean-squared error (MMSE) filters designed with constraints
on the shape and magnitude of the feedback filters for the multi-branch MIMO
receivers and show that the proposed MMSE design does not require a significant
additional complexity over the single-branch MMSE design. The proposed
multi-branch MMSE DF detectors are compared with several existing detectors and
are shown to achieve a performance close to the optimal maximum likelihood
detector while requiring significantly lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4677</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4677</id><created>2013-04-16</created><authors><author><keyname>Gobithasan</keyname></author><author><keyname>R.</keyname></author><author><keyname>Norziah</keyname></author><author><keyname>O.</keyname></author><author><keyname>Jamaludin</keyname></author><author><keyname>A</keyname><forenames>M.</forenames></author></authors><title>Developing a General algorithm for Ball Curve with GC2</title><categories>cs.CG</categories><journal-ref>2005 Proceedings of the 2nd International Conference on Research
  and Education in Mathematics (ICREM 2), May 25th- 26th 2005, Serdang,
  Malaysia, Pg. 526-531</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper dwells in developing a general algorithm for constructing a
piecewise Ball Curve with curvature continuity (GC2). The proposed algorithm
requires GC2 data in which the designer must define unit tangent vectors and
signed curvatures at each interpolating points. As a numerical example, a vase
is constructed using GC2 piecewise Ball Curve
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4679</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4679</id><created>2013-04-17</created><authors><author><keyname>Hu</keyname><forenames>Huiyi</forenames></author><author><keyname>Laurent</keyname><forenames>Thomas</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Bertozzi</keyname><forenames>Andrea L.</forenames></author></authors><title>A Method Based on Total Variation for Network Modularity Optimization
  using the MBO Scheme</title><categories>cs.SI math.OC physics.soc-ph</categories><comments>23 pages</comments><msc-class>62H30, 91C20, 91D30, 94C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of network structure is pervasive in sociology, biology, computer
science, and many other disciplines. One of the most important areas of network
science is the algorithmic detection of cohesive groups of nodes called
&quot;communities&quot;. One popular approach to find communities is to maximize a
quality function known as {\em modularity} to achieve some sort of optimal
clustering of nodes. In this paper, we interpret the modularity function from a
novel perspective: we reformulate modularity optimization as a minimization
problem of an energy functional that consists of a total variation term and an
$\ell_2$ balance term. By employing numerical techniques from image processing
and $\ell_1$ compressive sensing -- such as convex splitting and the
Merriman-Bence-Osher (MBO) scheme -- we develop a variational algorithm for the
minimization problem. We present our computational results using both synthetic
benchmark networks and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4680</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4680</id><created>2013-04-17</created><updated>2013-04-17</updated><authors><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>A New Analysis of Compressive Sensing by Stochastic Proximal Gradient
  Descent</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this manuscript, we analyze the sparse signal recovery (compressive
sensing) problem from the perspective of convex optimization by stochastic
proximal gradient descent. This view allows us to significantly simplify the
recovery analysis of compressive sensing. More importantly, it leads to an
efficient optimization algorithm for solving the regularized optimization
problem related to the sparse recovery problem. Compared to the existing
approaches, there are two advantages of the proposed algorithm. First, it
enjoys a geometric convergence rate and therefore is computationally efficient.
Second, it guarantees that the support set of any intermediate solution
generated by the proposed algorithm is concentrated on the support set of the
optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4682</identifier>
 <datestamp>2013-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4682</id><created>2013-04-17</created><updated>2013-05-10</updated><authors><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Gao</keyname><forenames>Haoyu</forenames></author><author><keyname>Yang</keyname><forenames>Mingmin</forenames></author><author><keyname>Guan</keyname><forenames>Wanqiu</forenames></author><author><keyname>Ma</keyname><forenames>Haixin</forenames></author><author><keyname>Qian</keyname><forenames>Weining</forenames></author><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author><author><keyname>Yang</keyname><forenames>Xiaoguang</forenames></author></authors><title>What are Chinese Talking about in Hot Weibos?</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SinaWeibo is a Twitter-like social network service emerging in China in
recent years. People can post weibos (microblogs) and communicate with others
on it. Based on a dataset of 650 million weibos from August 2009 to January
2012 crawled from APIs of SinaWeibo, we study the hot ones that have been
reposted for at least 1000 times. We find that hot weibos can be roughly
classified into eight categories, i.e. Entertainment &amp; Fashion, Hot Social
Events, Leisure &amp; Mood, Life &amp; Health, Seeking for Help, Sales Promotion,
Fengshui &amp; Fortune and Deleted Weibos. In particular, Leisure &amp; Mood and Hot
Social Events account for almost 65% of all the hot weibos. This reflects very
well the fundamental dual-structure of the current society of China: On the one
hand, economy has made a great progress and quite a part of people are now
living a relatively prosperous and fairly easy life. On the other hand, there
still exist quite a lot of serious social problems, such as government
corruptions and environmental pollutions. It is also shown that users' posting
and reposting behaviors are greatly affected by their identity factors (gender,
verification status, and regional location). For instance, (1) Two thirds of
the hot weibos are created by male users. (2) Although verified users account
for only 0.1% in SinaWeibo, 46.5% of the hot weibos are contributed by them.
Very interestingly, 39.2% are written by SPA users. A more or less pathetic
fact is that only 14.4% of the hot weibos are created by grassroots (individual
users that are neither SPA nor verified). (3) Users from different areas of
China have distinct posting and reposting behaviors which usually reflect very
their local cultures. Homophily is also examined for people's reposting
behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4691</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4691</id><created>2013-04-17</created><authors><author><keyname>Khovanova</keyname><forenames>Tanya</forenames></author><author><keyname>Scully</keyname><forenames>Ziv</forenames></author></authors><title>Efficient Calculation of Determinants of Symbolic Matrices with Many
  Variables</title><categories>cs.SC</categories><comments>9 pages</comments><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient matrix determinant calculations have been studied since the 19th
century. Computers expand the range of determinants that are practically
calculable to include matrices with symbolic entries. However, the fastest
determinant algorithms for numerical matrices are often not the fastest for
symbolic matrices with many variables. We compare the performance of two
algorithms, fraction-free Gaussian elimination and minor expansion, on symbolic
matrices with many variables. We show that, under a simplified theoretical
model, minor expansion is faster in most situations. We then propose
optimizations for minor expansion and demonstrate their effectiveness with
empirical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4693</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4693</id><created>2013-04-17</created><updated>2014-09-29</updated><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Structured Lattice Codes for Some Two-User Gaussian Networks with
  Cognition, Coordination and Two Hops</title><categories>cs.IT math.IT</categories><comments>revision for IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a number of two-user interference networks with multiple-antenna
transmitters/receivers, transmitter side information in the form of linear
combinations (over finite-field) of the information messages, and two-hop
relaying. We start with a Cognitive Interference Channel (CIC) where one of the
transmitters (non-cognitive) has knowledge of a rank-1 linear combination of
the two information messages, while the other transmitter (cognitive) has
access to a rank-2 linear combination of the same messages. This is referred to
as the Network-Coded CIC, since such linear combination may be the result of
some random linear network coding scheme implemented in the backbone wired
network. For such channel we develop an achievable region based on a few novel
concepts: Precoded Compute and Forward (PCoF) with Channel Integer Alignment
(CIA), combined with standard Dirty-Paper Coding. We also develop a capacity
region outer bound and find the sum symmetric GDoF of the Network-Coded CIC.
Through the GDoF characterization, we show that knowing &quot;mixed data&quot; (linear
combinations of the information messages) provides an unbounded spectral
efficiency gain over the classical CIC counterpart, if the ratio of SNR to INR
is larger than certain threshold. Then, we consider a Gaussian relay network
having the two-user MIMO IC as the main building block. We use PCoF with CIA to
convert the MIMO IC into a deterministic finite-field IC. Then, we use a linear
precoding scheme over the finite-field to eliminate interference in the
finite-field domain. Using this unified approach, we characterize the symmetric
sum rate of the two-user MIMO IC with coordination, cognition, and two-hops. We
also provide finite-SNR results which show that the proposed coding schemes are
competitive against state of the art interference avoidance based on orthogonal
access, for Rayleigh fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4704</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4704</id><created>2013-04-17</created><updated>2013-07-25</updated><authors><author><keyname>Carlson</keyname><forenames>Jean M.</forenames></author><author><keyname>Alderson</keyname><forenames>David L.</forenames></author><author><keyname>Stromberg</keyname><forenames>Sean P.</forenames></author><author><keyname>Bassett</keyname><forenames>Danielle S.</forenames></author><author><keyname>Craparo</keyname><forenames>Emily M.</forenames></author><author><keyname>Gutierrez-Villarreal</keyname><forenames>Francisco</forenames></author><author><keyname>Otani</keyname><forenames>Thomas</forenames></author></authors><title>Measuring and Modeling Behavioral Decision Dynamics in Collective
  Evacuation</title><categories>physics.soc-ph cs.SI</categories><comments>Approved for public release; distribution is unlimited</comments><doi>10.1371/journal.pone.0087380</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying and quantifying factors influencing human decision making remains
an outstanding challenge, impacting the performance and predictability of
social and technological systems. In many cases, system failures are traced to
human factors including congestion, overload, miscommunication, and delays.
Here we report results of a behavioral network science experiment, targeting
decision making in a natural disaster. In each scenario, individuals are faced
with a forced &quot;go&quot; versus &quot;no go&quot; evacuation decision, based on information
available on competing broadcast and peer-to-peer sources. In this controlled
setting, all actions and observations are recorded prior to the decision,
enabling development of a quantitative decision making model that accounts for
the disaster likelihood, severity, and temporal urgency, as well as competition
between networked individuals for limited emergency resources. Individual
differences in behavior within this social setting are correlated with
individual differences in inherent risk attitudes, as measured by standard
psychological assessments. Identification of robust methods for quantifying
human decisions in the face of risk has implications for policy in disasters
and other threat scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4711</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4711</id><created>2013-04-17</created><authors><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author><author><keyname>Gupta</keyname><forenames>Ankur</forenames></author></authors><title>Automated Switching System for Skin Pixel Segmentation in Varied
  Lighting</title><categories>cs.CV</categories><comments>6 pages</comments><journal-ref>19th IEEE International Conference on Mechatronics and Machine
  Vision in Practice, Auckland, New Zealand, 28-30 Nov, 2012, pp. 26-31</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Computer Vision, colour-based spatial techniquesoften assume a static skin
colour model. However, skin colour perceived by a camera can change when
lighting changes. In common real environment multiple light sources impinge on
the skin. Moreover, detection techniques may vary when the image under study is
taken under different lighting condition than the one that was earlier under
consideration. Therefore, for robust skin pixel detection, a dynamic skin
colour model that can cope with the changes must be employed. This paper shows
that skin pixel detection in a digital colour image can be significantly
improved by employing automated colour space switching methods. In the root of
the switching technique which is employed in this study, lies the statistical
mean of value of the skin pixels in the image which in turn has been derived
from the Value, measures as a third component of the HSV. The study is based on
experimentations on a set of images where capture time conditions varying from
highly illuminated to almost dark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4731</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4731</id><created>2013-04-17</created><authors><author><keyname>Martin-Hernandez</keyname><forenames>J.</forenames></author><author><keyname>Wang</keyname><forenames>H.</forenames></author><author><keyname>Van Mieghem</keyname><forenames>P.</forenames></author><author><keyname>D'Agostino</keyname><forenames>G.</forenames></author></authors><title>On Synchronization of Interdependent Networks</title><categories>cs.SY math.OC nlin.AO</categories><comments>27 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the synchronization of diffusively-coupled systems on
networks strongly depends on the network topology. In particular, the so-called
algebraic connectivity $\mu_{N-1}$, or the smallest non-zero eigenvalue of the
discrete Laplacian operator plays a crucial role on synchronization, graph
partitioning, and network robustness. In our study, synchronization is placed
in the general context of networks-of-networks, where single network models are
replaced by a more realistic hierarchy of interdependent networks. The present
work shows, analytically and numerically, how the algebraic connectivity
experiences sharp transitions after the addition of sufficient links among
interdependent networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4738</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4738</id><created>2013-04-17</created><authors><author><keyname>Hor&#xe1;&#x10d;ek</keyname><forenames>Jaroslav</forenames></author><author><keyname>Hlad&#xed;k</keyname><forenames>Milan</forenames></author></authors><title>Computing Enclosures of Overdetermined Interval Linear Systems</title><categories>cs.NA</categories><comments>Presented at SCAN 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers special types of interval linear systems - overdetermined
systems. Simply said these systems have more equations than variables. The
solution set of an interval linear system is a collection of all solutions of
all instances of an interval system. By the instance we mean a point real
system that emerges when we independently choose a real number from each
interval coefficient of the interval system. Enclosing the solution set of
these systems is in some ways more difficult than for square systems. The main
goal of this work is to present various methods for solving overdetermined
interval linear systems. We would like to present them in an understandable way
even for nonspecialists in a field of linear systems. The second goal is a
numerical comparison of all the methods on random interval linear systems
regarding widths of enclosures, computation times and other special properties
of methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4750</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4750</id><created>2013-04-17</created><authors><author><keyname>Coudert</keyname><forenames>David</forenames><affiliation>Inria Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Hogie</keyname><forenames>Luc</forenames><affiliation>Inria Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Lancin</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>Inria Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Papadimitriou</keyname><forenames>Dimitri</forenames><affiliation>Inria Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>P&#xe9;rennes</keyname><forenames>St&#xe9;phane</forenames><affiliation>Inria Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Tahiri</keyname><forenames>Issam</forenames><affiliation>Inria Sophia Antipolis / Laboratoire I3S</affiliation></author></authors><title>Feasibility study on distributed simulations of BGP</title><categories>cs.NI</categories><proxy>ccsd</proxy><report-no>RR-8283</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Autonomous System (AS)-level topology of the Internet that currently
comprises 40k ASs, is growing at a rate of about 10% per year. In these
conditions, Border Gateway Protocol (BGP), the inter-domain routing protocol of
the Internet starts to show its limits, among others in terms of the number of
routing table entries it can dynamically process and control. To overcome this
challenging situation, the design but also the evaluation of alternative
dynamic routing models and their comparison with BGP shall be performed by
means of simulation. For this purpose, DRMSim, a Dynamic Routing Model
Simulator, was developed that provides the means for large-scale simulations of
various routing models including BGP. By means of this discrete-event
simulator, execution of path-vector routing, e.g. BGP, and other compact
routing models have been successfully performed on network topologies
comprising more than ten thousand (abstract) nodes. However, to simulate
dynamic routing schemes like BGP, DRMSim needs enhancements to support current
Internet size (40k ASs) and even more by considering its evolution (up to 100k
ASs). This paper proposes a feasibility study of the extension of DRMSim so as
to support the Distributed Parallel Discrete Event paradigm. We first detail
the possible distribution models and their associated communication overhead.
Then, we analyze the communication overhead of such a distributed simulator by
executing BGP on a partitioned topology according to different scenarios.
Finally, we conclude on the feasibility of such a simulator by computing the
expected additional time required by a distributed simulation of BGP compared
to its sequential simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4758</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4758</id><created>2013-04-17</created><updated>2013-12-30</updated><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author><author><keyname>de Leeuw</keyname><forenames>Karl</forenames></author></authors><title>Bitcoin and Beyond: Exclusively Informational Monies</title><categories>cs.CY cs.CR</categories><comments>82 pages. Revision of v2: the Paragraph on monopresence and
  pseudomonopresence has been improved and extended; the paragraph on units for
  monies of account has been extended; several minor clarifications have been
  included; 8 additional references were added; improvements were made of small
  errors throughout the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The famous new money Bitcoin is classified as a technical informational money
(TIM). Besides introducing the idea of a TIM, a more extreme notion of
informational money will be developed: exclusively informational money (EXIM).
  The informational coins (INCOs) of an EXIM can be in control of an agent but
are not owned by any agent. INCOs of an EXIM cannot be stolen, but they can be
lost, or thrown away. The difference between an EXIM and a TIM shows up when
considering a user perspective on security matters. Security for an EXIM user
is discussed in substantial detail, with the remarkable conclusion that
computer security (security models, access control, user names, passwords,
firewalls etc.) is not always essential for an EXIM, while the application of
cryptography based information security is unavoidable for the use of an EXIM.
  Bitcoin seems to meet the criteria of an EXIM, but the assertion that
&quot;Bitcoin is an EXIM&quot;, might also be considered problematic. As a thought
experiment we will contemplate Bitguilder, a hypothetical copy of Bitcoin that
qualifies as an EXIM.
  A business ethics assessment of Bitcoin is made which reveals a number of
worries. By combining Bitguilder with a so-called technical informational
near-money (TINM) a dual money system, having two units with a fluctuating
rate, may be obtained. It seems that a dual money can remedy some, but not all,
of the ethical worries that arise when contemplating Bitcoin after
hypothetically having become a dominant form of money.
  The contributions that Bitcoin's designers can potentially make to the
evolution of EXIMs and TIMs is analyzed in terms of the update of the portfolio
of money related natural kinds that comes with Bitcoin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4765</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4765</id><created>2013-04-17</created><authors><author><keyname>Hichri</keyname><forenames>Soumaya</forenames></author><author><keyname>Benzarti</keyname><forenames>Faouzi</forenames></author><author><keyname>Amiri</keyname><forenames>Hamid</forenames></author></authors><title>Robust Noise Filtering in Image Sequences</title><categories>cs.CV</categories><comments>5 pages</comments><journal-ref>International Journal of Computer Applications Volume 50 No.18,
  July 2012, ISSN 0975-8887</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image sequences filtering have recently become a very important technical
problem especially with the advent of new technology in multimedia and video
systems applications. Often image sequences are corrupted by some amount of
noise introduced by the image sensor and therefore inherently present in the
imaging process. The main problem in the image sequences is how to deal with
spatio-temporal and non stationary signals. In this paper, we propose a robust
method for noise removal of image sequence based on coupled spatial and
temporal anisotropic diffusion. The idea is to achieve an adaptive smoothing in
both spatial and temporal directions, by solving a nonlinear diffusion
equation. This allows removing noise while preserving all spatial and temporal
discontinuities
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4778</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4778</id><created>2013-04-17</created><updated>2014-07-22</updated><authors><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Alishahi</keyname><forenames>Kasra</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author></authors><title>Finite-Length Scaling of Polar Codes</title><categories>cs.IT math.IT</categories><comments>In IEEE Transactions on Information Theory, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a binary-input memoryless output-symmetric channel $W$. Such a
channel has a capacity, call it $I(W)$, and for any $R&lt;I(W)$ and strictly
positive constant $P_{\rm e}$ we know that we can construct a coding scheme
that allows transmission at rate $R$ with an error probability not exceeding
$P_{\rm e}$. Assume now that we let the rate $R$ tend to $I(W)$ and we ask how
we have to &quot;scale&quot; the blocklength $N$ in order to keep the error probability
fixed to $P_{\rm e}$. We refer to this as the &quot;finite-length scaling&quot; behavior.
This question was addressed by Strassen as well as Polyanskiy, Poor and Verdu,
and the result is that $N$ must grow at least as the square of the reciprocal
of $I(W)-R$.
  Polar codes are optimal in the sense that they achieve capacity. In this
paper, we are asking to what degree they are also optimal in terms of their
finite-length behavior. Our approach is based on analyzing the dynamics of the
un-polarized channels. The main results of this paper can be summarized as
follows. Consider the sum of Bhattacharyya parameters of sub-channels chosen
(by the polar coding scheme) to transmit information. If we require this sum to
be smaller than a given value $P_{\rm e}&gt;0$, then the required block-length $N$
scales in terms of the rate $R &lt; I(W)$ as $N \geq
\frac{\alpha}{(I(W)-R)^{\underline{\mu}}}$, where $\alpha$ is a positive
constant that depends on $P_{\rm e}$ and $I(W)$, and $\underline{\mu} = 3.579$.
Also, we show that with the same requirement on the sum of Bhattacharyya
parameters, the block-length scales in terms of the rate like $N \leq
\frac{\beta}{(I(W)-R)^{\overline{\mu}}}$, where $\beta$ is a constant that
depends on $P_{\rm e}$ and $I(W)$, and $\overline{\mu}=6$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4795</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4795</id><created>2013-04-17</created><authors><author><keyname>Chen</keyname><forenames>Shixi</forenames></author><author><keyname>Zhou</keyname><forenames>Shuigeng</forenames></author></authors><title>Recursive Mechanism: Towards Node Differential Privacy and Unrestricted
  Joins [Full Version, Draft 0.1]</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing studies on differential privacy mainly consider aggregation on data
sets where each entry corresponds to a particular participant to be protected.
In many situations, a user may pose a relational algebra query on a sensitive
database, and desires differentially private aggregation on the result of the
query. However, no known work is capable to release this kind of aggregation
when the query contains unrestricted join operations. This severely limits the
applications of existing differential privacy techniques because many data
analysis tasks require unrestricted joins. One example is subgraph counting on
a graph. Existing methods for differentially private subgraph counting address
only edge differential privacy and are subject to very simple subgraphs. Before
this work, whether any nontrivial graph statistics can be released with
reasonable accuracy under node differential privacy is still an open problem.
  In this paper, we propose a novel differentially private mechanism to release
an approximation to a linear statistic of the result of some positive
relational algebra calculation over a sensitive database. Unrestricted joins
are supported in our mechanism. The error bound of the approximate answer is
roughly proportional to the \emph{empirical sensitivity} of the query --- a new
notion that measures the maximum possible change to the query answer when a
participant withdraws its data from the sensitive database. For subgraph
counting, our mechanism provides the first solution to achieve node
differential privacy, for any kind of subgraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4806</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4806</id><created>2013-04-17</created><updated>2013-07-22</updated><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author></authors><title>Unsupervised model-free representation learning</title><categories>cs.LG stat.ML</categories><comments>Appears in parts in the proceedings of ISIT'13 and ALT'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous control and learning problems face the situation where sequences of
high-dimensional highly dependent data are available, but no or little feedback
is provided to the learner. To address this issue, we formulate the following
problem. Given a series of observations X_0,...,X_n coming from a large
(high-dimensional) space X, find a representation function f mapping X to a
finite space Y such that the series f(X_0),...,f(X_n) preserve as much
information as possible about the original time-series dependence in
X_0,...,X_n. We show that, for stationary time series, the function f can be
selected as the one maximizing the time-series information h_0(f(X))- h_\infty
(f(X)) where h_0(f(X)) is the Shannon entropy of f(X_0) and h_\infty (f(X)) is
the entropy rate of the time series f(X_0),...,f(X_n),... Implications for the
problem of optimal control are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4811</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4811</id><created>2013-04-17</created><authors><author><keyname>Kim</keyname><forenames>Yongjune</forenames></author><author><keyname>Cho</keyname><forenames>Kyoung Lae</forenames></author><author><keyname>Son</keyname><forenames>Hongrak</forenames></author><author><keyname>Kim</keyname><forenames>Jaehong</forenames></author><author><keyname>Kong</keyname><forenames>Jun Jin</forenames></author><author><keyname>Lee</keyname><forenames>Jaejin</forenames></author><author><keyname>Kumar</keyname><forenames>B. V. K. Vijaya</forenames></author></authors><title>Modulation Coding for Flash Memories</title><categories>cs.IT math.IT</categories><comments>7 pages, 9 figures, Proc. IEEE International Conference on Computing,
  Networking and Communications (ICNC), Jan. 2013</comments><doi>10.1109/ICCNC.2013.6504220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aggressive scaling down of flash memories has threatened data reliability
since the scaling down of cell sizes gives rise to more serious degradation
mechanisms such as cell-to-cell interference and lateral charge spreading. The
effect of these mechanisms has pattern dependency and some data patterns are
more vulnerable than other ones. In this paper, we will categorize data
patterns taking into account degradation mechanisms and pattern dependency. In
addition, we propose several modulation coding schemes to improve the data
reliability by transforming original vulnerable data patterns into more robust
ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4819</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4819</id><created>2013-04-17</created><updated>2013-04-17</updated><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Hu</keyname><forenames>Guangda</forenames></author></authors><title>Matching-Vector Families and LDCs Over Large Modulo</title><categories>math.CO cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove new upper bounds on the size of families of vectors in $\Z_m^n$ with
restricted modular inner products, when $m$ is a large integer. More formally,
if $\vec{u}_1,\ldots,\vec{u}_t \in \Z_m^n$ and $\vec{v}_1,\ldots,\vec{v}_t \in
\Z_m^n$ satisfy $\langle\vec{u}_i,\vec{v}_i\rangle\equiv0\pmod m$ and
$\langle\vec{u}_i,\vec{v}_j\rangle\not\equiv0\pmod m$ for all $i\neq j\in[t]$,
we prove that $t \leq O(m^{n/2+8.47})$. This improves a recent bound of $t \leq
m^{n/2 + O(\log(m))}$ by \cite{BDL13} and is the best possible up to the
constant 8.47 when $m$ is sufficiently larger than $n$.
  The maximal size of such families, called `Matching-Vector families', shows
up in recent constructions of locally decodable error correcting codes (LDCs)
and determines the rate of the code. Using our result we are able to show that
these codes, called Matching-Vector codes, must have encoding length at least
$K^{19/18}$ for $K$-bit messages, regardless of their query complexity. This
improves a known super linear bound of $ K2^{\Omega({\sqrt{\log K}})}$ proved
in \cite{DGY11}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4821</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4821</id><created>2013-04-17</created><authors><author><keyname>Kim</keyname><forenames>Yongjune</forenames></author><author><keyname>Kumar</keyname><forenames>B. V. K. Vijaya</forenames></author></authors><title>Coding for Memory with Stuck-at Defects</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, IEEE International Conference on Communications
  (ICC), Jun. 2013</comments><doi>10.1109/ICC.2013.6655249</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an encoding scheme for partitioned linear block
codes (PLBC) which mask the stuck-at defects in memories. In addition, we
derive an upper bound and the estimate of the probability that masking fails.
Numerical results show that PLBC can efficiently mask the defects with the
proposed encoding scheme. Also, we show that our upper bound is very tight by
using numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4837</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4837</id><created>2013-04-17</created><authors><author><keyname>Sharma</keyname><forenames>Amit</forenames></author><author><keyname>Gemici</keyname><forenames>Mevlana</forenames></author><author><keyname>Cosley</keyname><forenames>Dan</forenames></author></authors><title>Friends, Strangers, and the Value of Ego Networks for Recommendation</title><categories>cs.SI physics.soc-ph</categories><comments>5 pages, ICWSM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two main approaches to using social network information in recommendation
have emerged: augmenting collaborative filtering with social data and
algorithms that use only ego-centric data. We compare the two approaches using
movie and music data from Facebook, and hashtag data from Twitter. We find that
recommendation algorithms based only on friends perform no worse than those
based on the full network, even though they require much less data and
computational resources. Further, our evidence suggests that locality of
preference, or the non-random distribution of item preferences in a social
network, is a driving force behind the value of incorporating social network
information into recommender algorithms. When locality is high, as in Twitter
data, simple k-nn recommenders do better based only on friends than they do if
they draw from the entire network. These results help us understand when, and
why, social network information is likely to support recommendation systems,
and show that systems that see ego-centric slices of a complete network (such
as websites that use Facebook logins) or have computational limitations (such
as mobile devices) may profitably use ego-centric recommendation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4865</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4865</id><created>2013-04-17</created><authors><author><keyname>Machado</keyname><forenames>Ra&#xfa;l</forenames></author></authors><title>On the Generalized Hermite-Based Lattice Boltzmann Construction, Lattice
  Sets, Weights, Moments, Distribution Functions and High-Order Models</title><categories>cs.CE physics.comp-ph physics.flu-dyn</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The influence of the use of the generalized Hermite polynomial on the
Hermite-based lattice Boltzmann (LB) construction approach, lattice sets, the
thermal weights, moments and the equilibrium distribution function (EDF) are
addressed. A new moment system is proposed. The theoretical possibility to
obtain a high-order Hermite-based LB model capable to exactly match some first
hydrodynamic moments thermally 1) on-Cartesian lattice, 2) with thermal weights
in the EDF, 3) whilst the highest possible hydrodynamic moments that are
exactly matched are obtained with the shortest on-Cartesian lattice sets with
some fixed real-valued temperatures, is also analyzed.
  Keywords: Lattice Boltzmann, fluid dynamics, kinetic theory, distribution
function
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4889</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4889</id><created>2013-04-17</created><updated>2013-04-19</updated><authors><author><keyname>Cheney</keyname><forenames>Nick</forenames></author><author><keyname>Clune</keyname><forenames>Jeff</forenames></author><author><keyname>Yosinski</keyname><forenames>Jason</forenames></author><author><keyname>Lipson</keyname><forenames>Hod</forenames></author></authors><title>Hands-free Evolution of 3D-printable Objects via Eye Tracking</title><categories>cs.NE cs.HC</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive evolution has shown the potential to create amazing and complex
forms in both 2-D and 3-D settings. However, the algorithm is slow and users
quickly become fatigued. We propose that the use of eye tracking for
interactive evolution systems will both reduce user fatigue and improve
evolutionary success. We describe a systematic method for testing the
hypothesis that eye tracking driven interactive evolution will be a more
successful and easier-to-use design method than traditional interactive
evolution methods driven by mouse clicks. We provide preliminary results that
support the possibility of this proposal, and lay out future work to
investigate these advantages in extensive clinical trials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4893</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4893</id><created>2013-04-17</created><updated>2013-07-31</updated><authors><author><keyname>Jafarian</keyname><forenames>Matin</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author></authors><title>Formation control with binary information</title><categories>cs.SY</categories><comments>21 pages, 9 figures. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of formation keeping of a network of
strictly passive systems when very coarse information is exchanged. We assume
that neighboring agents only know whether their relative position is larger or
smaller than the prescribed one. This assumption results in very simple control
laws that direct the agents closer or away from each other and take values in
finite sets. We show that the task of formation keeping while tracking a
desired trajectory and rejecting matched disturbances is still achievable under
the very coarse information scenario. In contrast with other results of
practical convergence with coarse or quantized information, here the control
task is achieved exactly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4910</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4910</id><created>2013-04-17</created><updated>2013-12-02</updated><authors><author><keyname>Vats</keyname><forenames>Divyanshu</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author></authors><title>A Junction Tree Framework for Undirected Graphical Model Selection</title><categories>stat.ML cs.AI cs.IT math.IT</categories><comments>This paper will appear in the Journal of Machine Learning Research
  (JMLR). See http://www.ima.umn.edu/~dvats/JunctionTreeUGMS.html for code</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An undirected graphical model is a joint probability distribution defined on
an undirected graph G*, where the vertices in the graph index a collection of
random variables and the edges encode conditional independence relationships
among random variables. The undirected graphical model selection (UGMS) problem
is to estimate the graph G* given observations drawn from the undirected
graphical model. This paper proposes a framework for decomposing the UGMS
problem into multiple subproblems over clusters and subsets of the separators
in a junction tree. The junction tree is constructed using a graph that
contains a superset of the edges in G*. We highlight three main properties of
using junction trees for UGMS. First, different regularization parameters or
different UGMS algorithms can be used to learn different parts of the graph.
This is possible since the subproblems we identify can be solved independently
of each other. Second, under certain conditions, a junction tree based UGMS
algorithm can produce consistent results with fewer observations than the usual
requirements of existing algorithms. Third, both our theoretical and
experimental results show that the junction tree framework does a significantly
better job at finding the weakest edges in a graph than existing methods. This
property is a consequence of both the first and second properties. Finally, we
note that our framework is independent of the choice of the UGMS algorithm and
can be used as a wrapper around standard UGMS algorithms for more accurate
graph estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4915</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4915</id><created>2013-04-17</created><authors><author><keyname>Mahajan</keyname><forenames>Aditya</forenames></author><author><keyname>Dahiya</keyname><forenames>M. S.</forenames></author><author><keyname>Sanghvi</keyname><forenames>H. P.</forenames></author></authors><title>Forensic Analysis of Instant Messenger Applications on Android Devices</title><categories>cs.CY cs.CR</categories><doi>10.5120/11602-6965</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on conducting forensic data analysis of 2 widely used IMs
applications on Android phones WhatsApp and Viber. The tests and analysis were
performed with the aim of determining what data and information can be found on
the devices internal memory for instant messengers eg chat messaging logs and
history send &amp; received image or video files etc. The experiments and results
show that heavy amount of potential evidences and valuable data can be found on
Android phones by forensic investigators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4921</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4921</id><created>2013-04-17</created><updated>2016-02-01</updated><authors><author><keyname>Hatami</keyname><forenames>Pooya</forenames></author><author><keyname>Sachdeva</keyname><forenames>Sushant</forenames></author><author><keyname>Tulsiani</keyname><forenames>Madhur</forenames></author></authors><title>An Arithmetic Analogue of Fox's Triangle Removal Argument</title><categories>math.CO cs.DM</categories><comments>To appear in Online Journal of Analytic Combinatorics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an arithmetic version of the recent proof of the triangle removal
lemma by Fox [Fox11], for the group $\mathbb{F}_2^n$.
  A triangle in $\mathbb{F}_2^n$ is a triple $(x,y,z)$ such that $x+y+z = 0$.
The triangle removal lemma for $\mathbb{F}_2^n$ states that for every $\epsilon
&gt; 0$ there is a $\delta &gt; 0$, such that if a subset $A$ of $\mathbb{F}_2^n$
requires the removal of at least $\epsilon \cdot 2^n$ elements to make it
triangle-free, then it must contain at least $\delta \cdot 2^{2n}$ triangles.
This problem was first studied by Green [Gre05] who proved a lower bound on
$\delta$ using an arithmetic regularity lemma. Regularity based lower bounds
for triangle removal in graphs were recently improved by Fox and we give a
direct proof of an analogous improvement for triangle removal in
$\mathbb{F}_2^n$.
  The improved lower bound was already known to follow (for triangle-removal in
all groups), using Fox's removal lemma for directed cycles and a reduction by
Kr\'{a}l, Serra and Vena [KSV09] (see [Fox11,CF13]). The purpose of this note
is to provide a direct Fourier-analytic proof for the group $\mathbb{F}_2^n.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4925</identifier>
 <datestamp>2013-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4925</id><created>2013-04-17</created><updated>2013-06-14</updated><authors><author><keyname>Eppe</keyname><forenames>Manfred</forenames></author><author><keyname>Bhatt</keyname><forenames>Mehul</forenames></author><author><keyname>Dylla</keyname><forenames>Frank</forenames></author></authors><title>h-approximation: History-Based Approximation of Possible World Semantics
  as ASP</title><categories>cs.AI</categories><comments>12th International Conference on Logic Programming and Nonmonotonic
  Reasoning (LPNMR 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approximation of the Possible Worlds Semantics (PWS) for action
planning. A corresponding planning system is implemented by a transformation of
the action specification to an Answer-Set Program. A novelty is support for
postdiction wrt. (a) the plan existence problem in our framework can be solved
in NP, as compared to $\Sigma_2^P$ for non-approximated PWS of Baral(2000); and
(b) the planner generates optimal plans wrt. a minimal number of actions in
$\Delta_2^P$. We demo the planning system with standard problems, and
illustrate its integration in a larger software framework for robot control in
a smart home.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4927</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4927</id><created>2013-04-17</created><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author></authors><title>Homogeneous Weights and M\&quot;obius Functions on Finite Rings</title><categories>cs.IT math.IT math.RA</categories><comments>This paper has been published in a Chinese journal, see below; here
  is the English version</comments><msc-class>94B05, 13A99</msc-class><journal-ref>Y. Fan and H. Liu, Homogeneous weights of finite rings and
  M\&quot;obius functions(Chinese), Math. Ann.(Chinese), 31A (2010), 355-364</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The homogeneous weights and the M\&quot;obius functions and Euler phi-functions on
finite rings are discussed; some computational formulas for these functions on
finite principal ideal rings are characterized; for the residue rings of
integers, they are reduced to the classical number-theoretical M\&quot;obius
functions and the classical number-theoretical Euler phi-functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4928</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4928</id><created>2013-04-17</created><authors><author><keyname>Jiang</keyname><forenames>Luo-Luo</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author></authors><title>If cooperation is likely punish mildly: Insights from economic
  experiments based on the snowdrift game</title><categories>q-bio.PE cs.GT physics.soc-ph</categories><comments>15 pages, 6 figures; accepted for publication in PLoS ONE</comments><journal-ref>PLoS ONE 8 (2013) e64677</journal-ref><doi>10.1371/journal.pone.0064677</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Punishment may deter antisocial behavior. Yet to punish is costly, and the
costs often do not offset the gains that are due to elevated levels of
cooperation. However, the effectiveness of punishment depends not only on how
costly it is, but also on the circumstances defining the social dilemma. Using
the snowdrift game as the basis, we have conducted a series of economic
experiments to determine whether severe punishment is more effective than mild
punishment. We have observed that severe punishment is not necessarily more
effective, even if the cost of punishment is identical in both cases. The
benefits of severe punishment become evident only under extremely adverse
conditions, when to cooperate is highly improbable in the absence of sanctions.
If cooperation is likely, mild punishment is not less effective and leads to
higher average payoffs, and is thus the much preferred alternative. Presented
results suggest that the positive effects of punishment stem not only from
imposed fines, but may also have a psychological background. Small fines can do
wonders in motivating us to chose cooperation over defection, but without the
paralyzing effect that may be brought about by large fines. The later should be
utilized only when absolutely necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4948</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4948</id><created>2013-04-17</created><authors><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Schwartz</keyname><forenames>Roy</forenames></author><author><keyname>Sharma</keyname><forenames>Ankit</forenames></author><author><keyname>Singh</keyname><forenames>Mohit</forenames></author></authors><title>On the Approximation of Submodular Functions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular functions are a fundamental object of study in combinatorial
optimization, economics, machine learning, etc. and exhibit a rich
combinatorial structure. Many subclasses of submodular functions have also been
well studied and these subclasses widely vary in their complexity. Our
motivation is to understand the relative complexity of these classes of
functions. Towards this, we consider the question of how well can one class of
submodular functions be approximated by another (simpler) class of submodular
functions. Such approximations naturally allow algorithms designed for the
simpler class to be applied to the bigger class of functions. We prove both
upper and lower bounds on such approximations.
  Our main results are:
  1. General submodular functions can be approximated by cut functions of
directed graphs to a factor of $n^2/4$, which is tight.
  2. General symmetric submodular functions$^{1}$ can be approximated by cut
functions of undirected graphs to a factor of $n-1$, which is tight up to a
constant.
  3. Budgeted additive functions can be approximated by coverage functions to a
factor of $e/(e-1)$, which is tight.
  Here $n$ is the size of the ground set on which the submodular function is
defined.
  We also observe that prior works imply that monotone submodular functions can
be approximated by coverage functions with a factor between $O(\sqrt{n} \log
n)$ and $\Omega(n^{1/3} /\log^2 n) $.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4963</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4963</id><created>2013-04-17</created><authors><author><keyname>Hartonen</keyname><forenames>T.</forenames></author><author><keyname>Alava</keyname><forenames>M. J.</forenames></author></authors><title>How important tasks are performed: peer review</title><categories>physics.soc-ph cs.DL</categories><comments>7 pages, 3 figures</comments><journal-ref>Sci. Rep. 3, 1679 (2013)</journal-ref><doi>10.1038/srep01679</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advancement of various fields of science depends on the actions of
individual scientists via the peer review process. The referees' work patterns
and stochastic nature of decision making both relate to the particular features
of refereeing and to the universal aspects of human behavior. Here, we show
that the time a referee takes to write a report on a scientific manuscript
depends on the final verdict. The data is compared to a model, where the review
takes place in an ongoing competition of completing an important composite task
with a large number of concurrent ones - a Deadline -effect. In peer review
human decision making and task completion combine both long-range
predictability and stochastic variation due to a large degree of ever-changing
external &quot;friction&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4964</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4964</id><created>2013-04-17</created><updated>2014-11-10</updated><authors><author><keyname>Hansen</keyname><forenames>Samantha</forenames></author><author><keyname>Plantenga</keyname><forenames>Todd</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author></authors><title>Newton-Based Optimization for Kullback-Leibler Nonnegative Tensor
  Factorizations</title><categories>math.NA cs.NA</categories><comments>Clarified notation in section 3.1.1, and used simpler score()
  function in section B.2</comments><doi>10.1080/10556788.2015.1009977</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor factorizations with nonnegative constraints have found application in
analyzing data from cyber traffic, social networks, and other areas. We
consider application data best described as being generated by a Poisson
process (e.g., count data), which leads to sparse tensors that can be modeled
by sparse factor matrices. In this paper we investigate efficient techniques
for computing an appropriate canonical polyadic tensor factorization based on
the Kullback-Leibler divergence function. We propose novel subproblem solvers
within the standard alternating block variable approach. Our new methods
exploit structure and reformulate the optimization problem as small independent
subproblems. We employ bound-constrained Newton and quasi-Newton methods. We
compare our algorithms against other codes, demonstrating superior speed for
high accuracy results and the ability to quickly find sparse solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4965</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4965</id><created>2013-04-17</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Improvement/Extension of Modular Systems as Combinatorial Reengineering
  (Survey)</title><categories>cs.AI</categories><comments>24 pages, 28 figures, 14 tables. arXiv admin note: text overlap with
  arXiv:1212.1735</comments><msc-class>68T20, 68M10, 90B50, 90B40, 90C27, 90C29, 90C59, 93B51</msc-class><acm-class>J.6; I.2; I.2.8; A.1; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes development (improvement/extension) approaches for
composite (modular) systems (as combinatorial reengineering). The following
system improvement/extension actions are considered: (a) improvement of systems
component(s) (e.g., improvement of a system component, replacement of a system
component); (b) improvement of system component interconnection
(compatibility); (c) joint improvement improvement of system components(s) and
their interconnection; (d) improvement of system structure (replacement of
system part(s), addition of a system part, deletion of a system part,
modification of system structure). The study of system improvement approaches
involve some crucial issues: (i) scales for evaluation of system components and
component compatibility (quantitative scale, ordinal scale, poset-like scale,
scale based on interval multiset estimate), (ii) evaluation of integrated
system quality, (iii) integration methods to obtain the integrated system
quality. The system improvement/extension strategies can be examined as
seleciton/combination of the improvement action(s) above and as modification of
system structure. The strategies are based on combinatorial optimization
problems (e.g., multicriteria selection, knapsack problem, multiple choice
problem, combinatorial synthesis based on morphological clique problem,
assignment/reassignment problem, graph recoloring problem, spanning problems,
hotlink assignment). Here, heuristics are used. Various system
improvement/extension strategies are presented including illustrative numerical
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4974</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4974</id><created>2013-04-17</created><authors><author><keyname>Cie&#x15b;li&#x144;ski</keyname><forenames>Jan L.</forenames></author><author><keyname>Moroz</keyname><forenames>Leonid V.</forenames></author></authors><title>Fast exact digital differential analyzer for circle generation</title><categories>cs.GR cs.SY</categories><comments>14 pages</comments><msc-class>65D17, 68U07, 65L12</msc-class><acm-class>I.3.3; G.1.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of the paper we present a short review of applications of
digital differential analyzers (DDA) to generation of circles showing that they
can be treated as one-step numerical schemes. In the second part we present and
discuss a novel fast algorithm based on a two-step numerical scheme (explicit
midpoint rule). Although our algorithm is as cheap as the simplest one-step DDA
algoritm (and can be represented in terms of shifts and additions), it
generates circles with maximal accuracy, i.e., it is exact up to round-off
errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4985</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4985</id><created>2013-04-17</created><updated>2013-07-09</updated><authors><author><keyname>Krishnamoorthy</keyname><forenames>Bala</forenames></author><author><keyname>Smith</keyname><forenames>Gavin</forenames></author></authors><title>Non Total-Unimodularity Neutralized Simplicial Complexes</title><categories>math.AT cs.CG math.OC</categories><comments>Identified a class of complexes that are guaranteed to be NTU
  neutralized (Theorem 8.1). Added one figure, and improved the presentation in
  several places</comments><msc-class>55U10, 55N99, 52B12, 90C10</msc-class><acm-class>F.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a simplicial complex K with weights on its simplices and a chain on it,
the Optimal Homologous Chain Problem (OHCP) is to find a chain with minimal
weight that is homologous (over the integers) to the given chain. The OHCP is
NP-complete, but if the boundary matrix of K is totally unimodular (TU), it
becomes solvable in polynomial time when modeled as a linear program (LP). We
define a condition on the simplicial complex called non total-unimodularity
neutralized, or NTU neutralized, which ensures that even when the boundary
matrix is not TU, the OHCP LP must contain an integral optimal vertex for every
input chain. This condition is a property of K, and is independent of the input
chain and the weights on the simplices. This condition is strictly weaker than
the boundary matrix being TU. More interestingly, the polytope of the OHCP LP
may not be integral under this condition. Still, an integral optimal vertex
exists for every right-hand side, i.e., for every input chain. Hence a much
larger class of OHCP instances can be solved in polynomial time than previously
considered possible. As a special case, we show that 2-complexes with trivial
first homology group are guaranteed to be NTU neutralized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4986</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4986</id><created>2013-04-17</created><updated>2014-11-22</updated><authors><author><keyname>Jackson</keyname><forenames>Marcel</forenames></author><author><keyname>Kowalski</keyname><forenames>Tomasz</forenames></author><author><keyname>Niven</keyname><forenames>Todd</forenames></author></authors><title>Digraph related constructions and the complexity of digraph homomorphism
  problems</title><categories>math.CO cs.CC cs.DM</categories><comments>27 pages</comments><msc-class>05C20, 05C15, 08B05</msc-class><acm-class>F.2.0; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role of polymorphisms in determining the complexity of constraint
satisfaction problems is well established. In this context we study the
stability of CSP complexity and polymorphism properties under some basic graph
theoretic constructions. As applications we prove the algebraic CSP dichotomy
conjecture holds for digraphs whose symmetric closure is a complete graph, and
observe a collapse in the applicability of algorithms for CSPs over directed
graphs with both a total source and a total sink: the corresponding CSP is
solvable by the &quot;few subpowers algorithm&quot; if and only if it is solvable by a
local consistency check algorithm. Moreover, we find that the property of
&quot;strict width&quot; and solvability by few subpowers are unstable under first order
reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.4994</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.4994</id><created>2013-04-17</created><authors><author><keyname>Ch&#xe1;vez</keyname><forenames>Edgar</forenames></author><author><keyname>Ch&#xe1;vez-C&#xe1;liz</keyname><forenames>Ana C.</forenames></author><author><keyname>L&#xf3;pez-L&#xf3;pez</keyname><forenames>Jorge L.</forenames></author></authors><title>Polygon Matching and Indexing Under Affine Transformations</title><categories>cs.CV</categories><msc-class>51N10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a collection $\{Z_1,Z_2,\ldots,Z_m\}$ of $n$-sided polygons in the
plane and a query polygon $W$ we give algorithms to find all $Z_\ell$ such that
$W=f(Z_\ell)$ with $f$ an unknown similarity transformation in time independent
of the size of the collection. If $f$ is a known affine transformation, we show
how to find all $Z_\ell$ such that $W=f(Z_\ell)$ in $O(n+\log(m))$ time.
  For a pair $W,W^\prime$ of polygons we can find all the pairs
$Z_\ell,Z_{\ell^\prime}$ such that $W=f(Z_\ell)$ and
$W^\prime=f(Z_{\ell^\prime})$ for an unknown affine transformation $f$ in
$O(m+n)$ time.
  For the case of triangles we also give bounds for the problem of matching
triangles with variable vertices, which is equivalent to affine matching
triangles in noisy conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5007</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5007</id><created>2013-04-17</created><updated>2013-08-23</updated><authors><author><keyname>Liu</keyname><forenames>Yi-Kai</forenames></author></authors><title>Building one-time memories from isolated qubits</title><categories>quant-ph cs.IT math.IT</categories><comments>36 pages; v2: better organized, better motivated, and easier to read</comments><journal-ref>Proceedings of the 5th conference on Innovations in Theoretical
  Computer Science (ITCS 2014), pp.269-286</journal-ref><doi>10.1145/2554797.2554823</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-time memories (OTM's) are simple tamper-resistant cryptographic devices,
which can be used to implement one-time programs, a very general form of
software protection and program obfuscation. Here we investigate the
possibility of building OTM's using quantum mechanical devices. It is known
that OTM's cannot exist in a fully-quantum world or in a fully-classical world.
Instead, we propose a new model based on &quot;isolated qubits&quot; -- qubits that can
only be accessed using local operations and classical communication (LOCC).
This model combines a quantum resource (single-qubit measurements) with a
classical restriction (on communication between qubits), and can be implemented
using current technologies, such as nitrogen vacancy centers in diamond. In
this model, we construct OTM's that are information-theoretically secure
against one-pass LOCC adversaries that use 2-outcome measurements.
  Our construction resembles Wiesner's old idea of quantum conjugate coding,
implemented using random error-correcting codes; our proof of security uses
entropy chaining to bound the supremum of a suitable empirical process. In
addition, we conjecture that our random codes can be replaced by some class of
efficiently-decodable codes, to get computationally-efficient OTM's that are
secure against computationally-bounded LOCC adversaries.
  In addition, we construct data-hiding states, which allow an LOCC sender to
encode an (n-O(1))-bit messsage into n qubits, such that at most half of the
message can be extracted by a one-pass LOCC receiver, but the whole message can
be extracted by a general quantum receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5010</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5010</id><created>2013-04-17</created><updated>2013-04-30</updated><authors><author><keyname>Chen</keyname><forenames>Sixia</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Russell</keyname><forenames>Alexander</forenames></author></authors><title>Small-Bias Sets for Nonabelian Groups: Derandomizing the Alon-Roichman
  Theorem</title><categories>cs.CC math.CO math.GR math.RT</categories><comments>Our results on solvable groups have been significantly improved,
  giving eps-biased sets of polynomial (as opposed to quasipolynomial) size</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analogy with epsilon-biased sets over Z_2^n, we construct explicit
epsilon-biased sets over nonabelian finite groups G. That is, we find sets S
subset G such that | Exp_{x in S} rho(x)| &lt;= epsilon for any nontrivial
irreducible representation rho. Equivalently, such sets make G's Cayley graph
an expander with eigenvalue |lambda| &lt;= epsilon. The Alon-Roichman theorem
shows that random sets of size O(log |G| / epsilon^2) suffice. For groups of
the form G = G_1 x ... x G_n, our construction has size poly(max_i |G_i|, n,
epsilon^{-1}), and we show that a set S \subset G^n considered by Meka and
Zuckerman that fools read-once branching programs over G is also epsilon-biased
in this sense. For solvable groups whose abelian quotients have constant
exponent, we obtain epsilon-biased sets of size (log |G|)^{1+o(1)}
poly(epsilon^{-1}). Our techniques include derandomized squaring (in both the
matrix product and tensor product senses) and a Chernoff-like bound on the
expected norm of the product of independently random operators that may be of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5015</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5015</id><created>2013-04-18</created><authors><author><keyname>Mohan</keyname><forenames>Ramya</forenames></author></authors><title>Network Analysis and Application Control Software based on Client-Server
  Architecture</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Applications 68(12):34-39, April
  2013</journal-ref><doi>10.5120/11634-7111</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper outlines a comprehensive model to increase system efficiency,
preserve network bandwidth, monitor incoming and outgoing packets, ensure the
security of confidential files and reduce power wastage in an organization.
This model illustrates the use and potential application of a Network Analysis
Tool (NAT) in a multi-computer set-up of any scale. The model is designed to
run in the background and not hamper any currently executing applications,
while using minimum system resources. It was developed as open source software,
using VB. Net, with a view to overcoming limitations of legacy systems and
financial restrictions in small-to mid-level organizations like businesses and
educational institutes. It is fully-customizable and serves as a simple and
open-source alternative to existing software. The NAT relies on simple
client-server architecture and uses remote access to monitor and maintain the
computers on a network, for example logging off a user or shutting down a
computer after a certain &quot;idle&quot; time, enabling and disabling applications,
troubleshooting and so on. The NAT was tested in a laboratory and resultant
data is presented, along with the results of a survey that was conducted among
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5022</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5022</id><created>2013-04-18</created><authors><author><keyname>Dave</keyname><forenames>Shalvi</forenames></author><author><keyname>Trivedi</keyname><forenames>Bhushan</forenames></author><author><keyname>Mahadevia</keyname><forenames>Jimit</forenames></author></authors><title>Efficacy of Attack detection capability of IDPS based on it's deployment
  in wired and wireless environment</title><categories>cs.CR</categories><comments>13 pages, 10 figures</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.5, No.2, March 2013</journal-ref><doi>10.5121/ijnsa.2013.5208</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrusion Detection and/or Prevention Systems (IDPS) represent an important
line of defence against a variety of attacks that can compromise the security
and proper functioning of an enterprise information system. Along with the
widespread evolution of new emerging services, the quantity and impact of
attacks have continuously increased, attackers continuously find
vulnerabilities at various levels, from the network itself to operating system
and applications, exploit them to crack system and services. Network defence
and network monitoring has become an essential component of computer security
to predict and prevent attacks. Unlike traditional Intrusion Detection System
(IDS), Intrusion Detection and Prevention System (IDPS) have additional
features to secure computer networks. In this paper, we present a detailed
study of how deployment of an IDPS plays a key role in its performance and the
ability to detect and prevent known as well as unknown attacks. We categorize
IDPS based on deployment as Network-based, host-based, and Perimeter-based and
Hybrid. A detailed comparison is shown in this paper and finally we justify our
proposed solution, which deploys agents at host-level to give better
performance in terms of reduced rate of false positives and accurate detection
and prevention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5034</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5034</id><created>2013-04-18</created><updated>2014-03-04</updated><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Jovanovic</keyname><forenames>Miodrag</forenames><affiliation>FT R and D</affiliation></author><author><keyname>Karray</keyname><forenames>Mohamed Kadhem</forenames><affiliation>FT R and D</affiliation></author></authors><title>Quality of Real-Time Streaming in Wireless Cellular Networks -
  Stochastic Modeling and Analysis</title><categories>cs.NI math.PR</categories><comments>(06/2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new stochastic service model with capacity sharing and
interruptions, appropriate for the evaluation of the quality of real-time
streaming (RTS), like e.g. mobile TV, in wireless cellular networks. The
general model takes into account multi-class Markovian process of call
arrivals, (to capture different radio channel conditions, requested streaming
bit-rates and durations) and allows for a general resource allocation policy
saying which users are temporarily denied the requested fixed streaming
bit-rates (put in outage) due to resource constraints. We give expressions for
several important performance characteristics of the model, including mean time
spent in outage and mean number of outage incidents for a typical user of a
given class. These expressions involve only stationary probabilities of the
(free) traffic demand process, which is a vector of independent Poisson random
variables describing the number of users of different classes. In order to
analyze RTS in 3GPP Long Term Evolution (LTE) cellular networks, we specify our
general model assuming orthogonal user channels with the peak bit-rates close
to the theoretical Shannon's bound in the additive white Gaussian noise (AWGN)
channel, which leads to the resource constraints in a multi-rate linear form.
In this setting we consider a natural class of least-effort-served-first
resource allocation policies, for which the characteristics of the model can be
further evaluated using Fourier analysis of Poisson variables. Within this
class we identify and evaluate an optimal and a fair policy, the latter being
suggested by LTE implementations. We also propose some intermediate policies,
which allow to solve the optimality/fairness tradeoff caused by unequal user
radio-channel conditions. Our results can be used for the evaluation of the
quality of RTS in LTE networks and dimensioning of these networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5038</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5038</id><created>2013-04-18</created><updated>2014-10-29</updated><authors><author><keyname>Zhang</keyname><forenames>Hui</forenames></author><author><keyname>Yan</keyname><forenames>Ming</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author></authors><title>One condition for solution uniqueness and robustness of both
  l1-synthesis and l1-analysis minimizations</title><categories>cs.IT math.IT math.OC</categories><comments>15 pages, 0 figures</comments><report-no>Rice CAAM technical report 13-10, 2013</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $\ell_1$-synthesis model and the $\ell_1$-analysis model recover
structured signals from their undersampled measurements. The solution of former
is a sparse sum of dictionary atoms, and that of the latter makes sparse
correlations with dictionary atoms. This paper addresses the question: when can
we trust these models to recover specific signals? We answer the question with
a condition that is both necessary and sufficient to guarantee the recovery to
be unique and exact and, in presence of measurement noise, to be robust. The
condition is one--for--all in the sense that it applies to both of the
$\ell_1$-synthesis and $\ell_1$-analysis models, to both of their constrained
and unconstrained formulations, and to both the exact recovery and robust
recovery cases. Furthermore, a convex infinity--norm program is introduced for
numerically verifying the condition. A comprehensive comparison with related
existing conditions are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5045</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5045</id><created>2013-04-18</created><authors><author><keyname>Chang</keyname><forenames>Chii</forenames></author><author><keyname>Ling</keyname><forenames>Sea</forenames></author></authors><title>Towards an Infrastructure-less SOA for Mobile Web Service Composition</title><categories>cs.DC cs.NI</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service composition enables customizable services to be provided to the
service consumers. Since the capabilities and the performances of mobile
devices (e.g., smart phone, PDA, handheld media player) have improved, a mobile
device can be utilized to interact with external mobile service providers
towards providing composite Web service to remote clients. Existing approaches
on mobile-hosted service composition are usually platform dependent, and rely
on centralized infrastructure. Such approaches are not feasible in an open,
mobile infrastructure-less environment, in which networked services are
implemented using different technologies, devices are capable of dynamically
joining or leaving the network, and a centralized management entity is
nonexistent. This paper proposes a solution to enable mobile Web service
composition in an open, infrastructure-less environment based on loosely
coupled SOA techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5051</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5051</id><created>2013-04-18</created><authors><author><keyname>Mitra</keyname><forenames>Shubhadip</forenames></author><author><keyname>Dutta</keyname><forenames>Partha</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Arnab</forenames></author></authors><title>Constraint Satisfaction over Generalized Staircase Constraints</title><categories>cs.AI cs.DS</categories><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key research interests in the area of Constraint Satisfaction
Problem (CSP) is to identify tractable classes of constraints and develop
efficient solutions for them. In this paper, we introduce generalized staircase
(GS) constraints which is an important generalization of one such tractable
class found in the literature, namely, staircase constraints. GS constraints
are of two kinds, down staircase (DS) and up staircase (US). We first examine
several properties of GS constraints, and then show that arc consistency is
sufficient to determine a solution to a CSP over DS constraints. Further, we
propose an optimal O(cd) time and space algorithm to compute arc consistency
for GS constraints where c is the number of constraints and d is the size of
the largest domain. Next, observing that arc consistency is not necessary for
solving a DSCSP, we propose a more efficient algorithm for solving it. With
regard to US constraints, arc consistency is not known to be sufficient to
determine a solution, and therefore, methods such as path consistency or
variable elimination are required. Since arc consistency acts as a subroutine
for these existing methods, replacing it by our optimal O(cd) arc consistency
algorithm produces a more efficient method for solving a USCSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5063</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5063</id><created>2013-04-18</created><updated>2013-04-26</updated><authors><author><keyname>Bannour</keyname><forenames>Hichem</forenames></author><author><keyname>Hudelot</keyname><forenames>C&#xe9;line</forenames></author></authors><title>Combinaison d'information visuelle, conceptuelle, et contextuelle pour
  la construction automatique de hierarchies semantiques adaptees a
  l'annotation d'images</title><categories>cs.CV cs.LG cs.MM</categories><comments>RFIA 2012 (Reconnaissance des Formes et Intelligence Artificielle)
  Lyon, France pg. 462-469. 9 pages</comments><msc-class>68T45</msc-class><acm-class>I.4.10</acm-class><journal-ref>RFIA 2012 (Reconnaissance des Formes et Intelligence Artificielle)
  Lyon, France pg. 462-469</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper proposes a new methodology to automatically build semantic
hierarchies suitable for image annotation and classification. The building of
the hierarchy is based on a new measure of semantic similarity. The proposed
measure incorporates several sources of information: visual, conceptual and
contextual as we defined in this paper. The aim is to provide a measure that
best represents image semantics. We then propose rules based on this measure,
for the building of the final hierarchy, and which explicitly encode
hierarchical relationships between different concepts. Therefore, the built
hierarchy is used in a semantic hierarchical classification framework for image
annotation. Our experiments and results show that the hierarchy built improves
classification results.
  Ce papier propose une nouvelle methode pour la construction automatique de
hierarchies semantiques adaptees a la classification et a l'annotation
d'images. La construction de la hierarchie est basee sur une nouvelle mesure de
similarite semantique qui integre plusieurs sources d'informations: visuelle,
conceptuelle et contextuelle que nous definissons dans ce papier. L'objectif
est de fournir une mesure qui est plus proche de la semantique des images. Nous
proposons ensuite des regles, basees sur cette mesure, pour la construction de
la hierarchie finale qui encode explicitement les relations hierarchiques entre
les differents concepts. La hierarchie construite est ensuite utilisee dans un
cadre de classification semantique hierarchique d'images en concepts visuels.
Nos experiences et resultats montrent que la hierarchie construite permet
d'ameliorer les resultats de la classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5068</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5068</id><created>2013-04-18</created><authors><author><keyname>Thai</keyname><forenames>Tuan Tran</forenames></author><author><keyname>Lacan</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Lochin</keyname><forenames>Emmanuel</forenames></author></authors><title>Joint On-the-Fly Network Coding/Video Quality Adaptation for Real-Time
  Delivery</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a redundancy adaptation algorithm for an on-the-fly
erasure network coding scheme called Tetrys in the context of real-time video
transmission. The algorithm exploits the relationship between the redundancy
ratio used by Tetrys and the gain or loss in encoding bit rate from changing a
video quality parameter called the Quantization Parameter (QP). Our evaluations
show that with equal or less bandwidth occupation, the video protected by
Tetrys with redundancy adaptation algorithm obtains a PSNR gain up to or more 4
dB compared to the video without Tetrys protection. We demonstrate that the
Tetrys redundancy adaptation algorithm performs well with the variations of
both loss pattern and delay induced by the networks. We also show that Tetrys
with the redundancy adaptation algorithm outperforms FEC with and without
redundancy adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5069</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5069</id><created>2013-04-18</created><authors><author><keyname>Rafler</keyname><forenames>Stephan</forenames></author></authors><title>The Tap code - a code similar to Morse code for communication by tapping</title><categories>cs.IT math.IT</categories><comments>11 pages, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A code is presented for fast, easy and efficient communication over channels
that allow only two signal types: a single sound (e.g. a knock), or no sound
(i.e. silence). This is a true binary code while Morse code is a ternary code
and does not work in such situations. Thus the presented code is more universal
than Morse and can be used in much more situations. Additionally it is very
tolerant to variations in signal strength or duration. The paper contains
various ways in which the code can be derived, that all lead to the same code.
It also contains a comparison to other, similar codes, including the Morse
code, in regards to efficiency and other attributes. The replacement of Morse
code with Tap code is not proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5073</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5073</id><created>2013-04-18</created><authors><author><keyname>Upadhya</keyname><forenames>Vidyadhar</forenames></author><author><keyname>Jalihal</keyname><forenames>Devendra</forenames></author></authors><title>Blind Non-parametric Statistics for Multichannel Detection Based on
  Statistical Covariances</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, Accepted for publication in IEEE Workshop on
  Emerging COgnitive Radio Applications and aLgorithms, 2013 (IEEE CORAL 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of detecting the presence of a spatially correlated
multichannel signal corrupted by additive Gaussian noise (i.i.d across
sensors). No prior knowledge is assumed about the system parameters such as the
noise variance, number of sources and correlation among signals. It is well
known that the GLRT statistics for this composite hypothesis testing problem
are asymptotically optimal and sensitive to variation in system model or its
parameter. To address these shortcomings we present a few non-parametric
statistics which are functions of the elements of Bartlett decomposed sample
covariance matrix. They are designed such that the detection performance is
immune to the uncertainty in the knowledge of noise variance. The analysis
presented verifies the invariability of threshold value and identifies a few
specific scenarios where the proposed statistics have better performance
compared to GLRT statistics. The sensitivity of the statistic to correlation
among streams, number of sources and sample size at low signal to noise ratio
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5075</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5075</id><created>2013-04-18</created><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author></authors><title>On the Rate of Information Loss in Memoryless Systems</title><categories>cs.IT math.IT</categories><comments>9 pages, 4 figures; submitted to a conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present results about the rate of (relative) information loss
induced by passing a real-valued, stationary stochastic process through a
memoryless system. We show that for a special class of systems the information
loss rate is closely related to the difference of differential entropy rates of
the input and output processes. It is further shown that the rate of (relative)
information loss is bounded from above by the (relative) information loss the
system induces on a random variable distributed according to the process's
marginal distribution.
  As a side result, in this work we present sufficient conditions such that for
a continuous-valued Markovian input process also the output process possesses
the Markov property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5081</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5081</id><created>2013-04-18</created><authors><author><keyname>Wallentowitz</keyname><forenames>Stefan</forenames></author><author><keyname>Wagner</keyname><forenames>Philipp</forenames></author><author><keyname>Tempelmeier</keyname><forenames>Michael</forenames></author><author><keyname>Wild</keyname><forenames>Thomas</forenames></author><author><keyname>Herkersdorf</keyname><forenames>Andreas</forenames></author></authors><title>Open Tiled Manycore System-on-Chip</title><categories>cs.AR</categories><comments>7 pages, 2 figures</comments><report-no>DPA-13052</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manycore System-on-Chip include an increasing amount of processing elements
and have become an important research topic for improvements of both hardware
and software. While research can be conducted using system simulators,
prototyping requires a variety of components and is very time consuming. With
the Open Tiled Manycore System-on-Chip (OpTiMSoC) we aim at building such an
environment for use in our and other research projects as prototyping platform.
  This paper describes the project goals and aspects of OpTiMSoC and summarizes
the current status and ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5084</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5084</id><created>2013-04-18</created><authors><author><keyname>Baum</keyname><forenames>Marcus</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Extended Object Tracking with Random Hypersurface Models</title><categories>cs.SY</categories><comments>Draft accepted for publication in IEEE Transactions on Aerospace and
  Electronic Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Random Hypersurface Model (RHM) is introduced that allows for estimating
a shape approximation of an extended object in addition to its kinematic state.
An RHM represents the spatial extent by means of randomly scaled versions of
the shape boundary. In doing so, the shape parameters and the measurements are
related via a measurement equation that serves as the basis for a Gaussian
state estimator. Specific estimators are derived for elliptic and star-convex
shapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5088</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5088</id><created>2013-04-18</created><updated>2013-04-19</updated><authors><author><keyname>Zheng</keyname><forenames>Meng</forenames></author><author><keyname>Pawe&#x142;czak</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>S&#x142;awomir</forenames></author><author><keyname>Yu</keyname><forenames>Haibin</forenames></author></authors><title>Planning of Cellular Networks Enhanced by Energy Harvesting</title><categories>cs.ET cs.NI</categories><comments>accepted to IEEE Communications Letters [source code available]</comments><journal-ref>IEEE Communications Letters, vol.:17, no: 6, pp.: 1092-1095, 2013</journal-ref><doi>10.1109/LCOMM.2013.043013.122667</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We pose a novel cellular network planning problem, considering the use of
renewable energy sources and a fundamentally new concept of energy balancing,
and propose a novel algorithm to solve it. In terms of the network capital and
operational expenditure, we conclude that savings can be made by enriching
cellular infrastructure with energy harvesting sources, in comparison to
traditional deployment methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5097</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5097</id><created>2013-04-18</created><updated>2014-04-06</updated><authors><author><keyname>Rutherford</keyname><forenames>Alex</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author><author><keyname>Rahwan</keyname><forenames>Iyad</forenames></author><author><keyname>Dsouza</keyname><forenames>Sohan</forenames></author><author><keyname>McInerney</keyname><forenames>James</forenames></author><author><keyname>Naroditskiy</keyname><forenames>Victor</forenames></author><author><keyname>Venanzi</keyname><forenames>Matteo</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author><author><keyname>deLara</keyname><forenames>J. R.</forenames></author><author><keyname>Wahlstedt</keyname><forenames>Eero</forenames></author><author><keyname>Miller</keyname><forenames>Steven U.</forenames></author></authors><title>Targeted Social Mobilisation in a Global Manhunt</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>10 pages, 11 figures (Added Supplementary Information)</comments><journal-ref>PLoS One (2013) 8 (9)</journal-ref><doi>10.1371/journal.pone.0074628</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social mobilization, the ability to mobilize large numbers of people via
social networks to achieve highly distributed tasks, has received significant
attention in recent times. This growing capability, facilitated by modern
communication technology, is highly relevant to endeavors which require the
search for individuals that posses rare information or skill, such as finding
medical doctors during disasters, or searching for missing people. An open
question remains, as to whether in time-critical situations, people are able to
recruit in a targeted manner, or whether they resort to so-called blind search,
recruiting as many acquaintances as possible via broadcast communication. To
explore this question, we examine data from our recent success in the U.S.
State Department's Tag Challenge, which required locating and photographing 5
target persons in 5 different cities in the United States and Europe in less
than 12 hours, based only on a single mug-shot. We find that people are able to
consistently route information in a targeted fashion even under increasing time
pressure. We derive an analytical model for global mobilization and use it to
quantify the extent to which people were targeting others during recruitment.
Our model estimates that approximately 1 in 3 messages were of targeted fashion
during the most time-sensitive period of the challenge.This is a novel
observation at such short temporal scales, and calls for opportunities for
devising viral incentive schemes that provide distance- or time-sensitive
rewards to approach the target geography more rapidly, with applications in
multiple areas from emergency preparedness, to political mobilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5099</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5099</id><created>2013-04-18</created><authors><author><keyname>Medeiros</keyname><forenames>Vivian</forenames></author><author><keyname>Gomes</keyname><forenames>Antonio Tadeu Azevedo</forenames></author></authors><title>Expressando Atributos N\~ao-Funcionais em Workflows Cient\'ificos</title><categories>cs.CE cs.SE</categories><comments>In portuguese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present OSC, a scientific workflow specification language
based on software architecture principles. In contrast with other approaches,
OSC employs connectors as first-class constructs. In this way, we leverage
reusability and compositionality in the workflow modeling process, specially in
the configuration of mechanisms that manage non-functional attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5100</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5100</id><created>2013-04-18</created><authors><author><keyname>Mill&#xe1;n</keyname><forenames>V&#xed;ctor M. L&#xf3;pez</forenames></author><author><keyname>Cholvi</keyname><forenames>Vicent</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Luis</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author></authors><title>Improving Resource Location with Locally Precomputed Partial Random
  Walks</title><categories>cs.NI</categories><comments>25 pages, 15 figures. arXiv admin note: substantial text overlap with
  arXiv:1107.4660</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random walks can be used to search complex networks for a desired resource.
To reduce search lengths, we propose a mechanism based on building random walks
connecting together partial walks (PW) previously computed at each network
node. Resources found in each PW are registered. Searches can then jump over
PWs where the resource is not located. However, we assume that perfect
recording of resources may be costly, and hence, probabilistic structures like
Bloom filters are used. Then, unnecessary hops may come from false positives at
the Bloom filters. Two variations of this mechanism have been considered,
depending on whether we first choose a PW in the current node and then check it
for the resource, or we first check all PWs and then choose one. In addition,
PWs can be either simple random walks or self-avoiding random walks. Analytical
models are provided to predict expected search lengths and other magnitudes of
the resulting four mechanisms. Simulation experiments validate these
predictions and allow us to compare these techniques with simple random walk
searches, finding very large reductions of expected search lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5101</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5101</id><created>2013-04-18</created><authors><author><keyname>Dorta-Gonzalez</keyname><forenames>Pablo</forenames></author><author><keyname>Dorta-Gonzalez</keyname><forenames>Maria Isabel</forenames></author></authors><title>Impact maturity times and citation time windows: The 2-year maximum
  journal impact factor</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>24 pages, 5 tables and 3 figures. arXiv admin note: text overlap with
  arXiv:1007.4749, arXiv:1208.6122 by other authors</comments><doi>10.1016/j.joi.2013.03.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Journal metrics are employed for the assessment of scientific scholar
journals from a general bibliometric perspective. In this context, the Thomson
Reuters journal impact factors (JIF) are the citation-based indicators most
used. The 2-year journal impact factor (2-JIF) counts citations to one and two
year old articles, while the 5-year journal impact factor (5-JIF) counts
citations from one to five year old articles. Nevertheless, these indicators
are not comparable among fields of science for two reasons: (i) each field has
a different impact maturity time, and (ii) because of systematic differences in
publication and citation behaviour across disciplines. In fact, the 5-JIF
firstly appeared in the Journal Citation Reports (JCR) in 2007 with the purpose
of making more comparable impacts in fields in which impact matures slowly.
However, there is not an optimal fixed impact maturity time valid for all the
fields. In some of them two years provides a good performance whereas in others
three or more years are necessary. Therefore, there is a problem when comparing
a journal from a field in which impact matures slowly with a journal from a
field in which impact matures rapidly. In this work, we propose the 2-year
maximum journal impact factor (2M-JIF), a new impact indicator that considers
the 2-year rolling citation time window of maximum impact instead of the
previous 2-year time window. Finally, an empirical application comparing 2-JIF,
5-JIF, and 2M-JIF shows that the maximum rolling target window reduces the
between-group variance with respect to the within-group variance in a random
sample of about six hundred journals from eight different fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5107</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5107</id><created>2013-04-18</created><authors><author><keyname>Dorta-Gonzalez</keyname><forenames>Pablo</forenames></author><author><keyname>Dorta-Gonzalez</keyname><forenames>Maria Isabel</forenames></author></authors><title>Comparing journals from different fields of Science and Social Science
  through a JCR Subject Categories Normalized Impact Factor</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>28 pages, 4 tables and 5 figures. arXiv admin note: text overlap with
  arXiv:1007.4749 by other authors</comments><journal-ref>Scientometrics 95(2), 645-672 (2013)</journal-ref><doi>10.1007/s11192-012-0929-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The journal Impact Factor (IF) is not comparable among fields of Science and
Social Science because of systematic differences in publication and citation
behaviour across disciplines. In this work, a decomposing of the field
aggregate impact factor into five normally distributed variables is presented.
Considering these factors, a Principal Component Analysis is employed to find
the sources of the variance in the JCR subject categories of Science and Social
Science. Although publication and citation behaviour differs largely across
disciplines, principal components explain more than 78% of the total variance
and the average number of references per paper is not the primary factor
explaining the variance in impact factors across categories. The Categories
Normalized Impact Factor (CNIF) based on the JCR subject category list is
proposed and compared with the IF. This normalization is achieved by
considering all the indexing categories of each journal. An empirical
application, with one hundred journals in two or more subject categories of
economics and business, shows that the gap between rankings is reduced around
32% in the journals analyzed. This gap is obtained as the maximum distance
among the ranking percentiles from all categories where each journal is
included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5109</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5109</id><created>2013-04-18</created><authors><author><keyname>Perrot</keyname><forenames>Kevin</forenames></author><author><keyname>R&#xe9;mila</keyname><forenames>Eric</forenames></author></authors><title>Kadanoff Sand Pile Model. Avalanche Structure and Wave Shape</title><categories>cs.DM math.CO</categories><comments>30 pages. arXiv admin note: text overlap with arXiv:1106.2670,
  arXiv:1101.5940</comments><journal-ref>Theoretical Computer Science, ISSN 0304-3975,
  10.1016/j.tcs.2013.01.033</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sand pile models are dynamical systems describing the evolution from $N$
stacked grains to a stable configuration. It uses local rules to depict grain
moves and iterate it until reaching a fixed configuration from which no rule
can be applied. Physicists L. Kadanoff {\em et al} inspire KSPM, extending the
well known {\em Sand Pile Model} (SPM). In KSPM($D$), we start from a pile of
$N$ stacked grains and apply the rule: $D\!-\!1$ grains can fall from column
$i$ onto columns $i+1,i+2,\dots,i+D\!-\!1$ if the difference of height between
columns $i$ and $i\!+\!1$ is greater or equal to $D$. Toward the study of fixed
points (stable configurations on which no grain can move) obtained from $N$
stacked grains, we propose an iterative study of KSPM evolution consisting in
the repeated addition of one grain on a heap of sand, triggering an avalanche
at each iteration. We develop a formal background for the study of avalanches,
resumed in a finite state word transducer, and explain how this transducer may
be used to predict the form of fixed points. Further precise developments
provide a plain formula for fixed points of KSPM(3), showing the emergence of a
wavy shape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5110</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5110</id><created>2013-04-18</created><authors><author><keyname>Dorta-Gonzalez</keyname><forenames>Pablo</forenames></author><author><keyname>Dorta-Gonzalez</keyname><forenames>Maria Isabel</forenames></author></authors><title>Central indexes to the citation distribution: A complement to the
  h-index</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>23 pages, 3 tables and 6 figures</comments><journal-ref>Scientometrics 88(3), 729-745 (2011)</journal-ref><doi>10.1007/s11192-011-0453-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The citation distribution of a researcher shows the impact of their
production and determines the success of their scientific career. However, its
application in scientific evaluation is difficult due to the bi-dimensional
character of the distribution. Some bibliometric indexes that try to synthesize
in a numerical value the principal characteristics of this distribution have
been proposed recently. In contrast with other bibliometric measures, the
biases that the distribution tails provoke, are reduced by the h-index.
However, some limitations in the discrimination among researchers with
different publication habits are presented in this index. This index penalizes
selective researchers, distinguished by the large number of citations received,
as compared to large producers. In this work, two original sets of indexes, the
central area indexes and the central interval indexes, that complement the
h-index to include the central shape of the citation distribution, are proposed
and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5112</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5112</id><created>2013-04-18</created><authors><author><keyname>Wang</keyname><forenames>Chuang</forenames></author><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author></authors><title>Simplifying Generalized Belief Propagation on Redundant Region Graphs</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>15 pages, including 7 figures. Will be submitted to the Proceedings
  of the International Meeting on &quot;Inference, Computation, and Spinn Glasses&quot;,
  July 28-30, 2013, Sapporo, Japan</comments><journal-ref>Journal of Physics: Conference Series 473, 012004 (2013)</journal-ref><doi>10.1088/1742-6596/473/1/012004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cluster variation method has been developed into a general theoretical
framework for treating short-range correlations in many-body systems after it
was first proposed by Kikuchi in 1951. On the numerical side, a message-passing
approach called generalized belief propagation (GBP) was proposed by Yedidia,
Freeman and Weiss about a decade ago as a way of computing the minimal value of
the cluster variational free energy and the marginal distributions of clusters
of variables. However the GBP equations are often redundant, and it is quite a
non-trivial task to make the GBP iteration converges to a fixed point. These
drawbacks hinder the application of the GBP approach to finite-dimensional
frustrated and disordered systems.
  In this work we report an alternative and simple derivation of the GBP
equations starting from the partition function expression. Based on this
derivation we propose a natural and systematic way of removing the redundance
of the GBP equations. We apply the simplified generalized belief propagation
(SGBP) equations to the two-dimensional and the three-dimensional ferromagnetic
Ising model and Edwards-Anderson spin glass model. The numerical results
confirm that the SGBP message-passing approach is able to achieve satisfactory
performance on these model systems. We also suggest that a subset of the SGBP
equations can be neglected in the numerical iteration process without affecting
the final results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5140</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5140</id><created>2013-04-18</created><updated>2013-06-17</updated><authors><author><keyname>Rusu</keyname><forenames>Irena</forenames></author></authors><title>MinMax-Profiles: A Unifying View of Common Intervals, Nested Common
  Intervals and Conserved Intervals of K Permutations</title><categories>cs.DS</categories><comments>25 pages, 2 figures</comments><msc-class>68W32, 68Q25</msc-class><acm-class>F.2.3; G.2.1; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common intervals of K permutations over the same set of n elements were
firstly investigated by T. Uno and M.Yagiura (Algorithmica, 26:290:309, 2000),
who proposed an efficient algorithm to find common intervals when K=2. Several
particular classes of intervals have been defined since then, e.g. conserved
intervals and nested common intervals, with applications mainly in genome
comparison. Each such class, including common intervals, led to the development
of a specific algorithmic approach for K=2, and - except for nested common
intervals - for its extension to an arbitrary K.
  In this paper, we propose a common and efficient algorithmic framework for
finding different types of common intervals in a set P of K permutations, with
arbitrary K. Our generic algorithm is based on a global representation of the
information stored in P, called the MinMax-profile of P, and an efficient data
structure, called an LR-stack, that we introduce here. We show that common
intervals (and their subclasses of irreducible common intervals and same-sign
common intervals), nested common intervals (and their subclass of maximal
nested common intervals) as well as conserved intervals (and their subclass of
irreducible conserved intervals) may be obtained by appropriately setting the
parameters of our algorithm in each case. All the resulting algorithms run in
O(Kn+N)-time and need O(n) additional space, where N is the number of
solutions. The algorithms for nested common intervals and maximal nested common
intervals are new for K&gt;2, in the sense that no other algorithm has been given
so far to solve the problem with the same complexity, or better. The other
algorithms are as efficient as the best known algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5149</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5149</id><created>2013-04-18</created><updated>2014-02-10</updated><authors><author><keyname>Anshelevich</keyname><forenames>Elliot</forenames></author><author><keyname>Postl</keyname><forenames>John</forenames></author><author><keyname>Wexler</keyname><forenames>Tom</forenames></author></authors><title>Assignment Games with Conflicts: Price of Total Anarchy and Convergence
  Results via Semi-Smoothness</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study assignment games in which jobs select machines, and in which certain
pairs of jobs may conflict, which is to say they may incur an additional cost
when they are both assigned to the same machine, beyond that associated with
the increase in load. Questions regarding such interactions apply beyond
allocating jobs to machines: when people in a social network choose to align
themselves with a group or party, they typically do so based upon not only the
inherent quality of that group, but also who amongst their friends (or enemies)
choose that group as well. We show how semi-smoothness, a recently introduced
generalization of smoothness, is necessary to find tight or near-tight bounds
on the price of total anarchy, and thus on the quality of correlated and Nash
equilibria, for several natural job-assignment games with interacting jobs. For
most cases, our bounds on the price of total anarchy are either exactly 2 or
approach 2. We also prove new convergence results implied by semi-smoothness
for our games. Finally we consider coalitional deviations, and prove results
about the existence and quality of Strong equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5150</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5150</id><created>2013-04-18</created><authors><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Urbanke</keyname><forenames>Ruediger</forenames></author></authors><title>The Least Degraded and the Least Upgraded Channel with respect to a
  Channel Family</title><categories>cs.IT math.IT</categories><comments>Submitted to 2013 IEEE Information Theory Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a family of binary-input memoryless output-symmetric (BMS) channels
having a fixed capacity, we derive the BMS channel having the highest (resp.
lowest) capacity among all channels that are degraded (resp. upgraded) with
respect to the whole family. We give an explicit characterization of this
channel as well as an explicit formula for the capacity of this channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5153</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5153</id><created>2013-04-18</created><updated>2013-06-28</updated><authors><author><keyname>Girard</keyname><forenames>Antoine</forenames></author></authors><title>A composition theorem for bisimulation functions</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard engineering approach to modelling of complex systems is highly
compositional. In order to be able to understand (or to control) the behavior
of a complex dynamical systems, it is often desirable, if not necessary, to
view this system as an interconnection of smaller interacting subsystems, each
of these subsystems having its own functionalities. In this paper, we propose a
compositional approach to the computation of bisimulation functions for
dynamical systems. Bisimulation functions are quantitative generalizations of
the classical bisimulation relations. They have been shown useful for
simulation-based verification or for the computation of approximate symbolic
abstractions of dynamical systems. In this technical note, we present a
constructive result for the composition of bisimulation functions. For a
complex dynamical system consisting of several interconnected subsystems, it
allows us to compute a bisimulation function from the knowledge of a
bisimulation function for each of the subsystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5159</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5159</id><created>2013-04-18</created><authors><author><keyname>Hoang</keyname><forenames>Trong Nghia</forenames></author><author><keyname>Low</keyname><forenames>Kian Hsiang</forenames></author></authors><title>Interactive POMDP Lite: Towards Practical Planning to Predict and
  Exploit Intentions for Interacting with Self-Interested Agents</title><categories>cs.AI cs.MA</categories><comments>23rd International Joint Conference on Artificial Intelligence (IJCAI
  2013), Extended version with proofs, 24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key challenge in non-cooperative multi-agent systems is that of developing
efficient planning algorithms for intelligent agents to interact and perform
effectively among boundedly rational, self-interested agents (e.g., humans).
The practicality of existing works addressing this challenge is being
undermined due to either the restrictive assumptions of the other agents'
behavior, the failure in accounting for their rationality, or the prohibitively
expensive cost of modeling and predicting their intentions. To boost the
practicality of research in this field, we investigate how intention prediction
can be efficiently exploited and made practical in planning, thereby leading to
efficient intention-aware planning frameworks capable of predicting the
intentions of other agents and acting optimally with respect to their predicted
intentions. We show that the performance losses incurred by the resulting
planning policies are linearly bounded by the error of intention prediction.
Empirical evaluations through a series of stochastic games demonstrate that our
policies can achieve better and more robust performance than the
state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5164</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5164</id><created>2013-04-18</created><authors><author><keyname>Cosentino</keyname><forenames>Alessandro</forenames></author><author><keyname>Kothari</keyname><forenames>Robin</forenames></author><author><keyname>Paetznick</keyname><forenames>Adam</forenames></author></authors><title>Dequantizing read-once quantum formulas</title><categories>quant-ph cs.CC</categories><comments>14 pages, 8 figures, to appear in proceedings of TQC 2013</comments><journal-ref>8th Conference on the Theory of Quantum Computation, Communication
  and Cryptography (TQC 2013), Leibniz International Proceedings in Informatics
  (LIPIcs) 22, pp. 80-92 (2013)</journal-ref><doi>10.4230/LIPIcs.TQC.2013.80</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum formulas, defined by Yao [FOCS '93], are the quantum analogs of
classical formulas, i.e., classical circuits in which all gates have fanout
one. We show that any read-once quantum formula over a gate set that contains
all single-qubit gates is equivalent to a read-once classical formula of the
same size and depth over an analogous classical gate set. For example, any
read-once quantum formula over Toffoli and single-qubit gates is equivalent to
a read-once classical formula over Toffoli and NOT gates. We then show that the
equivalence does not hold if the read-once restriction is removed. To show the
power of quantum formulas without the read-once restriction, we define a new
model of computation called the one-qubit model and show that it can compute
all boolean functions. This model may also be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5168</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5168</id><created>2013-04-18</created><authors><author><keyname>Liu</keyname><forenames>Jialu</forenames></author></authors><title>Image Retrieval based on Bag-of-Words model</title><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article gives a survey for bag-of-words (BoW) or bag-of-features model
in image retrieval system. In recent years, large-scale image retrieval shows
significant potential in both industry applications and research problems. As
local descriptors like SIFT demonstrate great discriminative power in solving
vision problems like object recognition, image classification and annotation,
more and more state-of-the-art large scale image retrieval systems are trying
to rely on them. A common way to achieve this is first quantizing local
descriptors into visual words, and then applying scalable textual indexing and
retrieval schemes. We call this model as bag-of-words or bag-of-features model.
The goal of this survey is to give an overview of this model and introduce
different strategies when building the system based on this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5181</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5181</id><created>2013-04-18</created><authors><author><keyname>Khairnar</keyname><forenames>Vaishali D.</forenames></author><author><keyname>Pradhan</keyname><forenames>S. N.</forenames></author></authors><title>Comparative Study of Simulation for Vehicular Ad-hoc Network</title><categories>cs.NI</categories><comments>4 pages, International Journal of Computer Applications</comments><doi>10.5120/7196-9967</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have discussed about the number of automobiles that has been
increased on the road in the past few years. Due to high density of vehicles,
the potential threats and road accident is increasing. Wireless technology is
aiming to equip technology in vehicles to reduce these factors by sending
messages to each other. The vehicular safety application should be thoroughly
tested before it is deployed in a real world to use. Simulator tool has been
preferred over out door experiment because it simple, easy and cheap. VANET
requires that a traffic and network simulator should be used together to
perform this test. Many tools exist for this purpose but most of them have the
problem with the proper interaction. Simulating vehicular networks with
external stimulus to analyze its effect on wireless communication but to do
this job a good simulator is also needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5185</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5185</id><created>2013-04-18</created><updated>2013-04-30</updated><authors><author><keyname>Artale</keyname><forenames>Alessandro</forenames></author><author><keyname>Kontchakov</keyname><forenames>Roman</forenames></author><author><keyname>Wolter</keyname><forenames>Frank</forenames></author><author><keyname>Zakharyaschev</keyname><forenames>Michael</forenames></author></authors><title>Temporal Description Logic for Ontology-Based Data Access (Extended
  Version)</title><categories>cs.LO cs.AI</categories><comments>Full version of the IJCAI 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our aim is to investigate ontology-based data access over temporal data with
validity time and ontologies capable of temporal conceptual modelling. To this
end, we design a temporal description logic, TQL, that extends the standard
ontology language OWL 2 QL, provides basic means for temporal conceptual
modelling and ensures first-order rewritability of conjunctive queries for
suitably defined data instances with validity time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5189</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5189</id><created>2013-04-18</created><authors><author><keyname>Khairnar</keyname><forenames>Vaishali D.</forenames></author><author><keyname>Pradhan</keyname><forenames>S. N.</forenames></author></authors><title>Mobility Models for Vehicular Ad-hoc Network Simulation</title><categories>cs.NI</categories><comments>5 pages, 12 figures, International Journal of Computer Applications</comments><doi>10.5120/1573-2103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the emerging applications that belong to ambient systems is to
transparently and directly interconnect vehicles on roads, making an ad hoc
network that enables a variety of applications through distributed software
without the need of any fixed and dedicated infrastructure. The network as well
as the embedded computers and sensors in the vehicle will be invisible to the
driver, who will get the required services during his journey. New type of ad
hoc network is the Vehicular Ad hoc Network, in which vehicles constitute the
mobile nodes in the network. Due to the prohibitive cost of deploying and
implementing such as system in a real world, most research work in VANET relies
on simulations for evaluation purpose. The key concept for VANET simulations is
a real world vehicular mobility model which will ensures conclusions drawn from
simulation experiments will carry through to real world deployments. In this
paper we present a tool SUMO, MOVE that allows users to easily generate real
world mobility models for VANET simulations. MOVE tool is built on top of SUMO
which is open source micro traffic simulator. Output of MOVE is a real world
mobility model and can used by NS-2 and Qual-net simulator. In this paper we
evaluate and compare ad hoc routing performance for vehicular nodes using MOVE,
which is using random way point model. The simulation results are obtained when
nodes are moving according to a real world mobility model which is
significantly different from that of the generally used random way point model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5190</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5190</id><created>2013-04-18</created><authors><author><keyname>Patil</keyname><forenames>Saurabh D.</forenames></author><author><keyname>Thombare</keyname><forenames>D. V.</forenames></author><author><keyname>Khairnar</keyname><forenames>Vaishali D.</forenames></author></authors><title>DEMO: Simulation of Realistic Mobility Model and Implementation of
  802.11p (DSRC) for Vehicular Networks (VANET)</title><categories>cs.NI</categories><comments>4 pages, 6 figures, International Journal of Computer Application</comments><doi>10.5120/6390-8811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ad hoc network of vehicles (VANET) consists of vehicles that exchange
information via radio in order to improve road safety, traffic management and
do better distribution of traffic load in time and space. Along with this it
allows Internet access for passengers and users of vehicles. A significant
characteristic while studying VANETs is the requirement of having a mobility
model that gives aspects of real vehicular traffic. These scenarios play an
important role in performance of VANETs. In our paper we have demonstration and
description of generating realistic mobility model using various tools such as
eWorld, OpenStreetMap, SUMO and TraNS. Generated mobility scenario is added to
NS-2.34 (Network Simulator) for analysis of DSR and AODV routing protocol under
802.11p (DSRC/WAVE) and 802.11a. Results after analysis shows 802.11p is more
suitable than 802.11a for VANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5197</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5197</id><created>2013-04-18</created><authors><author><keyname>D'Elia</keyname><forenames>Daniele Cono</forenames></author><author><keyname>Demetrescu</keyname><forenames>Camil</forenames></author><author><keyname>Finocchi</keyname><forenames>Irene</forenames></author></authors><title>Ball-Larus Path Profiling Across Multiple Loop iterations</title><categories>cs.PL cs.PF</categories><comments>13 pages, 14 figures</comments><acm-class>C.4; D.2.2; D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the hottest paths in the control flow graph of a routine can
direct optimizations to portions of the code where most resources are consumed.
This powerful methodology, called path profiling, was introduced by Ball and
Larus in the mid 90s and has received considerable attention in the last 15
years for its practical relevance. A shortcoming of Ball-Larus path profiling
was the inability to profile cyclic paths, making it difficult to mine
interesting execution patterns that span multiple loop iterations. Previous
results, based on rather complex algorithms, have attempted to circumvent this
limitation at the price of significant performance losses already for a small
number of iterations. In this paper, we present a new approach to multiple
iterations path profiling, based on data structures built on top of the
original Ball-Larus numbering technique. Our approach allows it to profile all
executed paths obtained as a concatenation of up to k Ball-Larus acyclic paths,
where k is a user-defined parameter. An extensive experimental investigation on
a large variety of Java benchmarks on the Jikes RVM shows that, surprisingly,
our approach can be even faster than Ball-Larus due to fewer operations on
smaller hash tables, producing compact representations of cyclic paths even for
large values of k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5199</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5199</id><created>2013-04-16</created><authors><author><keyname>Gaudel</keyname><forenames>M. C.</forenames></author><author><keyname>Lassaigne</keyname><forenames>R.</forenames></author><author><keyname>Magniez</keyname><forenames>F.</forenames></author><author><keyname>de Rougemont</keyname><forenames>M.</forenames></author></authors><title>Some approximations in Model Checking and Testing</title><categories>cs.LO cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking and testing are two areas with a similar goal: to verify that
a system satisfies a property. They start with different hypothesis on the
systems and develop many techniques with different notions of approximation,
when an exact verification may be computationally too hard. We present some
notions of approximation with their logic and statistics backgrounds, which
yield several techniques for model checking and testing: Bounded Model
Checking, Approximate Model Checking, Approximate Black-Box Checking,
Approximate Model-based Testing and Approximate Probabilistic Model Checking.
All these methods guarantee some quality and efficiency of the verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5212</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5212</id><created>2013-04-18</created><authors><author><keyname>Chau</keyname><forenames>Duc Phu</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Bremond</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Thonnat</keyname><forenames>Monique</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>Object Tracking in Videos: Approaches and Issues</title><categories>cs.CV</categories><proxy>ccsd</proxy><journal-ref>The International Workshop &quot;Rencontres UNS-UD&quot; (RUNSUD) (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile object tracking has an important role in the computer vision
applications. In this paper, we use a tracked target-based taxonomy to present
the object tracking algorithms. The tracked targets are divided into three
categories: points of interest, appearance and silhouette of mobile objects.
Advantages and limitations of the tracking approaches are also analyzed to find
the future directions in the object tracking domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5213</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5213</id><created>2013-04-18</created><authors><author><keyname>SalahEldeen</keyname><forenames>Hany M.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Carbon Dating The Web: Estimating the Age of Web Resources</title><categories>cs.IR cs.DL</categories><comments>This work is published at TempWeb03 workshop at WWW 2013 conference
  in Rio de Janeiro, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the course of web research it is often necessary to estimate the creation
datetime for web resources (in the general case, this value can only be
estimated). While it is feasible to manually establish likely datetime values
for small numbers of resources, this becomes infeasible if the collection is
large. We present &quot;carbon date&quot;, a simple web application that estimates the
creation date for a URI by polling a number of sources of evidence and
returning a machine-readable structure with their respective values. To
establish a likely datetime, we poll bitly for the first time someone shortened
the URI, topsy for the first time someone tweeted the URI, a Memento aggregator
for the first time it appeared in a public web archive, Google's time of last
crawl, and the Last-Modified HTTP response header of the resource itself. We
also examine the backlinks of the URI as reported by Google and apply the same
techniques for the resources that link to the URI. We evaluated our tool on a
gold-standard data set of 1200 URIs in which the creation date was manually
verified. We were able to estimate a creation date for 75.90% of the resources,
with 32.78% having the correct value. Given the different nature of the URIs,
the union of the various methods produces the best results. While the Google
last crawl date and topsy account for nearly 66% of the closest answers,
eliminating the web archives or Last-Modified from the results produces the
largest overall negative impact on the results. The carbon date application is
available for download or use via a webAPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5214</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5214</id><created>2013-04-18</created><updated>2014-02-10</updated><authors><author><keyname>Bank</keyname><forenames>Bernd</forenames><affiliation>LIX</affiliation></author><author><keyname>Giusti</keyname><forenames>Marc</forenames><affiliation>LIX</affiliation></author><author><keyname>Heintz</keyname><forenames>Joos</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author></authors><title>Intrinsic complexity estimates in polynomial optimization</title><categories>cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that point searching in basic semialgebraic sets and the search
for globally minimal points in polynomial optimization tasks can be carried out
using $(s\,d)^{O(n)}$ arithmetic operations, where $n$ and $s$ are the numbers
of variables and constraints and $d$ is the maximal degree of the polynomials
involved.\spar \noindent We associate to each of these problems an intrinsic
system degree which becomes in worst case of order $(n\,d)^{O(n)}$ and which
measures the intrinsic complexity of the task under consideration.\spar
\noindent We design non-uniformly deterministic or uniformly probabilistic
algorithms of intrinsic, quasi-polynomial complexity which solve these
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5220</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5220</id><created>2013-04-18</created><updated>2014-09-22</updated><authors><author><keyname>Mondelli</keyname><forenames>Marco</forenames></author><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>Scaling Exponent of List Decoders with Applications to Polar Codes</title><categories>cs.IT math.IT</categories><comments>14 pages, submitted to IEEE Trans. Inform. Theory and presented in
  part to ITW'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the significant performance gains which polar codes experience
under successive cancellation list decoding, their scaling exponent is studied
as a function of the list size. In particular, the error probability is fixed
and the trade-off between block length and back-off from capacity is analyzed.
A lower bound is provided on the error probability under $\rm MAP$ decoding
with list size $L$ for any binary-input memoryless output-symmetric channel and
for any class of linear codes such that their minimum distance is unbounded as
the block length grows large. Then, it is shown that under $\rm MAP$ decoding,
although the introduction of a list can significantly improve the involved
constants, the scaling exponent itself, i.e., the speed at which capacity is
approached, stays unaffected for any finite list size. In particular, this
result applies to polar codes, since their minimum distance tends to infinity
as the block length increases. A similar result is proved for genie-aided
successive cancellation decoding when transmission takes place over the binary
erasure channel, namely, the scaling exponent remains constant for any fixed
number of helps from the genie. Note that since genie-aided successive
cancellation decoding might be strictly worse than successive cancellation list
decoding, the problem of establishing the scaling exponent of the latter
remains open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5232</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5232</id><created>2013-04-18</created><updated>2015-07-16</updated><authors><author><keyname>Radulescu</keyname><forenames>Anca</forenames></author></authors><title>Neural network spectral robustness under perturbations of the underlying
  graph</title><categories>q-bio.NC cs.DM math.CO</categories><comments>manuscript: 27 pages; references: 5 pages; 2 appendices; 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have been using graph theoretical approaches to model complex
networks (such as social, infrastructural or biological networks), and how
their hardwired circuitry relates to their dynamic evolution in time.
Understanding how configuration reflects on the coupled behavior in a system of
dynamic nodes can be of great importance, for example in the context of how the
brain connectome is affecting brain function. However, the connectivity
patterns that appear in brain networks, and their individual effects on network
dynamics, are far from being fully understood.
  We study the connections between edge configuration and dynamics in a simple
oriented network composed of two interconnected cliques (representative of
brain feedback regulatory circuitry). In this paper, our main goal is to study
the spectra of the graph adjacency and Laplacian matrices, with a focus on
three aspects in particular: (1) the sensitivity/robustness the spectrum in
response to varying the intra and inter-modular edge density, (2) the effects
on the spectrum of perturbing the edge configuration, while keeping the
densities fixed and (3) the effects of increasing the network size. We study
some tractable aspects analytically, then simulate more general results
numerically. This paper aims to clarify, from analytical and modeling
perspectives, the underpinnings of our related work, which further addresses
how graph properties affect the network's temporal dynamics and phase
transitions.
  We propose that this type of results may be helpful when studying small
networks such as macroscopic brain circuits. We suggest potential applications
to understanding synaptic restructuring in learning networks, and the effects
of network configuration to function of emotion-regulatory neural circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5247</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5247</id><created>2013-04-18</created><updated>2013-10-13</updated><authors><author><keyname>Zwirn</keyname><forenames>Herve</forenames></author></authors><title>Computational Irreducibility and Computational Analogy</title><categories>cs.CC</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous paper, we provided a formal definition for the concept of
computational irreducibility (CIR), i.e. the fact for a function f from N to N
that it is impossible to compute f(n) without following approximately the same
path than computing successively all the values f(i) from i=1 to n. Our
definition is based on the concept of E Turing machines (for Enumerating Turing
Machines) and on the concept of approximation of E Turing machines for which we
also gave a formal definition. We precise here these definitions through some
modifications intended to improve the robustness of the concept. We introduce
then a new concept: the Computational Analogy and prove some properties of
computationally analog functions. Computational Analogy is an equivalence
relation which allows partitioning the set of computable functions in classes
whose members have the same properties regarding to their computational
irreducibility and their computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5251</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5251</id><created>2013-04-18</created><authors><author><keyname>Khan</keyname><forenames>Yousuf Ibrahim</forenames></author></authors><title>Applications of Dynamical Systems in Engineering</title><categories>cs.SY</categories><comments>32 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the current possible applications of Dynamical Systems in
Engineering. The applications of chaos, fractals have proven to be an exciting
and fruitful endeavor. These applications are highly diverse ranging over such
fields as Electrical, Electronics and Computer Engineering. Dynamical Systems
theory describes general patterns found in the solution of systems of nonlinear
equations. The theory focuses upon those equations representing the change of
processes in time. This paper offers the issue of applying dynamical systems
methods to a wider circle of Engineering problems. There are three components
to our approach: ongoing and possible applications of Fractals, Chaos Theory
and Dynamical Systems. Some basic and useful computer simulation of Dynamical
System related problems have been shown also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5257</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5257</id><created>2013-04-18</created><updated>2013-04-26</updated><authors><author><keyname>Hansen</keyname><forenames>Michael</forenames></author><author><keyname>Goldstone</keyname><forenames>Robert L.</forenames></author><author><keyname>Lumsdaine</keyname><forenames>Andrew</forenames></author></authors><title>What Makes Code Hard to Understand?</title><categories>cs.SE</categories><comments>19 pages, 2 figures</comments><acm-class>H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What factors impact the comprehensibility of code? Previous research suggests
that expectation-congruent programs should take less time to understand and be
less prone to errors. We present an experiment in which participants with
programming experience predict the exact output of ten small Python programs.
We use subtle differences between program versions to demonstrate that
seemingly insignificant notational changes can have profound effects on
correctness and response times. Our results show that experience increases
performance in most cases, but may hurt performance significantly when
underlying assumptions about related code statements are violated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5260</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5260</id><created>2013-04-18</created><updated>2013-07-01</updated><authors><author><keyname>Akhmetzhanov</keyname><forenames>Andrei R.</forenames></author><author><keyname>Worden</keyname><forenames>Lee</forenames></author><author><keyname>Dushoff</keyname><forenames>Jonathan</forenames></author></authors><title>Effects of mixing in threshold models of social behavior</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We consider the dynamics of an extension of the influential Granovetter model
of social behavior, where individuals are affected by their personal
preferences and observation of the neighbors' behavior. Individuals are
arranged in a network (usually, the square lattice) and each has a state and a
fixed threshold for behavior changes. We simulate the system asynchronously
either by picking a random individual and either update its state or exchange
it with another randomly chosen individual (mixing). We describe the dynamics
analytically in the fast-mixing limit by using the mean-field approximation and
investigate it mainly numerically in case of a finite mixing. We show that the
dynamics converge to a manifold in state space, which determines the possible
equilibria, and show how to estimate the projection of manifold by using
simulated trajectories, emitted from different initial points.
  We show that the effects of considering the network can be decomposed into
finite-neighborhood effects, and finite-mixing-rate effects, which have
qualitatively similar effects. Both of these effects increase the tendency of
the system to move from a less-desired equilibrium to the &quot;ground state&quot;. Our
findings can be used to probe shifts in behavioral norms and have implications
for the role of information flow in determining when social norms that have
become unpopular (such as foot binding or female genital cutting) persist or
vanish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5264</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5264</id><created>2013-04-18</created><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>An optimal lower bound for monotonicity testing over hypergrids</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For positive integers $n, d$, consider the hypergrid $[n]^d$ with the
coordinate-wise product partial ordering denoted by $\prec$. A function $f:
[n]^d \mapsto \mathbb{N}$ is monotone if $\forall x \prec y$, $f(x) \leq f(y)$.
A function $f$ is $\eps$-far from monotone if at least an $\eps$-fraction of
values must be changed to make $f$ monotone. Given a parameter $\eps$, a
\emph{monotonicity tester} must distinguish with high probability a monotone
function from one that is $\eps$-far.
  We prove that any (adaptive, two-sided) monotonicity tester for functions
$f:[n]^d \mapsto \mathbb{N}$ must make $\Omega(\eps^{-1}d\log n - \eps^{-1}\log
\eps^{-1})$ queries. Recent upper bounds show the existence of $O(\eps^{-1}d
\log n)$ query monotonicity testers for hypergrids. This closes the question of
monotonicity testing for hypergrids over arbitrary ranges. The previous best
lower bound for general hypergrids was a non-adaptive bound of $\Omega(d \log
n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5278</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5278</id><created>2013-04-18</created><authors><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author><author><keyname>Sickert</keyname><forenames>Salomon</forenames></author></authors><title>On Refinements of Boolean and Parametric Modal Transition Systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the extensions of modal transition systems (MTS), namely Boolean
MTS and parametric MTS and we investigate the refinement problems over both
classes. Firstly, we reduce the problem of modal refinement over both classes
to a problem solvable by a QBF solver and provide experimental results showing
our technique scales well. Secondly, we extend the algorithm for thorough
refinement of MTS providing better complexity then via reductions to previously
studied problems. Finally, we investigate the relationship between modal and
thorough refinement on the two classes and show how the thorough refinement can
be approximated by the modal refinement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5281</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5281</id><created>2013-04-18</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Gaiser</keyname><forenames>Andreas</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author></authors><title>Automata with Generalized Rabin Pairs for Probabilistic Model Checking
  and LTL Synthesis</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model-checking problem for probabilistic systems crucially relies on the
translation of LTL to deterministic Rabin automata (DRW). Our recent Safraless
translation for the LTL(F,G) fragment produces smaller automata as compared to
the traditional approach. In this work, instead of DRW we consider
deterministic automata with acceptance condition given as disjunction of
generalized Rabin pairs (DGRW). The Safraless translation of LTL(F,G) formulas
to DGRW results in smaller automata as compared to DRW. We present algorithms
for probabilistic model-checking as well as game solving for DGRW conditions.
Our new algorithms lead to improvement both in terms of theoretical bounds as
well as practical evaluation. We compare PRISM with and without our new
translation, and show that the new translation leads to significant
improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5297</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5297</id><created>2013-04-18</created><authors><author><keyname>Anshari</keyname><forenames>Muhammad</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author></authors><title>Shifting Role of Customer from Recipient To Partner of Care In
  Healthcare Organization</title><categories>cs.CY</categories><comments>The 13th ASEAN Graduate Business and Economics Program (AGBEP) Annual
  Meeting &amp; Conference April, 2 2013</comments><report-no>13 p. 11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most recent e-health initiatives perceive customers (patients) as recipients
of medical care where they do not have a significant role in the process of
health decision making. However, the advancement of Web 2.0 offers patients to
have a greater role in the decision making process related to their health as
they can be empowered with the ability to access and control information that
fits with their personalized needs. However, providing patient empowerment in
e-health through Web 2.0 is challenging task because the complexity nature of
healthcare business processes. Empowerment closely relates to the concept of
Customer Relationship Management (CRM) in managing good relationships with the
customers. The adoption of Web 2.0 in CRM systems is known as Social CRM or CRM
2.0. Social CRM emerges to accommodate dynamic means of interaction between
patients with their healthcare providers. The aim of this paper is to present a
model that embeds empowerment of patient through Social CRM intervention that
may extend the role of the patient as an individual health actor, a social
health agent, and a medical care partner. A survey has been conducted to gain a
feedback from customers regarding the proposed model. A prototype derived from
the model namely Clinic 2.0 has also been developed. Using the prototype we
measure its impact towards customer satisfaction and health literacy. The
results show that the system intervention through Clinic 2.0 improves the level
of satisfaction and health literacy of participants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5299</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5299</id><created>2013-04-18</created><updated>2014-02-14</updated><authors><author><keyname>Korattikara</keyname><forenames>Anoop</forenames></author><author><keyname>Chen</keyname><forenames>Yutian</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget</title><categories>cs.LG stat.ML</categories><comments>v4 - version accepted by ICML2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can we make Bayesian posterior MCMC sampling more efficient when faced with
very large datasets? We argue that computing the likelihood for N datapoints in
the Metropolis-Hastings (MH) test to reach a single binary decision is
computationally inefficient. We introduce an approximate MH rule based on a
sequential hypothesis test that allows us to accept or reject samples with high
confidence using only a fraction of the data required for the exact MH rule.
While this method introduces an asymptotic bias, we show that this bias can be
controlled and is more than offset by a decrease in variance due to our ability
to draw more samples per unit of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5302</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5302</id><created>2013-04-18</created><authors><author><keyname>Eguchi</keyname><forenames>Satoshi</forenames></author></authors><title>&quot;Superluminal&quot; FITS File Processing on Multiprocessors: Zero Time Endian
  Conversion Technique</title><categories>astro-ph.IM cs.PF</categories><comments>25 pages, 9 figures, 12 tables, accepted for publication in PASP</comments><doi>10.1086/671105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FITS is the standard file format in astronomy, and it has been extended
to agree with astronomical needs of the day. However, astronomical datasets
have been inflating year by year. In case of ALMA telescope, a ~ TB scale
4-dimensional data cube may be produced for one target. Considering that
typical Internet bandwidth is a few 10 MB/s at most, the original data cubes in
FITS format are hosted on a VO server, and the region which a user is
interested in should be cut out and transferred to the user (Eguchi et al.,
2012). The system will equip a very high-speed disk array to process a TB scale
data cube in a few 10 seconds, and disk I/O speed, endian conversion and data
processing one will be comparable. Hence to reduce the endian conversion time
is one of issues to realize our system. In this paper, I introduce a technique
named &quot;just-in-time endian conversion&quot;, which delays the endian conversion for
each pixel just before it is really needed, to sweep out the endian conversion
time; by applying this method, the FITS processing speed increases 20% for
single threading, and 40% for multi-threading compared to CFITSIO. The speed-up
by the method tightly relates to modern CPU architecture to improve the
efficiency of instruction pipelines due to break of &quot;causality&quot;, a programmed
instruction code sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5304</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5304</id><created>2013-04-19</created><authors><author><keyname>Torrieri</keyname><forenames>Don</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>Exclusion and Guard Zones in DC-CDMA Ad Hoc Networks</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Communications. arXiv admin note:
  substantial text overlap with arXiv:1207.2825</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The central issue in direct-sequence code-division multiple-access (DS-CDMA)
ad hoc networks is the prevention of a near-far problem. This paper considers
two types of guard zones that may be used to control the near-far problem: a
fundamental exclusion zone and an additional CSMA guard zone that may be
established by the carrier-sense multiple-access (CSMA) protocol. In the
exclusion zone, no mobiles are physically present, modeling the minimum
physical separation among mobiles that is always present in actual networks.
Potentially interfering mobiles beyond a transmitting mobile's exclusion zone,
but within its CSMA guard zone, are deactivated by the protocol. This paper
provides an analysis of DS-CSMA networks with either or both types of guard
zones. A network of finite extent with a finite number of mobiles and uniform
clustering as the spatial distribution is modeled. The analysis applies a
closed-form expression for the outage probability in the presence of Nakagami
fading, conditioned on the network geometry. The tradeoffs between exclusion
zones and CSMA guard zones are explored for DS-CDMA and unspread networks. The
spreading factor and the guard-zone radius provide design flexibility in
achieving specified levels of average outage probability and transmission
capacity. The advantage of an exclusion zone over a CSMA guard zone is that
since the network is not thinned, the number of active mobiles remains
constant, and higher transmission capacities can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5313</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5313</id><created>2013-04-19</created><authors><author><keyname>Prajapati</keyname><forenames>Somesh Kumar</forenames></author><author><keyname>Changder</keyname><forenames>Suvamoy</forenames></author><author><keyname>Sarkar</keyname><forenames>Anirban</forenames></author></authors><title>Trust Management Model for Cloud Computing Environment</title><categories>cs.CR</categories><comments>5 Pages, 2 Figures, Conference</comments><journal-ref>Somesh Kumar Prajapati, Suvamoy Changder, Anirban Sarkar, &quot;Trust
  Management Model for Cloud Computing Environment&quot;, International Conference
  on Computing Communication and Advanced Network, India, PP 1-5, March 15-17,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software as a service or (SaaS) is a new software development and deployment
paradigm over the cloud and offers Information Technology services dynamically
as &quot;on-demand&quot; basis over the internet. Trust is one of the fundamental
security concepts on storing and delivering such services. In general, trust
factors are integrated into such existent security frameworks in order to add a
security level to entities collaborations through the trust relationship.
However, deploying trust factor in the secured cloud environment are more
complex engineering task due to the existence of heterogeneous types of service
providers and consumers. In this paper, a formal trust management model has
been introduced to manage the trust and its properties for SaaS in cloud
computing environment. The model is capable to represent the direct trust,
recommended trust, reputation etc. formally. For the analysis of the trust
properties in the cloud environment, the proposed approach estimates the trust
value and uncertainty of each peer by computing decay function, number of
positive interactions, reputation factor and satisfaction level for the
collected information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5315</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5315</id><created>2013-04-19</created><authors><author><keyname>Kim</keyname><forenames>Joongheon</forenames></author><author><keyname>Tian</keyname><forenames>Yafei</forenames></author><author><keyname>Mangold</keyname><forenames>Stefan</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Quality-Aware Coding and Relaying for 60 GHz Real-Time Wireless Video
  Broadcasting</title><categories>cs.NI</categories><comments>5 pages, 8 figures, To appear in IEEE International Conference on
  Communications (ICC), 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Wireless streaming of high-definition video is a promising application for 60
GHz links, since multi-Gigabit/s data rates are possible. In particular we
consider a sports stadium broadcasting system where video signals from multiple
cameras are transmitted to a central location. Due to the high pathloss of
60\,GHz radiation over the large distances encountered in this setting, the use
of relays is required. This paper designs a quality-aware coding and relaying
algorithm for maximization of the overall video quality. We consider the
setting that the source can split its data stream into parallel streams, which
can be transmitted via different relays to the destination. For this, we derive
the related formulation and re-formulate it as convex programming, which can
guarantee optimal solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5317</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5317</id><created>2013-04-19</created><authors><author><keyname>Kachewar</keyname><forenames>Rohan R.</forenames></author></authors><title>K model for designing Data Driven Test Automation Frameworks and its
  Design Architecture Snow Leopard</title><categories>cs.SE</categories><journal-ref>International Journal of Computer Applications 09758887 Volume 31
  No.7, October 2011</journal-ref><doi>10.5120/3835-5331</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated testing improves the efficiency of testing practice at various
levels of projects in the organization. Unfortunately, we do not have a common
architecture or common standards for designing frameworks across different test
levels, projects and test tools which can assist developers, testers and
business analysts. To address the above problem, in this paper, I have first
proposed a unique reference model and then a design architecture using the
proposed model for designing any Data Driven Automation Frameworks. The
reference model is K model which can be used for modeling any data-driven
automation framework. The unique Design architecture, based on above model is
Snow Leopard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5319</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5319</id><created>2013-04-19</created><authors><author><keyname>Kiechle</keyname><forenames>Martin</forenames></author><author><keyname>Hawe</keyname><forenames>Simon</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>A Joint Intensity and Depth Co-Sparse Analysis Model for Depth Map
  Super-Resolution</title><categories>cs.CV</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-resolution depth maps can be inferred from low-resolution depth
measurements and an additional high-resolution intensity image of the same
scene. To that end, we introduce a bimodal co-sparse analysis model, which is
able to capture the interdependency of registered intensity and depth
information. This model is based on the assumption that the co-supports of
corresponding bimodal image structures are aligned when computed by a suitable
pair of analysis operators. No analytic form of such operators exist and we
propose a method for learning them from a set of registered training signals.
This learning process is done offline and returns a bimodal analysis operator
that is universally applicable to natural scenes. We use this to exploit the
bimodal co-sparse analysis model as a prior for solving inverse problems, which
leads to an efficient algorithm for depth map super-resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5330</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5330</id><created>2013-04-19</created><authors><author><keyname>You</keyname><forenames>Lizhao</forenames></author><author><keyname>Yuan</keyname><forenames>Zimu</forenames></author><author><keyname>Tang</keyname><forenames>Bin</forenames></author><author><keyname>Chen</keyname><forenames>Guihai</forenames></author></authors><title>Minimum Latency Broadcast Scheduling in Single-Radio Multi-Channel
  Wireless Ad-Hoc Networks</title><categories>cs.NI</categories><comments>6 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We study the minimum latency broadcast scheduling (MLBS) problem in
Single-Radio Multi-Channel (SR-MC) wireless ad-hoc networks (WANETs), which are
modeled by Unit Disk Graphs. Nodes with this capability have their fixed
reception channels, but can switch their transmission channels to communicate
with their neighbors. The single-radio and multi-channel model prevents
existing algorithms for single-channel networks achieving good performance.
First, the common assumption that one transmission reaches all the neighboring
nodes does not hold naturally. Second, the multi-channel dimension provides new
opportunities to schedule the broadcast transmissions in parallel. We show MLBS
problem in SR-MC WANETs is NP-hard, and present a benchmark algorithm: Basic
Transmission Scheduling (BTS), which has approximation ratio of 4k + 12. Here k
is the number of orthogonal channels in SR-MC WANETs. Then we propose an
Enhanced Transmission Scheduling (ETS) algorithm, improving the approximation
ratio to k + 23. Simulation results show that ETS achieves better performance
over BTS, and the performance of ETS approaches the lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5334</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5334</id><created>2013-04-19</created><updated>2014-08-03</updated><authors><author><keyname>Yu</keyname><forenames>Seung Min</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Optimization of Spectrum Allocation and Subsidization in Mobile
  Communication Services</title><categories>cs.GT cs.NI</categories><comments>This paper has been withdrawn by the author due to some errors in the
  proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile traffic explosion causes spectrum shortage and polarization of data
usage among users, which will eventually decrease user welfare in mobile
communication services. Governments around the world are planning to make more
spectrum available for mobile broadband use, and the key policy issue is to
find an efficient spectrum allocation method that will improve user welfare. In
this paper, we propose a data subsidy scheme where the regulator offers a
spectrum price discount to mobile network operators (MNOs) in return for
imposing the responsibility of providing a predefined data amount to users free
of charge. To analyze the subsidy effect, we adopt the two-stage approach of
Cournot and Bertrand competition, and find a Nash equilibrium of the
competition. An interesting observation is that the increase in user welfare
does not involve MNO profit loss and the increasing amount is higher than the
regulator's expenses for implementing the data subsidy scheme. The most of the
paper is for the duopoly competition, which is extended to the general case,
finally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5336</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5336</id><created>2013-04-19</created><authors><author><keyname>Dover</keyname><forenames>Thomas P.</forenames></author></authors><title>Legacy Forensics: An Emerging Challenge</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the passage of time and as new types of storage devices are introduced
into the marketplace, contemporary devices will slowly lose their compatibility
with current operating systems and PC hardware. As a result, such legacy
devices will pose an analytical challenge to the field of digital forensics.
Dated technology, while still fully functional, is becoming increasingly
incompatible with most contemporary computing hardware and software and thus
cannot be properly examined in present-day digital forensic environments. This
fact will not be lost on those who utilize legacy hardware to commit criminal
acts. This paper describes the technical challenge of accessing legacy devices
by describing an effort to resuscitate a Bernoulli Drive, a portable storage
device manufactured in 1983 by Iomega Corporation. A number of lessons-learned
are provided and the implication of legacy devices to digital forensic science
is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5346</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5346</id><created>2013-04-19</created><authors><author><keyname>Briguglio</keyname><forenames>Luigi</forenames></author><author><keyname>Eichinger</keyname><forenames>Frank</forenames></author><author><keyname>Nigrelli</keyname><forenames>Massimiliano</forenames></author><author><keyname>Ruiz-Andino</keyname><forenames>Javier Lucio</forenames></author></authors><title>Marketplaces for Energy Demand-Side Management based on Future-Internet
  Technology</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Renewable energies become more important, and they contribute to the EU's
goals for greenhouse-gas reduction. However, their fluctuating nature calls for
demand-side-management techniques, which balance energy generation and
consumption. Such techniques are currently not broadly deployed. This paper
describes the latest results from the FINSENY project on how Future-Internet
enablers and market mechanisms can be used to realise such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5350</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5350</id><created>2013-04-19</created><updated>2013-09-02</updated><authors><author><keyname>Contal</keyname><forenames>Emile</forenames></author><author><keyname>Buffoni</keyname><forenames>David</forenames></author><author><keyname>Robicquet</keyname><forenames>Alexandre</forenames></author><author><keyname>Vayatis</keyname><forenames>Nicolas</forenames></author></authors><title>Parallel Gaussian Process Optimization with Upper Confidence Bound and
  Pure Exploration</title><categories>cs.LG stat.ML</categories><journal-ref>Proceedings of ECML 2013, pp.225-240</journal-ref><doi>10.1007/978-3-642-40988-2_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the challenge of maximizing an unknown function f
for which evaluations are noisy and are acquired with high cost. An iterative
procedure uses the previous measures to actively select the next estimation of
f which is predicted to be the most useful. We focus on the case where the
function can be evaluated in parallel with batches of fixed size and analyze
the benefit compared to the purely sequential procedure in terms of cumulative
regret. We introduce the Gaussian Process Upper Confidence Bound and Pure
Exploration algorithm (GP-UCB-PE) which combines the UCB strategy and Pure
Exploration in the same batch of evaluations along the parallel iterations. We
prove theoretical upper bounds on the regret with batches of size K for this
procedure which show the improvement of the order of sqrt{K} for fixed
iteration cost over purely sequential versions. Moreover, the multiplicative
constants involved have the property of being dimension-free. We also confirm
empirically the efficiency of GP-UCB-PE on real and synthetic problems compared
to state-of-the-art competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5357</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5357</id><created>2013-04-19</created><authors><author><keyname>Ernvall</keyname><forenames>Toni</forenames></author></authors><title>Exact-Regenerating Codes between MBR and MSR Points</title><categories>cs.DC cs.IT math.IT</categories><comments>5 pages, 2 figures, submitted to ITW 2013</comments><msc-class>68P30</msc-class><journal-ref>2013 IEEE Information Theory Workshop, Sevilla, pages 1-5</journal-ref><doi>10.1109/ITW.2013.6691307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study distributed storage systems with exact repair. We give
a construction for regenerating codes between the minimum storage regenerating
(MSR) and the minimum bandwidth regenerating (MBR) points and show that in the
case that the parameters n, k, and d are close to each other our constructions
are close to optimal when comparing to the known capacity when only functional
repair is required. We do this by showing that when the distances of the
parameters n, k, and d are fixed but the actual values approach to infinity,
the fraction of the performance of our codes with exact repair and the known
capacity of codes with functional repair approaches to one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5373</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5373</id><created>2013-04-19</created><updated>2014-06-06</updated><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>Cording</keyname><forenames>Patrick Hagge</forenames></author><author><keyname>G&#xf8;rtz</keyname><forenames>Inge Li</forenames></author></authors><title>Compact q-gram Profiling of Compressed Strings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the q-gram profile of a string \str of
size $N$ compressed by a context-free grammar with $n$ production rules. We
present an algorithm that runs in $O(N-\alpha)$ expected time and uses
$O(n+q+\kq)$ space, where $N-\alpha\leq qn$ is the exact number of characters
decompressed by the algorithm and $\kq\leq N-\alpha$ is the number of distinct
q-grams in $\str$. This simultaneously matches the current best known time
bound and improves the best known space bound. Our space bound is
asymptotically optimal in the sense that any algorithm storing the grammar and
the q-gram profile must use $\Omega(n+q+\kq)$ space. To achieve this we
introduce the q-gram graph that space-efficiently captures the structure of a
string with respect to its q-grams, and show how to construct it from a
grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5378</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5378</id><created>2013-04-19</created><updated>2013-04-24</updated><authors><author><keyname>Kumar</keyname><forenames>R. Ram</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Kannan</forenames></author><author><keyname>Narasimha-Shenoi</keyname><forenames>Prasanth G.</forenames></author></authors><title>Fair Sets of Some Class of Graphs</title><categories>cs.DM math.CO</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a non empty set $S$ of vertices of a graph, the partiality of a vertex
with respect to $S$ is the difference between maximum and minimum of the
distances of the vertex to the vertices of $S$. The vertices with minimum
partiality constitute the fair center of the set. Any vertex set which is the
fair center of some set of vertices is called a fair set. In this paper we
prove that the induced subgraph of any fair set is connected in the case of
trees and characterise block graphs as the class of chordal graphs for which
the induced subgraph of all fair sets are connected. The fair sets of $K_{n}$,
$K_{m,n}$, $K_{n}-e$, wheel graphs, odd cycles and symmetric even graphs are
identified. The fair sets of the Cartesian product graphs are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5384</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5384</id><created>2013-04-19</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Quantum Popov robust stability analysis of an optical cavity containing
  a saturated Kerr medium</title><categories>quant-ph cs.SY math.OC</categories><comments>A shortened version will appear in the Proceedings of the 2013
  European Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies results on the robust stability of nonlinear quantum
systems to a system consisting an optical cavity containing a saturated Kerr
medium. The system is characterized by a Hamiltonian operator which contains a
non-quadratic term involving a quartic function of the annihilation and
creation operators. A saturated version of the Kerr nonlinearity leads to a
sector bounded nonlinearity which enables a quantum small gain theorem to be
applied to this system in order to analyze its stability. Also, a non-quadratic
version of a quantum Popov stability criterion is presented and applied to
analyze the stability of this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5385</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5385</id><created>2013-04-19</created><authors><author><keyname>Cooper</keyname><forenames>S. Barry</forenames></author></authors><title>The Mathematician's Bias - and the Return to Embodied Computation</title><categories>math.LO cs.GL</categories><journal-ref>In &quot;A Computable Universe - Understanding and Exploring Nature as
  Computation&quot; (Ed. Hector Zenil), World Scientific, 2013, pp. 125-142</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are growing uncertainties surrounding the classical model of
computation established by G\&quot;odel, Church, Kleene, Turing and others in the
1930s onwards. The mismatch between the Turing machine conception, and the
experiences of those more practically engaged in computing, has parallels with
the wider one between science and those working creatively or intuitively out
in the 'real' world. The scientific outlook is more flexible and basic than
some understand or want to admit. The science is subject to limitations which
threaten careers. We look at embodiment and disembodiment of computation as the
key to the mismatch, and find Turing had the right idea all along - amongst a
productive confusion of ideas about computation in the real and the abstract
worlds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5388</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5388</id><created>2013-04-19</created><updated>2014-02-27</updated><authors><author><keyname>Creignou</keyname><forenames>Nadia</forenames></author><author><keyname>Egly</keyname><forenames>Uwe</forenames></author><author><keyname>Schmidt</keyname><forenames>Johannes</forenames></author></authors><title>Complexity Classifications for logic-based Argumentation</title><categories>cs.CC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider logic-based argumentation in which an argument is a pair (Fi,al),
where the support Fi is a minimal consistent set of formulae taken from a given
knowledge base (usually denoted by De) that entails the claim al (a formula).
We study the complexity of three central problems in argumentation: the
existence of a support Fi ss De, the validity of a support and the relevance
problem (given psi is there a support Fi such that psi ss Fi?). When arguments
are given in the full language of propositional logic these problems are
computationally costly tasks, the validity problem is DP-complete, the others
are SigP2-complete. We study these problems in Schaefer's famous framework
where the considered propositional formulae are in generalized conjunctive
normal form. This means that formulae are conjunctions of constraints build
upon a fixed finite set of Boolean relations Ga (the constraint language). We
show that according to the properties of this language Ga, deciding whether
there exists a support for a claim in a given knowledge base is either
polynomial, NP-complete, coNP-complete or SigP2-complete. We present a
dichotomous classification, P or DP-complete, for the verification problem and
a trichotomous classification for the relevance problem into either polynomial,
NP-complete, or SigP2-complete. These last two classifications are obtained by
means of algebraic tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5389</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5389</id><created>2013-04-19</created><authors><author><keyname>Aziza</keyname><forenames>Sabri</forenames></author><author><keyname>Laila</keyname><forenames>Kjiri</forenames></author></authors><title>Patterns to analyze requirements of a Decisional Information System</title><categories>cs.OH</categories><comments>paper accepted alson in SEDEXS'2012: International Conference On
  Software Engineering, Databases and EXpert Systems, Settat, Maroc, 14-16 Juin
  2012, International Journal of Computer Application (IJCA), Special Issue On
  Software Engineering Databases and EXpert Systems (SEDEXS), Number 2, ISBN:
  973-93-80870-26-8, 17 Septembre 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The domain of analysis and conception of Decisional Information System (DIS)
is, highly, applying new techniques and methods to succeed the process of the
decision and minimizing the time of conception. Our objective in this paper is
to define a group of patterns to ensure a systematic reuse of our approach to
analyse a DIS s business requirements. We seek, through this work, to guide the
discovery of an organizations business requirements, expressed as goals by
introducing the notion of context, to promote good processes design for a DIS,
to capitalize the process and models proposed in our approach and systematize
reuse steps of this approach to analyze similar projects or adapt them as
needed. The patterns are at the same time the process s patterns and product s
patterns as they capitalize models and their associated processes. These
patterns are represented according to the PSIGMA formalism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5402</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5402</id><created>2013-04-19</created><authors><author><keyname>Verma</keyname><forenames>Trivik</forenames></author><author><keyname>Ellens</keyname><forenames>Wendy</forenames></author><author><keyname>Kooij</keyname><forenames>Robert E.</forenames></author></authors><title>Context-Independent Centrality Measures Underestimate the Vulnerability
  of Power Grids</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>Pre-Proceedings of CRITIS '12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power grids vulnerability is a key issue in society. A component failure may
trigger cascades of failures across the grid and lead to a large blackout.
Complex network approaches have shown a direction to study some of the problems
faced by power grids. Within Complex Network Analysis structural
vulnerabilities of power grids have been studied mostly using purely
topological approaches, which assumes that flow of power is dictated by
shortest paths. However, this fails to capture the real flow characteristics of
power grids. We have proposed a flow redistribution mechanism that closely
mimics the flow in power grids using the PTDF. With this mechanism we enhance
existing cascading failure models to study the vulnerability of power grids.
  We apply the model to the European high-voltage grid to carry out a
comparative study for a number of centrality measures. `Centrality' gives an
indication of the criticality of network components. Our model offers a way to
find those centrality measures that give the best indication of node
vulnerability in the context of power grids, by considering not only the
network topology but also the power flowing through the network. In addition,
we use the model to determine the spare capacity that is needed to make the
grid robust to targeted attacks. We also show a brief comparison of the end
results with other power grid systems to generalise the result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5404</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5404</id><created>2013-04-19</created><updated>2014-05-05</updated><authors><author><keyname>Gupta</keyname><forenames>Ankit</forenames></author><author><keyname>Briat</keyname><forenames>Corentin</forenames></author><author><keyname>Khammash</keyname><forenames>Mustafa</forenames></author></authors><title>A scalable computational framework for establishing long-term behavior
  of stochastic reaction networks</title><categories>q-bio.MN cs.SY math.OC math.PR</categories><comments>31 pages, 9 figures</comments><doi>10.1371/journal.pcbi.1003669</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reaction networks are systems in which the populations of a finite number of
species evolve through predefined interactions. Such networks are found as
modeling tools in many biological disciplines such as biochemistry, ecology,
epidemiology, immunology, systems biology and synthetic biology. It is now
well-established that, for small population sizes, stochastic models for
biochemical reaction networks are necessary to capture randomness in the
interactions. The tools for analyzing such models, however, still lag far
behind their deterministic counterparts. In this paper, we bridge this gap by
developing a constructive framework for examining the long-term behavior and
stability properties of the reaction dynamics in a stochastic setting. In
particular, we address the problems of determining ergodicity of the reaction
dynamics, which is analogous to having a globally attracting fixed point for
deterministic dynamics. We also examine when the statistical moments of the
underlying process remain bounded with time and when they converge to their
steady state values. The framework we develop relies on a blend of ideas from
probability theory, linear algebra and optimization theory. We demonstrate that
the stability properties of a wide class of biological networks can be assessed
from our sufficient theoretical conditions that can be recast as efficient and
scalable linear programs, well-known for their tractability. It is notably
shown that the computational complexity is often linear in the number of
species. We illustrate the validity, the efficiency and the wide applicability
of our results on several reaction networks arising in biochemistry, systems
biology, epidemiology and ecology. The biological implications of the results
as well as an example of a non-ergodic biological network are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5409</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5409</id><created>2013-04-19</created><updated>2014-10-15</updated><authors><author><keyname>Gottschlich</keyname><forenames>Carsten</forenames></author><author><keyname>Huckemann</keyname><forenames>Stephan</forenames></author></authors><title>Separating the Real from the Synthetic: Minutiae Histograms as
  Fingerprints of Fingerprints</title><categories>cs.CV cs.AI cs.DB</categories><journal-ref>IET Biometrics, vol. 3, no. 4, pp. 291-301, Dec. 2014</journal-ref><doi>10.1049/iet-bmt.2013.0065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we show that by the current state-of-the-art synthetically
generated fingerprints can easily be discriminated from real fingerprints. We
propose a method based on second order extended minutiae histograms (MHs) which
can distinguish between real and synthetic prints with very high accuracy. MHs
provide a fixed-length feature vector for a fingerprint which are invariant
under rotation and translation. This 'test of realness' can be applied to
synthetic fingerprints produced by any method. In this work, tests are
conducted on the 12 publicly available databases of FVC2000, FVC2002 and
FVC2004 which are well established benchmarks for evaluating the performance of
fingerprint recognition algorithms; 3 of these 12 databases consist of
artificial fingerprints generated by the SFinGe software. Additionally, we
evaluate the discriminative performance on a database of synthetic fingerprints
generated by the software of Bicz versus real fingerprint images. We conclude
with suggestions for the improvement of synthetic fingerprint generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5416</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5416</id><created>2013-04-19</created><authors><author><keyname>Rajatheva</keyname><forenames>Nandana</forenames></author></authors><title>The Worst Case ISI channels and the Uniqueness of the Corresponding
  Minimum Eigenvalue</title><categories>cs.IT math.IT</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intersymbol interference (ISI) is a major cause of degradation in the
receiver performance of high-speed data communications systems. This arises
mainly due to limited bandwidth available. The minimum Euclidean distance
between any two symbol sequences is an important parameter in this case at
moderate to high signal to noise ratios. It is proven here that as ISI
increases the minimum distance strictly decreases when the worst case scenario
is considered. From this it follows that the minimum eigenvalue of the worst
case ISI channel of a given length is unique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5418</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5418</id><created>2013-04-19</created><updated>2013-07-04</updated><authors><author><keyname>Ballier</keyname><forenames>Alexis</forenames></author></authors><title>Universality in symbolic dynamics constrained by Medvedev degrees</title><categories>math.DS cs.OH</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a weak notion of universality in symbolic dynamics and, by
generalizing a proof of Mike Hochman, we prove that this yields necessary
conditions on the forbidden patterns defining a universal subshift: These
forbidden patterns are necessarily in a Medvedev degree greater or equal than
the degree of the set of subshifts for which it is universal.
  We also show that this necessary condition is optimal by giving constructions
of universal subshifts in the same Medvedev degree as the subshifts they
simulate and prove that this universality can be achieved by the sofic
projective subdynamics of such a subshift as soon as the necessary conditions
are verified.
  This could be summarized as: There are obstructions for the existence of
universal subshifts due to the theory of computability and they are the only
ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5429</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5429</id><created>2013-04-19</created><updated>2014-04-07</updated><authors><author><keyname>Etessami</keyname><forenames>Kousha</forenames></author><author><keyname>Stewart</keyname><forenames>Alistair</forenames></author><author><keyname>Yannakakis</keyname><forenames>Mihalis</forenames></author></authors><title>A note on the complexity of comparing succinctly represented integers,
  with an application to maximum probability parsing</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The following two decision problems capture the complexity of comparing
integers or rationals that are succinctly represented in
product-of-exponentials notation, or equivalently, via arithmetic circuits
using only multiplication and division gates, and integer inputs:
  Input instance: four lists of positive integers: a_1, ...., a_n ; b_1,....,
b_n ; c_1,....,c_m ; d_1, ...., d_m ; where each of the integers is represented
in binary.
  Problem 1 (equality testing): Decide whether a_1^{b_1} a_2^{b_2} ....
a_n^{b_n} = c_1^{d_1} c_2^{d_2} .... c_m^{d_m} .
  Problem 2 (inequality testing): Decide whether a_1^{b_1} a_2^{b_2} ...
a_n^{b_n} &gt;= c_1^{d_1} c_2^{d_2} .... c_m^{d_m} .
  Problem 1 is easily decidable in polynomial time using a simple iterative
algorithm. Problem 2 is much harder. We observe that the complexity of Problem
2 is intimately connected to deep conjectures and results in number theory. In
particular, if a refined form of the ABC conjecture formulated by Baker in 1998
holds, or if the older Lang-Waldschmidt conjecture (formulated in 1978) on
linear forms in logarithms holds, then Problem 2 is decidable in P-time (in the
standard Turing model of computation). Moreover, it follows from the best
available quantitative bounds on linear forms in logarithms, e.g., by Baker and
W\&quot;{u}stholz (1993) or Matveev (2000), that if m and n are fixed universal
constants then Problem 2 is decidable in P-time (without relying on any
conjectures). This latter fact was observed earlier by Shub (1993).
  We describe one application: P-time maximum probability parsing for arbitrary
stochastic context-free grammars (where \epsilon-rules are allowed).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5438</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5438</id><created>2013-04-19</created><updated>2013-07-17</updated><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames><affiliation>University of Mons</affiliation></author><author><keyname>Menet</keyname><forenames>Quentin</forenames><affiliation>University of Mons</affiliation></author></authors><title>Simple strategies for Banach-Mazur games and fairly correct systems</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2013, arXiv:1307.4162</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 119, 2013, pp. 21-34</journal-ref><doi>10.4204/EPTCS.119.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2006, Varacca and V\&quot;olzer proved that on finite graphs, omega-regular
large sets coincide with omega-regular sets of probability 1, by using the
existence of positional strategies in the related Banach-Mazur games. Motivated
by this result, we try to understand relations between sets of probability 1
and various notions of simple strategies (including those introduced in a
recent paper of Gr\&quot;adel and Lessenich). Then, we introduce a generalisation of
the classical Banach-Mazur game and in particular, a probabilistic version
whose goal is to characterise sets of probability 1 (as classical Banach-Mazur
games characterise large sets). We obtain a determinacy result for these games,
when the winning set is a countable intersection of open sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5449</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5449</id><created>2013-04-19</created><authors><author><keyname>Lecoutre</keyname><forenames>Christophe</forenames></author><author><keyname>Paris</keyname><forenames>Nicolas</forenames></author><author><keyname>Roussel</keyname><forenames>Olivier</forenames></author><author><keyname>Tabary</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Solving WCSP by Extraction of Minimal Unsatisfiable Cores</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usual techniques to solve WCSP are based on cost transfer operations coupled
with a branch and bound algorithm. In this paper, we focus on an approach
integrating extraction and relaxation of Minimal Unsatisfiable Cores in order
to solve this problem. We decline our approach in two ways: an incomplete,
greedy, algorithm and a complete one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5457</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5457</id><created>2013-04-19</created><authors><author><keyname>Lee</keyname><forenames>Joonseok</forenames></author><author><keyname>Lee</keyname><forenames>Kisung</forenames></author><author><keyname>Kim</keyname><forenames>Jennifer G.</forenames></author></authors><title>Personalized Academic Research Paper Recommendation System</title><categories>cs.IR cs.DL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A huge number of academic papers are coming out from a lot of conferences and
journals these days. In these circumstances, most researchers rely on key-based
search or browsing through proceedings of top conferences and journals to find
their related work. To ease this difficulty, we propose a Personalized Academic
Research Paper Recommendation System, which recommends related articles, for
each researcher, that may be interesting to her/him. In this paper, we first
introduce our web crawler to retrieve research papers from the web. Then, we
define similarity between two research papers based on the text similarity
between them. Finally, we propose our recommender system developed using
collaborative filtering methods. Our evaluation results demonstrate that our
system recommends good quality research papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5475</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5475</id><created>2013-04-19</created><authors><author><keyname>Schubotz</keyname><forenames>Moritz</forenames></author></authors><title>Making Math Searchable in Wikipedia</title><categories>cs.DL</categories><comments>7 pages, 2 figures, Conference on Intelligent Computer Mathematics,
  July 9-14 2012, Bremen, Germany. To be published in Lecture Notes, Artificial
  Intelligence, Springer</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia, the world largest encyclopedia contains a lot of knowledge that is
expressed as formulae exclusively. Unfortunately, this knowledge is currently
not fully accessible by intelligent information retrieval systems. This immense
body of knowledge is hidden form value-added services, such as search. In this
paper, we present our MathSearch implementation for Wikipedia that enables
users to perform a combined text and fully unlock the potential benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5476</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5476</id><created>2013-04-19</created><authors><author><keyname>Park</keyname><forenames>Jeongmi</forenames></author><author><keyname>Sano</keyname><forenames>Yoshio</forenames></author></authors><title>The niche graphs of interval orders</title><categories>math.CO cs.DM</categories><comments>7 pages</comments><msc-class>05C75, 05C20, 06A06</msc-class><journal-ref>Discussiones Mathematicae Graph Theory 34 (2014) 353-359</journal-ref><doi>10.7151/dmgt.1741</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The niche graph of a digraph $D$ is the (simple undirected) graph which has
the same vertex set as $D$ and has an edge between two distinct vertices $x$
and $y$ if and only if $N^+_D(x) \cap N^+_D(y) \neq \emptyset$ or $N^-_D(x)
\cap N^-_D(y) \neq \emptyset$, where $N^+_D(x)$ (resp. $N^-_D(x)$) is the set
of out-neighbors (resp. in-neighbors) of $x$ in $D$. A digraph $D=(V,A)$ is
called a semiorder (or a unit interval order) if there exist a real-valued
function $f:V \to \mathbb{R}$ on the set $V$ and a positive real number $\delta
\in \mathbb{R}$ such that $(x,y) \in A$ if and only if $f(x) &gt; f(y) + \delta$.
A digraph $D=(V,A)$ is called an interval order if there exists an assignment
$J$ of a closed real interval $J(x) \subset \mathbb{R}$ to each vertex $x \in
V$ such that $(x,y) \in A$ if and only if $\min J(x) &gt; \max J(y)$.
  S. -R. Kim and F. S. Roberts characterized the competition graphs of
semiorders and interval orders in 2002, and Y. Sano characterized the
competition-common enemy graphs of semiorders and interval orders in 2010. In
this note, we give characterizations of the niche graphs of semiorders and
interval orders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5479</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5479</id><created>2013-04-19</created><updated>2014-07-18</updated><authors><author><keyname>de Haan</keyname><forenames>Ronald</forenames></author><author><keyname>Kanj</keyname><forenames>Iyad</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Local Backbones</title><categories>cs.CC cs.AI</categories><comments>A previous version appeared in the proceedings of the 16th
  International Conference on Theory and Applications of Satisfiability Testing
  (SAT 2013)</comments><journal-ref>Proceedings of SAT 2013, LNCS 7962, pp. 377-393, 2013</journal-ref><doi>10.1007/978-3-642-39071-5_28</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A backbone of a propositional CNF formula is a variable whose truth value is
the same in every truth assignment that satisfies the formula. The notion of
backbones for CNF formulas has been studied in various contexts. In this paper,
we introduce local variants of backbones, and study the computational
complexity of detecting them. In particular, we consider k-backbones, which are
backbones for sub-formulas consisting of at most k clauses, and iterative
k-backbones, which are backbones that result after repeated instantiations of
k-backbones. We determine the parameterized complexity of deciding whether a
variable is a k-backbone or an iterative k-backbone for various restricted
formula classes, including Horn, definite Horn, and Krom. We also present some
first empirical results regarding backbones for CNF-Satisfiability (SAT). The
empirical results we obtain show that a large fraction of the backbones of
structured SAT instances are local, in contrast to random instances, which
appear to have few local backbones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5485</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5485</id><created>2013-04-19</created><authors><author><keyname>Green</keyname><forenames>Alexander S.</forenames></author><author><keyname>Lumsdaine</keyname><forenames>Peter LeFanu</forenames></author><author><keyname>Ross</keyname><forenames>Neil J.</forenames></author><author><keyname>Selinger</keyname><forenames>Peter</forenames></author><author><keyname>Valiron</keyname><forenames>Beno&#xee;t</forenames></author></authors><title>An Introduction to Quantum Programming in Quipper</title><categories>cs.PL cs.ET quant-ph</categories><comments>15 pages, RC2013</comments><journal-ref>Lecture Notes in Computer Science 7948:110-124, Springer, 2013</journal-ref><doi>10.1007/978-3-642-38986-3_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quipper is a recently developed programming language for expressing quantum
computations. This paper gives a brief tutorial introduction to the language,
through a demonstration of how to make use of some of its key features. We
illustrate many of Quipper's language features by developing a few well known
examples of Quantum computation, including quantum teleportation, the quantum
Fourier transform, and a quantum circuit for addition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5498</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5498</id><created>2013-04-19</created><updated>2013-09-27</updated><authors><author><keyname>Heule</keyname><forenames>Marijn J. H.</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>A SAT Approach to Clique-Width</title><categories>cs.DS cs.DM</categories><comments>proofs in section 3 updated, results remain unchanged</comments><journal-ref>Proceedings of SAT 2013, LNCS 7962, pp. 318-334, 2013</journal-ref><doi>10.1007/978-3-642-39071-5_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clique-width is a graph invariant that has been widely studied in
combinatorics and computer science. However, computing the clique-width of a
graph is an intricate problem, the exact clique-width is not known even for
very small graphs. We present a new method for computing the clique-width of
graphs based on an encoding to propositional satisfiability (SAT) which is then
evaluated by a SAT solver. Our encoding is based on a reformulation of
clique-width in terms of partitions that utilizes an efficient encoding of
cardinality constraints. Our SAT-based method is the first to discover the
exact clique-width of various small graphs, including famous graphs from the
literature as well as random graphs of various density. With our method we
determined the smallest graphs that require a small pre-described clique-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5504</identifier>
 <datestamp>2013-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5504</id><created>2013-04-19</created><updated>2013-05-10</updated><authors><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>Efficient Stochastic Gradient Descent for Strongly Convex Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We motivate this study from a recent work on a stochastic gradient descent
(SGD) method with only one projection \citep{DBLP:conf/nips/MahdaviYJZY12},
which aims at alleviating the computational bottleneck of the standard SGD
method in performing the projection at each iteration, and enjoys an $O(\log
T/T)$ convergence rate for strongly convex optimization. In this paper, we make
further contributions along the line. First, we develop an epoch-projection SGD
method that only makes a constant number of projections less than $\log_2T$ but
achieves an optimal convergence rate $O(1/T)$ for {\it strongly convex
optimization}. Second, we present a proximal extension to utilize the structure
of the objective function that could further speed-up the computation and
convergence for sparse regularized loss minimization problems. Finally, we
consider an application of the proposed techniques to solving the high
dimensional large margin nearest neighbor classification problem, yielding a
speed-up of orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5507</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5507</id><created>2013-04-19</created><authors><author><keyname>Lampos</keyname><forenames>Vasileios</forenames></author><author><keyname>Lansdall-Welfare</keyname><forenames>Thomas</forenames></author><author><keyname>Araya</keyname><forenames>Ricardo</forenames></author><author><keyname>Cristianini</keyname><forenames>Nello</forenames></author></authors><title>Analysing Mood Patterns in the United Kingdom through Twitter Content</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social Media offer a vast amount of geo-located and time-stamped textual
content directly generated by people. This information can be analysed to
obtain insights about the general state of a large population of users and to
address scientific questions from a diversity of disciplines. In this work, we
estimate temporal patterns of mood variation through the use of emotionally
loaded words contained in Twitter messages, possibly reflecting underlying
circadian and seasonal rhythms in the mood of the users. We present a method
for computing mood scores from text using affective word taxonomies, and apply
it to millions of tweets collected in the United Kingdom during the seasons of
summer and winter. Our analysis results in the detection of strong and
statistically significant circadian patterns for all the investigated mood
types. Seasonal variation does not seem to register any important divergence in
the signals, but a periodic oscillation within a 24-hour period is identified
for each mood type. The main common characteristic for all emotions is their
mid-morning peak, however their mood score patterns differ in the evenings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5509</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5509</id><created>2013-04-19</created><authors><author><keyname>Akbar</keyname><forenames>M.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>A. A.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author></authors><title>On Modeling Geometric Joint Sink Mobility with Delay-Tolerant
  Cluster-less Wireless Sensor Networks</title><categories>cs.NI</categories><journal-ref>4th IEEE Technically Co-Sponsored International Conference on
  Smart Communications in Network Technologies (SaCoNet'13) 2013, Paris, France</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moving Sink (MS) in Wireless Sensor Networks (WSNs) has appeared as a
blessing because it collects data directly from the nodes where the concept of
relay nodes is becomes obsolete. There are, however, a few challenges to be
taken care of, like data delay tolerance and trajectory of MS which is NP-hard.
In our proposed scheme, we divide the square field in small squares. Middle
point of the partitioned area is the sojourn location of the sink, and nodes
around MS are in its transmission range, which send directly the sensed data in
a delay-tolerant fashion. Two sinks are moving simultaneously; one inside and
having four sojourn locations and other in outer trajectory having twelve
sojourn locations. Introduction of the joint mobility enhances network life and
ultimately throughput. As the MS comes under the NP-hard problem, we convert it
into a geometric problem and define it as, Geometric Sink Movement (GSM). A set
of linear programming equations has also been given in support of GSM which
prolongs network life time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5518</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5518</id><created>2013-04-19</created><updated>2013-05-03</updated><authors><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Raman</keyname><forenames>Venkatesh</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Upper and Lower Bounds for Weak Backdoor Set Detection</title><categories>cs.DS</categories><comments>A short version will appear in the proceedings of the 16th
  International Conference on Theory and Applications of Satisfiability Testing</comments><journal-ref>Proceedings of SAT 2013, LNCS 7962, pp. 394-402, 2013</journal-ref><doi>10.1007/978-3-642-39071-5_29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain upper and lower bounds for running times of exponential time
algorithms for the detection of weak backdoor sets of 3CNF formulas,
considering various base classes. These results include (omitting polynomial
factors), (i) a 4.54^k algorithm to detect whether there is a weak backdoor set
of at most k variables into the class of Horn formulas; (ii) a 2.27^k algorithm
to detect whether there is a weak backdoor set of at most k variables into the
class of Krom formulas. These bounds improve an earlier known bound of 6^k. We
also prove a 2^k lower bound for these problems, subject to the Strong
Exponential Time Hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5530</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5530</id><created>2013-04-19</created><updated>2014-12-10</updated><authors><author><keyname>Tappenden</keyname><forenames>Rachael</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Gondzio</keyname><forenames>Jacek</forenames></author></authors><title>Inexact Coordinate Descent: Complexity and Preconditioning</title><categories>math.OC cs.AI stat.ML</categories><comments>32 pages, 6 tables, 2 figures, 1 algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of minimizing a convex function using a
randomized block coordinate descent method. One of the key steps at each
iteration of the algorithm is determining the update to a block of variables.
Existing algorithms assume that in order to compute the update, a particular
subproblem is solved exactly. In his work we relax this requirement, and allow
for the subproblem to be solved inexactly, leading to an inexact block
coordinate descent method. Our approach incorporates the best known results for
exact updates as a special case. Moreover, these theoretical guarantees are
complemented by practical considerations: the use of iterative techniques to
determine the update as well as the use of preconditioning for further
acceleration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5531</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5531</id><created>2013-04-19</created><authors><author><keyname>Westbrook</keyname><forenames>Edwin</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Swarat</forenames></author></authors><title>A Semantics for Approximate Program Transformations</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approximate program transformation is a transformation that can change the
semantics of a program within a specified empirical error bound. Such
transformations have wide applications: they can decrease computation time,
power consumption, and memory usage, and can, in some cases, allow
implementations of incomputable operations. Correctness proofs of approximate
program transformations are by definition quantitative. Unfortunately, unlike
with standard program transformations, there is as of yet no modular way to
prove correctness of an approximate transformation itself. Error bounds must be
proved for each transformed program individually, and must be re-proved each
time a program is modified or a different set of approximations are applied. In
this paper, we give a semantics that enables quantitative reasoning about a
large class of approximate program transformations in a local, composable way.
Our semantics is based on a notion of distance between programs that defines
what it means for an approximate transformation to be correct up to an error
bound. The key insight is that distances between programs cannot in general be
formulated in terms of metric spaces and real numbers. Instead, our semantics
admits natural notions of distance for each type construct; for example,
numbers are used as distances for numerical data, functions are used as
distances for functional data, an polymorphic lambda-terms are used as
distances for polymorphic data. We then show how our semantics applies to two
example approximations: replacing reals with floating-point numbers, and loop
perforation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5545</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5545</id><created>2013-04-19</created><authors><author><keyname>Groza</keyname><forenames>Adrian</forenames></author></authors><title>Designing Electronic Markets for Defeasible-based Contractual Agents</title><categories>cs.MA</categories><comments>LAF Workshop 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of punishment policies applied to specific domains linking agents
actions to material penalties is an open research issue. The proposed framework
applies principles of contract law to set penalties: expectation damages,
opportunity cost, reliance damages, and party design remedies. In order to
decide which remedy provides maximum welfare within an electronic market, a
simulation environment called DEMCA (Designing Electronic Markets for
Contractual Agents) was developed. Knowledge representation and the reasoning
capabilities of the agents are based on an extended version of temporal
defeasible logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5546</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5546</id><created>2013-04-19</created><authors><author><keyname>Kl&#xf6;ckner</keyname><forenames>Andreas</forenames></author><author><keyname>Warburton</keyname><forenames>Timothy</forenames></author><author><keyname>Hesthaven</keyname><forenames>Jan S.</forenames></author></authors><title>Solving Wave Equations on Unstructured Geometries</title><categories>cs.MS cs.NA</categories><comments>GPU Computing Gems, edited by Wen-mei Hwu, Elsevier (2011), ISBN
  9780123859631, Chapter 18</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Waves are all around us--be it in the form of sound, electromagnetic
radiation, water waves, or earthquakes. Their study is an important basic tool
across engineering and science disciplines. Every wave solver serving the
computational study of waves meets a trade-off of two figures of merit--its
computational speed and its accuracy. Discontinuous Galerkin (DG) methods fall
on the high-accuracy end of this spectrum. Fortuitously, their computational
structure is so ideally suited to GPUs that they also achieve very high
computational speeds. In other words, the use of DG methods on GPUs
significantly lowers the cost of obtaining accurate solutions. This article
aims to give the reader an easy on-ramp to the use of this technology, based on
a sample implementation which demonstrates a highly accurate, GPU-capable,
real-time visualizing finite element solver in about 1500 lines of code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5550</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5550</id><created>2013-04-19</created><authors><author><keyname>Groza</keyname><forenames>Adrian</forenames></author><author><keyname>Barbur</keyname><forenames>Gabriel</forenames></author><author><keyname>Blaga</keyname><forenames>Bogdan</forenames></author></authors><title>OntoRich - A Support Tool for Semi-Automatic Ontology Enrichment and
  Evaluation</title><categories>cs.AI</categories><comments>ACAM 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the OntoRich framework, a support tool for semi-automatic
ontology enrichment and evaluation. The WordNet is used to extract candidates
for dynamic ontology enrichment from RSS streams. With the integration of
OpenNLP the system gains access to syntactic analysis of the RSS news. The
enriched ontologies are evaluated against several qualitative metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5553</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5553</id><created>2013-04-19</created><authors><author><keyname>Kl&#xf6;ckner</keyname><forenames>Andreas</forenames></author><author><keyname>Pinto</keyname><forenames>Nicolas</forenames></author><author><keyname>Catanzaro</keyname><forenames>Bryan</forenames></author><author><keyname>Lee</keyname><forenames>Yunsup</forenames></author><author><keyname>Ivanov</keyname><forenames>Paul</forenames></author><author><keyname>Fasih</keyname><forenames>Ahmed</forenames></author></authors><title>GPU Scripting and Code Generation with PyCUDA</title><categories>cs.SE</categories><journal-ref>GPU Computing Gems, edited by Wen-mei Hwu, Elsevier (2011), ISBN
  9780123859631, Chapter 27</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-level scripting languages are in many ways polar opposites to GPUs. GPUs
are highly parallel, subject to hardware subtleties, and designed for maximum
throughput, and they offer a tremendous advance in the performance achievable
for a significant number of computational problems. On the other hand,
scripting languages such as Python favor ease of use over computational speed
and do not generally emphasize parallelism. PyCUDA is a package that attempts
to join the two together. This chapter argues that in doing so, a programming
environment is created that is greater than just the sum of its two parts.
  We would like to note that nearly all of this chapter applies in unmodified
form to PyOpenCL, a sister project of PyCUDA, whose goal it is to realize the
same concepts as PyCUDA for OpenCL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5554</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5554</id><created>2013-04-19</created><authors><author><keyname>Groza</keyname><forenames>Adrian</forenames></author><author><keyname>Indrie</keyname><forenames>Sergiu</forenames></author></authors><title>Enacting Social Argumentative Machines in Semantic Wikipedia</title><categories>cs.AI</categories><comments>UBICC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research advocates the idea of combining argumentation theory with the
social web technology, aiming to enact large scale or mass argumentation. The
proposed framework allows mass-collaborative editing of structured arguments in
the style of semantic wikipedia. The long term goal is to apply the abstract
machinery of argumentation theory to more practical applications based on human
generated arguments, such as deliberative democracy, business negotiation, or
self-care. The ARGNET system was developed based on ther Semantic MediaWiki
framework and on the Argument Interchange Format (AIF) ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5560</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5560</id><created>2013-04-19</created><authors><author><keyname>Cicalese</keyname><forenames>Ferdinando</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Giaquinta</keyname><forenames>Emanuele</forenames></author><author><keyname>Laber</keyname><forenames>Eduardo Sany</forenames></author><author><keyname>Lipt&#xe1;k</keyname><forenames>Zsuzsanna</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author><author><keyname>Tomescu</keyname><forenames>Alexandru I.</forenames></author></authors><title>Indexes for Jumbled Pattern Matching in Strings, Trees and Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider how to index strings, trees and graphs for jumbled pattern
matching when we are asked to return a match if one exists. For example, we
show how, given a tree containing two colours, we can build a quadratic-space
index with which we can find a match in time proportional to the size of the
match. We also show how we need only linear space if we are content with
approximate matches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5563</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5563</id><created>2013-04-19</created><authors><author><keyname>Wang</keyname><forenames>Qixin</forenames></author><author><keyname>Li</keyname><forenames>Menghui</forenames></author><author><keyname>Zu</keyname><forenames>Hualong</forenames></author><author><keyname>Gao</keyname><forenames>Mingyi</forenames></author><author><keyname>Cao</keyname><forenames>Chenghua</forenames></author><author><keyname>Xia</keyname><forenames>Li Charlie</forenames></author></authors><title>A quantitative evaluation of health care system in US, China, and Sweden</title><categories>stat.AP cs.CY</categories><comments>6 figures, 2 tables</comments><journal-ref>HealthMED 4 (2013) 1064-1074</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study is mainly aimed at evaluating the effectiveness of current health
care systems of several representative countries and improving that of the US.
To achieve these goals, a people-oriented non-linear evaluation model is
designed. It comprises one major evaluation metric and four minor metrics. The
major metric is constituted by combining possible factors that most
significantly determine or affect the life expectancy of people in this
country. The four minor metrics evaluate less important aspects of health care
systems and are subordinate to the major one. The authors rank some of the
health care systems in the world according to the major metric and detect
problems in them with the help of minor ones. It is concluded that the health
care system of Sweden scores higher than the US and Chinese system scores lower
than that of the US. Especially, the health care system of US lags behind a
little bit compared with its economic power. At last, it is reasonable for the
American government to optimize the arrangement of funding base on the result
of goal programming model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5565</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5565</id><created>2013-04-19</created><authors><author><keyname>Srihari</keyname><forenames>Sriganesh</forenames></author><author><keyname>Ragan</keyname><forenames>Mark A.</forenames></author></authors><title>Computing Pathways to Systems Biology: Key Contributions of
  Computational Methods in Pathway Identification</title><categories>q-bio.MN cs.CE</categories><comments>18 pages, 1 figure, survey article</comments><msc-class>68</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Understanding large molecular networks consisting of entities such as genes,
proteins or RNAs that interact in complex ways to drive the cellular machinery
has been an active focus of systems biology. Computational approaches have
played a key role in systems biology by complementing theoretical and
experimental approaches. Here we roadmap some key contributions of
computational methods developed over the last decade in the reconstruction of
biological pathways. We position these contributions in a 'systems biology
perspective' to reemphasize their roles in unraveling cellular mechanisms and
to understand 'systems biology diseases' including cancer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5566</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5566</id><created>2013-04-19</created><authors><author><keyname>Cotterell</keyname><forenames>Michael E.</forenames></author><author><keyname>Medina</keyname><forenames>Terrance</forenames></author></authors><title>A Markov Model for Ontology Alignment</title><categories>cs.DB cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosion of available data along with the need to integrate and utilize
that data has led to a pressing interest in data integration techniques. In
terms of Semantic Web technologies, Ontology Alignment is a key step in the
process of integrating heterogeneous knowledge bases. In this paper, we present
the Edge Confidence technique, a modification and improvement over the popular
Similarity Flooding technique for Ontology Alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5568</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5568</id><created>2013-04-19</created><authors><author><keyname>Fuller</keyname><forenames>Andrew</forenames></author><author><keyname>Budimcic</keyname><forenames>Vedran</forenames></author></authors><title>DORI: Distributed Outdoor Robotic Instruments</title><categories>cs.RO</categories><comments>36 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DORI (Distributed Outdoor Robotic Instruments) is a remotely controlled
vehicle that is designed to simulate a planetary exploration mission. DORI is
equipped with over 20 environmental sensors and can perform basic data
analysis, logging and remote upload. The individual components are distributed
across a fault-tolerant bus for redundancy. A partial sensor list includes
atmospheric pressure, rainfall, wind speed, GPS, gyroscopic inertia, linear
acceleration, magnetic field strength, temperature, laser and ultrasonic
distance sensing, as well as digital audio and video capture. The project uses
recycled consumer electronics devices as a low-cost source for sensor
components. This report describes the hardware design of DORI including sensor
electronics, embedded firmware, and physical construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5574</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5574</id><created>2013-04-19</created><authors><author><keyname>Li</keyname><forenames>Liangbin</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>Maximum-rate Transmission with Improved Diversity Gain for Interference
  Networks</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) was shown effective for interference management
to improve transmission rate in terms of the degree of freedom (DoF) gain. On
the other hand, orthogonal space-time block codes (STBCs) were widely used in
point-to-point multi-antenna channels to enhance transmission reliability in
terms of the diversity gain. In this paper, we connect these two ideas, i.e.,
IA and space-time block coding, to improve the designs of alignment precoders
for multi-user networks. Specifically, we consider the use of Alamouti codes
for IA because of its rate-one transmission and achievability of full diversity
in point-to-point systems. The Alamouti codes protect the desired link by
introducing orthogonality between the two symbols in one Alamouti codeword, and
create alignment at the interfering receiver. We show that the proposed
alignment methods can maintain the maximum DoF gain and improve the ergodic
mutual information in the long-term regime, while increasing the diversity gain
to 2 in the short-term regime. The presented examples of interference networks
have two antennas at each node and include the two-user X channel, the
interferring multi-access channel (IMAC), and the interferring broadcast
channel (IBC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5575</identifier>
 <datestamp>2013-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5575</id><created>2013-04-19</created><updated>2013-04-25</updated><authors><author><keyname>Que</keyname><forenames>Qichao</forenames></author><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author></authors><title>Inverse Density as an Inverse Problem: The Fredholm Equation Approach</title><categories>cs.LG stat.ML</categories><comments>Fixing a few typos in last version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of estimating the ratio $\frac{q}{p}$
where $p$ is a density function and $q$ is another density, or, more generally
an arbitrary function. Knowing or approximating this ratio is needed in various
problems of inference and integration, in particular, when one needs to average
a function with respect to one probability distribution, given a sample from
another. It is often referred as {\it importance sampling} in statistical
inference and is also closely related to the problem of {\it covariate shift}
in transfer learning as well as to various MCMC methods. It may also be useful
for separating the underlying geometry of a space, say a manifold, from the
density function defined on it.
  Our approach is based on reformulating the problem of estimating
$\frac{q}{p}$ as an inverse problem in terms of an integral operator
corresponding to a kernel, and thus reducing it to an integral equation, known
as the Fredholm problem of the first kind. This formulation, combined with the
techniques of regularization and kernel methods, leads to a principled
kernel-based framework for constructing algorithms and for analyzing them
theoretically.
  The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized
Estimator) is flexible, simple and easy to implement.
  We provide detailed theoretical analysis including concentration bounds and
convergence rates for the Gaussian kernel in the case of densities defined on
$\R^d$, compact domains in $\R^d$ and smooth $d$-dimensional sub-manifolds of
the Euclidean space.
  We also show experimental results including applications to classification
and semi-supervised learning within the covariate shift framework and
demonstrate some encouraging experimental comparisons. We also show how the
parameters of our algorithms can be chosen in a completely unsupervised manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5583</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5583</id><created>2013-04-19</created><updated>2013-10-15</updated><authors><author><keyname>Talwalkar</keyname><forenames>Ameet</forenames></author><author><keyname>Mackey</keyname><forenames>Lester</forenames></author><author><keyname>Mu</keyname><forenames>Yadong</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Distributed Low-rank Subspace Segmentation</title><categories>cs.CV cs.DC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision problems ranging from image clustering to motion segmentation to
semi-supervised learning can naturally be framed as subspace segmentation
problems, in which one aims to recover multiple low-dimensional subspaces from
noisy and corrupted input data. Low-Rank Representation (LRR), a convex
formulation of the subspace segmentation problem, is provably and empirically
accurate on small problems but does not scale to the massive sizes of modern
vision datasets. Moreover, past work aimed at scaling up low-rank matrix
factorization is not applicable to LRR given its non-decomposable constraints.
In this work, we propose a novel divide-and-conquer algorithm for large-scale
subspace segmentation that can cope with LRR's non-decomposable constraints and
maintains LRR's strong recovery guarantees. This has immediate implications for
the scalability of subspace segmentation, which we demonstrate on a benchmark
face recognition dataset and in simulations. We then introduce novel
applications of LRR-based subspace segmentation to large-scale semi-supervised
learning for multimedia event detection, concept detection, and image tagging.
In each case, we obtain state-of-the-art results and order-of-magnitude speed
ups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5587</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5587</id><created>2013-04-20</created><updated>2013-05-15</updated><authors><author><keyname>Prasath</keyname><forenames>V. B. Surya</forenames></author><author><keyname>Moreno</keyname><forenames>Juan C.</forenames></author><author><keyname>Palaniappan</keyname><forenames>K.</forenames></author></authors><title>Color image denoising by chromatic edges based vector valued diffusion</title><categories>cs.CV</categories><comments>Submitted to IEEE Signal Processing Letters, 4 pages, 4 figures, 2
  tables. Some mistakes were corrected from previous version</comments><msc-class>68U10</msc-class><acm-class>I.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we propose to denoise digital color images via an improved
geometric diffusion scheme. By introducing edges detected from all three color
channels into the diffusion the proposed scheme avoids color smearing
artifacts. Vector valued diffusion is used to control the smoothing and the
geometry of color images are taken into consideration. Color edge strength
function computed from different planes is introduced and it stops the
diffusion spread across chromatic edges. Experimental results indicate that the
scheme achieves good denoising with edge preservation when compared to other
related schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5590</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5590</id><created>2013-04-20</created><updated>2013-11-06</updated><authors><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Nedi&#x107;</keyname><forenames>Angelia</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>Distributed Constrained Optimization by Consensus-Based Primal-Dual
  Perturbation Method</title><categories>cs.SY math.OC</categories><comments>32 pages; Revised and submitted to IEEE TRANSACTIONS ON Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various distributed optimization methods have been developed for solving
problems which have simple local constraint sets and whose objective function
is the sum of local cost functions of distributed agents in a network.
Motivated by emerging applications in smart grid and distributed sparse
regression, this paper studies distributed optimization methods for solving
general problems which have a coupled global cost function and have inequality
constraints. We consider a network scenario where each agent has no global
knowledge and can access only its local mapping and constraint functions. To
solve this problem in a distributed manner, we propose a consensus-based
distributed primal-dual perturbation (PDP) algorithm. In the algorithm, agents
employ the average consensus technique to estimate the global cost and
constraint functions via exchanging messages with neighbors, and meanwhile use
a local primal-dual perturbed subgradient method to approach a global optimum.
The proposed PDP method not only can handle smooth inequality constraints but
also non-smooth constraints such as some sparsity promoting constraints arising
in sparse optimization. We prove that the proposed PDP algorithm converges to
an optimal primal-dual solution of the original problem, under standard problem
and network assumptions. Numerical results illustrating the performance of the
proposed algorithm for a distributed demand response control problem in smart
grid are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5591</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5591</id><created>2013-04-20</created><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Parameterized Complexity of 1-Planarity</title><categories>cs.DS</categories><comments>WADS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding a 1-planar drawing for a general graph,
where a 1-planar drawing is a drawing in which each edge participates in at
most one crossing. Since this problem is known to be NP-hard we investigate the
parameterized complexity of the problem with respect to the vertex cover
number, tree-depth, and cyclomatic number. For these parameters we construct
fixed-parameter tractable algorithms. However, the problem remains NP-complete
for graphs of bounded bandwidth, pathwidth, or treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5594</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5594</id><created>2013-04-20</created><updated>2013-04-23</updated><authors><author><keyname>Shroff</keyname><forenames>Siddharth</forenames></author><author><keyname>Dabhi</keyname><forenames>Vipul</forenames></author></authors><title>Dew Point modelling using GEP based multi objective optimization</title><categories>cs.NE</categories><comments>14 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:1304.4055</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different techniques are used to model the relationship between temperatures,
dew point and relative humidity. Gene expression programming is capable of
modelling complex realities with great accuracy, allowing at the same time, the
extraction of knowledge from the evolved models compared to other learning
algorithms. We aim to use Gene Expression Programming for modelling of dew
point. Generally, accuracy of the model is the only objective used by selection
mechanism of GEP. This will evolve large size models with low training error.
To avoid this situation, use of multiple objectives, like accuracy and size of
the model are preferred by Genetic Programming practitioners. Solution to a
multi-objective problem is a set of solutions which satisfies the objectives
given by decision maker. Multi objective based GEP will be used to evolve
simple models. Various algorithms widely used for multi objective optimization,
like NSGA II and SPEA 2, are tested on different test problems. The results
obtained thereafter gives idea that SPEA 2 is better than NSGA II based on the
features like execution time, number of solutions obtained and convergence
rate. We selected SPEA 2 for dew point prediction. The multi-objective base GEP
produces accurate and simpler (smaller) solutions compared to solutions
produced by plain GEP for dew point predictions. Thus multi objective base GEP
produces better solutions by considering the dual objectives of fitness and
size of the solution. These simple models can be used to predict future values
of dew point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5602</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5602</id><created>2013-04-20</created><authors><author><keyname>Divroodi</keyname><forenames>Ali Rezaei</forenames></author><author><keyname>Nguyen</keyname><forenames>Linh Anh</forenames></author></authors><title>Bisimulation-Based Comparisons for Interpretations in Description Logics</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1104.1964</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study comparisons between interpretations in description logics with
respect to &quot;logical consequences&quot; of the form of semi-positive concepts (like
semi-positive concept assertions). Such comparisons are characterized by
conditions similar to the ones of bisimulations. The simplest among the
considered logics is a variant of PDL (propositional dynamic logic). The others
extend that logic with inverse roles, nominals, quantified number restrictions,
the universal role, and/or the concept constructor for expressing the local
reflexivity of a role. The studied problems are: preservation of semi-positive
concepts with respect to comparisons, the Hennessy-Milner property for
comparisons, and minimization of interpretations that preserves semi-positive
concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5604</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5604</id><created>2013-04-20</created><authors><author><keyname>Bui</keyname><forenames>Marc</forenames><affiliation>LAISC</affiliation></author><author><keyname>Lamure</keyname><forenames>Michel</forenames><affiliation>EDISS</affiliation></author><author><keyname>Lavallee</keyname><forenames>Ivan</forenames><affiliation>LAISC</affiliation></author></authors><title>La machine \alpha: mod\`ele g\'en\'erique pour les algorithmes naturels</title><categories>cs.CC</categories><comments>19 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  So far, following the works of A.M. Turing, the algorithms were considered as
the mathematical abstraction from which we could write programs for computers
whose principle was based on the theoretical concept of Turing machine. We
start here from the observation that natural algorithms or rather algorithms of
the nature which are massively parallel, autoadaptative and reproductible, and
for which we do not know how they really work, nor why, are not easily
specified by the current theoretical model of Universal Turing machine, or
Universal Computer. In particular the aspects of communications, evolutionary
rules (rulers), random (unpredictable) events, just like the genetic code, are
taken into account only by subtleties which oblige to break the theory. We
shall propose one \textit{universal model} of algorithm called machine-alpha
which contains and generalizes the existing models. --- Jusqu'ici, suite aux
travaux de A.M.Turing [Turing, 1936], les algorithmes ont \'et\'e vus comme
l'abstraction \`a partir de laquelle on pouvait \'ecrire des programmes pour
des ordinateurs dont le principe \'etait lui-m\^eme issu du concept th\'eorique
de machine de Turing. Nous partons ici du constat que les algorithmes naturels
ou plut\^ot les algorithmes de la nature, massivement parall\`eles,
autoadaptatifs et auto reproductibles, dont on ne sait pas comment ils
fonctionnent r\'eellement, ni pourquoi, ne sont pas ais\'ement sp\'ecifi\'es
par le mod\`ele th\'eorique actuel de Machine de Turing Universelle, ou de
Calculateur Universel ; en particulier les aspects de communications, de
r\`egles \'evolutives, d' \'ev\'enements al\'eatoires, \`a l'image du code
g\'en\'etique, ne sont pris en compte que par ajout d'artifices \`a la
th\'eorie. Nous nous proposons ici de montrer comment aborder ces probl\`emes
en repensant le mod\`ele th\'eorique. Nous proposerons un mod\`ele
d'algorithme, appel\'e ici machine-\alpha qui contient et g\'en\'eralise les
mod\`eles existants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5610</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5610</id><created>2013-04-20</created><authors><author><keyname>Lesner</keyname><forenames>Boris</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Scherrer</keyname><forenames>Bruno</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Tight Performance Bounds for Approximate Modified Policy Iteration with
  Non-Stationary Policies</title><categories>math.OC cs.AI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider approximate dynamic programming for the infinite-horizon
stationary $\gamma$-discounted optimal control problem formalized by Markov
Decision Processes. While in the exact case it is known that there always
exists an optimal policy that is stationary, we show that when using value
function approximation, looking for a non-stationary policy may lead to a
better performance guarantee. We define a non-stationary variant of MPI that
unifies a broad family of approximate DP algorithms of the literature. For this
algorithm we provide an error propagation analysis in the form of a performance
bound of the resulting policies that can improve the usual performance bound by
a factor $O(1-\gamma)$, which is significant when the discount factor $\gamma$
is close to 1. Doing so, our approach unifies recent results for Value and
Policy Iteration. Furthermore, we show, by constructing a specific
deterministic MDP, that our performance guarantee is tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5617</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5617</id><created>2013-04-20</created><updated>2015-10-19</updated><authors><author><keyname>Mondal</keyname><forenames>Nabarun</forenames></author><author><keyname>Ghosh</keyname><forenames>Partha P.</forenames></author></authors><title>Another Asymptotic Notation : &quot;Almost&quot;</title><categories>cs.CC</categories><comments>Was sent as mail to Robert Sidgewick, now submitted to SOP
  Transactions on Applied Mathematics(AM)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asymptotic notations are heavily used while analysing runtimes of algorithms.
Present paper argues that some of these usages are non trivial, therefore
incurring errors in communication of ideas. After careful reconsidera- tion of
the various existing notations a new notation is proposed. This notation has
similarities with the other heavily used notations like Big-Oh, Big Theta,
while being more accurate when describing the order relationship. It has been
argued that this notation is more suitable for describing algorithm runtime
than Big-Oh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5620</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5620</id><created>2013-04-20</created><authors><author><keyname>Gagen</keyname><forenames>Michael J</forenames></author></authors><title>Isomorphic Strategy Spaces in Game Theory</title><categories>cs.GT math.OC</categories><comments>160 pages, 43 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This book summarizes ongoing research introducing probability space
isomorphic mappings into the strategy spaces of game theory. This approach is
motivated by discrepancies between probability theory and game theory when
applied to the same strategic situation. In particular, probability theory and
game theory can disagree on calculated values of the Fisher information, the
log likelihood function, entropy gradients, the rank and Jacobian of variable
transforms, and even the dimensionality and volume of the underlying
probability parameter spaces. These differences arise as probability theory
employs structure preserving isomorphic mappings when constructing strategy
spaces to analyze games. In contrast, game theory uses weaker mappings which
change some of the properties of the underlying probability distributions
within the mixed strategy space. Here, we explore how using strong isomorphic
mappings to define game strategy spaces can alter rational outcomes in simple
games . Specific example games considered are the chain store paradox, the
trust game, the ultimatum game, the public goods game, the centipede game, and
the iterated prisoner's dilemma. In general, our approach provides rational
outcomes which are consistent with observed human play and might thereby
resolve some of the paradoxes of game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5625</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5625</id><created>2013-04-20</created><authors><author><keyname>Albers</keyname><forenames>Susanne</forenames></author><author><keyname>Hellwig</keyname><forenames>Matthias</forenames></author></authors><title>Online Makespan Minimization with Parallel Schedules</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In online makespan minimization a sequence of jobs $\sigma = J_1,..., J_n$
has to be scheduled on $m$ identical parallel machines so as to minimize the
maximum completion time of any job. We investigate the problem with an
essentially new model of resource augmentation. Here, an online algorithm is
allowed to build several schedules in parallel while processing $\sigma$. At
the end of the scheduling process the best schedule is selected. This model can
be viewed as providing an online algorithm with extra space, which is invested
to maintain multiple solutions. The setting is of particular interest in
parallel processing environments where each processor can maintain a single or
a small set of solutions.
  We develop a $(4/3+\eps)$-competitive algorithm, for any $0&lt;\eps\leq 1$, that
uses a number of $1/\eps^{O(\log (1/\eps))}$ schedules. We also give a
$(1+\eps)$-competitive algorithm, for any $0&lt;\eps\leq 1$, that builds a
polynomial number of $(m/\eps)^{O(\log (1/\eps) / \eps)}$ schedules. This value
depends on $m$ but is independent of the input $\sigma$. The performance
guarantees are nearly best possible. We show that any algorithm that achieves a
competitiveness smaller than 4/3 must construct $\Omega(m)$ schedules. Our
algorithms make use of novel guessing schemes that (1) predict the optimum
makespan of a job sequence $\sigma$ to within a factor of $1+\eps$ and (2)
guess the job processing times and their frequencies in $\sigma$. In (2) we
have to sparsify the universe of all guesses so as to reduce the number of
schedules to a constant.
  The competitive ratios achieved using parallel schedules are considerably
smaller than those in the standard problem without resource augmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5629</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5629</id><created>2013-04-20</created><authors><author><keyname>Goldfarb</keyname><forenames>Doron</forenames></author><author><keyname>Arends</keyname><forenames>Max</forenames></author><author><keyname>Froschauer</keyname><forenames>Josef</forenames></author><author><keyname>Merkl</keyname><forenames>Dieter</forenames></author></authors><title>Art History on Wikipedia, a Macroscopic Observation</title><categories>cs.SI cs.DL</categories><comments>6 pages, 5 figures</comments><acm-class>J.5; H.3.1; H.3.4</acm-class><journal-ref>2012. Proceedings of the 3rd Annual ACM Web Science Conference.
  ACM, New York, NY, USA. pp. 163 - 168</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How are articles about art historical actors interlinked within Wikipedia?
Lead by this question, we seek an overview on the link structure of a domain
specific subset of Wikipedia articles. We use an established domain-specific
person name authority, the Getty Union List of Artist Names (ULAN), in order to
externally identify relevant actors. Besides containing consistent biographical
person data, this database also provides associative relationships between its
person records, serving as a reference link structure for comparison. As a
first step, we use mappings between the ULAN and English Dbpedia provided by
the Virtual Internet Authority File (VIAF). This way, we are able to identify
18,002 relevant person articles. Examining the link structure between these
resources reveals interesting insight about the high level structure of art
historical knowledge as it is represented on Wikipedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5633</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5633</id><created>2013-04-20</created><authors><author><keyname>Hovnanyan</keyname><forenames>V. H.</forenames></author><author><keyname>Nahapetyan</keyname><forenames>H. E.</forenames></author><author><keyname>Poghosyan</keyname><forenames>Su. S.</forenames></author><author><keyname>Poghosyan</keyname><forenames>V. S.</forenames></author></authors><title>Tighter Upper Bounds for the Minimum Number of Calls and Rigorous
  Minimal Time in Fault-Tolerant Gossip Schemes</title><categories>cs.IT cs.DS math.IT</categories><comments>19 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The gossip problem (telephone problem) is an information dissemination
problem in which each of $n$ nodes of a communication network has a unique
piece of information that must be transmitted to all the other nodes using
two-way communications (telephone calls) between the pairs of nodes.
  During a call between the given two nodes, they exchange the whole
information known to them at that moment.
  In this paper we investigate the $k$-fault-tolerant gossip problem, which is
a generalization of the gossip problem, where at most $k$ arbitrary faults of
calls are allowed.
  The problem is to find the minimal number of calls $\tau(n,k)$ needed to
guarantee the $k$-fault-tolerance.
  We construct two classes of $k$-fault-tolerant gossip schemes (sequences of
calls) and found two upper bounds of $\tau(n,k)$, which improve the previously
known results.
  The first upper bound for general even $n$ is $\tau(n,k) \leq 1/2 n
\lceil\log_2 n\rceil + 1/2 n k$.
  This result is used to obtain the upper bound for general odd $n$.
  From the expressions for the second upper bound it follows that $\tau(n,k)
\leq 2/3 n k + O(n)$ for large $n$.
  Assuming that the calls can take place simultaneously, it is also of interest
to find $k$-fault-tolerant gossip schemes, which can spread the full
information in minimal time. For even $n$ we showed that the minimal time is
$T(n,k)=\lceil\log_2 n\rceil + k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5634</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5634</id><created>2013-04-20</created><authors><author><keyname>Xu</keyname><forenames>Chang</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author></authors><title>A Survey on Multi-view Learning</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In recent years, a great many methods of learning from multi-view data by
considering the diversity of different views have been proposed. These views
may be obtained from multiple sources or different feature subsets. In trying
to organize and highlight similarities and differences between the variety of
multi-view learning approaches, we review a number of representative multi-view
learning algorithms in different areas and classify them into three groups: 1)
co-training, 2) multiple kernel learning, and 3) subspace learning. Notably,
co-training style algorithms train alternately to maximize the mutual agreement
on two distinct views of the data; multiple kernel learning algorithms exploit
kernels that naturally correspond to different views and combine kernels either
linearly or non-linearly to improve learning performance; and subspace learning
algorithms aim to obtain a latent subspace shared by multiple views by assuming
that the input views are generated from this latent subspace. Though there is
significant variance in the approaches to integrating multiple views to improve
learning performance, they mainly exploit either the consensus principle or the
complementary principle to ensure the success of multi-view learning. Since
accessing multiple views is the fundament of multi-view learning, with the
exception of study on learning a model from multiple views, it is also valuable
to study how to construct multiple views and how to evaluate these views.
Overall, by exploring the consistency and complementary properties of different
views, multi-view learning is rendered more effective, more promising, and has
better generalization ability than single-view learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5643</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5643</id><created>2013-04-20</created><authors><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author></authors><title>Satisfiability and Canonisation of Timely Constraints</title><categories>math.CO cs.MA</categories><comments>Based upon Chapter 5 of arXiv:1206.2032</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We abstractly formulate an analytic problem that arises naturally in the
study of coordination in multi-agent systems. Let I be a set of arbitrary
cardinality (the set of actions) and assume that for each pair of distinct
actions (i,j), we are given a number \delta(i,j). We say that a function t,
specifying a time for each action, satisfies the timely constraint {\delta} if
for every pair of distinct actions (i,j), we have t(j)-t(i) &lt;= \delta(i,j) (and
thus also t(j)-t(i) &gt;= -\delta(j,i)). While the approach that first comes to
mind for analysing these definitions is an analytic/geometric one, it turns out
that graph-theoretic tools yield powerful results when applied to these
definitions. Using such tools, we characterise the set of satisfiable timely
constraints, and reduce the problem of satisfiability of a timely constraint to
the all-pairs shortest-path problem, and for finite I, furthermore to the
negative-cycle detection problem. Moreover, we constructively show that every
satisfiable timely constraint has a minimal satisfying function - a key
milestone on the way to optimally solving a large class of coordination
problems - and reduce the problem of finding this minimal satisfying function,
as well as the problems of classifying and comparing timely constraints, to the
all-pairs shortest-path problem. At the heart of our analysis lies the
constructive definition of a &quot;nicely-behaved&quot; representative for each class of
timely constraints sharing the same set of satisfying functions. We show that
this canonical representative, as well as the map from such canonical
representatives to the the sets of functions satisfying the classes of timely
constraints they represent, has many desired properties, which provide deep
insights into the structure underlying the above definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5649</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5649</id><created>2013-04-20</created><authors><author><keyname>Naruse</keyname><forenames>Makoto</forenames></author><author><keyname>Aono</keyname><forenames>Masashi</forenames></author><author><keyname>Kim</keyname><forenames>Song-Ju</forenames></author></authors><title>Nanoscale photonic network for solution searching and decision making
  problems</title><categories>cs.ET</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nature-inspired devices and architectures are attracting considerable
attention for various purposes, including the development of novel computing
techniques based on spatiotemporal dynamics, exploiting stochastic processes
for computing, and reducing energy dissipation. This paper demonstrates that
networks of optical energy transfers between quantum nanostructures mediated by
optical near-field interactions occurring at scales far below the wavelength of
light could be utilized for solving a constraint satisfaction problem (CSP),
the satisfiability problem (SAT), and a decision making problem. The optical
energy transfer from smaller quantum dots to larger ones, which is a quantum
stochastic process, depends on the existence of resonant energy levels between
the quantum dots or a state-filling effect occurring at the larger quantum
dots. Such a spatiotemporal mechanism yields different evolutions of energy
transfer patterns in multi-quantum-dot systems. We numerically demonstrate that
networks of optical energy transfers can be used for solution searching and
decision making. We consider that such an approach paves the way to a novel
physical informatics in which both coherent and dissipative processes are
exploited, with low energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5661</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5661</id><created>2013-04-20</created><authors><author><keyname>Kneuss</keyname><forenames>Etienne</forenames></author><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Kuraj</keyname><forenames>Ivan</forenames></author><author><keyname>Suter</keyname><forenames>Philippe</forenames></author></authors><title>On Integrating Deductive Synthesis and Verification Systems</title><categories>cs.PL cs.LO cs.SE</categories><comments>17 pages. 46 references</comments><report-no>EPFL-REPORT-186043</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe techniques for synthesis and verification of recursive functional
programs over unbounded domains. Our techniques build on top of an algorithm
for satisfiability modulo recursive functions, a framework for deductive
synthesis, and complete synthesis procedures for algebraic data types. We
present new counterexample-guided algorithms for constructing verified
programs. We have implemented these algorithms in an integrated environment for
interactive verification and synthesis from relational specifications. Our
system was able to synthesize a number of useful recursive functions that
manipulate unbounded numbers and data structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5666</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5666</id><created>2013-04-20</created><updated>2014-03-27</updated><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>The Structure and Quantum Capacity of a Partially Degradable Quantum
  Channel</title><categories>quant-ph cs.IT math.IT</categories><comments>59 pages, 13 figures, Journal-ref: IEEE Access</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantum capacity of degradable quantum channels has been proven to be
additive. On the other hand, there is no general rule for the behavior of
quantum capacity for non-degradable quantum channels. We introduce the set of
partially degradable (PD) quantum channels to answer the question of additivity
of quantum capacity for a well-separable subset of non-degradable channels. A
quantum channel is partially degradable if the channel output can be used to
simulate the degraded environment state. PD channels could exist both in the
degradable, non-degradable and conjugate degradable family. We define the term
partial simulation, which is a clear benefit that arises from the structure of
the complementary channel of a PD channel. We prove that the quantum capacity
of an arbitrary dimensional PD channel is additive. We also demonstrate that
better quantum data rates can be achieved over a PD channel in comparison to
standard (non-PD) channels. Our results indicate that the partial degradability
property can be exploited and yet still hold many benefits for quantum
communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5670</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5670</id><created>2013-04-20</created><authors><author><keyname>Anatolij</keyname><forenames>Platonov</forenames></author></authors><title>Particularities of Analog FCS Optimization</title><categories>cs.IT math.IT</categories><comments>4 pages, f Gigures, submitted to IEEE Communication Letters
  17.04.2013</comments><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is analyzed a performance of optimal feedback communication systems
with the analog transmitters in the forward channel (AFCS). It is shown that
measures and limit boundaries of AFCS performance are similar but differ from
those used in digital communications and information theory. The causes of the
differences are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5672</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5672</id><created>2013-04-20</created><authors><author><keyname>Rahman</keyname><forenames>Mahmudur</forenames></author><author><keyname>Carbunar</keyname><forenames>Bogdan</forenames></author><author><keyname>Banik</keyname><forenames>Madhusudan</forenames></author></authors><title>Fit and Vulnerable: Attacks and Defenses for a Health Monitoring Device</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fusion of social networks and wearable sensors is becoming increasingly
popular, with systems like Fitbit automating the process of reporting and
sharing user fitness data. In this paper we show that while compelling, the
integration of health data into social networks is fraught with privacy and
security vulnerabilities. Case in point, by reverse engineering the
communication protocol, storage details and operation codes, we identified
several vulnerabilities in Fitbit. We have built FitBite, a suite of tools that
exploit these vulnerabilities to launch a wide range of attacks against Fitbit.
Besides eavesdropping, injection and denial of service, several attacks can
lead to rewards and financial gains. We have built FitLock, a lightweight
defense system that protects Fitbit while imposing only a small overhead. Our
experiments on BeagleBoard and Xperia devices show that FitLock's end-to-end
overhead over Fitbit is only 2.4%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5677</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5677</id><created>2013-04-20</created><authors><author><keyname>Fux</keyname><forenames>Vladimir</forenames></author><author><keyname>Maill&#xe9;</keyname><forenames>Patrick</forenames></author><author><keyname>Bonnin</keyname><forenames>Jean-Marie</forenames></author><author><keyname>Kaci</keyname><forenames>Nassim</forenames></author></authors><title>Efficiency or fairness: managing applications with different delay
  sensitivities in heterogeneous wireless networks</title><categories>cs.NI cs.GT</categories><comments>Work presented at the WoWMoM 2013 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current intensively changing technological environment, wireless
network operators try to manage the increase of global traffic, optimizing the
use of the available resources. This involves associating each user to one of
its reachable wireless networks; a decision that can be made on the user side,
in which case inefficiencies stem from user selfishness.
  This paper aims at correcting that efficiency loss through the use of a
one-dimensional incentive signal, interpreted as a price. While the so-called
Pigovian taxes allow to deal with homogeneous users, we consider here two
classes with different sensitivities to the Quality of Service, reflecting the
dichotomy between delay-sensitive and delay-insensitive applications. We
consider a geographic area covered by two wireless networks, among which users
choose based on a trade-off between the quality of service and the price to
pay.
  Using a non-atomic routing game model, we study analytically the case of
constant demand levels. We show that when properly designed, the incentives
elicit efficient user-network associations. Moreover, those optimal incentives
can be simply computed by the wireless operator, using only some information
that is easily available. Finally, the performance of the incentive scheme
under dynamic demand (users opening and closing connections over time) are
investigated through simulations, our incentive scheme also yielding
significant improvements in that case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5678</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5678</id><created>2013-04-20</created><authors><author><keyname>Stambaugh</keyname><forenames>Carly</forenames></author><author><keyname>Yang</keyname><forenames>Hui</forenames></author><author><keyname>Breuer</keyname><forenames>Felix</forenames></author></authors><title>Analytic Feature Selection for Support Vector Machines</title><categories>cs.LG stat.ML</categories><comments>To be presented at 9th International Conference on Machine Learning
  and Data Mining MLDM 2013. 15 pages, 2 figures</comments><acm-class>I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support vector machines (SVMs) rely on the inherent geometry of a data set to
classify training data. Because of this, we believe SVMs are an excellent
candidate to guide the development of an analytic feature selection algorithm,
as opposed to the more commonly used heuristic methods. We propose a
filter-based feature selection algorithm based on the inherent geometry of a
feature set. Through observation, we identified six geometric properties that
differ between optimal and suboptimal feature sets, and have statistically
significant correlations to classifier performance. Our algorithm is based on
logistic and linear regression models using these six geometric properties as
predictor variables. The proposed algorithm achieves excellent results on high
dimensional text data sets, with features that can be organized into a handful
of feature types; for example, unigrams, bigrams or semantic structural
features. We believe this algorithm is a novel and effective approach to
solving the feature selection problem for linear SVMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5688</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5688</id><created>2013-04-21</created><authors><author><keyname>Vissoci</keyname><forenames>Joao Ricardo Nickenig</forenames></author><author><keyname>Rodrigues</keyname><forenames>Clarissa G.</forenames></author><author><keyname>de Andrade</keyname><forenames>Luciano</forenames></author><author><keyname>Santana</keyname><forenames>Jose Eduardo</forenames></author><author><keyname>Zaveri</keyname><forenames>Amrapali</forenames></author><author><keyname>Pietrobon</keyname><forenames>Ricardo</forenames></author></authors><title>A Framework for Reproducible, Interactive Research: Application to
  health and social sciences</title><categories>cs.DL cs.CY stat.CO</categories><comments>09 pages, 01 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this article is to introduce a reporting framework for
reproducible, interactive research applied to Big Clinical Data, based on open
source technologies. The framework is constituted by the following three axes:
(i) data, (ii) analytical codes and (iii) dissemination. In this paper,
different documentation formats and online repositories are introduced. To
integrate and manage the reproducible contents, we propose the R Language as
the tool of choice. All the information is then published and gathered in a
website for different projects. This framework is free and user friendly and is
proposed to enhance reproducibility of health-science reports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5700</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5700</id><created>2013-04-21</created><authors><author><keyname>Tian</keyname><forenames>Ye</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>Guiding Blind Transmitters: Degrees of Freedom Optimal Interference
  Alignment Using Relays</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE Transactions on Information Theory,
  April 1 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel state information (CSI) at the transmitters (CSIT) is of importance
for interference alignment schemes to achieve the optimal degrees of freedom
(DoF) for wireless networks. This paper investigates the impact of half-duplex
relays on the degrees of freedom (DoF) of the X channel and the interference
channel when the transmitters are blind in the sense that no ISIT is available.
In particular, it is shown that adding relay nodes with global CSI to the
communication model is sufficient to recover the DoF that is the optimal for
these models with global CSI at the transmitters. The relay nodes in essence
help steer the directions of the transmitted signals to facilitate interference
alignment to achieve the optimal DoF with CSIT. The general MxN X channel with
relays and the K-user interference channel are both investigated, and
sufficient conditions on the number of antennas at the relays and the number of
relays needed to achieve the optimal DoF with CSIT are established. Using
relays, the optimal DoF can be achieved in finite channel uses. The DoF for the
case when relays only have delayed CSI is also investigated, and it is shown
that with delayed CSI at the relay the optimal DoF with full CSIT cannot be
achieved. Special cases of the X channel and interference channel are
investigated to obtain further design insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5702</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5702</id><created>2013-04-21</created><updated>2014-05-11</updated><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>Goertz</keyname><forenames>Inge Li</forenames></author><author><keyname>Landau</keyname><forenames>Gad M.</forenames></author><author><keyname>Weimann</keyname><forenames>Oren</forenames></author></authors><title>Tree Compression with Top Trees</title><categories>cs.DS</categories><comments>An extended abstract of this paper appeared at the 40th International
  Colloquium on Automata, Languages and Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new compression scheme for labeled trees based on top trees.
Our compression scheme is the first to simultaneously take advantage of
internal repeats in the tree (as opposed to the classical DAG compression that
only exploits rooted subtree repeats) while also supporting fast navigational
queries directly on the compressed representation. We show that the new
compression scheme achieves close to optimal worst-case compression, can
compress exponentially better than DAG compression, is never much worse than
DAG compression, and supports navigational queries in logarithmic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5705</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5705</id><created>2013-04-21</created><authors><author><keyname>Bera</keyname><forenames>Rajendra K.</forenames></author></authors><title>A novice looks at emotional cognition</title><categories>cs.AI</categories><comments>11 pages, 1 figure</comments><msc-class>68, 81, 92</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling emotional-cognition is in a nascent stage and therefore wide-open
for new ideas and discussions. In this paper the author looks at the modeling
problem by bringing in ideas from axiomatic mathematics, information theory,
computer science, molecular biology, non-linear dynamical systems and quantum
computing and explains how ideas from these disciplines may have applications
in modeling emotional-cognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5706</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5706</id><created>2013-04-21</created><authors><author><keyname>Bakholdin</keyname><forenames>I. B.</forenames></author></authors><title>Calculation and analysis of solitary waves and kinks in elastic tubes</title><categories>cs.CE math-ph math.MP nlin.PS</categories><comments>17 pages, 10 figures</comments><msc-class>35Q35, 35Q53, 74F10, 65Z05, 68-04, 37K40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to analysis of different models that describe waves in
fluid-filled and gas-filled elastic tubes and development of methods of
calculation and numerical analysis of solutions with solitary waves and kinks
for these models. Membrane model and plate model are used for tube. Two types
of solitary waves are found. One-parametric families are stable and may be used
as shock structures. Null-parametric solitary waves are unstable. The process
of split of such solitary waves is investigated. It may lead to appearance of
solutions with kinks. Kink solutions are null-parametric and stable. General
theory of reversible shocks is used for analysis of numerical solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5714</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5714</id><created>2013-04-21</created><authors><author><keyname>Ivan</keyname><forenames>Szabolcs</forenames></author><author><keyname>Nagy-Gyorgy</keyname><forenames>Judit</forenames></author></authors><title>On the structure and syntactic complexity of generalized definite
  languages</title><categories>cs.FL cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a forbidden pattern characterization for the class of generalized
definite languages, show that the corresponding problem is NL-complete and can
be solved in quadratic time. We also show that their syntactic complexity
coincides with that of the definite languages and give an upper bound of n! for
this measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5719</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5719</id><created>2013-04-21</created><updated>2015-01-05</updated><authors><author><keyname>Dolev</keyname><forenames>Danny</forenames></author><author><keyname>Heljanko</keyname><forenames>Keijo</forenames></author><author><keyname>J&#xe4;rvisalo</keyname><forenames>Matti</forenames></author><author><keyname>Korhonen</keyname><forenames>Janne H.</forenames></author><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Rybicki</keyname><forenames>Joel</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author><author><keyname>Wieringa</keyname><forenames>Siert</forenames></author></authors><title>Synchronous Counting and Computational Algorithm Design</title><categories>cs.DC cs.CC cs.DS</categories><comments>35 pages, extended and revised version</comments><doi>10.1007/978-3-319-03089-0_17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a complete communication network on $n$ nodes, each of which is a
state machine. In synchronous 2-counting, the nodes receive a common clock
pulse and they have to agree on which pulses are &quot;odd&quot; and which are &quot;even&quot;. We
require that the solution is self-stabilising (reaching the correct operation
from any initial state) and it tolerates $f$ Byzantine failures (nodes that
send arbitrary misinformation). Prior algorithms are expensive to implement in
hardware: they require a source of random bits or a large number of states.
  This work consists of two parts. In the first part, we use computational
techniques (often known as synthesis) to construct very compact deterministic
algorithms for the first non-trivial case of $f = 1$. While no algorithm exists
for $n &lt; 4$, we show that as few as 3 states per node are sufficient for all
values $n \ge 4$. Moreover, the problem cannot be solved with only 2 states per
node for $n = 4$, but there is a 2-state solution for all values $n \ge 6$.
  In the second part, we develop and compare two different approaches for
synthesising synchronous counting algorithms. Both approaches are based on
casting the synthesis problem as a propositional satisfiability (SAT) problem
and employing modern SAT-solvers. The difference lies in how to solve the SAT
problem: either in a direct fashion, or incrementally within a counter-example
guided abstraction refinement loop. Empirical results suggest that the former
technique is more efficient if we want to synthesise time-optimal algorithms,
while the latter technique discovers non-optimal algorithms more quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5723</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5723</id><created>2013-04-21</created><updated>2014-12-04</updated><authors><author><keyname>Frenkel</keyname><forenames>P&#xe9;ter E.</forenames></author><author><keyname>Weiner</keyname><forenames>Mih&#xe1;ly</forenames></author></authors><title>Classical information storage in an $n$-level quantum system</title><categories>cs.IT math-ph math.IT math.MP quant-ph</categories><comments>13 pages</comments><journal-ref>Comm. Math. Phys. 340 (2015), no. 2, 563--574</journal-ref><doi>10.1007/s00220-015-2463-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A game is played by a team of two --- say Alice and Bob --- in which the
value of a random variable $x$ is revealed to Alice only, who cannot freely
communicate with Bob. Instead, she is given a quantum $n$-level system,
respectively a classical $n$-state system, which she can put in possession of
Bob in any state she wishes. We evaluate how successfully they managed to store
and recover the value of $x$ in the used system by requiring Bob to specify a
value $z$ and giving a reward of value $ f(x,z)$ to the team.
  We show that whatever the probability distribution of $x$ and the reward
function $f$ are, when using a quantum $n$-level system, the maximum expected
reward obtainable with the best possible team strategy is equal to that
obtainable with the use of a classical $n$-state system.
  The proof relies on mixed discriminants of positive matrices and --- perhaps
surprisingly --- an application of the Supply--Demand Theorem for bipartite
graphs. As a corollary, we get an infinite set of new, dimension dependent
inequalities regarding positive operator valued measures and density operators
on complex $n$-space.
  As a further corollary, we see that the greatest value, with respect to a
given distribution of $x$, of the mutual information $I(x;z)$ that is
obtainable using an $n$-level quantum system equals the analogous maximum for a
classical $n$-state system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5725</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5725</id><created>2013-04-21</created><authors><author><keyname>Tahir</keyname><forenames>M.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Iqbal</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Alrajeh</keyname><forenames>N.</forenames></author></authors><title>On Adaptive Energy Efficient Transmission in WSNs</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1303.6242</comments><journal-ref>International Journal of Distributed Sensor Networks, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major challenges in design of Wireless Sensor Networks (WSNs) is
to reduce energy consumption of sensor nodes to prolong lifetime of
finite-capacity batteries. In this paper, we propose Energy-efficient Adaptive
Scheme for Transmission (EAST) in WSNs. EAST is an IEEE 802.15.4 standard
compliant. In this scheme, open-loop is used for temperature-aware link quality
estimation and compensation. Whereas, closed-loop feedback process helps to
divide network into three logical regions to minimize overhead of control
packets. Threshold on transmitter power loss (RSSIloss) and current number of
nodes (nc(t)) in each region help to adapt transmit power level (Plevel)
according to link quality changes due to temperature variation. Evaluation of
propose scheme is done by considering mobile sensor nodes and reference node
both static and mobile. Simulation results show that propose scheme effectively
adapts transmission Plevel to changing link quality with less control packets
overhead and energy consumption as compared to classical approach with single
region in which maximum transmitter Plevel assigned to compensate temperature
variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5743</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5743</id><created>2013-04-21</created><authors><author><keyname>Gu&#xe9;ret</keyname><forenames>Christophe</forenames></author><author><keyname>Chambers</keyname><forenames>Tamy</forenames></author><author><keyname>Reijnhoudt</keyname><forenames>Linda</forenames></author><author><keyname>van der Most</keyname><forenames>Frank</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author></authors><title>Genericity versus expressivity - an exercise in semantic interoperable
  research information systems for Web Science</title><categories>cs.DL</categories><comments>Long version of a paper submitted to the WebScience 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The web does not only enable new forms of science, it also creates new
possibilities to study science and new digital scholarship. This paper brings
together multiple perspectives: from individual researchers seeking the best
options to display their activities and market their skills on the academic job
market; to academic institutions, national funding agencies, and countries
needing to monitor the science system and account for public money spending. We
also address the research interests aimed at better understanding the
self-organising and complex nature of the science system through researcher
tracing, the identification of the emergence of new fields, and knowledge
discovery using large-data mining and non-linear dynamics. In particular this
paper draws attention to the need for standardisation and data interoperability
in the area of research information as an indispensable pre-condition for any
science modelling. We discuss which levels of complexity are needed to provide
a globally, interoperable, and expressive data infrastructure for research
information. With possible dynamic science model applications in mind, we
introduce the need for a &quot;middle-range&quot; level of complexity for data
representation and propose a conceptual model for research data based on a core
international ontology with national and local extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5745</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5745</id><created>2013-04-21</created><updated>2014-12-28</updated><authors><author><keyname>Tadrous</keyname><forenames>John</forenames></author><author><keyname>Eryilmaz</keyname><forenames>Atilla</forenames></author><author><keyname>Gamal</keyname><forenames>Hesham El</forenames></author></authors><title>Proactive Data Download and User Demand Shaping for Data Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose and study optimal proactive resource allocation and
demand shaping for data networks. Motivated by the recent findings on the
predictability of human behavior patterns in data networks, and the emergence
of highly capable handheld devices, our design aims to smooth out the network
traffic over time and minimize the data delivery costs.
  Our framework utilizes proactive data services as well as smart content
recommendation schemes for shaping the demand. Proactive data services take
place during the off-peak hours based on a statistical prediction of a demand
profile for each user, whereas smart content recommendation assigns modified
valuations to data items so as to render the users' demand less uncertain.
Hence, our recommendation scheme aims to boost the performance of proactive
services within the allowed flexibility of user requirements. We conduct
theoretical performance analysis that quantifies the leveraged cost reduction
through the proposed framework. We show that the cost reduction scales at the
same rate as the cost function scales with the number of users. Further, we
prove that \emph{demand shaping} through smart recommendation strictly reduces
the incurred cost even below that of proactive downloads without
recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5746</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5746</id><created>2013-04-21</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author></authors><title>Long Circuits and Large Euler Subgraphs</title><categories>cs.DS</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An undirected graph is Eulerian if it is connected and all its vertices are
of even degree. Similarly, a directed graph is Eulerian, if for each vertex its
in-degree is equal to its out-degree. It is well known that Eulerian graphs can
be recognized in polynomial time while the problems of finding a maximum
Eulerian subgraph or a maximum induced Eulerian subgraph are NP-hard. In this
paper, we study the parameterized complexity of the following Euler subgraph
problems:
  - Large Euler Subgraph: For a given graph G and integer parameter k, does G
contain an induced Eulerian subgraph with at least k vertices?
  - Long Circuit: For a given graph G and integer parameter k, does G contain
an Eulerian subgraph with at least k edges?
  Our main algorithmic result is that Large Euler Subgraph is fixed parameter
tractable (FPT) on undirected graphs. We find this a bit surprising because the
problem of finding an induced Eulerian subgraph with exactly k vertices is
known to be W[1]-hard. The complexity of the problem changes drastically on
directed graphs. On directed graphs we obtained the following complexity
dichotomy: Large Euler Subgraph is NP-hard for every fixed k&gt;3 and is solvable
in polynomial time for k&lt;=3. For Long Circuit, we prove that the problem is FPT
on directed and undirected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5753</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5753</id><created>2013-04-21</created><updated>2013-07-16</updated><authors><author><keyname>Salah</keyname><forenames>Almila Akdag</forenames></author><author><keyname>Wyatt</keyname><forenames>Sally</forenames></author><author><keyname>Passi</keyname><forenames>Samir</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author></authors><title>Mapping EINS -- An exercise in mapping the Network of Excellence in
  Internet Science</title><categories>cs.DL</categories><journal-ref>Conference Proceedings of the First International Conference on
  Internet Science, April 9-11, 2013 Brussels. Pages 75-78</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper demonstrates the application of bibliometric mapping techniques in
the area of funded research networks. We discuss how science maps can be used
to facilitate communication inside newly formed communities, but also to
account for their activities to funding agencies. We present the mapping of
EINS as case -- an FP7 funded Network of Excellence. Finally, we discuss how
these techniques can be used to serve as knowledge maps for interdisciplinary
working experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5755</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5755</id><created>2013-04-21</created><authors><author><keyname>Kishor</keyname><forenames>Puneet</forenames></author><author><keyname>Seneviratne</keyname><forenames>Oshani</forenames></author><author><keyname>Giansiracusa</keyname><forenames>Noah</forenames></author></authors><title>Policy Aware Geospatial Data</title><categories>cs.OH</categories><comments>5 pages. Accepted for ACMGIS 2009, but withdrawn because ACM would
  not include this paper unless I presented in person (prior commitments
  prevented me from travel even though I had registered)</comments><acm-class>E.0</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Digital Rights Management (DRM) prevents end-users from using content in a
manner inconsistent with its creator's wishes. The license describing these
use-conditions typically accompanies the content as its metadata. A resulting
problem is that the license and the content can get separated and lose track of
each other. The best metadata have two distinct qualities--they are created
automatically without user intervention, and they are embedded within the data
that they describe. If licenses are also created and transported this way, data
will always have licenses, and the licenses will be readily examinable. When
two or more datasets are combined, a new dataset, and with it a new license,
are created. This new license is a function of the licenses of the component
datasets and any additional conditions that the person combining the datasets
might want to impose. Following the notion of a data-purpose algebra, we model
this phenomenon by interpreting the transfer and conjunction of data as
inducing an algebraic operation on the corresponding licenses. When a dataset
passes from one source to the next its license is transformed in a
deterministic way, and similarly when datasets are combined the associated
licenses are combined in a non-trivial algebraic manner. Modern,
computer-savvy, licensing regimes such as Creative Commons allow writing the
license in a special kind of language called Creative Commons Rights Expression
Language (ccREL). ccREL allows creating and embedding the license using RDFa
utilizing XHTML. This is preferred over DRM which includes the rights in a
binary file completely opaque to nearly all users. The colocation of metadata
with human-visible XHTML makes the license more transparent. In this paper we
describe a methodology for creating and embedding licenses in geographic data
utilizing ccREL, and programmatically examining embedded licenses in component
data...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5758</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5758</id><created>2013-04-21</created><updated>2013-10-02</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Liu</keyname><forenames>Che-Yu</forenames></author></authors><title>Prior-free and prior-dependent regret bounds for Thompson Sampling</title><categories>stat.ML cs.LG</categories><comments>A previous version appeared under the title 'A note on the Bayesian
  regret of Thompson Sampling with an arbitrary prior'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the stochastic multi-armed bandit problem with a prior
distribution on the reward distributions. We are interested in studying
prior-free and prior-dependent regret bounds, very much in the same spirit as
the usual distribution-free and distribution-dependent bounds for the
non-Bayesian stochastic bandit. Building on the techniques of Audibert and
Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling
attains an optimal prior-free bound in the sense that for any prior
distribution its Bayesian regret is bounded from above by $14 \sqrt{n K}$. This
result is unimprovable in the sense that there exists a prior distribution such
that any algorithm has a Bayesian regret bounded from below by $\frac{1}{20}
\sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al.
[2013] (where the optimal mean is known as well as a lower bound on the
smallest gap) and we show that in this case the regret of Thompson Sampling is
in fact uniformly bounded over time, thus showing that Thompson Sampling can
greatly take advantage of the nice properties of these priors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5773</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5773</id><created>2013-04-21</created><updated>2014-02-12</updated><authors><author><keyname>Gaitan</keyname><forenames>Frank</forenames></author><author><keyname>Clark</keyname><forenames>Lane</forenames></author></authors><title>Graph isomorphism and adiabatic quantum computing</title><categories>quant-ph cs.DS math-ph math.CO math.MP</categories><comments>22 pages; 18 figures; and 6 tables; version to appear in Physical
  Review A</comments><journal-ref>Phys. Rev. A vol. 89, 022342 (2014)</journal-ref><doi>10.1103/PhysRevA.89.022342</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Graph Isomorphism problem two N-vertex graphs G and G' are given and
the task is to determine whether there exists a permutation of the vertices of
G that preserves adjacency and transforms G into G'. If yes, then G and G' are
said to be isomorphic; otherwise they are non-isomorphic. The GI problem is an
important problem in computer science and is thought to be of comparable
difficulty to integer factorization. In this paper we present a quantum
algorithm that solves arbitrary instances of GI and can also determine all
automorphisms of a given graph. We show how the GI problem can be converted to
a combinatorial optimization problem that can be solved using adiabatic quantum
evolution. We numerically simulate the algorithm's quantum dynamics and show
that it correctly: (i) distinguishes non-isomorphic graphs; (ii) recognizes
isomorphic graphs; and (iii) finds all automorphisms of a given graph G. We
then discuss the GI quantum algorithm's experimental implementation, and close
by showing how it can be leveraged to give a quantum algorithm that solves
arbitrary instances of the NP-Complete Sub-Graph Isomorphism problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5774</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5774</id><created>2013-04-21</created><updated>2015-11-17</updated><authors><author><keyname>Berlinkov</keyname><forenames>Mikhail V.</forenames></author></authors><title>On the probability of being synchronizable</title><categories>cs.FL cs.DM math.CO</categories><comments>Highest trees has been changed with $1$-branches to get a stable pair
  completely defined by the letter</comments><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a random automaton with $n$ states and any fixed non-singleton
alphabet is synchronizing with high probability. Moreover, we also prove that
the convergence rate is exactly $1-\Theta(\frac{1}{n})$ as conjectured by
Cameron \cite{CamConj} for the most interesting binary alphabet case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5777</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5777</id><created>2013-04-21</created><updated>2014-05-16</updated><authors><author><keyname>Tavenas</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Improved bounds for reduction to depth 4 and depth 3</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Koiran showed that if a $n$-variate polynomial of degree $d$ (with
$d=n^{O(1)}$) is computed by a circuit of size $s$, then it is also computed by
a homogeneous circuit of depth four and of size
$2^{O(\sqrt{d}\log(d)\log(s))}$. Using this result, Gupta, Kamath, Kayal and
Saptharishi gave a $\exp(O(\sqrt{d\log(d)\log(n)\log(s)}))$ upper bound for the
size of the smallest depth three circuit computing a $n$-variate polynomial of
degree $d=n^{O(1)}$ given by a circuit of size $s$.
  We improve here Koiran's bound. Indeed, we show that if we reduce an
arithmetic circuit to depth four, then the size becomes
$\exp(O(\sqrt{d\log(ds)\log(n)}))$. Mimicking Gupta, Kamath, Kayal and
Saptharishi's proof, it also implies the same upper bound for depth three
circuits.
  This new bound is not far from optimal in the sense that Gupta, Kamath, Kayal
and Saptharishi also showed a $2^{\Omega(\sqrt{d})}$ lower bound for the size
of homogeneous depth four circuits such that gates at the bottom have fan-in at
most $\sqrt{d}$. Finally, we show that this last lower bound also holds if the
fan-in is at least $\sqrt{d}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5779</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5779</id><created>2013-04-21</created><updated>2013-05-11</updated><authors><author><keyname>Mrabet</keyname><forenames>Nadia El</forenames><affiliation>LIASD</affiliation></author><author><keyname>Poinsot</keyname><forenames>Laurent</forenames><affiliation>LIPN</affiliation></author></authors><title>Pairings from a tensor product point of view</title><categories>math.RA cs.CR math.GR</categories><comments>15 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pairings are particular bilinear maps, and as any bilinear maps they factor
through the tensor product as group homomorphisms. Besides, nothing seems to
prevent us to construct pairings on other abelian groups than elliptic curves
or more general abelian varieties. The point of view adopted in this
contribution is based on these two observations. Thus we present an elliptic
curve free study of pairings which is essentially based on tensor products of
abelian groups (or modules). Tensor products of abelian groups are even
explicitly computed under finiteness conditions. We reveal that the existence
of pairings depends on the non-degeneracy of some universal bilinear map,
called the canonical bilinear map. In particular it is shown that the
construction of a pairing on $A\times A$ is always possible whatever a finite
abelian group $A$ is. We also propose some new constructions of pairings, one
of them being based on the notion of group duality which is related to the
concept of non-degeneracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5790</identifier>
 <datestamp>2013-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5790</id><created>2013-04-21</created><updated>2013-05-31</updated><authors><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Knopp</keyname><forenames>Raymond</forenames></author><author><keyname>Salim</keyname><forenames>Umer</forenames></author></authors><title>Gaussian Half-Duplex Relay Networks: improved constant gap and
  connections with the assignment problem</title><categories>cs.IT math.IT</categories><comments>Substantial text overlaps with Section VIII in arXiv: 1301.5522.
  Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a general Gaussian relay network where a source
transmits a message to a destination with the help of N half-duplex relays. It
proves that the information theoretic cut-set upper bound to the capacity can
be achieved to within 2:021(N +2) bits with noisy network coding, thereby
reducing the previously known gap. Further improved gap results are presented
for more structured networks like diamond networks. It is then shown that the
generalized Degrees-of-Freedom of a general Gaussian half-duplex relay network
is the solution of a linear program, where the coefficients of the linear
inequality constraints are proved to be the solution of several linear
programs, known in graph theory as the assignment problem, for which efficient
numerical algorithms exist. The optimal schedule, that is, the optimal value of
the 2^N possible transmit-receive configurations/states for the relays, is
investigated and known results for diamond networks are extended to general
relay networks. It is shown, for the case of 2 relays, that only 3 out of the 4
possible states have strictly positive probability. Extensive experimental
results show that, for a general N-relay network with N&lt;9, the optimal schedule
has at most N +1 states with strictly positive probability. As an extension of
a conjecture presented for diamond networks, it is conjectured that this result
holds for any HD relay network and any number of relays. Finally, a 2-relay
network is studied to determine the channel conditions under which selecting
the best relay is not optimal, and to highlight the nature of the rate gain due
to multiple relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5793</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5793</id><created>2013-04-21</created><updated>2014-08-22</updated><authors><author><keyname>Tyagi</keyname><forenames>Hemant</forenames></author><author><keyname>G&#xe4;rtner</keyname><forenames>Bernd</forenames></author></authors><title>Continuum armed bandit problem of few variables in high dimensions</title><categories>cs.LG</categories><comments>(1) Appeared in proceedings of 11th Workshop on Approximation and
  Online Algorithms (WAOA 2013). (2) Corrected minor typos in previous version
  (3) 17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the stochastic and adversarial settings of continuum armed
bandits where the arms are indexed by [0,1]^d. The reward functions r:[0,1]^d
-&gt; R are assumed to intrinsically depend on at most k coordinate variables
implying r(x_1,..,x_d) = g(x_{i_1},..,x_{i_k}) for distinct and unknown
i_1,..,i_k from {1,..,d} and some locally Holder continuous g:[0,1]^k -&gt; R with
exponent 0 &lt; alpha &lt;= 1. Firstly, assuming (i_1,..,i_k) to be fixed across
time, we propose a simple modification of the CAB1 algorithm where we construct
the discrete set of sampling points to obtain a bound of
O(n^((alpha+k)/(2*alpha+k)) (log n)^((alpha)/(2*alpha+k)) C(k,d)) on the
regret, with C(k,d) depending at most polynomially in k and sub-logarithmically
in d. The construction is based on creating partitions of {1,..,d} into k
disjoint subsets and is probabilistic, hence our result holds with high
probability. Secondly we extend our results to also handle the more general
case where (i_1,...,i_k) can change over time and derive regret bounds for the
same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5799</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5799</id><created>2013-04-21</created><authors><author><keyname>Pignolet</keyname><forenames>Yvonne-Anne</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author><author><keyname>Tredan</keyname><forenames>Gilles</forenames></author></authors><title>Request Complexity of VNet Topology Extraction: Dictionary-Based Attacks</title><categories>cs.NI cs.DS</categories><comments>full version of paper at NETYS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The network virtualization paradigm envisions an Internet where arbitrary
virtual networks (VNets) can be specified and embedded over a shared substrate
(e.g., the physical infrastructure). As VNets can be requested at short notice
and for a desired time period only, the paradigm enables a flexible service
deployment and an efficient resource utilization.
  This paper investigates the security implications of such an architecture. We
consider a simple model where an attacker seeks to extract secret information
about the substrate topology, by issuing repeated VNet embedding requests. We
present a general framework that exploits basic properties of the VNet
embedding relation to infer the entire topology. Our framework is based on a
graph motif dictionary applicable for various graph classes. Moreover, we
provide upper bounds on the request complexity, the number of requests needed
by the attacker to succeed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5802</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5802</id><created>2013-04-21</created><authors><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author><author><keyname>Yang</keyname><forenames>Allen Y.</forenames></author><author><keyname>Dong</keyname><forenames>Roy</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Nonlinear Basis Pursuit</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressive sensing, the basis pursuit algorithm aims to find the sparsest
solution to an underdetermined linear equation system. In this paper, we
generalize basis pursuit to finding the sparsest solution to higher order
nonlinear systems of equations, called nonlinear basis pursuit. In contrast to
the existing nonlinear compressive sensing methods, the new algorithm that
solves the nonlinear basis pursuit problem is convex and not greedy. The novel
algorithm enables the compressive sensing approach to be used for a broader
range of applications where there are nonlinear relationships between the
measurements and the unknowns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5806</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5806</id><created>2013-04-21</created><authors><author><keyname>Holik</keyname><forenames>Lukas</forenames></author><author><keyname>Lengal</keyname><forenames>Ondrej</forenames></author><author><keyname>Rogalewicz</keyname><forenames>Adam</forenames></author><author><keyname>Simacek</keyname><forenames>Jiri</forenames></author><author><keyname>Vojnar</keyname><forenames>Tomas</forenames></author></authors><title>Fully Automated Shape Analysis Based on Forest Automata</title><categories>cs.LO cs.FL</categories><comments>Accepted to CAV'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forest automata (FA) have recently been proposed as a tool for shape analysis
of complex heap structures. FA encode sets of tree decompositions of heap
graphs in the form of tuples of tree automata. In order to allow for
representing complex heap graphs, the notion of FA allowed one to provide
user-defined FA (called boxes) that encode repetitive graph patterns of shape
graphs to be used as alphabet symbols of other, higher-level FA. In this paper,
we propose a novel technique of automatically learning the FA to be used as
boxes that avoids the need of providing them manually. Further, we propose a
significant improvement of the automata abstraction used in the analysis. The
result is an efficient, fully-automated analysis that can handle even as
complex data structures as skip lists, with the performance comparable to
state-of-the-art fully-automated tools based on separation logic, which,
however, specialise in dealing with linked lists only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5808</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5808</id><created>2013-04-21</created><updated>2013-10-04</updated><authors><author><keyname>Huang</keyname><forenames>Shenwei</forenames></author></authors><title>Improved Complexity Results on $k$-Coloring $P_t$-Free Graphs</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is $H$-free if it does not contain an induced subgraph isomorphic to
$H$. We denote by $P_k$ and $C_k$ the path and the cycle on $k$ vertices,
respectively. In this paper, we prove that 4-COLORING is NP-complete for
$P_7$-free graphs, and that 5-COLORING is NP-complete for $P_6$-free graphs.
These two results improve all previous results on $k$-coloring $P_t$-free
graphs, and almost complete the classification of complexity of $k$-COLORING
$P_t$-free graphs for $k\ge 4$ and $t\ge 1$, leaving as the only missing case
4-COLORING $P_6$-free graphs. We expect that 4-COLORING is polynomial time
solvable for $P_6$-free graphs; in support of this, we describe a polynomial
time algorithm for 4-COLORING $P_6$-free graphs which are also $P$-free, where
$P$ is the graph obtained from $C_4$ by adding a new vertex and making it
adjacent to exactly one vertex on the $C_4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5809</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5809</id><created>2013-04-21</created><authors><author><keyname>Dickenstein</keyname><forenames>Alicia</forenames></author><author><keyname>Emiris</keyname><forenames>Ioannis</forenames></author><author><keyname>Karasoulou</keyname><forenames>Anna</forenames></author></authors><title>Plane mixed discriminants and toric jacobians</title><categories>math.AG cs.SC</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial algebra offers a standard approach to handle several problems in
geometric modeling. A key tool is the discriminant of a univariate polynomial,
or of a well-constrained system of polynomial equations, which expresses the
existence of a multiple root. We concentrate on bivariate polynomials and
establish an original formula that relates the mixed discriminant of two
bivariate Laurent polynomials with fixed support, with the sparse resultant of
these polynomials and their toric Jacobian. This allows us to obtain a new
proof for the bidegree of the mixed discriminant as well as to establish
multipicativity formulas arising when one polynomial can be factored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5810</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5810</id><created>2013-04-21</created><updated>2013-07-01</updated><authors><author><keyname>Arenas</keyname><forenames>Marcelo</forenames></author><author><keyname>Botoeva</keyname><forenames>Elena</forenames></author><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Ryzhikov</keyname><forenames>Vladislav</forenames></author></authors><title>Exchanging OWL 2 QL Knowledge Bases</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge base exchange is an important problem in the area of data exchange
and knowledge representation, where one is interested in exchanging information
between a source and a target knowledge base connected through a mapping. In
this paper, we study this fundamental problem for knowledge bases and mappings
expressed in OWL 2 QL, the profile of OWL 2 based on the description logic
DL-Lite_R. More specifically, we consider the problem of computing universal
solutions, identified as one of the most desirable translations to be
materialized, and the problem of computing UCQ-representations, which optimally
capture in a target TBox the information that can be extracted from a source
TBox and a mapping by means of unions of conjunctive queries. For the former we
provide a novel automata-theoretic technique, and complexity results that range
from NP to EXPTIME, while for the latter we show NLOGSPACE-completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5817</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5817</id><created>2013-04-21</created><authors><author><keyname>Li</keyname><forenames>Sheng</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Haardt</keyname><forenames>Martin</forenames></author></authors><title>Frequency-Domain Group-based Shrinkage Estimators for UWB Systems</title><categories>cs.IT math.IT</categories><comments>8 figures</comments><journal-ref>IEEE Transactions on Vehicular Technology, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose low-complexity adaptive biased estimation
algorithms, called group-based shrinkage estimators (GSEs), for parameter
estimation and interference suppression scenarios with mechanisms to
automatically adjust the shrinkage factors. The proposed estimation algorithms
divide the target parameter vector into a number of groups and adaptively
calculate one shrinkage factor for each group. GSE schemes improve the
performance of the conventional least squares (LS) estimator in terms of the
mean-squared error (MSE), while requiring a very modest increase in complexity.
An MSE analysis is presented which indicates the lower bounds of the GSE
schemes with different group sizes. We prove that our proposed schemes
outperform the biased estimation with only one shrinkage factor and the best
performance of GSE can be obtained with the maximum number of groups. Then, we
consider an application of the proposed algorithms to single-carrier
frequency-domain equalization (SC-FDE) of direct-sequence ultra-wideband
(DS-UWB) systems, in which the structured channel estimation (SCE) algorithm
and the frequency domain receiver employ the GSE. The simulation results show
that the proposed algorithms significantly outperform the conventional unbiased
estimator in the analyzed scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5821</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5821</id><created>2013-04-21</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Hjorungnes</keyname><forenames>Are</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>A Unified Approach to Joint and Iterative Adaptive Interference
  Cancellation and Parameter Estimation for CDMA Systems in Multipath Channels</title><categories>cs.IT math.IT</categories><comments>4 figures</comments><journal-ref>IEEE Communications Letters 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a unified approach to joint adaptive parameter estimation
and interference cancellation (IC) for direct sequence
code-division-multiple-access (DS-CDMA) systems in multipath channels. A
unified framework is presented in which the IC problem is formulated as an
optimization problem with extra degrees of freedom of an IC parameter vector
for each stage and user. We propose a joint optimization method for estimating
the IC parameter vector, the linear receiver filter front-end, and the channel
along with minimum mean squared error (MMSE) expressions for the estimators.
Based on the proposed joint optimization approach, we derive low-complexity
stochastic gradient (SG) algorithms for estimating the desired parameters.
Simulation results for the uplink of a synchronous DS-CDMA system show that the
proposed methods significantly outperform the best known IC receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5822</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5822</id><created>2013-04-21</created><authors><author><keyname>Ghosh</keyname><forenames>Arpita</forenames></author><author><keyname>Kale</keyname><forenames>Satyen</forenames></author><author><keyname>Lang</keyname><forenames>Kevin</forenames></author><author><keyname>Moseley</keyname><forenames>Benjamin</forenames></author></authors><title>Bargaining for Revenue Shares on Tree Trading Networks</title><categories>cs.GT cs.AI</categories><comments>An extended abstract of this paper appears in Proceedings of IJCAI
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study trade networks with a tree structure, where a seller with a single
indivisible good is connected to buyers, each with some value for the good, via
a unique path of intermediaries. Agents in the tree make multiplicative revenue
share offers to their parent nodes, who choose the best offer and offer part of
it to their parent, and so on; the winning path is determined by who finally
makes the highest offer to the seller. In this paper, we investigate how these
revenue shares might be set via a natural bargaining process between agents on
the tree, specifically, egalitarian bargaining between endpoints of each edge
in the tree. We investigate the fixed point of this system of bargaining
equations and prove various desirable for this solution concept, including (i)
existence, (ii) uniqueness, (iii) efficiency, (iv) membership in the core, (v)
strict monotonicity, (vi) polynomial-time computability to any given accuracy.
Finally, we present numerical evidence that asynchronous dynamics with randomly
ordered updates always converges to the fixed point, indicating that the fixed
point shares might arise from decentralized bargaining amongst agents on the
trade network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5823</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5823</id><created>2013-04-21</created><updated>2013-04-28</updated><authors><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author></authors><title>Towards a Formal Distributional Semantics: Simulating Logical Calculi
  with Tensors</title><categories>math.LO cs.CL cs.LO</categories><comments>10 pages, to appear in Proceedings of the Second Joint Conference on
  Lexical and Computational Semantics. June 2013</comments><msc-class>68T50, 03B10</msc-class><acm-class>F.4.1; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of compositional distributional models of semantics
reconciling the empirical aspects of distributional semantics with the
compositional aspects of formal semantics is a popular topic in the
contemporary literature. This paper seeks to bring this reconciliation one step
further by showing how the mathematical constructs commonly used in
compositional distributional models, such as tensors and matrices, can be used
to simulate different aspects of predicate logic.
  This paper discusses how the canonical isomorphism between tensors and
multilinear maps can be exploited to simulate a full-blown quantifier-free
predicate calculus using tensors. It provides tensor interpretations of the set
of logical connectives required to model propositional calculi. It suggests a
variant of these tensor calculi capable of modelling quantifiers, using few
non-linear operations. It finally discusses the relation between these
variants, and how this relation should constitute the subject of future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5827</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5827</id><created>2013-04-21</created><authors><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>Xie</keyname><forenames>Shengli</forenames></author><author><keyname>Yu</keyname><forenames>Rong</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>An Efficient MAC Protocol with Selective Grouping and Cooperative
  Sensing in Cognitive Radio Networks</title><categories>cs.ET cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio networks, spectrum sensing is a crucial technique to
discover spectrum opportunities for the Secondary Users (SUs). The quality of
spectrum sensing is evaluated by both sensing accuracy and sensing efficiency.
Here, sensing accuracy is represented by the false alarm probability and the
detection probability while sensing efficiency is represented by the sensing
overhead and network throughput. In this paper, we propose a group-based
cooperative Medium Access Control (MAC) protocol called GC-MAC, which addresses
the tradeoff between sensing accuracy and efficiency. In GC-MAC, the
cooperative SUs are grouped into several teams. During a sensing period, each
team senses a different channel while SUs in the same team perform the joint
detection on the targeted channel. The sensing process will not stop unless an
available channel is discovered. To reduce the sensing overhead, an
SU-selecting algorithm is presented to selectively choose the cooperative SUs
based on the channel dynamics and usage patterns. Then, an analytical model is
built to study the sensing accuracy-efficiency tradeoff under two types of
channel conditions: time-invariant channel and time-varying channel. An
optimization problem that maximizes achievable throughput is formulated to
optimize the important design parameters. Both saturation and non-saturation
situations are investigated with respect to throughput and sensing overhead.
Simulation results indicate that the proposed protocol is able to significantly
decrease sensing overhead and increase network throughput with guaranteed
sensing accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5846</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5846</id><created>2013-04-22</created><authors><author><keyname>Molla</keyname><forenames>St&#xe9;phane</forenames><affiliation>LATP</affiliation></author><author><keyname>Torr&#xe9;sani</keyname><forenames>Bruno</forenames><affiliation>LATP</affiliation></author></authors><title>A hybrid scheme for encoding audio signal using hidden Markov models of
  waveforms</title><categories>math.ST cs.IT math.IT stat.TH</categories><proxy>ccsd</proxy><journal-ref>Applied and Computational Harmonic Analysis 18 (2005) 137-166</journal-ref><doi>10.1016/j.acha.2004.11.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on recent results related to audiophonic signals encoding
using time-scale and time-frequency transform. More precisely, non-linear,
structured approximations for tonal and transient components using local cosine
and wavelet bases will be described, yielding expansions of audio signals in
the form tonal + transient + residual. We describe a general formulation
involving hidden Markov models, together with corresponding rate estimates.
Estimators for the balance transient/tonal are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5849</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5849</id><created>2013-04-22</created><authors><author><keyname>Blasiok</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Kaminski</keyname><forenames>Marcin</forenames></author></authors><title>Chain minors are FPT</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two finite posets P and Q, P is a chain minor of Q if there exists a
partial function f from the elements of Q to the elements of P such that for
every chain in P there is a chain C_Q in Q with the property that f restricted
to C_Q is an isomorphism of chains. We give an algorithm to decide whether a
poset P is a chain minor of o poset Q that runs in time O(|Q| log |Q|) for
every fixed poset P. This solves an open problem from the monograph by Downey
and Fellows [Parameterized Complexity, 1999] who asked whether the problem was
fixed parameter tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5850</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5850</id><created>2013-04-22</created><authors><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Collings</keyname><forenames>Iain B.</forenames></author></authors><title>Large System Analysis of Linear Precoding in MISO Broadcast Channels
  with Confidential Messages</title><categories>cs.IT math.IT</categories><comments>to appear IEEE JSAC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance of regularized channel inversion
(RCI) precoding in large MISO broadcast channels with confidential messages
(BCC). We obtain a deterministic approximation for the achievable secrecy
sum-rate which is almost surely exact as the number of transmit antennas $M$
and the number of users $K$ grow to infinity in a fixed ratio $\beta=K/M$. We
derive the optimal regularization parameter $\xi$ and the optimal network load
$\beta$ that maximize the per-antenna secrecy sum-rate. We then propose a
linear precoder based on RCI and power reduction (RCI-PR) that significantly
increases the high-SNR secrecy sum-rate for $1&lt;\beta&lt;2$. Our proposed precoder
achieves a per-user secrecy rate which has the same high-SNR scaling factor as
both the following upper bounds: (i) the rate of the optimum RCI precoder
without secrecy requirements, and (ii) the secrecy capacity of a single-user
system without interference. Furthermore, we obtain a deterministic
approximation for the secrecy sum-rate achievable by RCI precoding in the
presence of channel state information (CSI) error. We also analyze the
performance of our proposed RCI-PR precoder with CSI error, and we determine
how the error must scale with the SNR in order to maintain a given rate gap to
the case with perfect CSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5856</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5856</id><created>2013-04-22</created><authors><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Fundamental Limits of Distributed Caching in D2D Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 3 figures, submitted to ITW 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless Device-to-Device (D2D) network where communication is
restricted to be single-hop, users make arbitrary requests from a finite
library of possible files and user devices cache information in the form of
linear combinations of packets from the files in the library (coded caching).
We consider the combined effect of coding in the caching and delivery phases,
achieving &quot;coded multicast gain&quot;, and of spatial reuse due to local short-range
D2D communication. Somewhat counterintuitively, we show that the coded
multicast gain and the spatial reuse gain do not cumulate, in terms of the
throughput scaling laws. In particular, the spatial reuse gain shown in our
previous work on uncoded random caching and the coded multicast gain shown in
this paper yield the same scaling laws behavior, but no further scaling law
gain can be achieved by using both coded caching and D2D spatial reuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5862</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5862</id><created>2013-04-22</created><updated>2013-05-29</updated><authors><author><keyname>Briggs</keyname><forenames>Forrest</forenames></author><author><keyname>Fern</keyname><forenames>Xiaoli Z.</forenames></author><author><keyname>Irvine</keyname><forenames>Jed</forenames></author></authors><title>Multi-Label Classifier Chains for Bird Sound</title><categories>cs.LG cs.SD stat.ML</categories><comments>6 pages, 1 figure, submission to ICML 2013 workshop on bioacoustics.
  Note: this is a minor revision- the blind submission format has been replaced
  with one that shows author names, and a few corrections have been made</comments><msc-class>Computer Science</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bird sound data collected with unattended microphones for automatic surveys,
or mobile devices for citizen science, typically contain multiple
simultaneously vocalizing birds of different species. However, few works have
considered the multi-label structure in birdsong. We propose to use an ensemble
of classifier chains combined with a histogram-of-segments representation for
multi-label classification of birdsong. The proposed method is compared with
binary relevance and three multi-instance multi-label learning (MIML)
algorithms from prior work (which focus more on structure in the sound, and
less on structure in the label sets). Experiments are conducted on two
real-world birdsong datasets, and show that the proposed method usually
outperforms binary relevance (using the same features and base-classifier), and
is better in some cases and worse in others compared to the MIML algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5863</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5863</id><created>2013-04-22</created><updated>2013-06-23</updated><authors><author><keyname>Diochnos</keyname><forenames>Dimitrios I.</forenames></author></authors><title>Commonsense Reasoning and Large Network Analysis: A Computational Study
  of ConceptNet 4</title><categories>cs.AI cs.SI</categories><comments>152 pages, 99 tables, 23 figures (76 sub-figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report a computational study of ConceptNet 4 is performed using tools
from the field of network analysis. Part I describes the process of extracting
the data from the SQL database that is available online, as well as how the
closure of the input among the assertions in the English language is computed.
This part also performs a validation of the input as well as checks for the
consistency of the entire database. Part II investigates the structural
properties of ConceptNet 4. Different graphs are induced from the knowledge
base by fixing different parameters. The degrees and the degree distributions
are examined, the number and sizes of connected components, the transitivity
and clustering coefficient, the cores, information related to shortest paths in
the graphs, and cliques. Part III investigates non-overlapping, as well as
overlapping communities that are found in ConceptNet 4. Finally, Part IV
describes an investigation on rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5869</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5869</id><created>2013-04-22</created><updated>2013-05-12</updated><authors><author><keyname>Renny</keyname></author><author><keyname>Chandra</keyname><forenames>Reza</forenames></author><author><keyname>Ruhama</keyname><forenames>Syamsi</forenames></author><author><keyname>Sarjono</keyname><forenames>Mochammad Wisuda</forenames></author></authors><title>Exploring Tracer Study Service in Career Center Web Site of Indonesia
  Higher Education</title><categories>cs.CY</categories><journal-ref>Vol. 11 No. 3 March 2013 International Journal of Computer Science
  and Information Security</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality competence of worker the present do not meet labor market criteria
and the low level of labor productivity, the lack of communication between the
labor market with education, changing of socio-economic structure and global
political influence labor market, the development of science and technology
very rapidly lead to fundamental changes in terms of qualifications,
competencies and requirements for entering the workforce. Tracer Study results
can be used by universities to determine the success of the educational process
that has been done towards their students. Therefore, universities need a
technology services to support the optimization of the use of tracer study. One
of that is the use of a website to facilitate the conduct tracer study. Most
services tracer study provides information to college, like year graduated, got
a job waiting period, the first salary to work, first job, the relevance of the
curriculum to the work, and compliance with the major areas of work taken in
college. Tracer study feature in Career Center Website affect the popularity
website especially in traffic and rich file website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5870</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5870</id><created>2013-04-22</created><updated>2013-09-17</updated><authors><author><keyname>Chitnis</keyname><forenames>Rajesh</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author></authors><title>Parameterized Complexity of the Anchored k-Core Problem for Directed
  Graphs</title><categories>cs.DS</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bhawalkar, Kleinberg, Lewi, Roughgarden, and Sharma [ICALP 2012] introduced
the Anchored k-Core problem, where the task is for a given graph G and integers
b, k, and p to find an induced subgraph H with at least p vertices (the core)
such that all but at most b vertices (called anchors) of H are of degree at
least k. In this paper, we extend the notion of k-core to directed graphs and
provide a number of new algorithmic and complexity results for the directed
version of the problem. We show that
  - The decision version of the problem is NP-complete for every k&gt;=1 even if
the input graph is restricted to be a planar directed acyclic graph of maximum
degree at most k+2.
  - The problem is fixed parameter tractable (FPT) parameterized by the size of
the core p for k=1, and W[1]-hard for k&gt;=2.
  - When the maximum degree of the graph is at most \Delta, the problem is FPT
parameterized by p+\Delta if k&gt;= \Delta/2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5872</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5872</id><created>2013-04-22</created><updated>2013-10-09</updated><authors><author><keyname>Naor</keyname><forenames>Moni</forenames></author><author><keyname>Yogev</keyname><forenames>Eylon</forenames></author></authors><title>Sliding Bloom Filters</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Bloom filter is a method for reducing the space (memory) required for
representing a set by allowing a small error probability. In this paper we
consider a \emph{Sliding Bloom Filter}: a data structure that, given a stream
of elements, supports membership queries of the set of the last $n$ elements (a
sliding window), while allowing a small error probability. We formally define
the data structure and its relevant parameters and analyze the time and memory
requirements needed to achieve them. We give a low space construction that runs
in O(1) time per update with high probability (that is, for all sequences with
high probability all operations take constant time) and provide an almost
matching lower bound on the space that shows that our construction has the best
possible space consumption up to an additive lower order term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5876</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5876</id><created>2013-04-22</created><authors><author><keyname>Wang</keyname><forenames>Yang</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author><author><keyname>Glover</keyname><forenames>Fred</forenames></author><author><keyname>L&#xfc;</keyname><forenames>Zhipeng</forenames></author></authors><title>Solving the minimum sum coloring problem via binary quadratic
  programming</title><categories>cs.DS cs.DM</categories><comments>Short pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, binary quadratic programming (BQP) has been successively
applied to solve several combinatorial optimization problems. We consider in
this paper a study of using the BQP model to solve the minimum sum coloring
problem (MSCP). For this purpose, we recast the MSCP with a quadratic model
which is then solved via a recently proposed Path Relinking (PR) algorithm
designed for the general BQP. Based on a set of MSCP benchmark instances, we
investigate the performance of this solution approach compared with existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5878</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5878</id><created>2013-04-22</created><authors><author><keyname>Bader</keyname><forenames>Markus</forenames></author><author><keyname>Prankl</keyname><forenames>Johann</forenames></author><author><keyname>Vincze</keyname><forenames>Markus</forenames></author></authors><title>Visual Room-Awareness for Humanoid Robot Self-Localization</title><categories>cs.RO</categories><comments>Part of the OAGM/AAPR 2013 proceedings (1304.1876)</comments><report-no>OAGM-AAPR/2013/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humanoid robots without internal sensors such as a compass tend to lose their
orientation after a fall. Furthermore, re-initialisation is often ambiguous due
to symmetric man-made environments. The room-awareness module proposed here is
inspired by the results of psychological experiments and improves existing
self-localization strategies by mapping and matching the visual background with
colour histograms. The matching algorithm uses a particle-filter to generate
hypotheses of the viewing directions independent of the self-localization
algorithm and generates confidence values for various possible poses. The
robot's behaviour controller uses those confidence values to control
self-localization algorithm to converge to the most likely pose and prevents
the algorithm from getting stuck in local minima. Experiments with a symmetric
Standard Platform League RoboCup playing field with a simulated and a real
humanoid NAO robot show the significant improvement of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5880</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5880</id><created>2013-04-22</created><authors><author><keyname>Abchir</keyname><forenames>M. -A.</forenames><affiliation>CHART</affiliation></author><author><keyname>Truck</keyname><forenames>Isis</forenames><affiliation>CHART</affiliation></author><author><keyname>Pappa</keyname><forenames>Anna</forenames><affiliation>LIASD</affiliation></author></authors><title>Dealing with natural language interfaces in a geolocation context</title><categories>cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the geolocation field where high-level programs and low-level devices
coexist, it is often difficult to find a friendly user inter- face to configure
all the parameters. The challenge addressed in this paper is to propose
intuitive and simple, thus natural lan- guage interfaces to interact with
low-level devices. Such inter- faces contain natural language processing and
fuzzy represen- tations of words that facilitate the elicitation of
business-level objectives in our context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5892</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5892</id><created>2013-04-22</created><updated>2013-04-22</updated><authors><author><keyname>Kalinowski</keyname><forenames>Thomas</forenames></author><author><keyname>Nardoytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>A Social Welfare Optimal Sequential Allocation Procedure</title><categories>cs.AI cs.GT cs.MA</categories><comments>To appear in the Proceedings of IJCAI 2013, International Joint
  Conference on Artificial Intelligence</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a simple sequential allocation procedure for sharing indivisible
items between agents in which agents take turns to pick items. Supposing
additive utilities and independence between the agents, we show that the
expected utility of each agent is computable in polynomial time. Using this
result, we prove that the expected utilitarian social welfare is maximized when
agents take alternate turns. We also argue that this mechanism remains optimal
when agents behave strategically
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5893</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5893</id><created>2013-04-22</created><authors><author><keyname>Al-Fedaghi</keyname><forenames>Sabah</forenames></author></authors><title>Conceptual Understanding of Computer Program Execution: Application to
  C++</title><categories>cs.PL</categories><comments>11 pages, 18 figures</comments><journal-ref>International Journal of Computer Science Issues, Volume 10, Issue
  2, March 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A visual programming language uses pictorial tools such as diagrams to
represent its structural units and control stream. It is useful for enhancing
understanding, maintenance, verification, testing, and parallelism. This paper
proposes a diagrammatic methodology that produces a conceptual representation
of instructions for programming source codes. Without loss of generality in the
potential for using the methodology in a wider range of applications, this
paper focuses on using these diagrams in teaching of C++ programming. C++
programming constructs are represented in the proposed method in order to show
that it can provide a foundation for understanding the behavior of running
programs. Applying the method to actual C++ classes demonstrates that it
improves understanding of the activities in the computer system corresponding
to a C++ program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5894</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5894</id><created>2013-04-22</created><updated>2013-04-23</updated><authors><author><keyname>Cornelis</keyname><forenames>Bruno</forenames></author><author><keyname>Yang</keyname><forenames>Yun</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author><author><keyname>Dooms</keyname><forenames>Ann</forenames></author><author><keyname>Daubechies</keyname><forenames>Ingrid</forenames></author><author><keyname>Dunson</keyname><forenames>David</forenames></author></authors><title>Bayesian crack detection in ultra high resolution multimodal images of
  paintings</title><categories>cs.CV cs.LG</categories><comments>8 pages, double column</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The preservation of our cultural heritage is of paramount importance. Thanks
to recent developments in digital acquisition techniques, powerful image
analysis algorithms are developed which can be useful non-invasive tools to
assist in the restoration and preservation of art. In this paper we propose a
semi-supervised crack detection method that can be used for high-dimensional
acquisitions of paintings coming from different modalities. Our dataset
consists of a recently acquired collection of images of the Ghent Altarpiece
(1432), one of Northern Europe's most important art masterpieces. Our goal is
to build a classifier that is able to discern crack pixels from the background
consisting of non-crack pixels, making optimal use of the information that is
provided by each modality. To accomplish this we employ a recently developed
non-parametric Bayesian classifier, that uses tensor factorizations to
characterize any conditional probability. A prior is placed on the parameters
of the factorization such that every possible interaction between predictors is
allowed while still identifying a sparse subset among these predictors. The
proposed Bayesian classifier, which we will refer to as conditional Bayesian
tensor factorization or CBTF, is assessed by visually comparing classification
results with the Random Forest (RF) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5897</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5897</id><created>2013-04-22</created><authors><author><keyname>Abchir</keyname><forenames>Mohammed-Amine</forenames></author><author><keyname>Truck</keyname><forenames>Isis</forenames></author></authors><title>Towards an Extension of the 2-tuple Linguistic Model to Deal With
  Unbalanced Linguistic Term sets</title><categories>cs.AI</categories><journal-ref>Kybernetika, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the domain of Computing with words (CW), fuzzy linguistic approaches are
known to be relevant in many decision-making problems. Indeed, they allow us to
model the human reasoning in replacing words, assessments, preferences,
choices, wishes... by ad hoc variables, such as fuzzy sets or more
sophisticated variables.
  This paper focuses on a particular model: Herrera &amp; Martinez' 2-tuple
linguistic model and their approach to deal with unbalanced linguistic term
sets. It is interesting since the computations are accomplished without loss of
information while the results of the decision-making processes always refer to
the initial linguistic term set. They propose a fuzzy partition which
distributes data on the axis by using linguistic hierarchies to manage the
non-uniformity. However, the required input (especially the density around the
terms) taken by their fuzzy partition algorithm may be considered as too much
demanding in a real-world application, since density is not always easy to
determine. Moreover, in some limit cases (especially when two terms are very
closed semantically to each other), the partition doesn't comply with the data
themselves, it isn't close to the reality. Therefore we propose to modify the
required input, in order to offer a simpler and more faithful partition. We
have added an extension to the package jFuzzyLogic and to the corresponding
script language FCL. This extension supports both 2-tuple models: Herrera &amp;
Martinez' and ours. In addition to the partition algorithm, we present two
aggregation algorithms: the arithmetic means and the addition. We also discuss
these kinds of 2-tuple models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5910</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5910</id><created>2013-04-22</created><authors><author><keyname>Fournier</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Perifel</keyname><forenames>Sylvain</forenames></author><author><keyname>de Verclos</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>On fixed-polynomial size circuit lower bounds for uniform polynomials in
  the sense of Valiant</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assuming the Generalised Riemann Hypothesis (GRH), we show that for all k,
there exist polynomials with coefficients in $\MA$ having no arithmetic
circuits of size O(n^k) over the complex field (allowing any complex constant).
We also build a family of polynomials that can be evaluated in AM having no
arithmetic circuits of size O(n^k). Then we investigate the link between
fixed-polynomial size circuit bounds in the Boolean and arithmetic settings. In
characteristic zero, it is proved that $\NP \not\subset \size(n^k)$, or $\MA
\subset \size(n^k)$, or NP=MA imply lower bounds on the circuit size of uniform
polynomials in n variables from the class VNP over the complex field, assuming
GRH. In positive characteristic p, uniform polynomials in VNP have circuits of
fixed-polynomial size if and only if both VP=VNP over F_p and Mod_pP has
circuits of fixed-polynomial size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5923</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5923</id><created>2013-04-22</created><authors><author><keyname>Vabishchevich</keyname><forenames>P. N.</forenames></author><author><keyname>Vasil'ev</keyname><forenames>V. I.</forenames></author></authors><title>Numerical solving the identification problem for the lower coefficient
  of parabolic equation</title><categories>cs.NA math.NA</categories><comments>14 pages, 7 figures</comments><msc-class>65J22 65M32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the theory and practice of inverse problems for partial differential
equations (PDEs) much attention is paid to the problem of the identification of
coefficients from some additional information. This work deals with the problem
of determining in a multidimensional parabolic equation the lower coefficient
that depends on time only. To solve numerically a nonlinear inverse problem,
linearized approximations in time are constructed using standard finite element
procedures in space. The computational algorithm is based on a special
decomposition, where the transition to a new time level is implemented via
solving two standard elliptic problems. The numerical results presented here
for a model 2D problem demonstrate capabilities of the proposed computational
algorithms for approximate solving inverse problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5934</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5934</id><created>2013-04-22</created><authors><author><keyname>Caskurlu</keyname><forenames>Bugra</forenames></author><author><keyname>Subramani</keyname><forenames>K.</forenames></author></authors><title>On Partial Vertex Cover on Bipartite Graphs and Trees</title><categories>cs.CC cs.DS</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the Vertex Cover problem is in P on bipartite graphs,
however; the computational complexity of the Partial Vertex Cover problem on
bipartite graphs is open. In this paper, we first show that the Partial Vertex
Cover problem is NP-hard on bipartite graphs. We then identify an interesting
special case of bipartite graphs, for which the Partial Vertex Cover problem
can be solved in polynomial-time. We also show that the set of acyclic
bipartite graphs, i.e., forests, and the set of bipartite graph where the
degree of each vertex is at most 3 fall into that special case. Therefore, we
prove that the Partial Vertex Cover problem is in P on trees, and it is also in
P on the set of bipartite graphs where the degree of each vertex is at most 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5938</identifier>
 <datestamp>2013-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5938</id><created>2013-04-22</created><authors><author><keyname>Ortega</keyname><forenames>F&#xe1;bio Jos&#xe9; Muneratti</forenames></author><author><keyname>Ruggiero</keyname><forenames>Wilson Vicente</forenames></author></authors><title>Security Policies for WFMS with Rich Business Logic - A Model Suitable
  for Analysis</title><categories>cs.CR</categories><comments>9 pages, 4 figures</comments><acm-class>K.6.5; I.6.5; D.2.2</acm-class><journal-ref>International Journal of Computer Science and Information Security
  (IJCSIS) Vol. 11, No. 4, April 2013, pp. 1-9</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a formal metamodel for the specification of security
policies for workflows in online service systems designed to be suitable for
the modeling and analysis of complex business-related rules as well as
traditional access control. A translation of the model into a colored Petri net
is shown and an example of policy for an online banking system is described. By
writing predicates for querying the resulting state-space of the Petri net, a
connection between the formalized model and a higher-level description of the
security policy can be made, indicating the feasibility of the intended method
for validating such descriptions. Thanks to the independent nature among tasks
related to different business services, represented by restrictions in the
information flow within the metamodel, the state-space may be fractioned for
analysis, avoiding the state-space explosion problem. Related existing models
are discussed, pointing the gain in expressiveness of business rules and the
analysis of insecure state paths rather than simply insecure states in the
proposed model. The successful representation and analysis of the policy from
the example combined with reasonings for the general case attest the adequacy
of the proposed approach for its intended application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5940</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5940</id><created>2013-04-22</created><updated>2013-07-17</updated><authors><author><keyname>Shariati</keyname><forenames>Nafiseh</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Low-Complexity Channel Estimation in Large-Scale MIMO using Polynomial
  Expansion</title><categories>cs.IT math.IT</categories><comments>Published at IEEE International Symposium on Personal, Indoor and
  Mobile Radio Communications (PIMRC 2013), 8-11 September 2013, 6 pages, 4
  figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers pilot-based channel estimation in large-scale
multiple-input multiple-output (MIMO) communication systems, also known as
&quot;massive MIMO&quot;. Unlike previous works on this topic, which mainly considered
the impact of inter-cell disturbance due to pilot reuse (so-called pilot
contamination), we are concerned with the computational complexity. The
conventional minimum mean square error (MMSE) and minimum variance unbiased
(MVU) channel estimators rely on inverting covariance matrices, which has cubic
complexity in the multiplication of number of antennas at each side. Since this
is extremely expensive when there are hundreds of antennas, we propose to
approximate the inversion by an L-order matrix polynomial. A set of
low-complexity Bayesian channel estimators, coined Polynomial ExpAnsion CHannel
(PEACH) estimators, are introduced. The coefficients of the polynomials are
optimized to yield small mean square error (MSE). We show numerically that
near-optimal performance is achieved with low polynomial orders. In practice,
the order L can be selected to balance between complexity and MSE.
Interestingly, pilot contamination is beneficial to the PEACH estimators in the
sense that smaller L can be used to achieve near-optimal MSEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5942</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5942</id><created>2013-04-22</created><authors><author><keyname>Cela</keyname><forenames>Eranda</forenames></author><author><keyname>Stanek</keyname><forenames>Rostislav</forenames></author></authors><title>Heuristics for the data arrangement problem on regular trees</title><categories>math.OC cs.DS</categories><comments>34 pages, 16 figures, 4 tables</comments><msc-class>90C27</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data arrangement problem on regular trees (DAPT) consists in assigning
the vertices of a given graph G to the leaves of a d-regular tree T such that
the sum of the pairwise distances of all pairs of leaves in T which correspond
to edges of G is minimised. Luczak and Noble [6] have shown that this problem
is NP-hard for every fixed d larger than or equal to 2. In this paper we
propose construction and local search heuristics for the DAPT and introduce a
lower bound for this problem. The analysis of the performance of the heuristics
is based on two considerations: a) the quality of the solutions produced by the
heuristics as compared to the respective lower bounds b) for a special class of
instances with known optimal solution we evaluate the gap between the optimal
value of the objective function and the objective function value attained by
the heuristic solution, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5961</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5961</id><created>2013-04-22</created><authors><author><keyname>Pfandler</keyname><forenames>Andreas</forenames></author><author><keyname>R&#xfc;mmele</keyname><forenames>Stefan</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Backdoors to Abduction</title><categories>cs.AI cs.CC cs.LO</categories><comments>12 pages, a short version will appear in the proceedings of the 23rd
  International Joint Conference on Artificial Intelligence (IJCAI 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abductive reasoning (or Abduction, for short) is among the most fundamental
AI reasoning methods, with a broad range of applications, including fault
diagnosis, belief revision, and automated planning. Unfortunately, Abduction is
of high computational complexity; even propositional Abduction is
\Sigma_2^P-complete and thus harder than NP and coNP. This complexity barrier
rules out the existence of a polynomial transformation to propositional
satisfiability (SAT). In this work we use structural properties of the
Abduction instance to break this complexity barrier. We utilize the problem
structure in terms of small backdoor sets. We present fixed-parameter tractable
transformations from Abduction to SAT, which make the power of today's SAT
solvers available to Abduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5966</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5966</id><created>2013-04-22</created><authors><author><keyname>Korpar</keyname><forenames>Matija</forenames></author><author><keyname>Sikic</keyname><forenames>Mile</forenames></author></authors><title>SW# - GPU enabled exact alignments on genome scale</title><categories>cs.DC cs.CE q-bio.GN</categories><comments>3 pages, 1 figure, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sequence alignment is one of the oldest and the most famous problems in
bioinformatics. Even after 45 years, for one reason or another, this problem is
still actual; current solutions are trade-offs between execution time, memory
consumption and accuracy. We purpose SW#, a new CUDA GPU enabled and memory
efficient implementation of dynamic programming algorithms for local alignment.
In this implementation indels are treated using the affine gap model. Although
there are other GPU implementations of the Smith-Waterman algorithm, SW# is the
only publicly available implementation that can produce sequence alignments on
genome-wide scale. For long sequences, our implementation is at least a few
hundred times faster than a CPU version of the same algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5970</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5970</id><created>2013-04-22</created><authors><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Petit</keyname><forenames>Thierry</forenames></author><author><keyname>Siala</keyname><forenames>Mohamed</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Three Generalizations of the FOCUS Constraint</title><categories>cs.AI</categories><journal-ref>IJCAI 2013 proceedings</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FOCUS constraint expresses the notion that solutions are concentrated. In
practice, this constraint suffers from the rigidity of its semantics. To tackle
this issue, we propose three generalizations of the FOCUS constraint. We
provide for each one a complete filtering algorithm as well as discussing
decompositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5971</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5971</id><created>2013-04-22</created><authors><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Schweer</keyname><forenames>Nils</forenames></author><author><keyname>Reinhardt</keyname><forenames>Jan-Marc</forenames></author></authors><title>A Competitive Strategy for Distance-Aware Online Shape Allocation</title><categories>cs.DS cs.CG</categories><comments>15 pages, 9 figures, 3 tables; extended abstract version appears in
  WALCOM 2013, LNCS 7748, pp. 41-52</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following online allocation problem: Given a unit square S,
and a sequence of numbers n_i between 0 and 1, with partial sum bounded by 1;
at each step i, select a region C_i of previously unassigned area n_i in S. The
objective is to make these regions compact in a distance-aware sense: minimize
the maximum (normalized) average Manhattan distance between points from the
same set C_i. Related location problems have received a considerable amount of
attention; in particular, the problem of determining the &quot;optimal shape of a
city&quot;, i.e., allocating a single n_i has been studied. We present an online
strategy, based on an analysis of space-filling curves; for continuous shapes,
we prove a factor of 1.8092, and 1.7848 for discrete point sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5973</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5973</id><created>2013-04-22</created><updated>2013-06-22</updated><authors><author><keyname>Goldberg</keyname><forenames>Andrew V.</forenames></author><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author><author><keyname>Savchenko</keyname><forenames>Ruslan</forenames></author></authors><title>Separating Hierarchical and General Hub Labelings</title><categories>cs.DS math.CO</categories><comments>11 pages, minor corrections, MFCS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of distance oracles, a labeling algorithm computes vertex
labels during preprocessing. An $s,t$ query computes the corresponding distance
from the labels of $s$ and $t$ only, without looking at the input graph. Hub
labels is a class of labels that has been extensively studied. Performance of
the hub label query depends on the label size. Hierarchical labels are a
natural special kind of hub labels. These labels are related to other problems
and can be computed more efficiently. This brings up a natural question of the
quality of hierarchical labels. We show that there is a gap: optimal
hierarchical labels can be polynomially bigger than the general hub labels. To
prove this result, we give tight upper and lower bounds on the size of
hierarchical and general labels for hypercubes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5974</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5974</id><created>2013-04-22</created><authors><author><keyname>Xu</keyname><forenames>Kevin S.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Dynamic stochastic blockmodels: Statistical models for time-evolving
  networks</title><categories>cs.SI cs.LG physics.soc-ph stat.ME</categories><acm-class>G.3; G.2.2</acm-class><journal-ref>Proceedings of the 6th International Conference on Social
  Computing, Behavioral-Cultural Modeling, and Prediction (2013) 201-210</journal-ref><doi>10.1007/978-3-642-37210-0_22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Significant efforts have gone into the development of statistical models for
analyzing data in the form of networks, such as social networks. Most existing
work has focused on modeling static networks, which represent either a single
time snapshot or an aggregate view over time. There has been recent interest in
statistical modeling of dynamic networks, which are observed at multiple points
in time and offer a richer representation of many complex phenomena. In this
paper, we propose a state-space model for dynamic networks that extends the
well-known stochastic blockmodel for static networks to the dynamic setting. We
then propose a procedure to fit the model using a modification of the extended
Kalman filter augmented with a local search. We apply the procedure to analyze
a dynamic social network of email communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5991</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5991</id><created>2013-04-22</created><authors><author><keyname>Vidyarthi</keyname><forenames>Shalabh</forenames></author><author><keyname>Shukla</keyname><forenames>Kaushal K</forenames></author></authors><title>Approximation Algorithms for Vehicle Routing Problems with Stochastic
  Demands on Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the vehicle routing problem with stochastic demands (VRPSD) on
tree structured networks with a single depot. The problem we are concerned with
in this paper is to find a set of tours for the vehicle with minimum expected
length. Every tour begins at the depot, visits a subset of customers and
returns to the depot without violating the capacity constraint. Randomized
approximation algorithm achieving approximation guarantees of 2 for
split-delivery VRPSD, and 3 for un-split delivery VRPSD are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.5993</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.5993</id><created>2013-04-22</created><authors><author><keyname>Gaba</keyname><forenames>Siddharth</forenames></author><author><keyname>Sheridan</keyname><forenames>Patrick</forenames></author><author><keyname>Zhou</keyname><forenames>Jiantao</forenames></author><author><keyname>Choi</keyname><forenames>Shinhyun</forenames></author><author><keyname>Lu</keyname><forenames>Wei</forenames></author></authors><title>Stochastic Memristive Devices for Computing and Neuromorphic
  Applications</title><categories>cond-mat.other cs.ET</categories><comments>20 Pages, 5 Figures</comments><journal-ref>Nanoscale, 2013,5, 5872-5878</journal-ref><doi>10.1039/C3NR01176C</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanoscale resistive switching devices (memristive devices or memristors) have
been studied for a number of applications ranging from non-volatile memory,
logic to neuromorphic systems. However a major challenge is to address the
potentially large variations in space and in time in these nanoscale devices.
Here we show that in metal-filament based memristive devices the switching can
be fully stochastic. While individual switching events are random, the
distribution and probability of switching can be well predicted and controlled.
Rather than trying to force high switching probabilities using excessive
voltage or time, the inherent stochastic nature of resistive switching allows
these binary devices to be used as building blocks for novel error-tolerant
computing schemes such as stochastic computing and provide a needed &quot;analog&quot;
feature in neuromorphic applications. To verify such potential, we demonstrated
memristor-based stochastic bitstreams in both time and space domains, and show
that an array of binary memristors can act as a multi-level &quot;analog&quot; device for
neuromorphic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6000</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6000</id><created>2013-04-22</created><authors><author><keyname>Tan</keyname><forenames>Jin</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>Dai</keyname><forenames>Liyi</forenames></author></authors><title>Mixture Gaussian Signal Estimation with L_infty Error Metric</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating an input signal from noisy measurements
in both parallel scalar Gaussian channels and linear mixing systems. The
performance of the estimation process is quantified by the $\ell_\infty$ norm
error metric. We first study the minimum mean $\ell_\infty$ error estimator in
parallel scalar Gaussian channels, and verify that, when the input is
independent and identically distributed (i.i.d.) mixture Gaussian, the Wiener
filter is asymptotically optimal with probability 1. For linear mixing systems
with i.i.d. sparse Gaussian or mixture Gaussian inputs, under the assumption
that the relaxed belief propagation (BP) algorithm matches Tanaka's fixed point
equation, applying the Wiener filter to the output of relaxed BP is also
asymptotically optimal with probability 1. However, in order to solve the
practical problem where the signal dimension is finite, we apply an estimation
algorithm that has been proposed in our previous work, and illustrate that an
$\ell_\infty$ error minimizer can be approximated by an $\ell_p$ error
minimizer provided the value of $p$ is properly chosen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6007</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6007</id><created>2013-04-22</created><authors><author><keyname>Peserico</keyname><forenames>Enoch</forenames></author></authors><title>Paging with dynamic memory capacity</title><categories>cs.DS cs.OS cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a generalization of the classic paging problem that allows the
amount of available memory to vary over time - capturing a fundamental property
of many modern computing realities, from cloud computing to multi-core and
energy-optimized processors. It turns out that good performance in the
&quot;classic&quot; case provides no performance guarantees when memory capacity
fluctuates: roughly speaking, moving from static to dynamic capacity can mean
the difference between optimality within a factor 2 in space and time, and
suboptimality by an arbitrarily large factor. More precisely, adopting the
competitive analysis framework, we show that some online paging algorithms,
despite having an optimal (h,k)-competitive ratio when capacity remains
constant, are not (3,k)-competitive for any arbitrarily large k in the presence
of minimal capacity fluctuations. In this light it is surprising that several
classic paging algorithms perform remarkably well even if memory capacity
changes adversarially - even without taking those changes into explicit
account! In particular, we prove that LFD still achieves the minimum number of
faults, and that several classic online algorithms such as LRU have a &quot;dynamic&quot;
(h,k)-competitive ratio that is the best one can achieve without knowledge of
future page requests, even if one had perfect knowledge of future capacity
fluctuations (an exact characterization of this ratio shows it is almost,
albeit not quite, equal to the &quot;classic&quot; ratio k/(k-h+1)). In other words, with
careful management, knowing/predicting future memory resources appears far less
crucial to performance than knowing/predicting future data accesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6023</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6023</id><created>2013-04-22</created><updated>2013-09-27</updated><authors><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author></authors><title>Spaces, Trees and Colors: The Algorithmic Landscape of Document
  Retrieval on Sequences</title><categories>cs.IR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document retrieval is one of the best established information retrieval
activities since the sixties, pervading all search engines. Its aim is to
obtain, from a collection of text documents, those most relevant to a pattern
query. Current technology is mostly oriented to &quot;natural language&quot; text
collections, where inverted indices are the preferred solution. As successful
as this paradigm has been, it fails to properly handle some East Asian
languages and other scenarios where the &quot;natural language&quot; assumptions do not
hold. In this survey we cover the recent research in extending the document
retrieval techniques to a broader class of sequence collections, which has
applications bioinformatics, data and Web mining, chemoinformatics, software
engineering, multimedia information retrieval, and many others. We focus on the
algorithmic aspects of the techniques, uncovering a rich world of relations
between document retrieval challenges and fundamental problems on trees,
strings, range queries, discrete geometry, and others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6026</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6026</id><created>2013-04-22</created><updated>2013-09-28</updated><authors><author><keyname>El-Khatib</keyname><forenames>Rafah</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author><author><keyname>Urbanke</keyname><forenames>Ruediger</forenames></author></authors><title>Displacement Convexity, A Useful Framework for the Study of Spatially
  Coupled Codes</title><categories>cs.IT math.IT</categories><comments>Extension of paper submitted to ITW 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial coupling has recently emerged as a powerful paradigm to construct
graphical models that work well under low-complexity message-passing
algorithms. Although much progress has been made on the analysis of spatially
coupled models under message passing, there is still room for improvement, both
in terms of simplifying existing proofs as well as in terms of proving
additional properties.
  We introduce one further tool for the analysis, namely the concept of
displacement convexity. This concept plays a crucial role in the theory of
optimal transport and, quite remarkably, it is also well suited for the
analysis of spatially coupled systems. In cases where the concept applies,
displacement convexity allows functionals of distributions which are not convex
in the usual sense to be represented in an alternative form, so that they are
convex with respect to the new parametrization. As a proof of concept we
consider spatially coupled $(l,r)$-regular Gallager ensembles when transmission
takes place over the binary erasure channel. We show that the potential
function of the coupled system is displacement convex. Due to possible
translational degrees of freedom convexity by itself falls short of
establishing the uniqueness of the minimizing profile. For the spatially
coupled $(l,r)$-regular system strict displacement convexity holds when a
global translation degree of freedom is removed. Implications for the
uniqueness of the minimizer and for solutions of the density evolution equation
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6027</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6027</id><created>2013-04-22</created><updated>2013-04-24</updated><authors><author><keyname>Chan</keyname><forenames>Chun Lam</forenames></author><author><keyname>Cai</keyname><forenames>Sheng</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Near-Optimal Stochastic Threshold Group Testing</title><categories>cs.IT math.IT</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate and analyze a stochastic threshold group testing problem
motivated by biological applications. Here a set of $n$ items contains a subset
of $d \ll n$ defective items. Subsets (pools) of the $n$ items are tested --
the test outcomes are negative, positive, or stochastic (negative or positive
with certain probabilities that might depend on the number of defectives being
tested in the pool), depending on whether the number of defective items in the
pool being tested are fewer than the {\it lower threshold} $l$, greater than
the {\it upper threshold} $u$, or in between. The goal of a {\it stochastic
threshold group testing} scheme is to identify the set of $d$ defective items
via a &quot;small&quot; number of such tests. In the regime that $l = o(d)$ we present
schemes that are computationally feasible to design and implement, and require
near-optimal number of tests (significantly improving on existing schemes). Our
schemes are robust to a variety of models for probabilistic threshold group
testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6033</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6033</id><created>2013-04-22</created><authors><author><keyname>Vaiter</keyname><forenames>Samuel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Fadili</keyname><forenames>Jalal</forenames><affiliation>GREYC</affiliation></author></authors><title>Robust Polyhedral Regularization</title><categories>cs.IT math.IT</categories><comments>To be published in 10th international conference on Sampling Theory
  and Applications - Full papers</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish robustness to noise perturbations of polyhedral
regularization of linear inverse problems. We provide a sufficient condition
that ensures that the polyhedral face associated to the true vector is equal to
that of the recovered one. This criterion also implies that the $\ell^2$
recovery error is proportional to the noise level for a range of parameter. Our
criterion is expressed in terms of the hyperplanes supporting the faces of the
unit polyhedral ball of the regularization. This generalizes to an arbitrary
polyhedral regularization results that are known to hold for sparse synthesis
and analysis $\ell^1$ regularization which are encompassed in this framework.
As a byproduct, we obtain recovery guarantees for $\ell^\infty$ and
$\ell^1-\ell^\infty$ regularization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6038</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6038</id><created>2013-04-22</created><authors><author><keyname>Braibant</keyname><forenames>Thomas</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Jourdan</keyname><forenames>Jacques-Henri</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>VERIMAG - IMAG</affiliation></author></authors><title>Implementing hash-consed structures in Coq</title><categories>cs.PL cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on three different approaches to use hash-consing in programs
certified with the Coq system, using binary decision diagrams (BDD) as running
example. The use cases include execution inside Coq, or execution of the
extracted OCaml code. There are different trade-offs between faithful use of
pristine extracted code, and code that is fine-tuned to make use of OCaml
programming constructs not available in Coq. We discuss the possible
consequences in terms of performances and guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6039</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6039</id><created>2013-04-22</created><updated>2013-07-12</updated><authors><author><keyname>Faug&#xe8;re</keyname><forenames>Jean-Charles</forenames><affiliation>INRIA Paris-Rocquencourt, LIP6</affiliation></author><author><keyname>Gaudry</keyname><forenames>Pierrick</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Huot</keyname><forenames>Louise</forenames><affiliation>INRIA Paris-Rocquencourt, LIP6</affiliation></author><author><keyname>Renault</keyname><forenames>Gu&#xe9;na&#xeb;l</forenames><affiliation>INRIA Paris-Rocquencourt, LIP6</affiliation></author></authors><title>Polynomial Systems Solving by Fast Linear Algebra</title><categories>cs.SC</categories><comments>27 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial system solving is a classical problem in mathematics with a wide
range of applications. This makes its complexity a fundamental problem in
computer science. Depending on the context, solving has different meanings. In
order to stick to the most general case, we consider a representation of the
solutions from which one can easily recover the exact solutions or a certified
approximation of them. Under generic assumption, such a representation is given
by the lexicographical Gr\&quot;obner basis of the system and consists of a set of
univariate polynomials. The best known algorithm for computing the
lexicographical Gr\&quot;obner basis is in $\widetilde{O}(d^{3n})$ arithmetic
operations where $n$ is the number of variables and $d$ is the maximal degree
of the equations in the input system. The notation $\widetilde{O}$ means that
we neglect polynomial factors in $n$. We show that this complexity can be
decreased to $\widetilde{O}(d^{\omega n})$ where $2 \leq \omega &lt; 2.3727$ is
the exponent in the complexity of multiplying two dense matrices. Consequently,
when the input polynomial system is either generic or reaches the B\'ezout
bound, the complexity of solving a polynomial system is decreased from
$\widetilde{O}(D^3)$ to $\widetilde{O}(D^\omega)$ where $D$ is the number of
solutions of the system. To achieve this result we propose new algorithms which
rely on fast linear algebra. When the degree of the equations are bounded
uniformly by a constant we propose a deterministic algorithm. In the unbounded
case we present a Las Vegas algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6057</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6057</id><created>2013-04-22</created><authors><author><keyname>Lanctot</keyname><forenames>Marc</forenames></author><author><keyname>Saffidine</keyname><forenames>Abdallah</forenames></author><author><keyname>Veness</keyname><forenames>Joel</forenames></author><author><keyname>Archibald</keyname><forenames>Christopher</forenames></author><author><keyname>Winands</keyname><forenames>Mark H. M.</forenames></author></authors><title>Monte Carlo *-Minimax Search</title><categories>cs.GT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Monte Carlo *-Minimax Search (MCMS), a Monte Carlo
search algorithm for turned-based, stochastic, two-player, zero-sum games of
perfect information. The algorithm is designed for the class of of densely
stochastic games; that is, games where one would rarely expect to sample the
same successor state multiple times at any particular chance node. Our approach
combines sparse sampling techniques from MDP planning with classic pruning
techniques developed for adversarial expectimax planning. We compare and
contrast our algorithm to the traditional *-Minimax approaches, as well as MCTS
enhanced with the Double Progressive Widening, on four games: Pig, EinStein
W\&quot;urfelt Nicht!, Can't Stop, and Ra. Our results show that MCMS can be
competitive with enhanced MCTS variants in some domains, while consistently
outperforming the equivalent classic approaches given the same amount of
thinking time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6067</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6067</id><created>2013-04-22</created><authors><author><keyname>Teich</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Schr&#xf6;der-Preikschat</keyname><forenames>Wolfgang</forenames></author><author><keyname>Herkersdorf</keyname><forenames>Andreas</forenames></author></authors><title>Invasive Computing - Common Terms and Granularity of Invasion</title><categories>cs.OS</categories><report-no>DPA-13052</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future MPSoCs with 1000 or more processor cores on a chip require new means
for resource-aware programming in order to deal with increasing imperfections
such as process variation, fault rates, aging effects, and power as well as
thermal problems. On the other hand, predictable program executions are
threatened if not impossible if no proper means of resource isolation and
exclusive use may be established on demand. In view of these problems and
menaces, invasive computing enables an application programmer to claim for
processing resources and spread computations to claimed processors dynamically
at certain points of the program execution.
  Such decisions may be depending on the degree of application parallelism and
the state of the underlying resources such as utilization, load, and
temperature, but also with the goal to provide predictable program execution on
MPSoCs by claiming processing resources exclusively as the default and thus
eliminating interferences and creating the necessary isolation between multiple
concurrently running applications. For achieving this goal, invasive computing
introduces new programming constructs for resource-aware programming that
meanwhile, for testing purpose, have been embedded into the parallel computing
language X10 as developed by IBM using a library-based approach.
  This paper presents major ideas and common terms of invasive computing as
investigated by the DFG Transregional Collaborative Research Centre TR89.
Moreoever, a reflection is given on the granularity of resources that may be
requested by invasive programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6078</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6078</id><created>2013-04-19</created><authors><author><keyname>Letia</keyname><forenames>Ioan Alfred</forenames></author><author><keyname>Groza</keyname><forenames>Adrian</forenames></author></authors><title>Automating the Dispute Resolution in Task Dependency Network</title><categories>cs.AI</categories><comments>IAT 2005. arXiv admin note: substantial text overlap with
  arXiv:1304.5545</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When perturbation or unexpected events do occur, agents need protocols for
repairing or reforming the supply chain. Unfortunate contingency could increase
too much the cost of performance, while breaching the current contract may be
more efficient. In our framework the principles of contract law are applied to
set penalties: expectation damages, opportunity cost, reliance damages, and
party design remedies, and they are introduced in the task dependency model
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6099</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6099</id><created>2013-04-19</created><updated>2013-07-12</updated><authors><author><keyname>Kucerova</keyname><forenames>A.</forenames></author><author><keyname>Leps</keyname><forenames>M.</forenames></author></authors><title>Soft computing-based calibration of microplane M4 model parameters:
  Methodology and validation</title><categories>cs.CE</categories><comments>24 pages, 12 figures, 14 tables, submitted to Advances in Engineering
  Software, corrected and extended after the first review</comments><journal-ref>Advances in Engineering Software, 72, 226-235, 2014</journal-ref><doi>10.1016/j.advengsoft.2014.01.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constitutive models for concrete based on the microplane concept have
repeatedly proven their ability to well-reproduce its non-linear response on
material as well as structural scales. The major obstacle to a routine
application of this class of models is, however, the calibration of
microplane-related constants from macroscopic data. The goal of this paper is
two-fold: (i) to introduce the basic ingredients of a robust inverse procedure
for the determination of dominant parameters of the M4 model proposed by Bazant
and co-workers based on cascade Artificial Neural Networks trained by
Evolutionary Algorithm and (ii) to validate the proposed methodology against a
representative set of experimental data. The obtained results demonstrate that
the soft computing-based method is capable of delivering the searched response
with an accuracy comparable to the values obtained by expert users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6108</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6108</id><created>2013-04-22</created><authors><author><keyname>Charon</keyname><forenames>Nicolas</forenames></author><author><keyname>Trouv&#xe9;</keyname><forenames>Alain</forenames></author></authors><title>The varifold representation of non-oriented shapes for diffeomorphic
  registration</title><categories>cs.CG cs.CV math.DG</categories><comments>33 pages, 10 figures</comments><journal-ref>SIAM Journal on Imaging Sciences, 2013, Vol. 6, No. 4 : pp.
  2547-2580</journal-ref><doi>10.1137/130918885</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of orientation that naturally arises
when representing shapes like curves or surfaces as currents. In the field of
computational anatomy, the framework of currents has indeed proved very
efficient to model a wide variety of shapes. However, in such approaches,
orientation of shapes is a fundamental issue that can lead to several drawbacks
in treating certain kind of datasets. More specifically, problems occur with
structures like acute pikes because of canceling effects of currents or with
data that consists in many disconnected pieces like fiber bundles for which
currents require a consistent orientation of all pieces. As a promising
alternative to currents, varifolds, introduced in the context of geometric
measure theory by F. Almgren, allow the representation of any non-oriented
manifold (more generally any non-oriented rectifiable set). In particular, we
explain how varifolds can encode numerically non-oriented objects both from the
discrete and continuous point of view. We show various ways to build a Hilbert
space structure on the set of varifolds based on the theory of reproducing
kernels. We show that, unlike the currents' setting, these metrics are
consistent with shape volume (theorem 4.1) and we derive a formula for the
variation of metric with respect to the shape (theorem 4.2). Finally, we
propose a generalization to non-oriented shapes of registration algorithms in
the context of Large Deformations Metric Mapping (LDDMM), which we detail with
a few examples in the last part of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6116</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6116</id><created>2013-04-22</created><authors><author><keyname>Hart</keyname><forenames>Sergiu</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>The Menu-Size Complexity of Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the menu size of auctions as a measure of auction complexity and
study how it affects revenue. Our setting has a single revenue-maximizing
seller selling two or more heterogeneous items to a single buyer whose private
values for the items are drawn from a (possibly correlated) known distribution,
and whose valuation is additive over the items. We show that the revenue may
increase arbitrarily with menu size and that a bounded menu size can not ensure
any positive fraction of the optimal revenue. The menu size turns out to &quot;nail
down&quot; the revenue properties of deterministic auctions: their menu size may be
at most exponential in the number of items and indeed their revenue may be
larger than that achievable by the simplest types of auctions by a factor that
is exponential in the number of items but no larger. Our model is related to a
previously studied &quot;unit-demand&quot; model and our results also answer an open
problem in that model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6123</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6123</id><created>2013-04-22</created><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Two-Unicast Two-Hop Interference Network: Finite-Field Model</title><categories>cs.IT math.IT</categories><comments>Submitted to ITW 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel framework to convert the $K$-user multiple
access channel (MAC) over $\FF_{p^m}$ into the $K$-user MAC over ground field
$\FF_{p}$ with $m$ multiple inputs/outputs (MIMO). This framework makes it
possible to develop coding schemes for MIMO channel as done in symbol extension
for time-varying channel. Using aligned network diagonalization based on this
framework, we show that the sum-rate of $(2m-1)\log{p}$ is achievable for a
$2\times 2\times 2$ interference channel over $\FF_{p^m}$. We also provide some
relation between field extension and symbol extension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6128</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6128</id><created>2013-04-22</created><authors><author><keyname>Avci</keyname><forenames>Serhat Nazim</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Network Coding-Based Link Failure Recovery over Large Arbitrary Networks</title><categories>cs.NI</categories><comments>A copy is submitted to Globecom 2013 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding-based link failure recovery techniques provide near-hitless
recovery and offer high capacity efficiency. Diversity coding is the first
technique to incorporate coding in this field and is easy to implement over
small arbitrary networks. However, its capacity efficiency is restricted by its
systematic coding and high design complexity even though it has lower
complexity than the other coding-based recovery techniques. Alternative
techniques mitigate some of these limitations, but they are difficult to
implement over arbitrary networks. In this paper, we propose a novel
non-systematic coding technique and a simple design algorithm to implement the
diversity coding-based (or network coding-based) recovery over arbitrary
networks. The design framework consists of two parts. An ILP formulation for
each part is developed. The simulation results suggest that both the novel
coding structure and the novel design algorithm lead to higher capacity
efficiency for near-hitless recovery. The new design algorithm is able to
achieve optimal results in large arbitrary networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6133</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6133</id><created>2013-04-22</created><authors><author><keyname>Anantharam</keyname><forenames>Venkat</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Kamath</keyname><forenames>Sudeep</forenames></author><author><keyname>Nair</keyname><forenames>Chandra</forenames></author></authors><title>On Maximal Correlation, Hypercontractivity, and the Data Processing
  Inequality studied by Erkip and Cover</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a new geometric characterization of the
Hirschfeld-Gebelein-R\'{e}nyi maximal correlation of a pair of random $(X,Y)$,
as well as of the chordal slope of the nontrivial boundary of the
hypercontractivity ribbon of $(X,Y)$ at infinity. The new characterizations
lead to simple proofs for some of the known facts about these quantities. We
also provide a counterexample to a data processing inequality claimed by Erkip
and Cover, and find the correct tight constant for this kind of inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6146</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6146</id><created>2013-04-22</created><authors><author><keyname>Jain</keyname><forenames>Advait</forenames></author><author><keyname>Killpack</keyname><forenames>Marc D.</forenames></author><author><keyname>Edsinger</keyname><forenames>Aaron</forenames></author><author><keyname>Kemp</keyname><forenames>Charles C.</forenames></author></authors><title>Manipulation in Clutter with Whole-Arm Tactile Sensing</title><categories>cs.RO</categories><comments>This is the first version of a paper that we submitted to the
  International Journal of Robotics Research on December 31, 2011 and uploaded
  to our website on January 16, 2012</comments><journal-ref>The International Journal of Robotics Research April 2013 vol. 32
  no. 4 pg. 458-482</journal-ref><doi>10.1177/0278364912471865</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We begin this paper by presenting our approach to robot manipulation, which
emphasizes the benefits of making contact with the world across the entire
manipulator. We assume that low contact forces are benign, and focus on the
development of robots that can control their contact forces during
goal-directed motion. Inspired by biology, we assume that the robot has
low-stiffness actuation at its joints, and tactile sensing across the entire
surface of its manipulator. We then describe a novel controller that exploits
these assumptions. The controller only requires haptic sensing and does not
need an explicit model of the environment prior to contact. It also handles
multiple contacts across the surface of the manipulator. The controller uses
model predictive control (MPC) with a time horizon of length one, and a linear
quasi-static mechanical model that it constructs at each time step. We show
that this controller enables both real and simulated robots to reach goal
locations in high clutter with low contact forces. Our experiments include
tests using a real robot with a novel tactile sensor array on its forearm
reaching into simulated foliage and a cinder block. In our experiments, robots
made contact across their entire arms while pushing aside movable objects,
deforming compliant objects, and perceiving the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6152</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6152</id><created>2013-04-22</created><authors><author><keyname>Liu</keyname><forenames>Jingjing</forenames></author><author><keyname>Li</keyname><forenames>Peng</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Iterative Detection and Decoding for MIMO Systems with Knowledge-Aided
  Message Passing Algorithms</title><categories>cs.IT math.IT</categories><comments>3 figures. Asilomar 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of iterative detection and decoding
(IDD) for multi-antenna systems using low-density parity-check (LDPC) codes.
The proposed IDD system consists of a soft-input soft-output parallel
interference (PIC) cancellation scheme with linear minimum mean-square error
(MMSE) receive filters and two novel belief propagation (BP) decoding
algorithms. The proposed BP algorithms exploit the knowledge of short cycles in
the graph structure and the reweighting factors derived from the hypergraph's
expansion. Simulation results show that when used to perform IDD for
multi-antenna systems both proposed BP decoding algorithms can consistently
outperform existing BP techniques with a small number of decoding iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6154</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6154</id><created>2013-04-22</created><authors><author><keyname>Li</keyname><forenames>Peng</forenames></author><author><keyname>Liu</keyname><forenames>Jingjing</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Iterative Decision Feedback Detection Algorithms for Multi-User
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>5 figures, ICASSP 2012. arXiv admin note: text overlap with
  arXiv:1302.5958</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive iterative decision multi-feedback detection algorithm with
constellation constraints is proposed for multiuser multi-antenna systems. An
enhanced detection and interference cancellation is performed by introducing
multiple constellation points as decision candidates. A complexity reduction
strategy is developed to avoid redundant processing with reliable decisions
along with an adaptive recursive least squares algorithm for time-varying
channels. An iterative detection and decoding scheme is also considered with
the proposed detection algorithm. Simulations show that the proposed technique
has a complexity as low as the conventional decision feedback detector while it
obtains a performance close to the maximum likelihood detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6157</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6157</id><created>2013-04-22</created><authors><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Al-nahari</keyname><forenames>Azzam Y.</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Collings</keyname><forenames>Iain B.</forenames></author></authors><title>Linear Precoding for Broadcast Channels with Confidential Messages under
  Transmit-Side Channel Correlation</title><categories>cs.IT math.IT</categories><comments>to appear IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the performance of regularized channel inversion
(RCI) precoding in multiple-input single-output (MISO) broadcast channels with
confidential messages under transmit-side channel correlation. We derive a
deterministic equivalent for the achievable per-user secrecy rate which is
almost surely exact as the number of transmit antennas and the number of users
grow to infinity in a fixed ratio, and we determine the optimal regularization
parameter that maximizes the secrecy rate. Furthermore, we obtain deterministic
equivalents for the secrecy rates achievable by: (i) zero forcing precoding and
(ii) single user beamforming. The accuracy of our analysis is validated by
simulations of finite-size systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6159</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6159</id><created>2013-04-22</created><authors><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Collings</keyname><forenames>Iain B.</forenames></author></authors><title>Secrecy Sum-Rates with Regularized Channel Inversion Precoding under
  Imperfect CSI at the Transmitter</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP), May 2013. arXiv admin note: text overlap with
  arXiv:1304.5850</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance of regularized channel inversion
precoding in MISO broadcast channels with confidential messages under imperfect
channel state information at the transmitter (CSIT). We obtain an approximation
for the achievable secrecy sum-rate which is almost surely exact as the number
of transmit antennas and the number of users grow to infinity in a fixed ratio.
Simulations prove this anaylsis accurate even for finite-size systems. For FDD
systems, we determine how the CSIT error must scale with the SNR, and we derive
the number of feedback bits required to ensure a constant high-SNR rate gap to
the case with perfect CSIT. For TDD systems, we study the optimum amount of
channel training that maximizes the high-SNR secrecy sum-rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6161</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6161</id><created>2013-04-22</created><updated>2013-07-29</updated><authors><author><keyname>Sihn</keyname><forenames>Myong-Son</forenames></author><author><keyname>Kim</keyname><forenames>Ryul</forenames></author></authors><title>Separation Properties and Related Bounds of Collusion-secure
  Fingerprinting Codes</title><categories>cs.CR cs.IT math.IT</categories><comments>Withdrawn by authors because the proof of theorem 2.1 is not correct
  and most of results are coincided with those of arXiv:1208.2076</comments><report-no>KISU-MATH-2013-E-R-021</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the separation properties and related bounds of
some codes. We tried to obtain a new existence result for $(w_1,
w_2)$-separating codes and discuss the &quot;optimality&quot; of the upper bounds. Next
we tried to study some interesting relationship between separation and
existence of non-trivial subspace subcodes for Reed-Solomon codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6172</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6172</id><created>2013-04-23</created><updated>2013-12-20</updated><authors><author><keyname>Guo</keyname><forenames>Jing</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author></authors><title>Outage Probability in Arbitrarily-Shaped Finite Wireless Networks</title><categories>cs.IT math.IT</categories><comments>accepted to appear in IEEE Transactions on Communications</comments><journal-ref>IEEE Transactions on Communications, vol. 62, no. 2, pp. 699-712,
  Feb., 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the outage performance in finite wireless networks.
Unlike most prior works, which either assumed a specific network shape or
considered a special location of the reference receiver, we propose two general
frameworks for analytically computing the outage probability at any arbitrary
location of an arbitrarily-shaped finite wireless network: (i) a moment
generating function-based framework which is based on the numerical inversion
of the Laplace transform of a cumulative distribution and (ii) a reference link
power gain-based framework which exploits the distribution of the fading power
gain between the reference transmitter and receiver. The outage probability is
spatially averaged over both the fading distribution and the possible locations
of the interferers. The boundary effects are accurately accounted for using the
probability distribution function of the distance of a random node from the
reference receiver. For the case of the node locations modeled by a Binomial
point process and Nakagami-$m$ fading channel, we demonstrate the use of the
proposed frameworks to evaluate the outage probability at any location inside
either a disk or polygon region. The analysis illustrates the location
dependent performance in finite wireless networks and highlights the importance
of accurately modeling the boundary effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6174</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6174</id><created>2013-04-23</created><updated>2014-05-29</updated><authors><author><keyname>Mattei</keyname><forenames>Nicholas</forenames></author><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>How Hard Is It to Control an Election by Breaking Ties?</title><categories>cs.AI cs.DS cs.GT</categories><comments>Revised and expanded version including longer proofs and additional
  results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of controlling the result of an
election by breaking ties strategically. This problem is equivalent to the
problem of deciding the winner of an election under parallel universes
tie-breaking. When the chair of the election is only asked to break ties to
choose between one of the co-winners, the problem is trivially easy. However,
in multi-round elections, we prove that it can be NP-hard for the chair to
compute how to break ties to ensure a given result. Additionally, we show that
the form of the tie-breaking function can increase the opportunities for
control. Indeed, we prove that it can be NP-hard to control an election by
breaking ties even with a two-stage voting rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6176</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6176</id><created>2013-04-23</created><authors><author><keyname>Zhang</keyname><forenames>Yang</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author></authors><title>An Auction Mechanism for Resource Allocation in Mobile Cloud Computing
  Systems</title><categories>cs.DC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mobile cloud computing system is composed of heterogeneous services and
resources to be allocated by the cloud service provider to mobile cloud users.
On one hand, some of these resources are substitutable (e.g., users can use
storage from different places) that they have similar functions to the users.
On the other hand, some resources are complementary that the user will need
them as a bundle (e.g., users need both wireless connection and storage for
online photo posting). In this paper, we first model the resource allocation
process of a mobile cloud computing system as an auction mechanism with premium
and discount factors. The premium and discount factors indicate complementary
and substitutable relations among cloud resources provided by the service
provider. Then, we analyze the individual rationality and incentive
compatibility (truthfulness) properties of the users in the proposed auction
mechanism. The optimal solutions of the resource allocation and cost charging
schemes in the auction mechanism is discussed afterwards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6181</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6181</id><created>2013-04-23</created><authors><author><keyname>Geng</keyname><forenames>Guang-Gang</forenames></author><author><keyname>Jin</keyname><forenames>Xiao-Bo</forenames></author><author><keyname>Zhang</keyname><forenames>Xin-Chang</forenames></author><author><keyname>Zhang</keyname><forenames>De-Xian</forenames></author></authors><title>Evaluating Web Content Quality via Multi-scale Features</title><categories>cs.IR</categories><comments>4 pages, 1 figures, ecml/pkdd 2010 discovery challenge</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web content quality measurement is crucial to various web content processing
applications. This paper will explore multi-scale features which may affect the
quality of a host, and develop automatic statistical methods to evaluate the
Web content quality. The extracted properties include statistical content
features, page and host level link features and TFIDF features. The experiments
on ECML/PKDD 2010 Discovery Challenge data set show that the algorithm is
effective and feasible for the quality tasks of multiple languages, and the
multi-scale features have different identification ability and provide good
complement to each other for most tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6188</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6188</id><created>2013-04-23</created><updated>2015-12-21</updated><authors><author><keyname>Bansal</keyname><forenames>Nikhil</forenames></author><author><keyname>D&#xfc;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Thang</keyname><forenames>Nguyen Kim</forenames></author><author><keyname>V&#xe1;squez</keyname><forenames>&#xd3;scar C.</forenames></author></authors><title>The local-global conjecture for scheduling with non-linear cost</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical scheduling problem on a single machine, on which we
need to schedule sequentially $n$ given jobs. Every job $j$ has a processing
time $p_j$ and a priority weight $w_j$, and for a given schedule a completion
time $C_j$. In this paper we consider the problem of minimizing the objective
value $\sum_j w_j C_j^\beta$ for some fixed constant $\beta&gt;0$. This
non-linearity is motivated for example by the learning effect of a machine
improving its efficiency over time, or by the speed scaling model. For
$\beta=1$, the well-known Smith's rule that orders job in the non-increasing
order of $w_j/p_j$ give the optimum schedule. However, for $\beta \neq 1$, the
complexity status of this problem is open. Among other things, a key issue here
is that the ordering between a pair of jobs is not well-defined, and might
depend on where the jobs lie in the schedule and also on the jobs between them.
We investigate this question systematically and substantially generalize the
previously known results in this direction. These results lead to interesting
new dominance properties among schedules which lead to huge speed up in exact
algorithms for the problem. An experimental study evaluates the impact of these
properties on the exact algorithm A*.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6189</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6189</id><created>2013-04-23</created><updated>2013-10-01</updated><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Korhonen</keyname><forenames>Janne H.</forenames></author></authors><title>On the parameterized complexity of cutting a few vertices from a graph</title><categories>cs.DS</categories><journal-ref>38th International Symposium on Mathematical Foundations of
  Computer Science (MFCS 2013), pages 421-432</journal-ref><doi>10.1007/978-3-642-40313-2_38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the parameterized complexity of separating a small set of vertices
from a graph by a small vertex-separator. That is, given a graph $G$ and
integers $k$, $t$, the task is to find a vertex set $X$ with $|X| \le k$ and
$|N(X)| \le t$. We show that
  - the problem is fixed-parameter tractable (FPT) when parameterized by $t$
but W[1]-hard when parameterized by $k$, and
  - a terminal variant of the problem, where $X$ must contain a given vertex
$s$, is W[1]-hard when parameterized either by $k$ or by $t$ alone, but is FPT
when parameterized by $k + t$.
  We also show that if we consider edge cuts instead of vertex cuts, the
terminal variant is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6192</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6192</id><created>2013-04-23</created><authors><author><keyname>Anwar</keyname><forenames>Hafeez</forenames></author><author><keyname>Zambanini</keyname><forenames>Sebastian</forenames></author><author><keyname>Kampel</keyname><forenames>Martin</forenames></author></authors><title>A Bag of Visual Words Approach for Symbols-Based Coarse-Grained Ancient
  Coin Classification</title><categories>cs.CV</categories><comments>Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)</comments><report-no>OAGM AND AAPR/2013/17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of Numismatics provides the names and descriptions of the symbols
minted on the ancient coins. Classification of the ancient coins aims at
assigning a given coin to its issuer. Various issuers used various symbols for
their coins. We propose to use these symbols for a framework that will coarsely
classify the ancient coins. Bag of visual words (BoVWs) is a well established
visual recognition technique applied to various problems in computer vision
like object and scene recognition. Improvements have been made by incorporating
the spatial information to this technique. We apply the BoVWs technique to our
problem and use three symbols for coarse-grained classification. We use
rectangular tiling, log-polar tiling and circular tiling to incorporate spatial
information to BoVWs. Experimental results show that the circular tiling proves
superior to the rest of the methods for our problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6213</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6213</id><created>2013-04-23</created><authors><author><keyname>Perko</keyname><forenames>Roland</forenames></author><author><keyname>Schnabel</keyname><forenames>Thomas</forenames></author><author><keyname>Fritz</keyname><forenames>Gerald</forenames></author><author><keyname>Almer</keyname><forenames>Alexander</forenames></author><author><keyname>Paletta</keyname><forenames>Lucas</forenames></author></authors><title>Counting people from above: Airborne video based crowd analysis</title><categories>cs.CV</categories><comments>Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)</comments><report-no>OAGM-AAPR/2013/15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd monitoring and analysis in mass events are highly important
technologies to support the security of attending persons. Proposed methods
based on terrestrial or airborne image/video data often fail in achieving
sufficiently accurate results to guarantee a robust service. We present a novel
framework for estimating human count, density and motion from video data based
on custom tailored object detection techniques, a regression based density
estimate and a total variation based optical flow extraction. From the gathered
features we present a detailed accuracy analysis versus ground truth
measurements. In addition, all information is projected into world coordinates
to enable a direct integration with existing geo-information systems. The
resulting human counts demonstrate a mean error of 4% to 9% and thus represent
a most efficient measure that can be robustly applied in security critical
services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6232</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6232</id><created>2013-04-23</created><authors><author><keyname>Gilbert</keyname><forenames>Anna C.</forenames></author><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author><author><keyname>Strauss</keyname><forenames>Martin J.</forenames></author></authors><title>L2/L2-foreach sparse recovery with low risk</title><categories>cs.DS</categories><comments>1 figure, extended abstract to appear in ICALP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the &quot;foreach&quot; sparse recovery problem with failure
probability $p$. The goal of which is to design a distribution over $m \times
N$ matrices $\Phi$ and a decoding algorithm $\algo$ such that for every
$\vx\in\R^N$, we have the following error guarantee with probability at least
$1-p$ \[\|\vx-\algo(\Phi\vx)\|_2\le C\|\vx-\vx_k\|_2,\] where $C$ is a constant
(ideally arbitrarily close to 1) and $\vx_k$ is the best $k$-sparse
approximation of $\vx$.
  Much of the sparse recovery or compressive sensing literature has focused on
the case of either $p = 0$ or $p = \Omega(1)$. We initiate the study of this
problem for the entire range of failure probability. Our two main results are
as follows: \begin{enumerate} \item We prove a lower bound on $m$, the number
measurements, of $\Omega(k\log(n/k)+\log(1/p))$ for $2^{-\Theta(N)}\le p &lt;1$.
Cohen, Dahmen, and DeVore \cite{CDD2007:NearOptimall2l2} prove that this bound
is tight. \item We prove nearly matching upper bounds for \textit{sub-linear}
time decoding. Previous such results addressed only $p = \Omega(1)$.
\end{enumerate}
  Our results and techniques lead to the following corollaries: (i) the first
ever sub-linear time decoding $\lolo$ &quot;forall&quot; sparse recovery system that
requires a $\log^{\gamma}{N}$ extra factor (for some $\gamma&lt;1$) over the
optimal $O(k\log(N/k))$ number of measurements, and (ii) extensions of Gilbert
et al. \cite{GHRSW12:SimpleSignals} results for information-theoretically
bounded adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6237</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6237</id><created>2013-04-23</created><authors><author><keyname>Zachariah</keyname><forenames>Dave</forenames></author><author><keyname>De Angelis</keyname><forenames>Alessio</forenames></author><author><keyname>Dwivedi</keyname><forenames>Satyam</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>Self-Localization of Asynchronous Wireless Nodes With Parameter
  Uncertainties</title><categories>math.ST cs.IT cs.NI math.IT stat.TH</categories><journal-ref>IEEE Signal Processing Letters, June 2013, Vol. 20, No. 6, pages
  551-554</journal-ref><doi>10.1109/LSP.2013.2255592</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a wireless network localization scenario in which the need for
synchronized nodes is avoided. It consists of a set of fixed anchor nodes
transmitting according to a given sequence and a self-localizing receiver node.
The setup can accommodate additional nodes with unknown positions participating
in the sequence. We propose a localization method which is robust with respect
to uncertainty of the anchor positions and other system parameters. Further, we
investigate the Cram\'er-Rao bound for the considered problem and show through
numerical simulations that the proposed method attains the bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6241</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6241</id><created>2013-04-23</created><updated>2013-07-02</updated><authors><author><keyname>Kim</keyname><forenames>Chol-Un</forenames></author><author><keyname>An</keyname><forenames>Dok-Jun</forenames></author><author><keyname>Han</keyname><forenames>Song</forenames></author></authors><title>A Security Protocol for the Identification and Data Encrypt Key
  Management of Secure Mobile Devices</title><categories>cs.CR cs.IT math.IT</categories><comments>7 pages, 1 figure, in version 2 added a secure cryptographic key
  management protocol based on the secure user authentication scheme in version
  1 and references, changed the title; version 3 developed abstract and
  conclusions, accepted in JTPC</comments><report-no>KISU-MATH-2012-E-R-016</report-no><msc-class>94A62 (primary), 68M12 (secondary)</msc-class><journal-ref>Journal of Theoretical Physics and Cryptography, Vol.3, July 2013,
  pp21-24</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we proposed an identification and data encrypt key manage
protocol that can be used in some security system based on such secure devices
as secure USB memories or RFIDs, which are widely used for identifying persons
or other objects recently. In general, the default functions of the security
system using a mobile device are the authentication for the owner of the device
and secure storage of data stored on the device. We proposed a security model
that consists of the server and mobile devices in order to realize these
security features. In this model we defined the secure communication protocol
for the authentication and management of data encryption keys using a private
key encryption algorithm with the public key between the server and mobile
devices. In addition, we was performed the analysis for the attack to the
communication protocol between the mobile device and server. Using the
communication protocol, the system will attempt to authenticate the mobile
device. The data decrypt key is transmitted only if the authentication process
is successful. The data in the mobile device can be decrypted using the key.
Our analysis proved that this Protocol ensures anonymity, prevents replay
attacks and realizes the interactive identification between the security
devices and the authentication server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6245</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6245</id><created>2013-04-23</created><authors><author><keyname>Wu</keyname><forenames>Chia-Lung</forenames></author><author><keyname>Cheny</keyname><forenames>Po-Ning</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Shieh</keyname><forenames>Shin-Lin</forenames></author></authors><title>A Two-Phase Maximum-Likelihood Sequence Estimation for Receivers with
  Partial CSI</title><categories>cs.IT math.IT</categories><comments>5 pages and 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimality of the conventional maximum likelihood sequence estimation
(MLSE), also known as the Viterbi Algorithm (VA), relies on the assumption that
the receiver has perfect knowledge of the channel coefficients or channel state
information (CSI). However, in practical situations that fail the assumption,
the MLSE method becomes suboptimal and then exhaustive checking is the only way
to obtain the ML sequence. At this background, considering directly the ML
criterion for partial CSI, we propose a two-phase low-complexity MLSE
algorithm, in which the first phase performs the conventional MLSE algorithm in
order to retain necessary information for the backward VA performed in the
second phase. Simulations show that when the training sequence is moderately
long in comparison with the entire data block such as 1/3 of the block, the
proposed two-phase MLSE can approach the performance of the optimal exhaustive
checking. In a normal case, where the training sequence consumes only 0.14 of
the bandwidth, our proposed method still outperforms evidently the conventional
MLSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6255</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6255</id><created>2013-04-23</created><authors><author><keyname>Brandst&#xe4;dt</keyname><forenames>Andreas</forenames></author><author><keyname>Milanic</keyname><forenames>Martin</forenames></author><author><keyname>Nevries</keyname><forenames>Ragnar</forenames></author></authors><title>New Polynomial Cases of the Weighted Efficient Domination Problem</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a finite undirected graph. A vertex dominates itself and all its
neighbors in G. A vertex set D is an efficient dominating set (e.d. for short)
of G if every vertex of G is dominated by exactly one vertex of D. The
Efficient Domination (ED) problem, which asks for the existence of an e.d. in
G, is known to be NP-complete even for very restricted graph classes.
  In particular, the ED problem remains NP-complete for 2P3-free graphs and
thus for P7-free graphs. We show that the weighted version of the problem
(abbreviated WED) is solvable in polynomial time on various subclasses of
2P3-free and P7-free graphs, including (P2+P4)-free graphs, P5-free graphs and
other classes.
  Furthermore, we show that a minimum weight e.d. consisting only of vertices
of degree at most 2 (if one exists) can be found in polynomial time. This
contrasts with our NP-completeness result for the ED problem on planar
bipartite graphs with maximum degree 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6257</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6257</id><created>2013-04-23</created><updated>2014-08-13</updated><authors><author><keyname>Bliss</keyname><forenames>Catherine A.</forenames></author><author><keyname>Frank</keyname><forenames>Morgan R.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>An Evolutionary Algorithm Approach to Link Prediction in Dynamic Social
  Networks</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 12 figures, 4 tables, Submitted to the Journal of
  Computational Science</comments><journal-ref>Bliss, C. A., Frank, M. R., Danforth, C. M. &amp; P. S. Dodds. (2014).
  An Evolutionary Algorithm Approach to Link Prediction in Dynamic Social
  Networks. Journal of Computational Science, 5(5):750-764</journal-ref><doi>10.1016/j.jocs.2014.01.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world, complex phenomena have underlying structures of evolving
networks where nodes and links are added and removed over time. A central
scientific challenge is the description and explanation of network dynamics,
with a key test being the prediction of short and long term changes. For the
problem of short-term link prediction, existing methods attempt to determine
neighborhood metrics that correlate with the appearance of a link in the next
observation period. Recent work has suggested that the incorporation of
topological features and node attributes can improve link prediction. We
provide an approach to predicting future links by applying the Covariance
Matrix Adaptation Evolution Strategy (CMA-ES) to optimize weights which are
used in a linear combination of sixteen neighborhood and node similarity
indices. We examine a large dynamic social network with over $10^6$ nodes
(Twitter reciprocal reply networks), both as a test of our general method and
as a problem of scientific interest in itself. Our method exhibits fast
convergence and high levels of precision for the top twenty predicted links.
Based on our findings, we suggest possible factors which may be driving the
evolution of Twitter reciprocal reply networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6263</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6263</id><created>2013-04-23</created><authors><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Hou</keyname><forenames>Jianfeng</forenames></author><author><keyname>Liu</keyname><forenames>Guizhen</forenames></author></authors><title>On total colorings of 1-planar graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is 1-planar if it can be drawn on the plane so that each edge is
crossed by at most one other edge. In this paper, we confirm the total-coloring
conjecture for 1-planar graphs with maximum degree at least 13.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6266</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6266</id><created>2013-04-23</created><authors><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>List total coloring of pseudo-outerplanar graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is pseudo-outerplanar if each of its blocks has an embedding in the
plane so that the vertices lie on a fixed circle and the edges lie inside the
disk of this circle with each of them crossing at most one another. It is
proved that every pseudo-outerplanar graph with maximum degree \Delta\geq 5 is
totally (\Delta+1)-choosable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6274</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6274</id><created>2013-04-23</created><updated>2013-07-29</updated><authors><author><keyname>Padhye</keyname><forenames>Rohan</forenames></author><author><keyname>Khedker</keyname><forenames>Uday P.</forenames></author></authors><title>Interprocedural Data Flow Analysis in Soot using Value Contexts</title><categories>cs.PL</categories><comments>SOAP 2013 Final Version</comments><acm-class>F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interprocedural analysis is precise if it is flow sensitive and fully
context-sensitive even in the presence of recursion. Many methods of
interprocedural analysis sacrifice precision for scalability while some are
precise but limited to only a certain class of problems.
  Soot currently supports interprocedural analysis of Java programs using graph
reachability. However, this approach is restricted to IFDS/IDE problems, and is
not suitable for general data flow frameworks such as heap reference analysis
and points-to analysis which have non-distributive flow functions.
  We describe a general-purpose interprocedural analysis framework for Soot
using data flow values for context-sensitivity. This framework is not
restricted to problems with distributive flow functions, although the lattice
must be finite. It combines the key ideas of the tabulation method of the
functional approach and the technique of value-based termination of call string
construction.
  The efficiency and precision of interprocedural analyses is heavily affected
by the precision of the underlying call graph. This is especially important for
object-oriented languages like Java where virtual method invocations cause an
explosion of spurious call edges if the call graph is constructed naively. We
have instantiated our framework with a flow and context-sensitive points-to
analysis in Soot, which enables the construction of call graphs that are far
more precise than those constructed by Soot's SPARK engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6276</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6276</id><created>2013-04-23</created><authors><author><keyname>Ramezanian</keyname><forenames>Mohammad Ardeshir Rasoul</forenames></author></authors><title>Epistemic Learning Programs A Calculus for Describing Epistemic Action
  Models</title><categories>cs.LO</categories><comments>35 pages</comments><msc-class>68T27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Epistemic Logic makes it possible to model and reason about
information change in multi-agent systems. Information change is mathematically
modeled through epistemic action Kripke models introduced by Baltag et al.
Also, van Ditmarsch interprets the information change as a relation between
epistemic states and sets of epistemic states and to describe it formally, he
considers a special constructor LB called learning operator. Inspired by this,
it seems natural to us that the basic source of information change in a
multi-agent system should be learning an announcement by some agents together,
privately, concurrently or even wrongly. Hence moving along this path, we
introduce the notion of a learning program and prove that all fi?nite K45
action models can be described by our learning programs
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6279</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6279</id><created>2013-04-23</created><updated>2013-09-17</updated><authors><author><keyname>Farooque</keyname><forenames>Mahfuza</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Graham-Lengrand</keyname><forenames>St&#xe9;phane</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author></authors><title>Sequent Calculi with procedure calls</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce two focussed sequent calculi, LKp(T) and LK+(T),
that are based on Miller-Liang's LKF system for polarised classical logic. The
novelty is that those sequent calculi integrate the possibility to call a
decision procedure for some background theory T, and the possibility to
polarise literals &quot;on the fly&quot; during proof-search. These features are used in
our other works to simulate the DPLL(T) procedure as proof-search in the
extension of LKp(T) with a cut-rule. In this report we therefore prove
cut-elimination in LKp(T). Contrary to what happens in the empty theory, the
polarity of literals affects the provability of formulae in presence of a
theory T. On the other hand, changing the polarities of connectives does not
change the provability of formulae, only the shape of proofs. In order to prove
this, we introduce a second sequent calculus, LK+(T) that extends LKp(T) with a
relaxed focussing discipline, but we then show an encoding of LK+(T) back into
the more restrictive system LK(T). We then prove completeness of LKp(T) (and
therefore of LK+(T)) with respect to first-order reasoning modulo the ground
propositional lemmas of the background theory T .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6280</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6280</id><created>2013-04-23</created><authors><author><keyname>Lev&#xe9;</keyname><forenames>Florence</forenames><affiliation>MIS</affiliation></author><author><keyname>Richomme</keyname><forenames>Gw&#xe9;na&#xeb;l</forenames><affiliation>LIRMM, UM3</affiliation></author></authors><title>On Quasiperiodic Morphisms</title><categories>cs.DM math.CO</categories><comments>12 pages</comments><proxy>ccsd</proxy><journal-ref>9th International Conference, WORDS 2013, Turku : Finlande (2013)</journal-ref><doi>10.1007/978-3-642-40579-2_20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weakly and strongly quasiperiodic morphisms are tools introduced to study
quasiperiodic words. Formally they map respectively at least one or any
non-quasiperiodic word to a quasiperiodic word. Considering them both on finite
and infinite words, we get four families of morphisms between which we study
relations. We provide algorithms to decide whether a morphism is strongly
quasiperiodic on finite words or on infinite words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6281</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6281</id><created>2013-04-23</created><updated>2014-03-17</updated><authors><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Subspace Recovery from Structured Union of Subspaces</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lower dimensional signal representation schemes frequently assume that the
signal of interest lies in a single vector space. In the context of the
recently developed theory of compressive sensing (CS), it is often assumed that
the signal of interest is sparse in an orthonormal basis. However, in many
practical applications, this requirement may be too restrictive. A
generalization of the standard sparsity assumption is that the signal lies in a
union of subspaces. Recovery of such signals from a small number of samples has
been studied recently in several works. Here, we consider the problem of
subspace recovery in which our goal is to identify the subspace (from the
union) in which the signal lies using a small number of samples, in the
presence of noise. More specifically, we derive performance bounds and
conditions under which reliable subspace recovery is guaranteed using maximum
likelihood (ML) estimation. We begin by treating general unions and then obtain
the results for the special case in which the subspaces have structure leading
to block sparsity. In our analysis, we treat both general sampling operators
and random sampling matrices. With general unions, we show that under certain
conditions, the number of measurements required for reliable subspace recovery
in the presence of noise via ML is less than that implied using the restricted
isometry property which guarantees signal recovery. In the special case of
block sparse signals, we quantify the gain achievable over standard sparsity in
subspace recovery. Our results also strengthen existing results on sparse
support recovery in the presence of noise under the standard sparsity model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6284</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6284</id><created>2013-04-23</created><updated>2013-05-27</updated><authors><author><keyname>Grabmayer</keyname><forenames>Clemens</forenames></author><author><keyname>Rochel</keyname><forenames>Jan</forenames></author></authors><title>Expressibility in the Lambda Calculus with mu</title><categories>cs.PL</categories><comments>24 pages, 7 figures, Extended version of an article in the
  proceedings of the 24th International Conference on Rewriting Techniques and
  Applications, Einhoven, the Netherlands, June 24-26, 2013</comments><acm-class>F.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a problem connected to the unfolding semantics of functional
programming languages: give a useful characterization of those infinite
lambda-terms that are lambda_{letrec}-expressible in the sense that they arise
as infinite unfoldings of terms in lambda_{letrec}, the lambda-calculus with
letrec. We provide two characterizations, using concepts we introduce for
infinite lambda-terms: regularity, strong regularity, and binding-capturing
chains. It turns out that lambda_{letrec}-expressible infinite lambda-terms
form a proper subclass of the regular infinite lambda-terms. In this paper we
establish these characterizations only for expressibility in lambda_{mu}, the
lambda-calculus with explicit mu-recursion. We show that for all infinite
lambda-terms T the following are equivalent: (i): T is lambda_{mu}-expressible;
(ii): T is strongly regular; (iii): T is regular, and it only has finite
binding-capturing chains.
  We define regularity and strong regularity for infinite lambda-terms as two
different generalizations of regularity for infinite first-order terms: as the
existence of only finitely many subterms that are defined as the reducts of two
rewrite relations for decomposing lambda-terms. These rewrite relations act on
infinite lambda-terms furnished with a marked prefix of abstractions for
collecting decomposed lambda-abstractions and keeping the terms closed under
decomposition. They differ in how vacuous abstractions in the prefix are
removed.
  This report accompanies the article with the same title for the proceedings
of the conference RTA 2013, and mainly differs from that by providing the proof
of the characterization of lambda_{mu}-expressibility with binding-capturing
chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6291</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6291</id><created>2013-04-23</created><authors><author><keyname>Wang</keyname><forenames>Fang</forenames></author><author><keyname>Li</keyname><forenames>Yi</forenames></author></authors><title>Learning Visual Symbols for Parsing Human Poses in Images</title><categories>cs.CV</categories><comments>IJCAI 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parsing human poses in images is fundamental in extracting critical visual
information for artificial intelligent agents. Our goal is to learn
self-contained body part representations from images, which we call visual
symbols, and their symbol-wise geometric contexts in this parsing process. Each
symbol is individually learned by categorizing visual features leveraged by
geometric information. In the categorization, we use Latent Support Vector
Machine followed by an efficient cross validation procedure to learn visual
symbols. Then, these symbols naturally define geometric contexts of body parts
in a fine granularity. When the structure of the compositional parts is a tree,
we derive an efficient approach to estimating human poses in images.
Experiments on two large datasets suggest our approach outperforms state of the
art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6296</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6296</id><created>2013-04-23</created><authors><author><keyname>Bos</keyname><forenames>Arie</forenames></author></authors><title>Hilbert curves in 2 dimensions generated by L-systems</title><categories>cs.CG</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating Hilbert curves in Z^2 using L-systems appears to be efficient and
easy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6297</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6297</id><created>2013-04-23</created><updated>2013-10-25</updated><authors><author><keyname>Belkhir</keyname><forenames>Walid</forenames></author><author><keyname>Chevalier</keyname><forenames>Yannick</forenames></author><author><keyname>Rusinowitch</keyname><forenames>Michael</forenames></author></authors><title>Guarded Variable Automata over Infinite Alphabets</title><categories>cs.FL</categories><comments>29 pages. arXiv admin note: text overlap with arXiv:1302.4205</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define guarded variable automata (GVAs), a simple extension of finite
automata over infinite alphabets. In this model the transitions are labelled by
letters or variables ranging over an infinite alphabet and guarded by
conjunction of equalities and disequalities. GVAs are well-suited for modeling
component-based applications such as web services. They are closed under
intersection, union, concatenation and Kleene operator, and their nonemptiness
problem is PSPACE-complete. We show that the simulation preorder of GVAs is
decidable. Our proof relies on the characterization of the simulation by means
of games and strategies. This result can be applied to service composition
synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6301</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6301</id><created>2013-04-23</created><authors><author><keyname>Demri</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Dhar</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Sangnier</keyname><forenames>Arnaud</forenames></author></authors><title>On the Complexity of Verifying Regular Properties on Flat Counter
  Systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the approximation methods for the verification of counter systems, one
of them consists in model-checking their flat unfoldings. Unfortunately, the
complexity characterization of model-checking problems for such operational
models is not always well studied except for reachability queries or for Past
LTL. In this paper, we characterize the complexity of model-checking problems
on flat counter systems for the specification languages including first-order
logic, linear mu-calculus, infinite automata, and related formalisms. Our
results span different complexity classes (mainly from PTime to PSpace) and
they apply to languages in which arithmetical constraints on counter values are
systematically allowed. As far as the proof techniques are concerned, we
provide a uniform approach that focuses on the main issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6316</identifier>
 <datestamp>2013-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6316</id><created>2013-04-23</created><updated>2013-09-05</updated><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>About Strongly Universal Cellular Automata</title><categories>cs.DM nlin.CG</categories><comments>In Proceedings MCU 2013, arXiv:1309.1043</comments><proxy>Selena Clancy</proxy><msc-class>68R05</msc-class><acm-class>F.2.2</acm-class><journal-ref>EPTCS 128, 2013, pp. 93-125</journal-ref><doi>10.4204/EPTCS.128.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct a strongly universal cellular automaton on the
line with 11 states and the standard neighbourhood. We embed this construction
into several tilings of the hyperbolic plane and of the hyperbolic 3D space
giving rise to strongly universal cellular automata with 10 states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6321</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6321</id><created>2013-04-23</created><authors><author><keyname>Bodlaender</keyname><forenames>Hans</forenames></author><author><keyname>Drange</keyname><forenames>P&#xe5;l G.</forenames></author><author><keyname>Dregi</keyname><forenames>Markus S.</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>A O(c^k n) 5-Approximation Algorithm for Treewidth</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm that for an input n-vertex graph G and integer k&gt;0, in
time 2^[O(k)]n either outputs that the treewidth of G is larger than k, or
gives a tree decomposition of G of width at most 5k+4. This is the first
algorithm providing a constant factor approximation for treewidth which runs in
time single-exponential in k and linear in n. Treewidth based computations are
subroutines of numerous algorithms. Our algorithm can be used to speed up many
such algorithms to work in time which is single-exponential in the treewidth
and linear in the input size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6333</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6333</id><created>2013-04-23</created><authors><author><keyname>Grochow</keyname><forenames>Joshua A.</forenames></author></authors><title>Unifying and generalizing known lower bounds via geometric complexity
  theory</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that most arithmetic circuit lower bounds and relations between lower
bounds naturally fit into the representation-theoretic framework suggested by
geometric complexity theory (GCT), including: the partial derivatives technique
(Nisan-Wigderson), the results of Razborov and Smolensky on $AC^0[p]$,
multilinear formula and circuit size lower bounds (Raz et al.), the degree
bound (Strassen, Baur-Strassen), the connected components technique (Ben-Or),
depth 3 arithmetic circuit lower bounds over finite fields
(Grigoriev-Karpinski), lower bounds on permanent versus determinant
(Mignon-Ressayre, Landsberg-Manivel-Ressayre), lower bounds on matrix
multiplication (B\&quot;{u}rgisser-Ikenmeyer) (these last two were already known to
fit into GCT), the chasms at depth 3 and 4 (Gupta-Kayal-Kamath-Saptharishi;
Agrawal-Vinay; Koiran), matrix rigidity (Valiant) and others. That is, the
original proofs, with what is often just a little extra work, already provide
representation-theoretic obstructions in the sense of GCT for their respective
lower bounds. This enables us to expose a new viewpoint on GCT, whereby it is a
natural unification and broad generalization of known results. It also shows
that the framework of GCT is at least as powerful as known methods, and gives
many new proofs-of-concept that GCT can indeed provide significant asymptotic
lower bounds. This new viewpoint also opens up the possibility of fruitful
two-way interactions between previous results and the new methods of GCT; we
provide several concrete suggestions of such interactions. For example, the
representation-theoretic viewpoint of GCT naturally provides new properties to
consider in the search for new lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6358</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6358</id><created>2013-04-23</created><authors><author><keyname>Bar-Noy</keyname><forenames>Amotz</forenames></author><author><keyname>Rawitz</keyname><forenames>Dror</forenames></author><author><keyname>Terlecky</keyname><forenames>Peter</forenames></author></authors><title>Maximizing Barrier Coverage Lifetime with Mobile Sensors</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor networks are ubiquitously used for detection and tracking and as a
result covering is one of the main tasks of such networks. We study the problem
of maximizing the coverage lifetime of a barrier by mobile sensors with limited
battery powers, where the coverage lifetime is the time until there is a
breakdown in coverage due to the death of a sensor. Sensors are first deployed
and then coverage commences. Energy is consumed in proportion to the distance
traveled for mobility, while for coverage, energy is consumed in direct
proportion to the radius of the sensor raised to a constant exponent. We study
two variants which are distinguished by whether the sensing radii are given as
part of the input or can be optimized, the fixed radii problem and the variable
radii problem. We design parametric search algorithms for both problems for the
case where the final order of the sensors is predetermined and for the case
where sensors are initially located at barrier endpoints. In contrast, we show
that the variable radii problem is strongly NP-hard and provide hardness of
approximation results for fixed radii for the case where all the sensors are
initially co-located at an internal point of the barrier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6360</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6360</id><created>2013-04-23</created><authors><author><keyname>Senge</keyname><forenames>Sebastian</forenames></author></authors><title>Assessment of Path Reservation in Distributed Real-Time Vehicle Guidance</title><categories>cs.MA</categories><comments>Accepted for IEEE Intelligent Vehicles 2013 Workshop on &quot;Environment
  Perception and Navigation for Intelligent Vehicles&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we assess the impact of path reservation as an additional
feature in our distributed real-time vehicle guidance protocol BeeJamA. Through
our microscopic simulations we show that na\&quot;{\i}ve reservation of links
without any further measurements is only an improvement in case of complete
market penetration, otherwise it even reduces the performance of our approach
based on real-time link loads. Moreover, we modified the reservation process to
incorporate current travel times and show that this improves the results in our
simulations when at least 40% market penetration is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6363</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6363</id><created>2013-04-22</created><authors><author><keyname>Cooper</keyname><forenames>S. Barry</forenames></author></authors><title>Incomputability after Alan Turing</title><categories>math.HO cs.CC cs.LO</categories><journal-ref>Notices of the American Mathematical Society, Volume 59, Number 6,
  2012, pp. 776-784</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incomputability as a mathematical notion arose from work of Alan Turing and
Alonzo Church in the 1930s. Like Turing himself, it attracted less attention
than it deserved beyond the confines of mathematics. Today our experiences in
computer science, physics, biology, artificial intelligence, economics and the
humanities point to the importance of the notion for understanding the world
around us. This article takes a Turing centenary look at how the interface
between the computable world and the incomputable formed a central theme in
Turing's work - from the early establishment of the standard model of the
stored program computer, to the late work on emergence of form in nature, and
the first approaches to understanding and simulating human intelligence.
Turing's thinking was remarkably prescient, and his legacy still impacts on
much of our work. The incomputable may turn out to be a specially important
part of the legacy, with consequences that Alan Turing himself could not have
envisaged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6379</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6379</id><created>2013-04-23</created><authors><author><keyname>Jassim</keyname><forenames>Firas A.</forenames></author></authors><title>Semi-Optimal Edge Detector based on Simple Standard Deviation with
  Adjusted Thresholding</title><categories>cs.CV</categories><comments>6 pages, 1 table, 6 figures</comments><journal-ref>International Journal of Computer Applications,Volume 68, No.2,
  April 2013</journal-ref><doi>10.5120/11555-6834</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper proposes a novel method which combines both median filter and
simple standard deviation to accomplish an excellent edge detector for image
processing. First of all, a denoising process must be applied on the grey scale
image using median filter to identify pixels which are likely to be
contaminated by noise. The benefit of this step is to smooth the image and get
rid of the noisy pixels. After that, the simple statistical standard deviation
could be computed for each 2X2 window size. If the value of the standard
deviation inside the 2X2 window size is greater than a predefined threshold,
then the upper left pixel in the 2?2 window represents an edge. The visual
differences between the proposed edge detector and the standard known edge
detectors have been shown to support the contribution in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6383</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6383</id><created>2013-04-23</created><updated>2014-01-25</updated><authors><author><keyname>Panagiotakopoulos</keyname><forenames>Constantinos</forenames></author><author><keyname>Tsampouka</keyname><forenames>Petroula</forenames></author></authors><title>The Stochastic Gradient Descent for the Primal L1-SVM Optimization
  Revisited</title><categories>cs.LG cs.AI</categories><comments>In v2 the numerical results are obtained using the latest release 1.7
  of Cygwin and the g++ compiler version 4.5.3. We also consider in the
  experiments the algorithms SvmSgd and SGD-QN. A slightly shorter version of
  this paper appeared in ECML/PKDD 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reconsider the stochastic (sub)gradient approach to the unconstrained
primal L1-SVM optimization. We observe that if the learning rate is inversely
proportional to the number of steps, i.e., the number of times any training
pattern is presented to the algorithm, the update rule may be transformed into
the one of the classical perceptron with margin in which the margin threshold
increases linearly with the number of steps. Moreover, if we cycle repeatedly
through the possibly randomly permuted training set the dual variables defined
naturally via the expansion of the weight vector as a linear combination of the
patterns on which margin errors were made are shown to obey at the end of each
complete cycle automatically the box constraints arising in dual optimization.
This renders the dual Lagrangian a running lower bound on the primal objective
tending to it at the optimum and makes available an upper bound on the relative
accuracy achieved which provides a meaningful stopping criterion. In addition,
we propose a mechanism of presenting the same pattern repeatedly to the
algorithm which maintains the above properties. Finally, we give experimental
evidence that algorithms constructed along these lines exhibit a considerably
improved performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6388</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6388</id><created>2013-04-23</created><authors><author><keyname>Esik</keyname><forenames>Zoltan</forenames></author><author><keyname>Ivan</keyname><forenames>Szabolcs</forenames></author></authors><title>Operational characterization of scattered MCFLs -- Technical Report</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a Kleene-type operational characterization of Muller context-free
languages (MCFLs) of well-ordered and scattered words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6393</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6393</id><created>2013-04-23</created><authors><author><keyname>Chehreghani</keyname><forenames>Mostafa Haghir</forenames></author></authors><title>Efficient Algorithms for Approximate Triangle Counting</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting the number of triangles in a graph has many important applications
in network analysis. Several frequently computed metrics like the clustering
coefficient and the transitivity ratio need to count the number of triangles in
the network. Furthermore, triangles are one of the most important graph classes
considered in network mining. In this paper, we present a new randomized
algorithm for approximate triangle counting. The algorithm can be adopted with
different sampling methods and give effective triangle counting methods. In
particular, we present two sampling methods, called the \textit{$q$-optimal
sampling} and the \textit{edge sampling}, which respectively give $O(sm)$ and
$O(sn)$ time algorithms with nice error bounds ($m$ and $n$ are respectively
the number of edges and vertices in the graph and $s$ is the number of
samples). Among others, we show, for example, that if an upper bound
$\widetilde{\Delta^e}$ is known for the number of triangles incident to every
edge, the proposed method provides an $1\pm \epsilon$ approximation which runs
in $O( \frac{\widetilde{\Delta^e} n \log n}{\widehat{\Delta^e} \epsilon^2} )$
time, where $\widehat{\Delta^e}$ is the average number of triangles incident to
an edge. Finally we show that the algorithm can be adopted with streams. Then
it, for example, will perform 2 passes over the data (if the size of the graph
is known, otherwise it needs 3 passes) and will use $O(sn)$ space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6420</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6420</id><created>2013-04-23</created><authors><author><keyname>Chitnis</keyname><forenames>Rajesh</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author></authors><title>Preventing Unraveling in Social Networks Gets Harder</title><categories>cs.SI cs.DM cs.DS</categories><comments>To appear in AAAI 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The behavior of users in social networks is often observed to be affected by
the actions of their friends. Bhawalkar et al. \cite{bhawalkar-icalp}
introduced a formal mathematical model for user engagement in social networks
where each individual derives a benefit proportional to the number of its
friends which are engaged. Given a threshold degree $k$ the equilibrium for
this model is a maximal subgraph whose minimum degree is $\geq k$. However the
dropping out of individuals with degrees less than $k$ might lead to a
cascading effect of iterated withdrawals such that the size of equilibrium
subgraph becomes very small. To overcome this some special vertices called
&quot;anchors&quot; are introduced: these vertices need not have large degree. Bhawalkar
et al. \cite{bhawalkar-icalp} considered the \textsc{Anchored $k$-Core}
problem: Given a graph $G$ and integers $b, k$ and $p$ do there exist a set of
vertices $B\subseteq H\subseteq V(G)$ such that $|B|\leq b, |H|\geq p$ and
every vertex $v\in H\setminus B$ has degree at least $k$ is the induced
subgraph $G[H]$. They showed that the problem is NP-hard for $k\geq 2$ and gave
some inapproximability and fixed-parameter intractability results. In this
paper we give improved hardness results for this problem. In particular we show
that the \textsc{Anchored $k$-Core} problem is W[1]-hard parameterized by $p$,
even for $k=3$. This improves the result of Bhawalkar et al.
\cite{bhawalkar-icalp} (who show W[2]-hardness parameterized by $b$) as our
parameter is always bigger since $p\geq b$. Then we answer a question of
Bhawalkar et al. \cite{bhawalkar-icalp} by showing that the \textsc{Anchored
$k$-Core} problem remains NP-hard on planar graphs for all $k\geq 3$, even if
the maximum degree of the graph is $k+2$. Finally we show that the problem is
FPT on planar graphs parameterized by $b$ for all $k\geq 7$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6442</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6442</id><created>2013-04-23</created><authors><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Kharlamov</keyname><forenames>Evgeny</forenames></author><author><keyname>Montali</keyname><forenames>Marco</forenames></author><author><keyname>Santoso</keyname><forenames>Ario</forenames></author><author><keyname>Zheleznyakov</keyname><forenames>Dmitriy</forenames></author></authors><title>Verification of Inconsistency-Aware Knowledge and Action Bases (Extended
  Version)</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Description Logic Knowledge and Action Bases (KABs) have been recently
introduced as a mechanism that provides a semantically rich representation of
the information on the domain of interest in terms of a DL KB and a set of
actions to change such information over time, possibly introducing new objects.
In this setting, decidability of verification of sophisticated temporal
properties over KABs, expressed in a variant of first-order mu-calculus, has
been shown. However, the established framework treats inconsistency in a
simplistic way, by rejecting inconsistent states produced through action
execution. We address this problem by showing how inconsistency handling based
on the notion of repairs can be integrated into KABs, resorting to
inconsistency-tolerant semantics. In this setting, we establish decidability
and complexity of verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6450</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6450</id><created>2013-04-23</created><authors><author><keyname>Hon</keyname><forenames>Wing-Kai</forenames></author><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Liu</keyname><forenames>Hsiang Hsuan</forenames></author><author><keyname>Poon</keyname><forenames>Sheung-Hung</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>On independence domination</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a graph. The independence-domination number is the maximum over all
independent sets I in G of the minimal number of vertices needed to dominate I.
In this paper we investigate the computational complexity of independence
domination for graphs in several graph classes related to cographs. We present
an exact exponential algorithm. We also present a PTAS for planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6459</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6459</id><created>2013-04-23</created><updated>2015-10-26</updated><authors><author><keyname>Wang</keyname><forenames>Cheng</forenames></author><author><keyname>Zhang</keyname><forenames>Zhen-Zhen</forenames></author><author><keyname>Zhou</keyname><forenames>Jieren</forenames></author></authors><title>Bounding Transport Complexity for Content Dissemination in Online Social
  Networks</title><categories>cs.SI cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the issue on measuring the transport difficulty for
content dissemination in online social networks (OSNs). We define a new metric,
called \emph{transport load}, to measure the load imposed by the OSN on the
carrier communication network. It involves two key factors: data arrival
process at users and transport distance of messages. Furthermore, we define the
fundamental limit of transport load as the \emph{transport complexity}, i.e.,
the \emph{minimum required} transport load for an OSN over a given carrier
communication network. To model the content dissemination in OSNs, we formulate
the geographical distribution feature of dissemination sessions in OSNs by
introducing a four-layered system model. It consists of physical layer, social
layer, content layer, and session layer. We dig the mutual relevances among
these four layers as the preconditions for bounding the transport complexity.
Specifically, taking into account the scale-free property of users' degree
centrality in OSNs, we introduce the Zipf's distribution to describe the
friendship degree of users. Furthermore, in order to model the interest-driven
session patterns in real-world OSNs, we define a new session pattern, called
Social-InterestCast. Combining with the users' behaviour in real-life OSNs and
the experimental results of our evaluation, we make a reasonable assumption
that: For the session with a source node, say $v$, the number of destinations
chosen from all friends of $v$ follows a Zipf's distribution. Based on our
system model, we derive the lower bound on transport complexity for the content
dissemination in a large-scale OSN under this common probability distribution
in the social sciences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6460</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6460</id><created>2013-04-23</created><authors><author><keyname>Lariviere</keyname><forenames>Vincent</forenames></author><author><keyname>Lozano</keyname><forenames>George A.</forenames></author><author><keyname>Gingras</keyname><forenames>Yves</forenames></author></authors><title>Are elite journals declining?</title><categories>cs.DL</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work indicates that over the past 20 years, the highest quality work
have been published in an increasingly diverse and larger group of journals. In
this paper we examine whether this diversification has also affected the
handful of elite journals that are traditionally considered to be the best. We
examine citation patterns over the past 40 years of 7 long-standing
traditionally elite journals and 6 journals that have been increasing in
importance over the past 20 years. To be among the top 5% or 1% cited papers,
papers now need about twice as many citations as they did 40 years ago. Since
the late 1980s and early 1990s elite journals have been publishing a decreasing
proportion of these top cited papers. This also applies to the two journals
that are typically considered as the top venues and often used as bibliometric
indicators of &quot;excellence&quot;, Science and Nature. On the other hand, several new
and established journals are publishing an increasing proportion of most cited
papers. These changes bring new challenges and opportunities for all parties.
Journals can enact policies to increase or maintain their relative position in
the journal hierarchy. Researchers now have the option to publish in more
diverse venues knowing that their work can still reach the same audiences.
Finally, evaluators and administrators need to know that although there will
always be a certain prestige associated with publishing in &quot;elite&quot; journals,
journal hierarchies are in constant flux so inclusion of journals into this
group is not permanent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6468</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6468</id><created>2013-04-23</created><authors><author><keyname>Zu</keyname><forenames>Keke</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Switched Lattice Reduction-Aided Linear Detection Techniques
  for MIMO Systems</title><categories>cs.IT math.IT</categories><comments>6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice reduction (LR) aided multiple-input-multiple-out (MIMO) linear
detection can achieve the maximum receive diversity of the maximum likelihood
detection (MLD). By emloying the most commonly used Lenstra, Lenstra, and L.
Lovasz (LLL) algorithm, an equivalent channel matrix which is shorter and
nearly orthogonal is obtained. And thus the noise enhancement is greatly
reduced by employing the LR-aided detection. One problem is that the LLL
algorithm can not guarantee to find the optimal basis. The optimal lattice
basis can be found by the Korkin and Zolotarev (KZ) reduction. However, the KZ
reduction is infeasible in practice due to its high complexity. In this paper,
a simple algorithm is proposed based on the complex LLL (CLLL) algorithm to
approach the optimal performance while maintaining a reasonable complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6470</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6470</id><created>2013-04-23</created><authors><author><keyname>Zu</keyname><forenames>Keke</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Haardt</keyname><forenames>Martin</forenames></author></authors><title>Low-Complexity Lattice Reduction-Aided Channel Inversion Methods for
  Large-Dimensional Multi-User MIMO Systems</title><categories>cs.IT math.IT</categories><comments>3 figures</comments><journal-ref>Asilomar 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-complexity precoding {algorithms} are proposed in this work to reduce the
computational complexity and improve the performance of regularized block
diagonalization (RBD) {based} precoding {schemes} for large multi-user {MIMO}
(MU-MIMO) systems. The proposed algorithms are based on a channel inversion
technique, QR decompositions{,} and lattice reductions to decouple the MU-MIMO
channel into equivalent SU-MIMO channels. Simulation results show that the
proposed precoding algorithms can achieve almost the same sum-rate performance
as RBD precoding, substantial bit error rate (BER) performance gains{,} and a
simplified receiver structure, while requiring a lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6473</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6473</id><created>2013-04-23</created><authors><author><keyname>Novacek</keyname><forenames>Vit</forenames></author><author><keyname>Naseer</keyname><forenames>Aisha</forenames></author></authors><title>Technical report: Linking the scientific and clinical data with
  KI2NA-LHC</title><categories>cs.CY cs.DB cs.DL</categories><comments>A longer version of a paper originally published at the IEEE
  conference on Computer-Based Medical Systems (CBMS'13), under the name:
  Linking the Scientific and Clinical Data with KI2NA-LHC - An Outline (authors
  are the same)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a use case and propose a system for data and knowledge
integration in life sciences. In particular, we focus on linking clinical
resources (electronic patient records) with scientific documents and data
(research articles, biomedical ontologies and databases). Our motivation is
two-fold. Firstly, we aim to instantly provide scientific context of particular
patient cases for clinicians in order for them to propose treatments in a more
informed way. Secondly, we want to build a technical infrastructure for
researchers that will allow them to semi-automatically formulate and evaluate
their hypothesis against longitudinal patient data. This paper describes the
proposed system and its typical usage in a broader context of KI2NA, an ongoing
collaboration between the DERI research institute and Fujitsu Laboratories. We
introduce an architecture of the proposed framework called KI2NA-LHC (for
Linked Health Care) and outline the details of its implementation. We also
describe typical usage scenarios and propose a methodology for evaluation of
the whole framework. The main goal of this paper is to introduce our ongoing
work to a broader expert audience. By doing so, we aim to establish an
early-adopter community for our work and elicit feedback we could reflect in
the development of the prototype so that it is better tailored to the
requirements of target users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6475</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6475</id><created>2013-04-23</created><updated>2015-07-14</updated><authors><author><keyname>Avron</keyname><forenames>Haim</forenames></author><author><keyname>Druinsky</keyname><forenames>Alex</forenames></author><author><keyname>Gupta</keyname><forenames>Anshul</forenames></author></authors><title>Revisiting Asynchronous Linear Solvers: Provable Convergence Rate
  Through Randomization</title><categories>cs.DC cs.DS cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchronous methods for solving systems of linear equations have been
researched since Chazan and Miranker's pioneering 1969 paper on chaotic
relaxation. The underlying idea of asynchronous methods is to avoid processor
idle time by allowing the processors to continue to make progress even if not
all progress made by other processors has been communicated to them.
  Historically, the applicability of asynchronous methods for solving linear
equations was limited to certain restricted classes of matrices, such as
diagonally dominant matrices. Furthermore, analysis of these methods focused on
proving convergence in the limit. Comparison of the asynchronous convergence
rate with its synchronous counterpart and its scaling with the number of
processors were seldom studied, and are still not well understood.
  In this paper, we propose a randomized shared-memory asynchronous method for
general symmetric positive definite matrices. We rigorously analyze the
convergence rate and prove that it is linear, and is close to that of the
method's synchronous counterpart if the processor count is not excessive
relative to the size and sparsity of the matrix. We also present an algorithm
for unsymmetric systems and overdetermined least-squares. Our work presents a
significant improvement in the applicability of asynchronous linear solvers as
well as in their convergence analysis, and suggests randomization as a key
paradigm to serve as a foundation for asynchronous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6476</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6476</id><created>2013-04-23</created><authors><author><keyname>Daniels</keyname><forenames>Noah M.</forenames></author></authors><title>Remote Homology Detection in Proteins Using Graphical Models</title><categories>cs.CE q-bio.QM</categories><comments>Doctoral dissertation</comments><doi>10.1109/TCBB.2014.2344682</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the amino acid sequence of a protein, researchers often infer its
structure and function by finding homologous, or evolutionarily-related,
proteins of known structure and function. Since structure is typically more
conserved than sequence over long evolutionary distances, recognizing remote
protein homologs from their sequence poses a challenge.
  We first consider all proteins of known three-dimensional structure, and
explore how they cluster according to different levels of homology. An
automatic computational method reasonably approximates a human-curated
hierarchical organization of proteins according to their degree of homology.
  Next, we return to homology prediction, based only on the one-dimensional
amino acid sequence of a protein. Menke, Berger, and Cowen proposed a Markov
random field model to predict remote homology for beta-structural proteins, but
their formulation was computationally intractable on many beta-strand
topologies.
  We show two different approaches to approximate this random field, both of
which make it computationally tractable, for the first time, on all protein
folds. One method simplifies the random field itself, while the other retains
the full random field, but approximates the solution through stochastic search.
Both methods achieve improvements over the state of the art in remote homology
detection for beta-structural protein folds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6478</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6478</id><created>2013-04-23</created><authors><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author><author><keyname>Wang</keyname><forenames>Weiran</forenames></author></authors><title>The K-modes algorithm for clustering</title><categories>cs.LG stat.ME stat.ML</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many clustering algorithms exist that estimate a cluster centroid, such as
K-means, K-medoids or mean-shift, but no algorithm seems to exist that clusters
data by returning exactly K meaningful modes. We propose a natural definition
of a K-modes objective function by combining the notions of density and cluster
assignment. The algorithm becomes K-means and K-medoids in the limit of very
large and very small scales. Computationally, it is slightly slower than
K-means but much faster than mean-shift or K-medoids. Unlike K-means, it is
able to find centroids that are valid patterns, truly representative of a
cluster, even with nonconvex clusters, and appears robust to outliers and
misspecification of the scale and number of clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6480</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6480</id><created>2013-04-24</created><authors><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author><author><keyname>Li</keyname><forenames>Yuanzhi</forenames></author><author><keyname>He</keyname><forenames>Di</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>A Theoretical Analysis of NDCG Type Ranking Measures</title><categories>cs.LG cs.IR stat.ML</categories><comments>COLT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem in ranking is to design a ranking measure for evaluation of
ranking functions. In this paper we study, from a theoretical perspective, the
widely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures.
Although there are extensive empirical studies of NDCG, little is known about
its theoretical properties. We first show that, whatever the ranking function
is, the standard NDCG which adopts a logarithmic discount, converges to 1 as
the number of items to rank goes to infinity. On the first sight, this result
is very surprising. It seems to imply that NDCG cannot differentiate good and
bad ranking functions, contradicting to the empirical success of NDCG in many
applications. In order to have a deeper understanding of ranking measures in
general, we propose a notion referred to as consistent distinguishability. This
notion captures the intuition that a ranking measure should have such a
property: For every pair of substantially different ranking functions, the
ranking measure can decide which one is better in a consistent manner on almost
all datasets. We show that NDCG with logarithmic discount has consistent
distinguishability although it converges to the same limit for all ranking
functions. We next characterize the set of all feasible discount functions for
NDCG according to the concept of consistent distinguishability. Specifically we
show that whether NDCG has consistent distinguishability depends on how fast
the discount decays, and 1/r is a critical point. We then turn to the cut-off
version of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k for
various choices of k and the discount functions. Experimental results on real
Web search datasets agree well with the theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6482</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6482</id><created>2013-04-24</created><updated>2014-04-27</updated><authors><author><keyname>Keiren</keyname><forenames>Jeroen J. A.</forenames></author><author><keyname>Wesselink</keyname><forenames>Wieger</forenames></author><author><keyname>Willemse</keyname><forenames>Tim A. C.</forenames></author></authors><title>Improved Static Analysis of Parameterised Boolean Equation Systems using
  Control Flow Reconstruction</title><categories>cs.LO</categories><comments>This is an extended version containing full proofs, and results with
  both versions of our analysis. Scripts and results corresponding to this
  submission can be found at
  https://github.com/jkeiren/pbesstategraph-experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a sound static analysis technique for fighting the combinatorial
explosion of parameterised Boolean equation systems (PBESs). These essentially
are systems of mutually recursive fixed point equations ranging over
first-order logic formulae. Our method detects parameters that are not live by
analysing a control flow graph of a PBES, and it subsequently eliminates such
parameters. We show that a naive approach to constructing a control flow graph,
needed for the analysis, may suffer from an exponential blow-up, and we define
an approximate analysis that avoids this problem. The effectiveness of our
techniques is evaluated using a number of case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6485</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6485</id><created>2013-04-24</created><updated>2013-09-30</updated><authors><author><keyname>He</keyname><forenames>Biao</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author></authors><title>Secure On-Off Transmission Design with Channel Estimation Errors</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE TIFS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical layer security has recently been regarded as an emerging technique
to complement and improve the communication security in future wireless
networks. The current research and development in physical layer security is
often based on the ideal assumption of perfect channel knowledge or the
capability of variable-rate transmissions. In this work, we study the secure
transmission design in more practical scenarios by considering channel
estimation errors at the receiver and investigating both fixed-rate and
variable-rate transmissions. Assuming quasi-static fading channels, we design
secure on-off transmission schemes to maximize the throughput subject to a
constraint on secrecy outage probability. For systems with given and fixed
encoding rates, we show how the optimal on-off transmission thresholds and the
achievable throughput vary with the amount of knowledge on the eavesdropper's
channel. In particular, our design covers the interesting case where the
eavesdropper also uses the pilots sent from the transmitter to obtain imperfect
channel estimation. An interesting observation is that using too much pilot
power can harm the throughput of secure transmission if both the legitimate
receiver and the eavesdropper have channel estimation errors, while the secure
transmission always benefits from increasing pilot power when only the
legitimate receiver has channel estimation errors but not the eavesdropper.
When the encoding rates are controllable parameters to design, we further
derive both a non-adaptive and an adaptive rate transmission schemes by jointly
optimizing the encoding rates and the on-off transmission thresholds to
maximize the throughput of secure transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6486</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6486</id><created>2013-04-24</created><authors><author><keyname>Lanjewar</keyname><forenames>Ashutosh</forenames><affiliation>M.Tech. Student, Digital Communication, T.I.E.I.T</affiliation></author><author><keyname>Gupta</keyname><forenames>Neelesh</forenames><affiliation>M.Tech. Student, Digital Communication, T.I.E.I.T</affiliation></author></authors><title>Optimizing Cost, Delay, Packet Loss and Network Load in AODV Routing
  Protocol</title><categories>cs.NI</categories><comments>6 Pages, 7 Figures, Paper is registered with IJCSIS Journal Vol. 11,
  No. 4, April 2013</comments><journal-ref>International Journal of Computer Science and Information Security
  (IJCSIS Journal Vol. 11, No. 4, April 2013)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  AODV is Ad-hoc On-Demand Distance Vector.A mobile ad-hoc network is a
self-configuring network of mobile devices connected by wireless. MANET does
not have any fixed infrastructure. The device in a MANET is free to move in any
direction and will form the connection as per the requirement of the network.
Due to changing topology maintenance of factors like Packet loss, End to End
Delay, Number of hops, delivery ratio and controlling the network load is of
great challenge. This paper mainly concentrates on reducing the factors such as
cost, End-to-End Delay, Network Load and Packet loss in AODV routing protocol.
The NS-2 is used for the simulation purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6487</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6487</id><created>2013-04-24</created><authors><author><keyname>Zhen</keyname><forenames>Liangli</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author><author><keyname>Peng</keyname><forenames>Xi</forenames></author><author><keyname>Peng</keyname><forenames>Dezhong</forenames></author></authors><title>Locally linear representation for subspace learning and clustering</title><categories>cs.LG stat.ML</categories><comments>2 pages</comments><journal-ref>Electronics Letters 50 (13), 942-943, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a key to construct a similarity graph in graph-oriented subspace
learning and clustering. In a similarity graph, each vertex denotes a data
point and the edge weight represents the similarity between two points. There
are two popular schemes to construct a similarity graph, i.e., pairwise
distance based scheme and linear representation based scheme. Most existing
works have only involved one of the above schemes and suffered from some
limitations. Specifically, pairwise distance based methods are sensitive to the
noises and outliers compared with linear representation based methods. On the
other hand, there is the possibility that linear representation based
algorithms wrongly select inter-subspaces points to represent a point, which
will degrade the performance. In this paper, we propose an algorithm, called
Locally Linear Representation (LLR), which integrates pairwise distance with
linear representation together to address the problems. The proposed algorithm
can automatically encode each data point over a set of points that not only
could denote the objective point with less residual error, but also are close
to the point in Euclidean space. The experimental results show that our
approach is promising in subspace learning and subspace clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6489</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6489</id><created>2013-04-24</created><authors><author><keyname>Baccelli</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LINCS, INRIA Rocquencourt</affiliation></author><author><keyname>Mathieu</keyname><forenames>Fabien</forenames><affiliation>LINCS, INRIA Rocquencourt</affiliation></author><author><keyname>Norros</keyname><forenames>Ilkka</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Varloot</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Can P2P Networks be Super-Scalable?</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1108.4129</comments><proxy>ccsd</proxy><journal-ref>IEEE Infocom 2013 - 32nd IEEE International Conference on Computer
  Communications (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new model for peer-to-peer networking which takes the network
bottlenecks into account beyond the access. This model can cope with key
features of P2P networking like degree or locality constraints together with
the fact that distant peers often have a smaller rate than nearby peers. Using
a network model based on rate functions, we give a closed form expression of
peers download performance in the system's fluid limit, as well as
approximations for the other cases. Our results show the existence of realistic
settings for which the average download time is a decreasing function of the
load, a phenomenon that we call super-scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6491</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6491</id><created>2013-04-24</created><authors><author><keyname>Li</keyname><forenames>Hongxing</forenames></author><author><keyname>Wu</keyname><forenames>Chuan</forenames></author><author><keyname>Li</keyname><forenames>Zongpeng</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author></authors><title>Virtual Machine Trading in a Federation of Clouds: Individual Profit and
  Social Welfare Maximization</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By sharing resources among different cloud providers, the paradigm of
federated clouds exploits temporal availability of resources and geographical
diversity of operational costs for efficient job service. While
interoperability issues across different cloud platforms in a cloud federation
have been extensively studied, fundamental questions on cloud economics remain:
When and how should a cloud trade resources (e.g., virtual machines) with
others, such that its net profit is maximized over the long run, while a
close-to-optimal social welfare in the entire federation can also be
guaranteed? To answer this question, a number of important, inter-related
decisions, including job scheduling, server provisioning and resource pricing,
should be dynamically and jointly made, while the long-term profit optimality
is pursued. In this work, we design efficient algorithms for inter-cloud
virtual machine (VM) trading and scheduling in a cloud federation. For VM
transactions among clouds, we design a double-auction based mechanism that is
strategyproof, individual rational, ex-post budget balanced, and efficient to
execute over time. Closely combined with the auction mechanism is a dynamic VM
trading and scheduling algorithm, which carefully decides the true valuations
of VMs in the auction, optimally schedules stochastic job arrivals with
different SLAs onto the VMs, and judiciously turns on and off servers based on
the current electricity prices. Through rigorous analysis, we show that each
individual cloud, by carrying out the dynamic algorithm in the online double
auction, can achieve a time-averaged profit arbitrarily close to the offline
optimum. Asymptotic optimality in social welfare is also achieved under
homogeneous cloud settings. We carry out trace-driven simulations to examine
the effectiveness of our algorithms and the achievable social welfare under
heterogeneous cloud settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6494</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6494</id><created>2013-04-24</created><authors><author><keyname>Wei&#xdf;</keyname><forenames>Benjamin</forenames></author><author><keyname>Centarti</keyname><forenames>Federico</forenames></author><author><keyname>Schmitt</keyname><forenames>Felix</forenames></author><author><keyname>Straub</keyname><forenames>Stephen</forenames></author></authors><title>Route-Based Detection of Conflicting ATC Clearances on Airports</title><categories>cs.SY</categories><comments>Presented at the International Symposium on Enhanced Solutions for
  Aircraft and Vehicle Surveillance Applications (ESAVS 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Runway incursions are among the most serious safety concerns in air traffic
control. Traditional A-SMGCS level 2 safety systems detect runway incursions
with the help of surveillance information only. In the context of SESAR,
complementary safety systems are emerging that also use other information in
addition to surveillance, and that aim at warning about potential runway
incursions at earlier points in time. One such system is &quot;conflicting ATC
clearances&quot;, which processes the clearances entered by the air traffic
controller into an electronic flight strips system and cross-checks them for
potentially dangerous inconsistencies. The cross-checking logic may be
implemented directly based on the clearances and on surveillance data, but this
is cumbersome. We present an approach that instead uses ground routes as an
intermediate layer, thereby simplifying the core safety logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6496</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6496</id><created>2013-04-24</created><updated>2013-05-30</updated><authors><author><keyname>Wong</keyname><forenames>Wing Shing</forenames></author></authors><title>Transmission Sequence Design and Allocation for Wide Area Ad Hoc
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine the problem of designing and allocating transmission
sequences to users in a mobile ad hoc network that has no spatial boundary. A
basic tenet of the transmission sequence approach for addressing media access
control is that under normal operating conditions, there is no feedback
triggered re-transmission. This obviously is a major departure from the
Slotted-ALOHA or CSMA type approaches. While these solutions enjoy excellent
throughput performance, a fundamental drawback is that they are based on
feedback information. For systems without naturally defined central controller
that can play the role of a base station, the task of providing feedback
information could easily become unmanageable. This highlights the advantage of
the feedback-free approach. A second advantage is the ability to handle
unlimited spatial coverage. We propose in this paper a concept for media access
control that is akin to frequency reuse. However, instead of reusing frequency,
the new approach allows transmission sequences be reused. A study of the
transmission sequence approach against other approaches is conducted by
comparing the minimal frame lengths that can guarantee the existence of
conflict-free transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6498</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6498</id><created>2013-04-24</created><authors><author><keyname>Fang</keyname><forenames>Huixing</forenames></author><author><keyname>Zhu</keyname><forenames>Huibiao</forenames></author><author><keyname>Shi</keyname><forenames>Jianqi</forenames></author></authors><title>Apricot - An Object-Oriented Modeling Language for Hybrid Systems</title><categories>cs.SE cs.LO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Apricot as an object-oriented language for modeling hybrid
systems. The language combines the features in domain specific language and
object-oriented language, that fills the gap between design and implementation,
as a result, we put forward the modeling language with simple and distinct
syntax, structure and semantics. In addition, we introduce the concept of
design by convention into Apricot.As the characteristic of object-oriented and
the component architecture in Apricot, we conclude that it is competent for
modeling hybrid systems without losing scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6499</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6499</id><created>2013-04-24</created><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author></authors><title>Logging safely in public spaces using color PINs</title><categories>cs.HC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, we are increasingly logging on many different Internet sites to
access private data like emails or photos remotely stored in the clouds. This
makes us all the more concerned with digital identity theft and passwords being
stolen either by key loggers or shoulder-surfing attacks. Quite surprisingly,
the current bottleneck of computer security when logging for authentication is
the User Interface (UI): How can we enter safely secret passwords when
concealed spy cameras or key loggers may be recording the login session?
Logging safely requires to design a secure Human Computer Interface (HCI)
robust to those attacks. We describe a novel method and system based on
entering secret ID passwords by means of associative secret UI passwords that
provides zero-knowledge to observers. We demonstrate the principles using a
color Personal Identification Numbers (PINs) login system and describes its
various extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6501</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6501</id><created>2013-04-24</created><authors><author><keyname>Argyriou</keyname><forenames>Evmorfia N.</forenames></author><author><keyname>Sotiraki</keyname><forenames>Aikaterini A.</forenames></author><author><keyname>Symvonis</keyname><forenames>Antonios</forenames></author></authors><title>Occupational Fraud Detection Through Visualization</title><categories>cs.CY cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Occupational fraud affects many companies worldwide causing them economic
loss and liability issues towards their customers and other involved entities.
Detecting internal fraud in a company requires significant effort and,
unfortunately cannot be entirely prevented. The internal auditors have to
process a huge amount of data produced by diverse systems, which are in most
cases in textual form, with little automated support. In this paper, we exploit
the advantages of information visualization and present a system that aims to
detect occupational fraud in systems which involve a pair of entities (e.g., an
employee and a client) and periodic activity. The main visualization is based
on a spiral system on which the events are drawn appropriately according to
their time-stamp. Suspicious events are considered those which appear along the
same radius or on close radii of the spiral. Before producing the
visualization, the system ranks both involved entities according to the
specifications of the internal auditor and generates a video file of the
activity such that events with strong evidence of fraud appear first in the
video. The system is also equipped with several different visualizations and
mechanisms in order to meet the requirements of an internal fraud detection
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6505</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6505</id><created>2013-04-24</created><authors><author><keyname>Schmitt</keyname><forenames>Felix</forenames></author><author><keyname>Heidger</keyname><forenames>Ralf</forenames></author><author><keyname>Straub</keyname><forenames>Stephen</forenames></author><author><keyname>Wei&#xdf;</keyname><forenames>Benjamin</forenames></author></authors><title>Software Design Principles of a DFS Tower A-CWP Prototype</title><categories>cs.SE</categories><comments>Presented at the International Symposium on Enhanced Solutions for
  Aircraft and Vehicle Surveillance Applications (ESAVS 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SESAR is supposed to boost the development of new operational procedures
together with the supporting systems in order to modernize the pan-European air
traffic management (ATM). One consequence of this development is that more and
more information is presented to - and has to be processed by - air traffic
control officers (ATCOs). Thus, there is a strong need for a software design
concept that fosters the development of an advanced (tower) controller working
position (A-CWP) that comprehensively integrates the still counting amount of
information while reducing the data management workload of ATCOs. We report on
our first hands-on experiences obtained during the development of an A-CWP
prototype that was used in two SESAR validation sessions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6506</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6506</id><created>2013-04-24</created><authors><author><keyname>Halimi</keyname><forenames>Oualid El</forenames></author><author><keyname>Derafshkavian</keyname><forenames>Peyman</forenames></author><author><keyname>Albeladi</keyname><forenames>Abdulrhman</forenames></author><author><keyname>Alrashdi</keyname><forenames>Faisal</forenames></author></authors><title>Toward Recovering Complete SRS for Softbody Simulation System and a
  Sample Application - a Team 4 SOEN6481-W13 Project Report</title><categories>cs.SE</categories><comments>60 Pages</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document gathers high-level users requirements and describes the system
features. It provides a detailed explanation of the main functionalities of the
system with a more emphasis on the stakeholders needs and wants. Indeed, the
document goes through design constraints that may restrict various aspects of
the design and implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6508</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6508</id><created>2013-04-24</created><authors><author><keyname>Okayama</keyname><forenames>Tomoaki</forenames></author></authors><title>Theoretical analysis of Sinc-collocation methods and Sinc-Nystr\&quot;{o}m
  methods for initial value problems</title><categories>math.NA cs.NA</categories><comments>Keywords: Sinc approximation, Sinc indefinite integration,
  differential equation, Volterra integral equation, tanh transformation,
  double-exponential transformation</comments><msc-class>65L05, 65R20, 65D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Sinc-collocation method has been proposed by Stenger, and he also gave
theoretical analysis of the method in the case of a `scalar' equation. This
paper extends the theoretical results to the case of a `system' of equations.
Furthermore, this paper proposes more efficient method by replacing the
variable transformation employed in Stenger's method. The efficiency is
confirmed by both of theoretical analysis and numerical experiments. In
addition to the existing and newly-proposed Sinc-collocation methods, this
paper also gives similar theoretical results for Sinc-Nystr\&quot;{o}m methods
proposed by Nurmuhammad et al. From a viewpoint of the computational cost, it
turns out that the newly-proposed Sinc-collocation method is the most efficient
among those methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6528</identifier>
 <datestamp>2013-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6528</id><created>2013-04-24</created><updated>2013-04-25</updated><authors><author><keyname>Kourtellaris</keyname><forenames>Christos</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author></authors><title>Nonanticipative Rate Distortion Function for General Source-Channel
  Matching</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we invoke a nonanticipative information Rate Distortion
Function (RDF) for sources with memory, and we analyze its importance in
probabilistic matching of the source to the channel so that transmission of a
symbol-by-symbol code with memory without anticipation is optimal, with respect
to an average distortion and excess distortion probability. We show
achievability of the symbol-by-symbol code with memory without anticipation,
and we evaluate the probabilistic performance of the code for a Markov source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6533</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6533</id><created>2013-04-24</created><authors><author><keyname>Cie&#x15b;li&#x144;ski</keyname><forenames>Jan L.</forenames></author></authors><title>Locally exact modifications of discrete gradient schemes</title><categories>physics.comp-ph cs.NA math.NA</categories><comments>16 pages plus 4 figures</comments><msc-class>65P10, 65L12, 34K28</msc-class><acm-class>G.1.7</acm-class><journal-ref>Physics Letters A 377 (8) (2013) 651-654</journal-ref><doi>10.1016/j.physleta.2013.01.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locally exact integrators preserve linearization of the original system at
every point. We construct energy-preserving locally exact discrete gradient
schemes for arbitrary multidimensional canonical Hamiltonian systems by
modifying classical discrete gradient schemes. Modifications of this kind are
found for any discrete gradient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6551</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6551</id><created>2013-04-24</created><updated>2013-08-01</updated><authors><author><keyname>L&#xed;n</keyname><forenames>V&#xe1;clav</forenames></author></authors><title>Decision-Theoretic Troubleshooting: Hardness of Approximation</title><categories>cs.AI cs.CC</categories><comments>The paper has been withdrawn since it has been published in IJAR
  (http://dx.doi.org/10.1016/j.ijar.2013.07.003)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision-theoretic troubleshooting is one of the areas to which Bayesian
networks can be applied. Given a probabilistic model of a malfunctioning
man-made device, the task is to construct a repair strategy with minimal
expected cost. The problem has received considerable attention over the past
two decades. Efficient solution algorithms have been found for simple cases,
whereas other variants have been proven NP-complete. We study several variants
of the problem found in literature, and prove that computing approximate
troubleshooting strategies is NP-hard. In the proofs, we exploit a close
connection to set-covering problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6554</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6554</id><created>2013-04-24</created><authors><author><keyname>Yan</keyname><forenames>Bowen</forenames></author><author><keyname>Gregory</keyname><forenames>Steve</forenames></author></authors><title>Identifying Communities and Key Vertices by Reconstructing Networks from
  Samples</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 17 figures</comments><journal-ref>PLoS ONE 8(4): e61006. (2013)</journal-ref><doi>10.1371/journal.pone.0061006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling techniques such as Respondent-Driven Sampling (RDS) are widely used
in epidemiology to sample &quot;hidden&quot; populations, such that properties of the
network can be deduced from the sample. We consider how similar techniques can
be designed that allow the discovery of the structure, especially the community
structure, of networks. Our method involves collecting samples of a network by
random walks and reconstructing the network by probabilistically coalescing
vertices, using vertex attributes to determine the probabilities. Even though
our method can only approximately reconstruct a part of the original network,
it can recover its community structure relatively well. Moreover, it can find
the key vertices which, when immunized, can effectively reduce the spread of an
infection through the original network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6572</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6572</id><created>2013-04-24</created><authors><author><keyname>Habeeb</keyname><forenames>Maggie</forenames></author><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author><author><keyname>Koupparis</keyname><forenames>Charalambos</forenames></author><author><keyname>Shpilrain</keyname><forenames>Vladimir</forenames></author></authors><title>Public key exchange using semidirect product of (semi)groups</title><categories>cs.CR math.GR</categories><comments>12 pages</comments><msc-class>68P25, 94A60, 20E22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a brand new key exchange protocol based on a
semidirect product of (semi)groups (more specifically, on extension of a
(semi)group by automorphisms), and then focus on practical instances of this
general idea. Our protocol can be based on any group, in particular on any
non-commutative group. One of its special cases is the standard Diffie-Hellman
protocol, which is based on a cyclic group. However, when our protocol is used
with a non-commutative (semi)group, it acquires several useful features that
make it compare favorably to the Diffie-Hellman protocol. Here we also suggest
a particular non-commutative semigroup (of matrices) as the platform and show
that security of the relevant protocol is based on a quite different assumption
compared to that of the standard Diffie-Hellman protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6574</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6574</id><created>2013-04-24</created><updated>2013-06-24</updated><authors><author><keyname>Escrig</keyname><forenames>David de Frutos</forenames><affiliation>Universidad Complutense de Madrid</affiliation></author><author><keyname>Gregorio-Rodr&#xed;guez</keyname><forenames>Carlos</forenames><affiliation>Universidad Complutense de Madrid</affiliation></author><author><keyname>Palomino</keyname><forenames>Miguel</forenames><affiliation>Universidad Complutense de Madrid</affiliation></author><author><keyname>Hern&#xe1;ndez</keyname><forenames>David Romero</forenames><affiliation>Universidad Complutense de Madrid</affiliation></author></authors><title>Unifying the Linear Time-Branching Time Spectrum of Process Semantics</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 2 (June 27,
  2013) lmcs:983</journal-ref><doi>10.2168/LMCS-9(2:11)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Van Glabbeek's linear time-branching time spectrum is one of the most
relevant work on comparative study on process semantics, in which semantics are
partially ordered by their discrimination power. In this paper we bring forward
a refinement of this classification and show how the process semantics can be
dealt with in a uniform way: based on the very natural concept of constrained
simulation we show how we can classify the spectrum in layers; for the families
lying in the same layer we show how to obtain in a generic way equational,
observational, logical and operational characterizations; relations among
layers are also very natural and differences just stem from the constraint
imposed on the simulations that rule the layers. Our methodology also shows how
to achieve a uniform treatment of semantic preorders and equivalences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6575</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6575</id><created>2013-04-24</created><authors><author><keyname>Hanmanthu</keyname><forenames>B.</forenames></author><author><keyname>Ram</keyname><forenames>B. Raghu</forenames></author><author><keyname>Niranjan</keyname><forenames>P.</forenames></author></authors><title>Third Party Privacy Preserving Protocol for Perturbation Based
  Classification of Vertically Fragmented Data Bases</title><categories>cs.CR cs.DB</categories><comments>Appeared in ICECIT-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy is become major issue in distributed data mining. In the literature
we can found many proposals of privacy preserving which can be divided into two
major categories that is trusted third party and multiparty based privacy
protocols. In case of trusted third party models the conventional asymmetric
cryptographic based techniques will be used and in case of multi party based
protocols data perturbed to make sure no other party to understand original
data. In order to enhance security features by combining strengths of both
models in this paper, we propose to use data perturbed techniques in third
party privacy preserving protocol to conduct the classification on vertically
fragmented data bases. Specially, we present a method to build Naive Bayes
classification from the disguised and decentralized databases. In order to
perform classification we propose third party protocol for secure computations.
We conduct experiments to compare the accuracy of our Naive Bayes with the one
built from the original undisguised data. Our results show that although the
data are disguised and decentralized, our method can still achieve fairly high
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6584</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6584</id><created>2013-04-24</created><authors><author><keyname>Castelluccia</keyname><forenames>Claude</forenames></author><author><keyname>Chaabane</keyname><forenames>Abdelberi</forenames></author><author><keyname>D&#xfc;rmuth</keyname><forenames>Markus</forenames></author><author><keyname>Perito</keyname><forenames>Daniele</forenames></author></authors><title>When Privacy meets Security: Leveraging personal information for
  password cracking</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passwords are widely used for user authentication and, despite their
weaknesses, will likely remain in use in the foreseeable future.
Human-generated passwords typically have a rich structure, which makes them
susceptible to guessing attacks. In this paper, we study the effectiveness of
guessing attacks based on Markov models. Our contributions are two-fold. First,
we propose a novel password cracker based on Markov models, which builds upon
and extends ideas used by Narayanan and Shmatikov (CCS 2005). In extensive
experiments we show that it can crack up to 69% of passwords at 10 billion
guesses, more than all probabilistic password crackers we compared again t.
Second, we systematically analyze the idea that additional personal information
about a user helps in speeding up password guessing. We find that, on average
and by carefully choosing parameters, we can guess up to 5% more passwords,
especially when the number of attempts is low. Furthermore, we show that the
gain can go up to 30% for passwords that are actually based on personal
attributes. These passwords are clearly weaker and should be avoided. Our
cracker could be used by an organization to detect and reject them. To the best
of our knowledge, we are the first to systematically study the relationship
between chosen passwords and users' personal information. We test and validate
our results over a wide collection of leaked password databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6588</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6588</id><created>2013-04-24</created><authors><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author><author><keyname>Zhou</keyname><forenames>Hang</forenames></author></authors><title>Graph Reconstruction via Distance Oracles</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of reconstructing a hidden graph given access to a
distance oracle. We design randomized algorithms for the following problems:
reconstruction of a degree bounded graph with query complexity
$\tilde{O}(n^{3/2})$; reconstruction of a degree bounded outerplanar graph with
query complexity $\tilde{O}(n)$; and near-optimal approximate reconstruction of
a general graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6589</identifier>
 <datestamp>2013-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6589</id><created>2013-04-24</created><updated>2013-08-07</updated><authors><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author></authors><title>Partitions of Frobenius Rings Induced by the Homogeneous Weight</title><categories>cs.IT math.IT math.RA</categories><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The values of the homogeneous weight are determined for finite Frobenius
rings that are a direct product of local Frobenius rings. This is used to
investigate the partition induced by this weight and its dual partition under
character-theoretic dualization. A characterization is given of those rings for
which the induced partition is reflexive or even self-dual.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6591</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6591</id><created>2013-04-24</created><authors><author><keyname>Yukawa</keyname><forenames>Masahiro</forenames></author><author><keyname>Amari</keyname><forenames>Shun-ichi</forenames></author></authors><title>Lp-Regularized Least Squares (0&lt;p&lt;1) and Critical Path</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The least squares problem is formulated in terms of Lp quasi-norm
regularization (0&lt;p&lt;1). Two formulations are considered: (i) an Lp-constrained
optimization and (ii) an Lp-penalized (unconstrained) optimization. Due to the
nonconvexity of the Lp quasi-norm, the solution paths of the regularized least
squares problem are not ensured to be continuous. A critical path, which is a
maximal continuous curve consisting of critical points, is therefore considered
separately. The critical paths are piecewise smooth, as can be seen from the
viewpoint of the variational method, and generally contain non-optimal points
such as saddle points and local maxima as well as global/local minima. Along
each critical path, the correspondence between the regularization parameters
(which govern the 'strength' of regularization in the two formulations) is
non-monotonic and, more specifically, it has multiplicity. Two paths of
critical points connecting the origin and an ordinary least squares (OLS)
solution are highlighted. One is a main path starting at an OLS solution, and
the other is a greedy path starting at the origin. Part of the greedy path can
be constructed with a generalized Minkowskian gradient. The breakpoints of the
greedy path coincide with the step-by-step solutions generated by using
orthogonal matching pursuit (OMP), thereby establishing a direct link between
OMP and Lp-regularized least squares.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6593</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6593</id><created>2013-04-24</created><updated>2013-08-26</updated><authors><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>V&#xe9;gh</keyname><forenames>L&#xe1;szl&#xf3; A.</forenames></author></authors><title>Fixed-parameter algorithms for minimum cost edge-connectivity
  augmentation</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider connectivity-augmentation problems in a setting where each
potential new edge has a nonnegative cost associated with it, and the task is
to achieve a certain connectivity target with at most p new edges of minimum
total cost. The main result is that the minimum cost augmentation of
edge-connectivity from k-1 to k with at most p new edges is fixed-parameter
tractable parameterized by p and admits a polynomial kernel. We also prove the
fixed-parameter tractability of increasing edge-connectivity from 0 to 2, and
increasing node-connectivity from 1 to 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6599</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6599</id><created>2013-04-24</created><updated>2013-07-26</updated><authors><author><keyname>Barbier</keyname><forenames>Jean</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author><author><keyname>Zhang</keyname><forenames>Pan</forenames></author></authors><title>Robust error correction for real-valued signals via message-passing
  decoding and spatial coupling</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures, IEEE Information Theory Workshop, Seville,
  September 9-13, 2013</comments><journal-ref>IEEE Information Theory Workshop (ITW 2013), 1-5 (2013)</journal-ref><doi>10.1109/ITW.2013.6691262</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the error correction scheme of real-valued signals when the
codeword is corrupted by gross errors on a fraction of entries and a small
noise on all the entries. Combining the recent developments of approximate
message passing and the spatially-coupled measurement matrix in compressed
sensing we show that the error correction and its robustness towards noise can
be enhanced considerably. We discuss the performance in the large signal limit
using previous results on state evolution, as well as for finite size signals
through numerical simulations. Even for relatively small sizes, the approach
proposed here outperforms convex-relaxation-based decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6601</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6601</id><created>2013-04-24</created><updated>2013-10-31</updated><authors><author><keyname>Eom</keyname><forenames>Young-Ho</forenames></author><author><keyname>Frahm</keyname><forenames>Klaus M.</forenames></author><author><keyname>Bencz&#xfa;r</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Time evolution of Wikipedia network ranking</title><categories>physics.soc-ph cs.IR cs.SI</categories><comments>10 pages, 11 figures. Accepted for publication in EPJB</comments><journal-ref>Eur. Phys. J. B. (2013) 86: 492</journal-ref><doi>10.1140/epjb/e2013-40432-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the time evolution of ranking and spectral properties of the Google
matrix of English Wikipedia hyperlink network during years 2003 - 2011. The
statistical properties of ranking of Wikipedia articles via PageRank and
CheiRank probabilities, as well as the matrix spectrum, are shown to be
stabilized for 2007 - 2011. A special emphasis is done on ranking of Wikipedia
personalities and universities. We show that PageRank selection is dominated by
politicians while 2DRank, which combines PageRank and CheiRank, gives more
accent on personalities of arts. The Wikipedia PageRank of universities
recovers 80 percents of top universities of Shanghai ranking during the
considered time period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6603</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6603</id><created>2013-04-24</created><updated>2015-02-10</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Petrov</keyname><forenames>Tatjana</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author><author><keyname>Koeppl</keyname><forenames>Heinz</forenames></author></authors><title>Optimal Kullback-Leibler Aggregation via Information Bottleneck</title><categories>cs.SY cs.IT math.IT</categories><comments>13 pages, 4 figures</comments><journal-ref>IEEE Trans. Autom. Control, vol. PP, no. 99, p. 1-13, 2015</journal-ref><doi>10.1109/TAC.2014.2364971</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a method for reducing a regular, discrete-time
Markov chain (DTMC) to another DTMC with a given, typically much smaller number
of states. The cost of reduction is defined as the Kullback-Leibler divergence
rate between a projection of the original process through a partition function
and a DTMC on the correspondingly partitioned state space. Finding the reduced
model with minimal cost is computationally expensive, as it requires an
exhaustive search among all state space partitions, and an exact evaluation of
the reduction cost for each candidate partition. Our approach deals with the
latter problem by minimizing an upper bound on the reduction cost instead of
minimizing the exact cost; The proposed upper bound is easy to compute and it
is tight if the original chain is lumpable with respect to the partition. Then,
we express the problem in the form of information bottleneck optimization, and
propose using the agglomerative information bottleneck algorithm for searching
a sub-optimal partition greedily, rather than exhaustively. The theory is
illustrated with examples and one application scenario in the context of
modeling bio-molecular interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6613</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6613</id><created>2013-04-23</created><authors><author><keyname>Kelsey</keyname><forenames>Thomas W.</forenames></author><author><keyname>Dodwell</keyname><forenames>Sarah K.</forenames></author><author><keyname>Wilkinson</keyname><forenames>A. Graham</forenames></author><author><keyname>Greve</keyname><forenames>Tine</forenames></author><author><keyname>Andersen</keyname><forenames>Claus Y.</forenames></author><author><keyname>Anderson</keyname><forenames>Richard A.</forenames></author><author><keyname>Wallace</keyname><forenames>W. Hamish B.</forenames></author></authors><title>Ovarian volume throughout life: a validated normative model</title><categories>q-bio.TO cs.CE</categories><comments>14 pages, 7 figures</comments><msc-class>92B05</msc-class><acm-class>G.1.2</acm-class><doi>10.1371/journal.pone.0071465</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The measurement of ovarian volume has been shown to be a useful indirect
indicator of the ovarian reserve in women of reproductive age, in the diagnosis
and management of a number of disorders of puberty and adult reproductive
function, and is under investigation as a screening tool for ovarian cancer. To
date there is no normative model of ovarian volume throughout life. By
searching the published literature for ovarian volume in healthy females, and
using our own data from multiple sources (combined n = 59,994) we have
generated and robustly validated the first model of ovarian volume from
conception to 82 years of age. This model shows that 69% of the variation in
ovarian volume is due to age alone. We have shown that in the average case
ovarian volume rises from 0.7 mL (95% CI 0.4 -- 1.1 mL) at 2 years of age to a
peak of 7.7 mL (95% CI 6.5 -- 9.2 mL) at 20 years of age with a subsequent
decline to about 2.8mL (95% CI 2.7 -- 2.9 mL) at the menopause and smaller
volumes thereafter. Our model allows us to generate normal values and ranges
for ovarian volume throughout life. This is the first validated normative model
of ovarian volume from conception to old age; it will be of use in the
diagnosis and management of a number of diverse gynaecological and reproductive
conditions in females from birth to menopause and beyond.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6614</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6614</id><created>2013-04-24</created><authors><author><keyname>Fang</keyname><forenames>Yi</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Tong</keyname><forenames>Kin-Fai</forenames></author></authors><title>Performance Analysis of Protograph LDPC Codes for Nakagami-$m$ Fading
  Relay Channels</title><categories>cs.IT math.IT</categories><comments>15 pages, 3 figures, accepted, IET Commun., Apri. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the error performance of the protograph (LDPC)
codes over Nakagami-$m$ fading relay channels. We first calculate the decoding
thresholds of the protograph codes over such channels with different fading
depths (i.e., different values of $m$) by exploiting the modified protograph
extrinsic information transfer (PEXIT) algorithm. Furthermore, based on the
PEXIT analysis and using Gaussian approximation, we derive the bit-error-rate
(BER) expressions for the error-free (EF) relaying protocol and
decode-and-forward (DF) relaying protocol. We finally compare the threshold
with the theoretical BER and the simulated BER results of the protograph codes.
It reveals that the performance of DF protocol is approximately the same as
that of EF protocol. Moreover, the theoretical BER expressions, which are shown
to be reasonably consistent with the decoding thresholds and the simulated
BERs, are able to evaluate the system performance and predict the decoding
threshold with lower complexity as compared to the modified PEXIT algorithm. As
a result, this work can facilitate the design of the protograph codes for the
wireless communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6617</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6617</id><created>2013-04-24</created><authors><author><keyname>Abdallah</keyname><forenames>Saeed</forenames></author><author><keyname>Psaromiligkos</keyname><forenames>Ioannis N.</forenames></author></authors><title>EM-based Semi-blind Channel Estimation in AF Two-Way Relay Networks</title><categories>cs.IT math.IT stat.OT</categories><comments>3 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an expectation maximization (EM)-based algorithm for semi-blind
channel estimation of reciprocal channels in amplify-and-forward (AF) two-way
relay networks (TWRNs). By incorporating both data samples and pilots into the
estimation, the proposed algorithm provides substantially higher accuracy than
the conventional training-based approach. Furthermore, the proposed algorithm
has a linear computational complexity per iteration and converges after a small
number of iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6626</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6626</id><created>2013-04-24</created><authors><author><keyname>Wenzel</keyname><forenames>Makarius</forenames></author></authors><title>PIDE as front-end technology for Coq</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Isabelle/PIDE is the current Prover IDE technology for Isabelle. It has been
developed in ML and Scala in the past 4-5 years for this particular proof
assistant, but with an open mind towards other systems. PIDE is based on an
asynchronous document model, where the prover receives edits continuously and
updates its internal state accordingly. The interpretation of edits and the
policies for proof document processing are determined by the prover. The editor
front-end merely takes care of visual rendering of formal document content.
  Here we report on an experiment to connect Coq to the PIDE infrastructure of
Isabelle. This requires to re-implement the core PIDE protocol layer of
Isabelle/ML in OCaml. The payload for semantic processing of proof document
content is restricted to lexical analysis in the sense of existing CoqIde
functionality. This is sufficient as proof-of-concept for PIDE connectivity.
Actual proof processing is then a matter of improving Coq towards timeless and
stateless proof processing, independently of PIDE technicalities. The
implementation worked out smoothly and required minimal changes to the refined
PIDE architecture of Isabelle2013.
  This experiment substantiates PIDE as general approach to prover interaction.
It illustrates how other provers of the greater ITP family can participate by
following similar reforms of the classic TTY loop as was done for Isabelle in
the past few years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6627</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6627</id><created>2013-04-24</created><authors><author><keyname>Bahmani</keyname><forenames>Sohail</forenames></author><author><keyname>Boufounos</keyname><forenames>Petros T.</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>Robust 1-bit Compressive Sensing via Gradient Support Pursuit</title><categories>cs.IT math.IT math.OC math.ST stat.TH</categories><comments>18 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a formulation of 1-bit Compressed Sensing (CS) problem
based on the maximum likelihood estimation framework. In order to solve the
problem we apply the recently proposed Gradient Support Pursuit algorithm, with
a minor modification. Assuming the proposed objective function has a Stable
Restricted Hessian, the algorithm is shown to accurately solve the 1-bit CS
problem. Furthermore, the algorithm is compared to the state-of-the-art 1-bit
CS algorithms through numerical simulations. The results suggest that the
proposed method is robust to noise and at mid to low input SNR regime it
achieves the best reconstruction SNR vs. execution time trade-off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6656</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6656</id><created>2013-04-16</created><authors><author><keyname>Flammini</keyname><forenames>Francesco</forenames></author><author><keyname>Marrone</keyname><forenames>Stefano</forenames></author><author><keyname>Mazzocca</keyname><forenames>Nicola</forenames></author><author><keyname>Vittorini</keyname><forenames>Valeria</forenames></author></authors><title>A new modeling approach to the safety evaluation of N-modular redundant
  computer systems in presence of imperfect maintenance</title><categories>cs.SE</categories><doi>10.1016/j.ress.2009.02.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of safety-critical control systems are based on N-modular
redundant architectures, using majority voters on the outputs of independent
computation units. In order to assess the compliance of these architectures
with international safety standards, the frequency of hazardous failures must
be analyzed by developing and solving proper formal models. Furthermore, the
impact of maintenance faults has to be considered, since imperfect maintenance
may degrade the safety integrity level of the system. In this paper we present
both a failure model for voting architectures based on Bayesian Networks and a
maintenance model based on Continuous Time Markov Chains, and we propose to
combine them according to a compositional multiformalism modeling approach in
order to analyze the impact of imperfect maintenance on the system safety. We
also show how the proposed approach promotes the reuse and the interchange of
models as well the interchange of solving tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6663</identifier>
 <datestamp>2013-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6663</id><created>2013-04-24</created><updated>2013-04-25</updated><authors><author><keyname>Mishra</keyname><forenames>B.</forenames></author><author><keyname>Meyer</keyname><forenames>G.</forenames></author><author><keyname>Sepulchre</keyname><forenames>R.</forenames></author></authors><title>Low-rank optimization for distance matrix completion</title><categories>math.OC cs.LG stat.ML</categories><comments>In Proceedings of the 50th IEEE Conference on Decision and Control
  and European Control Conference, 2011</comments><doi>10.1109/CDC.2011.6160810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of low-rank distance matrix completion. This
problem amounts to recover the missing entries of a distance matrix when the
dimension of the data embedding space is possibly unknown but small compared to
the number of considered data points. The focus is on high-dimensional
problems. We recast the considered problem into an optimization problem over
the set of low-rank positive semidefinite matrices and propose two efficient
algorithms for low-rank distance matrix completion. In addition, we propose a
strategy to determine the dimension of the embedding space. The resulting
algorithms scale to high-dimensional problems and monotonically converge to a
global solution of the problem. Finally, numerical experiments illustrate the
good performance of the proposed algorithms on benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6666</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6666</id><created>2013-04-24</created><updated>2013-10-18</updated><authors><author><keyname>Efthymiou</keyname><forenames>Charilaos</forenames></author></authors><title>MCMC sampling colourings and independent sets of G(n,d/n) near the
  uniqueness threshold</title><categories>cs.DM math.CO math.PR</categories><comments>In each of the proofs of Theorems 6,7 we add a small clarification
  about the block of vertices B'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling from Gibbs distribution is a central problem in computer science as
well as in statistical physics. In this work we focus on the k-colouring model}
and the hard-core model with fugacity \lambda when the underlying graph is an
instance of Erdos-Renyi random graph G(n,p), where p=d/n and d is fixed.
  We use the Markov Chain Monte Carlo method for sampling from the
aforementioned distributions. In particular, we consider Glauber (block)
dynamics. We show a dramatic improvement on the bounds for rapid mixing in
terms of the number of colours and the fugacity for the corresponding models.
For both models the bounds we get are only within small constant factors from
the conjectured ones by the statistical physicists.
  We use Path Coupling to show rapid mixing. For k and \lambda in the range of
our interest the technical challenge is to cope with the high degree vertices,
i.e. vertices of degree much larger than the expected degree d. The usual
approach to this problem is to consider block updates rather than single vertex
updates for the Markov chain. Taking appropriately defined blocks the effect of
high degree vertices somehow diminishes. However, devising such a construction
of blocks is a highly non trivial task.
  We develop for a first time a weighting schema for the paths of the
underlying graph. Vertices which belong to &quot;light&quot; paths, only, can be placed
at the boundaries of the blocks. Then the tree-like local structure of G(n,d/n)
allows the construction of simple structured blocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6672</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6672</id><created>2013-04-11</created><authors><author><keyname>Zodpe</keyname><forenames>Harshali</forenames></author><author><keyname>Wani</keyname><forenames>Prakash</forenames></author><author><keyname>Mehta</keyname><forenames>Rakesh</forenames></author></authors><title>Hardware Implementation of Algorithm for Cryptanalysis</title><categories>cs.CR cs.AR</categories><comments>9 pages, 7 figures</comments><journal-ref>International Journal on Cryptography and Information Security
  (IJCIS), Vol.3, No.1, March 2013</journal-ref><doi>10.5121/ijcis.2013.3102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptanalysis of block ciphers involves massive computations which are
independent of each other and can be instantiated simultaneously so that the
solution space is explored at a faster rate. With the advent of low cost Field
Programmable Gate Arrays, building special purpose hardware for computationally
intensive applications has now become possible. For this the Data Encryption
Standard is used as a proof of concept. This paper presents the design for
Hardware implementation of DES cryptanalysis on FPGA using exhaustive key
search. Two architectures viz. Rolled and Unrolled DES architecture are
compared and based on experimental result the Rolled architecture is
implemented on FPGA. The aim of this work is to make cryptanalysis faster and
better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6685</identifier>
 <datestamp>2013-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6685</id><created>2013-04-24</created><updated>2013-08-26</updated><authors><author><keyname>Brody</keyname><forenames>Joshua</forenames></author><author><keyname>Hatami</keyname><forenames>Pooya</forenames></author></authors><title>Distance-Sensitive Property Testing Lower Bounds</title><categories>cs.CC</categories><comments>14 pages. arXiv admin note: text overlap with arXiv:1202.3479</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider several property testing problems and ask how the
query complexity depends on the distance parameter $\eps$. We achieve new lower
bounds in this setting for the problems of testing whether a function is
monotone and testing whether the function has low Fourier degree. For
monotonicity testing, our lower bound matches the recent upper bound of
Chakrabarty and Seshadhri.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6690</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6690</id><created>2013-04-24</created><updated>2014-01-21</updated><authors><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Edfors</keyname><forenames>Ove</forenames></author><author><keyname>Tufvesson</keyname><forenames>Fredrik</forenames></author><author><keyname>Marzetta</keyname><forenames>Thomas L.</forenames></author></authors><title>Massive MIMO for Next Generation Wireless Systems</title><categories>cs.IT math.IT</categories><comments>Final manuscript, to appear in IEEE Communications Magazine</comments><journal-ref>IEEE Communications Magazine, Vol. 52, No. 2, pp. 186-195, Feb.
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-user Multiple-Input Multiple-Output (MIMO) offers big advantages over
conventional point-to-point MIMO: it works with cheap single-antenna terminals,
a rich scattering environment is not required, and resource allocation is
simplified because every active terminal utilizes all of the time-frequency
bins. However, multi-user MIMO, as originally envisioned with roughly equal
numbers of service-antennas and terminals and frequency division duplex
operation, is not a scalable technology. Massive MIMO (also known as
&quot;Large-Scale Antenna Systems&quot;, &quot;Very Large MIMO&quot;, &quot;Hyper MIMO&quot;, &quot;Full-Dimension
MIMO&quot; &amp; &quot;ARGOS&quot;) makes a clean break with current practice through the use of a
large excess of service-antennas over active terminals and time division duplex
operation. Extra antennas help by focusing energy into ever-smaller regions of
space to bring huge improvements in throughput and radiated energy efficiency.
Other benefits of massive MIMO include the extensive use of inexpensive
low-power components, reduced latency, simplification of the media access
control (MAC) layer, and robustness to intentional jamming. The anticipated
throughput depend on the propagation environment providing asymptotically
orthogonal channels to the terminals, but so far experiments have not disclosed
any limitations in this regard. While massive MIMO renders many traditional
research problems irrelevant, it uncovers entirely new problems that urgently
need attention: the challenge of making many low-cost low-precision components
that work effectively together, acquisition and synchronization for
newly-joined terminals, the exploitation of extra degrees of freedom provided
by the excess of service-antennas, reducing internal power consumption to
achieve total energy efficiency reductions, and finding new deployment
scenarios. This paper presents an overview of the massive MIMO concept and
contemporary research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6693</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6693</id><created>2013-04-24</created><authors><author><keyname>Che</keyname><forenames>Pak Hou</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author></authors><title>Reliable Deniable Communication: Hiding Messages in Noise</title><categories>cs.IT math.IT</categories><comments>Longer version of paper accepted for presentation at ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A transmitter Alice may wish to {\it reliably} transmit a message to a
receiver Bob over a binary symmetric channel (BSC), while simultaneously
ensuring that her transmission is {\it deniable} from an eavesdropper Willie.
That is, if Willie listening to Alice's transmissions over a {&quot;significantly
noisier&quot;} BSC than the one to Bob, he should be unable to estimate even whether
Alice is transmitting. Even when Alice's (potential) communication scheme is
publicly known to Willie (with {\it no} common randomness between Alice and
Bob), we prove that over $n$ channel uses Alice can transmit a message of
length ${\cal O}(\sqrt{n})$ bits to Bob, deniably from Willie. We also prove
information-theoretically order-optimality of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6694</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6694</id><created>2013-04-24</created><authors><author><keyname>Mavroudi</keyname><forenames>Anna</forenames></author><author><keyname>Hadzilacos</keyname><forenames>Thanasis</forenames></author></authors><title>Technical report- Evaluation methodology in the REVIT project</title><categories>cs.CY</categories><comments>report on eLearning that describes the evaluation methodology
  followed in the case of the REVIT project (Revitalizing Remote Schools for
  LifeLong Distance elearning, http://revit.cti.gr, Lifelong Learning Programme
  / Transversal Programme / Key Activity 3: ICT -European Commission, EACEA,
  from January 2009 till December 2011)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The purpose of this technical report is to present a methodological approach
for evaluating the REVIT (http://revit.cti.gr) educational venture as a whole
and assessing the learning effectiveness of the resulting collaborative
e-courses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6707</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6707</id><created>2013-04-24</created><updated>2013-10-11</updated><authors><author><keyname>Mihal&#xe1;k</keyname><forenames>Mat&#xfa;&#x161;</forenames></author><author><keyname>&#x160;r&#xe1;mek</keyname><forenames>Rastislav</forenames></author><author><keyname>Widmayer</keyname><forenames>Peter</forenames></author></authors><title>Counting approximately-shortest paths in directed acyclic graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a directed acyclic graph with positive edge-weights, two vertices s and
t, and a threshold-weight L, we present a fully-polynomial time
approximation-scheme for the problem of counting the s-t paths of length at
most L. We extend the algorithm for the case of two (or more) instances of the
same problem. That is, given two graphs that have the same vertices and edges
and differ only in edge-weights, and given two threshold-weights L_1 and L_2,
we show how to approximately count the s-t paths that have length at most L_1
in the first graph and length at most L_2 in the second graph. We believe that
our algorithms should find application in counting approximate solutions of
related optimization problems, where finding an (optimum) solution can be
reduced to the computation of a shortest path in a purpose-built auxiliary
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.6709</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.6709</id><created>2013-04-24</created><authors><author><keyname>Sanderson</keyname><forenames>Robert</forenames></author><author><keyname>Ciccarese</keyname><forenames>Paolo</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author></authors><title>Designing the W3C Open Annotation Data Model</title><categories>cs.DL</categories><comments>10 pages, 13 figures, accepted to ACM Conference on Web Science 2013</comments><acm-class>H.5.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Open Annotation Core Data Model specifies an interoperable framework for
creating associations between related resources, called annotations, using a
methodology that conforms to the Architecture of the World Wide Web. Open
Annotations can easily be shared between platforms, with sufficient richness of
expression to satisfy complex requirements while remaining simple enough to
also allow for the most common use cases, such as attaching a piece of text to
a single web resource. This paper presents the W3C Open Annotation Community
Group specification and the rationale behind the scoping and technical
decisions that were made. It also motivates interoperable Annotations via use
cases, and provides a brief analysis of the advantages over previous
specifications.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="44000" completeListSize="102538">1122234|45001</resumptionToken>
</ListRecords>
</OAI-PMH>
