<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:47:32Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|77001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00387</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00387</id><created>2015-05-02</created><updated>2015-11-03</updated><authors><author><keyname>Srivastava</keyname><forenames>Rupesh Kumar</forenames></author><author><keyname>Greff</keyname><forenames>Klaus</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Highway Networks</title><categories>cs.LG cs.NE</categories><comments>6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop.
  Full paper is at arXiv:1507.06228</comments><msc-class>68T01</msc-class><acm-class>I.2.6; G.1.6</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  There is plenty of theoretical and empirical evidence that depth of neural
networks is a crucial ingredient for their success. However, network training
becomes more difficult with increasing depth and training of very deep networks
remains an open problem. In this extended abstract, we introduce a new
architecture designed to ease gradient-based training of very deep networks. We
refer to networks with this architecture as highway networks, since they allow
unimpeded information flow across several layers on &quot;information highways&quot;. The
architecture is characterized by the use of gating units which learn to
regulate the flow of information through a network. Highway networks with
hundreds of layers can be trained directly using stochastic gradient descent
and with a variety of activation functions, opening up the possibility of
studying extremely deep and efficient architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00388</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00388</id><created>2015-05-02</created><authors><author><keyname>Bun</keyname><forenames>Mark</forenames></author><author><keyname>Zhandry</keyname><forenames>Mark</forenames></author></authors><title>Order-Revealing Encryption and the Hardness of Private Learning</title><categories>cs.CR cs.CC cs.LG</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An order-revealing encryption scheme gives a public procedure by which two
ciphertexts can be compared to reveal the ordering of their underlying
plaintexts. We show how to use order-revealing encryption to separate
computationally efficient PAC learning from efficient $(\epsilon,
\delta)$-differentially private PAC learning. That is, we construct a concept
class that is efficiently PAC learnable, but for which every efficient learner
fails to be differentially private. This answers a question of Kasiviswanathan
et al. (FOCS '08, SIAM J. Comput. '11).
  To prove our result, we give a generic transformation from an order-revealing
encryption scheme into one with strongly correct comparison, which enables the
consistent comparison of ciphertexts that are not obtained as the valid
encryption of any message. We believe this construction may be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00389</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00389</id><created>2015-05-02</created><authors><author><keyname>Li</keyname><forenames>Zhaoxin</forenames></author><author><keyname>Wang</keyname><forenames>Kuanquan</forenames></author><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author><author><keyname>Meng</keyname><forenames>Deyu</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author></authors><title>Detail-preserving and Content-aware Variational Multi-view Stereo
  Reconstruction</title><categories>cs.CV</categories><comments>14 pages,16 figures. Submitted to IEEE Transaction on image
  processing</comments><doi>10.1109/TIP.2015.2507400</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate recovery of 3D geometrical surfaces from calibrated 2D multi-view
images is a fundamental yet active research area in computer vision. Despite
the steady progress in multi-view stereo reconstruction, most existing methods
are still limited in recovering fine-scale details and sharp features while
suppressing noises, and may fail in reconstructing regions with few textures.
To address these limitations, this paper presents a Detail-preserving and
Content-aware Variational (DCV) multi-view stereo method, which reconstructs
the 3D surface by alternating between reprojection error minimization and mesh
denoising. In reprojection error minimization, we propose a novel inter-image
similarity measure, which is effective to preserve fine-scale details of the
reconstructed surface and builds a connection between guided image filtering
and image registration. In mesh denoising, we propose a content-aware
$\ell_{p}$-minimization algorithm by adaptively estimating the $p$ value and
regularization parameters based on the current input. It is much more promising
in suppressing noise while preserving sharp features than conventional
isotropic mesh smoothing. Experimental results on benchmark datasets
demonstrate that our DCV method is capable of recovering more surface details,
and obtains cleaner and more accurate reconstructions than state-of-the-art
methods. In particular, our method achieves the best results among all
published methods on the Middlebury dino ring and dino sparse ring datasets in
terms of both completeness and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00391</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00391</id><created>2015-05-02</created><updated>2015-12-16</updated><authors><author><keyname>Lykouris</keyname><forenames>Thodoris</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Tardos</keyname><forenames>Eva</forenames></author></authors><title>Learning and Efficiency in Games with Dynamic Population</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the quality of outcomes in repeated games when the population of
players is dynamically changing and participants use learning algorithms to
adapt to the changing environment. Game theory classically considers Nash
equilibria of one-shot games, while in practice many games are players
repeatedly, and in such games players often use algorithmic tools to learn to
play in the given environment. Learning in repeated games has only been studied
when the population playing the game is stable over time.
  We analyze efficiency of repeated games in dynamically changing environments,
motivated by application domains such as packet routing and Internet
ad-auctions. We prove that, in many classes of games, if players choose their
strategies in a way that guarantees low adaptive regret, then high social
welfare is ensured, even under very frequent changes. This result extends
previous work, which showed high welfare for learning outcomes in stable
environments. A main technical tool for our analysis is the existence of a
solution to the welfare maximization problem that is both close to optimal and
relatively stable over time. Such a solution serves as a benchmark in the
efficiency analysis of learning outcomes. We show that such stable and
near-optimal solutions exist for many problems, even in cases when the exact
optimal solution can be very unstable. We develop direct techniques to show the
existence of a stable solution in some classes of games. Further, we show that
a sufficient condition for the existence of stable solutions is the existence
of a differentially private algorithm for the welfare maximization problem. We
demonstrate our techniques by focusing on three classes of games as examples:
simultaneous item auctions, bandwidth allocation mechanisms and congestion
games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00393</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00393</id><created>2015-05-03</created><updated>2015-07-23</updated><authors><author><keyname>Visin</keyname><forenames>Francesco</forenames></author><author><keyname>Kastner</keyname><forenames>Kyle</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Matteucci</keyname><forenames>Matteo</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>ReNet: A Recurrent Neural Network Based Alternative to Convolutional
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a deep neural network architecture for object
recognition based on recurrent neural networks. The proposed network, called
ReNet, replaces the ubiquitous convolution+pooling layer of the deep
convolutional neural network with four recurrent neural networks that sweep
horizontally and vertically in both directions across the image. We evaluate
the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and
SVHN. The result suggests that ReNet is a viable alternative to the deep
convolutional neural network, and that further investigation is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00396</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00396</id><created>2015-05-03</created><updated>2016-03-02</updated><authors><author><keyname>Basciftci</keyname><forenames>Y. Ozan</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Ashikhmin</keyname><forenames>Alexei</forenames></author></authors><title>Physical Layer Security in Massive MIMO</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a single-cell downlink massive MIMO communication in the presence
of an adversary capable of jamming and eavesdropping simultaneously. We show
that massive MIMO communication is naturally resilient to no training-phase
jamming attack in which the adversary jams only the data communication and
eavesdrops both the data communication and the training. Specifically, we show
that the secure degrees of freedom (DoF) attained in the presence of such an
attack is identical to the maximum DoF attained under no attack. Further, we
evaluate the number of antennas that base station (BS) requires in order to
establish information theoretic security without even a need for Wyner
encoding. Next, we show that things are completely different once the adversary
starts jamming the training phase. Specifically, we consider an attack, called
training-phase jamming in which the adversary jams and eavesdrops both the
training and the data communication. We show that under such an attack, the
maximum secure DoF is equal to zero. Furthermore, the maximum achievable rates
of users vanish even in the asymptotic regime in the number of BS antennas. To
counter this attack, we develop a defense strategy in which we use a secret key
to encrypt the pilot sequence assignments to hide them from the adversary,
rather than encrypt the data. We show that, if the cardinality of the set of
pilot signals are scaled appropriately, hiding the pilot signal assignments
from the adversary enables the users to achieve secure DoF, identical to the
maximum achievable DoF under no attack. Finally, we discuss how computational
cryptography is a legitimate candidate to hide the pilot signal assignments.
Indeed, while information theoretic security is not achieved with cryptography,
the computational power necessary for the adversary to achieve a non-zero
mutual information leakage rate goes to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00398</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00398</id><created>2015-05-03</created><authors><author><keyname>Wang</keyname><forenames>Ruoxi</forenames></author><author><keyname>Li</keyname><forenames>Yingzhou</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author><author><keyname>Darve</keyname><forenames>Eric</forenames></author></authors><title>Structured Block Basis Factorization for Scalable Kernel Matrix
  Evaluation</title><categories>stat.ML cs.LG cs.NA</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel matrices are popular in machine learning and scientific computing, but
they are limited by their quadratic complexity in both construction and
storage. It is well-known that as one varies the kernel parameter, e.g., the
width parameter in radial basis function kernels, the kernel matrix changes
from a smooth low-rank kernel to a diagonally-dominant and then fully-diagonal
kernel. Low-rank approximation methods have been widely-studied, mostly in the
first case, to reduce the memory storage and the cost of computing
matrix-vector products. Here, we use ideas from scientific computing to propose
an extension of these methods to situations where the matrix is not
well-approximated by a low-rank matrix. In particular, we construct an
efficient block low-rank approximation method---which we call the Block Basis
Factorization---and we show that it has $\mathcal{O}(n)$ complexity in both
time and memory. Our method works for a wide range of kernel parameters,
extending the domain of applicability of low-rank approximation methods, and
our empirical results demonstrate the stability (small standard deviation in
error) and superiority over current state-of-art kernel approximation
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00399</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00399</id><created>2015-05-03</created><authors><author><keyname>Lin</keyname><forenames>Christopher H.</forenames></author><author><keyname>Kolobov</keyname><forenames>Andrey</forenames></author><author><keyname>Kamar</keyname><forenames>Ece</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric</forenames></author></authors><title>Metareasoning for Planning Under Uncertainty</title><categories>cs.AI</categories><comments>Extended version of IJCAI 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional model for online planning under uncertainty assumes that an
agent can stop and plan without incurring costs for the time spent planning.
However, planning time is not free in most real-world settings. For example, an
autonomous drone is subject to nature's forces, like gravity, even while it
thinks, and must either pay a price for counteracting these forces to stay in
place, or grapple with the state change caused by acquiescing to them. Policy
optimization in these settings requires metareasoning---a process that trades
off the cost of planning and the potential policy improvement that can be
achieved. We formalize and analyze the metareasoning problem for Markov
Decision Processes (MDPs). Our work subsumes previously studied special cases
of metareasoning and shows that in the general case, metareasoning is at most
polynomially harder than solving MDPs with any given algorithm that disregards
the cost of thinking. For reasons we discuss, optimal general metareasoning
turns out to be impractical, motivating approximations. We present approximate
metareasoning procedures which rely on special properties of the BRTDP planning
algorithm and explore the effectiveness of our methods on a variety of
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00401</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00401</id><created>2015-05-03</created><authors><author><keyname>Powers</keyname><forenames>David M. W.</forenames></author></authors><title>Visualization of Tradeoff in Evaluation: from Precision-Recall &amp; PN to
  LIFT, ROC &amp; BIRD</title><categories>cs.LG cs.AI cs.IR stat.ME stat.ML</categories><comments>23 pages, 12 equations, 2 figures, 2 tables, 1 sidebar</comments><report-no>KIT-14-002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation often aims to reduce the correctness or error characteristics of a
system down to a single number, but that always involves trade-offs. Another
way of dealing with this is to quote two numbers, such as Recall and Precision,
or Sensitivity and Specificity. But it can also be useful to see more than
this, and a graphical approach can explore sensitivity to cost, prevalence,
bias, noise, parameters and hyper-parameters.
  Moreover, most techniques are implicitly based on two balanced classes, and
our ability to visualize graphically is intrinsically two dimensional, but we
often want to visualize in a multiclass context. We review the dichotomous
approaches relating to Precision, Recall, and ROC as well as the related LIFT
chart, exploring how they handle unbalanced and multiclass data, and deriving
new probabilistic and information theoretic variants of LIFT that help deal
with the issues associated with the handling of multiple and unbalanced
classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00412</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00412</id><created>2015-05-03</created><authors><author><keyname>Galiano</keyname><forenames>Gonzalo</forenames></author><author><keyname>Velasco</keyname><forenames>Juli&#xe1;n</forenames></author></authors><title>On a fast bilateral filtering formulation using functional
  rearrangements</title><categories>cs.CV</categories><comments>29 pages, Journal of Mathematical Imaging and Vision, 2015. arXiv
  admin note: substantial text overlap with arXiv:1406.7128</comments><msc-class>68U10</msc-class><doi>10.1007/s10851-015-0583-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an exact reformulation of a broad class of neighborhood filters,
among which the bilateral filters, in terms of two functional rearrangements:
the decreasing and the relative rearrangements.
  Independently of the image spatial dimension (one-dimensional signal, image,
volume of images, etc.), we reformulate these filters as integral operators
defined in a one-dimensional space corresponding to the level sets measures.
  We prove the equivalence between the usual pixel-based version and the
rearranged version of the filter. When restricted to the discrete setting, our
reformulation of bilateral filters extends previous results for the so-called
fast bilateral filtering. We, in addition, prove that the solution of the
discrete setting, understood as constant-wise interpolators, converges to the
solution of the continuous setting.
  Finally, we numerically illustrate computational aspects concerning quality
approximation and execution time provided by the rearranged formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00422</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00422</id><created>2015-05-03</created><authors><author><keyname>Erjomina</keyname><forenames>Irina</forenames></author><author><keyname>Rozentsvaig</keyname><forenames>Aleksandr</forenames></author><author><keyname>Ziatdinov</keyname><forenames>Rushan</forenames></author></authors><title>Server component installation and testing of the university information
  and educational environment on the Moodle LMS platform</title><categories>cs.OH</categories><comments>in Russian</comments><journal-ref>Izvestiya Sochi State University, Vol. 34, No. 1, Apr. 2015, pp.
  24-32</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The informational educational environment (IEE) of an institution is a
complex multilevel system which, along with methodical, organizational and
cultural resources, accumulates the intellectual and technical potential of a
university, as well as the informative and activity components of the learners
and teachers. In practice, the formation of IEE is actually based on the
creation of information technologies and their integration into the existing
educational environment of the institution. The management of this system is
carried out using specialized equipment and software. For the successful
formation and operation of IEE, in the present work we review software products
that form the basis of the organization of interactive and web interactions
between students, teachers and all participants of the educational process. We
analyse the technical capabilities that have provided users with IEE services
such as the Apache web server with connected modules PHP, MySQL, the Java
virtual machine and the Red5 server. We demonstrate the possibility of
obtaining results from the interaction of these products, and reports on users'
work in webinars, video conferences and web conferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00423</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00423</id><created>2015-05-03</created><authors><author><keyname>Grabocka</keyname><forenames>Josif</forenames></author><author><keyname>Schilling</keyname><forenames>Nicolas</forenames></author><author><keyname>Schmidt-Thieme</keyname><forenames>Lars</forenames></author></authors><title>Optimal Time-Series Motifs</title><categories>cs.AI cs.LG</categories><comments>Submitted to KDD2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motifs are the most repetitive/frequent patterns of a time-series. The
discovery of motifs is crucial for practitioners in order to understand and
interpret the phenomena occurring in sequential data. Currently, motifs are
searched among series sub-sequences, aiming at selecting the most frequently
occurring ones. Search-based methods, which try out series sub-sequence as
motif candidates, are currently believed to be the best methods in finding the
most frequent patterns.
  However, this paper proposes an entirely new perspective in finding motifs.
We demonstrate that searching is non-optimal since the domain of motifs is
restricted, and instead we propose a principled optimization approach able to
find optimal motifs. We treat the occurrence frequency as a function and
time-series motifs as its parameters, therefore we \textit{learn} the optimal
motifs that maximize the frequency function. In contrast to searching, our
method is able to discover the most repetitive patterns (hence optimal), even
in cases where they do not explicitly occur as sub-sequences. Experiments on
several real-life time-series datasets show that the motifs found by our method
are highly more frequent than the ones found through searching, for exactly the
same distance threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00424</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00424</id><created>2015-05-03</created><authors><author><keyname>P&#x142;o&#x144;ski</keyname><forenames>Piotr</forenames></author><author><keyname>Stefan</keyname><forenames>Dorota</forenames></author><author><keyname>Sulej</keyname><forenames>Robert</forenames></author><author><keyname>Zaremba</keyname><forenames>Krzysztof</forenames></author></authors><title>Electron Neutrino Classification in Liquid Argon Time Projection Chamber
  Detector</title><categories>cs.CV physics.ins-det</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neutrinos are one of the least known elementary particles. The detection of
neutrinos is an extremely difficult task since they are affected only by weak
sub-atomic force or gravity. Therefore large detectors are constructed to
reveal neutrino's properties. Among them the Liquid Argon Time Projection
Chamber (LAr-TPC) detectors provide excellent imaging and particle
identification ability for studying neutrinos. The computerized methods for
automatic reconstruction and identification of particles are needed to fully
exploit the potential of the LAr-TPC technique. Herein, the novel method for
electron neutrino classification is presented. The method constructs a feature
descriptor from images of observed event. It characterizes the signal
distribution propagated from vertex of interest, where the particle interacts
with the detector medium. The classifier is learned with a constructed feature
descriptor to decide whether the images represent the electron neutrino or
cascade produced by photons. The proposed approach assumes that the position of
primary interaction vertex is known. The method's performance in dependency to
the noise in a primary vertex position and deposited energy of particles is
studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00426</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00426</id><created>2015-05-03</created><authors><author><keyname>Shen</keyname><forenames>Juei-Chin</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Kwang-Cheng</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>High-Dimensional CSI Acquisition in Massive MIMO: Sparsity-Inspired
  Approaches</title><categories>cs.IT math.IT</categories><comments>15 pages, 3 figures, 1 table, submitted to IEEE Systems Journal
  Special Issue on 5G Wireless Systems with Massive MIMO</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO has been regarded as one of the key technologies for 5G wireless
networks, as it can significantly improve both the spectral efficiency and
energy efficiency. The availability of high-dimensional channel side
information (CSI) is critical for its promised performance gains, but the
overhead of acquiring CSI may potentially deplete the available radio
resources. Fortunately, it has recently been discovered that harnessing various
sparsity structures in massive MIMO channels can lead to significant overhead
reduction, and thus improve the system performance. This paper presents and
discusses the use of sparsity-inspired CSI acquisition techniques for massive
MIMO, as well as the underlying mathematical theory. Sparsity-inspired
approaches for both frequency-division duplexing and time-division duplexing
massive MIMO systems will be examined and compared from an overall system
perspective, including the design trade-offs between the two duplexing modes,
computational complexity of acquisition algorithms, and applicability of
sparsity structures. Meanwhile, some future prospects for research on
high-dimensional CSI acquisition to meet practical demands will be identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00429</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00429</id><created>2015-05-03</created><authors><author><keyname>Gargiulo</keyname><forenames>Floriana</forenames></author><author><keyname>Gandica</keyname><forenames>Yerali</forenames></author><author><keyname>Carletti</keyname><forenames>Timoteo</forenames></author></authors><title>Urban skylines from Schelling model</title><categories>physics.soc-ph cs.CY nlin.AO</categories><comments>16 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a metapopulation version of the Schelling model where two kinds of
agents relocate themselves, with unconstrained destination, if their local
fitness is lower than a tolerance threshold. We show that, for small values of
the latter, the population redistributes highly heterogeneously among the
available places. The system thus stabilizes on these heterogeneous skylines
after a long quasi-stationary transient period, during which the population
remains in a well mixed phase. Varying the tolerance passing from large to
small values, we identify three possible global regimes: microscopic clusters
with local coexistence of both kinds of agents, macroscopic clusters with local
coexistence (soft segregation), macroscopic clusters with local segregation but
homogeneous densities (hard segregation). The model is studied numerically and
complemented with an analytical study in the limit of extremely large node
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00430</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00430</id><created>2015-05-03</created><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>A Unified Stability Analysis Approach for a Class of Interconnected
  System</title><categories>cs.SY</categories><comments>6 pages,1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the structural perspective, this paper investigates a new formulation of
the concept of input-to-state stability (ISS), and based on this formulation,
proposes a new stability analysis approach for a class of interconnected
system. The new formulation of ISS is better able to reflect the tendency of
the state $x(t)$ tracking the input $u(t)$ and weakens the conservative of the
original form. The stability analysis method which transforms the
interconnected system into the equivalent cascade form, does not depend on the
Lyapunov function, breaks through the limitation of the small-gain theorem and
extends the application of ISS. As its applications in three typical kinds of
interconnected systems, this method is used to prove the small-gain theorem
again and analyzes the stability of a class of interconnected system and the
consensus of the multi-agent system (MAS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00432</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00432</id><created>2015-05-03</created><authors><author><keyname>Fernando</keyname><forenames>Basura</forenames></author><author><keyname>Karaoglu</keyname><forenames>Sezer</forenames></author><author><keyname>Saha</keyname><forenames>Sajib Kumar</forenames></author></authors><title>Object Class Detection and Classification using Multi Scale Gradient and
  Corner Point based Shape Descriptors</title><categories>cs.CV</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel multi scale gradient and a corner point based
shape descriptors. The novel multi scale gradient based shape descriptor is
combined with generic Fourier descriptors to extract contour and region based
shape information. Shape information based object class detection and
classification technique with a random forest classifier has been optimized.
Proposed integrated descriptor in this paper is robust to rotation, scale,
translation, affine deformations, noisy contours and noisy shapes. The new
corner point based interpolated shape descriptor has been exploited for fast
object detection and classification with higher accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00437</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00437</id><created>2015-05-03</created><updated>2016-02-24</updated><authors><author><keyname>Hoy</keyname><forenames>Darrell</forenames></author><author><keyname>Nekipelov</keyname><forenames>Denis</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Robust Data-Driven Guarantees in Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of welfare in auctions comes traditionally via one of two
approaches: precise but fragile inference of the exact details of a setting
from data or robust but coarse theoretical price of anarchy bounds that hold in
any setting. As markets get more and more dynamic and bidders become more and
more sophisticated, the weaknesses of each approach are magnified.
  In this paper, we provide tools for analyzing and estimating the empirical
price of anarchy of an auction. The empirical price of anarchy is the worst
case efficiency loss of any auction that could have produced the data, relative
to the optimal.
  Our techniques are based on inferring simple properties of auctions:
primarily the expected revenue and the expected payments and allocation
probabilities from possible bids. These quantities alone allow us to
empirically estimate the revenue covering parameter of an auction which allows
us to re-purpose the theoretical machinery of \citet{HHT14} for empirical
purposes. Moreover, we show that under general conditions the revenue covering
parameter estimated from the data approaches the true parameter with the error
decreasing at the rate proportional to the square root of the number of
auctions and at most polynomially in the number of agents.
  Finally, we apply our techniques to a selection of advertising auctions on
Microsoft's Bing and find empirical results that are a significant improvement
over the theoretical worst-case bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00438</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00438</id><created>2015-05-03</created><authors><author><keyname>Yang</keyname><forenames>Jianjun</forenames></author><author><keyname>Shen</keyname><forenames>Ju</forenames></author></authors><title>Leading Undergraduate Students to Big Data Generation</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People are facing a flood of data today. Data are being collected at
unprecedented scale in many areas, such as networking, image processing,
virtualization, scientific computation, and algorithms. The huge data nowadays
are called Big Data. Big data is an all encompassing term for any collection of
data sets so large and complex that it becomes difficult to process them using
traditional data processing applications. In this article, the authors present
a unique way which uses network simulator and tools of image processing to
train students abilities to learn, analyze, manipulate, and apply Big Data.
Thus they develop students handson abilities on Big Data and their critical
thinking abilities. The authors used novel image based rendering algorithm with
user intervention to generate realistic 3D virtual world. The learning outcomes
are significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00444</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00444</id><created>2015-05-03</created><authors><author><keyname>Luttrell</keyname><forenames>Stephen</forenames></author></authors><title>Some Theoretical Properties of a Network of Discretely Firing Neurons</title><categories>cs.NE</categories><comments>14 pages</comments><acm-class>I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of optimising a network of discretely firing neurons is
addressed. An objective function is introduced which measures the average
number of bits that are needed for the network to encode its state. When this
is minimised, it is shown that this leads to a number of results, such as
topographic mappings, piecewise linear dependence on the input of the
probability of a neuron firing, and factorial encoder networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00447</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00447</id><created>2015-05-03</created><authors><author><keyname>Turri</keyname><forenames>Valerio</forenames></author><author><keyname>Besselink</keyname><forenames>Bart</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Cooperative look-ahead control for fuel-efficient and safe heavy-duty
  vehicle platooning</title><categories>cs.SY</categories><comments>16 pages, 16 figures, submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The operation of groups of heavy-duty vehicles (HDVs) at a small
inter-vehicular distance (known as platoon) allows to lower the overall
aerodynamic drag and, therefore, to reduce fuel consumption and greenhouse gas
emissions. However, due to the large mass and limited engine power of HDVs,
slopes have a significant impact on the feasible and optimal speed profiles
that each vehicle can and should follow. Therefore maintaining a short
inter-vehicular distance as required by platooning without coordination between
vehicles can often result in inefficient or even unfeasible trajectories. In
this paper we propose a two-layer control architecture for HDV platooning aimed
to safely and fuel-efficiently coordinate the vehicles in the platoon. Here,
the layers are responsible for the inclusion of preview information on road
topography and the real-time control of the vehicles, respectively. Within this
architecture, dynamic programming is used to compute the fuel-optimal speed
profile for the entire platoon and a distributed model predictive control
framework is developed for the real-time control of the vehicles. The
effectiveness of the proposed controller is analyzed by means of simulations of
several realistic scenarios that suggest a possible fuel saving of up to 12%
for the follower vehicles compared to the use of standard platoon controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00449</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00449</id><created>2015-05-03</created><authors><author><keyname>Jin</keyname><forenames>Yan</forenames></author><author><keyname>Hamiez</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author></authors><title>Algorithms for the minimum sum coloring problem: a review</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Minimum Sum Coloring Problem (MSCP) is a generalization of the well-known
vertex coloring problem. Due to its theoretical and practical relevance, the
MSCP attracts increasing attention. The only existing review on the problem
dates back to 2004 and mainly covers the history of the MSCP and the
theoretical developments on specific graphs. In recent years, the field has
witnessed significant progresses on practical solution algorithms. The purpose
of this review is to provide a comprehensive inspection of the most recent and
representative MSCP algorithms. To be informative, we identify the general
framework followed by these algorithms and the key ingredients that make them
successful. By classifying the main search strategies and putting forward the
critical elements of the reviewed methods, we wish to encourage future
development of more powerful methods and motivate new applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00457</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00457</id><created>2015-05-03</created><updated>2015-12-13</updated><authors><author><keyname>Gupta</keyname><forenames>Yayati</forenames></author><author><keyname>Saxena</keyname><forenames>Akrati</forenames></author><author><keyname>Das</keyname><forenames>Debarati</forenames></author><author><keyname>Iyengar</keyname><forenames>S. R. S.</forenames></author></authors><title>Modeling Memetics using Edge Diversity</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of meme propagation and the prediction of meme trajectory are
emerging areas of interest in the field of complex networks research. In
addition to the properties of the meme itself, the structural properties of the
underlying network decides the speed and the trajectory of the propagating
meme. In this paper, we provide an artificial framework for studying the meme
propagation patterns. Firstly, the framework includes a synthetic network which
simulates a real world network and acts as a testbed for meme simulation.
Secondly, we propose a meme spreading model based on the diversity of edges in
the network. Through the experiments conducted, we show that the generated
synthetic network combined with the proposed spreading model is able to
simulate a real world meme spread. Our proposed model is validated by the
propagation of the Higgs boson meme on Twitter as well as many real world
social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00463</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00463</id><created>2015-05-03</created><authors><author><keyname>Kumbhkar</keyname><forenames>Ratnesh</forenames></author><author><keyname>Kuber</keyname><forenames>Tejashri</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author><author><keyname>Seskar</keyname><forenames>Ivan</forenames></author></authors><title>Opportunistic Spectrum Allocation for Max-Min Rate in NC-OFDMA</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We envision a scenario of opportunistic spectrum access among multiple links
when the available spectrum is not contiguous due to the presence of external
interference sources. Non-contiguous Orthogonal Frequency Division Multiplexing
(NC-OFDM) is a promising technique to utilize such disjoint frequency bands in
an efficient manner. In this paper we study the problem of fair spectrum
allocation across multiple NCOFDM-enabled point-to-point cognitive radio links
under certain practical considerations that arise from such non-contiguous
access. When using NC-OFDMA, the channels allocated to a cognitive link are
spread across several disjoint frequency bands leading to a large spectral span
for that link. Increased spectral span requires higher sampling rates, leading
to increased power consumption in the ADC/DAC of the transmit/receive nodes. In
this context, this paper proposes a spectrum allocation that maximizes the
minimum rate achieved by the cognitive radio links, under a constraint on the
maximum permissible spectral span. Under constant transmit powers and
orthogonal spectrum allocation, such an optimization is a mixed-integer linear
program and can be solved efficiently. There exists a clear trade-off between
the max-min rate achieved and the maximum permissible spectral span. The
spectral allocation obtained from the proposed optimization framework is shown
to be close to the tradeoff boundary, thus showing the effectiveness of the
proposed technique. We find that it is possible to limit the spectrum span
without incurring a significant penalty on the max-min rate under different
interference environments. We also discuss an experimental evaluation of the
techniques developed here using the Universal Software Radio Peripheral (USRP)
enabled ORBIT radio network testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00468</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00468</id><created>2015-05-03</created><updated>2016-03-07</updated><authors><author><keyname>Antol</keyname><forenames>Stanislaw</forenames></author><author><keyname>Agrawal</keyname><forenames>Aishwarya</forenames></author><author><keyname>Lu</keyname><forenames>Jiasen</forenames></author><author><keyname>Mitchell</keyname><forenames>Margaret</forenames></author><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Parikh</keyname><forenames>Devi</forenames></author></authors><title>VQA: Visual Question Answering</title><categories>cs.CL cs.CV</categories><comments>International Conference on Computer Vision (ICCV) 2015. Updates in
  this version: Improved model, &quot;nearest neighbor&quot; and &quot;most popular answer per
  question type&quot; baseline results on test-standard, clarification about the age
  annotations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the task of free-form and open-ended Visual Question Answering
(VQA). Given an image and a natural language question about the image, the task
is to provide an accurate natural language answer. Mirroring real-world
scenarios, such as helping the visually impaired, both the questions and
answers are open-ended. Visual questions selectively target different areas of
an image, including background details and underlying context. As a result, a
system that succeeds at VQA typically needs a more detailed understanding of
the image and complex reasoning than a system producing generic image captions.
Moreover, VQA is amenable to automatic evaluation, since many open-ended
answers contain only a few words or a closed set of answers that can be
provided in a multiple-choice format. We provide a dataset containing ~0.25M
images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the
information it provides. Numerous baselines for VQA are provided and compared
with human performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00477</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00477</id><created>2015-05-03</created><authors><author><keyname>Langone</keyname><forenames>Rocco</forenames></author><author><keyname>Mall</keyname><forenames>Raghvendra</forenames></author><author><keyname>Alzate</keyname><forenames>Carlos</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author></authors><title>Kernel Spectral Clustering and applications</title><categories>cs.LG stat.ML</categories><comments>chapter contribution to the book &quot;Unsupervised Learning Algorithms&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter we review the main literature related to kernel spectral
clustering (KSC), an approach to clustering cast within a kernel-based
optimization setting. KSC represents a least-squares support vector machine
based formulation of spectral clustering described by a weighted kernel PCA
objective. Just as in the classifier case, the binary clustering model is
expressed by a hyperplane in a high dimensional space induced by a kernel. In
addition, the multi-way clustering can be obtained by combining a set of binary
decision functions via an Error Correcting Output Codes (ECOC) encoding scheme.
Because of its model-based nature, the KSC method encompasses three main steps:
training, validation, testing. In the validation stage model selection is
performed to obtain tuning parameters, like the number of clusters present in
the data. This is a major advantage compared to classical spectral clustering
where the determination of the clustering parameters is unclear and relies on
heuristics. Once a KSC model is trained on a small subset of the entire data,
it is able to generalize well to unseen test points. Beyond the basic
formulation, sparse KSC algorithms based on the Incomplete Cholesky
Decomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization are
reviewed. In that respect, we show how it is possible to handle large scale
data. Also, two possible ways to perform hierarchical clustering and a soft
clustering method are presented. Finally, real-world applications such as image
segmentation, power load time-series clustering, document clustering and big
data learning are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00478</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00478</id><created>2015-05-03</created><authors><author><keyname>Endrullis</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Zantema</keyname><forenames>Hans</forenames></author></authors><title>Proving Looping and Non-Looping Non-Termination by Finite Automata</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new technique is presented to prove non-termination of term rewriting. The
basic idea is to find a non-empty regular language of terms that is closed
under rewriting and does not contain normal forms. It is automated by
representing the language by a tree automaton with a fixed number of states,
and expressing the mentioned requirements in a SAT formula. Satisfiability of
this formula implies non-termination. Our approach succeeds for many examples
where all earlier techniques fail, for instance for the S-rule from combinatory
logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00482</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00482</id><created>2015-05-03</created><authors><author><keyname>Azizyan</keyname><forenames>Martin</forenames></author><author><keyname>Chen</keyname><forenames>Yen-Chi</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Risk Bounds For Mode Clustering</title><categories>math.ST cs.LG stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Density mode clustering is a nonparametric clustering method. The clusters
are the basins of attraction of the modes of a density estimator. We study the
risk of mode-based clustering. We show that the clustering risk over the
cluster cores --- the regions where the density is high --- is very small even
in high dimensions. And under a low noise condition, the overall cluster risk
is small even beyond the cores, in high dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00484</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00484</id><created>2015-05-03</created><updated>2015-12-21</updated><authors><author><keyname>Mo</keyname><forenames>Jianhua</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Limited Feedback in Multiple-Antenna Systems with One-Bit Quantization</title><categories>cs.IT math.IT</categories><comments>Asilomar Conference on Signals, Systems, and Computers 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication systems with low-resolution analog-to-digital-converters (ADCs)
can exploit channel state information at the transmitter (CSIT) and receiver.
This paper presents initial results on codebook design and performance analysis
for limited feedback systems with one-bit ADCs. Different from the
high-resolution case, the absolute phase at the receiver is important to align
the phase of the received signals when the received signal is sliced by one-bit
ADCs. A new codebook design for the beamforming case is proposed that
separately quantizes the channel direction and the residual phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00487</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00487</id><created>2015-05-03</created><updated>2015-10-19</updated><authors><author><keyname>Venugopalan</keyname><forenames>Subhashini</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Mooney</keyname><forenames>Raymond</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>Sequence to Sequence -- Video to Text</title><categories>cs.CV</categories><comments>ICCV 2015 camera-ready. Includes code, project page and LSMDC
  challenge results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world videos often have complex dynamics; and methods for generating
open-domain video descriptions should be sensitive to temporal structure and
allow both input (sequence of frames) and output (sequence of words) of
variable length. To approach this problem, we propose a novel end-to-end
sequence-to-sequence model to generate captions for videos. For this we exploit
recurrent neural networks, specifically LSTMs, which have demonstrated
state-of-the-art performance in image caption generation. Our LSTM model is
trained on video-sentence pairs and learns to associate a sequence of video
frames to a sequence of words in order to generate a description of the event
in the video clip. Our model naturally is able to learn the temporal structure
of the sequence of frames as well as the sequence model of the generated
sentences, i.e. a language model. We evaluate several variants of our model
that exploit different visual features on a standard set of YouTube videos and
two movie description datasets (M-VAD and MPII-MD).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00489</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00489</id><created>2015-05-03</created><authors><author><keyname>Silina</keyname><forenames>Yulia</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author></authors><title>The Distant Heart: Mediating Long-Distance Relationships through
  Connected Computational Jewelry</title><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the world where increasingly mobility and long-distance relationships with
family, friends and loved-ones became commonplace, there exists a gap in
intimate interpersonal communication mediated by technology. Considering the
advances in the field of mediation of relationships through technology, as well
as prevalence of use of jewelry as love-tokens for expressing a wish to be
remembered and to evoke the presence of the loved-one, developments in the new
field of computational jewelry offer some truly exciting possibilities. In this
paper we investigate the role that the jewelry-like form factor of prototypes
can play in the context of studying effects of computational jewelry in
mediating long-distance relationships.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00494</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00494</id><created>2015-05-03</created><authors><author><keyname>Movahed</keyname><forenames>Amin</forenames></author><author><keyname>Reed</keyname><forenames>Mark C.</forenames></author><author><keyname>Tajbakhsh</keyname><forenames>Shahriar Etemadi</forenames></author></authors><title>EXIT Chart Analysis of Turbo Compressed Sensing Using Message Passing
  De-Quantization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an iterative decoding method, which we call turbo-CS, for the
reception of concatenated source-channel encoded sparse signals transmitted
over an AWGN channel. The turbo-CS encoder applies 1-bit compressed sensing as
a source encoder concatenated serially with a convolutional channel encoder. At
the turbo-CS decoder, an iterative joint source-channel decoding method is
proposed for signal reconstruction. We analyze, for the first time, the
convergence of turbo-CS decoder by determining an EXIT chart of the constituent
decoders. We modify the soft-outputs of the decoder to improve the signal
reconstruction performance of turbo-CS decoder. For a fixed signal
reconstruction performance RSNR of 10 dB, we achieve more than 5 dB of
improvement in the channel SNR after 6 iterations of the turbo-CS.
Alternatively, for a fixed SNR of -1 dB, we achieve a 10 dB improvement in
RSNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00496</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00496</id><created>2015-05-03</created><authors><author><keyname>Badruddin</keyname><forenames>S. Ahsan</forenames></author><author><keyname>Ali</keyname><forenames>S. M. Dildar</forenames></author></authors><title>Recent Developments in the Optimization of Space Robotics for Perception
  in Planetary Exploration</title><categories>cs.RO</categories><comments>12 pages, Presented in the International Conference on Space - 2014,
  Proceedings of the International Conference on Space - 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The following paper reviews recent developments in the field of optimization
of space robotics. The extent of focus of this paper is on the perception
(robotic sense of analyzing surroundings) in space robots in the exploration of
extra-terrestrial planets. Robots play a crucial role in exploring
extra-terrestrial and planetary bodies. Their advantages are far from being
counted on finger tips. With the advent of autonomous robots in the field of
robotics, the role for space exploration has further hustled up. Optimization
of such autonomous robots has turned into a necessity of the hour. Optimized
robots tend to have a superior role in space exploration. With so many
considerations to monitor, an optimized solution will nevertheless help a
planetary rover perform better under tight circumstances. Keeping in view the
above mentioned area, the paper describes recent developments in the
optimization of autonomous extra-terrestrial rovers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00506</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00506</id><created>2015-05-03</created><authors><author><keyname>Dorogush</keyname><forenames>Elena G.</forenames></author><author><keyname>Kurzhanskiy</keyname><forenames>Alex A.</forenames></author></authors><title>Modeling Toll Lanes and Dynamic Pricing Control</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of dynamic pricing for toll lanes on
freeways. The proposed toll mechanism is broken up into two parts: (1) the
supply side feedback control that computes the desired split ratios for the
incoming traffic flows between the general purpose and the toll lanes; and (2)
the demand side price setting algorithm that aims to enforce the computed split
ratios.
  The split ratio controller is designed and tested in the context of the
link-node Cell Transmission Model with the modified node model of in/out flow
distribution. The equilibrium structure of this traffic model is presented; and
the case, in which the existence of a toll lane is meaningful, is discussed.
  For the price setting, two alternative approaches are presented. The first
one is commonly used, and it relies on the known Value of Time (VoT)
distribution. Its shortcoming, however, is in the difficulty of the VoT
distribution estimation. The second approach employs the auction mechanism,
where travelers make bids on places in the toll lane. The advantage of this
approach is that it enables direct control over how many vehicles will be
allowed into the toll lane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00508</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00508</id><created>2015-05-03</created><updated>2015-07-27</updated><authors><author><keyname>Boodaghians</keyname><forenames>Shant</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author></authors><title>The Combinatorial World (of Auctions) According to GARP</title><categories>cs.GT</categories><doi>10.1007/978-3-662-48433-3_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Revealed preference techniques are used to test whether a data set is
compatible with rational behaviour. They are also incorporated as constraints
in mechanism design to encourage truthful behaviour in applications such as
combinatorial auctions. In the auction setting, we present an efficient
combinatorial algorithm to find a virtual valuation function with the optimal
(additive) rationality guarantee. Moreover, we show that there exists such a
valuation function that both is individually rational and is minimum (that is,
it is component-wise dominated by any other individually rational, virtual
valuation function that approximately fits the data). Similarly, given upper
bound constraints on the valuation function, we show how to fit the maximum
virtual valuation function with the optimal additive rationality guarantee. In
practice, revealed preference bidding constraints are very demanding. We
explain how approximate rationality can be used to create relaxed revealed
preference constraints in an auction. We then show how combinatorial methods
can be used to implement these relaxed constraints. Worst/best-case welfare
guarantees that result from the use of such mechanisms can be quantified via
the minimum/maximum virtual valuation function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00511</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00511</id><created>2015-05-03</created><authors><author><keyname>Uchoa</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Healy</keyname><forenames>Cornelius T.</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Iterative Detection and Decoding Algorithms using LDPC Codes for MIMO
  Systems in Block-Fading Channels</title><categories>cs.IT math.IT</categories><comments>17 pages, 4 figures, IEEE Transactions on Vehicular Technology, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose iterative detection and decoding (IDD) algorithms with Low-Density
Parity-Check (LDPC) codes for Multiple Input Multiple Output (MIMO) systems
operating in block-fading and fast Rayleigh fading channels. Soft-input
soft-output minimum mean-square error receivers with successive interference
cancellation are considered. In particular, we devise a novel strategy to
improve the bit error rate (BER) performance of IDD schemes, which takes into
account the soft \textit{a posteriori} output of the decoder in a block-fading
channel when Root-Check LDPC codes are used. A MIMO IDD receiver with soft
information processing that exploits the code structure and the behavior of the
log likelihood ratios is also developed. Moreover, we present a scheduling
algorithm for decoding LDPC codes in block-fading channels. Simulations show
that the proposed techniques result in significant gains in terms of BER for
both block-fading and fast-fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00519</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00519</id><created>2015-05-03</created><authors><author><keyname>Summers</keyname><forenames>Cameron</forenames></author><author><keyname>Popp</keyname><forenames>Phillip</forenames></author></authors><title>Large Scale Discovery of Seasonal Music From User Data</title><categories>cs.IR cs.MM</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The consumption history of online media content such as music and video
offers a rich source of data from which to mine information. Trends in this
data are of particular interest because they reflect user preferences as well
as associated cultural contexts that can be exploited in systems such as
recommendation or search. This paper classifies songs as seasonal using a
large, real-world dataset of user listening data. Results show strong
performance of classification of Christmas music with Gaussian Mixture Models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00521</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00521</id><created>2015-05-04</created><updated>2016-01-12</updated><authors><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author></authors><title>Reinforcement Learning Neural Turing Machines - Revised</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Neural Turing Machine (NTM) is more expressive than all previously
considered models because of its external memory. It can be viewed as a broader
effort to use abstract external Interfaces and to learn a parametric model that
interacts with them.
  The capabilities of a model can be extended by providing it with proper
Interfaces that interact with the world. These external Interfaces include
memory, a database, a search engine, or a piece of software such as a theorem
verifier. Some of these Interfaces are provided by the developers of the model.
However, many important existing Interfaces, such as databases and search
engines, are discrete.
  We examine feasibility of learning models to interact with discrete
Interfaces. We investigate the following discrete Interfaces: a memory Tape, an
input Tape, and an output Tape. We use a Reinforcement Learning algorithm to
train a neural network that interacts with such Interfaces to solve simple
algorithmic tasks. Our Interfaces are expressive enough to make our model
Turing complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00523</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00523</id><created>2015-05-04</created><authors><author><keyname>Chong</keyname><forenames>Yong Shean</forenames></author><author><keyname>Tay</keyname><forenames>Yong Haur</forenames></author></authors><title>Modeling Representation of Videos for Anomaly Detection using Deep
  Learning: A Review</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1411.4423 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This review article surveys the current progresses made toward video-based
anomaly detection. We address the most fundamental aspect for video anomaly
detection, that is, video feature representation. Much research works have been
done in finding the right representation to perform anomaly detection in video
streams accurately with an acceptable false alarm rate. However, this is very
challenging due to large variations in environment and human movement, and high
space-time complexity due to huge dimensionality of video data. The weakly
supervised nature of deep learning algorithms can help in learning
representations from the video data itself instead of manually designing the
right feature for specific scenes. In this paper, we would like to review the
existing methods of modeling video representations using deep learning
techniques for the task of anomaly detection and action recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00529</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00529</id><created>2015-05-04</created><authors><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Rawls</keyname><forenames>Stephen</forenames></author><author><keyname>AbdAlmageed</keyname><forenames>Wael</forenames></author><author><keyname>Natarajan</keyname><forenames>Premkumar</forenames></author></authors><title>Learning Document Image Binarization from Data</title><categories>cs.CV</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a fully trainable binarization solution for degraded
document images. Unlike previous attempts that often used simple features with
a series of pre- and post-processing, our solution encodes all heuristics about
whether or not a pixel is foreground text into a high-dimensional feature
vector and learns a more complicated decision function. In particular, we
prepare features of three types: 1) existing features for binarization such as
intensity [1], contrast [2], [3], and Laplacian [4], [5]; 2) reformulated
features from existing binarization decision functions such those in [6] and
[7]; and 3) our newly developed features, namely the Logarithm Intensity
Percentile (LIP) and the Relative Darkness Index (RDI). Our initial
experimental results show that using only selected samples (about 1.5% of all
available training data), we can achieve a binarization performance comparable
to those fine-tuned (typically by hand), state-of-the-art methods.
Additionally, the trained document binarization classifier shows good
generalization capabilities on out-of-domain data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00542</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00542</id><created>2015-05-04</created><updated>2015-05-05</updated><authors><author><keyname>Bellini</keyname><forenames>Emanuele</forenames></author><author><keyname>Sala</keyname><forenames>Massimiliano</forenames></author></authors><title>A deterministic algorithm for the distance and weight distribution of
  binary nonlinear codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a binary nonlinear code, we provide a deterministic algorithm to
compute its weight and distance distribution, and in particular its minimum
weight and its minimum distance, which takes advantage of fast Fourier
techniques. This algorithm's performance is similar to that of best-known
algorithms for the average case, while it is especially efficient for codes
with low information rate. We provide complexity estimates for several cases of
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00553</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00553</id><created>2015-05-04</created><authors><author><keyname>Nayyar</keyname><forenames>Naumaan</forenames></author><author><keyname>Kalathil</keyname><forenames>Dileep</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>On Regret-Optimal Learning in Decentralized Multi-player Multi-armed
  Bandits</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning in single-player and multiplayer
multiarmed bandit models. Bandit problems are classes of online learning
problems that capture exploration versus exploitation tradeoffs. In a
multiarmed bandit model, players can pick among many arms, and each play of an
arm generates an i.i.d. reward from an unknown distribution. The objective is
to design a policy that maximizes the expected reward over a time horizon for a
single player setting and the sum of expected rewards for the multiplayer
setting. In the multiplayer setting, arms may give different rewards to
different players. There is no separate channel for coordination among the
players. Any attempt at communication is costly and adds to regret. We propose
two decentralizable policies, $\tt E^3$ ($\tt E$-$\tt cubed$) and $\tt
E^3$-$\tt TS$, that can be used in both single player and multiplayer settings.
These policies are shown to yield expected regret that grows at most as
O($\log^{1+\epsilon} T$). It is well known that $\log T$ is the lower bound on
the rate of growth of regret even in a centralized case. The proposed
algorithms improve on prior work where regret grew at O($\log^2 T$). More
fundamentally, these policies address the question of additional cost incurred
in decentralized online learning, suggesting that there is at most an
$\epsilon$-factor cost in terms of order of regret. This solves a problem of
relevance in many domains and had been open for a while.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00555</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00555</id><created>2015-05-04</created><updated>2015-05-04</updated><authors><author><keyname>Fu</keyname><forenames>Jian</forenames></author><author><keyname>Ma</keyname><forenames>Xutai</forenames></author><author><keyname>Li</keyname><forenames>Wenjiang</forenames></author><author><keyname>Sun</keyname><forenames>Shuo</forenames></author></authors><title>Beyond Quantum Computation Based on Classical Entanglement</title><categories>quant-ph cs.IT math.IT</categories><comments>28 pages, 13 figures, Welcome to comment! Major update version of
  arXiv:1003.6033</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that a tensor product structure and classical entanglement can
be obtained by introducing pseudorandom phase sequences into classical fields
with two orthogonal modes. Using the classical entanglement, we discuss
efficient simulation of several typical quantum states, including product
state, Bell states, GHZ state, and W state. By performing quadrature
demodulation scheme, we propose a sequence permutation mechanism to simulate
certain quantum states and a generalized gate array model to simulate quantum
algorithm, such as Shor's algorithm and Grover's algorithm. The research on
classical simulation of quantum states is important, for it not only enables
potential beyond quantum computation, but also provides useful insights into
fundamental concepts of quantum mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00558</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00558</id><created>2015-05-04</created><updated>2015-05-11</updated><authors><author><keyname>Muqaddas</keyname><forenames>Ammar</forenames></author></authors><title>Triple State QuickSort, A replacement for the C/C++ library qsort</title><categories>cs.DS cs.PF</categories><comments>31 pages, 49 Figures. Minor fix in page 15 and a typo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An industrial grade Quicksort function along with its new algorithm is
presented. Compared to 4 other well known implementations of Quicksort, the new
algorithm reduces both the number of comparisons and swaps in most cases while
staying close to the best of the 4 in worst cases. We trade space for
performance, at the price of n/2 temporary extra spaces in the worst case. Run
time tests reveal an overall improvement of at least 15.8% compared to the
overall best of the other 4 functions. Furthermore, our function scores a 32.7%
run time improvement against Yaroslavskiy's new Dual Pivot Quicksort. Our
function is pointer based, which is meant as a replacement for the C/C++
library qsort(). But we also provide an array based function of the same
algorithm for easy porting to different programming languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00562</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00562</id><created>2015-05-04</created><authors><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author><author><keyname>Mondal</keyname><forenames>Arijit</forenames></author></authors><title>Approximation of Capacity for ISI Channels with One-bit Output
  Quantization</title><categories>cs.IT math.IT math.PR stat.AP</categories><comments>Will be presented at ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by recent high bandwidth communication systems, Inter-Symbol
Interference (ISI) channels with 1-bit quantized output are considered under an
average-power-constrained continuous input. While the exact capacity is
difficult to characterize, an approximation that matches with the exact channel
output up to a probability of error is provided. The approximation does not
have additive noise, but constrains the channel output (without noise) to be
above a threshold in absolute value. The capacity under the approximation is
computed using methods involving standard Gibbs distributions. Markovian
achievable schemes approaching the approximate capacity are provided. The
methods used over the approximate ISI channel result in ideas for practical
coding schemes for ISI channels with 1-bit output quantization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00564</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00564</id><created>2015-05-04</created><authors><author><keyname>Lindner</keyname><forenames>Gerd</forenames></author><author><keyname>Staudt</keyname><forenames>Christian L.</forenames></author><author><keyname>Hamann</keyname><forenames>Michael</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Structure-Preserving Sparsification of Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Sparsification reduces the size of networks while preserving structural and
statistical properties of interest. Various sparsifying algorithms have been
proposed in different contexts. We contribute the first systematic conceptual
and experimental comparison of \textit{edge sparsification} methods on a
diverse set of network properties. It is shown that they can be understood as
methods for rating edges by importance and then filtering globally by these
scores. In addition, we propose a new sparsification method (\textit{Local
Degree}) which preserves edges leading to local hub nodes. All methods are
evaluated on a set of 100 Facebook social networks with respect to network
properties including diameter, connected components, community structure, and
multiple node centrality measures. Experiments with our implementations of the
sparsification methods (using the open-source network analysis tool suite
NetworKit) show that many network properties can be preserved down to about
20\% of the original set of edges. Furthermore, the experimental results allow
us to differentiate the behavior of different methods and show which method is
suitable with respect to which property. Our Local Degree method is fast enough
for large-scale networks and performs well across a wider range of properties
than previously proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00566</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00566</id><created>2015-05-04</created><authors><author><keyname>Dey</keyname><forenames>Palash</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>Estimating the Margin of Victory of an Election using Sampling</title><categories>cs.AI cs.MA</categories><comments>To appear in IJCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The margin of victory of an election is a useful measure to capture the
robustness of an election outcome. It also plays a crucial role in determining
the sample size of various algorithms in post election audit, polling etc. In
this work, we present efficient sampling based algorithms for estimating the
margin of victory of elections.
  More formally, we introduce the \textsc{$(c, \epsilon, \delta)$--Margin of
Victory} problem, where given an election $\mathcal{E}$ on $n$ voters, the goal
is to estimate the margin of victory $M(\mathcal{E})$ of $\mathcal{E}$ within
an additive factor of $c MoV(\mathcal{E})+\epsilon n$. We study the
\textsc{$(c, \epsilon, \delta)$--Margin of Victory} problem for many commonly
used voting rules including scoring rules, approval, Bucklin, maximin, and
Copeland$^{\alpha}.$ We observe that even for the voting rules for which
computing the margin of victory is NP-Hard, there may exist efficient sampling
based algorithms, as observed in the cases of maximin and Copeland$^{\alpha}$
voting rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00571</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00571</id><created>2015-05-04</created><authors><author><keyname>Shekhovtsov</keyname><forenames>Alexander</forenames></author></authors><title>Higher Order Maximum Persistency and Comparison Theorems</title><categories>cs.CV cs.DM math.CO</categories><comments>Submitted to CVIU Special Issuie on Inference in Graphical Models</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We address combinatorial problems that can be formulated as minimization of a
partially separable function of discrete variables (energy minimization in
graphical models, weighted constraint satisfaction, pseudo-Boolean
optimization, 0-1 polynomial programming). For polyhedral relaxations of such
problems it is generally not true that variables integer in the relaxed
solution will retain the same values in the optimal discrete solution. Those
which do are called persistent. Such persistent variables define a part of a
globally optimal solution. Once identified, they can be excluded from the
problem, reducing its size.
  To any polyhedral relaxation we associate a sufficient condition proving
persistency of a subset of variables. We set up a specially constructed linear
program which determines the set of persistent variables maximal with respect
to the relaxation. The condition improves as the relaxation is tightened and
possesses all its invariances. The proposed framework explains a variety of
existing methods originating from different areas of research and based on
different principles. A theoretical comparison is established that relates
these methods to the standard linear relaxation and proves that the proposed
technique identifies same or larger set of persistent variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00573</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00573</id><created>2015-05-04</created><authors><author><keyname>Vishwakarma</keyname><forenames>Sanjay</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>MIMO DF Relay Beamforming for Secrecy with Artificial Noise, Imperfect
  CSI, and Finite-Alphabet</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider decode-and-forward (DF) relay beamforming with
imperfect channel state information (CSI), cooperative artificial noise (AN)
injection, and finite-alphabet input in the presence of an user and $J$
non-colluding eavesdroppers. The communication between the source and the user
is aided by a multiple-input-multiple-output (MIMO) DF relay. We use the fact
that a wiretap code consists of two parts: i) common message (non-secret), and
ii) secret message. The source transmits two independent messages: i) common
message (non-secret), and ii) secret message. The common message is transmitted
at a fixed rate $R_{0}$, and it is intended for the user. The secret message is
also intended for the user but it should be kept secret from the $J$
eavesdroppers. The source and the MIMO DF relay operate under individual power
constraints. In order to improve the secrecy rate, the MIMO relay also injects
artificial noise. The CSI on all the links are assumed to be imperfect and CSI
errors are assumed to be norm bounded. In order to maximize the worst case
secrecy rate, we maximize the worst case link information rate to the user
subject to: i) the individual power constraints on the source and the MIMO
relay, and ii) the best case link information rates to $J$ eavesdroppers be
less than or equal to $R_{0}$ in order to support a fixed common message rate
$R_{0}$. Numerical results showing the effect of perfect/imperfect CSI,
presence/absence of AN with finite-alphabet input on the secrecy rate are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00577</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00577</id><created>2015-05-04</created><authors><author><keyname>Ngenzi</keyname><forenames>Alexander</forenames></author><author><keyname>R</keyname><forenames>Selvarani</forenames></author><author><keyname>Nair</keyname><forenames>Suchithra R.</forenames></author></authors><title>Dynamic resource management in Cloud datacenters for Server
  consolidation</title><categories>cs.DC</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud resource management has been a key factor for the cloud datacenters
development. Many cloud datacenters have problems in understanding and
implementing the techniques to manage, allocate and migrate the resources in
their premises. The consequences of improper resource management may result
into underutilized and wastage of resources which may also result into poor
service delivery in these datacenters. Resources like, CPU, memory, Hard disk
and servers need to be well identified and managed. In this Paper, Dynamic
Resource Management Algorithm(DRMA) shall limit itself in the management of CPU
and memory as the resources in cloud datacenters. The target is to save those
resources which may be underutilized at a particular period of time. It can be
achieved through Implementation of suitable algorithms. Here, Bin packing
algorithm can be used whereby the best fit algorithm is deployed to obtain
results and compared to select suitable algorithm for efficient use of
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00581</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00581</id><created>2015-05-04</created><authors><author><keyname>Lombardi</keyname><forenames>Eric</forenames></author><author><keyname>Wolf</keyname><forenames>Christian</forenames></author><author><keyname>Celiktutan</keyname><forenames>Oya</forenames></author><author><keyname>Sankur</keyname><forenames>B&#xfc;lent</forenames></author></authors><title>Activity recognition from videos with parallel hypergraph matching on
  GPUs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method for activity recognition from videos based
on sparse local features and hypergraph matching. We benefit from special
properties of the temporal domain in the data to derive a sequential and fast
graph matching algorithm for GPUs.
  Traditionally, graphs and hypergraphs are frequently used to recognize
complex and often non-rigid patterns in computer vision, either through graph
matching or point-set matching with graphs. Most formulations resort to the
minimization of a difficult discrete energy function mixing geometric or
structural terms with data attached terms involving appearance features.
Traditional methods solve this minimization problem approximately, for instance
with spectral techniques.
  In this work, instead of solving the problem approximatively, the exact
solution for the optimal assignment is calculated in parallel on GPUs. The
graphical structure is simplified and regularized, which allows to derive an
efficient recursive minimization algorithm. The algorithm distributes
subproblems over the calculation units of a GPU, which solves them in parallel,
allowing the system to run faster than real-time on medium-end GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00582</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00582</id><created>2015-05-04</created><authors><author><keyname>Elkhalil</keyname><forenames>Khalil M.</forenames></author><author><keyname>Eltayeb</keyname><forenames>Mohammed E.</forenames></author><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Bahrami</keyname><forenames>Hamid Reza</forenames></author></authors><title>On the Feedback Reduction of Relay Aided Multiuser Networks using
  Compressive Sensing</title><categories>cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a feedback reduction scheme for full-duplex
relay-aided multiuser networks. The proposed scheme permits the base station
(BS) to obtain channel state information (CSI) from a subset of strong users
under substantially reduced feedback overhead. More specifically, we cast the
problem of user identification and CSI estimation as a block sparse signal
recovery problem in compressive sensing (CS). Using existing CS block recovery
algorithms, we first obtain the identity of the strong users and then estimate
their CSI using the best linear unbiased estimator (BLUE). To minimize the
effect of noise on the estimated CSI, we introduce a back-off strategy that
optimally backs-off on the noisy estimated CSI and derive the error covariance
matrix of the post-detection noise. In addition to this, we provide exact
closed form expressions for the average maximum equivalent SNR at the
destination user. Numerical results show that the proposed algorithm
drastically reduces the feedback air-time and achieves a rate close to that
obtained by scheduling schemes that require dedicated error-free feedback from
all the network users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00589</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00589</id><created>2015-05-04</created><authors><author><keyname>Onwuzurike</keyname><forenames>Lucky</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author></authors><title>Danger is My Middle Name: Experimenting with SSL Vulnerabilities in
  Android Apps</title><categories>cs.CR cs.SE</categories><comments>A preliminary version of this paper appears in the Proceedings of ACM
  WiSec 2015. This is the full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a measurement study of information leakage and SSL
vulnerabilities in popular Android apps. We perform static and dynamic analysis
on 100 apps, downloaded at least 10M times, that request full network access.
Our experiments show that, although prior work has drawn a lot of attention to
SSL implementations on mobile platforms, several popular apps (32/100) accept
all certificates and all hostnames, and four actually transmit sensitive data
unencrypted. We set up an experimental testbed simulating man-in-the-middle
attacks and find that many apps (up to 91% when the adversary has a certificate
installed on the victim's device) are vulnerable, allowing the attacker to
access sensitive information, including credentials, files, personal details,
and credit card numbers. Finally, we provide a few recommendations to app
developers and highlight several open research problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00599</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00599</id><created>2015-05-04</created><authors><author><keyname>Chalopin</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Godard</keyname><forenames>Emmanuel</forenames></author><author><keyname>Naudin</keyname><forenames>Antoine</forenames></author></authors><title>Anonymous Graph Exploration with Binoculars</title><categories>cs.DS cs.DC</categories><comments>Preliminary version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the exploration of networks by a mobile agent. It is long
known that, without global information about the graph, it is not possible to
make the agent halts after the exploration except if the graph is a tree. We
therefore endow the agent with binoculars, a sensing device that can show the
local structure of the environment at a constant distance of the agent current
location. We show that, with binoculars, it is possible to explore and halt in
a large class of non-tree networks. We give a complete characterization of the
class of networks that can be explored using binoculars using standard notions
of discrete topology. Our characterization is constructive, we present an
Exploration algorithm that is universal; this algorithm explores any network
explorable with binoculars, and never halts in non-explorable networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00605</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00605</id><created>2015-05-04</created><authors><author><keyname>Rass</keyname><forenames>Stefan</forenames></author><author><keyname>Schartner</keyname><forenames>Peter</forenames></author><author><keyname>Wamser</keyname><forenames>Markus</forenames></author></authors><title>Oblivious Lookup Tables</title><categories>cs.CR</categories><comments>accepted for presentation at the 15th Central European Conference on
  Cryptology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following question: given a group-homomorphic public-key
encryption $E$, a ciphertext $c=E(x,pk)$ hiding a value $x$ using a key $pk$,
and a &quot;suitable&quot; description of a function $f$, can we evaluate $E(f(x), pk)$
without decrypting $c$? We call this an &quot;oblivious lookup table&quot; and show the
existence of such a primitive. To this end, we describe a concrete
construction, discuss its security and relations to other cryptographic
primitives, and point out directions of future investigations towards
generalizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00612</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00612</id><created>2015-05-04</created><authors><author><keyname>Drange</keyname><forenames>P&#xe5;l Gr&#xf8;n&#xe5;s</forenames></author><author><keyname>Dregi</keyname><forenames>Markus Sortland</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Sullivan</keyname><forenames>Blair D.</forenames></author></authors><title>On the Threshold of Intractability</title><categories>cs.DS cs.CC cs.SI</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of the graph modification problems
Threshold Editing and Chain Editing, adding and deleting as few edges as
possible to transform the input into a threshold (or chain) graph. In this
article, we show that both problems are NP-complete, resolving a conjecture by
Natanzon, Shamir, and Sharan (Discrete Applied Mathematics, 113(1):109--128,
2001). On the positive side, we show the problem admits a quadratic vertex
kernel. Furthermore, we give a subexponential time parameterized algorithm
solving Threshold Editing in $2^{O(\surd k \log k)} + \text{poly}(n)$ time,
making it one of relatively few natural problems in this complexity class on
general graphs. These results are of broader interest to the field of social
network analysis, where recent work of Brandes (ISAAC, 2014) posits that the
minimum edit distance to a threshold graph gives a good measure of consistency
for node centralities. Finally, we show that all our positive results extend to
the related problem of Chain Editing, as well as the completion and deletion
variants of both problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00619</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00619</id><created>2015-05-04</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author></authors><title>Using higher-order Fourier analysis over general fields</title><categories>cs.DS cs.CC cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order Fourier analysis, developed over prime fields, has been recently
used in different areas of computer science, including list decoding,
algorithmic decomposition and testing. We extend the tools of higher-order
Fourier analysis to analyze functions over general fields. Using these new
tools, we revisit the results in the above areas.
  * For any fixed finite field $\mathbb{K}$, we show that the list decoding
radius of the generalized Reed Muller code over $\mathbb{K}$ equals the minimum
distance of the code. Previously, this had been proved over prime fields [BL14]
and for the case when $|\mathbb{K}|-1$ divides the order of the code [GKZ08].
  * For any fixed finite field $\mathbb{K}$, we give a polynomial time
algorithm to decide whether a given polynomial $P: \mathbb{K}^n \to \mathbb{K}$
can be decomposed as a particular composition of lesser degree polynomials.
This had been previously established over prime fields [Bha14, BHT15].
  * For any fixed finite field $\mathbb{K}$, we prove that all locally
characterized affine-invariant properties of functions $f: \mathbb{K}^n \to
\mathbb{K}$ are testable with one-sided error. The same result was known when
$\mathbb{K}$ is prime [BFHHL13] and when the property is linear [KS08].
Moreover, we show that for any fixed finite field $\mathbb{F}$, an
affine-invariant property of functions $f: \mathbb{K}^n \to \mathbb{F}$, where
$\mathbb{K}$ is a growing field extension over $\mathbb{F}$, is testable if it
is locally characterized by constraints of bounded weight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00641</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00641</id><created>2015-05-04</created><updated>2015-05-05</updated><authors><author><keyname>Bayer</keyname><forenames>Immanuel</forenames></author></authors><title>fastFM: A Library for Factorization Machines</title><categories>cs.LG cs.IR</categories><comments>Source Code is available at https://github.com/ibayer/fastFM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Factorization Machines (FM) are only used in a narrow range of applications
and are not part of the standard toolbox of machine learning models. This is a
pity, because even though FMs are recognized as being very successful for
recommender system type applications they are a general model to deal with
sparse and high dimensional features. Our Factorization Machine implementation
provides easy access to many solvers and supports regression, classification
and ranking tasks. Such an implementation simplifies the use of FM's for a wide
field of applications. This implementation has the potential to improve our
understanding of the FM model and drive new development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00644</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00644</id><created>2015-05-04</created><updated>2015-06-22</updated><authors><author><keyname>Murase</keyname><forenames>Yohsuke</forenames></author><author><keyname>Jo</keyname><forenames>Hang-Hyun</forenames></author><author><keyname>T&#xf6;r&#xf6;k</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author></authors><title>Modeling the role of relationship fading and breakup in social network
  formation</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 5 figures</comments><journal-ref>PLoS ONE 10(7): e0133005 (2015)</journal-ref><doi>10.1371/journal.pone.0133005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social networks of human individuals, social relationships do not
necessarily last forever as they can either fade gradually with time, resulting
in link aging, or terminate abruptly, causing link deletion, as even old
friendships may cease. In this paper, we study a social network formation model
where we introduce several ways by which a link termination takes place. If we
adopt the link aging, we get a more modular structure with more homogeneously
distributed link weights within communities than when link deletion is used. By
investigating distributions and relations of various network characteristics,
we find that the empirical findings are better reproduced with the link
deletion model. This indicates that link deletion plays a more prominent role
in organizing social networks than link aging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00651</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00651</id><created>2015-05-04</created><authors><author><keyname>Joda</keyname><forenames>Roghayeh</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Delay-Distortion-Power Trade Offs in Quasi-Stationary Source
  Transmission over Block Fading Channels</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1202.6175</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates delay-distortion-power trade offs in transmission of
quasi-stationary sources over block fading channels by studying encoder and
decoder buffering techniques to smooth out the source and channel variations.
Four source and channel coding schemes that consider buffer and power
constraints are presented to minimize the reconstructed source distortion. The
first one is a high performance scheme, which benefits from optimized source
and channel rate adaptation. In the second scheme, the channel coding rate is
fixed and optimized along with transmission power with respect to channel and
source variations; hence this scheme enjoys simplicity of implementation. The
two last schemes have fixed transmission power with optimized adaptive or fixed
channel coding rate. For all the proposed schemes, closed form solutions for
mean distortion, optimized rate and power are provided and in the high SNR
regime, the mean distortion exponent and the asymptotic mean power gains are
derived. The proposed schemes with buffering exploit the diversity due to
source and channel variations. Specifically, when the buffer size is limited,
fixed channel rate adaptive power scheme outperforms an adaptive rate fixed
power scheme. Furthermore, analytical and numerical results demonstrate that
with limited buffer size, the system performance in terms of reconstructed
signal SNR saturates as transmission power is increased, suggesting that
appropriate buffer size selection is important to achieve a desired
reconstruction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00662</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00662</id><created>2015-05-04</created><updated>2015-11-23</updated><authors><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author><author><keyname>Stewart</keyname><forenames>Alistair</forenames></author></authors><title>Optimal Learning via the Fourier Transform for Sums of Independent
  Integer Random Variables</title><categories>cs.DS cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>Main differences from v1: Changed title and restructured
  introduction. Added new sample optimal algorithm. Generalized sample lower
  bound for any value of k</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure and learnability of sums of independent integer random
variables (SIIRVs). For $k \in \mathbb{Z}_{+}$, a $k$-SIIRV of order $n \in
\mathbb{Z}_{+}$ is the probability distribution of the sum of $n$ independent
random variables each supported on $\{0, 1, \dots, k-1\}$. We denote by ${\cal
S}_{n,k}$ the set of all $k$-SIIRVs of order $n$.
  In this paper, we tightly characterize the sample and computational
complexity of learning $k$-SIIRVs. More precisely, we design a computationally
efficient algorithm that uses $\widetilde{O}(k/\epsilon^2)$ samples, and learns
an arbitrary $k$-SIIRV within error $\epsilon,$ in total variation distance.
Moreover, we show that the {\em optimal} sample complexity of this learning
problem is $\Theta((k/\epsilon^2)\sqrt{\log(1/\epsilon)}).$ Our algorithm
proceeds by learning the Fourier transform of the target $k$-SIIRV in its
effective support. Its correctness relies on the {\em approximate sparsity} of
the Fourier transform of $k$-SIIRVs -- a structural property that we establish,
roughly stating that the Fourier transform of $k$-SIIRVs has small magnitude
outside a small set.
  Along the way we prove several new structural results about $k$-SIIRVs. As
one of our main structural contributions, we give an efficient algorithm to
construct a sparse {\em proper} $\epsilon$-cover for ${\cal S}_{n,k},$ in total
variation distance. We also obtain a novel geometric characterization of the
space of $k$-SIIRVs. Our characterization allows us to prove a tight lower
bound on the size of $\epsilon$-covers for ${\cal S}_{n,k}$, and is the key
ingredient in our tight sample complexity lower bound.
  Our approach of exploiting the sparsity of the Fourier transform in
distribution learning is general, and has recently found additional
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00663</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00663</id><created>2015-05-04</created><updated>2015-10-02</updated><authors><author><keyname>Chiu</keyname><forenames>Wei-Chen</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>See the Difference: Direct Pre-Image Reconstruction and Pose Estimation
  by Differentiating HOG</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Histogram of Oriented Gradient (HOG) descriptor has led to many advances
in computer vision over the last decade and is still part of many state of the
art approaches. We realize that the associated feature computation is piecewise
differentiable and therefore many pipelines which build on HOG can be made
differentiable. This lends to advanced introspection as well as opportunities
for end-to-end optimization. We present our implementation of $\nabla$HOG based
on the auto-differentiation toolbox Chumpy and show applications to pre-image
visualization and pose estimation which extends the existing differentiable
renderer OpenDR pipeline. Both applications improve on the respective
state-of-the-art HOG approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00670</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00670</id><created>2015-05-04</created><authors><author><keyname>Shin</keyname><forenames>Hoo-Chang</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Kim</keyname><forenames>Lauren</forenames></author><author><keyname>Seff</keyname><forenames>Ari</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database
  for Automated Image Interpretation</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite tremendous progress in computer vision, there has not been an attempt
for machine learning on very large-scale medical image databases. We present an
interleaved text/image deep learning system to extract and mine the semantic
interactions of radiology images and reports from a national research
hospital's Picture Archiving and Communication System. With natural language
processing, we mine a collection of representative ~216K two-dimensional key
images selected by clinicians for diagnostic reference, and match the images
with their descriptions in an automated manner. Our system interleaves between
unsupervised learning and supervised learning on document- and sentence-level
text collections, to generate semantic labels and to predict them given an
image. Given an image of a patient scan, semantic topics in radiology levels
are predicted, and associated key-words are generated. Also, a number of
frequent disease types are detected as present or absent, to provide more
specific interpretation of a patient scan. This shows the potential of
large-scale learning and prediction in electronic patient records available in
most modern clinical institutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00672</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00672</id><created>2015-05-04</created><authors><author><keyname>Ghazi</keyname><forenames>Aboubakr Achraf El</forenames></author><author><keyname>Taghdiri</keyname><forenames>Mana</forenames></author></authors><title>Analyzing Alloy Formulas using an SMT Solver: A Case Study</title><categories>cs.LO</categories><comments>5th International Workshop on Automated Formal Methods (AFM), 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes how Yices, a modern SAT Modulo theories solver, can be
used to analyze the address-book problem expressed in Alloy, a first-order
relational logic with transitive closure. Current analysis of Alloy models - as
performed by the Alloy Analyzer - is based on SAT solving and thus, is done
only with respect to finitized types. Our analysis generalizes this approach by
taking advantage of the background theories available in Yices, and avoiding
type finitization when possible. Consequently, it is potentially capable of
proving that an assertion is a tautology - a capability completely missing from
the Alloy Analyzer. This paper also reports on our experimental results that
compare the performance of our analysis to that of the Alloy Analyzer for
various versions of the address book problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00685</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00685</id><created>2015-05-04</created><authors><author><keyname>Tarighati</keyname><forenames>Alla</forenames></author><author><keyname>Jalden</keyname><forenames>Joakim</forenames></author></authors><title>Rate Allocation for Decentralized Detection in Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>Accepted at SPAWC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of decentralized detection where peripheral nodes
make noisy observations of a phenomenon and send quantized information about
the phenomenon towards a fusion center over a sum-rate constrained multiple
access channel. The fusion center then makes a decision about the state of the
phenomenon based on the aggregate received data. Using the Chernoff information
as a performance metric, Chamberland and Veeravalli previously studied the
structure of optimal rate allocation strategies for this scenario under the
assumption of an unlimited number of sensors. Our key contribution is to extend
these result to the case where there is a constraint on the maximum number of
active sensors. In particular, we find sufficient conditions under which the
uniform rate allocation is an optimal strategy, and then numerically verify
that these conditions are satisfied for some relevant sensor design rules under
a Gaussian observation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00687</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00687</id><created>2015-05-04</created><updated>2015-10-06</updated><authors><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author></authors><title>Unsupervised Learning of Visual Representations using Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is strong supervision necessary for learning a good visual representation? Do
we really need millions of semantically-labeled images to train a Convolutional
Neural Network (CNN)? In this paper, we present a simple yet surprisingly
powerful approach for unsupervised learning of CNN. Specifically, we use
hundreds of thousands of unlabeled videos from the web to learn visual
representations. Our key idea is that visual tracking provides the supervision.
That is, two patches connected by a track should have similar visual
representation in deep feature space since they probably belong to the same
object or object part. We design a Siamese-triplet network with a ranking loss
function to train this CNN representation. Without using a single image from
ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train
an ensemble of unsupervised networks that achieves 52% mAP (no bounding box
regression). This performance comes tantalizingly close to its
ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We
also show that our unsupervised network can perform competitively in other
tasks such as surface-normal estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00692</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00692</id><created>2015-05-04</created><authors><author><keyname>Parter</keyname><forenames>Merav</forenames></author></authors><title>Dual Failure Resilient BFS Structure</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study {\em breadth-first search (BFS)} spanning trees, and address the
problem of designing a sparse {\em fault-tolerant} BFS structure, or {\em
FT-BFS } for short, resilient to the failure of up to two edges in the given
undirected unweighted graph $G$, i.e., a sparse subgraph $H$ of $G$ such that
subsequent to the failure of up to two edges, the surviving part $H'$ of $H$
still contains a BFS spanning tree for (the surviving part of) $G$. FT-BFS
structures, as well as the related notion of replacement paths, have been
studied so far for the restricted case of a single failure. It has been noted
widely that when concerning shortest-paths in a variety of contexts, there is a
sharp qualitative difference between a single failure and two or more failures.
  Our main results are as follows. We present an algorithm that for every
$n$-vertex unweighted undirected graph $G$ and source node $s$ constructs a
(two edge failure) FT-BFS structure rooted at $s$ with $O(n^{5/3})$ edges. To
provide a useful theory of shortest paths avoiding 2 edges failures, we take a
principled approach to classifying the arrangement these paths. We believe that
the structural analysis provided in this paper may decrease the barrier for
understanding the general case of $f\geq 2$ faults and pave the way to the
future design of $f$-fault resilient structures for $f \geq 2$. We also provide
a matching lower bound, which in fact holds for the general case of $f \geq 1$
and multiple sources $S \subseteq V$. It shows that for every $f\geq 1$, and
integer $1 \leq \sigma \leq n$, there exist $n$-vertex graphs with a source set
$S \subseteq V$ of cardinality $\sigma$ for which any FT-BFS structure rooted
at each $s \in S$, resilient to up to $f$-edge faults has
$\Omega(\sigma^{1/(f+1)} \cdot n^{2-1/(f+1)})$ edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00693</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00693</id><created>2015-05-04</created><authors><author><keyname>Henne</keyname><forenames>Vitali</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schlag</keyname><forenames>Sebastian</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>n-Level Hypergraph Partitioning</title><categories>cs.DS</categories><acm-class>G.2.2; D.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a multilevel algorithm for hypergraph partitioning that contracts
the vertices one at a time and thus allows very high quality. This includes a
rating function that avoids nonuniform vertex weights, an efficient
&quot;semi-dynamic&quot; hypergraph data structure, a very fast coarsening algorithm, and
two new local search algorithms. One is a $k$-way hypergraph adaptation of
Fiduccia-Mattheyses local search and gives high quality at reasonable cost. The
other is an adaptation of size-constrained label propagation to hypergraphs.
Comparisons with hMetis and PaToH indicate that the new algorithm yields better
quality over several benchmark sets and has a running time that is comparable
to hMetis. Using label propagation local search is several times faster than
hMetis and gives better quality than PaToH for a VLSI benchmark set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00707</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00707</id><created>2015-05-04</created><authors><author><keyname>Berth&#xe9;</keyname><forenames>Val&#xe9;rie</forenames></author><author><keyname>De Felice</keyname><forenames>Clelia</forenames></author><author><keyname>Delecroix</keyname><forenames>Vincent</forenames></author><author><keyname>Dolce</keyname><forenames>Francesco</forenames></author><author><keyname>Leroy</keyname><forenames>Julien</forenames></author><author><keyname>Perrin</keyname><forenames>Dominique</forenames></author><author><keyname>reutenauer</keyname><forenames>Christophe</forenames></author><author><keyname>Rindone</keyname><forenames>Giuseppina</forenames></author></authors><title>Specular sets</title><categories>cs.DM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1405.3529</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of specular sets which are subsets of groups called
here specular and which form a natural generalization of free groups. These
sets are an abstract generalization of the natural codings of linear
involutions. We prove several results concerning the subgroups generated by
return words and by maximal bifix codes in these sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00709</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00709</id><created>2015-05-04</created><authors><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>Parameterized Approximation Algorithms for Packing Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past decade, many parameterized algorithms were developed for packing
problems. Our goal is to obtain tradeoffs that improve the running times of
these algorithms at the cost of computing approximate solutions. Consider a
packing problem for which there is no known algorithm with approximation ratio
$\alpha$, and a parameter $k$. If the value of an optimal solution is at least
$k$, we seek a solution of value at least $\alpha k$; otherwise, we seek an
arbitrary solution. Clearly, if the best known parameterized algorithm that
finds a solution of value $t$ runs in time $O^*(f(t))$ for some function $f$,
we are interested in running times better than $O^*(f(\alpha k))$. We present
tradeoffs between running times and approximation ratios for the $P_2$-Packing,
$3$-Set $k$-Packing and $3$-Dimensional $k$-Matching problems. Our tradeoffs
are based on combinations of several known results, as well as a computation of
&quot;approximate lopsided universal sets.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00720</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00720</id><created>2015-05-04</created><authors><author><keyname>Nekipelov</keyname><forenames>Denis</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Tardos</keyname><forenames>Eva</forenames></author></authors><title>Econometrics for Learning Agents</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this paper is to develop a theory of inference of player
valuations from observed data in the generalized second price auction without
relying on the Nash equilibrium assumption. Existing work in Economics on
inferring agent values from data relies on the assumption that all participant
strategies are best responses of the observed play of other players, i.e. they
constitute a Nash equilibrium. In this paper, we show how to perform inference
relying on a weaker assumption instead: assuming that players are using some
form of no-regret learning. Learning outcomes emerged in recent years as an
attractive alternative to Nash equilibrium in analyzing game outcomes, modeling
players who haven't reached a stable equilibrium, but rather use algorithmic
learning, aiming to learn the best way to play from previous observations. In
this paper we show how to infer values of players who use algorithmic learning
strategies. Such inference is an important first step before we move to testing
any learning theoretic behavioral model on auction data. We apply our
techniques to a dataset from Microsoft's sponsored search ad auction system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00729</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00729</id><created>2015-05-04</created><updated>2015-06-20</updated><authors><author><keyname>Gupta</keyname><forenames>Udit</forenames></author></authors><title>Survey on security issues in file management in cloud computing
  environment</title><categories>cs.CR</categories><comments>5 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has pervaded through every aspect of Information technology
in past decade. It has become easier to process plethora of data, generated by
various devices in real time, with the advent of cloud networks. The privacy of
users data is maintained by data centers around the world and hence it has
become feasible to operate on that data from lightweight portable devices. But
with ease of processing comes the security aspect of the data. One such
security aspect is secure file transfer either internally within cloud or
externally from one cloud network to another. File management is central to
cloud computing and it is paramount to address the security concerns which
arise out of it. This survey paper aims to elucidate the various protocols
which can be used for secure file transfer and analyze the ramifications of
using each protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00731</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00731</id><created>2015-05-04</created><updated>2016-02-12</updated><authors><author><keyname>Bienvenu</keyname><forenames>Laurent</forenames></author><author><keyname>Desfontaines</keyname><forenames>Damien</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author></authors><title>Generic algorithms for halting problem and optimal machines revisited</title><categories>math.LO cs.CC</categories><comments>a preliminary version was presented at the ICALP 2015 conference</comments><msc-class>68Q17, 68Q30</msc-class><acm-class>E.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The halting problem is undecidable --- but can it be solved for &quot;most&quot;
inputs? This natural question was considered in a number of papers, in
different settings. We revisit their results and show that most of them can be
easily proven in a natural framework of optimal machines (considered in
algorithmic information theory) using the notion of Kolmogorov complexity.
  We also consider some related questions about this framework and about
asymptotic properties of the halting problem. In particular, we show that the
fraction of terminating programs cannot have a limit, and all limit points are
Martin-L\&quot;of random reals. We then consider mass problems of finding an
approximate solution of halting problem and probabilistic algorithms for them,
proving both positive and negative results.
  We consider the fraction of terminating programs that require a long time for
termination, and describe this fraction using the busy beaver function. We also
consider approximate versions of separation problems, and revisit Schnorr's
results about optimal numberings showing how they can be generalized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00737</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00737</id><created>2015-05-04</created><authors><author><keyname>Haloi</keyname><forenames>Mrinal</forenames></author><author><keyname>Dandapat</keyname><forenames>Samarendra</forenames></author><author><keyname>Sinha</keyname><forenames>Rohit</forenames></author></authors><title>A Gaussian Scale Space Approach For Exudates Detection, Classification
  And Severity Prediction</title><categories>cs.CV</categories><comments>Accepted in ICIP 2015, Quebec city, Canada</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of Computer Aided Diagnosis system for diabetic retinopathy,
we present a novel method for detection of exudates and their classification
for disease severity prediction. The method is based on Gaussian scale space
based interest map and mathematical morphology. It makes use of support vector
machine for classification and location information of the optic disc and the
macula region for severity prediction. It can efficiently handle luminance
variation and it is suitable for varied sized exudates. The method has been
probed in publicly available DIARETDB1V2 and e-ophthaEX databases. For exudate
detection the proposed method achieved a sensitivity of 96.54% and prediction
of 98.35% in DIARETDB1V2 database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00752</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00752</id><created>2015-05-04</created><authors><author><keyname>Br&#xe6;ndeland</keyname><forenames>Asbj&#xf8;rn</forenames></author></authors><title>A family of greedy algorithms for finding maximum independent sets</title><categories>cs.DS</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The greedy algorithm A iterates over a set of uniformly sized independent
sets of a given graph G and checks for each set S which non-neighbor of S, if
any, is best suited to be added to S, until no more suitable non-neighbors are
found for any of the sets. The algorithms receives as arguments the graph, the
heuristic used to evaluate the independent set candidates, and the initial
cardinality of the independent sets, and returns the final set of independent
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00755</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00755</id><created>2015-05-04</created><authors><author><keyname>Verhodubs</keyname><forenames>Olegs</forenames></author></authors><title>Towards the Ontology Web Search Engine</title><categories>cs.IR cs.AI cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The project of the Ontology Web Search Engine is presented in this paper. The
main purpose of this paper is to develop such a project that can be easily
implemented. Ontology Web Search Engine is software to look for and index
ontologies in the Web. OWL (Web Ontology Languages) ontologies are meant, and
they are necessary for the functioning of the SWES (Semantic Web Expert
System). SWES is an expert system that will use found ontologies from the Web,
generating rules from them, and will supplement its knowledge base with these
generated rules. It is expected that the SWES will serve as a universal expert
system for the average user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00768</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00768</id><created>2015-05-04</created><updated>2015-08-25</updated><authors><author><keyname>Nowzari</keyname><forenames>Cameron</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Analysis and Control of Epidemics: A survey of spreading processes on
  complex networks</title><categories>math.OC cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article reviews and presents various solved and open problems in the
development, analysis, and control of epidemic models. We are interested in
presenting a relatively concise report for new engineers looking to enter the
field of spreading processes on complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00769</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00769</id><created>2015-05-04</created><authors><author><keyname>Kamath</keyname><forenames>Sudeep</forenames></author><author><keyname>Anantharam</keyname><forenames>Venkat</forenames></author></authors><title>On Non-Interactive Simulation of Joint Distributions</title><categories>cs.IT math.IT</categories><comments>22 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following non-interactive simulation problem: Alice and Bob
observe sequences $X^n$ and $Y^n$ respectively where $\{(X_i, Y_i)\}_{i=1}^n$
are drawn i.i.d. from $P(x,y),$ and they output $U$ and $V$ respectively which
is required to have a joint law that is close in total variation to a specified
$Q(u,v).$ It is known that the maximal correlation of $U$ and $V$ must
necessarily be no bigger than that of $X$ and $Y$ if this is to be possible.
Our main contribution is to bring hypercontractivity to bear as a tool on this
problem. In particular, we show that if $P(x,y)$ is the doubly symmetric binary
source, then hypercontractivity provides stronger impossibility results than
maximal correlation. Finally, we extend these tools to provide impossibility
results for the $k$-agent version of this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00796</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00796</id><created>2015-05-04</created><authors><author><keyname>Haustein</keyname><forenames>Stefanie</forenames></author><author><keyname>Bowman</keyname><forenames>Timothy D.</forenames></author><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author></authors><title>When is an article actually published? An analysis of online
  availability, publication, and indexation dates</title><categories>cs.DL</categories><comments>accepted for presentation at 15th International Conference on
  Scientometrics and Informetrics (ISSI) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the acceleration of scholarly communication in the digital era, the
publication year is no longer a sufficient level of time aggregation for
bibliometric and social media indicators. Papers are increasingly cited before
they have been officially published in a journal issue and mentioned on Twitter
within days of online availability. In order to find a suitable proxy for the
day of online publication allowing for the computation of more accurate
benchmarks and fine-grained citation and social media event windows, various
dates are compared for a set of 58,896 papers published by Nature Publishing
Group, PLOS, Springer and Wiley-Blackwell in 2012. Dates include the online
date provided by the publishers, the month of the journal issue, the Web of
Science indexing date, the date of the first tweet mentioning the paper as well
as the Altmetric.com publication and first-seen dates. Comparing these dates,
the analysis reveals that large differences exist between publishers, leading
to the conclusion that more transparency and standardization is needed in the
reporting of publication dates. The date on which the fixed journal article
(Version of Record) is first made available on the publisher's website is
proposed as a consistent definition of the online date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00800</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00800</id><created>2015-05-04</created><updated>2015-09-09</updated><authors><author><keyname>Aminjavaheri</keyname><forenames>Amir</forenames></author><author><keyname>Farhang</keyname><forenames>Arman</forenames></author><author><keyname>RezazadehReyhani</keyname><forenames>Ahmad</forenames></author><author><keyname>Farhang-Boroujeny</keyname><forenames>Behrouz</forenames></author></authors><title>Impact of Timing and Frequency Offsets on Multicarrier Waveform
  Candidates for 5G</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a study of the candidate waveforms for 5G when they are
subject to timing and carrier frequency offset. These waveforms are: orthogonal
frequency division multiplexing (OFDM), generalized frequency division
multiplexing (GFDM), universal filtered multicarrier (UFMC), circular filter
bank multicarrier (C-FBMC), and linear filter bank multicarrier (FBMC). We are
particularly interested in multiple access interference (MAI) when a number of
users transmit their signals to a base station in an asynchronous or a
quasi-synchronous manner. We identify the source of MAI in these waveforms and
present some numerical analysis that confirm our findings. The goal of this
study is to answer the following question, &quot;Which one of the 5G candidate
waveforms has more relaxed synchronization requirements?&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00810</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00810</id><created>2015-05-04</created><updated>2015-11-21</updated><authors><author><keyname>Malak</keyname><forenames>Derya</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Optimizing Data Aggregation for Uplink Machine-to-Machine Communication
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>33 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine-to-machine (M2M) communication's severe power limitations challenge
the interconnectivity, access management, and reliable communication of data.
In densely deployed M2M networks, controlling and aggregating the generated
data is critical. We propose an energy efficient data aggregation scheme for a
hierarchical M2M network. We develop a coverage probability-based optimal data
aggregation scheme for M2M devices to minimize the average total energy
expenditure per unit area per unit time or simply the {\em energy density} of
an M2M communication network. Our analysis exposes the key tradeoffs between
the energy density of the M2M network and the coverage characteristics for
successive and parallel transmission schemes that can be either half-duplex or
full-duplex. Comparing the rate and energy performances of the transmission
models, we observe that successive mode and half-duplex parallel mode have
better coverage characteristics compared to full-duplex parallel scheme.
Simulation results show that the uplink coverage characteristics dominate the
trend of the energy consumption for both successive and parallel schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00824</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00824</id><created>2015-05-04</created><authors><author><keyname>Dyer</keyname><forenames>Eva L.</forenames></author><author><keyname>Goldstein</keyname><forenames>Tom A.</forenames></author><author><keyname>Patel</keyname><forenames>Raajen</forenames></author><author><keyname>Kording</keyname><forenames>Konrad P.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Self-Expressive Decompositions for Matrix Approximation and Clustering</title><categories>cs.IT cs.CV cs.LG math.IT stat.ML</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-aware methods for dimensionality reduction and matrix decomposition aim
to find low-dimensional structure in a collection of data. Classical approaches
discover such structure by learning a basis that can efficiently express the
collection. Recently, &quot;self expression&quot;, the idea of using a small subset of
data vectors to represent the full collection, has been developed as an
alternative to learning. Here, we introduce a scalable method for computing
sparse SElf-Expressive Decompositions (SEED). SEED is a greedy method that
constructs a basis by sequentially selecting incoherent vectors from the
dataset. After forming a basis from a subset of vectors in the dataset, SEED
then computes a sparse representation of the dataset with respect to this
basis. We develop sufficient conditions under which SEED exactly represents low
rank matrices and vectors sampled from a unions of independent subspaces. We
show how SEED can be used in applications ranging from matrix approximation and
denoising to clustering, and apply it to numerous real-world datasets. Our
results demonstrate that SEED is an attractive low-complexity alternative to
other sparse matrix factorization approaches such as sparse PCA and
self-expressive methods for clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00828</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00828</id><created>2015-05-04</created><updated>2015-07-17</updated><authors><author><keyname>Comin</keyname><forenames>Carlo</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>Dynamic Consistency of Conditional Simple Temporal Networks via Mean
  Payoff Games: a Singly-Exponential Time DC-Checking</title><categories>cs.DS cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional Simple Temporal Network (CSTN) is a constraint-based
graph-formalism for conditional temporal planning. It offers a more flexible
formalism than the equivalent CSTP model of Tsamardinos, Vidal and Pollack,
from which it was derived mainly as a sound formalization. Three notions of
consistency arise for CSTNs and CSTPs: weak, strong, and dynamic. Dynamic
consistency is the most interesting notion, but it is also the most challenging
and it was conjectured to be hard to assess. Tsamardinos, Vidal and Pollack
gave a doubly-exponential time algorithm for deciding whether a CSTN is
dynamically-consistent and to produce, in the positive case, a dynamic
execution strategy of exponential size. In the present work we offer a proof
that deciding whether a CSTN is dynamically-consistent is coNP-hard and provide
the first singly-exponential time algorithm for this problem, also producing a
dynamic execution strategy whenever the input CSTN is dynamically-consistent.
The algorithm is based on a novel connection with Mean Payoff Games, a family
of two-player combinatorial games on graphs well known for having applications
in model-checking and formal verification. The presentation of such connection
is mediated by the Hyper Temporal Network model, a tractable generalization of
Simple Temporal Networks whose consistency checking is equivalent to
determining Mean Payoff Games. In order to analyze the algorithm we introduce a
refined notion of dynamic-consistency, named \epsilon-dynamic-consistency, and
present a sharp lower bounding analysis on the critical value of the reaction
time \hat{\varepsilon} where the CSTN transits from being, to not being,
dynamically-consistent. The proof technique introduced in this analysis of
\hat{\varepsilon} is applicable more in general when dealing with linear
difference constraints which include strict inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00835</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00835</id><created>2015-05-04</created><authors><author><keyname>Der</keyname><forenames>Ralf</forenames></author><author><keyname>Martius</keyname><forenames>Georg</forenames></author></authors><title>A novel plasticity rule can explain the development of sensorimotor
  intelligence</title><categories>cs.RO cs.LG q-bio.NC</categories><comments>18 pages, 5 figures, 7 videos</comments><msc-class>68T40, 37N35, 68T05, 91E40, 92B20</msc-class><acm-class>I.2.9; I.2.6</acm-class><doi>10.1073/pnas.1508400112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grounding autonomous behavior in the nervous system is a fundamental
challenge for neuroscience. In particular, the self-organized behavioral
development provides more questions than answers. Are there special functional
units for curiosity, motivation, and creativity? This paper argues that these
features can be grounded in synaptic plasticity itself, without requiring any
higher level constructs. We propose differential extrinsic plasticity (DEP) as
a new synaptic rule for self-learning systems and apply it to a number of
complex robotic systems as a test case. Without specifying any purpose or goal,
seemingly purposeful and adaptive behavior is developed, displaying a certain
level of sensorimotor intelligence. These surprising results require no system
specific modifications of the DEP rule but arise rather from the underlying
mechanism of spontaneous symmetry breaking due to the tight
brain-body-environment coupling. The new synaptic rule is biologically
plausible and it would be an interesting target for a neurobiolocal
investigation. We also argue that this neuronal mechanism may have been a
catalyst in natural evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00837</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00837</id><created>2015-05-04</created><authors><author><keyname>Carisimo</keyname><forenames>Esteban</forenames></author><author><keyname>Galperin</keyname><forenames>Hernan</forenames></author><author><keyname>Alvarez-Hamelin</keyname><forenames>Jos&#xe9; Ignacio</forenames></author></authors><title>A new intrinsic way to measure IXP performance: an experience in Bolivia</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bolivia, a landlocked emerging country in South America, has one of the
smallest networks in the whole Internet. Before the IXP implementation,
delivering packets between national ISPs had to be sent them through
international transit links. Being aware of this situation and looking for
increasing the number of users, Bolivian government enacted a law to gather all
national ISPs on a single IXP in 2013.
  In spite of several articles have researched about this topic, no one before
has set the focus on measuring the evolution of end-users parameters in a South
American developing country, moreover after a significant changing on the
topology. For the current work, we have mainly studied hop, latency, traffic
and route variation, a long a seven months. Topology have not been studied
because Bolivian ISPs must be connected each others under legal obligation.
  To achieve our measurement goals, and under absence of global-scale measuring
projects in this country, we have developed our own active-measurement platform
among local ASes. During the platform development we had to deal with local ISP
fears, governmental agencies and regulation pressures.
  We also survey the main previous papers on IXP analysis, and we classfied
them on obtained data and their sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00838</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00838</id><created>2015-05-04</created><authors><author><keyname>Peles</keyname><forenames>Slaven</forenames></author><author><keyname>Klus</keyname><forenames>Stefan</forenames></author></authors><title>Sparse Automatic Differentiation for Large-Scale Computations Using
  Abstract Elementary Algebra</title><categories>cs.MS</categories><comments>Submitted to ACM Transactions on Mathematical Software</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most numerical solvers and libraries nowadays are implemented to use
mathematical models created with language-specific built-in data types (e.g.
real in Fortran or double in C) and their respective elementary algebra
implementations. However, built-in elementary algebra typically has limited
functionality and often restricts flexibility of mathematical models and
analysis types that can be applied to those models. To overcome this
limitation, a number of domain-specific languages with more feature-rich
built-in data types have been proposed. In this paper, we argue that if
numerical libraries and solvers are designed to use abstract elementary algebra
rather than language-specific built-in algebra, modern mainstream languages can
be as effective as any domain-specific language. We illustrate our ideas using
the example of sparse Jacobian matrix computation. We implement an automatic
differentiation method that takes advantage of sparse system structures and is
straightforward to parallelize in MPI setting. Furthermore, we show that the
computational cost scales linearly with the size of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00841</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00841</id><created>2015-05-04</created><authors><author><keyname>Talaika</keyname><forenames>Aliaksandr</forenames></author><author><keyname>Biega</keyname><forenames>Joanna</forenames></author><author><keyname>Amarilli</keyname><forenames>Antoine</forenames></author><author><keyname>Suchanek</keyname><forenames>Fabian M.</forenames></author></authors><title>Harvesting Entities from the Web Using Unique Identifiers -- IBEX</title><categories>cs.DB cs.IR</categories><comments>30 pages, 5 figures, 9 tables. Complete technical report for A.
  Talaika, J. A. Biega, A. Amarilli, and F. M. Suchanek. IBEX: Harvesting
  Entities from the Web Using Unique Identifiers. WebDB workshop, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the prevalence of unique entity identifiers on the
Web. These are, e.g., ISBNs (for books), GTINs (for commercial products), DOIs
(for documents), email addresses, and others. We show how these identifiers can
be harvested systematically from Web pages, and how they can be associated with
human-readable names for the entities at large scale.
  Starting with a simple extraction of identifiers and names from Web pages, we
show how we can use the properties of unique identifiers to filter out noise
and clean up the extraction result on the entire corpus. The end result is a
database of millions of uniquely identified entities of different types, with
an accuracy of 73--96% and a very high coverage compared to existing knowledge
bases. We use this database to compute novel statistics on the presence of
products, people, and other entities on the Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00844</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00844</id><created>2015-05-04</created><authors><author><keyname>Alam</keyname><forenames>Muhammad Raisul</forenames></author><author><keyname>St-Hilaire</keyname><forenames>Marc</forenames></author><author><keyname>Kunz</keyname><forenames>Thomas</forenames></author></authors><title>A Unified Residential Energy Cost Optimization Model for Smart Grid -
  Significance and Challenge</title><categories>cs.SY</categories><comments>12 pages, 3 figures</comments><msc-class>90C26</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses the residential energy cost optimization problem in
smart grid. To date, most of the previous research only consider a partial
aspect of the cost optimization problem. As a result, they fail to analyze
scenarios when the interconnected components along with their properties have
to be considered simultaneously. The proposed model combines these partial
models into a single unified cost optimization model. Therefore, it is able to
analyze scenarios which are closer to practical implementation. Furthermore, it
is useful to analyze the behavior of a population (e.g., smart buildings, smart
cities, etc.) and properties of the components for specific scenarios (e.g.,
the impact of aggregate storage capacity, etc.). It allows energy trading in
microgrid which introduces a cost fairness problem. It ensures Pareto
optimality among the households which guarantees that no household will be
worse off to improve the cost of others. Results show that it can maintain the
user preferences and can react to a demand response program by rescheduling the
household loads and sources. Finally, the paper addresses the challenge of the
computational complexity of the proposed model, showing that solution time
increases exponentially with the problem size and proposes possible approaches
to solve this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00850</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00850</id><created>2015-05-04</created><updated>2015-07-29</updated><authors><author><keyname>Lemos</keyname><forenames>Jo&#xe3;o S.</forenames></author><author><keyname>Monteiro</keyname><forenames>Francisco A.</forenames></author><author><keyname>Sousa</keyname><forenames>Ivo</forenames></author><author><keyname>Rodrigues</keyname><forenames>Ant&#xf3;nio</forenames></author></authors><title>Full-Duplex Relaying in MIMO-OFDM Frequency-Selective Channels with
  Optimal Adaptive Filtering</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-band full-duplex transmission allows a relay station to theoretically
double its spectral efficiency by simultaneously receiving and transmitting in
the same frequency band, when compared to the traditional half-duplex or
out-of-band full-duplex counterpart. Consequently, the induced
self-interference suffered by the relay may reach considerable power levels,
which decreases the signal-to-interference-plus-noise ratio (SINR) in a
decode-and-forward (DF) relay, leading to a degradation of the relay
performance. This paper presents a technique to cope with the problem of
self-interference in broadband multiple-input multiple-output (MIMO) relays.
The proposed method uses a time-domain cancellation in a DF relay, where a
replica of the interfering signal is created with the help of a recursive least
squares (RLS) algorithm that estimates the interference frequency-selective
channel. Its convergence mean time is shown to be negligible by simulation
results, when compared to the length of a typical orthogonal-frequency division
multiplexing (OFDM) sequences. Moreover, the bit-error-rate (BER) and the SINR
in a OFDM transmission are evaluated, confirming that the proposed method
extends significantly the range of self-interference power to which the relay
is resilient to, when compared with other mitigation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00851</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00851</id><created>2015-05-04</created><authors><author><keyname>Wang</keyname><forenames>Zifu</forenames></author><author><keyname>Henneron</keyname><forenames>Thomas</forenames></author><author><keyname>Hofmann</keyname><forenames>Heath</forenames></author></authors><title>Space-Time Galerkin Projection of Electro-Magnetic Fields</title><categories>cs.CE physics.comp-ph</categories><comments>Published at Compumag 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial Galerkin projection transfers fields between different meshes. In the
area of finite element analysis of electromagnetic fields, it provides great
convenience for remeshing, multi-physics, domain decomposition methods, etc. In
this paper, a space-time Galerkin projection is developed in order to transfer
fields between different spatial and temporal discretization bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00853</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00853</id><created>2015-05-04</created><updated>2015-11-27</updated><authors><author><keyname>Xu</keyname><forenames>Bing</forenames></author><author><keyname>Wang</keyname><forenames>Naiyan</forenames></author><author><keyname>Chen</keyname><forenames>Tianqi</forenames></author><author><keyname>Li</keyname><forenames>Mu</forenames></author></authors><title>Empirical Evaluation of Rectified Activations in Convolutional Network</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the performance of different types of rectified
activation functions in convolutional neural network: standard rectified linear
unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified
linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU).
We evaluate these activation function on standard image classification task.
Our experiments suggest that incorporating a non-zero slope for negative part
in rectified activation units could consistently improve the results. Thus our
findings are negative on the common belief that sparsity is the key of good
performance in ReLU. Moreover, on small scale dataset, using deterministic
negative slope or learning it are both prone to overfitting. They are not as
effective as using their randomized counterpart. By using RReLU, we achieved
75.68\% accuracy on CIFAR-100 test set without multiple test or ensemble.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00855</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00855</id><created>2015-05-04</created><authors><author><keyname>Saleh</keyname><forenames>Babak</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Large-scale Classification of Fine-Art Paintings: Learning The Right
  Metric on The Right Feature</title><categories>cs.CV cs.IR cs.LG cs.MM</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years, the number of fine-art collections that are digitized
and publicly available has been growing rapidly. With the availability of such
large collections of digitized artworks comes the need to develop multimedia
systems to archive and retrieve this pool of data. Measuring the visual
similarity between artistic items is an essential step for such multimedia
systems, which can benefit more high-level multimedia tasks. In order to model
this similarity between paintings, we should extract the appropriate visual
features for paintings and find out the best approach to learn the similarity
metric based on these features. We investigate a comprehensive list of visual
features and metric learning approaches to learn an optimized similarity
measure between paintings. We develop a machine that is able to make
aesthetic-related semantic-level judgments, such as predicting a painting's
style, genre, and artist, as well as providing similarity measures optimized
based on the knowledge available in the domain of art historical
interpretation. Our experiments show the value of using this similarity measure
for the aforementioned prediction tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00862</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00862</id><created>2015-05-04</created><authors><author><keyname>Song</keyname><forenames>Shuangyong</forenames></author><author><keyname>Meng</keyname><forenames>Yao</forenames></author></authors><title>Classifying and Ranking Microblogging Hashtags with News Categories</title><categories>cs.IR</categories><comments>2 pages, no figure, to be appeared on RCIS 2015</comments><msc-class>68T50</msc-class><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In microblogging, hashtags are used to be topical markers, and they are
adopted by users that contribute similar content or express a related idea.
However, hashtags are created in a free style and there is no domain category
information about them, which make users hard to get access to organized
hashtag presentation. In this paper, we propose an approach that classifies
hashtags with news categories, and then carry out a domain-sensitive popularity
ranking to get hot hashtags in each domain. The proposed approach first trains
a domain classification model with news content and news category information,
then detects microblogs related to a hashtag to be its representative text,
based on which we can classify this hashtag with a domain. Finally, we
calculate the domain-sensitive popularity of each hashtag with multiple
factors, to get most hotly discussed hashtags in each domain. Preliminary
experimental results on a dataset from Sina Weibo, one of the largest Chinese
microblogging websites, show usefulness of the proposed approach on describing
hashtags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00863</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00863</id><created>2015-05-04</created><authors><author><keyname>Song</keyname><forenames>Shuangyong</forenames></author><author><keyname>Meng</keyname><forenames>Yao</forenames></author><author><keyname>Zheng</keyname><forenames>Zhongguang</forenames></author><author><keyname>Sun</keyname><forenames>Jun</forenames></author></authors><title>A Feature-based Classification Technique for Answering Multi-choice
  World History Questions</title><categories>cs.IR cs.AI cs.CL</categories><comments>5 pages, no figure</comments><msc-class>68T50</msc-class><acm-class>H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11.
In this paper, we describe our system for solving real-world university
entrance exam questions, which are related to world history. Wikipedia is used
as the main external resource for our system. Since problems with choosing
right/wrong sentence from multiple sentence choices account for about
two-thirds of the total, we individually design a classification based model
for solving this type of questions. For other types of questions, we also
design some simple methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00864</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00864</id><created>2015-05-04</created><updated>2015-11-16</updated><authors><author><keyname>Yang</keyname><forenames>Shihao</forenames></author><author><keyname>Santillana</keyname><forenames>Mauricio</forenames></author><author><keyname>Kou</keyname><forenames>S. C.</forenames></author></authors><title>Accurate estimation of influenza epidemics using Google search data via
  ARGO</title><categories>stat.AP cs.SI stat.ML</categories><comments>23 pages, 2 figures, Proceedings of the National Academy of Sciences
  (2015)</comments><doi>10.1073/pnas.1515373112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate real-time tracking of influenza outbreaks helps public health
officials make timely and meaningful decisions that could save lives. We
propose an influenza tracking model, ARGO (AutoRegression with GOogle search
data), that uses publicly available online search data. In addition to having a
rigorous statistical foundation, ARGO outperforms all previously available
Google-search-based tracking models, including the latest version of Google Flu
Trends, even though it uses only low-quality search data as input from publicly
available Google Trends and Google Correlate websites. ARGO not only
incorporates the seasonality in influenza epidemics but also captures changes
in people's online search behavior over time. ARGO is also flexible,
self-correcting, robust, and scalable, making it a potentially powerful tool
that can be used for real-time tracking of other social events at multiple
temporal and spatial resolutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00866</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00866</id><created>2015-05-04</created><authors><author><keyname>Moreno</keyname><forenames>Juan C.</forenames></author><author><keyname>Prasath</keyname><forenames>V. B. Surya</forenames></author><author><keyname>Vorotnikov</keyname><forenames>D.</forenames></author><author><keyname>Proenca</keyname><forenames>H.</forenames></author><author><keyname>Palaniappan</keyname><forenames>K.</forenames></author></authors><title>Adaptive diffusion constrained total variation scheme with application
  to `cartoon + texture + edge' image decomposition</title><categories>cs.CV</categories><msc-class>68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an image decomposition model involving a variational
(minimization) problem and an evolutionary partial differential equation (PDE).
We utilize a linear inhomogenuous diffusion constrained and weighted total
variation (TV) scheme for image adaptive decomposition. An adaptive weight
along with TV regularization splits a given image into three components
representing the geometrical (cartoon), textural (small scale - microtextures),
and edges (big scale - macrotextures). We study the wellposedness of the
coupled variational-PDE scheme along with an efficient numerical scheme based
on Chambolle's dual minimization method. We provide extensive experimental
results in cartoon-texture-edges decomposition, and denoising as well compare
with other related variational, coupled anisotropic diffusion PDE based
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00870</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00870</id><created>2015-05-04</created><updated>2015-06-26</updated><authors><author><keyname>Davis</keyname><forenames>Damek</forenames></author></authors><title>An $O(n\log(n))$ Algorithm for Projecting Onto the Ordered Weighted
  $\ell_1$ Norm Ball</title><categories>math.OC cs.LG</categories><comments>1 Figures, 1 table, 14 pages, Example added to appendix</comments><msc-class>49M99 (primary), 90C90, 90C25, 49N45 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ordered weighted $\ell_1$ (OWL) norm is a newly developed generalization
of the Octogonal Shrinkage and Clustering Algorithm for Regression (OSCAR)
norm. This norm has desirable statistical properties and can be used to perform
simultaneous clustering and regression. In this paper, we show how to compute
the projection of an $n$-dimensional vector onto the OWL norm ball in
$O(n\log(n))$ operations. In addition, we illustrate the performance of our
algorithm on a synthetic regression test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00874</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00874</id><created>2015-05-04</created><authors><author><keyname>Dasler</keyname><forenames>Philip</forenames></author><author><keyname>Mount</keyname><forenames>David M.</forenames></author></authors><title>On the Complexity of an Unregulated Traffic Crossing</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The steady development of motor vehicle technology will enable cars of the
near future to assume an ever increasing role in the decision making and
control of the vehicle itself. In the foreseeable future, cars will have the
ability to communicate with one another in order to better coordinate their
motion. This motivates a number of interesting algorithmic problems. One of the
most challenging aspects of traffic coordination involves traffic
intersections. In this paper we consider two formulations of a simple and
fundamental geometric optimization problem involving coordinating the motion of
vehicles through an intersection.
  We are given a set of $n$ vehicles in the plane, each modeled as a unit
length line segment that moves monotonically, either horizontally or
vertically, subject to a maximum speed limit. Each vehicle is described by a
start and goal position and a start time and deadline. The question is whether,
subject to the speed limit, there exists a collision-free motion plan so that
each vehicle travels from its start position to its goal position prior to its
deadline.
  We present three results. We begin by showing that this problem is
NP-complete with a reduction from 3-SAT. Second, we consider a constrained
version in which cars traveling horizontally can alter their speeds while cars
traveling vertically cannot. We present a simple algorithm that solves this
problem in $O(n \log n)$ time. Finally, we provide a solution to the discrete
version of the problem and prove its asymptotic optimality in terms of the
maximum delay of a vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00875</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00875</id><created>2015-05-04</created><updated>2015-10-06</updated><authors><author><keyname>Boman</keyname><forenames>Erik G.</forenames></author><author><keyname>Deweese</keyname><forenames>Kevin</forenames></author><author><keyname>Gilbert</keyname><forenames>John R.</forenames></author></authors><title>Evaluating the Potential of a Dual Randomized Kaczmarz Solver for
  Laplacian Linear Systems</title><categories>cs.DS</categories><comments>increased font size in figures for readability, added weak scaling
  figures, improved citations to application areas, changed terminology
  slightly from network graphs to irregular graphs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for solving Laplacian linear systems proposed by Kelner et al.
involves the random sampling and update of fundamental cycles in a graph.
Kelner et al. proved asymptotic bounds on the complexity of this method but did
not report experimental results. We seek to both evaluate the performance of
this approach and to explore improvements to it in practice. We compare the
performance of this method to other Laplacian solvers on a variety of real
world graphs. We consider different ways to improve the performance of this
method by exploring different ways of choosing the set of cycles and the
sequence of updates, with the goal of providing more flexibility and potential
parallelism. We propose a parallel model of the Kelner et al. method, for
evaluating potential parallelism in terms of the span of edges updated at each
iteration. We provide experimental results comparing the potential parallelism
of the fundamental cycle basis and our extended cycle set. Our preliminary
experiments show that choosing a non-fundamental set of cycles can save
significant work compared to a fundamental cycle basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00876</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00876</id><created>2015-05-05</created><authors><author><keyname>Batoul</keyname><forenames>Aicha</forenames></author><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>Constacyclic Codes Over Finite Principal Ideal Rings</title><categories>cs.IT math.IT math.RA</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we give an important isomorphism between contacyclic codes and
cyclic codes over finite principal ideal rings. Necessary and sufficient
conditions for the existence of non-trivial cyclic self-dual codes over finite
principal ideal rings are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00880</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00880</id><created>2015-05-05</created><updated>2015-09-27</updated><authors><author><keyname>Su</keyname><forenames>Hang</forenames></author><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author><author><keyname>Kalogerakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Learned-Miller</keyname><forenames>Erik</forenames></author></authors><title>Multi-view Convolutional Neural Networks for 3D Shape Recognition</title><categories>cs.CV cs.GR</categories><comments>v1: Initial version. v2: An updated ModelNet40 training/test split is
  used; results with low-rank Mahalanobis metric learning are added. v3 (ICCV
  2015): A second camera setup without the upright orientation assumption is
  added; some accuracy and mAP numbers are changed slightly because a small
  issue in mesh rendering related to specularities is fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A longstanding question in computer vision concerns the representation of 3D
shapes for recognition: should 3D shapes be represented with descriptors
operating on their native 3D formats, such as voxel grid or polygon mesh, or
can they be effectively represented with view-based descriptors? We address
this question in the context of learning to recognize 3D shapes from a
collection of their rendered views on 2D images. We first present a standard
CNN architecture trained to recognize the shapes' rendered views independently
of each other, and show that a 3D shape can be recognized even from a single
view at an accuracy far higher than using state-of-the-art 3D shape
descriptors. Recognition rates further increase when multiple views of the
shapes are provided. In addition, we present a novel CNN architecture that
combines information from multiple views of a 3D shape into a single and
compact shape descriptor offering even better recognition performance. The same
architecture can be applied to accurately recognize human hand-drawn sketches
of shapes. We conclude that a collection of 2D views can be highly informative
for 3D shape recognition and is amenable to emerging CNN architectures and
their derivatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00887</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00887</id><created>2015-05-05</created><updated>2016-01-27</updated><authors><author><keyname>Li</keyname><forenames>Jiyou</forenames></author><author><keyname>Luo</keyname><forenames>Chu</forenames></author><author><keyname>Xu</keyname><forenames>Zeying</forenames></author></authors><title>The Minimal and Maximal Sensitivity of the Simplified Weighted Sum
  Function</title><categories>cs.DM</categories><comments>6 pages</comments><msc-class>68R05</msc-class><acm-class>G.2.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Sensitivity is an important complexity measure of Boolean functions. In this
paper we present properties of the minimal and maximal sensitivity of the
simplified weighted sum function. A simple close formula of the minimal
sensitivity of the simplified weighted sum function is obtained. A phenomenon
is exhibited that the minimal sensitivity of the weighted sum function is
indeed an indicator of large primes, that is, for large prime number p, the
minimal sensitivity of the weighted sum function is always equal to one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00895</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00895</id><created>2015-05-05</created><authors><author><keyname>Chakrabarty</keyname><forenames>Indranil</forenames></author><author><keyname>Khan</keyname><forenames>Shahzor</forenames></author><author><keyname>Singh</keyname><forenames>Vanshdeep</forenames></author></authors><title>Dynamic Grover Search: Applications in Recommendation systems and
  Optimization problems</title><categories>quant-ph cs.DS</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years we have seen that Grover search algorithm [1] by using
quantum parallelism has revolutionized the field of solving huge class of NP
problems in comparison to classical systems. In this work we explore the idea
of extending the Grover search algorithm to approximate algorithms. Here we try
to analyze the applicability of Grover search to process an unstructured
database with dynamic selection function as compared to the static selection
function in the original work[1]. This allows us to extend the application of
Grover search to the field of randomized search algorithms. We further use the
Dynamic Grover search algorithm to define the goals for a recommendation
system, and define the algorithm for recommendation system for binomial
similarity distribution space giving us a quadratic speedup over traditional
unstructured recommendation systems. Finally we see how the Dynamic Grover
Search can be used to attack a wide range of optimization problems where we
improve complexity over existing optimization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00898</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00898</id><created>2015-05-05</created><updated>2015-12-07</updated><authors><author><keyname>Armbruster</keyname><forenames>Benjamin</forenames></author><author><keyname>Besenyei</keyname><forenames>&#xc1;d&#xe1;m</forenames></author><author><keyname>Simon</keyname><forenames>P&#xe9;ter L.</forenames></author></authors><title>Bounds for the expected value of one-step processes</title><categories>math.DS cs.SI</categories><comments>14 pages, 4 figures, revised</comments><msc-class>60J75, 34C11, 92D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean-field models are often used to approximate Markov processes with large
state-spaces. One-step processes, also known as birth-death processes, are an
important class of such processes and are processes with state space
$\{0,1,\ldots,N\}$ and where each transition is of size one. We derive explicit
bounds on the expected value of such a process, bracketing it between the
mean-field model and another simple ODE. Our bounds require that the Markov
transition rates are density dependent polynomials that satisfy a sign
condition. We illustrate the tightness of our bounds on the SIS epidemic
process and the voter model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00903</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00903</id><created>2015-05-05</created><authors><author><keyname>Cheng</keyname><forenames>Betty</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author><author><keyname>Gogolla</keyname><forenames>Martin</forenames></author><author><keyname>Grunske</keyname><forenames>Lars</forenames></author><author><keyname>Litoiu</keyname><forenames>Marin</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Hausi</forenames></author><author><keyname>Pelliccione</keyname><forenames>Patrizio</forenames></author><author><keyname>Perini</keyname><forenames>Anna</forenames></author><author><keyname>Qureshi</keyname><forenames>Nauman</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schneider</keyname><forenames>Daniel</forenames></author><author><keyname>Trollmann</keyname><forenames>Frank</forenames></author><author><keyname>Villegas</keyname><forenames>Norha</forenames></author></authors><title>Using Models at Runtime to Address Assurance for Self-Adaptive Systems</title><categories>cs.SE</categories><comments>36 pages, 2 figures, In: Models@run.time, LNCS 8378, pp. 101-136,
  Springer Publisher, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A self-adaptive software system modifies its behavior at runtime in response
to changes within the system or in its execution environment. The fulfillment
of the system requirements needs to be guaranteed even in the presence of
adverse conditions and adaptations. Thus, a key challenge for self-adaptive
software systems is assurance. Traditionally, confidence in the correctness of
a system is gained through a variety of activities and processes performed at
development time, such as design analysis and testing. In the presence of
selfadaptation, however, some of the assurance tasks may need to be performed
at runtime. This need calls for the development of techniques that enable
continuous assurance throughout the software life cycle. Fundamental to the
development of runtime assurance techniques is research into the use of models
at runtime (M@RT). This chapter explores the state of the art for usingM@RT to
address the assurance of self-adaptive software systems. It defines what
information can be captured by M@RT, specifically for the purpose of assurance,
and puts this definition into the context of existing work. We then outline key
research challenges for assurance at runtime and characterize assurance
methods. The chapter concludes with an exploration of selected application
areas where M@RT could provide significant benefits beyond existing assurance
techniques for adaptive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00904</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00904</id><created>2015-05-05</created><authors><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Roth</keyname><forenames>Alexander</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>Code Generator Composition for Model-Driven Engineering of Robotics
  Component &amp; Connector Systems</title><categories>cs.SE</categories><comments>12 pages, 4 figures, In: Proceedings of the 1st International
  Workshop on Model-Driven Robot Software Engineering (MORSE 2014), York, Great
  Britain, Volume 1319 of CEUR Workshop Proceedings, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Engineering software for robotics applications requires multidomain and
application-specific solutions. Model-driven engineering and modeling language
integration provide means for developing specialized, yet reusable models of
robotics software architectures. Code generators transform these platform
independent models into executable code specific to robotic platforms.
Generative software engineering for multidomain applications requires not only
the integration of modeling languages but also the integration of validation
mechanisms and code generators. In this paper we sketch a conceptual model for
code generator composition and show an instantiation of this model in the
MontiArc- Automaton framework. MontiArcAutomaton allows modeling software
architectures as component and connector models with different component
behavior modeling languages. Effective means for code generator integration are
a necessity for the post hoc integration of applicationspecific languages in
model-based robotics software engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00906</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00906</id><created>2015-05-05</created><authors><author><keyname>Katis</keyname><forenames>Evangelos</forenames></author></authors><title>Resource Management of energy-aware Cognitive Radio Networks and
  cloud-based Infrastructures</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of wireless networks has been rapidly developed during the past
decade due to the increasing popularity of the mobile devices. The great demand
for mobility and connectivity makes wireless networking a field whose
continuous technological development is very important as new challenges and
issues are arising. Many scientists and researchers are currently engaged in
developing new approaches and optimization methods in several topics of
wireless networking. This survey paper study works from the following topics:
Cognitive Radio Networks, Interactive Broadcasting, Energy Efficient Networks,
Cloud Computing and Resource Management, Interactive Marketing and
Optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00908</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00908</id><created>2015-05-05</created><authors><author><keyname>L&#xe9;on</keyname><forenames>Aur&#xe9;lia</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author></authors><title>Reinforced Decision Trees</title><categories>cs.LG</categories><report-no>Accepted as a poster at EWRL 2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to speed-up classification models when facing a large number of
categories, one usual approach consists in organizing the categories in a
particular structure, this structure being then used as a way to speed-up the
prediction computation. This is for example the case when using
error-correcting codes or even hierarchies of categories. But in the majority
of approaches, this structure is chosen \textit{by hand}, or during a
preliminary step, and not integrated in the learning process. We propose a new
model called Reinforced Decision Tree which simultaneously learns how to
organize categories in a tree structure and how to classify any input based on
this structure. This approach keeps the advantages of existing techniques (low
inference complexity) but allows one to build efficient classifiers in one
learning step. The learning algorithm is inspired by reinforcement learning and
policy-gradient techniques which allows us to integrate the two steps (building
the tree, and learning the classifier) in one single algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00911</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00911</id><created>2015-05-05</created><authors><author><keyname>Graziotin</keyname><forenames>Daniel</forenames></author></authors><title>Green open access in computer science - an exploratory study on
  author-based self-archiving awareness, practice, and inhibitors</title><categories>cs.DL cs.CY</categories><comments>11 pages, 7 figures. Published in Science Open Research journal</comments><doi>10.14293/A2199-1006.01.SOR-COMPSCI.LZQ19.v1</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Access to the work of others is something that is too often taken for
granted, yet problematic and difficult to be obtained unless someone pays for
it. Green and gold open access are claimed to be a solution to this problem.
While open access is gaining momentum in some fields, there is a limited and
seasoned knowledge about self-archiving in computer science. In particular,
there is an inadequate understanding of author-based self-archiving awareness,
practice, and inhibitors. This article reports an exploratory study of the
awareness of self-archiving, the practice of self-archiving, and the inhibitors
of self-archiving among authors in an Italian computer science faculty.
Forty-nine individuals among interns, PhD students, researchers, and professors
were recruited in a questionnaire (response rate of 72.8%). The quantitative
and qualitative responses suggested that there is still work needed in terms of
advocating green open access to computer science authors who seldom
self-archive and when they do, they often infringe the copyright transfer
agreements (CTAs) of the publishers. In addition, tools from the open-source
community are needed to facilitate author-based self-archiving, which should
comprise of an automatic check of the CTAs. The study identified nine factors
inhibiting the act of self-archiving among computer scientists. As a first
step, this study proposes several propositions regarding author-based
self-archiving in computer science that can be further investigated.
Recommendations to foster self-archiving in computer science, based on the
results, are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00914</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00914</id><created>2015-05-05</created><authors><author><keyname>Cadenas</keyname><forenames>Jos&#xe9; O.</forenames></author><author><keyname>Megson</keyname><forenames>Graham</forenames></author></authors><title>An Empirical Evaluation of Preconditioning Data for Accelerating Convex
  Hull Computations</title><categories>cs.CG</categories><comments>20 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convex hull describes the extent or shape of a set of data and is used
ubiquitously in computational geometry. Common algorithms to construct the
convex hull on a finite set of n points (x,y) range from O(nlogn) time to O(n)
time. However, it is often the case that a heuristic procedure is applied to
reduce the original set of n points to a set of s &lt; n points which contains the
hull and so accelerates the final hull finding procedure. We present an
algorithm to precondition data before building a 2D convex hull with integer
coordinates, with three distinct advantages. First, for all practical purposes,
it is linear; second, no explicit sorting of data is required and third, the
reduced set of s points is constructed such that it forms an ordered set that
can be directly pipelined into an O(n) time convex hull algorithm. Under these
criteria a fast (or O(n)) pre-conditioner in principle creates a fast convex
hull (approximately O(n)) for an arbitrary set of points. The paper empirically
evaluates and quantifies the acceleration generated by the method against the
most common convex hull algorithms. An extra acceleration of at least four
times when compared to previous existing preconditioning methods is found from
experiments on a dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00919</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00919</id><created>2015-05-05</created><updated>2016-01-28</updated><authors><author><keyname>Raviv</keyname><forenames>Netanel</forenames></author><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>Constructions of High-Rate MSR Codes over Small Fields</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel technique for construction of minimum storage regenerating (MSR)
codes is presented. Based on this technique, three explicit constructions of
MSR codes are given. The first two constructions provide access-optimal MSR
codes, with two and three parities, respectively, which attain the
sub-packetization bound for access-optimal codes. The third construction
provides longer MSR codes with three parities, which are not access-optimal,
and do not necessarily attain the sub-packetization bound.
  In addition to a minimum storage in a node, all three constructions allow the
entire data to be recovered from a minimal number of storage nodes. That is,
given storage $\ell$ in each node, the entire stored data can be recovered from
any $2\log_2 \ell$ for 2 parity nodes, and either $3\log_3\ell$ or
$4\log_3\ell$ for 3 parity nodes. Second, in the first two constructions, a
helper node accesses the minimum number of its symbols for repair of a failed
node (access-optimality). The generator matrix of these codes is based on
perfect matchings of complete graphs and hypergraphs, and on a rational
canonical form of matrices. The goal of this paper is to provide a construction
of such optimal codes over the smallest possible finite fields. For two
parities, the field size is reduced by a factor of two for access-optimal codes
compared to previous constructions. For three parities, in the first
construction the field size is $6\log_3 \ell+1$ (or $3\log_3 \ell+1$ for fields
with characteristic 2), and in the second construction the field size is
larger, yet linear in $\log_3\ell$. Both constructions with 3 parities provide
a significant improvement over existing previous works, since only non-explicit
constructions with exponential field size (in $\log_3\ell$) were known so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00920</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00920</id><created>2015-05-05</created><authors><author><keyname>J</keyname><forenames>BalaSuyambu</forenames></author><author><keyname>R</keyname><forenames>Radha</forenames></author><author><keyname>R</keyname><forenames>Rama</forenames></author></authors><title>New 2D CA based Image Encryption Scheme and a novel Non-Parametric Test
  for Pixel Randomness</title><categories>cs.CR</categories><comments>This work we had already submitted to Journal of Cryptology and was
  not published then we had submitted to IJIG, since its not processed for a
  long time we are withdrawing it and submitting to arXiv</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have proposed a new test for pixel randomness using
non-parametric method in statistics. In order to validate this new
non-parametric test we have designed an encryption scheme based on 2D cellular
automata. The strength of the designed encryption scheme is first assessed by
standard methods for security analysis and the pixel randomness is then
determined by the newly proposed non-parametric method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00921</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00921</id><created>2015-05-05</created><authors><author><keyname>Minelli</keyname><forenames>Mattia</forenames></author><author><keyname>Ma</keyname><forenames>Maode</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author><author><keyname>Kelif</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Sigelle</keyname><forenames>Marc</forenames></author><author><keyname>Godlewski</keyname><forenames>Philippe</forenames></author></authors><title>Uplink Energy-Delay Trade-off under Optimized Relay Placement in
  Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relay nodes-enhanced architectures are deemed a viable solution to enhance
coverage and capacity of nowadays cellular networks. Besides a number of
desirable features, these architectures reduce the average distance between
users and network nodes, thus allowing for battery savings for users
transmitting on the uplink. In this paper, we investigate the extent of these
savings, by optimizing relay nodes deployment in terms of uplink energy
consumption per transmitted bit, while taking into account a minimum uplink
average user delay that has to be guaranteed. A novel performance evaluation
framework for uplink relay networks is first proposed to study this
energy-delay trade-off. A simulated annealing is then run to find an optimized
relay placement solution under a delay constraint; exterior penalty functions
are used in order to deal with a difficult energy landscape, in particular when
the constraint is tight. Finally, results show that relay nodes deployment
consistently improve users uplink energy efficiency, under a wide range of
traffic conditions and that relays are particularly efficient in non-uniform
traffic scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00922</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00922</id><created>2015-05-05</created><authors><author><keyname>Graziotin</keyname><forenames>Daniel</forenames><affiliation>Free University of Bozen-Bolzano</affiliation></author><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames><affiliation>Free University of Bozen-Bolzano</affiliation></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames><affiliation>Free University of Bozen-Bolzano</affiliation></author></authors><title>Happy software developers solve problems better: psychological
  measurements in empirical software engineering</title><categories>cs.SE cs.HC</categories><comments>33 pages, 11 figures, published at PeerJ</comments><journal-ref>PeerJ, vol. 2, pp. e289, 2014</journal-ref><doi>10.7717/peerj.289</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  For more than 30 years, it has been claimed that a way to improve software
developers' productivity and software quality is to focus on people and to
provide incentives to make developers satisfied and happy. This claim has
rarely been verified in software engineering research, which faces an
additional challenge in comparison to more traditional engineering fields:
software development is an intellectual activity and is dominated by
often-neglected human aspects. Among the skills required for software
development, developers must possess high analytical problem-solving skills and
creativity for the software construction process. According to psychology
research, affects-emotions and moods-deeply influence the cognitive processing
abilities and performance of workers, including creativity and analytical
problem solving. Nonetheless, little research has investigated the correlation
between the affective states, creativity, and analytical problem-solving
performance of programmers. This article echoes the call to employ
psychological measurements in software engineering research. We report a study
with 42 participants to investigate the relationship between the affective
states, creativity, and analytical problem-solving skills of software
developers. The results offer support for the claim that happy developers are
indeed better problem solvers in terms of their analytical abilities. The
following contributions are made by this study: (1) providing a better
understanding of the impact of affective states on the creativity and
analytical problem-solving capacities of developers, (2) introducing and
validating psychological measurements, theories, and concepts of affective
states, creativity, and analytical-problem-solving skills in empirical software
engineering, and (3) raising the need for studying the human factors of
software engineering by employing a multidisciplinary viewpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00925</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00925</id><created>2015-05-05</created><authors><author><keyname>Srihari</keyname><forenames>Sriganesh</forenames></author><author><keyname>Leong</keyname><forenames>Hon Wai</forenames></author></authors><title>Parameterized Algorithms for Clustering PPI Networks</title><categories>q-bio.MN cs.CE</categories><comments>10 pages, 4 tables</comments><msc-class>92C42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of high-throughput wet lab technologies the amount of protein
interaction data available publicly has increased substantially, in turn
spurring a plethora of computational methods for in silico knowledge discovery
from this data. In this paper, we focus on parameterized methods for modeling
and solving complex computational problems encountered in such knowledge
discovery from protein data. Specifically, we concentrate on three relevant
problems today in proteomics, namely detection of lethal proteins, functional
modules and alignments from protein interaction networks. We propose novel
graph theoretic models for these problems and devise practical parameterized
algorithms. At a broader level, we demonstrate how these methods can be viable
alternatives for the several heurestic, randomized, approximation and
sub-optimal methods by arriving at parameterized yet optimal solutions for
these problems. We substantiate these theoretical results by experimenting on
real protein interaction data of S. cerevisiae (budding yeast) and verifying
the results using gene ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00936</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00936</id><created>2015-05-05</created><authors><author><keyname>Gianniotis</keyname><forenames>Nikolaos</forenames></author><author><keyname>K&#xfc;gler</keyname><forenames>Dennis</forenames></author><author><keyname>Tino</keyname><forenames>Peter</forenames></author><author><keyname>Polsterer</keyname><forenames>Kai</forenames></author><author><keyname>Misra</keyname><forenames>Ranjeev</forenames></author></authors><title>Autoencoding Time Series for Visualisation</title><categories>astro-ph.IM cs.NE</categories><comments>Published in ESANN 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for the visualisation of time series. To that end we
employ echo state networks to convert time series into a suitable vector
representation which is capable of capturing the latent dynamics of the time
series. Subsequently, the obtained vector representations are put through an
autoencoder and the visualisation is constructed using the activations of the
bottleneck. The crux of the work lies with defining an objective function that
quantifies the reconstruction error of these representations in a principled
manner. We demonstrate the method on synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00940</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00940</id><created>2015-05-05</created><authors><author><keyname>Bonaventura</keyname><forenames>Luca</forenames></author><author><keyname>Ferretti</keyname><forenames>Roberto</forenames></author></authors><title>Flux form Semi-Lagrangian methods for parabolic problems</title><categories>math.NA cs.CE cs.NA physics.ao-ph</categories><msc-class>35L02, 65M60, 65M25, 65M12, 65M08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A semi-Lagrangian method for parabolic problems is proposed, that extends
previous work by the authors to achieve a fully conservative, flux-form
discretization of linear and nonlinear diffusion equations. A basic consistency
and convergence analysis are proposed. Numerical examples validate the proposed
method and display its potential for consistent semi-Lagrangian discretization
of advection--diffusion and nonlinear parabolic problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00946</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00946</id><created>2015-05-05</created><updated>2015-05-12</updated><authors><author><keyname>Cicalese</keyname><forenames>Danilo</forenames></author><author><keyname>Giordano</keyname><forenames>Danilo</forenames></author><author><keyname>Finamore</keyname><forenames>Alessandro</forenames></author><author><keyname>Mellia</keyname><forenames>Marco</forenames></author><author><keyname>Munaf&#xf2;</keyname><forenames>Maurizio</forenames></author><author><keyname>Rossi</keyname><forenames>Dario</forenames></author><author><keyname>Joumblatt</keyname><forenames>Diana</forenames></author></authors><title>A First Look at Anycast CDN Traffic</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anycast routing is an IP solution that allows packets to be routed to the
topologically nearest server. Over the last years it has been commonly adopted
to manage some services running on top of UDP, e.g., public DNS resolvers,
multicast rendez-vous points, etc. However, recently the Internet have
witnessed the growth of new Anycast-enabled Content Delivery Networks (A-CDNs)
such as CloudFlare and EdgeCast, which provide their web services (i.e., TCP
traffic) entirely through anycast.
  To the best of our knowledge, little is known in the literature about the
nature and the dynamic of such traffic. For instance, since anycast depends on
the routing, the question is how stable are the paths toward the nearest
server. To bring some light on this question, in this work we provide a first
look at A-CDNs traffic by combining active and passive measurements. In
particular, building upon our previous work, we use active measurements to
identify and geolocate A-CDNs caches starting from a large set of IP addresses
related to the top-100k Alexa websites. We then look at the traffic of those
caches in the wild using a large passive dataset collected from a European ISP.
  We find that several A-CDN servers are encountered on a daily basis when
browsing the Internet. Routes to A-CDN servers are very stable, with few
changes that are observed on a monthly-basis (in contrast to more the dynamic
traffic policies of traditional CDNs). Overall, A-CDNs are a reality worth
further investigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00947</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00947</id><created>2015-05-05</created><authors><author><keyname>Xu</keyname><forenames>Haisheng</forenames></author><author><keyname>Blum</keyname><forenames>Rick S.</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Yuan</keyname><forenames>Jian</forenames></author></authors><title>Colocated MIMO Radar Waveform Design for Transmit Beampattern Formation</title><categories>cs.IT math.IT</categories><comments>22 pages, 6 figures, Accepted by IEEE Transactions on Aerospace and
  Electronic Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, colocated MIMO radar waveform design is considered by
minimizing the integrated side-lobe level to obtain beam patterns with lower
side-lobe levels than competing methods. First, a quadratic programming problem
is formulated to design beam patterns by using the criteria for a minimal
integrated side-lobe level. A theorem is derived that provides a closed-form
analytical optimal solution that appears to be an extension of the Rayleigh
quotient minimization for a possibly singular matrix in quadratic form. Such
singularities are shown to occur in the problem of interest, but proofs for the
optimum solution in these singular matrix cases could not be found in the
literature. Next, an additional constraint is added to obtain beam patterns
with desired 3 dB beamwidths, resulting in a nonconvex quadratically
constrained quadratic program which is NP-hard. A semidefinite program and a
Gaussian randomized semidefinite relaxation are used to determine feasible
solutions arbitrarily close to the solution to the original problem.
Theoretical and numerical analyses illustrate the impacts of changing the
number of transmitters and orthogonal waveforms employed in the designs.
Numerical comparisons are conducted to evaluate the proposed design approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00950</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00950</id><created>2015-05-05</created><authors><author><keyname>Burgos</keyname><forenames>Andres C.</forenames></author><author><keyname>Polani</keyname><forenames>Daniel</forenames></author></authors><title>Cooperation and antagonism in information exchange in a growth scenario
  with two species</title><categories>cs.MA cs.GT cs.IT math.IT</categories><comments>Submitted to the Journal of Theoretical Biology</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We consider a simple information-theoretic model of communication, in which
two species of bacteria have the option of exchanging information about their
environment, thereby improving their chances of survival. For this purpose, we
model a system consisting of two species whose dynamics in the world are
modelled by a bet-hedging strategy. It is well known that such models lend
themselves to elegant information-theoretical interpretations by relating their
respective long-term growth rate to the information the individual species has
about its environment. We are specifically interested in modelling how this
dynamics are affected when the species interact cooperatively or in an
antagonistic way in a scenario with limited resources. For this purpose, we
consider the exchange of environmental information between the two species in
the framework of a game. Our results show that a transition from a cooperative
to an antagonistic behaviour in a species results as a response to a change in
the availability of resources. Species cooperate in abundance of resources,
while they behave antagonistically in scarcity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00953</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00953</id><created>2015-05-05</created><authors><author><keyname>Zhang</keyname><forenames>Jiayi</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Han</keyname><forenames>Yanjun</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>On the Ergodic Capacity of MIMO Free-Space Optical Systems over
  Turbulence Channels</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures, This paper has been accepted by IEEE Journal on
  Selected Areas in Communications - Special Issue on Optical Wireless
  Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The free-space optical (FSO) communications can achieve high capacity with
huge unlicensed optical spectrum and low operational costs. The corresponding
performance analysis of FSO systems over turbulence channels is very limited,
especially when using multiple apertures at both transmitter and receiver
sides. This paper aim to provide the ergodic capacity characterization of
multiple-input multiple-output (MIMO) FSO systems over atmospheric
turbulence-induced fading channels. The fluctuations of the irradiance of
optical channels distorted by atmospheric conditions is usually described by a
gamma-gamma ($\Gamma \Gamma$) distribution, and the distribution of the sum of
$\Gamma \Gamma$ random variables (RVs) is required to model the MIMO optical
links. We use an $\alpha$-$\mu$ distribution to efficiently approximate the
probability density function (PDF) of the sum of independent and identical
distributed $\Gamma\Gamma$ RVs through moment-based estimators. Furthermore,
the PDF of the sum of independent, but not necessarily identically distributed
$\Gamma \Gamma$ RVs can be efficiently approximated by a finite weighted sum of
PDFs of $\Gamma \Gamma$ distributions. Based on these reliable approximations,
novel and precise analytical expressions for the ergodic capacity of MIMO FSO
systems are derived. Additionally, we deduce the asymptotic simple expressions
in high signal-to-noise ratio regimes, which provide useful insights into the
impact of the system parameters on the ergodic capacity. Finally, our proposed
results are validated via Monte-Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00956</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00956</id><created>2015-05-05</created><updated>2015-06-02</updated><authors><author><keyname>Burgos</keyname><forenames>Andres C.</forenames></author><author><keyname>Polani</keyname><forenames>Daniel</forenames></author></authors><title>Informational parasites in code evolution</title><categories>cs.MA</categories><comments>Accepted for the 13th European Conference on Artificial Life (ECAL
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous study, we considered an information-theoretic model of code
evolution. In it, agents obtain information about their (common) environment by
the perception of messages of other agents, which is determined by an
interaction probability (the structure of the population). For an agent to
understand another agent's messages, the former must either know the identity
of the latter, or the code producing the messages must be universally
interpretable. A universal code, however, introduces a vulnerability: a
parasitic entity can take advantage of it. Here, we investigate this problem.
In our specific setting, we consider a parasite to be an agent that tries to
inflict as much damage as possible in the mutual understanding of the
population (i.e. the parasite acts as a disinformation agent). We show that,
after introducing a parasite in the population, the former adopts a code such
that it captures the information about the environment that is missing in the
population. Such agent would be of great value, but only if the rest of the
population could understand its messages. However, it is of little use here,
since the parasite utilises the most common messages in the population to
express different concepts. Now we let the population respond by updating their
codes such that, in this arms race, they again maximise their mutual
understanding. As a result, there is a code drift in the population where the
utilisation of the messages of the parasite is avoided. A consequence of this
is that the information that the parasite possesses but the agents lack becomes
understandable and readily available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00965</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00965</id><created>2015-05-05</created><authors><author><keyname>Higham</keyname><forenames>Desmond J.</forenames></author></authors><title>An Introduction to Multilevel Monte Carlo for Option Valuation</title><categories>math.NA cs.CE physics.data-an q-fin.CP stat.CO</categories><comments>Submitted to International Journal of Computer Mathematics, special
  issue on Computational Methods in Finance</comments><msc-class>65C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo is a simple and flexible tool that is widely used in
computational finance. In this context, it is common for the quantity of
interest to be the expected value of a random variable defined via a stochastic
differential equation. In 2008, Giles proposed a remarkable improvement to the
approach of discretizing with a numerical method and applying standard Monte
Carlo. His multilevel Monte Carlo method offers an order of speed up given by
the inverse of epsilon, where epsilon is the required accuracy. So computations
can run 100 times more quickly when two digits of accuracy are required. The
multilevel philosophy has since been adopted by a range of researchers and a
wealth of practically significant results has arisen, most of which have yet to
make their way into the expository literature.
  In this work, we give a brief, accessible, introduction to multilevel Monte
Carlo and summarize recent results applicable to the task of option evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00985</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00985</id><created>2015-05-05</created><updated>2015-07-31</updated><authors><author><keyname>Paluch</keyname><forenames>Robert</forenames></author><author><keyname>Suchecki</keyname><forenames>Krzysztof</forenames></author><author><keyname>Holyst</keyname><forenames>Janusz</forenames></author></authors><title>Models of random graph hierarchies</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce two models of inclusion hierarchies: Random Graph Hierarchy
(RGH) and Limited Random Graph Hierarchy (LRGH). In both models a set of nodes
at a given hierarchy level is connected randomly, as in the Erd\H{o}s-R\'{e}nyi
random graph, with a fixed average degree equal to a system parameter $c$.
Clusters of the resulting network are treated as nodes at the next hierarchy
level and they are connected again at this level and so on, until the process
cannot continue. In the RGH model we use all clusters, including those of size
$1$, when building the next hierarchy level, while in the LRGH model clusters
of size $1$ stop participating in further steps. We find that in both models
the number of nodes at a given hierarchy level $h$ decreases approximately
exponentially with $h$. The height of the hierarchy $H$, i.e. the number of all
hierarchy levels, increases logarithmically with the system size $N$, i.e. with
the number of nodes at the first level. The height $H$ decreases monotonically
with the connectivity parameter $c$ in the RGH model and it reaches a maximum
for a certain $c_{max}$ in the LRGH model. The distribution of separate cluster
sizes in the LRGH model is a power law with an exponent about $-1.25$. The
above results follow from approximate analytical calculations and have been
confirmed by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00989</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00989</id><created>2015-05-05</created><authors><author><keyname>Dai</keyname><forenames>Kais</forenames></author><author><keyname>Nespereira</keyname><forenames>Celia G&#xf3;nzalez</forenames></author><author><keyname>Vilas</keyname><forenames>Ana Fern&#xe1;ndez</forenames></author><author><keyname>Redondo</keyname><forenames>Rebeca P. D&#xed;az</forenames></author></authors><title>Scraping and Clustering Techniques for the Characterization of Linkedin
  Profiles</title><categories>cs.SI cs.CY cs.IR</categories><comments>In proceedings of the Fourth International Conference on Information
  Technology Convergence and Services (ITCS 2015), pp. 1-15, January 2015,
  Zurich(Switzerland)</comments><doi>10.5121/csit.2015.50101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The socialization of the web has undertaken a new dimension after the
emergence of the Online Social Networks (OSN) concept. The fact that each
Internet user becomes a potential content creator entails managing a big amount
of data. This paper explores the most popular professional OSN: LinkedIn. A
scraping technique was implemented to get around 5 Million public profiles. The
application of natural language processing techniques (NLP) to classify the
educational background and to cluster the professional background of the
collected profiles led us to provide some insights about this OSN's users and
to evaluate the relationships between educational degrees and professional
careers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00990</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00990</id><created>2015-05-05</created><authors><author><keyname>Pfander</keyname><forenames>G&#xf6;tz E.</forenames></author><author><keyname>Zheltov</keyname><forenames>Pavel</forenames></author></authors><title>Identification of stochastic operators</title><categories>math.FA cs.IT math.IT math.ST stat.TH</categories><journal-ref>Applied and Computational Harmonic Analysis 36, 256-279, 2014</journal-ref><doi>10.1016/j.acha.2013.05.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the here developed functional analytic machinery we extend the
theory of operator sampling and identification to apply to operators with
stochastic spreading functions. We prove that identification with a delta train
signal is possible for a large class of stochastic operators that have the
property that the autocorrelation of the spreading function is supported on a
set of 4D volume less than one and this support set does not have a defective
structure. In fact, unlike in the case of deterministic operator
identification, the geometry of the support set has a significant impact on the
identifiability of the considered operator class. Also, we prove that,
analogous to the deterministic case, the restriction of the 4D volume of a
support set to be less or equal to one is necessary for identifiability of a
stochastic operator class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.00996</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.00996</id><created>2015-05-05</created><authors><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Fast Guided Filter</title><categories>cs.CV</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The guided filter is a technique for edge-aware image filtering. Because of
its nice visual quality, fast speed, and ease of implementation, the guided
filter has witnessed various applications in real products, such as image
editing apps in phones and stereo reconstruction, and has been included in
official MATLAB and OpenCV. In this note, we remind that the guided filter can
be simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. In
a variety of applications, this leads to a speedup of &gt;10x with almost no
visible degradation. We hope this acceleration will improve performance of
current applications and further popularize this filter. Code is released.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01005</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01005</id><created>2015-05-05</created><updated>2015-05-06</updated><authors><author><keyname>Ravi</keyname><forenames>Peruvemba Sundaram</forenames></author><author><keyname>Tuncel</keyname><forenames>Levent</forenames></author></authors><title>Approximation Ratio of LD Algorithm for Multi-Processor Scheduling and
  the Coffman-Sethi Conjecture</title><categories>cs.DS cs.DM math.CO math.OC</categories><comments>This paper, building on the intermediate results in arXiv:1312.3345
  (by the authors and Huang) proves that the Coffman-Sethi conjecture holds. As
  a result, this paper and arXIv:1312.3345 (cited in the current paper) have
  many definitions and mathematical statements in common, some of them in
  free-style text</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coffman and Sethi proposed a heuristic algorithm, called LD, for
multi-processor scheduling, to minimize makespan over flowtime-optimal
schedules. LD algorithm is a natural extension of a very well-known list
scheduling algorithm, Longest Processing Time (LPT) list scheduling, to our
bicriteria scheduling problem. Moreover, in 1976, Coffman and Sethi conjectured
that LD algorithm has precisely the following worst-case performance bound:
$\frac{5}{4} - \frac{3}{4(4m-1)}$, where m is the number of machines. In this
paper, utilizing some recent work by the authors and Huang, from 2013, which
exposed some very strong combinatorial properties of various presumed minimal
counterexamples to the conjecture, we provide a proof of this conjecture. The
problem and the LD algorithm have connections to other fundamental problems
(such as the assembly line-balancing problem) and to other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01016</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01016</id><created>2015-05-05</created><authors><author><keyname>Timo</keyname><forenames>Roy</forenames></author><author><keyname>Wigger</keyname><forenames>Michele</forenames></author></authors><title>Joint Cache-Channel Coding over Erasure Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>submitted as an invited paper to ISWCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cache-aided communications system in which a transmitter
communicates with many receivers over an erasure broadcast channel. The system
serves as a basic model for communicating on-demand content during periods of
high network congestion, where some content can be pre-placed in local caches
near the receivers. We formulate the cache-aided communications problem as a
joint cache-channel coding problem, and characterise some information-theoretic
tradeoffs between reliable communications rates and cache sizes. We show that
if the receivers experience different channel qualities, then using unequal
cache sizes and joint cache-channel coding improves system efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01034</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01034</id><created>2015-05-05</created><authors><author><keyname>Benavoli</keyname><forenames>Alessio</forenames></author><author><keyname>Piga</keyname><forenames>Dario</forenames></author></authors><title>A stochastic interpretation of set-membership filtering: application to
  polynomial systems through polytopic bounding</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Set-membership estimation is in general referred in literature as the
deterministic approach to state estimation, since its solution can be
formulated in the context of set-valued calculus and no stochastic calculations
are necessary. This turns out not to be entirely true. In this paper, we show
that set-membership estimation can be equivalently formulated in the stochastic
setting by employing sets of probability measures. Inferences in set-membership
estimation are thus carried out by computing expectations w.r.t. the updated
set of probability measures P as in the stochastic case. In particular, we show
that inferences can be computed by solving a particular semi-infinite linear
programming problem, which is a special case of the truncated moment problem in
which only the zero-th order moment is known (i.e., the support). By writing
the dual of the above semi-infinite linear programming problem, we show that,
if the nonlinearities in the measurement and process equations are polynomials
and if the bounding sets for initial state, process and measurement noises are
described by polynomial inequalities, then an approximation of this
semi-infinite linear programming problem can efficiently be obtained by using
the theory of sum-of-squares polynomial optimization. We then derive a smart
greedy procedure to compute a polytopic outer-approximation of the true
membership-set, by computing the minimum-volume polytope that outer-bounds the
set that includes all the means computed w.r.t. P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01056</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01056</id><created>2015-05-05</created><updated>2015-07-29</updated><authors><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Wang</keyname><forenames>Weixi</forenames></author><author><keyname>Wu</keyname><forenames>Chen</forenames></author><author><keyname>Hu</keyname><forenames>Jinxing</forenames></author></authors><title>Preprint Virtual Reality GIS and Cloud Service Based Traffic Analysis
  Platform</title><categories>cs.OH</categories><comments>This is the preprint version of our paper on The 23rd International
  Conference on Geoinformatics (Geoinformatics2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on The 23rd International
Conference on Geoinformatics (Geoinformatics2015). City traffic data has
several characteristics, such as large scale, diverse predictable and
real-time, which falls in the range of definition of Big Data. This paper
proposed a cloud service platform which targets for wise transportation is to
carry out unified management and mining analysis of the huge number of the
multivariate and heterogeneous dynamic transportation information, provides
real-time transportation information, increase the utilization efficiency of
transportation, promote transportation management and service level of travel
information and provide decision support of transportation management by
virtual reality as visual.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01065</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01065</id><created>2015-04-30</created><authors><author><keyname>Hegenbart</keyname><forenames>Sebastian</forenames></author><author><keyname>Kwitt</keyname><forenames>Roland</forenames></author><author><keyname>Uhl</keyname><forenames>Andreas</forenames></author></authors><title>Proceedings of The 39th Annual Workshop of the Austrian Association for
  Pattern Recognition (OAGM), 2015</title><categories>cs.CV</categories><comments>Index submitted before individual papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 39th annual workshop of the Austrian Association for Pattern Recognition
(OAGM/AAPR) provides a platform for presentation and discussion of research
progress as well as research projects within the OAGM/AAPR community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01071</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01071</id><created>2015-05-05</created><authors><author><keyname>Ameyed</keyname><forenames>Darine</forenames></author><author><keyname>Miraoui</keyname><forenames>Moeiz</forenames></author><author><keyname>Tadj</keyname><forenames>Chakib</forenames></author></authors><title>A Spatiotemporal Context Definition for Service Adaptation Prediction in
  a Pervasive Computing Environment</title><categories>cs.SE cs.AI cs.ET cs.HC</categories><comments>Context-aware; Pervasive Computing; Context Definition; 2015.
  International Journal of Advanced Studies in Computer Science and Engineering
  (IJASCSE) http://www.ijascse.org/publications ;2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pervasive systems refers to context-aware systems that can sense their
context, and adapt their behavior accordingly to provide adaptable services.
Proactive adaptation of such systems allows changing the service and the
context based on prediction. However, the definition of the context is still
vague and not suitable to prediction. In this paper we discuss and classify
previous definitions of context. Then, we present a new definition which allows
pervasive systems to understand and predict their contexts. We analyze the
essential lines that fall within the context definition, and propose some
scenarios to make it clear our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01072</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01072</id><created>2015-05-05</created><authors><author><keyname>Maiya</keyname><forenames>Arun S.</forenames></author><author><keyname>Visser</keyname><forenames>Dale</forenames></author><author><keyname>Wan</keyname><forenames>Andrew</forenames></author></authors><title>Mining Measured Information from Text</title><categories>cs.CL cs.IR</categories><comments>4 pages; 38th International ACM SIGIR Conference on Research and
  Development in Information Retrieval (SIGIR '15)</comments><acm-class>I.2.7; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to extract measured information from text (e.g., a
1370 degrees C melting point, a BMI greater than 29.9 kg/m^2 ). Such
extractions are critically important across a wide range of domains -
especially those involving search and exploration of scientific and technical
documents. We first propose a rule-based entity extractor to mine measured
quantities (i.e., a numeric value paired with a measurement unit), which
supports a vast and comprehensive set of both common and obscure measurement
units. Our method is highly robust and can correctly recover valid measured
quantities even when significant errors are introduced through the process of
converting document formats like PDF to plain text. Next, we describe an
approach to extracting the properties being measured (e.g., the property &quot;pixel
pitch&quot; in the phrase &quot;a pixel pitch as high as 352 {\mu}m&quot;). Finally, we
present MQSearch: the realization of a search engine with full support for
measured information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01074</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01074</id><created>2015-05-05</created><updated>2015-05-19</updated><authors><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author><author><keyname>Robinson-Garcia</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Jim&#xe9;nez-Contreras</keyname><forenames>Evaristo</forenames></author><author><keyname>de la Fuente</keyname><forenames>Enrique</forenames></author></authors><title>The BiPublishers ranking: Main results and methodological problems when
  constructing rankings of academic publishers</title><categories>cs.DL</categories><comments>Paper accepted for publication in Revista Espa\~nola de
  Documentaci\'on Cient\'ifica. v2 corrects some mispells and errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the results of the Bibliometric Indicators for Publishers project
(also known as BiPublishers). This project represents the first attempt to
systematically develop bibliometric publisher rankings. The data for this
project was derived from the Book Citation Index, and the study time period was
2009-2013. We have developed 42 rankings: 4 for by fields and 38 by
disciplines. We display six indicators by publisher divided into three types:
output, impact and publisher's profile. The aim is to capture different
characteristics of the research performance of publishers. 254 publishers were
processed and classified according to publisher type: commercial publishers and
university presses. We present the main publishers by fields. Then, we discuss
the main challenges presented when developing this type of tools. The
BiPublishers ranking is an on-going project which aims to develop and explore
new data sources and indicators to better capture and define the research
impact of publishers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01081</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01081</id><created>2015-05-05</created><authors><author><keyname>Classen</keyname><forenames>Jiska</forenames></author><author><keyname>Schulz</keyname><forenames>Matthias</forenames></author><author><keyname>Hollick</keyname><forenames>Matthias</forenames></author></authors><title>Practical Covert Channels for WiFi Systems</title><categories>cs.NI</categories><comments>Submitted to CNS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless covert channels promise to exfiltrate information with high
bandwidth by circumventing traditional access control mechanisms. Ideally, they
are only accessible by the intended recipient and---for regular system
users/operators---indistinguishable from normal operation. While a number of
theoretical and simulation studies exist in literature, the practical aspects
of WiFi covert channels are not well understood. Yet, it is particularly the
practical design and implementation aspect of wireless systems that provides
attackers with the latitude to establish covert channels: the ability to
operate under adverse conditions and to tolerate a high amount of signal
variations. Moreover, covert physical receivers do not have to be addressed
within wireless frames, but can simply eavesdrop on the transmission. In this
work, we analyze the possibilities to establish covert channels in WiFi systems
with emphasis on exploiting physical layer characteristics. We discuss design
alternatives for selected covert channel approaches and study their feasibility
in practice. By means of an extensive performance analysis, we compare the
covert channel bandwidth. We further evaluate the possibility of revealing the
introduced covert channels based on different detection capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01085</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01085</id><created>2015-05-05</created><authors><author><keyname>Fouhey</keyname><forenames>David F.</forenames></author><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author></authors><title>In Defense of the Direct Perception of Affordances</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of functional recognition or affordance estimation from images has
seen a revival in recent years. As originally proposed by Gibson, the
affordances of a scene were directly perceived from the ambient light: in other
words, functional properties like sittable were estimated directly from
incoming pixels. Recent work, however, has taken a mediated approach in which
affordances are derived by first estimating semantics or geometry and then
reasoning about the affordances. In a tribute to Gibson, this paper explores
his theory of affordances as originally proposed. We propose two approaches for
direct perception of affordances and show that they obtain good results and can
out-perform mediated approaches. We hope this paper can rekindle discussion
around direct perception and its implications in the long term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01098</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01098</id><created>2015-05-05</created><authors><author><keyname>Kataoka</keyname><forenames>Toshiki</forenames></author><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Towards concept analysis in categories: limit inferior as algebra, limit
  superior as coalgebra</title><categories>math.CT cs.LO</categories><comments>22 pages, 5 figures and 9 diagrams</comments><msc-class>18A30, 18A35, 18A40, 06A15, 06A75</msc-class><acm-class>I.2.6; I.2.4; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While computer programs and logical theories begin by declaring the concepts
of interest, be it as data types or as predicates, network computation does not
allow such global declarations, and requires *concept mining* and *concept
analysis* to extract shared semantics for different network nodes. Powerful
semantic analysis systems have been the drivers of nearly all paradigm shifts
on the web. In categorical terms, most of them can be described as
bicompletions of enriched matrices, generalizing the Dedekind-MacNeille-style
completions from posets to suitably enriched categories. Yet it has been well
known for more than 40 years that ordinary categories themselves in general do
not permit such completions. Armed with this new semantical view of
Dedekind-MacNeille completions, and of matrix bicompletions, we take another
look at this ancient mystery. It turns out that simple categorical versions of
the *limit superior* and *limit inferior* operations characterize a general
notion of Dedekind-MacNeille completion, that seems to be appropriate for
ordinary categories, and boils down to the more familiar enriched versions when
the limits inferior and superior coincide. This explains away the apparent gap
among the completions of ordinary categories, and broadens the path towards
categorical concept mining and analysis, opened in previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01110</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01110</id><created>2015-05-05</created><authors><author><keyname>Abroshan</keyname><forenames>Mahed</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author></authors><title>Zero Error Coordination</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a zero error coordination problem wherein the
nodes of a network exchange messages to be able to perfectly coordinate their
actions with the individual observations of each other. While previous works on
coordination commonly assume an asymptotically vanishing error, we assume
exact, zero error coordination. Furthermore, unlike previous works that employ
the empirical or strong notions of coordination, we define and use a notion of
set coordination. This notion of coordination bears similarities with the
empirical notion of coordination. We observe that set coordination, in its
special case of two nodes with a one-way communication link is equivalent with
the &quot;Hide and Seek&quot; source coding problem of McEliece and Posner. The Hide and
Seek problem has known intimate connections with graph entropy, rate distortion
theory, Renyi mutual information and even error exponents. Other special cases
of the set coordination problem relate to Witsenhausen's zero error rate and
the distributed computation problem. These connections motivate a better
understanding of set coordination, its connections with empirical coordination,
and its study in more general setups. This paper takes a first step in this
direction by proving new results for two node networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01111</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01111</id><created>2015-05-05</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author></authors><title>On Geo Location Services for Telecom Operators</title><categories>cs.NI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents location based service for telecom providers. Most of the
location-based services in the mobile networks are introduced and deployed by
Internet companies. It leaves for telecom just the role of the data channel.
Telecom providers should use their competitive advantages and offer own
solutions. In this paper, we discuss the sharing location information via
geo-messages. Geo messages let mobile users share location information as
signatures to the standard messages (e.g., email, SMS). Rather than let some
service constantly monitor (poll) the user location (as the most standalone
services do) or share location info within any social circle (social network
check-in, etc.) The Geo Messages approach lets users share location data on the
peer to peer basis. Users can share own location info with any existing
messaging systems. And messaging (e.g., SMS) is the traditional service for
telecom
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01116</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01116</id><created>2015-05-03</created><updated>2015-05-06</updated><authors><author><keyname>Kumar</keyname><forenames>Dinesh</forenames></author></authors><title>Approaching unstructured search from function bilateral symmetry
  detection - A quantum algorithm</title><categories>quant-ph cs.DS</categories><report-no>MTP32013IS-07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of symmetry is vital to problem solving. Most of the problems of
computer vision and computer graphics and machine intelligence in general, can
be reduced to symmetry detection problem. Unstructured search problem can also
be looked upon from symmetry detection point of view. Unstructured search can
be thought as searching a binary string satisfying some search condition in an
unsorted list of binary strings. In this paper unstructured search problem is
reduced to function bilateral symmetry detection problem with polynomial
overhead in terms of the size of the input.
  Keywords: Unstructured Search, Quantum algorithm, Function bilateral symmetry
detection, Decision Problem, Quantum Black Box, Solving NP complete problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01118</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01118</id><created>2015-05-05</created><authors><author><keyname>Wang</keyname><forenames>Zifu</forenames></author><author><keyname>Tang</keyname><forenames>Zuqi</forenames></author><author><keyname>Henneron</keyname><forenames>Thomas</forenames></author><author><keyname>Piriou</keyname><forenames>Francis</forenames></author><author><keyname>Mipo</keyname><forenames>Jean-Claude</forenames></author></authors><title>Energetic Galerkin Projection of Electromagnetic Fields between
  Different Meshes</title><categories>cs.CE</categories><comments>published at Compumag 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to project electromagnetic fields between different meshes with
respect to the conservation of energetic values, Galerkin projection
formulations based on the energetic norm are developed in this communication.
The proposed formulations are applied to an academic example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01120</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01120</id><created>2015-05-05</created><authors><author><keyname>Segal</keyname><forenames>Oren</forenames></author><author><keyname>Colangelo</keyname><forenames>Philip</forenames></author><author><keyname>Nasiri</keyname><forenames>Nasibeh</forenames></author><author><keyname>Qian</keyname><forenames>Zhuo</forenames></author><author><keyname>Margala</keyname><forenames>Martin</forenames></author></authors><title>SparkCL: A Unified Programming Framework for Accelerators on
  Heterogeneous Clusters</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce SparkCL, an open source unified programming framework based on
Java, OpenCL and the Apache Spark framework. The motivation behind this work is
to bring unconventional compute cores such as FPGAs/GPUs/APUs/DSPs and future
core types into mainstream programming use. The framework allows equal
treatment of different computing devices under the Spark framework and
introduces the ability to offload computations to acceleration devices. The new
framework is seamlessly integrated into the standard Spark framework via a
Java-OpenCL device programming layer which is based on Aparapi and a Spark
programming layer that includes new kernel function types and modified Spark
transformations and actions. The framework allows a single code base to target
any type of compute core that supports OpenCL and easy integration of new core
types into a Spark cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01121</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01121</id><created>2015-05-05</created><updated>2015-10-01</updated><authors><author><keyname>Malinowski</keyname><forenames>Mateusz</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>Ask Your Neurons: A Neural-based Approach to Answering Questions about
  Images</title><categories>cs.CV cs.AI cs.CL</categories><comments>ICCV'15 (Oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a question answering task on real-world images that is set up as a
Visual Turing Test. By combining latest advances in image representation and
natural language processing, we propose Neural-Image-QA, an end-to-end
formulation to this problem for which all parts are trained jointly. In
contrast to previous efforts, we are facing a multi-modal problem where the
language output (answer) is conditioned on visual and natural language input
(image and question). Our approach Neural-Image-QA doubles the performance of
the previous best approach on this problem. We provide additional insights into
the problem by analyzing how much information is contained only in the language
part for which we provide a new human baseline. To study human consensus, which
is related to the ambiguities inherent in this challenging task, we propose two
novel metrics and collect additional answers which extends the original DAQUAR
dataset to DAQUAR-Consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01128</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01128</id><created>2015-05-05</created><authors><author><keyname>Endrullis</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Hansen</keyname><forenames>Helle Hvid</forenames></author><author><keyname>Hendriks</keyname><forenames>Dimitri</forenames></author><author><keyname>Polonsky</keyname><forenames>Andrew</forenames></author><author><keyname>Silva</keyname><forenames>Alexandra</forenames></author></authors><title>A Coinductive Framework for Infinitary Rewriting and Equational
  Reasoning (Extended Version)</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.6224</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a coinductive framework for defining infinitary analogues of
equational reasoning and rewriting in a uniform way. We define the relation
=^infty, notion of infinitary equational reasoning, and -&gt;^infty, the standard
notion of infinitary rewriting as follows:
  =^infty := nu R. ( &lt;-_root + -&gt;_root + lift(R) )^*
  -&gt;^infty := mu R. nu S. ( -&gt;_root + lift(R) )^* ; lift(S)
  where
  lift(R) := { (f(s_1,...,s_n), f(t_1,...,t_n)) | s_1 R t_1,...,s_n R t_n } +
id ,
  and where mu is the least fixed point operator and nu is the greatest fixed
point operator.
  The setup captures rewrite sequences of arbitrary ordinal length, but it has
neither the need for ordinals nor for metric convergence. This makes the
framework especially suitable for formalizations in theorem provers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01130</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01130</id><created>2015-05-05</created><updated>2015-05-06</updated><authors><author><keyname>Bola&#xf1;os</keyname><forenames>Marc</forenames></author><author><keyname>Mestre</keyname><forenames>Ricard</forenames></author><author><keyname>Talavera</keyname><forenames>Estefan&#xed;a</forenames></author><author><keyname>Gir&#xf3;-i-Nieto</keyname><forenames>Xavier</forenames></author><author><keyname>Radeva</keyname><forenames>Petia</forenames></author></authors><title>Visual Summary of Egocentric Photostreams by Representative Keyframes</title><categories>cs.CV cs.IR</categories><comments>Paper accepted in the IEEE First International Workshop on Wearable
  and Ego-vision Systems for Augmented Experience (WEsAX). Turin, Italy. July
  3, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building a visual summary from an egocentric photostream captured by a
lifelogging wearable camera is of high interest for different applications
(e.g. memory reinforcement). In this paper, we propose a new summarization
method based on keyframes selection that uses visual features extracted by
means of a convolutional neural network. Our method applies an unsupervised
clustering for dividing the photostreams into events, and finally extracts the
most relevant keyframe for each event. We assess the results by applying a
blind-taste test on a group of 20 people who assessed the quality of the
summaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01131</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01131</id><created>2015-05-05</created><authors><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author><author><keyname>Kaynar</keyname><forenames>Dilsun</forenames></author><author><keyname>Sharma</keyname><forenames>Divya</forenames></author><author><keyname>Sinha</keyname><forenames>Arunesh</forenames></author></authors><title>Program Actions as Actual Causes: A Building Block for Accountability</title><categories>cs.CR</categories><comments>arXiv admin note: text overlap with arXiv:1501.00747</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protocols for tasks such as authentication, electronic voting, and secure
multiparty computation ensure desirable security properties if agents follow
their prescribed programs. However, if some agents deviate from their
prescribed programs and a security property is violated, it is important to
hold agents accountable by determining which deviations actually caused the
violation. Motivated by these applications, we initiate a formal study of
program actions as actual causes. Specifically, we define in an interacting
program model what it means for a set of program actions to be an actual cause
of a violation. We present a sound technique for establishing program actions
as actual causes. We demonstrate the value of this formalism in two ways.
First, we prove that violations of a specific class of safety properties always
have an actual cause. Thus, our definition applies to relevant security
properties. Second, we provide a cause analysis of a representative protocol
designed to address weaknesses in the current public key certification
infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01133</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01133</id><created>2015-05-05</created><authors><author><keyname>Khezeli</keyname><forenames>Kia</forenames></author><author><keyname>Chen</keyname><forenames>Jun</forenames></author></authors><title>Outer Bounds on the Admissible Source Region for Broadcast Channels with
  Correlated Sources</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two outer bounds on the admissible source region for broadcast channels with
correlated sources are presented: the first one is strictly tighter than the
existing outer bound by Gohari and Anantharam while the second one provides a
complete characterization of the admissible source region in the case where the
two sources are conditionally independent given the common part. These outer
bounds are deduced from the general necessary conditions established for the
lossy source broadcast problem via suitable comparisons between the virtual
broadcast channel (induced by the source and the reconstructions) and the
physical broadcast channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01137</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01137</id><created>2015-05-05</created><authors><author><keyname>Chen</keyname><forenames>Jun</forenames></author><author><keyname>He</keyname><forenames>Da-ke</forenames></author><author><keyname>Jagmohan</keyname><forenames>Ashish</forenames></author><author><keyname>Lastras-Monta&#xf1;o</keyname><forenames>Luis A.</forenames></author></authors><title>On the Reliability Function of Variable-Rate Slepian-Wolf Coding</title><categories>cs.IT math.IT</categories><comments>This is an old manuscript written in 2007-2008 based on our 2007
  Allerton conference paper with the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reliability function of variable-rate Slepian-Wolf coding is linked to
the reliability function of channel coding with constant composition codes,
through which computable lower and upper bounds are derived. The bounds
coincide at rates close to the Slepian-Wolf limit, yielding a complete
characterization of the reliability function in that rate regime. It is shown
that variable-rate Slepian-Wolf codes can significantly outperform fixed-rate
Slepian-Wolf codes in terms of rate-error tradeoff. The reliability function of
variable-rate Slepian-Wolf coding with rate below the Slepian-Wolf limit is
determined. In sharp contrast with fixed-rate Slepian-Wolf codes for which the
correct decoding probability decays to zero exponentially fast if the rate is
below the Slepian-Wolf limit, the correct decoding probability of variable-rate
Slepian-Wolf codes can be bounded away from zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01139</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01139</id><created>2015-05-04</created><authors><author><keyname>Mostafa</keyname><forenames>Hesham</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Lorenz K.</forenames></author><author><keyname>Indiveri</keyname><forenames>Giacomo</forenames></author></authors><title>An event-based architecture for solving constraint satisfaction problems</title><categories>cs.DC cs.AR</categories><comments>First two authors contributed equally to this work</comments><journal-ref>Nature Communications 6, Article number: 8941, 2015</journal-ref><doi>10.1038/ncomms9941</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint satisfaction problems (CSPs) are typically solved using
conventional von Neumann computing architectures. However, these architectures
do not reflect the distributed nature of many of these problems and are thus
ill-suited to solving them. In this paper we present a hybrid analog/digital
hardware architecture specifically designed to solve such problems. We cast
CSPs as networks of stereotyped multi-stable oscillatory elements that
communicate using digital pulses, or events. The oscillatory elements are
implemented using analog non-stochastic circuits. The non-repeating phase
relations among the oscillatory elements drive the exploration of the solution
space. We show that this hardware architecture can yield state-of-the-art
performance on a number of CSPs under reasonable assumptions on the
implementation. We present measurements from a prototype electronic chip to
demonstrate that a physical implementation of the proposed architecture is
robust to practical non-idealities and to validate the theory proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01140</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01140</id><created>2015-05-04</created><authors><author><keyname>Br&#xe6;ndeland</keyname><forenames>Asbj&#xf8;rn</forenames></author></authors><title>Depth-first search in split-by-edges trees</title><categories>cs.DS</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A layerwise search in a split-by-edges tree (as defined by Br{\ae}ndeland,
2015) of agiven graph produces a maximum independent set in exponential time. A
depth-first search produces an independent set, which may or may not be a
maximum, in linear time, but the worst case success rate is maybe not high
enough to make it really interesting. What may make depth-first searching in
split-by-edges trees interesting, though, is the pronounced oscillation of its
success rate along the graph size axis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01173</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01173</id><created>2015-05-05</created><authors><author><keyname>Pan</keyname><forenames>Hengyue</forenames></author><author><keyname>Wang</keyname><forenames>Bo</forenames></author><author><keyname>Jiang</keyname><forenames>Hui</forenames></author></authors><title>Deep Learning for Object Saliency Detection and Image Segmentation</title><categories>cs.CV</categories><comments>9 pages, 126 figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose several novel deep learning methods for object
saliency detection based on the powerful convolutional neural networks. In our
approach, we use a gradient descent method to iteratively modify an input image
based on the pixel-wise gradients to reduce a cost function measuring the
class-specific objectness of the image. The pixel-wise gradients can be
efficiently computed using the back-propagation algorithm. The discrepancy
between the modified image and the original one may be used as a saliency map
for the image. Moreover, we have further proposed several new training methods
to learn saliency-specific convolutional nets for object saliency detection, in
order to leverage the available pixel-wise segmentation information. Our
methods are extremely computationally efficient (processing 20-40 images per
second in one GPU). In this work, we use the computed saliency maps for image
segmentation. Experimental results on two benchmark tasks, namely Microsoft
COCO and Pascal VOC 2012, have shown that our proposed methods can generate
high-quality salience maps, clearly outperforming many existing methods. In
particular, our approaches excel in handling many difficult images, which
contain complex background, highly-variable salient objects, multiple objects,
and/or very small salient objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01180</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01180</id><created>2015-05-05</created><updated>2015-09-23</updated><authors><author><keyname>Ravanbakhsh</keyname><forenames>Hadi</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Sriram</forenames></author></authors><title>Counterexample Guided Synthesis of Switched Controllers for
  Reach-While-Stay Properties</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a counter-example guided inductive synthesis (CEGIS) framework
for synthesizing continuous-time switching controllers that guarantee reach
while stay (RWS) properties of the closed loop system. The solution is based on
synthesizing specially defined class of control Lyapunov functions (CLFs) for
switched systems, that yield switching controllers with a guaranteed minimum
dwell time in each mode. Next, we use a CEGIS-based approach to iteratively
solve the resulting quantified exists-forall constraints, and find a CLF. We
introduce relaxations to guarantee termination, as well as heuristics to
increase convergence speed. Finally, we evaluate our approach on a set of
benchmarks ranging from two to six state variables. Our evaluation includes a
preliminary comparison with related tools. The proposed approach shows the
promise of nonlinear SMT solvers for the synthesis of provably correct
switching control laws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01181</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01181</id><created>2015-05-05</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Deploying Dense Networks for Maximal Energy Efficiency: Small Cells Meet
  Massive MIMO</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Journal on Selected Areas in Communications (April
  2015), 30 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How would a cellular network designed for maximal energy efficiency look
like? To answer this fundamental question, we model future cellular networks
using stochastic geometry and obtain a new lower bound on the average uplink
spectral efficiency. This enables us to formulate a tractable energy efficiency
(EE) maximization problem and solve it analytically with respect to the density
of base stations (BSs), the transmit power levels, the number of BS antennas
and users per cell, and the pilot reuse factor. The closed-form expressions
obtained from this general EE maximization framework provide valuable insights
on the interplay between the optimization variables, hardware characteristics,
and propagation environment. Small cells are proved to give high EE, but the EE
improvement saturates quickly with the BS density. Interestingly, the maximal
EE is obtained by also equipping the BSs with multiple antennas and operate in
a &quot;massive MIMO&quot; fashion, where the array gain from coherent detection
mitigates interference and the multiplexing of many users reduces the energy
cost per user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01187</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01187</id><created>2015-05-05</created><authors><author><keyname>Wei</keyname><forenames>Changshuai</forenames></author><author><keyname>Lu</keyname><forenames>Qing</forenames></author></authors><title>GWGGI: software for genome-wide gene-gene interaction analysis</title><categories>q-bio.QM cs.DS q-bio.GN stat.AP</categories><journal-ref>BMC Genetics 2014, 15:101</journal-ref><doi>10.1186/s12863-014-0101-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: While the importance of gene-gene interactions in human diseases
has been well recognized, identifying them has been a great challenge,
especially through association studies with millions of genetic markers and
thousands of individuals. Computationally efficient and powerful tools are in
great need for the identification of new gene-gene interactions in
high-dimensional association studies. Result: We develop C++ software for
genome-wide gene-gene interaction analyses (GWGGI). GWGGI utilizes tree-based
algorithms to search a large number of genetic markers for a disease-associated
joint association with the consideration of high-order interactions, and then
uses non-parametric statistics to test the joint association. The package
includes two functions, likelihood ratio Mann-whitney (LRMW) and Tree
Assembling Mann-whitney (TAMW).We optimize the data storage and computational
efficiency of the software, making it feasible to run the genome-wide analysis
on a personal computer. The use of GWGGI was demonstrated by using two real
data-sets with nearly 500 k genetic markers. Conclusion: Through the empirical
study, we demonstrated that the genome-wide gene-gene interaction analysis
using GWGGI could be accomplished within a reasonable time on a personal
computer (i.e., ~3.5 hours for LRMW and ~10 hours for TAMW). We also showed
that LRMW was suitable to detect interaction among a small number of genetic
variants with moderate-to-strong marginal effect, while TAMW was useful to
detect interaction among a larger number of low-marginal-effect genetic
variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01189</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01189</id><created>2015-05-05</created><authors><author><keyname>Linial</keyname><forenames>Nati</forenames></author><author><keyname>Mosheiff</keyname><forenames>Jonathan</forenames></author></authors><title>On the Rigidity of Sparse Random Graphs</title><categories>math.CO cs.DM math.PR</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph with a trivial automorphism group is said to be rigid. Wright proved
that for $\frac{\log n}{n}+\omega(\frac 1n)\leq p\leq \frac 12$ a random graph
$G\in G(n,p)$ is rigid whp. It is not hard to see that this lower bound is
sharp and for $p&lt;\frac{(1-\epsilon)\log n}{n}$ with positive probability
$\text{aut}(G)$ is nontrivial. We show that in the sparser case $\omega(\frac 1
n)\leq p\leq \frac{\log n}{n}+\omega(\frac 1n)$, it holds whp that $G$'s
$2$-core is rigid. We conclude that for all $p$, a graph in $G(n,p)$ is
reconstrutible whp. In addition this yields for $\omega(\frac 1n)\leq p\leq
\frac 12$ a canonical labeling algorithm that almost surely runs in polynomial
time with $o(1)$ error rate. This extends the range for which such an algorithm
is currently known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01197</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01197</id><created>2015-05-05</created><updated>2015-09-26</updated><authors><author><keyname>Gkioxari</keyname><forenames>Georgia</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Contextual Action Recognition with R*CNN</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are multiple cues in an image which reveal what action a person is
performing. For example, a jogger has a pose that is characteristic for
jogging, but the scene (e.g. road, trail) and the presence of other joggers can
be an additional source of information. In this work, we exploit the simple
observation that actions are accompanied by contextual cues to build a strong
action recognition system. We adapt RCNN to use more than one region for
classification while still maintaining the ability to localize the action. We
call our system R*CNN. The action-specific models and the feature maps are
trained jointly, allowing for action specific representations to emerge. R*CNN
achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other
approaches in the field by a significant margin. Last, we show that R*CNN is
not limited to action recognition. In particular, R*CNN can also be used to
tackle fine-grained tasks such as attribute classification. We validate this
claim by reporting state-of-the-art performance on the Berkeley Attributes of
People dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01210</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01210</id><created>2015-05-05</created><authors><author><keyname>Bonnichsen</keyname><forenames>Lars F.</forenames></author><author><keyname>Probst</keyname><forenames>Christian W.</forenames></author><author><keyname>Karlsson</keyname><forenames>Sven</forenames></author></authors><title>Implementation of BT-trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document presents the full implementation details of BT-trees, a highly
efficient ordered map, and an evaluation which compares BT-trees with unordered
maps. BT- trees are often much faster than other ordered maps, and have
comparable performance to unordered map implementations. However, in benchmarks
which favor unordered maps, BT-trees are not faster than the fastest unordered
map implementations we know of.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01214</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01214</id><created>2015-05-05</created><authors><author><keyname>Saleh</keyname><forenames>Babak</forenames></author><author><keyname>Dontcheva</keyname><forenames>Mira</forenames></author><author><keyname>Hertzmann</keyname><forenames>Aaron</forenames></author><author><keyname>Liu</keyname><forenames>Zhicheng</forenames></author></authors><title>Learning Style Similarity for Searching Infographics</title><categories>cs.GR cs.CV cs.HC cs.IR cs.MM</categories><comments>6 pages, to appear in the 41st annual conference on Graphics
  Interface (GI) 2015,</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infographics are complex graphic designs integrating text, images, charts and
sketches. Despite the increasing popularity of infographics and the rapid
growth of online design portfolios, little research investigates how we can
take advantage of these design resources. In this paper we present a method for
measuring the style similarity between infographics. Based on human perception
data collected from crowdsourced experiments, we use computer vision and
machine learning algorithms to learn a style similarity metric for infographic
designs. We evaluate different visual features and learning algorithms and find
that a combination of color histograms and Histograms-of-Gradients (HoG)
features is most effective in characterizing the style of infographics. We
demonstrate our similarity metric on a preliminary image retrieval test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01221</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01221</id><created>2015-05-05</created><authors><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Lindauer</keyname><forenames>Marius</forenames></author><author><keyname>Balint</keyname><forenames>Adrian</forenames></author><author><keyname>Bayless</keyname><forenames>Sam</forenames></author><author><keyname>Hoos</keyname><forenames>Holger</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author></authors><title>The Configurable SAT Solver Challenge (CSSC)</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that different solution strategies work well for different
types of instances of hard combinatorial problems. As a consequence, most
solvers for the propositional satisfiability problem (SAT) expose parameters
that allow them to be customized to a particular family of instances. In the
international SAT competition series, these parameters are ignored: solvers are
run using a single default parameter setting (supplied by the authors) for all
benchmark instances in a given track. While this competition format rewards
solvers with robust default settings, it does not reflect the situation faced
by a practitioner who only cares about performance on one particular
application and can invest some time into tuning solver parameters for this
application. The new Configurable SAT Solver Competition (CSSC) compares
solvers in this latter setting, scoring each solver by the performance it
achieved after a fully automated configuration step. This article describes the
CSSC in more detail, and reports the results obtained in its two instantiations
so far, CSSC 2013 and 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01222</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01222</id><created>2015-05-05</created><authors><author><keyname>Fouladgar</keyname><forenames>A. M.</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>O.</forenames><affiliation>Shitz</affiliation></author><author><keyname>Sahin</keyname><forenames>O.</forenames><affiliation>Shitz</affiliation></author><author><keyname>Popovski</keyname><forenames>P.</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>S.</forenames><affiliation>Shitz</affiliation></author></authors><title>Joint Interference Alignment and Bi-Directional Scheduling for MIMO
  Two-Way Multi-Link Networks</title><categories>cs.IT math.IT</categories><comments>To be presented at ICC 2015, 6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By means of the emerging technique of dynamic Time Division Duplex (TDD), the
switching point between uplink and downlink transmissions can be optimized
across a multi-cell system in order to reduce the impact of inter-cell
interference. It has been recently recognized that optimizing also the order in
which uplink and downlink transmissions, or more generally the two directions
of a two-way link, are scheduled can lead to significant benefits in terms of
interference reduction. In this work, the optimization of bi-directional
scheduling is investigated in conjunction with the design of linear precoding
and equalization for a general multi-link MIMO two-way system. A simple
algorithm is proposed that performs the joint optimization of the ordering of
the transmissions in the two directions of the two-way links and of the linear
transceivers, with the aim of minimizing the interference leakage power.
Numerical results demonstrate the effectiveness of the proposed strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01225</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01225</id><created>2015-05-05</created><updated>2015-06-04</updated><authors><author><keyname>Chen</keyname><forenames>Ching-Hsien</forenames></author><author><keyname>Wu</keyname><forenames>Jwo-Yuh</forenames></author></authors><title>Amplitude-Aided 1-Bit Compressive Sensing Over Noisy Wireless Sensor
  Networks</title><categories>cs.IT math.IT</categories><comments>10 pages, 4 figures,accepted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstract-One-bit compressive sensing (CS) is known to be particularly suited
for resource-constrained wireless sensor networks (WSNs). In this paper, we
consider 1-bit CS over noisy WSNs subject to channel-induced bit flipping
errors, and propose an amplitude-aided signal reconstruction scheme, by which
(i) the representation points of local binary quantizers are designed to
minimize the loss of data fidelity caused by local sensing noise, quantization,
and bit sign flipping, and (ii) the fusion center adopts the conventional
minimization method for sparse signal recovery using the decoded and de-mapped
binary data. The representation points of binary quantizers are designed by
minimizing the mean square error (MSE) of the net data mismatch, taking into
account the distributions of the nonzero signal entries, local sensing noise,
quantization error, and bit flipping; a simple closed-form solution is then
obtained. Numerical simulations show that our method improves the estimation
accuracy when SNR is low or the number of sensors is small, as compared to
state-of-the-art 1-bit CS algorithms relying solely on the sign message for
signal recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01231</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01231</id><created>2015-05-05</created><authors><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Xiao</keyname><forenames>Chengshan</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Secure Massive MIMO Transmission in the Presence of an Active
  Eavesdropper</title><categories>cs.IT math.IT</categories><comments>To appear in ICC 15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate secure and reliable transmission strategies for
multi-cell multi-user massive multiple-input multiple-output (MIMO) systems in
the presence of an active eavesdropper. We consider a time-division duplex
system where uplink training is required and an active eavesdropper can attack
the training phase to cause pilot contamination at the transmitter. This forces
the precoder used in the subsequent downlink transmission phase to implicitly
beamform towards the eavesdropper, thus increasing its received signal power.
We derive an asymptotic achievable secrecy rate for matched filter precoding
and artificial noise (AN) generation at the transmitter when the number of
transmit antennas goes to infinity. For the achievability scheme at hand, we
obtain the optimal power allocation policy for the transmit signal and the AN
in closed form. For the case of correlated fading channels, we show that the
impact of the active eavesdropper can be completely removed if the transmit
correlation matrices of the users and the eavesdropper are orthogonal. Inspired
by this result, we propose a precoder null space design exploiting the low rank
property of the transmit correlation matrices of massive MIMO channels, which
can significantly degrade the eavesdropping capabilities of the active
eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01245</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01245</id><created>2015-05-05</created><authors><author><keyname>Hemati</keyname><forenames>Saied</forenames></author></authors><title>Mitigating Hardware Cyber-Security Risks in Error Correcting Decoders</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates hardware cyber-security risks associated with channel
decoders, which are commonly acquired as a black box in semiconductor industry.
It is shown that channel decoders are potentially attractive targets for
hardware cyber-security attacks and can be easily embedded with malicious
blocks. Several attack scenarios are considered in this work and suitable
methods for mitigating the risks are proposed. These methods are based on
randomizing the inputs of the channel decoder to obstruct the communications
between attackers and the malicious blocks, ideally without changing the
decoding performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01255</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01255</id><created>2015-05-06</created><updated>2015-10-30</updated><authors><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofan</forenames></author><author><keyname>Tang</keyname><forenames>Wallace K. S.</forenames></author></authors><title>Controllability of networked MIMO systems</title><categories>cs.SY</categories><comments>15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the state controllability of networked systems,
where the network topology is directed and weighted and the nodes are
higher-dimensional linear time-invariant (LTI) dynamical systems. We
investigate how the network topology, the node-system dynamics, the external
control inputs, and the inner interactions affect the controllability of a
networked system, and show that for a general networked
multi-input/multi-output (MIMO) system: 1) the controllability of the overall
network is an integrated result of the aforementioned relevant factors, which
cannot be decoupled into the controllability of individual node-systems and the
properties solely determined by the network topology, quite different from the
familiar notion of consensus or formation controllability; 2) if the network
topology is uncontrollable by external inputs, then the networked system with
identical nodes will be uncontrollable, even if it is structurally
controllable; 3) with a controllable network topology, controllability and
observability of the nodes together are necessary for the controllability of
the networked systems under some mild conditions, but nevertheless they are not
sufficient. For a networked system with single-input/single-output (SISO) LTI
nodes, we present precise necessary and sufficient conditions for the
controllability of a general network topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01257</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01257</id><created>2015-05-06</created><authors><author><keyname>Tommasi</keyname><forenames>Tatiana</forenames></author><author><keyname>Patricia</keyname><forenames>Novi</forenames></author><author><keyname>Caputo</keyname><forenames>Barbara</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>A Deeper Look at Dataset Bias</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presence of a bias in each image data collection has recently attracted a
lot of attention in the computer vision community showing the limits in
generalization of any learning method trained on a specific dataset. At the
same time, with the rapid development of deep learning architectures, the
activation values of Convolutional Neural Networks (CNN) are emerging as
reliable and robust image descriptors. In this paper we propose to verify the
potential of the DeCAF features when facing the dataset bias problem. We
conduct a series of analyses looking at how existing datasets differ among each
other and verifying the performance of existing debiasing methods under
different representations. We learn important lessons on which part of the
dataset bias problem can be considered solved and which open questions still
need to be tackled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01259</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01259</id><created>2015-05-06</created><authors><author><keyname>Conde</keyname><forenames>Rodolfo</forenames></author><author><keyname>Rajsbaum</keyname><forenames>Sergio</forenames></author></authors><title>The solvability of consensus in iterated models extended with
  safe-consensus</title><categories>cs.DC</categories><comments>49 pages, A preliminar version of the main results appeared in the
  SIROCCO 2014 proceedings</comments><msc-class>05Cxx, 68Q17, 68Q25, 68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The safe-consensus task was introduced by Afek, Gafni and Lieber (DISC'09) as
a weakening of the classic consensus. When there is concurrency, the consensus
output can be arbitrary, not even the input of any process. They showed that
safe-consensus is equivalent to consensus, in a wait-free system. We study the
solvability of consensus in three shared memory iterated models extended with
the power of safe-consensus black boxes. In the first model, for the $i$-th
iteration, processes write to the memory, invoke safe-consensus boxes and
finally they snapshot the memory. We show that in this model, any wait-free
implementation of consensus requires $\binom{n}{2}$ safe-consensus black-boxes
and this bound is tight. In a second iterated model, the processes write to
memory, then they snapshot it and finally they invoke safe-consensus boxes. We
prove that in this model, consensus cannot be implemented. In the last iterated
model, processes first invoke safe-consensus, then they write to memory and
finally they snapshot it. We show that this model is equivalent to the previous
model and thus consensus cannot be implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01265</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01265</id><created>2015-05-06</created><updated>2016-02-05</updated><authors><author><keyname>Ac&#xed;n</keyname><forenames>Antonio</forenames></author><author><keyname>Duan</keyname><forenames>Runyao</forenames></author><author><keyname>Roberson</keyname><forenames>David E.</forenames></author><author><keyname>Sainz</keyname><forenames>Ana Bel&#xe9;n</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>A new property of the Lov\'asz number and duality relations between
  graph parameters</title><categories>math.CO cs.IT math.IT quant-ph</categories><comments>16 pages, submitted to Discrete Applied Mathematics for a special
  issue in memory of Levon Khachatrian; v2 has a full proof of the duality
  between theta+ and theta- and a new author, some new references, and we
  corrected several small errors and typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for any graph $G$, by considering &quot;activation&quot; through the
strong product with another graph $H$, the relation $\alpha(G) \leq
\vartheta(G)$ between the independence number and the Lov\'{a}sz number of $G$
can be made arbitrarily tight: Precisely, the inequality \[
  \alpha(G \times H) \leq \vartheta(G \times H) = \vartheta(G)\,\vartheta(H) \]
becomes asymptotically an equality for a suitable sequence of ancillary graphs
$H$.
  This motivates us to look for other products of graph parameters of $G$ and
$H$ on the right hand side of the above relation. For instance, a result of
Rosenfeld and Hales states that \[
  \alpha(G \times H) \leq \alpha^*(G)\,\alpha(H), \] with the fractional
packing number $\alpha^*(G)$, and for every $G$ there exists $H$ that makes the
above an equality; conversely, for every graph $H$ there is a $G$ that attains
equality.
  These findings constitute some sort of duality of graph parameters, mediated
through the independence number, under which $\alpha$ and $\alpha^*$ are dual
to each other, and the Lov\'{a}sz number $\vartheta$ is self-dual. We also show
duality of Schrijver's and Szegedy's variants $\vartheta^-$ and $\vartheta^+$
of the Lov\'{a}sz number, and explore analogous notions for the chromatic
number under strong and disjunctive graph products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01286</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01286</id><created>2015-05-06</created><authors><author><keyname>Cohen</keyname><forenames>Dekel</forenames></author><author><keyname>Yehudai</keyname><forenames>Amiram</forenames></author></authors><title>Localization of real world regression Bugs using single execution</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regression bugs occur whenever software functionality that previously worked
as desired stops working, or no longer works as expected. Code changes, such as
bug fixes or new feature work, may result in a regression bug. Regression bugs
are an annoying and painful phenomena in the software development process,
requiring a great deal of effort to localize, effectively hindering team
progress. In this paper we present Regression Detective, a method which assists
the developer locating source code segments that caused a given regression bug.
Unlike some of the existing tools, our approach doesn't require an automated
test suite or executing past versions of the system. It is highly scalable to
millions of loc systems. The developer, who has no prior knowledge of the code
or the bug, reproduces the bug according to the steps described in the bug
database. We evaluated our approach with bugs from leading open source projects
(Eclipse, Tomcat, Ant). In over 90% of the cases, the developer only has to
examine 10-20 lines of code in order to locate the bug, regardless of the code
base size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01300</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01300</id><created>2015-05-06</created><authors><author><keyname>Gay</keyname><forenames>Dominique</forenames></author><author><keyname>Guigour&#xe8;s</keyname><forenames>Romain</forenames></author><author><keyname>Boull&#xe9;</keyname><forenames>Marc</forenames></author><author><keyname>Cl&#xe9;rot</keyname><forenames>Fabrice</forenames></author></authors><title>Cats &amp; Co: Categorical Time Series Coclustering</title><categories>cs.DB stat.ML</categories><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We suggest a novel method of clustering and exploratory analysis of temporal
event sequences data (also known as categorical time series) based on
three-dimensional data grid models. A data set of temporal event sequences can
be represented as a data set of three-dimensional points, each point is defined
by three variables: a sequence identifier, a time value and an event value.
Instantiating data grid models to the 3D-points turns the problem into
3D-coclustering.
  The sequences are partitioned into clusters, the time variable is discretized
into intervals and the events are partitioned into clusters. The cross-product
of the univariate partitions forms a multivariate partition of the
representation space, i.e., a grid of cells and it also represents a
nonparametric estimator of the joint distribution of the sequences, time and
events dimensions. Thus, the sequences are grouped together because they have
similar joint distribution of time and events, i.e., similar distribution of
events along the time dimension. The best data grid is computed using a
parameter-free Bayesian model selection approach. We also suggest several
criteria for exploiting the resulting grid through agglomerative hierarchies,
for interpreting the clusters of sequences and characterizing their components
through insightful visualizations. Extensive experiments on both synthetic and
real-world data sets demonstrate that data grid models are efficient, effective
and discover meaningful underlying patterns of categorical time series data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01303</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01303</id><created>2015-05-06</created><authors><author><keyname>Cohen</keyname><forenames>Joseph Paul</forenames></author><author><keyname>Ding</keyname><forenames>Wei</forenames></author><author><keyname>Bagherjeiran</keyname><forenames>Abraham</forenames></author></authors><title>Semi-Supervised Web Wrapper Repair via Recursive Tree Matching</title><categories>cs.IR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous data extraction pipelines using wrappers have become common and
integral parts of businesses dealing with stock, flight, or product
information. Extracting data from websites that use HTML templates is difficult
because available wrapper methods are not designed to deal with websites that
change over time (the inclusion or removal of HTML elements). We are the first
to perform large scale empirical analyses of the causes of shift and propose
the concept of domain entropy to quantify it. We draw from this analysis to
propose a new semi-supervised search approach called XTPath. XTPath combines
the existing XPath with carefully designed annotation extraction and informed
search strategies. XTPath is the first method to store contextual node
information from the training DOM and utilize it in a supervised manner. We
utilize this data with our proposed recursive tree matching method which
locates nodes most similar in context. The search is based on a heuristic
function that takes into account the similarity of a tree compared to the
structure that was present in the training data. We systematically evaluate
XTPath using 117,422 pages from 75 diverse websites in 8 vertical markets that
covers vastly different topics. Our XTPath method consistently outperforms
XPath and a current commercial system in terms of successful extractions in a
blackbox test. We are the first supervised wrapper extraction method to make
our code and datasets available (online here:
http://kdl.cs.umb.edu/w/datasets/).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01306</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01306</id><created>2015-05-06</created><authors><author><keyname>Guisado-G&#xe1;mez</keyname><forenames>Joan</forenames></author><author><keyname>Prat-P&#xe9;rez</keyname><forenames>Arnau</forenames></author></authors><title>Understanding Graph Structure of Wikipedia for Query Expansion</title><categories>cs.IR</categories><msc-class>68P20</msc-class><acm-class>H.3.3; E.1; G.2.2</acm-class><doi>10.1145/2764947.2764953</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge bases are very good sources for knowledge extraction, the ability
to create knowledge from structured and unstructured sources and use it to
improve automatic processes as query expansion. However, extracting knowledge
from unstructured sources is still an open challenge. In this respect,
understanding the structure of knowledge bases can provide significant benefits
for the effectiveness of such purpose. In particular, Wikipedia has become a
very popular knowledge base in the last years because it is a general
encyclopedia that has a large amount of information and thus, covers a large
amount of different topics. In this piece of work, we analyze how articles and
categories of Wikipedia relate to each other and how these relationships can
support a query expansion technique. In particular, we show that the structures
in the form of dense cycles with a minimum amount of categories tend to
identify the most relevant information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01311</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01311</id><created>2015-05-06</created><authors><author><keyname>Monacchi</keyname><forenames>Andrea</forenames></author><author><keyname>Versolatto</keyname><forenames>Fabio</forenames></author><author><keyname>Herold</keyname><forenames>Manuel</forenames></author><author><keyname>Egarter</keyname><forenames>Dominik</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>An Open Solution to Provide Personalized Feedback for Building Energy
  Management</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of renewable energy sources increases the complexity in
mantaining the power grid. In particular, the highly dynamic nature of
generation and consumption demands for a better utilization of energy
resources, which seen the cost of storage infrastructure, can only be achieved
through demand-response. Accordingly, the availability of energy and potential
overload situations can be reflected using a price signal. The effectiveness of
this mechanism arises from the flexibility of device operation, which is
nevertheless heavily reliant on the exchange of information between the grid
and its consumers. In this paper, we investigate the capability of an
interactive energy management system to timely inform users on energy usage, in
order to promote an optimal use of local resources. In particular, we analyze
data being collected in several households in Italy and Austria to gain
insights into usage behavior and drive the design of more effective systems.
The outcome is the formulation of energy efficiency policies for residential
buildings, as well as the design of an energy management system, consisting of
hardware measurement units and a management software. The Mj\&quot;olnir framework,
which we release for open use, provides a platform where various feedback
concepts can be implemented and assessed. This includes widgets displaying
disaggregated and aggregated consumption information, as well as daily
production and tailored advices. The formulated policies were implemented as an
advisor widget able to autonomously analyze usage and provide tailored energy
feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01319</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01319</id><created>2015-05-06</created><updated>2015-11-10</updated><authors><author><keyname>Arth</keyname><forenames>Clemens</forenames></author><author><keyname>Grasset</keyname><forenames>Raphael</forenames></author><author><keyname>Gruber</keyname><forenames>Lukas</forenames></author><author><keyname>Langlotz</keyname><forenames>Tobias</forenames></author><author><keyname>Mulloni</keyname><forenames>Alessandro</forenames></author><author><keyname>Wagner</keyname><forenames>Daniel</forenames></author></authors><title>The History of Mobile Augmented Reality</title><categories>cs.HC</categories><comments>43 pages, 18 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This document summarizes the major milestones in mobile Augmented Reality
between 1968 and 2014. Major parts of the list were compiled by the member of
the Christian Doppler Laboratory for Handheld Augmented Reality in 2010 (author
list in alphabetical order) for the ISMAR society. Later in 2013 it was
updated, and more recent work was added during preparation of this report.
Permission is granted to copy and modify.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01325</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01325</id><created>2015-05-06</created><updated>2015-05-07</updated><authors><author><keyname>Koczkodaj</keyname><forenames>Waldemar W.</forenames></author><author><keyname>Szybowski</keyname><forenames>Jacek</forenames></author></authors><title>On the Convergence of the Pairwise Comparisons Inconsistency Reduction
  Process</title><categories>cs.DM</categories><comments>16 page, 1 figure. For pairwise comparisons, the normalized vector o
  geometric means (GM) is equal to the normalized principal eigenvector (EV)
  for consistent matrices. For inconsistent matrices, the limit is &quot;make it
  consistent&quot; process is GM (specifically, not EV). This contribution finally
  concludes the discussion &quot;GM or EV&quot; originated in 1980s</comments><report-no>2015-05-05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates a powerful model, targeted to subjective assessments,
based on pairwise comparisons. It provides a proof that a distance-based
inconsistency reduction transforms an inconsistent pairwise comparisons (PC)
matrix into a consistent PC matrix which is generated by the geometric means of
rows of a given inconsistent PC matrix. The distance-based inconsistency
indicator was defined in 1993 for pairwise comparisons. Its convergence was
analyzed in 1996 (regretfully, with an incomplete proof; finally completed in
2010). However, there was no clear interpretation of the convergence limit
which is of considerable importance for applications and this study does so.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01326</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01326</id><created>2015-05-06</created><authors><author><keyname>Matsuoka</keyname><forenames>Satoshi</forenames></author></authors><title>Strong Typed Boehm Theorem and Functional Completeness on the Linear
  Lambda Calculus</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove a version of the typed B\&quot;{o}hm theorem on the linear
lambda calculus, which says, for any given types $A$ and $B$, when $s_1$ and
$s_2$ (respectively, $u_1$ and $u_2$) are different closed terms of $A$ (resp.
$B$), there is a term $t$ such that \[ t \, s_1 =_{\beta \eta {\rm c}} u_1
\quad \mbox{and} \quad t \, s_2 =_{\beta \eta {\rm c}} u_2 \, . \] Several
years ago, a weaker version of this theorem was proved, but the stronger
version was open. As a corollary of this theorem, we prove that if $A$ has two
different closed terms $s_1$ and $s_2$, then $A$ is functionally complete with
regard to $s_1$ and $s_2$. So far, it was only known that a few types are
functionally complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01335</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01335</id><created>2015-05-06</created><updated>2015-07-07</updated><authors><author><keyname>Di Fabio</keyname><forenames>Barbara</forenames></author><author><keyname>Ferri</keyname><forenames>Massimo</forenames></author></authors><title>Comparing persistence diagrams through complex vectors</title><categories>math.AT cs.CV</categories><comments>11 pages, 4 figures, 2 tables</comments><report-no>amsacta4233</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The natural pseudo-distance of spaces endowed with filtering functions is
precious for shape classification and retrieval; its optimal estimate coming
from persistence diagrams is the bottleneck distance, which unfortunately
suffers from combinatorial explosion. A possible algebraic representation of
persistence diagrams is offered by complex polynomials; since far polynomials
represent far persistence diagrams, a fast comparison of the coefficient
vectors can reduce the size of the database to be classified by the bottleneck
distance. This article explores experimentally three transformations from
diagrams to polynomials and three distances between the complex vectors of
coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01337</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01337</id><created>2015-05-06</created><authors><author><keyname>Nagele</keyname><forenames>Julian</forenames></author><author><keyname>Thiemann</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>Certification of Confluence Proofs using CeTA</title><categories>cs.LO</categories><comments>5 pages, International Workshop on Confluence 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CeTA was originally developed as a tool for certifying termination proofs
which have to be provided as certificates in the CPF-format. Its soundness is
proven as part of IsaFoR, the Isabelle Formalization of Rewriting. By now, CeTA
can also be used for certifying confluence and non-confluence proofs. In this
system description, we give a short overview on what kind of proofs are
supported, and what information has to be given in the certificates. As we will
see, only a small amount of information is required and so we hope that CSI
will not stay the only confluence tool which can produce certificates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01338</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01338</id><created>2015-05-06</created><authors><author><keyname>Sternagel</keyname><forenames>Thomas</forenames></author></authors><title>KBCV 2.0 - Automatic Completion Experiments</title><categories>cs.LO</categories><comments>IWC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the automatic mode of the new version of the
Knuth-Bendix Completion Visualizer. The internally used data structures have
been overhauled and the performance was dramatically improved by introducing
caching, parallelization, and term- indexing in the computation of critical
pairs and simplification. The new version is much faster and can complete three
more systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01340</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01340</id><created>2015-05-06</created><authors><author><keyname>Calude</keyname><forenames>Cristian S.</forenames></author><author><keyname>Desfontaines</keyname><forenames>Damien</forenames></author></authors><title>Universality and Almost Decidability</title><categories>cs.CC</categories><report-no>CDMTCS-462</report-no><journal-ref>Fundamenta Informaticae XXI (2014) 1001-1008</journal-ref><doi>10.3233/FI-2012-0000</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and study new definitions of universal and programmable universal
unary functions and consider a new simplicity criterion: almost decidability of
the halting set. A set of positive integers S is almost decidable if there
exists a decidable and generic (i.e. a set of natural density one) set whose
intersection with S is decidable. Every decidable set is almost decidable, but
the converse implication is false. We prove the existence of infinitely many
universal functions whose halting sets are generic (negligible, i.e. have
density zero) and (not) almost decidable. One result - namely, the existence of
infinitely many universal functions whose halting sets are generic (negligible)
and not almost decidable - solves an open problem in [9]. We conclude with some
open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01341</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01341</id><created>2015-05-06</created><authors><author><keyname>Born</keyname><forenames>Stefan</forenames></author><author><keyname>B&#xfc;cking</keyname><forenames>Ulrike</forenames></author><author><keyname>Springborn</keyname><forenames>Boris</forenames></author></authors><title>Quasiconformal distortion of projective transformations and discrete
  conformal maps</title><categories>math.CV cs.CG math.DG math.MG</categories><comments>11 pages, 9 figures</comments><msc-class>30C62, 52C26</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the quasiconformal distortion of projective transformations of
the real projective plane. For non-affine transformations, the contour lines of
quasiconformal distortion form a hyperbolic pencil of circles, and these are
the only circles that are mapped to circles. We apply this result to analyze
the quasiconformal distortion of the circumcircle preserving piecewise
projective interpolation between discretely conformally equivalent
triangulations. We show that another interpolation scheme, angle bisector
preserving piecewise projective interpolation, is in a sense optimal with
respect to quasiconformal distortion. These two interpolation schemes belong to
a one-parameter family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01342</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01342</id><created>2015-05-06</created><authors><author><keyname>Zahedi</keyname><forenames>Zohreh</forenames></author></authors><title>Analyzing readerships of International Iranian publications in Mendeley:
  an altmetrics study</title><categories>cs.DL</categories><comments>in Persian</comments><doi>10.6084/m9.figshare.1400482</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, the presence and distribution of both Mendeley readerships and
Web of Science citations for the publications published in the 43 Iranian
international journals indexed in Journal Citation Reports have been
investigated. The aim was to determine the impact, visibility and use of the
publications published by the Iranian international journals in Mendeley
compared to their citation impact; furthermore, to explore if there is any
relation between these two impact indicators (Mendeley readership counts and
WoS citation counts) for these publications. The DOIs of the 1,884 publications
used to extract the readerships data from Mendeley REST API in February 2014
and citations data until end of 2013 calculated using CWTS in-house WoS
database. SPSS (version 21) used to analyze the relationship between the
readerships and citations for those publications. The Mendeley usage
distribution both at the publication level (across publications years, fields
and document types) and at the user level (across users disciplines, academic
status and countries) have been investigated. These information will help to
understand the visibility and usage vs citation pattern and impact of Iranian
scientific outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01345</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01345</id><created>2015-05-06</created><updated>2015-09-28</updated><authors><author><keyname>Chakravarti</keyname><forenames>Mohnish</forenames></author><author><keyname>Kothari</keyname><forenames>Tanay</forenames></author></authors><title>A Comprehensive Study On The Applications Of Machine Learning For
  Diagnosis Of Cancer</title><categories>cs.LG</categories><comments>18 pages, 11 figures, 3 tables, 1 equation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collectively, lung cancer, breast cancer and melanoma was diagnosed in over
535,340 people out of which, 209,400 deaths were reported [13]. It is estimated
that over 600,000 people will be diagnosed with these forms of cancer in 2015.
Most of the deaths from lung cancer, breast cancer and melanoma result due to
late detection. All of these cancers, if detected early, are 100% curable. In
this study, we develop and evaluate algorithms to diagnose Breast cancer,
Melanoma, and Lung cancer. In the first part of the study, we employed a
normalised Gradient Descent and an Artificial Neural Network to diagnose breast
cancer with an overall accuracy of 91% and 95% respectively. In the second part
of the study, an artificial neural network coupled with image processing and
analysis algorithms was employed to achieve an overall accuracy of 93% A naive
mobile based application that allowed people to take diagnostic tests on their
phones was developed. Finally, a Support Vector Machine algorithm incorporating
image processing and image analysis algorithms was developed to diagnose lung
cancer with an accuracy of 94%. All of the aforementioned systems had very low
false positive and false negative rates. We are developing an online network
that incorporates all of these systems and allows people to collaborate
globally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01350</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01350</id><created>2015-05-06</created><authors><author><keyname>Yilmaz</keyname><forenames>Ozgur</forenames></author></authors><title>Classification of Occluded Objects using Fast Recurrent Processing</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1409.8576 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks are powerful tools for handling incomplete data
problems in computer vision, thanks to their significant generative
capabilities. However, the computational demand for these algorithms is too
high to work in real time, without specialized hardware or software solutions.
In this paper, we propose a framework for augmenting recurrent processing
capabilities into a feedforward network without sacrificing much from
computational efficiency. We assume a mixture model and generate samples of the
last hidden layer according to the class decisions of the output layer, modify
the hidden layer activity using the samples, and propagate to lower layers. For
visual occlusion problem, the iterative procedure emulates feedforward-feedback
loop, filling-in the missing hidden layer activity with meaningful
representations. The proposed algorithm is tested on a widely used dataset, and
shown to achieve 2$\times$ improvement in classification accuracy for occluded
objects. When compared to Restricted Boltzmann Machines, our algorithm shows
superior performance for occluded object classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01354</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01354</id><created>2015-05-06</created><authors><author><keyname>Masouros</keyname><forenames>C.</forenames></author><author><keyname>Zheng</keyname><forenames>G.</forenames></author></authors><title>Exploiting Known Interference as Green Signal Power for Downlink
  Beamforming Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a data-aided transmit beamforming scheme for the multi-user
multiple-input-single-output (MISO) downlink channel. While conventional
beamforming schemes aim at the minimization of the transmit power subject to
suppressing interference to guarantee quality of service (QoS) constraints,
here we use the knowledge of both data and channel state information (CSI) at
the transmitter to exploit, rather than suppress, constructive interference.
More specifically, we design a new precoding scheme for the MISO downlink that
minimizes the transmit power for generic phase shift keying (PSK) modulated
signals. The proposed precoder reduces the transmit power compared to
conventional schemes, by adapting the QoS constraints to accommodate
constructive interference as a source of useful signal power. By exploiting the
power of constructively interfering symbols, the proposed scheme achieves the
required QoS at lower transmit power. We extend this concept to the signal to
interference plus noise ratio (SINR) balancing problem, where higher SINR
values compared to the conventional SINR balancing optimization are achieved
for given transmit power budgets. In addition, we derive equivalent virtual
multicast formulations for both optimizations, both of which provide insights
of the optimal solution and facilitate the design of a more efficient solver.
Finally, we propose a robust beamforming technique to deal with imperfect CSI,
that also reduces the transmit power over conventional techniques, while
guaranteeing the required QoS. Our simulation and analysis show significant
power savings for small scale MISO downlink channels with the proposed
data-aided optimization compared to conventional beamforming optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01358</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01358</id><created>2015-05-06</created><authors><author><keyname>Mik&#x161;a</keyname><forenames>Mladen</forenames></author><author><keyname>Nordstr&#xf6;m</keyname><forenames>Jakob</forenames></author></authors><title>A Generalized Method for Proving Polynomial Calculus Degree Lower Bounds</title><categories>cs.CC cs.DM cs.LO math.CO</categories><comments>Full-length version of paper to appear in Proceedings of the 30th
  Annual Computational Complexity Conference (CCC '15), June 2015</comments><acm-class>F.2.2; F.1.3; I.2.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of obtaining lower bounds for polynomial calculus (PC)
and polynomial calculus resolution (PCR) on proof degree, and hence by
[Impagliazzo et al. '99] also on proof size. [Alekhnovich and Razborov '03]
established that if the clause-variable incidence graph of a CNF formula F is a
good enough expander, then proving that F is unsatisfiable requires high PC/PCR
degree. We further develop the techniques in [AR03] to show that if one can
&quot;cluster&quot; clauses and variables in a way that &quot;respects the structure&quot; of the
formula in a certain sense, then it is sufficient that the incidence graph of
this clustered version is an expander. As a corollary of this, we prove that
the functional pigeonhole principle (FPHP) formulas require high PC/PCR degree
when restricted to constant-degree expander graphs. This answers an open
question in [Razborov '02], and also implies that the standard CNF encoding of
the FPHP formulas require exponential proof size in polynomial calculus
resolution. Thus, while Onto-FPHP formulas are easy for polynomial calculus, as
shown in [Riis '93], both FPHP and Onto-PHP formulas are hard even when
restricted to bounded-degree expanders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01367</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01367</id><created>2015-05-06</created><authors><author><keyname>Strok</keyname><forenames>Fedor</forenames></author></authors><title>Applying FCA toolbox to Software Testing</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software testing uses wide range of different tools to enhance the
complicated process of defining quality of the system under test. Formal
Concept Analysis (FCA) provides us with algorithms of deriving formal ontology
from a set of objects and their attributes. With the use of FCA we can
considerably improve the efficiency of test case derivation. Moreover, an
FCA-based machine learning system supports the analysis of regression testing
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01371</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01371</id><created>2015-05-06</created><authors><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Xu</keyname><forenames>Lin</forenames></author></authors><title>Re-scale boosting for regression and classification</title><categories>cs.LG stat.ML</categories><comments>13 pages; 1 figure</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boosting is a learning scheme that combines weak prediction rules to produce
a strong composite estimator, with the underlying intuition that one can obtain
accurate prediction rules by combining &quot;rough&quot; ones. Although boosting is
proved to be consistent and overfitting-resistant, its numerical convergence
rate is relatively slow. The aim of this paper is to develop a new boosting
strategy, called the re-scale boosting (RBoosting), to accelerate the numerical
convergence rate and, consequently, improve the learning performance of
boosting. Our studies show that RBoosting possesses the almost optimal
numerical convergence rate in the sense that, up to a logarithmic factor, it
can reach the minimax nonlinear approximation rate. We then use RBoosting to
tackle both the classification and regression problems, and deduce a tight
generalization error estimate. The theoretical and experimental results show
that RBoosting outperforms boosting in terms of generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01374</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01374</id><created>2015-05-06</created><updated>2015-05-08</updated><authors><author><keyname>Shah</keyname><forenames>Shahid M.</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Achieving Shannon Capacity in a Fading Wiretap Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>20 Pages, 2 figures, submitted to IEEE Journal. arXiv admin note:
  substantial text overlap with arXiv:1404.5701</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable communication imposes an upper limit on the achievable rate, namely
the Shannon capacity. Wyner's wiretap coding, which ensures a security
constraint also in addition to reliability, results in decrease of the
achievable rate. To mitigate the loss in the secrecy rate, we propose a coding
scheme paper where we use sufficiently old messages as key and for this scheme
prove that multiple messages are secure with respect to (w.r.t.) all the
information possessed by the eavesdropper. We also show that we can achieve
security in the strong sense. Next we consider a fading wiretap channel with
full channel state information of the eavesdropper's channel and use our
coding/decoding scheme to achieve secrecy capacity close to the Shannon
capacity of the main channel (in the ergodic sense). Finally we also consider
the case where the transmitter does not have the instantaneous information of
the channel state of the eavesdropper, but only its distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01378</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01378</id><created>2015-05-06</created><authors><author><keyname>Battiston</keyname><forenames>Federico</forenames></author><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author></authors><title>Biased random walks on multiplex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biased random walks on complex networks are a particular type of walks whose
motion is biased on properties of the destination node, such as its degree. In
recent years they have been exploited to design efficient strategies to explore
a network, for instance by constructing maximally mixing trajectories or by
sampling homogeneously the nodes. In multiplex networks, the nodes are related
through different types of links (layers or communication channels), and the
presence of connections at different layers multiplies the number of possible
paths in the graph. In this work we introduce biased random walks on multiplex
networks and provide analytical solutions for their long-term properties such
as the stationary distribution and the entropy rate. We focus on degree-biased
walks and distinguish between two subclasses of random walks: extensive biased
walks consider the properties of each node separately at each layer, intensive
biased walks deal instead with intrinsically multiplex variables. We study the
effect of different structural properties, including the number of layers, the
presence and sign of inter-layer degree correlations, and the redundancy of
edges across layers, on the steady-state behaviour of the walkers, and we
investigate how to design an efficient exploration of the system. Finally, we
apply our results to the case of a multidimensional social network and to a
multimodal transportation system, showing how an appropriate tuning of the bias
parameters towards nodes which are truly multiplex allows to obtain a good
trade-off between a maximal entropy rate and a homogeneous sampling of the
nodes of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01390</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01390</id><created>2015-05-06</created><authors><author><keyname>Guang</keyname><forenames>Xuan</forenames></author><author><keyname>Lu</keyname><forenames>Jiyong</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>On the Optimality of Secure Network Coding</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE Communications Letters. One
  column,10 pages</comments><doi>10.1109/LCOMM.2015.2430862</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In network communications, information transmission often encounters
wiretapping attacks. Secure network coding is introduced to prevent information
from being leaked to adversaries. The investigation of performance bounds on
the numbers of source symbols and random symbols are two fundamental research
problems. For an important case that each wiretap-set with cardinality not
larger than $r$, Cai and Yeung proposed a coding scheme, which is optimal in
the senses of maximizing the number of source symbols and at the same time
minimizing the number of random symbols. In this letter, we further study
achievable lower bound on the number of random key and show that it just
depends on the security constraint, and particularly, is independent to the
information amount for transmission. This implies that when the number of
transmitted source message changes, we can't reduce the number of random key to
keep the same security level. We further give an intuitive interpretation on
our result. In addition, a similar construction of secure linear network codes
is proposed, which achieves this lower bound on the number of random key no
matter how much information is transmitted. At last, we also extend our result
to imperfect security case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01393</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01393</id><created>2015-05-06</created><authors><author><keyname>Atanassova</keyname><forenames>Iana</forenames></author><author><keyname>Bertin</keyname><forenames>Marc</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Mining Scientific Papers for Bibliometrics: a (very) Brief Survey of
  Methods and Tools</title><categories>cs.DL cs.CL</categories><comments>2 pages, paper accepted for the 15th International Society of
  Scientometrics and Informetrics Conference (ISSI)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Open Access movement in scientific publishing and search engines like
Google Scholar have made scientific articles more broadly accessible. During
the last decade, the availability of scientific papers in full text has become
more and more widespread thanks to the growing number of publications on online
platforms such as ArXiv and CiteSeer. The efforts to provide articles in
machine-readable formats and the rise of Open Access publishing have resulted
in a number of standardized formats for scientific papers (such as NLM-JATS,
TEI, DocBook). Our aim is to stimulate research at the intersection of
Bibliometrics and Computational Linguistics in order to study the ways
Bibliometrics can benefit from large-scale text analytics and sense mining of
scientific papers, thus exploring the interdisciplinarity of Bibliometrics and
Natural Language Processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01395</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01395</id><created>2015-04-27</created><authors><author><keyname>Farr&#xe1;n</keyname><forenames>J. I.</forenames></author><author><keyname>Garc&#xed;a-S&#xe1;nchez</keyname><forenames>P. A.</forenames></author></authors><title>The second Feng-Rao number for codes coming from inductive semigroups</title><categories>cs.IT math.AC math.CO math.IT math.NT</categories><msc-class>11T71, 20M14, 11Y55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The second Feng-Rao number of every inductive numerical semigroup is
explicitly computed. This number determines the asymptotical behaviour of the
order bound for the second Hamming weight of one-point AG codes. In particular,
this result is applied for the codes defined by asymptotically good towers of
function fields whose Weierstrass semigroups are inductive. In addition, some
properties of inductive numerical semigroups are studied, the involved
Ap\'{e}ry sets are computed in a recursive way, and some tests to check whether
a given numerical semigroups is inductive or not are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01410</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01410</id><created>2015-05-06</created><authors><author><keyname>Kindermann</keyname><forenames>Philipp</forenames></author><author><keyname>Schulz</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Spoerhase</keyname><forenames>Joachim</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>On Monotone Drawings of Trees</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A crossing-free straight-line drawing of a graph is monotone if there is a
monotone path between any pair of vertices with respect to some direction. We
show how to construct a monotone drawing of a tree with $n$ vertices on an
$O(n^{1.5}) \times O(n^{1.5})$ grid whose angles are close to the best possible
angular resolution. Our drawings are convex, that is, if every edge to a leaf
is substituted by a ray, the (unbounded) faces form convex regions. It is known
that convex drawings are monotone and, in the case of trees, also
crossing-free.
  A monotone drawing is strongly monotone if, for every pair of vertices, the
direction that witnesses the monotonicity comes from the vector that connects
the two vertices. We show that every tree admits a strongly monotone drawing.
For biconnected outerplanar graphs, this is easy to see. On the other hand, we
present a simply-connected graph that does not have a strongly monotone drawing
in any embedding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01419</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01419</id><created>2015-05-06</created><updated>2015-05-07</updated><authors><author><keyname>Liu</keyname><forenames>Ziqi</forenames></author><author><keyname>Wang</keyname><forenames>Yu-Xiang</forenames></author><author><keyname>Smola</keyname><forenames>Alexander J.</forenames></author></authors><title>Fast Differentially Private Matrix Factorization</title><categories>cs.LG cs.AI</categories><acm-class>G.2; I.2.6; G.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differentially private collaborative filtering is a challenging task, both in
terms of accuracy and speed. We present a simple algorithm that is provably
differentially private, while offering good performance, using a novel
connection of differential privacy to Bayesian posterior sampling via
Stochastic Gradient Langevin Dynamics. Due to its simplicity the algorithm
lends itself to efficient implementation. By careful systems design and by
exploiting the power law behavior of the data to maximize CPU cache bandwidth
we are able to generate 1024 dimensional models at a rate of 8.5 million
recommendations per second on a single PC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01421</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01421</id><created>2015-05-06</created><authors><author><keyname>Kalantari</keyname><forenames>Ashkan</forenames></author><author><keyname>Maleki</keyname><forenames>Sina</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Secrecy Energy Efficiency Optimization for MISO and SISO Communication
  Networks</title><categories>cs.IT math.IT</categories><comments>The 16th IEEE International Workshop on Signal Processing Advances in
  Wireless Communications (SPAWC), Stockholm, Sweden, June 28-July 1, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy-efficiency, high data rates and secure communications are essential
requirements of the future wireless networks. In this paper, optimizing the
secrecy energy efficiency is considered. The optimal beamformer is designed for
a MISO system with and without considering the minimum required secrecy rate.
Further, the optimal power control in a SISO system is carried out using an
efficient iterative method, and this is followed by analyzing the trade-off
between the secrecy energy efficiency and the secrecy rate for both MISO and
SISO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01422</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01422</id><created>2015-05-06</created><authors><author><keyname>Kalantari</keyname><forenames>Ashkan</forenames></author><author><keyname>Mohammadi</keyname><forenames>Mohammadali</forenames></author><author><keyname>Ardebilipour</keyname><forenames>Mehrdad</forenames></author></authors><title>Performance Analysis of Opportunistic Relaying Over Imperfect
  Non-identical Log-normal Fading Channels</title><categories>cs.IT math.IT</categories><comments>IEEE 22nd International Symposium on Personal, Indoor and Mobile
  Radio Communications (PIMRC), Toronto, ON, Canada, September 11-14, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the fact that full diversity order is achieved using the
&quot;best-relay&quot; selection technique, we consider opportunistic amplify-and-forward
and decode-and-forward relaying systems. We focus on the outage probability of
such a systems and then derive closed-form expressions for the outage
probability of these systems over independent but non-identical imperfect
Log-normal fading channels. We consider the error of channel estimation as a
Gaussian random variable. As a result the estimated channels distribution are
not Log-normal either as would be in the case of the Rayleigh fading channels.
This is exactly the reason why our simulation results do not exactly matched
with analytical results. However, this difference is negligible for a wide
variety of situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01429</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01429</id><created>2015-05-06</created><updated>2016-01-05</updated><authors><author><keyname>Ferreira</keyname><forenames>Julio Cesar</forenames></author><author><keyname>Vural</keyname><forenames>Elif</forenames></author><author><keyname>Guillemot</keyname><forenames>Christine</forenames></author></authors><title>Geometry-Aware Neighborhood Search for Learning Local Models for Image
  Reconstruction</title><categories>cs.CV cs.IT math.IT math.OC</categories><comments>15 pages, 10 figures and 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local learning of sparse image models has proven to be very effective to
solve inverse problems in many computer vision applications. To learn such
models, the data samples are often clustered using the K-means algorithm with
the Euclidean distance as a dissimilarity metric. However, the Euclidean
distance may not always be a good dissimilarity measure for comparing data
samples lying on a manifold. In this paper, we propose two algorithms for
determining a local subset of training samples from which a good local model
can be computed for reconstructing a given input test sample, where we take
into account the underlying geometry of the data. The first algorithm, called
Adaptive Geometry-driven Nearest Neighbor search (AGNN), is an adaptive scheme
which can be seen as an out-of-sample extension of the replicator graph
clustering method for local model learning. The second method, called
Geometry-driven Overlapping Clusters (GOC), is a less complex nonadaptive
alternative for training subset selection. The proposed AGNN and GOC methods
are evaluated in image super-resolution, deblurring and denoising applications
and shown to outperform spectral clustering, soft clustering, and geodesic
distance based subset selection in most settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01431</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01431</id><created>2015-05-06</created><updated>2015-05-10</updated><authors><author><keyname>Cohl</keyname><forenames>Howard S.</forenames></author><author><keyname>Schubotz</keyname><forenames>Moritz</forenames></author><author><keyname>McClain</keyname><forenames>Marjorie A.</forenames></author><author><keyname>Saunders</keyname><forenames>Bonita V.</forenames></author><author><keyname>Zou</keyname><forenames>Cherry Y.</forenames></author><author><keyname>Mohammed</keyname><forenames>Azeem S.</forenames></author><author><keyname>Danoff</keyname><forenames>Alex A.</forenames></author></authors><title>Growing the Digital Repository of Mathematical Formulae with Generic
  LaTeX Sources</title><categories>cs.DL cs.IR</categories><comments>I included an extra unrelated png file in the zip directory and it
  was falsely mentioned on a page 9. Previously I tried unsuccessfully to fix
  this. I removed the png file and now it is only 8 pages how it should be</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  One initial goal for the DRMF is to seed our digital compendium with
fundamental orthogonal polynomial formulae. We had used the data from the NIST
Digital Library of Mathematical Functions (DLMF) as initial seed for our DRMF
project. The DLMF input LaTeX source already contains some semantic information
encoded using a highly customized set of semantic LaTeX macros. Those macros
could be converted to content MathML using LaTeXML. During that conversion the
semantics were translated to an implicit DLMF content dictionary. This year, we
have developed a semantic enrichment process whose goal is to infer semantic
information from generic LaTeX sources. The generated context-free semantic
information is used to build DRMF formula home pages for each individual
formula. We demonstrate this process using selected chapters from the book
&quot;Hypergeometric Orthogonal Polynomials and their $q$-Analogues&quot; (2010) by
Koekoek, Lesky and Swarttouw (KLS) as well as an actively maintained addendum
to this book by Koornwinder (KLSadd). The generic input KLS and KLSadd LaTeX
sources describe the printed representation of the formulae, but does not
contain explicit semantic information. See http://drmf.wmflabs.org.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01439</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01439</id><created>2015-05-06</created><authors><author><keyname>Adamczyk</keyname><forenames>Marek</forenames></author><author><keyname>Grandoni</keyname><forenames>Fabrizio</forenames></author><author><keyname>Mukherjee</keyname><forenames>Joydeep</forenames></author></authors><title>Improved Approximation Algorithms for Stochastic Matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the Stochastic Matching problem, which is motivated
by applications in kidney exchange and online dating. We are given an
undirected graph in which every edge is assigned a probability of existence and
a positive profit, and each node is assigned a positive integer called timeout.
We know whether an edge exists or not only after probing it. On this random
graph we are executing a process, which one-by-one probes the edges and
gradually constructs a matching. The process is constrained in two ways: once
an edge is taken it cannot be removed from the matching, and the timeout of
node $v$ upper-bounds the number of edges incident to $v$ that can be probed.
The goal is to maximize the expected profit of the constructed matching.
  For this problem Bansal et al. (Algorithmica 2012) provided a
$3$-approximation algorithm for bipartite graphs, and a $4$-approximation for
general graphs. In this work we improve the approximation factors to $2.845$
and $3.709$, respectively.
  We also consider an online version of the bipartite case, where one side of
the partition arrives node by node, and each time a node $b$ arrives we have to
decide which edges incident to $b$ we want to probe, and in which order. Here
we present a $4.07$-approximation, improving on the $7.92$-approximation of
Bansal et al.
  The main technical ingredient in our result is a novel way of probing edges
according to a random but non-uniform permutation. Patching this method with an
algorithm that works best for large probability edges (plus some additional
ideas) leads to our improved approximation factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01446</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01446</id><created>2015-05-06</created><authors><author><keyname>Delling</keyname><forenames>Daniel</forenames></author><author><keyname>Dibbelt</keyname><forenames>Julian</forenames></author><author><keyname>Pajor</keyname><forenames>Thomas</forenames></author><author><keyname>Werneck</keyname><forenames>Renato F.</forenames></author></authors><title>Public Transit Labeling</title><categories>cs.DS</categories><comments>An extended abstract of this paper has been accepted at the 14th
  International Symposium on Experimental Algorithms (SEA'15)</comments><acm-class>G.2.2; G.2.3; H.2.8; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the journey planning problem in public transit networks. Developing
efficient preprocessing-based speedup techniques for this problem has been
challenging: current approaches either require massive preprocessing effort or
provide limited speedups. Leveraging recent advances in Hub Labeling, the
fastest algorithm for road networks, we revisit the well-known time-expanded
model for public transit. Exploiting domain-specific properties, we provide
simple and efficient algorithms for the earliest arrival, profile, and
multicriteria problems, with queries that are orders of magnitude faster than
the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01448</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01448</id><created>2015-05-06</created><authors><author><keyname>Leibovici</keyname><forenames>Thomas</forenames></author></authors><title>Taking back control of HPC file systems with Robinhood Policy Engine</title><categories>cs.DC cs.OS</categories><comments>International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</comments><proxy>Michael Brim</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, the largest Lustre file systems store billions of entries. On such
systems, classic tools based on namespace scanning become unusable. Operations
such as managing file lifetime, scheduling data copies, and generating overall
filesystem statistics become painful as they require collecting, sorting and
aggregating information for billions of records. Robinhood Policy Engine is an
open source software developed to address these challenges. It makes it
possible to schedule automatic actions on huge numbers of filesystem entries.
It also gives a synthetic understanding of file systems contents by providing
overall statistics about data ownership, age and size profiles. Even if it can
be used with any POSIX filesystem, Robinhood supports Lustre specific features
like OSTs, pools, HSM, ChangeLogs, and DNE. It implements specific support for
these features, and takes advantage of them to manage Lustre file systems
efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01457</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01457</id><created>2015-05-06</created><authors><author><keyname>Parandehgheibi</keyname><forenames>Marzieh</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Modeling the Impact of Communication Loss on the Power Grid under
  Emergency Control</title><categories>math.OC cs.SY</categories><comments>6 pages, 8 figures, Submitted to SmartGridComm 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the interaction between the power grid and the communication network
used for its control. We design a centralized emergency control scheme under
both full and partial communication support, to improve the performance of the
power grid. We use our emergency control scheme to model the impact of
communication loss on the grid. We show that unlike previous models used in the
literature, the loss of communication does not necessarily lead to the failure
of the correspondent power nodes; i.e. the &quot;point-wise&quot; failure model is not
appropriate. In addition, we show that the impact of communication loss is a
function of several parameters such as the size and structure of the power and
communication failure, as well as the operating mode of power nodes
disconnected from the communication network. Our model can be used to design
the dependency between the power grid and the communication network used for
its control, so as to maximize the benefit in terms of intelligent control,
while minimizing the risks due to loss of communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01459</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01459</id><created>2015-05-06</created><authors><author><keyname>Giard</keyname><forenames>Pascal</forenames></author><author><keyname>Sarkis</keyname><forenames>Gabi</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Unrolled Polar Decoders, Part I: Hardware Architectures</title><categories>cs.AR</categories><comments>10 pages, 8 figures, submitted to IEEE Journal on Selected Areas in
  Communications - Special Issue on Recent Advances In Capacity Approaching
  Codes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the first in a two-part series of papers on unrolled polar decoders.
In this paper (Part I), we present a family of architectures for hardware polar
decoders using a reduced-complexity successive-cancellation decoding algorithm.
The resulting fully-unrolled architectures are capable of achieving a coded
throughput in excess of 400 Gbps on an FPGA, two orders of magnitude greater
than current state-of-the-art polar decoders. Moreover, the proposed
architectures are flexible in a way that makes it possible to explore the
trade-off between resource usage and throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01460</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01460</id><created>2015-05-06</created><authors><author><keyname>Konrad</keyname><forenames>Christian</forenames></author></authors><title>Maximum Matching in Turnstile Streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the unweighted bipartite maximum matching problem in the one-pass
turnstile streaming model where the input stream consists of edge insertions
and deletions. In the insertion-only model, a one-pass $2$-approximation
streaming algorithm can be easily obtained with space $O(n \log n)$, where $n$
denotes the number of vertices of the input graph. We show that no such result
is possible if edge deletions are allowed, even if space $O(n^{3/2-\delta})$ is
granted, for every $\delta &gt; 0$. Specifically, for every $0 \le \epsilon \le
1$, we show that in the one-pass turnstile streaming model, in order to compute
a $O(n^{\epsilon})$-approximation, space $\Omega(n^{3/2 - 4\epsilon})$ is
required for constant error randomized algorithms, and, up to logarithmic
factors, space $O( n^{2-2\epsilon} )$ is sufficient. Our lower bound result is
proved in the simultaneous message model of communication and may be of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01462</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01462</id><created>2015-05-06</created><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Sivaraman</forenames></author><author><keyname>Bradley</keyname><forenames>Joseph</forenames></author><author><keyname>Parekh</keyname><forenames>Abhay</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology
  Dependence</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>39 pages, 5 figures. Significant extension of arXiv:1406.6618</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data in the form of pairwise comparisons arises in many domains, including
preference elicitation, sporting competitions, and peer grading among others.
We consider parametric ordinal models for such pairwise comparison data
involving a latent vector $w^* \in \mathbb{R}^d$ that represents the
&quot;qualities&quot; of the $d$ items being compared; this class of models includes the
two most widely used parametric models--the Bradley-Terry-Luce (BTL) and the
Thurstone models. Working within a standard minimax framework, we provide tight
upper and lower bounds on the optimal error in estimating the quality score
vector $w^*$ under this class of models. The bounds depend on the topology of
the comparison graph induced by the subset of pairs being compared via its
Laplacian spectrum. Thus, in settings where the subset of pairs may be chosen,
our results provide principled guidelines for making this choice. Finally, we
compare these error rates to those under cardinal measurement models and show
that the error rates in the ordinal and cardinal settings have identical
scalings apart from constant pre-factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01466</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01466</id><created>2015-05-06</created><updated>2015-11-10</updated><authors><author><keyname>Sarkis</keyname><forenames>Gabi</forenames></author><author><keyname>Giard</keyname><forenames>Pascal</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Fast List Decoders for Polar Codes</title><categories>cs.IT math.IT</categories><comments>to appear in the IEEE Journal on Selected Areas in Communications -
  Special Issue on Recent Advances In Capacity Approaching Codes, 2016</comments><doi>10.1109/JSAC.2015.2504299</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes asymptotically achieve the symmetric capacity of memoryless
channels, yet their error-correcting performance under successive-cancellation
(SC) decoding for short and moderate length codes is worse than that of other
modern codes such as low-density parity-check (LDPC) codes. Of the many methods
to improve the error-correction performance of polar codes, list decoding
yields the best results, especially when the polar code is concatenated with a
cyclic redundancy check (CRC). List decoding involves exploring several
decoding paths with SC decoding, and therefore tends to be slower than SC
decoding itself, by an order of magnitude in practical implementations. In this
paper, we present a new algorithm based on unrolling the decoding tree of the
code that improves the speed of list decoding by an order of magnitude when
implemented in software. Furthermore, we show that for software-defined radio
applications, our proposed algorithm is faster than the fastest software
implementations of LDPC decoders in the literature while offering comparable
error-correction performance at similar or shorter code lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01467</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01467</id><created>2015-05-06</created><authors><author><keyname>Assadi</keyname><forenames>Sepehr</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Yaroslavtsev</keyname><forenames>Grigory</forenames></author></authors><title>Tight Bounds for Linear Sketches of Approximate Matchings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We resolve the space complexity of linear sketches for approximating the
maximum matching problem in dynamic graph streams where the stream may include
both edge insertion and deletion. Specifically, we show that for any $\epsilon
&gt; 0$, there exists a one-pass streaming algorithm, which only maintains a
linear sketch of size $\tilde{O}(n^{2-3\epsilon})$ bits and recovers an
$n^\epsilon$-approximate maximum matching in dynamic graph streams, where $n$
is the number of vertices in the graph. In contrast to the extensively studied
insertion-only model, to the best of our knowledge, no non-trivial single-pass
streaming algorithms were previously known for approximating the maximum
matching problem on general dynamic graph streams.
  Furthermore, we show that our upper bound is essentially tight. Namely, any
linear sketch for approximating the maximum matching to within a factor of
$O(n^\epsilon)$ has to be of size $n^{2-3\epsilon -o(1)}$ bits. We establish
this lower bound by analyzing the corresponding simultaneous number-in-hand
communication model, with a combinatorial construction based on
Ruzsa-Szemer\'{e}di graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01474</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01474</id><created>2015-05-06</created><authors><author><keyname>Ffrancon</keyname><forenames>Robyn</forenames></author></authors><title>Retaining Experience and Growing Solutions</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generally, when genetic programming (GP) is used for function synthesis any
valuable experience gained by the system is lost from one problem to the next,
even when the problems are closely related. With the aim of developing a system
which retains beneficial experience from problem to problem, this paper
introduces the novel Node-by-Node Growth Solver (NNGS) algorithm which features
a component, called the controller, which can be adapted and improved for use
across a set of related problems. NNGS grows a single solution tree from root
to leaves. Using semantic backpropagation and acting locally on each node in
turn, the algorithm employs the controller to assign subsequent child nodes
until a fully formed solution is generated.
  The aim of this paper is to pave a path towards the use of a neural network
as the controller component and also, separately, towards the use of meta-GP as
a mechanism for improving the controller component. A proof-of-concept
controller is discussed which demonstrates the success and potential of the
NNGS algorithm. In this case, the controller constitutes a set of hand written
rules which can be used to deterministically and greedily solve standard
Boolean function synthesis benchmarks. Even before employing machine learning
to improve the controller, the algorithm vastly outperforms other well known
recent algorithms on run times, maintains comparable solution sizes, and has a
100% success rate on all Boolean function synthesis benchmarks tested so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01504</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01504</id><created>2015-05-06</created><updated>2015-06-16</updated><authors><author><keyname>Zhang</keyname><forenames>Shiliang</forenames></author><author><keyname>Jiang</keyname><forenames>Hui</forenames></author><author><keyname>Xu</keyname><forenames>Mingbin</forenames></author><author><keyname>Hou</keyname><forenames>Junfeng</forenames></author><author><keyname>Dai</keyname><forenames>Lirong</forenames></author></authors><title>A Fixed-Size Encoding Method for Variable-Length Sequences with its
  Application to Neural Network Language Models</title><categories>cs.NE cs.CL cs.LG</categories><comments>7 pages, 4 figures, Technical report (A shorter version will appear
  in ACL 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the new fixed-size ordinally-forgetting encoding
(FOFE) method, which can almost uniquely encode any variable-length sequence of
words into a fixed-size representation. FOFE can model the word order in a
sequence using a simple ordinally-forgetting mechanism according to the
positions of words. In this work, we have applied FOFE to feedforward neural
network language models (FNN-LMs). Experimental results have shown that without
using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform
not only the standard fixed-input FNN-LMs but also the popular RNN-LMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01511</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01511</id><created>2015-05-06</created><authors><author><keyname>Burnap</keyname><forenames>Pete</forenames></author><author><keyname>Gibson</keyname><forenames>Rachel</forenames></author><author><keyname>Sloan</keyname><forenames>Luke</forenames></author><author><keyname>Southern</keyname><forenames>Rosalynd</forenames></author><author><keyname>Williams</keyname><forenames>Matthew</forenames></author></authors><title>140 Characters to Victory?: Using Twitter to Predict the UK 2015 General
  Election</title><categories>cs.CY cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The election forecasting 'industry' is a growing one, both in the volume of
scholars producing forecasts and methodological diversity. In recent years a
new approach has emerged that relies on social media and particularly Twitter
data to predict election outcomes. While some studies have shown the method to
hold a surprising degree of accuracy there has been criticism over the lack of
consistency and clarity in the methods used, along with inevitable problems of
population bias. In this paper we set out a 'baseline' model for using Twitter
as an election forecasting tool that we then apply to the UK 2015 General
Election. The paper builds on existing literature by extending the use of
Twitter as a forecasting tool to the UK context and identifying its
limitations, particularly with regard to its application in a multi-party
environment with geographic concentration of power for minor parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01515</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01515</id><created>2015-05-06</created><authors><author><keyname>Bowman</keyname><forenames>Timothy D.</forenames></author></authors><title>Differences in Personal and Professional Tweets of Scholars</title><categories>cs.DL</categories><journal-ref>Aslib Journal of Information Management 67(3) (2015)</journal-ref><doi>10.1108/AJIM-12-2014-0180</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: This study shows that there were differences in the use of Twitter
by professors at universities in the Association of American Universities
(AAU). Affordance use differed between the personal and professional tweets of
professors. Framing behaviors were described that could impact the
interpretation of tweets by audience members. Design/methodology/approach: A
three phase research design was used that included surveys of professors,
categorization of tweets by Amazon's Mechanical Turk workers (i.e., turkers),
and categorization of tweets by active professors on Twitter. Findings: There
were significant differences found between professors that reported having a
Twitter account, significant differences found between types of Twitter
accounts (personal, professional, or both), and significant differences in the
affordances used in personal and professional tweets. Framing behaviors were
described that may assist altmetric researchers in distinguishing between
personal and professional tweets. Research limitations/implications (if
applicable): The study is limited by the sample population, survey instrument,
low survey response rate, and low Cohen's kappa. Practical implications (if
applicable): An overview of various affordances found in Twitter is provided
and a novel use of Amazon's Mechanical Turk for the categorization of tweets is
described that can be applied to future altmetric studies. Originality/value:
This work utilizes a socio-technical framework integrating social and
psychological theories to interpret results from the tweeting behavior of
professors and the interpretation of tweets by workers in Amazon's Mechanical
Turk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01519</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01519</id><created>2015-05-06</created><authors><author><keyname>Churbanov</keyname><forenames>Alexander G.</forenames></author><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>Numerical investigation of a space-fractional model of turbulent fluid
  flow in rectangular ducts</title><categories>cs.NA math.NA</categories><comments>19 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The models that are based of fractional derivatives should be highlighted
among promising new models to describe turbulent fluid flows. In the present
work, a steady-state flow in a duct is considered under the condition that the
turbulent diffusion is governed by a fractional power of the Laplace operator.
To study numerically flows in rectangular channels, finite-difference
approximations are employed. For approximate solving the corresponding boundary
value problem, the iterative method of conjugate gradients is used. At each
iteration, the problem with a fractional power of the grid Laplace operator is
solved. Predictions of turbulent flows in ducts at different Reynolds numbers
are presented via mean velocity fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01523</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01523</id><created>2015-05-06</created><updated>2016-02-15</updated><authors><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Fast and Powerful Hashing using Tabulation</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomized algorithms are often enjoyed for their simplicity, but the hash
functions employed to yield the desired probabilistic guarantees are often too
complicated to be practical. Here we survey recent results on how simple
hashing schemes based on tabulation provide unexpectedly strong guarantees.
  {\em Simple tabulation hashing\/} dates back to Zobrist [1970]. Keys are
viewed as consisting of $c$ characters and we have precomputed character tables
$h_1,...,h_c$ mapping characters to random hash values. A key $x=(x_1,...,x_c)$
is hashed to $h_1[x_1] \oplus h_2[x_2].....\oplus h_c[x_c]$. This schemes is
very fast with character tables in cache. While simple tabulation is not even
4-independent, it does provide many of the guarantees that are normally
obtained via higher independence, e.g., linear probing and Cuckoo hashing.
  Next we consider {\em twisted tabulation\/} where one input character is
&quot;twisted&quot; in a simple way. The resulting hash function has powerful
distributional properties: Chernoff-Hoeffding type tail bounds and a very small
bias for min-wise hashing. This is also yields an extremely fast pseudo-random
number generator that is provably good for many classic randomized algorithms
and data-structures.
  Finally, we consider {\em double tabulation\/} where we compose two simple
tabulation functions, applying one to the output of the other, and show that
this yields very high independence in the classic framework of Carter and
Wegman [1977]. In fact, w.h.p., for a given set of size proportional to that of
the space consumed, double tabulation gives fully-random hashing. We also
mention some more elaborate tabulation schemes getting near-optimal
independence for given time and space.
  While these tabulation schemes are all easy to implement and use, their
analysis is not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01535</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01535</id><created>2015-05-06</created><authors><author><keyname>Luong</keyname><forenames>Van Nghia</forenames></author><author><keyname>Nguyen</keyname><forenames>Ha Huy Cuong</forenames></author><author><keyname>Le</keyname><forenames>Van Son</forenames></author></authors><title>An improvement on fragmentation in Distribution Database Design Based on
  Knowledge-Oriented Clustering Techniques</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of optimizing distributed database includes: fragmentation and
positioning data. Several different approaches and algorithms have been
proposed to solve this problem. In this paper, we propose an algorithm that
builds the initial equivalence relation based on the distance threshold. This
threshold is also based on knowledge- oriented clustering techniques for both
of horizontal and vertical fragmentation. Similarity measures used in the
algorithms are the measures developed from the classical measures. Experimental
results carrying on the small data set match fragmented results based on the
classical algorithm. Execution time and data fragmentation significantly
reduced while the complexity of our algorithm in the general case is stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01539</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01539</id><created>2015-05-06</created><authors><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author></authors><title>Graphical Potential Games</title><categories>cs.GT cs.AI stat.ML</categories><comments>15 pages, To appear at The 26th International Conference on Game
  Theory, part of the Stony Brook Game Theory Summer Festival 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Potential games, originally introduced in the early 1990's by Lloyd Shapley,
the 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are a
very important class of models in game theory. They have special properties
such as the existence of Nash equilibria in pure strategies. This note
introduces graphical versions of potential games. Special cases of graphical
potential games have already found applicability in many areas of science and
engineering beyond economics, including artificial intelligence, computer
vision, and machine learning. They have been effectively applied to the study
and solution of important real-world problems such as routing and congestion in
networks, distributed resource allocation (e.g., public goods), and
relaxation-labeling for image segmentation. Implicit use of graphical potential
games goes back at least 40 years. Several classes of games considered standard
in the literature, including coordination games, local interaction games,
lattice games, congestion games, and party-affiliation games, are instances of
graphical potential games. This note provides several characterizations of
graphical potential games by leveraging well-known results from the literature
on probabilistic graphical models. A major contribution of the work presented
here that particularly distinguishes it from previous work is establishing that
the convergence of certain type of game-playing rules implies that the
agents/players must be embedded in some graphical potential game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01547</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01547</id><created>2015-05-06</created><authors><author><keyname>Ross</keyname><forenames>Gordon J</forenames></author><author><keyname>Jones</keyname><forenames>Tim</forenames></author></authors><title>Understanding the Heavy Tailed Dynamics in Human Behavior</title><categories>physics.soc-ph cs.SI stat.AP</categories><comments>9 pages in Physical Review E, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent availability of electronic datasets containing large volumes of
communication data has made it possible to study human behavior on a larger
scale than ever before. From this, it has been discovered that across a diverse
range of data sets, the inter-event times between consecutive communication
events obey heavy tailed power law dynamics. Explaining this has proved
controversial, and two distinct hypotheses have emerged. The first holds that
these power laws are fundamental, and arise from the mechanisms such as
priority queuing that humans use to schedule tasks. The second holds that they
are a statistical artifact which only occur in aggregated data when features
such as circadian rhythms and burstiness are ignored. We use a large social
media data set to test these hypotheses, and find that although models that
incorporate circadian rhythms and burstiness do explain part of the observed
heavy tails, there is residual unexplained heavy tail behavior which suggests a
more fundamental cause. Based on this, we develop a new quantitative model of
human behavior which improves on existing approaches, and gives insight into
the mechanisms underlying human interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01554</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01554</id><created>2015-05-06</created><updated>2015-10-07</updated><authors><author><keyname>Chen</keyname><forenames>Xinlei</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author></authors><title>Webly Supervised Learning of Convolutional Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to utilize large amounts of web data for learning
CNNs. Specifically inspired by curriculum learning, we present a two-step
approach for CNN training. First, we use easy images to train an initial visual
representation. We then use this initial CNN and adapt it to harder, more
realistic images by leveraging the structure of data and categories. We
demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on
ImageNet on Pascal VOC 2012. We also demonstrate the strength of webly
supervised learning by localizing objects in web images and training a R-CNN
style detector. It achieves the best performance on VOC 2007 where no VOC
training data is used. Finally, we show our approach is quite robust to noise
and performs comparably even when we use image search results from March 2013
(pre-CNN image search era).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01560</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01560</id><created>2015-05-06</created><authors><author><keyname>Nguyen</keyname><forenames>Tam V.</forenames></author><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Sepulveda</keyname><forenames>Jose</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Adaptive Nonparametric Image Parsing</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present an adaptive nonparametric solution to the image
parsing task, namely annotating each image pixel with its corresponding
category label. For a given test image, first, a locality-aware retrieval set
is extracted from the training data based on super-pixel matching similarities,
which are augmented with feature extraction for better differentiation of local
super-pixels. Then, the category of each super-pixel is initialized by the
majority vote of the $k$-nearest-neighbor super-pixels in the retrieval set.
Instead of fixing $k$ as in traditional non-parametric approaches, here we
propose a novel adaptive nonparametric approach which determines the
sample-specific k for each test image. In particular, $k$ is adaptively set to
be the number of the fewest nearest super-pixels which the images in the
retrieval set can use to get the best category prediction. Finally, the initial
super-pixel labels are further refined by contextual smoothing. Extensive
experiments on challenging datasets demonstrate the superiority of the new
solution over other state-of-the-art nonparametric solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01569</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01569</id><created>2015-05-06</created><updated>2015-07-25</updated><authors><author><keyname>Batagelj</keyname><forenames>Vladimir</forenames></author><author><keyname>Praprotnik</keyname><forenames>Selena</forenames></author></authors><title>An algebraic approach to temporal network analysis based on temporal
  quantities</title><categories>cs.SI cs.DM math.CO physics.soc-ph</categories><comments>The paper is based on our talks presented at the 1st European
  Conference on Social Networks, Barcelona (UAB), July 1-4, 2014</comments><msc-class>91D30, 16Y60, 90B10, 68R10, 93C55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a temporal network, the presence and activity of nodes and links can
change through time. To describe temporal networks we introduce the notion of
temporal quantities. We define the addition and multiplication of temporal
quantities in a way that can be used for the definition of addition and
multiplication of temporal networks. The corresponding algebraic structures are
semirings. The usual approach to (data) analysis of temporal networks is to
transform it into a sequence of time slices -- static networks corresponding to
selected time intervals and analyze each of them using standard methods to
produce a sequence of results. The approach proposed in this paper enables us
to compute these results directly. We developed fast algorithms for the
proposed operations. They are available as an open source Python library TQ
(Temporal Quantities) and a program Ianus. The proposed approach enables us to
treat as temporal quantities also other network characteristics such as
degrees, connectivity components, centrality measures, Pathfinder skeleton,
etc. To illustrate the developed tools we present some results from the
analysis of Franzosi's violence network and Corman's Reuters terror news
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01576</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01576</id><created>2015-05-07</created><authors><author><keyname>Sankaran</keyname><forenames>Bharath</forenames></author><author><keyname>Ghazvininejad</keyname><forenames>Marjan</forenames></author><author><keyname>He</keyname><forenames>Xinran</forenames></author><author><keyname>Kale</keyname><forenames>David</forenames></author><author><keyname>Cohen</keyname><forenames>Liron</forenames></author></authors><title>Learning and Optimization with Submodular Functions</title><categories>cs.LG</categories><comments>Tech Report - USC Computer Science CS-599, Convex and Combinatorial
  Optimization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many naturally occurring optimization problems one needs to ensure that
the definition of the optimization problem lends itself to solutions that are
tractable to compute. In cases where exact solutions cannot be computed
tractably, it is beneficial to have strong guarantees on the tractable
approximate solutions. In order operate under these criterion most optimization
problems are cast under the umbrella of convexity or submodularity. In this
report we will study design and optimization over a common class of functions
called submodular functions. Set functions, and specifically submodular set
functions, characterize a wide variety of naturally occurring optimization
problems, and the property of submodularity of set functions has deep
theoretical consequences with wide ranging applications. Informally, the
property of submodularity of set functions concerns the intuitive &quot;principle of
diminishing returns. This property states that adding an element to a smaller
set has more value than adding it to a larger set. Common examples of
submodular monotone functions are entropies, concave functions of cardinality,
and matroid rank functions; non-monotone examples include graph cuts, network
flows, and mutual information.
  In this paper we will review the formal definition of submodularity; the
optimization of submodular functions, both maximization and minimization; and
finally discuss some applications in relation to learning and reasoning using
submodular functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01577</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01577</id><created>2015-05-07</created><authors><author><keyname>Nakasho</keyname><forenames>Kazuhisa</forenames></author><author><keyname>Shidama</keyname><forenames>Yasunari</forenames></author></authors><title>Documentation Generator Focusing on Symbols for the HTML-ized Mizar
  Library</title><categories>cs.MS</categories><comments>5 pages, 1 figures, Conference on Intelligent Computer Mathematics
  2015 (CICM2015)</comments><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this project is to collect symbol information in the Mizar
Mathematical Library and manipulate it into practical and organized
documentation. Inspired by the MathWiki project and API reference systems for
computer programs, we developed a documentation generator focusing on symbols
for the HTML-ized Mizar library. The system has several helpful features,
including a symbol list, incremental search, and a referrer list. It targets
those who use proof assistance systems, the volume of whose libraries has been
rapidly increasing year by year.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01589</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01589</id><created>2015-05-07</created><updated>2015-05-08</updated><authors><author><keyname>Shen</keyname><forenames>Li</forenames></author><author><keyname>Chua</keyname><forenames>Teck Wee</forenames></author><author><keyname>Leman</keyname><forenames>Karianto</forenames></author></authors><title>Shadow Optimization from Structured Deep Edge Detection</title><categories>cs.CV</categories><comments>8 pages. CVPR 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Local structures of shadow boundaries as well as complex interactions of
image regions remain largely unexploited by previous shadow detection
approaches. In this paper, we present a novel learning-based framework for
shadow region recovery from a single image. We exploit the local structures of
shadow edges by using a structured CNN learning framework. We show that using
the structured label information in the classification can improve the local
consistency of the results and avoid spurious labelling. We further propose and
formulate a shadow/bright measure to model the complex interactions among image
regions. The shadow and bright measures of each patch are computed from the
shadow edges detected in the image. Using the global interaction constraints on
patches, we formulate a least-square optimization problem for shadow recovery
that can be solved efficiently. Our shadow recovery method achieves
state-of-the-art results on the major shadow benchmark databases collected
under various conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01594</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01594</id><created>2015-05-07</created><authors><author><keyname>Sahu</keyname><forenames>Saraswati B.</forenames></author></authors><title>Secure User Authentication &amp; Graphical Password using Cued Click-Points</title><categories>cs.CR</categories><comments>5pages,11 figures. arXiv admin note: substantial text overlap with
  http://www.ijareeie.com/upload/2013/july/20_GRAPHICAL.pdf and other sources,
  without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The major problem of user registration, mostly text base password, is well
known. In the login user be inclined to select simple passwords which is
frequently in mind that are straightforward for attackers to guess, difficult
machine created password mostly complicated to user take in mind. User
authenticate password using cued click points and Persuasive Cued Click Points
graphical password scheme which includes usability and security evaluations.
This paper includes the persuasion to secure user authentication &amp; graphical
password using cued click-points so that users select more random or more
difficult to guess the passwords. In click-based graphical passwords, image or
video frame that provide database to load the image, and then store all
information into database. Mainly passwords are composed of strings which have
letters as well as digits. Example is alpha-numeric type letters and digits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01596</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01596</id><created>2015-05-07</created><updated>2015-09-14</updated><authors><author><keyname>Agrawal</keyname><forenames>Pulkit</forenames></author><author><keyname>Carreira</keyname><forenames>Joao</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Learning to See by Moving</title><categories>cs.CV cs.NE cs.RO</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dominant paradigm for feature learning in computer vision relies on
training neural networks for the task of object recognition using millions of
hand labelled images. Is it possible to learn useful features for a diverse set
of visual tasks using any other form of supervision? In biology, living
organisms developed the ability of visual perception for the purpose of moving
and acting in the world. Drawing inspiration from this observation, in this
work we investigate if the awareness of egomotion can be used as a supervisory
signal for feature learning. As opposed to the knowledge of class labels,
information about egomotion is freely available to mobile agents. We show that
given the same number of training images, features learnt using egomotion as
supervision compare favourably to features learnt using class-label as
supervision on visual tasks of scene recognition, object recognition, visual
odometry and keypoint matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01599</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01599</id><created>2015-05-07</created><authors><author><keyname>Kume</keyname><forenames>Kenji</forenames></author><author><keyname>Nose-Togawa</keyname><forenames>Naoko</forenames></author></authors><title>Filter characteristics in image decomposition with singular spectrum
  analysis</title><categories>cs.CV cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Singular spectrum analysis is developed as a nonparametric spectral
decomposition of a time series. It can be easily extended to the decomposition
of multidimensional lattice-like data through the filtering interpretation. In
this viewpoint, the singular spectrum analysis can be understood as the
adaptive and optimal generation of the filters and their two-step
point-symmetric operation to the original data. In this paper, we point out
that, when applied to the multidimensional data, the adaptively generated
filters exhibit symmetry properties resulting from the bisymmetric nature of
the lag-covariance matrices. The eigenvectors of the lag-covariance matrix are
either symmetric or antisymmetric, and for the 2D image data, these lead to the
differential-type filters with even- or odd-order derivatives. The dominant
filter is a smoothing filter, reflecting the dominance of low-frequency
components of the photo images. The others are the edge-enhancement or the
noise filters corresponding to the band-pass or the high-pass filters. The
implication of the decomposition to the image denoising is briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01603</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01603</id><created>2015-05-07</created><authors><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Schaeffer</keyname><forenames>Jonathan</forenames></author><author><keyname>Pijls</keyname><forenames>Wim</forenames></author><author><keyname>de Bruin</keyname><forenames>Arie</forenames></author></authors><title>Best-First and Depth-First Minimax Search in Practice</title><categories>cs.AI</categories><comments>Computer Science in the Netherlands 1995. arXiv admin note: text
  overlap with arXiv:1404.1515</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most practitioners use a variant of the Alpha-Beta algorithm, a simple
depth-first pro- cedure, for searching minimax trees. SSS*, with its best-first
search strategy, reportedly offers the potential for more efficient search.
However, the complex formulation of the al- gorithm and its alleged excessive
memory requirements preclude its use in practice. For two decades, the search
efficiency of &quot;smart&quot; best-first SSS* has cast doubt on the effectiveness of
&quot;dumb&quot; depth-first Alpha-Beta. This paper presents a simple framework for
calling Alpha-Beta that allows us to create a variety of algorithms, including
SSS* and DUAL*. In effect, we formulate a best-first algorithm using
depth-first search. Expressed in this framework SSS* is just a special case of
Alpha-Beta, solving all of the perceived drawbacks of the algorithm. In
practice, Alpha-Beta variants typically evaluate less nodes than SSS*. A new
instance of this framework, MTD(f), out-performs SSS* and NegaScout, the
Alpha-Beta variant of choice by practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01606</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01606</id><created>2015-05-07</created><authors><author><keyname>Thakkar</keyname><forenames>Harsh</forenames></author><author><keyname>Iyer</keyname><forenames>Ganesh</forenames></author><author><keyname>Majumder</keyname><forenames>Prasenjit</forenames></author></authors><title>A comparative study of approaches in user-centered health information
  retrieval</title><categories>cs.IR</categories><comments>6 pages, 2 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we survey various user-centered or context-based biomedical
health information retrieval systems. We present and discuss the performance of
systems submitted in CLEF eHealth 2014 Task 3 for this purpose. We classify and
focus on comparing the two most prevalent retrieval models in biomedical
information retrieval namely: Language Model (LM) and Vector Space Model (VSM).
We also report on the effectiveness of using external medical resources and
ontologies like MeSH, Metamap, UMLS, etc. We observed that the L.M. based
retrieval systems outperform VSM based systems on various fronts. From the
results we conclude that the state-of-art system scores for MAP was 0.4146,
P@10 was 0.7560 and NDCG@10 was 0.7445, respectively. All of these score were
reported by systems built on language modelling approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01616</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01616</id><created>2015-05-07</created><authors><author><keyname>Aboulker</keyname><forenames>Pierre</forenames></author><author><keyname>Brettell</keyname><forenames>Nick</forenames></author><author><keyname>Havet</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Trotignon</keyname><forenames>Nicolas</forenames></author></authors><title>Colouring graphs with constraints on connectivity</title><categories>math.CO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ has maximal local edge-connectivity $k$ if the maximum number of
edge-disjoint paths between every pair of distinct vertices $x$ and $y$ is at
most $k$. We prove Brooks-type theorems for $k$-connected graphs with maximal
local edge-connectivity $k$, and for any graph with maximal local
edge-connectivity 3. We also consider several related graph classes defined by
constraints on connectivity. In particular, we show that there is a
polynomial-time algorithm that, given a $3$-connected graph $G$ with maximal
local connectivity 3, outputs an optimal colouring for $G$. On the other hand,
we prove, for $k \ge 3$, that $k$-colourability is NP-complete when restricted
to minimally $k$-connected graphs, and 3-colourability is NP-complete when
restricted to $(k-1)$-connected graphs with maximal local connectivity $k$.
Finally, we consider a parameterization of $k$-colourability based on the
number of vertices of degree at least $k+1$, and prove that, even when $k$ is
part of the input, the corresponding parameterized problem is FPT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01617</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01617</id><created>2015-05-07</created><authors><author><keyname>Zhao</keyname><forenames>Dengji</forenames></author><author><keyname>Ramchurn</keyname><forenames>Sarvapali D.</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author></authors><title>Incentive Design for Ridesharing with Uncertainty</title><categories>cs.GT</categories><comments>13 pages</comments><acm-class>J.4; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a ridesharing problem where there is uncertainty about the
completion of trips from both drivers and riders. Specifically, we study
ridesharing mechanisms that aim to incentivize commuters to reveal their
valuation for trips and their probability of undertaking their trips. Due to
the interdependence created by the uncertainty on commuters' valuations, we
show that the Groves mechanisms are not ex-post truthful even if there is only
one commuter whose valuation depends on the other commuters' uncertainty of
undertaking their trips. To circumvent this impossibility, we propose an
ex-post truthful mechanism, the best incentive we can design without
sacrificing social welfare in this setting. Our mechanism pays a commuter if
she undertakes her trip, otherwise she is penalized for not undertaking her
trip. Furthermore, we identify a sufficient and necessary condition under which
our mechanism is ex-post truthful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01619</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01619</id><created>2015-05-07</created><authors><author><keyname>Boyer</keyname><forenames>Claire</forenames></author><author><keyname>Bigot</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Weiss</keyname><forenames>Pierre</forenames></author></authors><title>Compressed sensing with structured sparsity and structured acquisition</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed Sensing (CS) is an appealing framework for applications such as
Magnetic Resonance Imaging (MRI). However, up-to-date, the sensing schemes
suggested by CS theories are made of random isolated measurements, which are
usually incompatible with the physics of acquisition. To reflect the physical
constraints of the imaging device, we introduce the notion of blocks of
measurements: the sensing scheme is not a set of isolated measurements anymore,
but a set of groups of measurements which may represent any arbitrary shape
(radial lines for instance). Structured acquisition with blocks of measurements
are easy to implement, and they give good reconstruction results in practice.
However, very few results exist on the theoretical guarantees of CS
reconstructions in this setting.
  In this paper, we fill the gap between CS theory and acquisitions made in
practice. To this end, the key feature to consider is the structured sparsity
of the signal to reconstruct. In this paper, we derive new CS results for
structured acquisitions and signal satisfying a prior structured sparsity. The
obtained results are RIPless, in the sense that they do not hold for any
$s$-sparse vector, but for sparse vectors with a given support $S$. Our results
are thus support-dependent, and they offer the possibility for flexible
assumptions on the structure of $S$. Moreover, our results are also
drawing-dependent, since we highlight an explicit dependency between the
probability of reconstructing a sparse vector and the way of choosing the
blocks of measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01620</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01620</id><created>2015-05-07</created><authors><author><keyname>Autexier</keyname><forenames>Serge</forenames></author><author><keyname>Hutter</keyname><forenames>Dieter</forenames></author></authors><title>Structure Formation in Large Theories</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structuring theories is one of the main approaches to reduce the
combinatorial explosion associated with reasoning and exploring large theories.
In the past we developed the notion of development graphs as a means to
represent and maintain structured theories. In this paper we present a
methodology and a resulting implementation to reveal the hidden structure of
flat theories by transforming them into detailed development graphs. We review
our approach using plain TSTP-representations of MIZAR articles obtaining more
structured and also more concise theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01621</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01621</id><created>2015-05-07</created><authors><author><keyname>Gogna</keyname><forenames>Anupriya</forenames></author><author><keyname>Majumdar</keyname><forenames>Angshul</forenames></author></authors><title>Blind Compressive Sensing Framework for Collaborative Filtering</title><categories>cs.IR cs.LG</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing works based on latent factor models have focused on representing the
rating matrix as a product of user and item latent factor matrices, both being
dense. Latent (factor) vectors define the degree to which a trait is possessed
by an item or the affinity of user towards that trait. A dense user matrix is a
reasonable assumption as each user will like/dislike a trait to certain extent.
However, any item will possess only a few of the attributes and never all.
Hence, the item matrix should ideally have a sparse structure rather than a
dense one as formulated in earlier works. Therefore we propose to factor the
ratings matrix into a dense user matrix and a sparse item matrix which leads us
to the Blind Compressed Sensing (BCS) framework. We derive an efficient
algorithm for solving the BCS problem based on Majorization Minimization (MM)
technique. Our proposed approach is able to achieve significantly higher
accuracy and shorter run times as compared to existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01625</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01625</id><created>2015-05-07</created><authors><author><keyname>Simsek</keyname><forenames>Meryem</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>G&#xfc;venc</keyname><forenames>Ismail</forenames></author></authors><title>Context-Aware Mobility Management in HetNets: A Reinforcement Learning
  Approach</title><categories>cs.NI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of small cell deployments in heterogeneous network (HetNet)
environments is expected to be a key feature of 4G networks and beyond, and
essential for providing higher user throughput and cell-edge coverage. However,
due to different coverage sizes of macro and pico base stations (BSs), such a
paradigm shift introduces additional requirements and challenges in dense
networks. Among these challenges is the handover performance of user equipment
(UEs), which will be impacted especially when high velocity UEs traverse
picocells. In this paper, we propose a coordination-based and context-aware
mobility management (MM) procedure for small cell networks using tools from
reinforcement learning. Here, macro and pico BSs jointly learn their long-term
traffic loads and optimal cell range expansion, and schedule their UEs based on
their velocities and historical rates (exchanged among tiers). The proposed
approach is shown to not only outperform the classical MM in terms of UE
throughput, but also to enable better fairness. In average, a gain of up to
80\% is achieved for UE throughput, while the handover failure probability is
reduced up to a factor of three by the proposed learning based MM approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01629</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01629</id><created>2015-05-07</created><authors><author><keyname>Wisniewski</keyname><forenames>Max</forenames></author><author><keyname>Steen</keyname><forenames>Alexander</forenames></author><author><keyname>Benzm&#xfc;ller</keyname><forenames>Christoph</forenames></author></authors><title>LeoPARD --- A Generic Platform for the Implementation of Higher-Order
  Reasoners</title><categories>cs.LO cs.AI cs.MA cs.MS</categories><comments>6 pages, to appear in the proceedings of CICM'2015 conference</comments><msc-class>03B35, 68T15</msc-class><acm-class>I.2.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LeoPARD supports the implementation of knowledge representation and reasoning
tools for higher-order logic(s). It combines a sophisticated data structure
layer (polymorphically typed {\lambda}-calculus with nameless spine notation,
explicit substitutions, and perfect term sharing) with an ambitious multi-agent
blackboard architecture (supporting prover parallelism at the term, clause, and
search level). Further features of LeoPARD include a parser for all TPTP
dialects, a command line interpreter, and generic means for the integration of
external reasoners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01630</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01630</id><created>2015-05-07</created><authors><author><keyname>Nielsen</keyname><forenames>Jimmy J.</forenames></author><author><keyname>Olsen</keyname><forenames>Rasmus L.</forenames></author><author><keyname>Madsen</keyname><forenames>Tatiana K.</forenames></author><author><keyname>Uguen</keyname><forenames>Bernard</forenames></author><author><keyname>Schwefel</keyname><forenames>Hans-Peter</forenames></author></authors><title>Location-Quality-aware Policy Optimisation for Relay Selection in Mobile
  Networks</title><categories>cs.NI</categories><comments>Accepted for publication in ACM/Springer Wireless Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relaying can improve the coverage and performance of wireless access
networks. In presence of a localisation system at the mobile nodes, the use of
such location estimates for relay node selection can be advantageous as such
information can be collected by access points in linear effort with respect to
number of mobile nodes (while the number of links grows quadratically).
However, the localisation error and the chosen update rate of location
information in conjunction with the mobility model affect the performance of
such location-based relay schemes; these parameters also need to be taken into
account in the design of optimal policies. This paper develops a Markov model
that can capture the joint impact of localisation errors and inaccuracies of
location information due to forwarding delays and mobility; the Markov model is
used to develop algorithms to determine optimal location-based relay policies
that take the aforementioned factors into account. The model is subsequently
used to analyse the impact of deployment parameter choices on the performance
of location-based relaying in WLAN scenarios with free-space propagation
conditions and in an measurement-based indoor office scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01631</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01631</id><created>2015-05-07</created><authors><author><keyname>Serna</keyname><forenames>Citlalli Gamez</forenames></author><author><keyname>Pillay</keyname><forenames>Ruven</forenames></author><author><keyname>Tremeau</keyname><forenames>Alain</forenames></author></authors><title>Data Fusion of Objects Using Techniques Such as Laser Scanning,
  Structured Light and Photogrammetry for Cultural Heritage Applications</title><categories>cs.CV</categories><journal-ref>Computational Color Imaging, Lecture Notes in Computer Science,
  Springer, 2015, pp. 208-224</journal-ref><doi>10.1007/978-3-319-15979-9_20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a semi-automatic 2D-3D local registration pipeline
capable of coloring 3D models obtained from 3D scanners by using uncalibrated
images. The proposed pipeline exploits the Structure from Motion (SfM)
technique in order to reconstruct a sparse representation of the 3D object and
obtain the camera parameters from image feature matches. We then coarsely
register the reconstructed 3D model to the scanned one through the Scale
Iterative Closest Point (SICP) algorithm. SICP provides the global scale,
rotation and translation parameters, using minimal manual user intervention. In
the final processing stage, a local registration refinement algorithm optimizes
the color projection of the aligned photos on the 3D object removing the
blurring/ghosting artefacts introduced due to small inaccuracies during the
registration. The proposed pipeline is capable of handling real world cases
with a range of characteristics from objects with low level geometric features
to complex ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01634</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01634</id><created>2015-05-07</created><updated>2016-02-01</updated><authors><author><keyname>Walk</keyname><forenames>Simon</forenames></author><author><keyname>Helic</keyname><forenames>Denis</forenames></author><author><keyname>Geigl</keyname><forenames>Florian</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>Activity Dynamics in Collaboration Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many online collaboration networks struggle to gain user activity and become
self-sustaining due to the ramp-up problem or dwindling activity within the
system. Prominent examples include online encyclopedias such as (Semantic)
MediaWikis, Question and Answering portals such as StackOverflow, and many
others. Only a small fraction of these systems manage to reach self-sustaining
activity, a level of activity that prevents the system from reverting to a
non-active state. In this paper, we model and analyze activity dynamics in
synthetic and empirical collaboration networks. Our approach is based on two
opposing and well-studied principles: (i) without incentives, users tend to
lose interest to contribute and thus, systems become inactive, and (ii) people
are susceptible to actions taken by their peers (social or peer influence).
With the activity dynamics model that we introduce in this paper we can
represent typical situations of such collaboration networks. For example,
activity in a collaborative network, without external impulses or investments,
will vanish over time, eventually rendering the system inactive. However, by
appropriately manipulating the activity dynamics and/or the underlying
collaboration networks, we can jump-start a previously inactive system and
advance it towards an active state. To be able to do so, we first describe our
model and its underlying mechanisms. We then provide illustrative examples of
empirical datasets and characterize the barrier that has to be breached by a
system before it can become self-sustaining in terms of critical mass and
activity dynamics. Additionally, we expand on this empirical illustration and
introduce a new metric p---the Activity Momentum---to assess the activity
robustness of collaboration networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01637</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01637</id><created>2015-05-07</created><authors><author><keyname>Farooqi</keyname><forenames>Shehroze</forenames></author><author><keyname>Ikram</keyname><forenames>Muhammad</forenames></author><author><keyname>Irfan</keyname><forenames>Gohar</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Friedman</keyname><forenames>Arik</forenames></author><author><keyname>Jourjon</keyname><forenames>Guillaume</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Shafiq</keyname><forenames>M. Zubair</forenames></author><author><keyname>Zaffar</keyname><forenames>Fareed</forenames></author></authors><title>Characterizing Seller-Driven Black-Hat Marketplaces</title><categories>cs.CY cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates two seller-driven black-hat online marketplaces,
SEOClerks and MyCheapJobs, aiming to shed light on the services they offer as
well as sellers and customers they attract. We perform a measurement-based
analysis based on complete crawls of their websites and find that the vast
majority of services target popular social media and e-commerce websites, such
as website backlinks, Instagram followers, or Twitter retweets, and estimate
revenues to be at least $1.3M for SEOClerks and $116K for MyCheapJobs. Our
analysis uncovers the characteristics of these two non-underground
seller-driven marketplaces and shows that many top sellers belong to an
&quot;insider ring&quot;, where accounts created close to the marketplaces' launch
account for the majority of the sales. We provide first-of-its-kind evidence
that marketplace operators may be involved in perpetuating fraudulent
activities and hope to facilitate deployment of technical, legal, and economic
countermeasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01658</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01658</id><created>2015-05-07</created><updated>2015-05-13</updated><authors><author><keyname>Branco</keyname><forenames>Paula</forenames></author><author><keyname>Torgo</keyname><forenames>Luis</forenames></author><author><keyname>Ribeiro</keyname><forenames>Rita</forenames></author></authors><title>A Survey of Predictive Modelling under Imbalanced Distributions</title><categories>cs.LG</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world data mining applications involve obtaining predictive models
using data sets with strongly imbalanced distributions of the target variable.
Frequently, the least common values of this target variable are associated with
events that are highly relevant for end users (e.g. fraud detection, unusual
returns on stock markets, anticipation of catastrophes, etc.). Moreover, the
events may have different costs and benefits, which when associated with the
rarity of some of them on the available training data creates serious problems
to predictive modelling techniques. This paper presents a survey of existing
techniques for handling these important applications of predictive analytics.
Although most of the existing work addresses classification tasks (nominal
target variables), we also describe methods designed to handle similar problems
within regression tasks (numeric target variables). In this survey we discuss
the main challenges raised by imbalanced distributions, describe the main
approaches to these problems, propose a taxonomy of these methods and refer to
some related problems within predictive modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01662</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01662</id><created>2015-05-07</created><authors><author><keyname>Paulson</keyname><forenames>Lawrence C.</forenames></author></authors><title>A Formalisation of Finite Automata using Hereditarily Finite Sets</title><categories>cs.FL cs.LO</categories><comments>Accepted to CADE-25 (International Conference on Automated
  Deduction), Berlin, August 2015</comments><acm-class>F.4.1; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hereditarily finite (HF) set theory provides a standard universe of sets, but
with no infinite sets. Its utility is demonstrated through a formalisation of
the theory of regular languages and finite automata, including the
Myhill-Nerode theorem and Brzozowski's minimisation algorithm. The states of an
automaton are HF sets, possibly constructed by product, sum, powerset and
similar operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01668</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01668</id><created>2015-05-07</created><updated>2015-12-08</updated><authors><author><keyname>Balthasar</keyname><forenames>Mark Ryan</forenames></author><author><keyname>Braca</keyname><forenames>Paolo</forenames></author><author><keyname>Zoubir</keyname><forenames>Abdelhak M.</forenames></author></authors><title>Distributed Multi-Target Tracking in Autonomous Sensor Networks</title><categories>cs.MA cs.SY stat.AP</categories><comments>12 pages, 8 figures</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-target tracking is an important problem in civilian and military
applications. This paper investigates distributed multi-target tracking using
autonomous sensor networks. Data association, which arises particularly in
multi-object scenarios, can be tackled by various solutions. We consider
sequential Monte Carlo implementations of the Probability Hypothesis Density
(PHD) filter based on random finite sets. This approach circumvents the data
association issue by jointly estimating all targets in the region of interest.
To this end, we develop the Multi-Sensor Particle PHD Filter (MS-PPHDF) as well
as a distributed version, called Diffusion Particle PHD Filter (D-PPHDF). Their
performance is evaluated in terms of the Optimal Subpattern Assignment (OSPA)
metric, benchmarked against a distributed extension of the Posterior
Cram\'er-Rao Lower Bound (PCRLB), and compared to the performance of an
existing distributed PHD Particle Filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01682</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01682</id><created>2015-05-07</created><authors><author><keyname>Kotelnikov</keyname><forenames>Evgenii</forenames></author><author><keyname>Kov&#xe1;cs</keyname><forenames>Laura</forenames></author><author><keyname>Voronkov</keyname><forenames>Andrei</forenames></author></authors><title>A First Class Boolean Sort in First-Order Theorem Proving and TPTP</title><categories>cs.LO</categories><doi>10.1007/978-3-319-20615-8_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To support reasoning about properties of programs operating with boolean
values one needs theorem provers to be able to natively deal with the boolean
sort. This way, program properties can be translated to first-order logic and
theorem provers can be used to prove program properties efficiently. However,
in the TPTP language, the input language of automated first-order theorem
provers, the use of the boolean sort is limited compared to other sorts, thus
hindering the use of first-order theorem provers in program analysis and
verification. In this paper, we present an extension FOOL of many-sorted
first-order logic, in which the boolean sort is treated as a first-class sort.
Boolean terms are indistinguishable from formulas and can appear as arguments
to functions. In addition, FOOL contains if-then-else and let-in constructs. We
define the syntax and semantics of FOOL and its model-preserving translation to
first-order logic. We also introduce a new technique of dealing with boolean
sorts in superposition-based theorem provers. Finally, we discuss how the TPTP
language can be changed to support FOOL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01694</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01694</id><created>2015-05-07</created><updated>2015-12-02</updated><authors><author><keyname>Shekatkar</keyname><forenames>Snehal M.</forenames></author><author><keyname>Bhagwat</keyname><forenames>Chandrasheel</forenames></author><author><keyname>Ambika</keyname><forenames>G.</forenames></author></authors><title>Divisibility patterns of natural numbers on a complex network</title><categories>cs.SI math.CO math.NT physics.soc-ph</categories><comments>13 pages, 9 figures</comments><journal-ref>Scientific Reports 5:14280 (2015)</journal-ref><doi>10.1038/srep14280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Investigation of divisibility properties of natural numbers is one of the
most important themes in the theory of numbers. Various tools have been
developed over the centuries to discover and study the various patterns in the
sequence of natural numbers in the context of divisibility. In the present
paper, we study the divisibility of natural numbers using the framework of a
growing complex network. In particular, using tools from the field of
statistical inference, we show that the network is scale-free but has a
non-stationary degree distribution. Along with this, we report a new kind of
similarity pattern for the local clustering, which we call &quot;stretching
similarity&quot;, in this network. We also show that the various characteristics
like average degree, global clustering coefficient and assortativity
coefficient of the network vary smoothly with the size of the network. Using
analytical arguments we estimate the asymptotic behavior of global clustering
and average degree which is validated using numerical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01695</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01695</id><created>2015-05-07</created><updated>2015-05-08</updated><authors><author><keyname>Bruggink</keyname><forenames>H. J. Sander</forenames></author><author><keyname>K&#xf6;nig</keyname><forenames>Barbara</forenames></author><author><keyname>Nolte</keyname><forenames>Dennis</forenames></author><author><keyname>Zantema</keyname><forenames>Hans</forenames></author></authors><title>Proving Termination of Graph Transformation Systems using Weighted Type
  Graphs over Semirings</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce techniques for proving uniform termination of graph
transformation systems, based on matrix interpretations for string rewriting.
We generalize this technique by adapting it to graph rewriting instead of
string rewriting and by generalizing to ordered semirings. In this way we
obtain a framework which includes the tropical and arctic type graphs
introduced in a previous paper and a new variant of arithmetic type graphs.
These type graphs can be used to assign weights to graphs and to show that
these weights decrease in every rewriting step in order to prove termination.
We present an example involving counters and discuss the implementation in the
tool Grez.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01700</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01700</id><created>2015-05-07</created><authors><author><keyname>Cramer</keyname><forenames>Ronald</forenames></author><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author></authors><title>An Improvement on the Hasse-Weil Bound and applications to Character
  Sums, Cryptography and Coding</title><categories>cs.DM math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hasse-Weil bound is a deep result in mathematics and has found wide
applications in mathematics, theoretical computer science, information theory
etc. In general, the bound is tight and cannot be improved. However, for some
special families of curves the bound could be improved substantially. In this
paper, we focus on the Hasse-Weil bound for the curve defined by $y^p-y=f(x)$
over the finite field $\F_q$, where $p$ is the characteristic of $\F_q$.
Recently, Kaufman and Lovett \cite[FOCS2011]{KL11} showed that the Hasse-Weil
bound can be improved for this family of curves with $f(x)=g(x)+h(x)$, where
$g(x)$ is a polynomial of degree $\ll \sqrt{q}$ and $h(x)$ is a sparse
polynomial of arbitrary degree but bounded weight degree. The other recent
improvement by Rojas-Leon and Wan \cite[Math. Ann. 2011]{RW11} shows that an
extra $\sqrt{p}$ can be removed for this family of curves if $p$ is very large
compared with polynomial degree of $f(x)$ and $\log_pq$.
  In this paper, we show that the Hasse-Weil bound for this special family of
curves can be improved if $q=p^n$ with odd $n$ which is the same case where
Serre \cite{Se85} improved the Hasse-Weil bound. However, our improvement is
greater than Serre's one for this special family of curves. Furthermore, our
improvement works for small $p$ as well compared with the requirement of large
$p$ by Rojas-Leon and Wan. In addition, our improvement finds interesting
applications to character sums, cryptography and coding theory. The key idea
behind is that this curve has the Hasse-Witt invariant $0$ and we show that the
Hasse-Weil bound can be improved for any curves with the Hasse-Witt invariant
$0$. The main tool used in our proof involves Newton polygon and some results
in algebraic geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01709</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01709</id><created>2015-05-07</created><authors><author><keyname>Saganowski</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Gliwa</keyname><forenames>Bogdan</forenames></author><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Ko&#x17a;lak</keyname><forenames>Jaros&#x142;aw</forenames></author></authors><title>Predicting Community Evolution in Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Entropy 2015, 17, 1-x manuscripts; doi:10.3390/e170x000x 46 pages</comments><journal-ref>Entropy 2015, 17, 3053-3096</journal-ref><doi>10.3390/e17053053</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Nowadays, sustained development of different social media can be observed
worldwide. One of the relevant research domains intensively explored recently
is analysis of social communities existing in social media as well as
prediction of their future evolution taking into account collected historical
evolution chains. These evolution chains proposed in the paper contain group
states in the previous time frames and its historical transitions that were
identified using one out of two methods: Stable Group Changes Identification
(SGCI) and Group Evolution Discovery (GED). Based on the observed evolution
chains of various length, structural network features are extracted, validated
and selected as well as used to learn classification models. The experimental
studies were performed on three real datasets with different profile: DBLP,
Facebook and Polish blogosphere. The process of group prediction was analysed
with respect to different classifiers as well as various descriptive feature
sets extracted from evolution chains of different length. The results revealed
that, in general, the longer evolution chains the better predictive abilities
of the classification models. However, chains of length 3 to 7 enabled the
GED-based method to almost reach its maximum possible prediction quality. For
SGCI, this value was at the level of 3 to 5 last periods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01711</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01711</id><created>2015-05-07</created><authors><author><keyname>Chouhan</keyname><forenames>Pushpinder Kaur</forenames></author><author><keyname>Yao</keyname><forenames>Feng</forenames></author><author><keyname>Yerima</keyname><forenames>Suleiman Y.</forenames></author><author><keyname>Sezer</keyname><forenames>Sakir</forenames></author></authors><title>Software as a Service: Analyzing Security Issues</title><categories>cs.CR cs.SE</categories><comments>International Conference on Big Data and Analytics for Business (BDAB
  2014), New Delhi, India, Dec. 28-29 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-as-a-service (SaaS) is a type of software service delivery model
which encompasses a broad range of business opportunities and challenges. Users
and service providers are reluctant to integrate their business into SaaS due
to its security concerns while at the same time they are attracted by its
benefits. This article highlights SaaS utility and applicability in different
environments like cloud computing, mobile cloud computing, software defined
networking and Internet of things. It then embarks on the analysis of SaaS
security challenges spanning across data security, application security and
SaaS deployment security. A detailed review of the existing mainstream
solutions to tackle the respective security issues mapping into different SaaS
security challenges is presented. Finally, possible solutions or techniques
which can be applied in tandem are presented for a secure SaaS platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01713</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01713</id><created>2015-05-07</created><updated>2015-11-30</updated><authors><author><keyname>Nielsen</keyname><forenames>Jimmy J.</forenames></author><author><keyname>Kim</keyname><forenames>Dong Min</forenames></author><author><keyname>Madue&#xf1;o</keyname><forenames>Germ&#xe1;n C.</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>A Tractable Model of the LTE Access Reservation Procedure for
  Machine-Type Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted, Revised, to be presented in IEEE Globecom 2015; v3: fixed
  error in eq. (4)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A canonical scenario in Machine-Type Communications (MTC) is the one
featuring a large number of devices, each of them with sporadic traffic. Hence,
the number of served devices in a single LTE cell is not determined by the
available aggregate rate, but rather by the limitations of the LTE access
reservation protocol. Specifically, the limited number of contention preambles
and the limited amount of uplink grants per random access response are crucial
to consider when dimensioning LTE networks for MTC. We propose a low-complexity
model of LTE's access reservation protocol that encompasses these two
limitations and allows us to evaluate the outage probability at click-speed.
The model is based chiefly on closed-form expressions, except for the part with
the feedback impact of retransmissions, which is determined by solving a fixed
point equation. Our model overcomes the incompleteness of the existing models
that are focusing solely on the preamble collisions. A comparison with the
simulated LTE access reservation procedure that follows the 3GPP
specifications, confirms that our model provides an accurate estimation of the
system outage event and the number of supported MTC devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01716</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01716</id><created>2015-05-07</created><authors><author><keyname>Burgess</keyname><forenames>Mark</forenames></author></authors><title>Spacetimes with Semantics (II), Scaling of agency, semantics, and
  tenancy</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Promise Theory as a calculus, I review how to define agency in a
scalable way, for the purpose of understanding semantic spacetimes. By
following simple scaling rules, replacing individual agents with `super-agents'
(sub-spaces), it is shown how agency can be scaled both dynamically and
semantically.
  The notion of occupancy and tenancy, or how space is used and filled in
different ways, is also defined, showing how spacetime can be shared between
independent parties, both by remote association and local encapsulation. I
describe how to build up dynamic and semantic continuity, by joining discrete
individual atoms and molecules of space into quasi-continuous lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01723</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01723</id><created>2015-05-07</created><authors><author><keyname>Hu</keyname><forenames>Yue</forenames></author><author><keyname>Zhao</keyname><forenames>Jichang</forenames></author><author><keyname>Wu</keyname><forenames>Junjie</forenames></author></authors><title>Emoticon-based Ambivalent Expression: A Hidden Indicator for Unusual
  Behaviors in Weibo</title><categories>cs.SI</categories><comments>Data sets can be downloaded freely from www.datatang.com/data/47207
  or http://pan.baidu.com/s/1mg67cbm. Any issues feel free to contact
  jichang@buaa.edu.cn</comments><doi>10.1371/journal.pone.0147079</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent decades have witnessed online social media being a big-data window for
quantificationally testifying conventional social theories and exploring much
detailed human behavioral patterns. In this paper, by tracing the emoticon use
in Weibo, a group of hidden &quot;ambivalent users&quot; are disclosed for frequently
posting ambivalent tweets containing both positive and negative emotions.
Further investigation reveals that this ambivalent expression could be a novel
indicator of many unusual social behaviors. For instance, ambivalent users with
the female as the majority like to make a sound in midnights or at weekends.
They mention their close friends frequently in ambivalent tweets, which attract
more replies and thus serve as a more private communication way. Ambivalent
users also respond differently to public affairs from others and demonstrate
more interests in entertainment and sports events. Moreover, the sentiment
shift of words adopted in ambivalent tweets is more evident than usual and
exhibits a clear &quot;negative to positive&quot; pattern. The above observations, though
being promiscuous seemingly, actually point to the self regulation of negative
mood in Weibo, which could find its base from the emotion management theories
in sociology but makes an interesting extension to the online environment.
Finally, as an interesting corollary, ambivalent users are found connected with
compulsive buyers and turn out to be perfect targets for online marketing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01728</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01728</id><created>2015-05-07</created><updated>2015-08-11</updated><authors><author><keyname>Prasad</keyname><forenames>Yamuna</forenames></author><author><keyname>Biswas</keyname><forenames>K. K.</forenames></author></authors><title>Integrating K-means with Quadratic Programming Feature Selection</title><categories>cs.CV cs.LG</categories><comments>17 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several data mining problems are characterized by data in high dimensions.
One of the popular ways to reduce the dimensionality of the data is to perform
feature selection, i.e, select a subset of relevant and non-redundant features.
Recently, Quadratic Programming Feature Selection (QPFS) has been proposed
which formulates the feature selection problem as a quadratic program. It has
been shown to outperform many of the existing feature selection methods for a
variety of applications. Though, better than many existing approaches, the
running time complexity of QPFS is cubic in the number of features, which can
be quite computationally expensive even for moderately sized datasets. In this
paper we propose a novel method for feature selection by integrating k-means
clustering with QPFS. The basic variant of our approach runs k-means to bring
down the number of features which need to be passed on to QPFS. We then enhance
this idea, wherein we gradually refine the feature space from a very coarse
clustering to a fine-grained one, by interleaving steps of QPFS with k-means
clustering. Every step of QPFS helps in identifying the clusters of irrelevant
features (which can then be thrown away), whereas every step of k-means further
refines the clusters which are potentially relevant. We show that our iterative
refinement of clusters is guaranteed to converge. We provide bounds on the
number of distance computations involved in the k-means algorithm. Further,
each QPFS run is now cubic in number of clusters, which can be much smaller
than actual number of features. Experiments on eight publicly available
datasets show that our approach gives significant computational gains (both in
time and memory), over standard QPFS as well as other state of the art feature
selection methods, even while improving the overall accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01731</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01731</id><created>2015-05-07</created><authors><author><keyname>Chitnis</keyname><forenames>Rajesh</forenames></author><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Esfandiari</keyname><forenames>Hossein</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>McGregor</keyname><forenames>Andrew</forenames></author><author><keyname>Monemizadeh</keyname><forenames>Morteza</forenames></author><author><keyname>Vorotnikova</keyname><forenames>Sofya</forenames></author></authors><title>Kernelization via Sampling with Applications to Dynamic Graph Streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a simple but powerful subgraph sampling primitive
that is applicable in a variety of computational models including dynamic graph
streams (where the input graph is defined by a sequence of edge/hyperedge
insertions and deletions) and distributed systems such as MapReduce. In the
case of dynamic graph streams, we use this primitive to prove the following
results:
  -- Matching: First, there exists an $\tilde{O}(k^2)$ space algorithm that
returns an exact maximum matching on the assumption the cardinality is at most
$k$. The best previous algorithm used $\tilde{O}(kn)$ space where $n$ is the
number of vertices in the graph and we prove our result is optimal up to
logarithmic factors. Our algorithm has $\tilde{O}(1)$ update time. Second,
there exists an $\tilde{O}(n^2/\alpha^3)$ space algorithm that returns an
$\alpha$-approximation for matchings of arbitrary size. (Assadi et al. (2015)
showed that this was optimal and independently and concurrently established the
same upper bound.) We generalize both results for weighted matching. Third,
there exists an $\tilde{O}(n^{4/5})$ space algorithm that returns a constant
approximation in graphs with bounded arboricity.
  -- Vertex Cover and Hitting Set: There exists an $\tilde{O}(k^d)$ space
algorithm that solves the minimum hitting set problem where $d$ is the
cardinality of the input sets and $k$ is an upper bound on the size of the
minimum hitting set. We prove this is optimal up to logarithmic factors. Our
algorithm has $\tilde{O}(1)$ update time. The case $d=2$ corresponds to minimum
vertex cover.
  Finally, we consider a larger family of parameterized problems (including
$b$-matching, disjoint paths, vertex coloring among others) for which our
subgraph sampling primitive yields fast, small-space dynamic graph stream
algorithms. We then show lower bounds for natural problems outside this family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01733</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01733</id><created>2015-05-07</created><authors><author><keyname>Chandra</keyname><forenames>Kishor</forenames></author><author><keyname>Prasad</keyname><forenames>R. Venkatesha</forenames></author><author><keyname>Quang</keyname><forenames>Bien</forenames></author><author><keyname>Niemegeers</keyname><forenames>I. G. M. M.</forenames></author></authors><title>CogCell: Cognitive Interplay between 60GHz Picocells and 2.4/5GHz
  Hotspots in the 5G Era</title><categories>cs.NI</categories><comments>14 PAGES in IEEE Communications Magazine, Special issue on Emerging
  Applications, Services and Engineering for Cognitive Cellular Systems
  (EASE4CCS), July 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid proliferation of wireless communication devices and the emergence of a
variety of new applications have triggered investigations into next-generation
mobile broadband systems, i.e., 5G. Legacy 2G--4G systems covering large areas
were envisioned to serve both indoor and outdoor environments. However, in the
5G-era, 80\% of overall traffic is expected to be generated in indoors. Hence,
the current approach of macro-cell mobile network, where there is no
differentiation between indoors and outdoors, needs to be reconsidered. We
envision 60\,GHz mmWave picocell architecture to support high-speed indoor and
hotspot communications. We envisage the 5G indoor network as a combination of-,
and interplay between, 2.4/5\,GHz having robust coverage and 60\,GHz links
offering high datarate. This requires an intelligent coordination and
cooperation. We propose 60\,GHz picocellular network architecture, called
CogCell, leveraging the ubiquitous WiFi. We propose to use 60\,GHz for the data
plane and 2.4/5GHz for the control plane. The hybrid network architecture
considers an opportunistic fall-back to 2.4/5\,GHz in case of poor connectivity
in the 60\,GHz domain. Further, to avoid the frequent re-beamforming in 60\,GHz
directional links due to mobility, we propose a cognitive module -- a
sensor-assisted intelligent beam switching procedure -- which reduces the
communication overhead. We believe that the CogCell concept will help future
indoor communications and possibly outdoor hotspots, where mobile stations and
access points collaborate with each other to improve the user experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01740</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01740</id><created>2015-05-07</created><authors><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jose</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Fast Spectral Unmixing based on Dykstra's Alternating Projection</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a fast spectral unmixing algorithm based on Dykstra's
alternating projection. The proposed algorithm formulates the fully constrained
least squares optimization problem associated with the spectral unmixing task
as an unconstrained regression problem followed by a projection onto the
intersection of several closed convex sets. This projection is achieved by
iteratively projecting onto each of the convex sets individually, following
Dyktra's scheme. The sequence thus obtained is guaranteed to converge to the
sought projection. Thanks to the preliminary matrix decomposition and variable
substitution, the projection is implemented intrinsically in a subspace, whose
dimension is very often much lower than the number of bands. A benefit of this
strategy is that the order of the computational complexity for each projection
is decreased from quadratic to linear time. Numerical experiments considering
diverse spectral unmixing scenarios provide evidence that the proposed
algorithm competes with the state-of-the-art, namely when the number of
endmembers is relatively small, a circumstance often observed in real
hyperspectral applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01742</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01742</id><created>2015-05-07</created><authors><author><keyname>Perez</keyname><forenames>J. A. Moreno</forenames></author><author><keyname>Consoli</keyname><forenames>S.</forenames></author></authors><title>On the Minimum Labelling Spanning bi-Connected Subgraph problem</title><categories>cs.DS math.OC</categories><comments>MIC 2015: The XI Metaheuristics International Conference, 3 pages,
  Agadir, June 7-10, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the minimum labelling spanning bi-connected subgraph problem
(MLSBP) replacing connectivity by bi-connectivity in the well known minimum
labelling spanning tree problem (MLSTP). A graph is bi-connected if, for every
two vertices, there are, at least, two vertex-disjoint paths joining them. The
problem consists in finding the spanning bi-connected subgraph or block with
minimum set of labels. We adapt the exact method of the MLSTP to solve the
MLSTB and the basic greedy constructive heuristic, the maximum vertex covering
algorithm (MVCA). This proce- dure is a basic component in the application of
metaheuristics to solve the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01746</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01746</id><created>2015-05-07</created><authors><author><keyname>Du</keyname><forenames>Xu</forenames></author><author><keyname>Tadrous</keyname><forenames>John</forenames></author><author><keyname>Dick</keyname><forenames>Chris</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Multiuser MIMO Beamforming with Full-duplex Open-loop Training</title><categories>cs.IT math.IT</categories><comments>The material in this paper was presented (without proof) in 16th IEEE
  International Workshop on Signal Processing Advances in Wireless
  Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, full-duplex radios are used to continuously update the channel
state information at the transmitter, which is required to compute the downlink
precoding matrix in MIMO broadcast channels. The full-duplex operation allows
leveraging channel reciprocity for open-loop uplink training to estimate the
downlink channels. However, the uplink transmission of training creates
interference at the downlink receiving mobile nodes. We characterize the
optimal training resource allocation and its associated spectral efficiency, in
the proposed open-loop training based full-duplex system. We also evaluate the
performance of the half-duplex counterpart to derive the relative gains of
full-duplex training. Despite the existence of inter-node interference due to
full-duplex, significant spectral efficiency improvement is attained over
half-duplex operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01749</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01749</id><created>2015-05-07</created><updated>2015-09-23</updated><authors><author><keyname>Gidaris</keyname><forenames>Spyros</forenames></author><author><keyname>Komodakis</keyname><forenames>Nikos</forenames></author></authors><title>Object detection via a multi-region &amp; semantic segmentation-aware CNN
  model</title><categories>cs.CV cs.LG cs.NE</categories><comments>Extended technical report -- short version to appear at ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an object detection system that relies on a multi-region deep
convolutional neural network (CNN) that also encodes semantic
segmentation-aware features. The resulting CNN-based representation aims at
capturing a diverse set of discriminative appearance factors and exhibits
localization sensitivity that is essential for accurate object localization. We
exploit the above properties of our recognition module by integrating it on an
iterative localization mechanism that alternates between scoring a box proposal
and refining its location with a deep CNN regression model. Thanks to the
efficient use of our modules, we detect objects with very high localization
accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we
achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published
work by a significant margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01750</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01750</id><created>2015-05-07</created><authors><author><keyname>Shvartzshnaider</keyname><forenames>Yan</forenames></author></authors><title>Immutable Views -- Access control (to your information) for masses</title><categories>cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are a lot of on going efforts in the research community as well as
industry around providing privacy-preserving and secure storage for personal
data. Although, over time it has adopted many tag lines such as Personal
Information Hub [12], personal container [8], DataBox [4], Personal Data Store
(PDS) [3] and many others, these are essentially reincarnations of a simple
idea: provide a secure way and place for users to store their information and
allow them to provision who has access to that information. In this paper, we
would like to discuss a way to facilitate access control mechanism (AC) in the
various &quot;personal cloud&quot; proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01753</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01753</id><created>2015-05-07</created><authors><author><keyname>Suhov</keyname><forenames>Y.</forenames></author><author><keyname>Sekeh</keyname><forenames>S. Yasaei</forenames></author><author><keyname>Stuhl</keyname><forenames>I.</forenames></author></authors><title>Weighted Gaussian entropy and determinant inequalities</title><categories>cs.IT math.IT</categories><msc-class>60A10, 60B05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We produce a series of results extending information-theoretical inequalities
(discussed by Dembo--Cover--Thomas in 1989-1991) to a weighted version of
entropy. The resulting inequalities involve the Gaussian weighted entropy; they
imply a number of new relations for determinants of positive-definite matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01757</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01757</id><created>2015-05-07</created><authors><author><keyname>Taghva</keyname><forenames>Kazem</forenames></author></authors><title>Contextual Analysis for Middle Eastern Languages with Hidden Markov
  Models</title><categories>cs.CL cs.AI</categories><journal-ref>International Journal on Natural Language Computing, vol. 4, No.
  4, August 2015, pp. 1-11</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Displaying a document in Middle Eastern languages requires contextual
analysis due to different presentational forms for each character of the
alphabet. The words of the document will be formed by the joining of the
correct positional glyphs representing corresponding presentational forms of
the characters. A set of rules defines the joining of the glyphs. As usual,
these rules vary from language to language and are subject to interpretation by
the software developers.
  In this paper, we propose a machine learning approach for contextual analysis
based on the first order Hidden Markov Model. We will design and build a model
for the Farsi language to exhibit this technology. The Farsi model achieves 94
\% accuracy with the training based on a short list of 89 Farsi vocabularies
consisting of 2780 Farsi characters.
  The experiment can be easily extended to many languages including Arabic,
Urdu, and Sindhi. Furthermore, the advantage of this approach is that the same
software can be used to perform contextual analysis without coding complex
rules for each specific language. Of particular interest is that the languages
with fewer speakers can have greater representation on the web, since they are
typically ignored by software developers due to lack of financial incentives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01765</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01765</id><created>2015-05-07</created><authors><author><keyname>Wang</keyname><forenames>Teng</forenames></author><author><keyname>Oral</keyname><forenames>Sarp</forenames></author><author><keyname>Pritchard</keyname><forenames>Michael</forenames></author><author><keyname>Vasko</keyname><forenames>Kevin</forenames></author><author><keyname>Yu</keyname><forenames>Weikuan</forenames></author></authors><title>Development of a Burst Buffer System for Data-Intensive Applications</title><categories>cs.DC cs.OS</categories><comments>International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</comments><proxy>Michael Brim</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern parallel filesystems such as Lustre are designed to provide high,
scalable I/O bandwidth in response to growing I/O requirements; however, the
bursty I/O characteristics of many data-intensive scientific applications make
it difficult for back-end parallel filesystems to efficiently handle I/O
requests. A burst buffer system, through which data can be temporarily buffered
via high-performance storage mediums, allows for gradual flushing of data to
back-end filesystems. In this paper, we explore issues surrounding the
development of a burst buffer system for data-intensive scientific
applications. Our initial results demonstrate that utilizing a burst buffer
system on top of the Lustre filesystem shows promise for dealing with the
intense I/O traffic generated by application checkpointing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01786</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01786</id><created>2015-05-07</created><authors><author><keyname>Kalantari</keyname><forenames>Ashkan</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Gao</keyname><forenames>Zhen</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Secrecy Analysis on Network Coding in Bidirectional Multibeam Satellite
  Communications</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Information Forensics and Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding is an efficient means to improve the spectrum efficiency of
satellite communications. However, its resilience to eavesdropping attacks is
not well understood. This paper studies the confidentiality issue in a
bidirectional satellite network consisting of two mobile users who want to
exchange message via a multibeam satellite using the XOR network coding
protocol. We aim to maximize the sum secrecy rate by designing the optimal
beamforming vector along with optimizing the return and forward link time
allocation. The problem is non-convex, and we find its optimal solution using
semidefinite programming together with a 1-D search. For comparison, we also
solve the sum secrecy rate maximization problem for a conventional reference
scheme without using network coding. Simulation results using realistic system
parameters demonstrate that the bidirectional scheme using network coding
provides considerably higher secrecy rate compared to that of the conventional
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01795</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01795</id><created>2015-05-07</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Chiranjoy</forenames></author><author><keyname>Sarkar</keyname><forenames>Bikramjit</forenames></author><author><keyname>Mukherjee</keyname><forenames>Debaprasad</forenames></author></authors><title>Encoding by DNA Relations and Randomization Through Chaotic Sequences
  for Image Encryption</title><categories>cs.CR</categories><comments>15 pages, 3 figures, 4 Tables, Review article; Submitted to journal
  Applied Soft Computing for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers in the field of DNA-based chaotic cryptography have recently
proposed a set of novel and efficient image encryption algorithms. In this
paper, we present a comprehensive summary of those techniques, which are
available in the literature. The discussion given in this paper is grouped into
three main areas. At first, we give a brief sketch of the backbone architecture
and the theoretical foundation of this field, based on which all the algorithms
were proposed. Next, we briefly discuss the set of image encryption algorithms
based on this architecture and categorized them as either encryption or
cryptanalyzing techniques. Finally, we present the different evaluation metrics
used to quantitatively measure the performance of such algorithms. We also
discuss the characteristic differences among these algorithms. We further
highlight the potential advances that are needed to improvise the present
state-of-the-art image encryption technique using DNA computing and chaos
theory. The primary objective of this survey is to provide researchers in the
field of DNA computing and chaos theory based image encryption a comprehensive
summary of the progress achieved so far and to facilitate them to identify a
few challenging future research areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01802</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01802</id><created>2015-05-07</created><authors><author><keyname>Natarajan</keyname><forenames>Nagarajan</forenames></author><author><keyname>Koyejo</keyname><forenames>Oluwasanmi</forenames></author><author><keyname>Ravikumar</keyname><forenames>Pradeep</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>Optimal Decision-Theoretic Classification Using Non-Decomposable
  Performance Metrics</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a general theoretical analysis of expected out-of-sample utility,
also referred to as decision-theoretic classification, for non-decomposable
binary classification metrics such as F-measure and Jaccard coefficient. Our
key result is that the expected out-of-sample utility for many performance
metrics is provably optimized by a classifier which is equivalent to a signed
thresholding of the conditional probability of the positive class. Our analysis
bridges a gap in the literature on binary classification, revealed in light of
recent results for non-decomposable metrics in population utility maximization
style classification. Our results identify checkable properties of a
performance metric which are sufficient to guarantee a probability ranking
principle. We propose consistent estimators for optimal expected out-of-sample
classification. As a consequence of the probability ranking principle,
computational requirements can be reduced from exponential to cubic complexity
in the general case, and further reduced to quadratic complexity in special
cases. We provide empirical results on simulated and benchmark datasets
evaluating the performance of the proposed algorithms for decision-theoretic
classification and comparing them to baseline and state-of-the-art methods in
population utility maximization for non-decomposable metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01809</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01809</id><created>2015-05-07</created><updated>2015-10-14</updated><authors><author><keyname>Devlin</keyname><forenames>Jacob</forenames></author><author><keyname>Cheng</keyname><forenames>Hao</forenames></author><author><keyname>Fang</keyname><forenames>Hao</forenames></author><author><keyname>Gupta</keyname><forenames>Saurabh</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Zweig</keyname><forenames>Geoffrey</forenames></author><author><keyname>Mitchell</keyname><forenames>Margaret</forenames></author></authors><title>Language Models for Image Captioning: The Quirks and What Works</title><categories>cs.CL cs.AI cs.CV cs.LG</categories><comments>See http://research.microsoft.com/en-us/projects/image_captioning for
  project information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two recent approaches have achieved state-of-the-art results in image
captioning. The first uses a pipelined process where a set of candidate words
is generated by a convolutional neural network (CNN) trained on images, and
then a maximum entropy (ME) language model is used to arrange these words into
a coherent sentence. The second uses the penultimate activation layer of the
CNN as input to a recurrent neural network (RNN) that then generates the
caption sequence. In this paper, we compare the merits of these different
language modeling approaches for the first time by using the same
state-of-the-art CNN as input. We examine issues in the different approaches,
including linguistic irregularities, caption repetition, and data set overlap.
By combining key aspects of the ME and RNN methods, we achieve a new record
performance over previously published results on the benchmark COCO dataset.
However, the gains we see in BLEU do not translate to human judgments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01810</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01810</id><created>2015-05-07</created><updated>2015-12-01</updated><authors><author><keyname>Khan</keyname><forenames>Khalid</forenames></author><author><keyname>Lobiyal</keyname><forenames>D. K.</forenames></author></authors><title>Bezier curves based on Lupas (p,q)-analogue of Bernstein polynomials in
  CAGD</title><categories>cs.GR</categories><comments>19 pages, 8 figures, (submitted), Operators and de-Casteljau
  algorithm revised</comments><msc-class>65D17, 41A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use the blending functions of Lupas type (rational)
(p,q)-Bernstein operators based on (p,q)-integers for construction of Lupas
(p,q)-Beezier curves (rational curves) and surfaces (rational surfaces) with
two shape parameters. We study the nature of degree elevation and degree
reduction for Lupas (p,q)-Bezier Bernstein functions. Parametric curves are
represented using Lupas (p,q)-Bernstein basis and the concept of total
positivity is applied to investigate the shape properties of the curve. We get
q-Bezier curve when we set the parameter p to the value 1: We also introduce a
de Casteljau algorithm for Lupas type (p,q)-Bernstein Bezier curves. The new
curves have some properties similar to q-Bezier curves. Moreover, we construct
the corresponding tensor product surfaces over the rectangular domain (u,v) \in
[0,1] \times [0,1] depending on four parameters. We also study the de Casteljau
algorithm and degree evaluation properties of the surfaces for these
generalization over the rectangular domain. Furthermore, some fundamental
properties for Lupas type (p,q)-Bernstein Bezier curves are discussed. We get
q-Bezier curves and surfaces for (u,v) \in [0,1] \times [0,1] when we set the
parameter p1 = p2 = 1. In Comparison to q-Bezier curves and surfaces based on
Phillips q-Bernstein polynomials, our generalizations show more flexibility in
choosing the value of p1; p2 and q1; q2 and superiority in shape control of
curves and surfaces. The shape parameters provide more convenience for the
curve and surface modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01811</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01811</id><created>2015-05-07</created><authors><author><keyname>Aminikashani</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Gu</keyname><forenames>Wenjun</forenames></author><author><keyname>Kavehrad</keyname><forenames>Mohsen</forenames></author></authors><title>Indoor Positioning in High Speed OFDM Visible Light Communications</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Milcom 2015, 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible Light Communication (VLC) technology using light emitting diodes
(LEDs) has been gaining increasing attention in recent years as it is appealing
for a wide range of applications such as indoor positioning. Orthogonal
frequency division multiplexing (OFDM) has been applied to indoor wireless
optical communications in order to mitigate the effect of multipath distortion
of the optical channel as well as increasing data rate. In this paper, we
investigate the indoor positioning accuracy of optical based OFDM techniques
used in VLC systems. A positioning algorithm based on power attenuation is used
to estimate the receiver coordinates. We further calculate the positioning
errors in all the locations of a room and compare them with those of single
carrier modulation scheme, i.e., on-off keying (OOK) modulation. We demonstrate
that OFDM positioning system outperforms its conventional counterpart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01817</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01817</id><created>2015-05-06</created><authors><author><keyname>Cerin&#x161;ek</keyname><forenames>Monika</forenames></author><author><keyname>Batagelj</keyname><forenames>Vladimir</forenames></author></authors><title>Generalized Two-mode Cores</title><categories>cs.SI</categories><comments>21 pages, 2 tables, 4 figures</comments><msc-class>05C69, 91D30, 68R10, 91C20</msc-class><doi>10.1016/j.socnet.2015.04.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The node set of a two-mode network consists of two disjoint subsets and all
its links are linking these two subsets. The links can be weighted. We
developed a new method for identifying important sub-networks in two-mode
networks. The method combines and extends the ideas from generalized cores in
one-mode networks and from (p, q)- cores for two-mode networks. In this paper
we introduce the notion of generalized two-mode cores and discuss some of their
properties. An efficient algorithm to determine generalized two-mode cores and
an analysis of its complexity are also presented. For illustration some results
obtained in analyses of real-life data are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01818</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01818</id><created>2015-05-05</created><updated>2016-01-22</updated><authors><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Bright</keyname><forenames>Jonathan</forenames></author></authors><title>Wikipedia traffic data and electoral prediction: towards theoretically
  informed models</title><categories>cs.SI physics.soc-ph</categories><comments>submitted to EPJ Data Science. Additional File 1 available at
  https://drive.google.com/open?id=0BxaGC-YCTO6SWkJhRXlrMVRYVlE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This aim of this article is to explore the potential use of Wikipedia page
view data for predicting electoral results. Responding to previous critiques of
work using socially generated data to predict elections, which have argued that
these predictions take place without any understanding of the mechanism which
enables them, we first develop a theoretical model which highlights why people
might seek information online at election time, and how this activity might
relate to overall electoral outcomes, focussing especially on how different
types of parties such as new and established parties might generate different
information seeking patterns. We test this model on a novel dataset drawn from
a variety of countries in the 2009 and 2014 European Parliament elections. We
show that while Wikipedia offers little insight into absolute vote outcomes, it
offers a good information about changes in both overall turnout at elections
and in vote share for particular parties. These results are used to enhance
existing theories about the drivers of aggregate patterns in online information
seeking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01820</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01820</id><created>2015-05-07</created><authors><author><keyname>Singh</keyname><forenames>Bikramjit</forenames></author><author><keyname>Koufos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author><author><keyname>Berry</keyname><forenames>Randall</forenames></author></authors><title>Co-primary inter-operator spectrum sharing over a limited spectrum pool
  using repeated games</title><categories>cs.NI</categories><comments>To be published in proceedings of IEEE International Conference on
  Communications (ICC) at London, Jun. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two small cell operators deployed in the same geographical area,
sharing spectrum resources from a common pool. A method is investigated to
coordinate the utilization of the spectrum pool without monetary transactions
and without revealing operator-specific information to other parties. For this,
we construct a protocol based on asking and receiving spectrum usage favors by
the operators, and keeping a book of the favors. A spectrum usage favor is
exchanged between the operators if one is asking for a permission to use some
of the resources from the pool on an exclusive basis, and the other is willing
to accept that. As a result, the proposed method does not force an operator to
take action. An operator with a high load may take spectrum usage favors from
an operator that has few users to serve, and it is likely to return these
favors in the future to show a cooperative spirit and maintain reciprocity. We
formulate the interactions between the operators as a repeated game and
determine rules to decide whether to ask or grant a favor at each stage game.
We illustrate that under frequent network load variations, which are expected
to be prominent in small cell deployments, both operators can attain higher
user rates as compared to the case of no coordination of the resource
utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01823</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01823</id><created>2015-05-07</created><authors><author><keyname>Singh</keyname><forenames>Bikramjit</forenames></author><author><keyname>Hailu</keyname><forenames>Sofonias</forenames></author><author><keyname>Koufos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Dowhuszko</keyname><forenames>Alexis A.</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author><author><keyname>Berry</keyname><forenames>Randall</forenames></author></authors><title>Coordination protocol for inter-operator spectrum sharing in co-primary
  5G small cell networks</title><categories>cs.NI</categories><comments>To be published in IEEE Communications Magazine, Jul. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider spectrum sharing between a limited set of operators having
similar rights for accessing spectrum. A coordination protocol acting on the
level of the Radio Access Network (RAN) is designed. The protocol is
non-cooperative, but assumes an agreement to a set of negotiation rules. The
signaling overhead is low, and knowledge of competitor's channel state
information is not assumed. No monetary transactions are involved; instead,
spectrum sharing is based on a RAN-internal virtual currency. The protocol is
applicable in a scenario of mutual renting and when the operators form a
spectrum pool. The protocol is reactive to variations in interference and load
of the operators, and shows gains in a simulated small cell scenario compared
to not using any coordination protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01825</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01825</id><created>2015-05-07</created><updated>2015-05-08</updated><authors><author><keyname>Ramsey</keyname><forenames>Joseph D.</forenames></author></authors><title>Effects of Nonparanormal Transform on PC and GES Search Accuracies</title><categories>cs.AI</categories><comments>10 pages, 18 tables, tech report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Liu, et al., 2009 developed a transformation of a class of non-Gaussian
univariate distributions into Gaussian distributions. Liu and collaborators
(2012) subsequently applied the transform to search for graphical causal models
for a number of empirical data sets. To our knowledge, there has been no
published investigation by simulation of the conditions under which the
transform aids, or harms, standard graphical model search procedures. We
consider here how the transform affects the performance of two search
algorithms in particular, PC (Spirtes et al., 2000; Meek 1995) and GES (Meek
1997; Chickering 2002). We find that the transform is harmless but ineffective
for most cases but quite effective in very special cases for GES, namely, for
moderate non-Gaussianity and moderate non-linearity. For strong-linearity,
another algorithm, PC-GES (a combination of PC with GES), is equally effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01858</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01858</id><created>2015-05-07</created><authors><author><keyname>Shalmashi</keyname><forenames>Serveh</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Sung</keyname><forenames>Ki Won</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Energy Efficiency and Sum Rate when Massive MIMO meets Device-to-Device
  Communication</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, to be presented at the IEEE International
  Conference on Communications (ICC) Workshop on Device-to-Device Communication
  for Cellular and Wireless Networks, London, UK, June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a scenario of short-range communication, known as
device-to-device (D2D) communication, where D2D users reuse the downlink
resources of a cellular network to transmit directly to their corresponding
receivers. In addition, multiple antennas at the base station (BS) are used in
order to simultaneously support multiple cellular users using multiuser or
massive MIMO. The network model considers a fixed number of cellular users and
that D2D users are distributed according to a homogeneous Poisson point process
(PPP). Two metrics are studied, namely, average sum rate (ASR) and energy
efficiency (EE). We derive tractable expressions and study the tradeoffs
between the ASR and EE as functions of the number of BS antennas and density of
D2D users for a given coverage area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01861</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01861</id><created>2015-05-07</created><updated>2015-06-04</updated><authors><author><keyname>Pan</keyname><forenames>Yingwei</forenames></author><author><keyname>Mei</keyname><forenames>Tao</forenames></author><author><keyname>Yao</keyname><forenames>Ting</forenames></author><author><keyname>Li</keyname><forenames>Houqiang</forenames></author><author><keyname>Rui</keyname><forenames>Yong</forenames></author></authors><title>Jointly Modeling Embedding and Translation to Bridge Video and Language</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatically describing video content with natural language is a fundamental
challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence
dynamics, has attracted increasing attention on visual interpretation. However,
most existing approaches generate a word locally with given previous words and
the visual content, while the relationship between sentence semantics and
visual content is not holistically exploited. As a result, the generated
sentences may be contextually correct but the semantics (e.g., subjects, verbs
or objects) are not true.
  This paper presents a novel unified framework, named Long Short-Term Memory
with visual-semantic Embedding (LSTM-E), which can simultaneously explore the
learning of LSTM and visual-semantic embedding. The former aims to locally
maximize the probability of generating the next word given previous words and
visual content, while the latter is to create a visual-semantic embedding space
for enforcing the relationship between the semantics of the entire sentence and
visual content. Our proposed LSTM-E consists of three components: a 2-D and/or
3-D deep convolutional neural networks for learning powerful video
representation, a deep RNN for generating sentences, and a joint embedding
model for exploring the relationships between visual content and sentence
semantics. The experiments on YouTube2Text dataset show that our proposed
LSTM-E achieves to-date the best reported performance in generating natural
sentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also
demonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO)
triplets to several state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01864</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01864</id><created>2015-05-07</created><authors><author><keyname>Alomari</keyname><forenames>Zakaria</forenames></author></authors><title>Multiplayer Games and their Need for Scalable and Secure State
  Management</title><categories>cs.CY cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, massively multiplayer online games (MMOGs) have become very
popular by providing more entertainment, therefore millions of players now
participate may interact with each other in a shared environment, even though
these players may be separated by huge geographic distances. Peer to Peer (P2P)
architectures become very popular in MMOG recently, due to their distributed
and collaborative nature, have low infrastructure costs, achieve fast response
times by creating direct connections between players and can achieve high
scalability. However, P2P architectures face many challenges and tend to be
vulnerable to cheating. Game distribution between peers makes maintaining
control of the game becomes more complicated. Therefore, broadcasting all state
changes to every player is not a viable solution to maintain a consistent game
state in a MMOGs. To successfully overcome the challenge of scale, MMOGs have
to employ sophisticated interest management techniques that only send relevant
state changes to each player. In this paper, In order to prevent cheaters to
gain unfair advantages in P2P-based MMOGs, several cheat-proof schemes have
been proposed that utilize a range of techniques such as cryptographic
mechanisms, Commitment and agreement protocols, and proxy architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01866</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01866</id><created>2015-05-07</created><authors><author><keyname>Rashmi</keyname><forenames>K. V.</forenames></author><author><keyname>Gilad-Bachrach</keyname><forenames>Ran</forenames></author></authors><title>DART: Dropouts meet Multiple Additive Regression Trees</title><categories>cs.LG stat.ML</categories><comments>AIStats 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Additive Regression Trees (MART), an ensemble model of boosted
regression trees, is known to deliver high prediction accuracy for diverse
tasks, and it is widely used in practice. However, it suffers an issue which we
call over-specialization, wherein trees added at later iterations tend to
impact the prediction of only a few instances, and make negligible contribution
towards the remaining instances. This negatively affects the performance of the
model on unseen data, and also makes the model over-sensitive to the
contributions of the few, initially added tress. We show that the commonly used
tool to address this issue, that of shrinkage, alleviates the problem only to a
certain extent and the fundamental issue of over-specialization still remains.
In this work, we explore a different approach to address the problem that of
employing dropouts, a tool that has been recently proposed in the context of
learning deep neural networks. We propose a novel way of employing dropouts in
MART, resulting in the DART algorithm. We evaluate DART on ranking, regression
and classification tasks, using large scale, publicly available datasets, and
show that DART outperforms MART in each of the tasks, with a significant
margin. We also show that DART overcomes the issue of over-specialization to a
considerable extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01874</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01874</id><created>2015-05-07</created><updated>2015-09-02</updated><authors><author><keyname>Kappen</keyname><forenames>Hilbert Johan</forenames></author><author><keyname>Ruiz</keyname><forenames>Hans Christian</forenames></author></authors><title>Adaptive importance sampling for control and inference</title><categories>cs.SY cs.RO</categories><comments>23 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path integral (PI) control problems are a restricted class of non-linear
control problems that can be solved formally as a Feyman-Kac path integral and
can be estimated using Monte Carlo sampling. In this contribution we review
path integral control theory in the finite horizon case.
  We subsequently focus on the problem how to compute and represent control
solutions. Within the PI theory, the question of how to compute becomes the
question of importance sampling. Efficient importance samplers are state
feedback controllers and the use of these requires an efficient representation.
Learning and representing effective state-feedback controllers for non-linear
stochastic control problems is a very challenging, and largely unsolved,
problem. We show how to learn and represent such controllers using ideas from
the cross entropy method. We derive a gradient descent method that allows to
learn feed-back controllers using an arbitrary parametrisation. We refer to
this method as the Path Integral Cross Entropy method or PICE. We illustrate
this method for some simple examples.
  The path integral control methods can be used to estimate the posterior
distribution in latent state models. In neuroscience these problems arise when
estimating connectivity from neural recording data using EM. We demonstrate the
path integral control method as an accurate alternative to particle filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01878</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01878</id><created>2015-05-07</created><authors><author><keyname>Pinto</keyname><forenames>Pedro</forenames></author><author><keyname>Abreu</keyname><forenames>Rui</forenames></author><author><keyname>Cardoso</keyname><forenames>Jo&#xe3;o M. P.</forenames></author></authors><title>Fault Detection in C Programs using Monitoring of Range Values:
  Preliminary Results</title><categories>cs.SE</categories><comments>53 pages, technical report for work done as part of the AutoSeer
  project</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report presents the work done as part of the AutoSeer project.
Our work in this project was to develop a source-to-source compiler, MANET, for
the C language that could be used for instrumentation of critical parts of
applications under testing. The intention was to guide the compilation flow and
define instrumentation strategies using the Aspect-Oriented Approach provided
by LARA. This allows a separation of the original target application and the
instrumentation secondary concerns.
  One of the goals of this work was the development of a source-to-source C
compiler that modifies code according to an input strategy. These modifications
could provide code transformations that target performance and instrumentation
for debugging, but in this work they are used to inject code that collects
information about the values that certain variables take during runtime. This
compiler is supported by an AOP approach that enables the definition of
instrumentation strategies. We decided to extend an existing source-to-source
compiler, Cetus, and couple it with LARA, an AOP language that is partially
abstracted from the target programming language.
  We propose and evaluate an approach to detect faults in C programs by
monitoring the range values of variables. We consider various monitoring
strategies and use two real-life applications, the GZIP file compressor and
ABS, a program provided by an industrial partner. The different strategies were
specified in LARA and automatically applied using MANET. The experimental
results show that our approach has potential but is hindered by not accounting
for values in arrays and control variables. We achieve prediction accuracies of
around 54% for ABS and 83% for GZIP, when comparing our approach to a more
traditional one, where the outputs are compared to an expected result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01881</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01881</id><created>2015-05-07</created><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Baldick</keyname><forenames>Ross</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Data Attacks on Power Grids: Leveraging Detection</title><categories>cs.CR cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data attacks on meter measurements in the power grid can lead to errors in
state estimation. This paper presents a new data attack model where an
adversary produces changes in state estimation despite failing bad-data
detection checks. The adversary achieves its objective by making the estimator
incorrectly identify correct measurements as bad data. The proposed attack
regime's significance lies in reducing the minimum sizes of successful attacks
to more than half of that of undetectable data attacks. Additionally, the
attack model is able to construct attacks on systems that are resilient to
undetectable attacks. The conditions governing a successful data attack of the
proposed model are presented along with guarantees on its performance. The
complexity of constructing an optimal attack is discussed and two polynomial
time approximate algorithms for attack vector construction are developed. The
performance of the proposed algorithms and efficacy of the hidden attack model
are demonstrated through simulations on IEEE test systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01887</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01887</id><created>2015-05-07</created><authors><author><keyname>Whitley</keyname><forenames>Darrell</forenames></author><author><keyname>Tin&#xf3;s</keyname><forenames>Renato</forenames></author><author><keyname>Chicano</keyname><forenames>Francisco</forenames></author></authors><title>Optimal Neuron Selection: NK Echo State Networks for Reinforcement
  Learning</title><categories>cs.NE</categories><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the NK Echo State Network. The problem of learning in
the NK Echo State Network is reduced to the problem of optimizing a special
form of a Spin Glass Problem known as an NK Landscape. No weight adjustment is
used; all learning is accomplished by spinning up (turning on) or spinning down
(turning off) neurons in order to find a combination of neurons that work
together to achieve the desired computation. For special types of NK
Landscapes, an exact global solution can be obtained in polynomial time using
dynamic programming. The NK Echo State Network is applied to a reinforcement
learning problem requiring a recurrent network: balancing two poles on a cart
given no velocity information. Empirical results shows that the NK Echo State
Network learns very rapidly and yields very good generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01888</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01888</id><created>2015-05-07</created><authors><author><keyname>Herman</keyname><forenames>M. W.</forenames></author><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author></authors><title>A Monte Carlo Study of Pairwise Comparisons</title><categories>cs.OH</categories><journal-ref>Information Processing Letters 57 (1996) 25-29</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consistent approximations obtained by geometric means ($GM$) and the
principal eigenvector ($EV$), turned out to be close enough for 1,000,000
not-so-inconsistent pairwise comparisons matrices. In this respect both methods
are accurate enough for most practical applications. As the enclosed Table 1
demonstrates, the biggest difference between average deviations of $GM$ and
$EV$ solutions is 0.00019 for the Euclidean metric and 0.00355 for the
Tchebychev metric.
  For practical applications, this precision is far better than expected. After
all we are talking, in most cases, about relative subjective comparisons and
one tenth of a percent is usually below our threshold of perception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01902</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01902</id><created>2015-05-07</created><authors><author><keyname>Bozoki</keyname><forenames>S.</forenames></author><author><keyname>Fulop</keyname><forenames>J.</forenames></author><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author></authors><title>An LP-based inconsistency monitoring of pairwise comparison matrices</title><categories>cs.OH</categories><msc-class>68W30</msc-class><journal-ref>Mathematical and Computer Modelling, v.54(1-2),789-793, 2011</journal-ref><doi>10.1016/j.mcm.2011.03.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distance-based inconsistency indicator, defined by the third author for the
consistency-driven pairwise comparisons method, is extended to the incomplete
case. The corresponding optimization problem is transformed into an equivalent
linear programming problem. The results can be applied in the process of
filling in the matrix as the decision maker gets automatic feedback. As soon as
a serious error occurs among the matrix elements, even due to a misprint, a
significant increase in the inconsistency index is reported. The high
inconsistency may be alarmed not only at the end of the process of filling in
the matrix but also during the completion process. Numerical examples are also
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01903</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01903</id><created>2015-05-07</created><authors><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author><author><keyname>Orlowski</keyname><forenames>M.</forenames></author></authors><title>An Algorithm for the Optimal Consistent Approximation to a Pairwise
  Comparisons Matrix by Orthogonal Projections</title><categories>cs.OH</categories><journal-ref>Computers &amp; Mathematics with Applications,34(10): 41-47, 1997</journal-ref><doi>10.1016/S0898-1221(97)00205-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The algorithm for finding the optimal consistent approximation of an
inconsistent pairwise comparisons matrix is based on a logarithmic
transformation of a pairwise comparisons matrix into a vector space with the
Euclidean metric. Orthogonal basis is introduced in the vector space. The
orthogonal projection of the transformed matrix onto the space formed by the
images of consistent matrices is the required consistent approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01911</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01911</id><created>2015-05-07</created><authors><author><keyname>Zhu</keyname><forenames>Xuanmin</forenames></author><author><keyname>Zhang</keyname><forenames>Yu-Xiang</forenames></author></authors><title>The amplification of weak measurements under quantum noise</title><categories>quant-ph cs.IT math.IT</categories><comments>10 pages,6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The influence of outside quantum noises on the amplification of weak
measurements is investigated. Three typical quantum noises are discussed. The
maximum values of the pointer's shifts decrease sharply with the strength of
the depolarizing channel and phase damping. In order to obtain significant
amplified signals, the preselection quantum systems must be kept away from the
two quantum noises. Interestingly, the amplification effect is immune to the
amplitude damping noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01918</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01918</id><created>2015-05-07</created><updated>2015-09-24</updated><authors><author><keyname>Cowan</keyname><forenames>Wesley</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>An Asymptotically Optimal Policy for Uniform Bandits of Unknown Support</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1504.05823</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of a controller sampling sequentially from a finite
number of $N \geq 2$ populations, specified by random variables $X^i_k$, $ i =
1,\ldots , N,$ and $k = 1, 2, \ldots$; where $X^i_k$ denotes the outcome from
population $i$ the $k^{th}$ time it is sampled. It is assumed that for each
fixed $i$, $\{ X^i_k \}_{k \geq 1}$ is a sequence of i.i.d. uniform random
variables over some interval $[a_i, b_i]$, with the support (i.e., $a_i, b_i$)
unknown to the controller. The objective is to have a policy $\pi$ for
deciding, based on available data, from which of the $N$ populations to sample
from at any time $n=1,2,\ldots$ so as to maximize the expected sum of outcomes
of $n$ samples or equivalently to minimize the regret due to lack on
information of the parameters $\{ a_i \}$ and $\{ b_i \}$. In this paper, we
present a simple inflated sample mean (ISM) type policy that is asymptotically
optimal in the sense of its regret achieving the asymptotic lower bound of
Burnetas and Katehakis (1996). Additionally, finite horizon regret bounds are
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01919</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01919</id><created>2015-05-07</created><updated>2015-05-12</updated><authors><author><keyname>Gupta</keyname><forenames>Puja</forenames></author></authors><title>Characterization of Performance Anomalies in Hadoop</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the huge variety of data and equally large-scale systems, there is not a
unique execution setting for these systems which can guarantee the best
performance for each query. In this project, we tried so study the impact of
different execution settings on execution time of workloads by varying them one
at a time. Using the data from these experiments, a decision tree was built
where each internal node represents the execution parameter, each branch
represents value chosen for the parameter and each leaf node represents a range
for execution time in minutes. The attribute in the decision tree to split the
dataset on is selected based on the maximum information gain or lowest entropy.
Once the tree is trained with the training samples, this tree can be used to
get approximate range for the expected execution time. When the actual
execution time differs from this expected value, a performance anomaly can be
detected. For a test dataset with 400 samples, 99% of samples had actual
execution time in the range predicted time by the decision tree. Also on
analyzing the constructed tree, an idea about what configuration can give
better performance for a given workload can be obtained. Initial experiments
suggest that the impact an execution parameter can have on the target attribute
(here execution time) can be related to the distance of that feature node from
the root of the constructed decision tree. From initial results the percent
change in the values of the target attribute for various value of the feature
node which is closer to the root is 6 times larger than when that same iii
feature node is away from the root node. This observation will depend on how
well the decision tree was trained and may not be true for every case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01920</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01920</id><created>2015-05-07</created><authors><author><keyname>Ding</keyname><forenames>Ming</forenames></author><author><keyname>Lopez-Perez</keyname><forenames>David</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author></authors><title>Will the Area Spectral Efficiency Monotonically Grow as Small Cells Go
  Dense?</title><categories>cs.NI cs.IT math.IT</categories><comments>Submitted to Globecom 2015. arXiv admin note: substantial text
  overlap with arXiv:1503.04251</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a sophisticated path loss model into the
stochastic geometry analysis incorporating both line-of-sight (LoS) and
non-line-of-sight (NLoS) transmissions to study their performance impact in
small cell networks (SCNs). Analytical results are obtained on the coverage
probability and the area spectral efficiency (ASE) assuming both a general path
loss model and a special case of path loss model recommended by the 3rd
Generation Partnership Project (3GPP) standards. The performance impact of LoS
and NLoS transmissions in SCNs in terms of the coverage probability and the ASE
is shown to be significant both quantitatively and qualitatively, compared with
previous work that does not differentiate LoS and NLoS transmissions.
Particularly, our analysis demonstrates that when the density of small cells is
larger than a threshold, the network coverage probability will decrease as
small cells become denser, which in turn makes the ASE suffer from a slow
growth or even a notable decrease. For practical regime of small cell density,
the performance results derived from our analysis are distinctively different
from previous results, and shed new insights on the design and deployment of
future dense/ultra-dense SCNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01924</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01924</id><created>2015-05-08</created><updated>2015-08-17</updated><authors><author><keyname>Ding</keyname><forenames>Ming</forenames></author><author><keyname>Perez</keyname><forenames>David Lopez</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author></authors><title>Approximation of Uplink Inter-Cell Interference in FDMA Small Cell
  Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>to appear in Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, for the first time, we analytically prove that the uplink (UL)
inter-cell interference in frequency division multiple access (FDMA) small cell
networks (SCNs) can be well approximated by a lognormal distribution under a
certain condition. The lognormal approximation is vital because it allows
tractable network performance analysis with closed-form expressions. The
derived condition, under which the lognormal approximation applies, does not
pose particular requirements on the shapes/sizes of user equipment (UE)
distribution areas as in previous works. Instead, our results show that if a
path loss related random variable (RV) associated with the UE distribution
area, has a low ratio of the 3rd absolute moment to the variance, the lognormal
approximation will hold. Analytical and simulation results show that the
derived condition can be readily satisfied in future dense/ultra-dense SCNs,
indicating that our conclusions are very useful for network performance
analysis of the 5th generation (5G) systems with more general cell deployment
beyond the widely used Poisson deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01927</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01927</id><created>2015-05-08</created><authors><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>A simpler sublinear algorithm for approximating the triangle count</title><categories>cs.DS cs.DM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent result of Eden, Levi, and Ron (ECCC 2015) provides a sublinear time
algorithm to estimate the number of triangles in a graph. Given an undirected
graph $G$, one can query the degree of a vertex, the existence of an edge
between vertices, and the $i$th neighbor of a vertex. Suppose the graph has $n$
vertices, $m$ edges, and $t$ triangles. In this model, Eden et al provided a
$O(\poly(\eps^{-1}\log n)(n/t^{1/3} + m^{3/2}/t))$ time algorithm to get a
$(1+\eps)$-multiplicative approximation for $t$, the triangle count. This paper
provides a simpler algorithm with the same running time (up to differences in
the $\poly(\eps^{-1}\log n)$ factor) that has a substantially simpler analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01933</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01933</id><created>2015-05-08</created><authors><author><keyname>Wang</keyname><forenames>Hui</forenames></author><author><keyname>Chan</keyname><forenames>Mun Choon</forenames></author><author><keyname>Ooi</keyname><forenames>Wei Tsang</forenames></author></authors><title>Wireless Multicast for Zoomable Video Streaming</title><categories>cs.NI cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zoomable video streaming refers to a new class of interactive video
applications, where users can zoom into a video stream to view a selected
region of interest in higher resolutions and pan around to move the region of
interest. The zoom and pan effects are typically achieved by breaking the
source video into a grid of independently decodable tiles. Streaming the tiles
to a set of heterogeneous users using broadcast is challenging, as users have
different link rates and different regions of interest at different resolution
levels. In this paper, we consider the following problem: given the subset of
tiles that each user requested, the link rate of each user, and the available
time slots, at which resolution should each tile be sent, to maximize the
overall video quality received by all users. We design an efficient algorithm
to solve the problem above, and evaluate the solution on a testbed using 10
mobile devices. Our method is able to achieve up to 12dB improvements over
other heuristic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01935</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01935</id><created>2015-05-08</created><authors><author><keyname>Anjum</keyname><forenames>Muhammad Ali Raza</forenames></author></authors><title>Adaptive System Identification using Markov Chain Monte Carlo</title><categories>cs.SY</categories><journal-ref>Anjum, Muhammad Ali Raza. &quot;Adaptive System Identification using
  Markov Chain Monte Carlo.&quot; TELKOMNIKA Indonesian Journal of Electrical
  Engineering 13, no. 1 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major problems in adaptive filtering is the problem of system
identification. It has been studied extensively due to its immense practical
importance in a variety of fields. The underlying goal is to identify the
impulse response of an unknown system. This is accomplished by placing a known
system in parallel and feeding both systems with the same input. Due to initial
disparity in their impulse responses, an error is generated between their
outputs. This error is set to tune the impulse response of known system in a
way that every change in impulse response reduces the magnitude of prospective
error. This process is repeated until the error becomes negligible and the
responses of both systems match. To specifically minimize the error, numerous
adaptive algorithms are available. They are noteworthy either for their low
computational complexity or high convergence speed. Recently, a method, known
as Markov Chain Monte Carlo (MCMC), has gained much attention due to its
remarkably low computational complexity. But despite this colossal advantage,
properties of MCMC method have not been investigated for adaptive system
identification problem. This article bridges this gap by providing a complete
treatment of MCMC method in the aforementioned context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01936</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01936</id><created>2015-05-08</created><authors><author><keyname>Chatterjee</keyname><forenames>Avishek</forenames></author><author><keyname>Govindu</keyname><forenames>Venu Madhav</forenames></author></authors><title>Noise in Structured-Light Stereo Depth Cameras: Modeling and its
  Applications</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Depth maps obtained from commercially available structured-light stereo based
depth cameras, such as the Kinect, are easy to use but are affected by
significant amounts of noise. This paper is devoted to a study of the intrinsic
noise characteristics of such depth maps, i.e. the standard deviation of noise
in estimated depth varies quadratically with the distance of the object from
the depth camera. We validate this theoretical model against empirical
observations and demonstrate the utility of this noise model in three popular
applications: depth map denoising, volumetric scan merging for 3D modeling, and
identification of 3D planes in depth maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01941</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01941</id><created>2015-05-08</created><updated>2015-05-12</updated><authors><author><keyname>Kibirige</keyname><forenames>George W.</forenames></author><author><keyname>Sanga</keyname><forenames>Camilius</forenames></author></authors><title>A Survey on Detection of Sinkhole Attack in Wireless Sensor Network</title><categories>cs.CR cs.NI</categories><comments>9 pages, 2 figures and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Network (WSN) consists of large number of low-cost,
resource-constrained sensor nodes. The constraints of the wireless sensor node
is their characteristics which include low memory, low computation power, they
are deployed in hostile area and left unattended, small range of communication
capability and low energy capabilities. Base on those characteristics makes
this network vulnerable to several attacks, such as sinkhole attack. Sinkhole
attack is a type of attack were compromised node tries to attract network
traffic by advertise its fake routing update. One of the impacts of sinkhole
attack is that, it can be used to launch other attacks like selective
forwarding attack, acknowledge spoofing attack and drops or altered routing
information. It can also used to send bogus information to base station. This
paper is focus on exploring and analyzing the existing solutions which used to
detect and identify sinkhole attack in wireless sensor network. The analysis is
based on advantages and limitation of the proposed solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01944</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01944</id><created>2015-05-08</created><authors><author><keyname>Xu</keyname><forenames>Shengkai</forenames></author><author><keyname>Xu</keyname><forenames>Dazhuan</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaofei</forenames></author><author><keyname>Shao</keyname><forenames>Hanqin</forenames></author></authors><title>Optimization Design and Analysis of Systematic LT codes over AWGN
  Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study systematic Luby Transform (SLT) codes over additive
white Gaussian noise (AWGN) channel. We introduce the encoding scheme of SLT
codes and give the bipartite graph for iterative belief propagation (BP)
decoding algorithm. Similar to low-density parity-check codes, Gaussian
approximation (GA) is applied to yield asymptotic performance of SLT codes.
Recent work about SLT codes has been focused on providing better encoding and
decoding algorithms and design of degree distributions. In our work, we propose
a novel linear programming method to optimize the degree distribution.
Simulation results show that the proposed distributions can provide better
bit-error-ratio (BER) performance. Moreover, we analyze the lower bound of SLT
codes and offer closed form expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01946</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01946</id><created>2015-05-08</created><authors><author><keyname>Alodeh</keyname><forenames>Maha</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Energy Efficient Symbol-Level Precoding in Multiuser MISO Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, to appear in SPAWC 2015. arXiv admin note:
  substantial text overlap with arXiv:1504.06749, arXiv:1408.4700</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the idea of exploiting interference among the
simultaneous multiuser transmissions in the downlink of multiple antennas
systems. Using symbol level precoding, a new approach towards addressing the
multiuser interference is discussed through jointly utilizing the channel state
information (CSI) and data information (DI). In this direction, the
interference among the data streams is transformed under certain conditions to
useful signal that can improve the signal to interference noise ratio (SINR) of
the downlink transmissions. In this context, new constructive interference
precoding techniques that tackle the transmit power minimization (min power)
with individual SINR constraints at each user's receivers are proposed.
Furthermore, we investigate the CI precoding design under the assumption that
the received MPSK symbol can reside in a relaxed region in order to be
correctly detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01950</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01950</id><created>2015-05-08</created><authors><author><keyname>Alodeh</keyname><forenames>Maha</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Symbol Based Precoding in The Downlink of Cognitive MISO Channels</title><categories>cs.IT math.IT</categories><comments>CROWNCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes symbol level precoding in the downlink of a MISO
cognitive system. The new scheme tries to jointly utilize the data and channel
information to design a precoding that minimizes the transmit power at a
cognitive base station (CBS); without violating the interference temperature
constraint imposed by the primary system. In this framework, the data
information is handled at symbol level which enables the characterization the
intra-user interference among the cognitive users as an additional source of
useful energy that should be exploited. A relation between the constructive
multiuser transmissions and physical-layer multicast system is established.
Extensive simulations are performed to validate the proposed technique and
compare it with conventional techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01953</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01953</id><created>2015-05-08</created><authors><author><keyname>Reyes</keyname><forenames>Juan Carlos De Los</forenames></author><author><keyname>Sch&#xf6;nlieb</keyname><forenames>Carola-Bibiane</forenames></author><author><keyname>Valkonen</keyname><forenames>Tuomo</forenames></author></authors><title>The structure of optimal parameters for image restoration problems</title><categories>math.OC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the qualitative properties of optimal regularisation parameters in
variational models for image restoration. The parameters are solutions of
bilevel optimisation problems with the image restoration problem as constraint.
A general type of regulariser is considered, which encompasses total variation
(TV), total generalized variation (TGV) and infimal-convolution total variation
(ICTV). We prove that under certain conditions on the given data optimal
parameters derived by bilevel optimisation problems exist. A crucial point in
the existence proof turns out to be the boundedness of the optimal parameters
away from $0$ which we prove in this paper. The analysis is done on the
original -- in image restoration typically non-smooth variational problem -- as
well as on a smoothed approximation set in Hilbert space which is the one
considered in numerical computations. For the smoothed bilevel problem we also
prove that it $\Gamma$ converges to the original problem as the smoothing
vanishes. All analysis is done in function spaces rather than on the
discretised learning problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01958</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01958</id><created>2015-05-08</created><authors><author><keyname>Wan</keyname><forenames>Yiming</forenames></author><author><keyname>Keviczky</keyname><forenames>Tamas</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author></authors><title>Direct identification of fault estimation filter for sensor faults</title><categories>cs.SY</categories><comments>Extended version of the paper accepted by IFAC Safeprocess2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a systematic method to directly identify a sensor fault estimation
filter from plant input/output data collected under fault-free condition. This
problem is challenging, especially when omitting the step of building an
explicit state-space plant model in data-driven design, because the inverse of
the underlying plant dynamics is required and needs to be stable. We show that
it is possible to address this problem by relying on a system-inversion-based
fault estimation filter that is parameterized using identified Markov
parameters. Our novel data-driven approach improves estimation performance by
avoiding the propagation of model reduction errors originating from
identification of the state-space plant model into the designed filter.
Furthermore, it allows additional design freedom to stabilize the obtained
filter under the same stabilizability condition as the existing model-based
system inversion. This crucial property enables its application to sensor
faults in unstable plants, where existing data-driven filter designs could not
be applied so far due to the lack of such stability guarantees (even after
stabilizing the closed-loop system). A numerical simulation example of sensor
faults in an unstable aircraft system illustrates the effectiveness of the
proposed new method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01962</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01962</id><created>2015-05-08</created><updated>2015-10-16</updated><authors><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Nebel</keyname><forenames>Markus</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>Applying Sorting Networks to Synthesize Optimized Sorting Libraries</title><categories>cs.DS cs.MS</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows an application of the theory of sorting networks to
facilitate the synthesis of optimized general purpose sorting libraries.
Standard sorting libraries are often based on combinations of the classic
Quicksort algorithm with insertion sort applied as the base case for small
fixed numbers of inputs. Unrolling the code for the base case by ignoring loop
conditions eliminates branching and results in code which is equivalent to a
sorting network. This enables the application of further program
transformations based on sorting network optimizations, and eventually the
synthesis of code from sorting networks. We show that if considering the number
of comparisons and swaps then theory predicts no real advantage of this
approach. However, significant speed-ups are obtained when taking advantage of
instruction level parallelism and non-branching conditional assignment
instructions, both of which are common in modern CPU architectures. We provide
empirical evidence that using code synthesized from efficient sorting networks
as the base case for Quicksort libraries results in significant real-world
speed-ups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01964</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01964</id><created>2015-05-08</created><updated>2015-07-14</updated><authors><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames></author></authors><title>A Team Based Variant of CTL</title><categories>cs.LO cs.CC</categories><comments>TIME 2015 conference version, modified title and motiviation</comments><msc-class>03B44</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce two variants of computation tree logic CTL based on team
semantics: an asynchronous one and a synchronous one. For both variants we
investigate the computational complexity of the satisfiability as well as the
model checking problem. The satisfiability problem is shown to be
EXPTIME-complete. Here it does not matter which of the two semantics are
considered. For model checking we prove a PSPACE-completeness for the
synchronous case, and show P-completeness for the asynchronous case.
Furthermore we prove several interesting fundamental properties of both
semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01980</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01980</id><created>2015-05-08</created><authors><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Evolving Boolean Networks with RNA Editing</title><categories>cs.NE q-bio.MN q-bio.PE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.4793,
  arXiv:1303.7220</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The editing of transcribed RNA by other molecules such that the form of the
final product differs from that specified in the corresponding DNA sequence is
ubiquitous. This paper uses an abstract, tunable Boolean genetic regulatory
network model to explore aspects of RNA editing. In particular, it is shown how
dynamically altering expressed sequences via a guide RNA-inspired mechanism can
be selected for by simulated evolution under various single and multicellular
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01986</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01986</id><created>2015-05-08</created><updated>2016-02-05</updated><authors><author><keyname>Huang</keyname><forenames>Kun</forenames></author><author><keyname>Parampalli</keyname><forenames>Udaya</forenames></author><author><keyname>Xian</keyname><forenames>Ming</forenames></author></authors><title>On Secrecy Capacity of Minimum Storage Regenerating Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the problem of characterizing the secrecy capacity
of minimum storage regenerating (MSR) codes under the passive {l1,
l2}-eavesdropper model, where the eavesdropper has access to data stored on l1
nodes as well as the repair data for an additional l2 nodes. We study it from
the information-theoretic perspective. First, some general properties of MSR
codes and a simple expression of upper bound on secrecy capacity are given.
Second, a new concept of stable MSR codes is introduced, where the stable
property is shown to be closely linked with secrecy capacity. Finally, a
comprehensive and explicit result on secrecy capacity in the linear MSR
scenario is present, which generalizes all related works in the literature and
also predicts the corresponding result for some unexplored linear MSR codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01988</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01988</id><created>2015-05-08</created><authors><author><keyname>G.</keyname><forenames>David Gonz&#xe1;lez</forenames></author><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Jyri</forenames></author></authors><title>Looking at Cellular Networks Through Canonical Domains and Conformal
  Mapping</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to cope with the rapidly increasing service demand in cellular
networks, more cells are needed with better resource usage efficiency. This
poses challenges for the network planning since service demand in practical
networks is not geographically uniform and, to cope with the non-uniform
service demand, network deployments are becoming increasingly irregular. This
paper introduces a new idea to deal with the non-uniform network topology.
Rather than capturing the network character (e.g. load distribution) by means
of stochastic methods, the proposed novel approach aims at transforming the
analysis from the physical (irregular) domain to a canonical/dual (uniform)
domain that simplifies the work due to its symmetry. To carry out this task,
physical and canonical domains are connected using the conformal
(Schwarz-Christoffel) mapping, that makes the rich and mature theory of Complex
Analysis available. The main contribution of this paper is to introduce and
validate the usability of conformal mapping in the load coupling analysis of
cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01993</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01993</id><created>2015-05-08</created><authors><author><keyname>Kasparian</keyname><forenames>Azniv</forenames></author><author><keyname>Marinov</keyname><forenames>Ivan</forenames></author></authors><title>Duursma's reduced polynomial</title><categories>cs.IT math.AG math.IT</categories><msc-class>Primary: 94B27, 14G50, Secondary: 11 T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The weight distribution of a linear code C is put in an explicit bijective
correspondence with Duursma's reduced polynomial of C. We prove that the
Riemann Hypothesis Analogue for a linear code C requires the formal
self-duality of C and imposes an upper bound on the cardinality q of the basic
field, depending on the dimension and the minimum distance of C.
  Duursma's reduced polynomial of the function field of a curve X of genus g
over the field with q elements is shown to provide a generating function for
the numbers of the effective divisors of non-negative degree degree of a
virtual function field of a curve of genus g-1 over the same finite field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.01998</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.01998</id><created>2015-05-08</created><authors><author><keyname>Andrzejewski</keyname><forenames>Witold</forenames></author><author><keyname>Gramacki</keyname><forenames>Artur</forenames></author><author><keyname>Gramacki</keyname><forenames>Jaros&#x142;aw</forenames></author></authors><title>Density Estimations for Approximate Query Processing on SIMD
  Architectures</title><categories>cs.DC cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate query processing (AQP) is an interesting alternative for exact
query processing. It is a tool for dealing with the huge data volumes where
response time is more important than perfect accuracy (this is typically the
case during initial phase of data exploration). There are many techniques for
AQP, one of them is based on probability density functions (PDF). PDFs are
typically calculated using nonparametric data-driven methods. One of the most
popular nonparametric method is the kernel density estimator (KDE). However, a
very serious drawback of using KDEs is the large number of calculations
required to compute them. The shape of final density function is very sensitive
to an entity called bandwidth or smoothing parameter. Calculating it's optimal
value is not a trivial task and in general is very time consuming. In this
paper we investigate the possibility of utilizing two SIMD architectures: SSE
CPU extensions and NVIDIA's CUDA architecture to accelerate finding of the
bandwidth. Our experiments show orders of magnitude improvements over a simple
sequential implementation of classical algorithms used for that task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02000</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02000</id><created>2015-05-08</created><authors><author><keyname>Lai</keyname><forenames>Matthew</forenames></author></authors><title>Deep Learning for Medical Image Segmentation</title><categories>cs.LG cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report provides an overview of the current state of the art deep
learning architectures and optimisation techniques, and uses the ADNI
hippocampus MRI dataset as an example to compare the effectiveness and
efficiency of different convolutional architectures on the task of patch-based
3-dimensional hippocampal segmentation, which is important in the diagnosis of
Alzheimer's Disease. We found that a slightly unconventional &quot;stacked 2D&quot;
approach provides much better classification performance than simple 2D patches
without requiring significantly more computational power. We also examined the
popular &quot;tri-planar&quot; approach used in some recently published studies, and
found that it provides much better results than the 2D approaches, but also
with a moderate increase in computational power requirement. Finally, we
evaluated a full 3D convolutional architecture, and found that it provides
marginally better results than the tri-planar approach, but at the cost of a
very significant increase in computational power requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02002</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02002</id><created>2015-05-08</created><authors><author><keyname>Dafonte-Gomez</keyname><forenames>Alberto</forenames></author></authors><title>The Key Elements of Viral Advertising. From Motivation to Emotion in the
  Most Shared Videos</title><categories>cs.SI</categories><acm-class>H.1.2; J.4; J.5</acm-class><journal-ref>Comunicar Journal 43: Media Prosumers (Vol. 22 - 2014)</journal-ref><doi>10.3916/C43-2014-20</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  From its origins in the mid 90s, the application of the concept of virality
to commercial communication has represented an opportunity for brands to cross
the traditional barriers of the audience concerning advertising and turn it
into active communicator of brand messages. Viral marketing is based, since
then, on two basic principles: offer free and engaging content that mask its
commercial purpose to the individual and using a peer-to-peer dissemination
system. The transformation of the passive spectator into an active user who
broadcasts advertising messages promoted by sponsors, responds to needs and
motivations of individuals and content features which have been described by
previous research in this field, mainly through quantitative methods based on
user perceptions. This paper focusses on those elements detected in its
previous research as promoters of the sharing action in the 25 most shared
viral video ads between 2006 and 2013 using content analysis. The results
obtained show the most common features in these videos and the prominent
presence of surprise and joy as dominant emotions in the most successful viral
videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02008</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02008</id><created>2015-05-08</created><authors><author><keyname>Klos</keyname><forenames>Michal</forenames></author><author><keyname>Wawrzyniak</keyname><forenames>Karol</forenames></author><author><keyname>Jakubek</keyname><forenames>Marcin</forenames></author></authors><title>Decomposition of Power Flow Used for Optimizing Zonal Configurations of
  Energy Market</title><categories>cs.CE cs.CY</categories><comments>5 pages, 2 figures, IEEE European Energy Markets 2015 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zonal configuration of energy market is often a consequence of political
borders. However there are a few methods developed to help with zonal
delimitation in respect to some measures. This paper presents the approach
aiming at reduction of the loop flow effect - an element of unscheduled flows
which introduces a loss of market efficiency. In order to undertake zonal
partitioning, a detailed decomposition of power flow is performed. Next, we
identify the zone which is a source of the problem and enhance delimitation by
dividing it into two zones. The procedure is illustrated by a study of simple
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02019</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02019</id><created>2015-05-08</created><updated>2015-07-09</updated><authors><author><keyname>Bury</keyname><forenames>Marc</forenames></author><author><keyname>Schwiegelshohn</keyname><forenames>Chris</forenames></author></authors><title>Sublinear Estimation of Weighted Matchings in Dynamic Data Streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm for estimating the weight of a maximum
weighted matching by augmenting any estimation routine for the size of an
unweighted matching. The algorithm is implementable in any streaming model
including dynamic graph streams. We also give the first constant estimation for
the maximum matching size in a dynamic graph stream for planar graphs (or any
graph with bounded arboricity) using $\tilde{O}(n^{4/5})$ space which also
extends to weighted matching. Using previous results by Kapralov, Khanna, and
Sudan (2014) we obtain a $\mathrm{polylog}(n)$ approximation for general graphs
using $\mathrm{polylog}(n)$ space in random order streams, respectively. In
addition, we give a space lower bound of $\Omega(n^{1-\varepsilon})$ for any
randomized algorithm estimating the size of a maximum matching up to a
$1+O(\varepsilon)$ factor for adversarial streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02020</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02020</id><created>2015-05-08</created><updated>2015-11-24</updated><authors><author><keyname>Mellor</keyname><forenames>Andrew</forenames></author><author><keyname>Mobilia</keyname><forenames>Mauro</forenames></author><author><keyname>Redner</keyname><forenames>Sidney</forenames></author><author><keyname>Rucklidge</keyname><forenames>Alastair M.</forenames></author><author><keyname>Ward</keyname><forenames>Jonathan A.</forenames></author></authors><title>Influence of Luddism on innovation diffusion</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO</categories><comments>11 pages, 7 figures</comments><journal-ref>Phys. Rev. E 92, 012806 (2015)</journal-ref><doi>10.1103/PhysRevE.92.012806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the classical Bass model of innovation diffusion to include a
new class of agents --- Luddites --- that oppose the spread of innovation. Our
model also incorporates ignorants, susceptibles, and adopters. When an ignorant
and a susceptible meet, the former is converted to a susceptible at a given
rate, while a susceptible spontaneously adopts the innovation at a constant
rate. In response to the \emph{rate} of adoption, an ignorant may become a
Luddite and permanently reject the innovation. Instead of reaching complete
adoption, the final state generally consists of a population of Luddites,
ignorants, and adopters. The evolution of this system is investigated
analytically and by stochastic simulations. We determine the stationary
distribution of adopters, the time needed to reach the final state, and the
influence of the network topology on the innovation spread. Our model exhibits
an important dichotomy: when the rate of adoption is low, an innovation spreads
slowly but widely; in contrast, when the adoption rate is high, the innovation
spreads rapidly but the extent of the adoption is severely limited by Luddites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02021</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02021</id><created>2015-05-08</created><updated>2015-09-23</updated><authors><author><keyname>Broberg</keyname><forenames>Niklas</forenames></author><author><keyname>van Delft</keyname><forenames>Bart</forenames></author><author><keyname>Sands</keyname><forenames>David</forenames></author></authors><title>The Anatomy and Facets of Dynamic Policies</title><categories>cs.CR</categories><comments>Technical Report of publication under the same name in Computer
  Security Foundations (CSF) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information flow policies are often dynamic; the security concerns of a
program will typically change during execution to reflect security-relevant
events. A key challenge is how to best specify, and give proper meaning to,
such dynamic policies. A large number of approaches exist that tackle that
challenge, each yielding some important, but unconnected, insight. In this work
we synthesise existing knowledge on dynamic policies, with an aim to establish
a common terminology, best practices, and frameworks for reasoning about them.
We introduce the concept of facets to illuminate subtleties in the semantics of
policies, and closely examine the anatomy of policies and the expressiveness of
policy specification mechanisms. We further explore the relation between
dynamic policies and the concept of declassification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02027</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02027</id><created>2015-05-08</created><authors><author><keyname>Alodeh</keyname><forenames>Maha</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Joint Channel Estimation and Pilot Allocation in Underlay Cognitive MISO
  Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, invited paper to IWCMC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radios have been proposed as agile technologies to boost the
spectrum utilization. This paper tackles the problem of channel estimation and
its impact on downlink transmissions in an underlay cognitive radio scenario.
We consider primary and cognitive base stations, each equipped with multiple
antennas and serving multiple users. Primary networks often suffer from the
cognitive interference, which can be mitigated by deploying beamforming at the
cognitive systems to spatially direct the transmissions away from the primary
receivers. The accuracy of the estimated channel state information (CSI) plays
an important role in designing accurate beamformers that can regulate the
amount of interference. However, channel estimate is affected by interference.
Therefore, we propose different channel estimation and pilot allocation
techniques to deal with the channel estimation at the cognitive systems, and to
reduce the impact of contamination at the primary and cognitive systems. In an
effort to tackle the contamination problem in primary and cognitive systems, we
exploit the information embedded in the covariance matrices to successfully
separate the channel estimate from other users' channels in correlated
cognitive single input multiple input (SIMO) channels. A minimum mean square
error (MMSE) framework is proposed by utilizing the second order statistics to
separate the overlapping spatial paths that create the interference. We
validate our algorithms by simulation and compare them to the state of the art
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02054</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02054</id><created>2015-05-08</created><updated>2015-11-19</updated><authors><author><keyname>Kim</keyname><forenames>Dong Min</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Reliable Uplink Communication through Double Association in Wireless
  Heterogeneous Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>4 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate methods for network association that improve the reliability
of uplink transmissions in dense wireless heterogeneous networks. The
stochastic geometry analysis shows that the double association, in which an
uplink transmission is transmitted to a macro Base Station (BS) and small BS,
significantly improves the probability of successful transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02056</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02056</id><created>2015-05-08</created><authors><author><keyname>Jiang</keyname><forenames>Junchen</forenames></author><author><keyname>Sekar</keyname><forenames>Vyas</forenames></author><author><keyname>Sun</keyname><forenames>Yi</forenames></author></authors><title>DDA: Cross-Session Throughput Prediction with Applications to Video
  Bitrate Selection</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User experience of video streaming could be greatly improved by selecting a
high-yet-sustainable initial video bitrate, and it is therefore critical to
accurately predict throughput before a video session starts. Inspired by
previous studies that show similarity among throughput of similar sessions
(e.g., those sharing same bottleneck link), we argue for a cross-session
prediction approach, where throughput measured on other sessions is used to
predict the throughput of a new session. In this paper, we study the challenges
of cross-session throughput prediction, develop an accurate throughput
predictor called DDA, and evaluate the performance of the predictor with
real-world datasets. We show that DDA can predict throughput more accurately
than simple predictors and conventional machine learning algorithms; e.g.,
DDA's 80%ile prediction error of DDA is &gt; 50% lower than other algorithms. We
also show that this improved accuracy enables video players to select a higher
sustainable initial bitrate; e.g., compared to initial bitrate without
prediction, DDA leads to 4x higher average bitrate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02063</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02063</id><created>2015-05-08</created><updated>2015-08-27</updated><authors><author><keyname>Pourbabaee</keyname><forenames>Bahareh</forenames></author><author><keyname>Meskin</keyname><forenames>Nader</forenames></author><author><keyname>Khorasani</keyname><forenames>Khashayar</forenames></author></authors><title>Sensor Fault Detection, Isolation and Identification Using Multiple
  Model-based Hybrid Kalman Filter for Gas Turbine Engines</title><categories>cs.SY</categories><comments>31 pages, 7 figures. This version is prepared in the single-column
  format with minor modifications compared to the previous version. It is
  submitted to IEEE Transaction On Control System Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel sensor fault detection, isolation and identification
(FDII) strategy is proposed by using the multiple model (MM) approach. The
scheme is based on multiple hybrid Kalman filters (HKF) which represents an
integration of a nonlinear mathematical model of the system with a number of
piecewise linear (PWL) models. The proposed fault detection and isolation (FDI)
scheme is capable of detecting and isolating sensor faults during the entire
operational regime of the system by interpolating the PWL models using a
Bayesian approach. Moreover, the proposed multiple HKF-based FDI scheme is
extended to identify the magnitude of a sensor fault by using a modified
generalized likelihood ratio (GLR) method which relies on the healthy
operational mode of the system. To illustrate the capabilities of our proposed
FDII methodology, extensive simulation studies are conducted for a nonlinear
gas turbine engine. Various single and concurrent sensor fault scenarios are
considered to demonstrate the effectiveness of our proposed on-line
hierarchical multiple HKF-based FDII scheme under different flight modes.
Finally, our proposed HKF-based FDI approach is compared with various filtering
methods such as the linear, extended, unscented and cubature Kalman filters
(LKF, EKF, UKF and CKF, respectively) corresponding to both interacting and
non-interacting multiple model (MM) based schemes. Our comparative studies
confirm the superiority of our proposed HKF method in terms of promptness of
the fault detection, lower false alarm rates, as well as robustness with
respect to the engine health parameters degradations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02070</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02070</id><created>2015-05-08</created><authors><author><keyname>Stojadinovi&#x107;</keyname><forenames>Mirko</forenames></author><author><keyname>Nikoli&#x107;</keyname><forenames>Mladen</forenames></author><author><keyname>Mari&#x107;</keyname><forenames>Filip</forenames></author></authors><title>Short Portfolio Training for CSP Solving</title><categories>cs.AI</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many different approaches for solving Constraint Satisfaction Problems (CSPs)
and related Constraint Optimization Problems (COPs) exist. However, there is no
single solver (nor approach) that performs well on all classes of problems and
many portfolio approaches for selecting a suitable solver based on simple
syntactic features of the input CSP instance have been developed. In this paper
we first present a simple portfolio method for CSP based on k-nearest neighbors
method. Then, we propose a new way of using portfolio systems --- training them
shortly in the exploitation time, specifically for the set of instances to be
solved and using them on that set. Thorough evaluation has been performed and
has shown that the approach yields good results. We evaluated several machine
learning techniques for our portfolio. Due to its simplicity and efficiency,
the selected k-nearest neighbors method is especially suited for our short
training approach and it also yields the best results among the tested methods.
We also confirm that our approach yields good results on SAT domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02074</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02074</id><created>2015-05-08</created><updated>2015-11-29</updated><authors><author><keyname>Ren</keyname><forenames>Mengye</forenames></author><author><keyname>Kiros</keyname><forenames>Ryan</forenames></author><author><keyname>Zemel</keyname><forenames>Richard</forenames></author></authors><title>Exploring Models and Data for Image Question Answering</title><categories>cs.LG cs.AI cs.CL cs.CV</categories><comments>12 pages. Conference paper at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims to address the problem of image-based question-answering (QA)
with new models and datasets. In our work, we propose to use neural networks
and visual semantic embeddings, without intermediate stages such as object
detection and image segmentation, to predict answers to simple questions about
images. Our model performs 1.8 times better than the only published results on
an existing image QA dataset. We also present a question generation algorithm
that converts image descriptions, which are widely available, into QA form. We
used this algorithm to produce an order-of-magnitude larger dataset, with more
evenly distributed answers. A suite of baseline results on this new dataset are
also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02075</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02075</id><created>2015-05-08</created><authors><author><keyname>Cantone</keyname><forenames>Domenico</forenames></author><author><keyname>Longo</keyname><forenames>Cristiano</forenames></author><author><keyname>Nicolosi-Asmundo</keyname><forenames>Marianna</forenames></author><author><keyname>Santamaria</keyname><forenames>Daniele Francesco</forenames></author></authors><title>Web ontology representation and reasoning via fragments of set theory</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use results from Computable Set Theory as a means to
represent and reason about description logics and rule languages for the
semantic web.
  Specifically, we introduce the description logic $\mathcal{DL}\langle
4LQS^R\rangle(\D)$--admitting features such as min/max cardinality constructs
on the left-hand/right-hand side of inclusion axioms, role chain axioms, and
datatypes--which turns out to be quite expressive if compared with
$\mathcal{SROIQ}(\D)$, the description logic underpinning the Web Ontology
Language OWL. Then we show that the consistency problem for
$\mathcal{DL}\langle 4LQS^R\rangle(\D)$-knowledge bases is decidable by
reducing it, through a suitable translation process, to the satisfiability
problem of the stratified fragment $4LQS^R$ of set theory, involving variables
of four sorts and a restricted form of quantification. We prove also that,
under suitable not very restrictive constraints, the consistency problem for
$\mathcal{DL}\langle 4LQS^R\rangle(\D)$-knowledge bases is
\textbf{NP}-complete. Finally, we provide a $4LQS^R$-translation of rules
belonging to the Semantic Web Rule Language (SWRL).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02085</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02085</id><created>2015-05-08</created><updated>2015-05-13</updated><authors><author><keyname>Shah</keyname><forenames>Shahid M.</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Mitigating Rate Loss in Fading Multiple Access Wiretap Channel to
  achieve Shannon Capacity Region</title><categories>cs.IT cs.CR math.IT</categories><comments>31 pages, 4 figures, submitted to IEEE Transactions on Information
  forensics and security. arXiv admin note: text overlap with arXiv:1410.8794</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security constraint results in \textit{rate-loss} in wiretap channels. In
this paper we propose a coding scheme for two user Multiple Access Channel with
Wiretap (MAC-WT), where previous messages are used as a key to enhance the
secrecy rates of both the users until we achieve the usual capacity region of a
Multiple Access Channel (MAC) without the wiretapper (Shannon capacity region).
With this scheme only the message that is being transmitted is proved to be
secure with respect to all the information that the adversary (eavesdropper)
possesses. In the second part of the paper we improve the security constraint
such that not only the message being transmitted is secure but all the messages
transmitted in the immediate past are secure w.r.t. all the information of the
eavesdropper. To achieve this goal we introduce secret key buffers at both the
users, as well as at the legitimate receiver (Bob). Finally we consider a
fading MAC-WT and show that with this coding/decoding scheme we can achieve the
capacity region of a fading MAC (in ergodic sense).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02091</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02091</id><created>2015-05-08</created><authors><author><keyname>Pauly</keyname><forenames>Arno</forenames></author><author><keyname>Davie</keyname><forenames>George</forenames></author><author><keyname>Fouch&#xe9;</keyname><forenames>Willem</forenames></author></authors><title>Weihrauch-completeness for layerwise computability</title><categories>cs.LO</categories><msc-class>03D30, 68Q30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of being Weihrauch-complete for layerwise
computability and provide several natural examples related to complex
oscillations, the law of the iterated logarithm and Birkhoff's theorem. We also
consider the hitting time operators, which share the Weihrauch degree of the
former examples, but fail to be layerwise computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02098</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02098</id><created>2015-05-08</created><authors><author><keyname>Nagaraj</keyname><forenames>Shirish</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author><author><keyname>Zeineddine</keyname><forenames>Khalid</forenames></author></authors><title>Distributed Optimization of Multi-Cell Uplink Co-operation with Backhaul
  Constraints</title><categories>cs.SY cs.IT math.IT</categories><comments>IEEE ICC Conference, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of uplink co-operative reception with constraints on
both backhaul bandwidth and the receiver aperture, or number of antenna signals
that can be processed. The problem is cast as a network utility (weighted sum
rate) maximization subject to computational complexity and architectural
bandwidth sharing constraints. We show that a relaxed version of the problem is
convex, and can be solved via a dual-decomposition. The proposed solution is
distributed in that each cell broadcasts a set of {\em demand prices} based on
the data sharing requests they receive. Given the demand prices, the algorithm
determines an antenna/cell ordering and antenna-selection for each scheduled
user in a cell. This algorithm, referred to as {\em LiquidMAAS}, iterates
between the preceding two steps. Simulations of realistic network scenarios
show that the algorithm exhibits fast convergence even for systems with large
number of cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02100</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02100</id><created>2015-05-08</created><updated>2015-09-29</updated><authors><author><keyname>Gramacki</keyname><forenames>Artur</forenames></author><author><keyname>Sawerwain</keyname><forenames>Marek</forenames></author><author><keyname>Gramacki</keyname><forenames>Jaros&#x142;aw</forenames></author></authors><title>FPGA-Based Bandwidth Selection for Kernel Density Estimation Using High
  Level Synthesis Approach</title><categories>cs.OH</categories><comments>23 pages, 6 figures, extended version of initial paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FPGA technology can offer significantly hi\-gher performance at much lower
power consumption than is available from CPUs and GPUs in many computational
problems. Unfortunately, programming for FPGA (using ha\-rdware description
languages, HDL) is a difficult and not-trivial task and is not intuitive for
C/C++/Java programmers. To bring the gap between programming effectiveness and
difficulty the High Level Synthesis (HLS) approach is promoting by main FPGA
vendors. Nowadays, time-intensive calculations are mainly performed on GPU/CPU
architectures, but can also be successfully performed using HLS approach. In
the paper we implement a bandwidth selection algorithm for kernel density
estimation (KDE) using HLS and show techniques which were used to optimize the
final FPGA implementation. We are also going to show that FPGA speedups,
comparing to highly optimized CPU and GPU implementations, are quite
substantial. Moreover, power consumption for FPGA devices is usually much less
than typical power consumption of the present CPUs and GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02108</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02108</id><created>2015-05-08</created><updated>2015-09-07</updated><authors><author><keyname>Miller</keyname><forenames>D.</forenames></author><author><keyname>Brossard</keyname><forenames>E.</forenames></author><author><keyname>Seitz</keyname><forenames>S.</forenames></author><author><keyname>Kemelmacher-Shlizerman</keyname><forenames>I.</forenames></author></authors><title>MegaFace: A Million Faces for Recognition at Scale</title><categories>cs.CV</categories><comments>Please see http://megaface.cs.washington.edu/ for code and data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent face recognition experiments on the LFW benchmark show that face
recognition is performing stunningly well, surpassing human recognition rates.
In this paper, we study face recognition at scale. Specifically, we have
collected from Flickr a \textbf{Million} faces and evaluated state of the art
face recognition algorithms on this dataset. We found that the performance of
algorithms varies--while all perform great on LFW, once evaluated at scale
recognition rates drop drastically for most algorithms. Interestingly, deep
learning based approach by \cite{schroff2015facenet} performs much better, but
still gets less robust at scale. We consider both verification and
identification problems, and evaluate how pose affects recognition at scale.
Moreover, we ran an extensive human study on Mechanical Turk to evaluate human
recognition at scale, and report results. All the photos are creative commons
photos and is released at \small{\url{http://megaface.cs.washington.edu/}} for
research and further experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02111</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02111</id><created>2015-05-08</created><authors><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author></authors><title>Power Decoding Reed--Solomon Codes Up to the Johnson Radius</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. IT. Results announced at ACCT-14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power decoding, or &quot;decoding using virtual interleaving&quot; is a technique for
decoding Reed--Solomon codes up to the Sudan radius. Since the method's
inception, it has been an open question if it possible to incorporate
&quot;multiplicities&quot;, the parameter allowing the Guruswami--Sudan algorithm to
decode up to the Johnson radius. In this paper we show how this can be done,
and describe how to efficiently solve the resulting key equations. We
investigate its failure behaviour theoretically as well as giving simulation
results, and we show how the method can be made practically faster using the
re-encoding technique or a syndrome formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02120</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02120</id><created>2015-05-08</created><authors><author><keyname>Calatroni</keyname><forenames>Luca</forenames></author><author><keyname>Chung</keyname><forenames>Cao</forenames></author><author><keyname>Reyes</keyname><forenames>Juan Carlos De Los</forenames></author><author><keyname>Sch&#xf6;nlieb</keyname><forenames>Carola-Bibiane</forenames></author><author><keyname>Valkonen</keyname><forenames>Tuomo</forenames></author></authors><title>Bilevel approaches for learning of variational imaging models</title><categories>math.OC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review some recent learning approaches in variational imaging, based on
bilevel optimisation, and emphasize the importance of their treatment in
function space. The paper covers both analytical and numerical techniques.
Analytically, we include results on the existence and structure of minimisers,
as well as optimality conditions for their characterisation. Based on this
information, Newton type methods are studied for the solution of the problems
at hand, combining them with sampling techniques in case of large databases.
The computational verification of the developed techniques is extensively
documented, covering instances with different type of regularisers, several
noise models, spatially dependent weights and large image databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02135</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02135</id><created>2015-05-08</created><authors><author><keyname>Maheshwari</keyname><forenames>Garima</forenames></author><author><keyname>Kumar</keyname><forenames>Animesh</forenames></author></authors><title>Optimal Quantization of TV White Space Regions for a Broadcast Based
  Geolocation Database</title><categories>cs.IT math.IT</categories><comments>8 pages, 12 figures, submitted to IEEE DySPAN (Technology) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current paradigm, TV white space databases communicate the available
channels over a reliable Internet connection to the secondary devices. For
places where an Internet connection is not available, such as in developing
countries, a broadcast based geolocation database can be considered. This
geolocation database will broadcast the TV white space (or the primary services
protection regions) on rate-constrained digital channel.
  In this work, the quantization or digital representation of protection
regions is considered for rate-constrained broadcast geolocation database.
Protection regions should not be declared as white space regions due to the
quantization error. In this work, circular and basis based approximations are
presented for quantizing the protection regions. In circular approximation,
quantization design algorithms are presented to protect the primary from
quantization error while minimizing the white space area declared as protected
region. An efficient quantizer design algorithm is presented in this case. For
basis based approximations, an efficient method to represent the protection
regions by an `envelope' is developed. By design this envelope is a sparse
approximation, i.e., it has lesser number of non-zero coefficients in the basis
when compared to the original protection region. The approximation methods
presented in this work are tested using three experimental data-sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02137</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02137</id><created>2015-05-06</created><updated>2015-05-28</updated><authors><author><keyname>Amer</keyname><forenames>Mohamed R.</forenames></author><author><keyname>Siddiquie</keyname><forenames>Behjat</forenames></author><author><keyname>Tamrakar</keyname><forenames>Amir</forenames></author><author><keyname>Salter</keyname><forenames>David A.</forenames></author><author><keyname>Lande</keyname><forenames>Brian</forenames></author><author><keyname>Mehri</keyname><forenames>Darius</forenames></author><author><keyname>Divakaran</keyname><forenames>Ajay</forenames></author></authors><title>Human Social Interaction Modeling Using Temporal Deep Networks</title><categories>cs.CY cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a novel approach to computational modeling of social interactions
based on modeling of essential social interaction predicates (ESIPs) such as
joint attention and entrainment. Based on sound social psychological theory and
methodology, we collect a new &quot;Tower Game&quot; dataset consisting of audio-visual
capture of dyadic interactions labeled with the ESIPs. We expect this dataset
to provide a new avenue for research in computational social interaction
modeling. We propose a novel joint Discriminative Conditional Restricted
Boltzmann Machine (DCRBM) model that combines a discriminative component with
the generative power of CRBMs. Such a combination enables us to uncover
actionable constituents of the ESIPs in two steps. First, we train the DCRBM
model on the labeled data and get accurate (76\%-49\% across various ESIPs)
detection of the predicates. Second, we exploit the generative capability of
DCRBMs to activate the trained model so as to generate the lower-level data
corresponding to the specific ESIP that closely matches the actual training
data (with mean square error 0.01-0.1 for generating 100 frames). We are thus
able to decompose the ESIPs into their constituent actionable behaviors. Such a
purely computational determination of how to establish an ESIP such as
engagement is unprecedented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02138</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02138</id><created>2015-05-08</created><updated>2015-10-30</updated><authors><author><keyname>Mistry</keyname><forenames>Dina</forenames></author><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author></authors><title>Committed activists and the reshaping of status-quo social consensus</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Physical Review E 92, 042805, 2015</journal-ref><doi>10.1103/PhysRevE.92.042805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role of committed minorities in shaping public opinion has been recently
addressed with the help of multi-agent models. However, previous studies
focused on homogeneous populations where zealots stand out only for their
stubbornness. Here, we consider the more general case in which individuals are
characterized by different propensities to communicate. In particular, we
correlate commitment with a higher tendency to push an opinion, acknowledging
the fact that individuals with unwavering dedication to a cause are also more
active in their attempts to promote their message. We show that these
\textit{activists} are not only more efficient in spreading their message but
that their efforts require an order of magnitude fewer individuals than a
randomly selected committed minority to bring the population over to a new
consensus. Finally, we address the role of communities, showing that partisan
divisions in the society can make it harder for committed individuals to flip
the status-quo social consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02140</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02140</id><created>2015-05-08</created><authors><author><keyname>Siddique</keyname><forenames>Umair</forenames></author><author><keyname>Hasan</keyname><forenames>Osman</forenames></author><author><keyname>Tahar</keyname><forenames>Sofi&#xe8;ne</forenames></author></authors><title>Towards the Formalization of Fractional Calculus in Higher-Order Logic</title><categories>cs.LO</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional calculus is a generalization of classical theories of integration
and differentiation to arbitrary order (i.e., real or complex numbers). In the
last two decades, this new mathematical modeling approach has been widely used
to analyze a wide class of physical systems in various fields of science and
engineering. In this paper, we describe an ongoing project which aims at
formalizing the basic theories of fractional calculus in the HOL Light theorem
prover. Mainly, we present the motivation and application of such formalization
efforts, a roadmap to achieve our goals, current status of the project and
future milestones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02141</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02141</id><created>2015-05-08</created><authors><author><keyname>Aminikashani</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Kavehrad</keyname><forenames>Mohsen</forenames></author></authors><title>Error Performance Analysis of FSO Links with Equal Gain Diversity
  Receivers over Double Generalized Gamma Fading Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Globecom 2015, conference, 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Free space optical (FSO) communication has been receiving increasing
attention in recent years with its ability to achieve ultra-high data rates
over unlicensed optical spectrum. A major performance limiting factor in FSO
systems is atmospheric turbulence which severely degrades the system
performance. To address this issue, multiple transmit and/or receive apertures
can be employed, and the performance can be improved via diversity gain. In
this paper, we investigate the bit error rate (BER) performance of FSO systems
with transmit diversity or receive diversity with equal gain combining (EGC)
over atmospheric turbulence channels described by the Double Generalized Gamma
(Double GG) distribution. The Double GG distribution, recently proposed,
generalizes many existing turbulence models in a closed-form expression and
covers all turbulence conditions. Since the distribution function of a sum of
Double GG random variables (RVs) appears in BER expression, we first derive a
closed-form upper bound for the distribution of the sum of Double GG
distributed RVs. A novel union upper bound for the average BER as well as
corresponding asymptotic expression is then derived and evaluated in terms of
Meijer's G-functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02142</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02142</id><created>2015-05-08</created><updated>2016-02-09</updated><authors><author><keyname>Billaudelle</keyname><forenames>Sebastian</forenames></author><author><keyname>Ahmad</keyname><forenames>Subutai</forenames></author></authors><title>Porting HTM Models to the Heidelberg Neuromorphic Computing Platform</title><categories>q-bio.NC cs.NE</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical Temporal Memory (HTM) is a computational theory of machine
intelligence based on a detailed study of the neocortex. The Heidelberg
Neuromorphic Computing Platform, developed as part of the Human Brain Project
(HBP), is a mixed-signal (analog and digital) large-scale platform for modeling
networks of spiking neurons. In this paper we present the first effort in
porting HTM networks to this platform. We describe a framework for simulating
key HTM operations using spiking network models. We then describe specific
spatial pooling and temporal memory implementations, as well as simulations
demonstrating that the fundamental properties are maintained. We discuss issues
in implementing the full set of plasticity rules using Spike-Timing Dependent
Plasticity (STDP), and rough place and route calculations. Although further
work is required, our initial studies indicate that it should be possible to
run large-scale HTM networks (including plasticity rules) efficiently on the
Heidelberg platform. More generally the exercise of porting high level HTM
algorithms to biophysical neuron models promises to be a fruitful area of
investigation for future studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02146</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02146</id><created>2015-05-08</created><updated>2015-09-26</updated><authors><author><keyname>Kuo</keyname><forenames>Weicheng</forenames></author><author><keyname>Hariharan</keyname><forenames>Bharath</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>DeepBox: Learning Objectness with Convolutional Networks</title><categories>cs.CV</categories><comments>ICCV 2015 Camera-ready version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing object proposal approaches use primarily bottom-up cues to rank
proposals, while we believe that objectness is in fact a high level construct.
We argue for a data-driven, semantic approach for ranking object proposals. Our
framework, which we call DeepBox, uses convolutional neural networks (CNNs) to
rerank proposals from a bottom-up method. We use a novel four-layer CNN
architecture that is as good as much larger networks on the task of evaluating
objectness while being much faster. We show that DeepBox significantly improves
over the bottom-up ranking, achieving the same recall with 500 proposals as
achieved by bottom-up methods with 2000. This improvement generalizes to
categories the CNN has never seen before and leads to a 4.5-point gain in
detection mAP. Our implementation achieves this performance while running at
260 ms per image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02155</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02155</id><created>2015-05-08</created><updated>2015-09-14</updated><authors><author><keyname>Chen</keyname><forenames>Jian-Jia</forenames></author><author><keyname>Huang</keyname><forenames>Wen-Hung</forenames></author><author><keyname>Liu</keyname><forenames>Cong</forenames></author></authors><title>Evaluate and Compare Two Utilization-Based Schedulability-Test
  Frameworks for Real-Time Systems</title><categories>cs.DS cs.OS</categories><comments>arXiv admin note: text overlap with arXiv:1501.07084</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report summarizes two general frameworks, namely k2Q and k2U, that have
been recently developed by us. The purpose of this report is to provide
detailed evaluations and comparisons of these two frameworks. These two
frameworks share some similar characteristics, but they are useful for
different application cases. These two frameworks together provide
comprehensive means for the users to automatically convert the pseudo
polynomial-time tests (or even exponential-time tests) into polynomial-time
tests with closed mathematical forms. With the quadratic and hyperbolic forms,
k2Q and k2U frameworks can be used to provide many quantitive features to be
measured and evaluated, like the total utilization bounds, speed-up factors,
etc., not only for uniprocessor scheduling but also for multiprocessor
scheduling. These frameworks can be viewed as &quot;blackbox&quot; interfaces for
providing polynomial-time schedulability tests and response time analysis for
real-time applications. We have already presented their advantages for being
applied in some models in the previous papers. However, it was not possible to
present a more comprehensive comparison between these two frameworks. We hope
this report can help the readers and users clearly understand the difference of
these two frameworks, their unique characteristics, and their advantages. We
demonstrate their differences and properties by using the traditional sporadic
realtime task models in uniprocessor scheduling and multiprocessor global
scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02186</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02186</id><created>2015-05-08</created><authors><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author><author><keyname>Troha</keyname><forenames>Carolyn</forenames></author></authors><title>Construction of Subspace Codes through Linkage</title><categories>cs.IT math.CO math.IT</categories><msc-class>11T71, 94B60, 51E23</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A construction is presented that allows to produce subspace codes of long
length using subspace codes of shorter length in combination with a rank metric
code. The subspace distance of the resulting code, called linkage code, is as
good as the minimum subspace distance of the constituent codes. As a special
application, the construction of the best known partial spreads is reproduced.
Finally, for a special case of linkage, a decoding algorithm is presented which
amounts to decoding with respect to the smaller constituent codes and which can
be parallelized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02192</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02192</id><created>2015-05-08</created><authors><author><keyname>Shrestha</keyname><forenames>Munik</forenames></author><author><keyname>Scarpino</keyname><forenames>Samuel V.</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>A message-passing approach for recurrent-state epidemic models on
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 8 figures</comments><doi>10.1103/PhysRevE.92.022821</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Epidemic processes are common out-of-equilibrium phenomena of broad
interdisciplinary interest. Recently, dynamic message-passing (DMP) has been
proposed as an efficient algorithm for simulating epidemic models on networks,
and in particular for estimating the probability that a given node will become
infectious at a particular time. To date, DMP has been applied exclusively to
models with one-way state changes, as opposed to models like SIS
(susceptible-infectious-susceptible) and SIRS
(susceptible-infectious-recovered-susceptible) where nodes can return to
previously inhabited states. Because many real-world epidemics can exhibit such
recurrent dynamics, we propose a DMP algorithm for complex, recurrent epidemic
models on networks. Our approach takes correlations between neighboring nodes
into account while preventing causal signals from backtracking to their
immediate source, and thus avoids &quot;echo chamber effects&quot; where a pair of
adjacent nodes each amplify the probability that the other is infectious. We
demonstrate that this approach well approximates results obtained from Monte
Carlo simulation and that its accuracy is often superior to the pair
approximation (which also takes second-order correlations into account).
Moreover, our approach is more computationally efficient than the pair
approximation, especially for complex epidemic models: the number of variables
in our DMP approach grows as $2mk$ where $m$ is the number of edges and $k$ is
the number of states, as opposed to $mk^2$ for the pair approximation. We
suspect that the resulting reduction in computational effort, as well as the
conceptual simplicity of DMP, will make it a useful tool in epidemic modeling,
especially for inference tasks where there is a large parameter space to
explore.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02199</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02199</id><created>2015-05-08</created><authors><author><keyname>Yazdi</keyname><forenames>S. M. Hossein Tabatabaei</forenames></author><author><keyname>Yuan</keyname><forenames>Yongbo</forenames></author><author><keyname>Ma</keyname><forenames>Jian</forenames></author><author><keyname>Zhao</keyname><forenames>Huimin</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>A Rewritable, Random-Access DNA-Based Storage System</title><categories>cs.IT math.IT</categories><comments>26 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the first DNA-based storage architecture that enables random
access to data blocks and rewriting of information stored at arbitrary
locations within the blocks. The newly developed architecture overcomes
drawbacks of existing read-only methods that require decoding the whole file in
order to read one data fragment. Our system is based on new constrained coding
techniques and accompanying DNA editing methods that ensure data reliability,
specificity and sensitivity of access, and at the same time provide
exceptionally high data storage capacity. As a proof of concept, we encoded
parts of the Wikipedia pages of six universities in the USA, and selected and
edited parts of the text written in DNA corresponding to three of these
schools. The results suggest that DNA is a versatile media suitable for both
ultrahigh density archival and rewritable storage applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02202</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02202</id><created>2015-05-08</created><authors><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew W.</forenames></author><author><keyname>Hiyama</keyname><forenames>Satoshi</forenames></author></authors><title>Design and Optimizing of On-Chip Kinesin Substrates for Molecular
  Communication</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE Transactions on Nanotechnology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lab-on-chip devices and point-of-care diagnostic chip devices are composed of
many different components such as nanosensors that must be able to communicate
with other components within the device. Molecular communication is a promising
solution for on-chip communication. In particular, kinesin driven microtubule
(MT) motility is an effective means of transferring information particles from
one component to another. However, finding an optimal shape for these channels
can be challenging. In this paper we derive a mathematical optimization model
that can be used to find the optimal channel shape and dimensions for any
transmission period. We derive three specific models for the rectangular
channels, regular polygonal channels, and regular polygonal ring channels. We
show that the optimal channel shapes are the square-shaped channel for the
rectangular channel, and circular-shaped channel for the other classes of
shapes. Finally, we show that among all 2 dimensional shapes the optimal design
choice that maximizes information rate is the circular-shaped channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02205</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02205</id><created>2015-05-08</created><authors><author><keyname>Alper</keyname><forenames>Jarod</forenames></author><author><keyname>Bogart</keyname><forenames>Tristram</forenames></author><author><keyname>Velasco</keyname><forenames>Mauricio</forenames></author></authors><title>A lower bound for the determinantal complexity of a hypersurface</title><categories>cs.CC math.AG</categories><comments>7 pages, 0 figures</comments><msc-class>68Q05, 68Q17, 14M12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the determinantal complexity of a hypersurface of degree $d &gt;
2$ is bounded below by one more than the codimension of the singular locus,
provided that this codimension is at least $5$. As a result, we obtain that the
determinantal complexity of the $3 \times 3$ permanent is $7$. We also prove
that for $n&gt; 3$, there is no nonsingular hypersurface in $\mathbf{P}^n$ of
degree $d$ that has an expression as a determinant of a $d \times d$ matrix of
linear forms while on the other hand for $n \le 3$, a general determinantal
expression is nonsingular. Finally, we answer a question of Ressayre by showing
that the determinantal complexity of the unique (singular) cubic surface
containing a single line is $5$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02206</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02206</id><created>2015-05-08</created><authors><author><keyname>Jayaraman</keyname><forenames>Dinesh</forenames></author><author><keyname>Grauman</keyname><forenames>Kristen</forenames></author></authors><title>Learning image representations equivariant to ego-motion</title><categories>cs.CV cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how images of objects and scenes behave in response to specific
ego-motions is a crucial aspect of proper visual development, yet existing
visual learning methods are conspicuously disconnected from the physical source
of their images. We propose to exploit proprioceptive motor signals to provide
unsupervised regularization in convolutional neural networks to learn visual
representations from egocentric video. Specifically, we enforce that our
learned features exhibit equivariance i.e. they respond systematically to
transformations associated with distinct ego-motions. With three datasets, we
show that our unsupervised feature learning system significantly outperforms
previous approaches on visual recognition and next-best-view prediction tasks.
In the most challenging test, we show that features learned from video captured
on an autonomous driving platform improve large-scale scene recognition in a
disjoint domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02211</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02211</id><created>2015-05-08</created><updated>2015-08-24</updated><authors><author><keyname>Wu</keyname><forenames>Tony F.</forenames></author><author><keyname>Ganesan</keyname><forenames>Karthik</forenames></author><author><keyname>Hu</keyname><forenames>Yunqing Alexander</forenames></author><author><keyname>Wong</keyname><forenames>H. -S. Philip</forenames></author><author><keyname>Wong</keyname><forenames>Simon</forenames></author><author><keyname>Mitra</keyname><forenames>Subhasish</forenames></author></authors><title>TPAD: Hardware Trojan Prevention and Detection for Trusted Integrated
  Circuits</title><categories>cs.AR cs.CR</categories><comments>17 pages, 23 figures. Extended version of paper to appear in IEEE
  Trans. on CAD</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are increasing concerns about possible malicious modifications of
integrated circuits (ICs) used in critical applications. Such attacks are often
referred to as hardware Trojans. While many techniques focus on hardware Trojan
detection during IC testing, it is still possible for attacks to go undetected.
Using a combination of new design techniques and new memory technologies, we
present a new approach that detects a wide variety of hardware Trojans during
IC testing and also during system operation in the field. Our approach can also
prevent a wide variety of attacks during synthesis, place-and-route, and
fabrication of ICs. It can be applied to any digital system, and can be tuned
for both traditional and split-manufacturing methods. We demonstrate its
applicability for both ASICs and FPGAs. Using fabricated test chips with Trojan
emulation capabilities and also using simulations, we demonstrate: 1. The area
and power costs of our approach can range between 7.4-165% and 0.07-60%,
respectively, depending on the design and the attacks targeted; 2. The speed
impact can be minimal (close to 0%); 3. Our approach can detect 99.998% of
Trojans (emulated using test chips) that do not require detailed knowledge of
the design being attacked; 4. Our approach can prevent 99.98% of specific
attacks (simulated) that utilize detailed knowledge of the design being
attacked (e.g., through reverse-engineering). 5. Our approach never produces
any false positives, i.e., it does not report attacks when the IC operates
correctly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02212</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02212</id><created>2015-05-08</created><updated>2015-05-12</updated><authors><author><keyname>Reshef</keyname><forenames>Yakir A.</forenames></author><author><keyname>Reshef</keyname><forenames>David N.</forenames></author><author><keyname>Sabeti</keyname><forenames>Pardis C.</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael M.</forenames></author></authors><title>Equitability, interval estimation, and statistical power</title><categories>math.ST cs.LG q-bio.QM stat.ME stat.ML stat.TH</categories><comments>Yakir A. Reshef and David N. Reshef are co-first authors, Pardis C.
  Sabeti and Michael M. Mitzenmacher are co-last authors. This paper, together
  with arXiv:1505.02212, subsumes arXiv:1408.4908</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For analysis of a high-dimensional dataset, a common approach is to test a
null hypothesis of statistical independence on all variable pairs using a
non-parametric measure of dependence. However, because this approach attempts
to identify any non-trivial relationship no matter how weak, it often
identifies too many relationships to be useful. What is needed is a way of
identifying a smaller set of relationships that merit detailed further
analysis.
  Here we formally present and characterize equitability, a property of
measures of dependence that aims to overcome this challenge. Notionally, an
equitable statistic is a statistic that, given some measure of noise, assigns
similar scores to equally noisy relationships of different types [Reshef et al.
2011]. We begin by formalizing this idea via a new object called the
interpretable interval, which functions as an interval estimate of the amount
of noise in a relationship of unknown type. We define an equitable statistic as
one with small interpretable intervals.
  We then draw on the equivalence of interval estimation and hypothesis testing
to show that under moderate assumptions an equitable statistic is one that
yields well powered tests for distinguishing not only between trivial and
non-trivial relationships of all kinds but also between non-trivial
relationships of different strengths. This means that equitability allows us to
specify a threshold relationship strength $x_0$ and to search for relationships
of all kinds with strength greater than $x_0$. Thus, equitability can be
thought of as a strengthening of power against independence that enables
fruitful analysis of data sets with a small number of strong, interesting
relationships and a large number of weaker ones. We conclude with a
demonstration of how our two equivalent characterizations of equitability can
be used to evaluate the equitability of a statistic in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02213</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02213</id><created>2015-05-08</created><updated>2015-05-12</updated><authors><author><keyname>Reshef</keyname><forenames>Yakir A.</forenames></author><author><keyname>Reshef</keyname><forenames>David N.</forenames></author><author><keyname>Finucane</keyname><forenames>Hilary K.</forenames></author><author><keyname>Sabeti</keyname><forenames>Pardis C.</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael M.</forenames></author></authors><title>Measuring dependence powerfully and equitably</title><categories>stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML</categories><comments>Yakir A. Reshef and David N. Reshef are co-first authors, Pardis C.
  Sabeti and Michael M. Mitzenmacher are co-last authors. This paper, together
  with arXiv:1505.02212, subsumes arXiv:1408.4908</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For high-dimensional datasets, it is common to evaluate a measure of
dependence on every variable pair and retain the highest-scoring pairs for
follow-up. If the statistic used systematically assigns higher scores to some
relationship types over others, important relationships may be overlooked. This
difficulty is avoided if the statistic is equitable [Reshef et al. 2015a],
i.e., if, for some measure of noise, it assigns similar scores to equally noisy
relationships regardless of relationship type.
  In this paper, we introduce and characterize a population measure of
dependence called MIC*. We show three ways that MIC* can be viewed: as the
population value of MIC, a highly equitable statistic from [Reshef et al.
2011], as a canonical &quot;smoothing&quot; of mutual information, and as the supremum of
an infinite sequence defined in terms of optimal one-dimensional partitions of
the marginals of the joint distribution. Based on this theory, we introduce an
efficient algorithm for computing MIC* from the density of a pair of random
variables, and we define a new consistent estimator MICe for MIC* that is
efficiently computable. (In contrast, there is no known polynomial-time
algorithm for computing MIC.) We show through simulations that MICe has better
bias-variance properties than MIC, and that it has high equitability with
respect to R^2 on a set of functional relationships. While MICe is designed for
equitability rather than independence testing, we introduce a related
statistic, TICe, that is a trivial side-product of the computation of MICe. We
prove the consistency of independence testing based on TICe and show in
simulations that this approach achieves excellent power.
  This paper is accompanied by a companion paper [Reshef et al. 2015b] focused
on in-depth empirical evaluation of several leading measures of dependence that
finds that the performance of MICe and TICe is state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02214</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02214</id><created>2015-05-08</created><updated>2015-05-12</updated><authors><author><keyname>Reshef</keyname><forenames>David N.</forenames></author><author><keyname>Reshef</keyname><forenames>Yakir A.</forenames></author><author><keyname>Sabeti</keyname><forenames>Pardis C.</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael M.</forenames></author></authors><title>An Empirical Study of Leading Measures of Dependence</title><categories>stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML</categories><comments>David N. Reshef and Yakir A. Reshef are co-first authors, Pardis C.
  Sabeti and Michael M. Mitzenmacher are co-last authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In exploratory data analysis, we are often interested in identifying
promising pairwise associations for further analysis while filtering out
weaker, less interesting ones. This can be accomplished by computing a measure
of dependence on all variable pairs and examining the highest-scoring pairs,
provided the measure of dependence used assigns similar scores to equally noisy
relationships of different types. This property, called equitability, is
formalized in Reshef et al. [2015b]. In addition to equitability, measures of
dependence can also be assessed by the power of their corresponding
independence tests as well as their runtime.
  Here we present extensive empirical evaluation of the equitability, power
against independence, and runtime of several leading measures of dependence.
These include two statistics introduced in Reshef et al. [2015a]: MICe, which
has equitability as its primary goal, and TICe, which has power against
independence as its goal. Regarding equitability, our analysis finds that MICe
is the most equitable method on functional relationships in most of the
settings we considered, although mutual information estimation proves the most
equitable at large sample sizes in some specific settings. Regarding power
against independence, we find that TICe, along with Heller and Gorfine's S^DDP,
is the state of the art on the relationships we tested. Our analyses also show
a trade-off between power against independence and equitability consistent with
the theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe are
significantly faster than many other measures of dependence tested, and
computing either one makes computing the other trivial. This suggests that a
fast and useful strategy for achieving a combination of power against
independence and equitability may be to filter relationships by TICe and then
to examine the MICe of only the significant ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02222</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02222</id><created>2015-05-08</created><authors><author><keyname>Cooper</keyname><forenames>Joshua</forenames></author><author><keyname>Overstreet</keyname><forenames>Ralph</forenames></author></authors><title>Coloring so that no Pythagorean Triple is Monochromatic</title><categories>math.CO cs.DM cs.LO math.NT</categories><comments>14 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:0809.3478</comments><msc-class>05C15 (Primary) 05B07, 05D10, 11B75, 05-04 (Secondary)</msc-class><acm-class>G.2.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the question of the &quot;partition regularity&quot; of the Pythagorean
equation a^2+b^2=c^2; in particular, can the natural numbers be assigned a
2-coloring, so that no Pythagorean triple (i.e., a solution to the equation) is
monochromatic? We prove that the hypergraph of Pythagorean triples can contain
no Steiner triple systems, a natural obstruction to 2-colorability. Then, after
transforming the question into one about 3-CNF satisfiability and applying some
reductions, a SAT solver is used to find a 2-coloring for {1,...,7664}. Work
continues as we seek to improve the reductions and extend the computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02224</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02224</id><created>2015-05-08</created><authors><author><keyname>Han</keyname><forenames>Fangqiu</forenames></author><author><keyname>Suri</keyname><forenames>Subhash</forenames></author><author><keyname>Yan</keyname><forenames>Xifeng</forenames></author></authors><title>Observability of Lattice Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a graph observability problem: how many edge colors are needed
for an unlabeled graph so that an agent, walking from node to node, can
uniquely determine its location from just the observed color sequence of the
walk?
  Specifically, let G(n,d) be an edge-colored subgraph of d-dimensional
(directed or undirected) lattice of size n^d = n * n * ... * n. We say that
G(n,d) is t-observable if an agent can uniquely determine its current position
in the graph from the color sequence of any t-dimensional walk, where the
dimension is the number of different directions spanned by the edges of the
walk. A walk in an undirected lattice G(n,d) has dimension between 1 and d, but
a directed walk can have dimension between 1 and 2d because of two different
orientations for each axis.
  We derive bounds on the number of colors needed for t-observability. Our main
result is that Theta(n^(d/t)) colors are both necessary and sufficient for
t-observability of G(n,d), where d is considered a constant.
  This shows an interesting dependence of graph observability on the ratio
between the dimension of the lattice and that of the walk. In particular, the
number of colors for full-dimensional walks is Theta(n^(1/2)) in the directed
case, and Theta(n) in the undirected case, independent of the lattice
dimension.
  All of our results extend easily to non-square lattices: given a lattice
graph of size N = n_1 * n_2 * ... * n_d, the number of colors for
t-observability is Theta (N^(1/t)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02230</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02230</id><created>2015-05-09</created><authors><author><keyname>Rathore</keyname><forenames>Abhishek</forenames></author></authors><title>Optimal Morse functions and $H(\mathcal{M}^2,\mathbb{A})$ in
  $\tilde{O}(N)$ time</title><categories>cs.CG</categories><comments>40 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we design a nearly linear time discrete Morse theory based
algorithm for computing homology groups of 2-manifolds, thereby establishing
the fact that computing homology groups of 2-manifolds is remarkably easy.
Unlike previous algorithms of similar flavor, our method works with
coefficients from arbitrary abelian groups. Another advantage of our method
lies in the fact that our algorithm actually elucidates the topological reason
that makes computation on 2-manifolds easy. This is made possible owing to a
new simple homotopy based construct that is referred to as \emph{expansion
frames}. To being with we obtain an optimal discrete gradient vector field
using expansion frames. This is followed by a pseudo-linear time dynamic
programming based computation of discrete Morse boundary operator. The
efficient design of optimal gradient vector field followed by fast computation
of boundary operator affords us near linearity in computation of homology
groups. Moreover, we define a new criterion for nearly optimal Morse functions
called pseudo-optimality. A Morse function is pseudo-optimal if we can obtain
an optimal Morse function from it, simply by means of critical cell
cancellations. Using expansion frames, we establish the surprising fact that an
arbitrary discrete Morse function on 2-manifolds is pseudo-optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02247</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02247</id><created>2015-05-09</created><authors><author><keyname>Holzmann</keyname><forenames>T.</forenames></author><author><keyname>Prettenthaler</keyname><forenames>R.</forenames></author><author><keyname>Pestana</keyname><forenames>J.</forenames></author><author><keyname>Muschick</keyname><forenames>D.</forenames></author><author><keyname>Graber</keyname><forenames>G.</forenames></author><author><keyname>Mostegel</keyname><forenames>C.</forenames></author><author><keyname>Fraundorfer</keyname><forenames>F.</forenames></author><author><keyname>Bischof</keyname><forenames>H.</forenames></author></authors><title>Performance Evaluation of Vision-Based Algorithms for MAVs</title><categories>cs.CV</categories><comments>Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</comments><report-no>OAGM/2015/14</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important focus of current research in the field of Micro Aerial Vehicles
(MAVs) is to increase the safety of their operation in general unstructured
environments. Especially indoors, where GPS cannot be used for localization,
reliable algorithms for localization and mapping of the environment are
necessary in order to keep an MAV airborne safely. In this paper, we compare
vision-based real-time capable methods for localization and mapping and point
out their strengths and weaknesses. Additionally, we describe algorithms for
state estimation, control and navigation, which use the localization and
mapping results of our vision-based algorithms as input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02248</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02248</id><created>2015-05-09</created><authors><author><keyname>Bonaventura</keyname><forenames>Luca</forenames></author></authors><title>Local Exponential Methods: a domain decomposition approach to
  exponential time integration of PDEs</title><categories>math.NA cs.NA physics.comp-ph</categories><msc-class>65L04, 65M08, 65M20, 65Z05, 86A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A local approach to the time integration of PDEs by exponential methods is
proposed, motivated by theoretical estimates by A.Iserles on the decay of
off-diagonal terms in the exponentials of sparse matrices. An overlapping
domain decomposition technique is outlined, that allows to replace the
computation of a global exponential matrix by a number of independent and
easily parallelizable local problems. Advantages and potential problems of the
proposed technique are discussed. Numerical experiments on simple, yet relevant
model problems show that the resulting method allows to increase computational
efficiency with respect to standard implementations of exponential methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02250</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02250</id><created>2015-05-09</created><authors><author><keyname>Pilanci</keyname><forenames>Mert</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Newton Sketch: A Linear-time Optimization Algorithm with
  Linear-Quadratic Convergence</title><categories>math.OC cs.DS cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a randomized second-order method for optimization known as the
Newton Sketch: it is based on performing an approximate Newton step using a
randomly projected or sub-sampled Hessian. For self-concordant functions, we
prove that the algorithm has super-linear convergence with exponentially high
probability, with convergence and complexity guarantees that are independent of
condition numbers and related problem-dependent quantities. Given a suitable
initialization, similar guarantees also hold for strongly convex and smooth
objectives without self-concordance. When implemented using randomized
projections based on a sub-sampled Hadamard basis, the algorithm typically has
substantially lower complexity than Newton's method. We also describe
extensions of our methods to programs involving convex constraints that are
equipped with self-concordant barriers. We discuss and illustrate applications
to linear programs, quadratic programs with convex constraints, logistic
regression and other generalized linear models, as well as semidefinite
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02251</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02251</id><created>2015-05-09</created><authors><author><keyname>Kosmopoulos</keyname><forenames>Aris</forenames></author><author><keyname>Paliouras</keyname><forenames>Georgios</forenames></author><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author></authors><title>Probabilistic Cascading for Large Scale Hierarchical Classification</title><categories>cs.LG cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchies are frequently used for the organization of objects. Given a
hierarchy of classes, two main approaches are used, to automatically classify
new instances: flat classification and cascade classification. Flat
classification ignores the hierarchy, while cascade classification greedily
traverses the hierarchy from the root to the predicted leaf. In this paper we
propose a new approach, which extends cascade classification to predict the
right leaf by estimating the probability of each root-to-leaf path. We provide
experimental results which indicate that, using the same classification
algorithm, one can achieve better results with our approach, compared to the
traditional flat and cascade classifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02252</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02252</id><created>2015-05-09</created><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Liu</keyname><forenames>Hualu</forenames></author></authors><title>Quasi-cyclic Codes of Index 1.5</title><categories>cs.IT math.IT</categories><msc-class>94B05, 94B65, 15B52</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce quasi-cyclic codes of index 1.5, construct such codes in terms
of polynomials and matrices; and prove that the quasi-cyclic codes of index 1.5
are asymptotically good.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02269</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02269</id><created>2015-05-09</created><authors><author><keyname>Ge</keyname><forenames>Zongyuan</forenames></author><author><keyname>Mccool</keyname><forenames>Christopher</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Corke</keyname><forenames>Peter</forenames></author></authors><title>Subset Feature Learning for Fine-Grained Category Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fine-grained categorisation has been a challenging problem due to small
inter-class variation, large intra-class variation and low number of training
images. We propose a learning system which first clusters visually similar
classes and then learns deep convolutional neural network features specific to
each subset. Experiments on the popular fine-grained Caltech-UCSD bird dataset
show that the proposed method outperforms recent fine-grained categorisation
methods under the most difficult setting: no bounding boxes are presented at
test time. It achieves a mean accuracy of 77.5%, compared to the previous best
performance of 73.2%. We also show that progressive transfer learning allows us
to first learn domain-generic features (for bird classification) which can then
be adapted to specific set of bird classes, yielding improvements in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02286</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02286</id><created>2015-05-09</created><authors><author><keyname>Sichani</keyname><forenames>Arash Kh.</forenames></author><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Covariance Dynamics and Entanglement in Translation Invariant Linear
  Quantum Stochastic Networks</title><categories>quant-ph cs.SY math-ph math.MP math.OC</categories><comments>11 pages, 3 figures, submitted to the 54th IEEE Conference on
  Decision and Control, December 15-18, 2015, Osaka, Japan</comments><msc-class>81Q93, 93A15, 81P40, 81S25, 93E15, 82C10, 82C20, 82B10, 42B10, 15B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a translation invariant network of identical
quantum stochastic systems subjected to external quantum noise. Each node of
the network is directly coupled to a finite number of its neighbours. This
network is modelled as an open quantum harmonic oscillator and is governed by a
set of linear quantum stochastic differential equations. The dynamic variables
of the network satisfy the canonical commutation relations. Similar large-scale
networks can be found, for example, in quantum metamaterials and optical
lattices. Using spatial Fourier transform techniques, we obtain a sufficient
condition for stability of the network in the case of finite interaction range,
and consider a mean square performance index for the stable network in the
thermodynamic limit. The Peres-Horodecki-Simon separability criterion is
employed in order to obtain sufficient and necessary conditions for quantum
entanglement of bipartite systems of nodes of the network in the Gaussian
invariant state. The results on stability and entanglement are extended to the
infinite chain of the linear quantum systems by letting the number of nodes go
to infinity. A numerical example is provided to illustrate the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02288</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02288</id><created>2015-05-09</created><authors><author><keyname>Benavoli</keyname><forenames>Alessio</forenames></author><author><keyname>Corani</keyname><forenames>Giorgio</forenames></author><author><keyname>Mangili</keyname><forenames>Francesca</forenames></author></authors><title>Should we really use post-hoc tests based on mean-ranks?</title><categories>cs.LG math.ST physics.data-an q-bio.QM stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The statistical comparison of multiple algorithms over multiple data sets is
fundamental in machine learning. This is typically carried out by the Friedman
test. When the Friedman test rejects the null hypothesis, multiple comparisons
are carried out to establish which are the significant differences among
algorithms. The multiple comparisons are usually performed using the mean-ranks
test. The aim of this technical note is to discuss the inconsistencies of the
mean-ranks post-hoc test with the goal of discouraging its use in machine
learning as well as in medicine, psychology, etc.. We show that the outcome of
the mean-ranks test depends on the pool of algorithms originally included in
the experiment. In other words, the outcome of the comparison between
algorithms A and B depends also on the performance of the other algorithms
included in the original experiment. This can lead to paradoxical situations.
For instance the difference between A and B could be declared significant if
the pool comprises algorithms C, D, E and not significant if the pool comprises
algorithms F, G, H. To overcome these issues, we suggest instead to perform the
multiple comparison using a test whose outcome only depends on the two
algorithms being compared, such as the sign-test or the Wilcoxon signed-rank
test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02294</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02294</id><created>2015-05-09</created><updated>2015-11-30</updated><authors><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author><author><keyname>Chen</keyname><forenames>Sheng</forenames></author><author><keyname>Fazayeli</keyname><forenames>Farideh</forenames></author><author><keyname>Sivakumar</keyname><forenames>Vidyashankar</forenames></author></authors><title>Estimation with Norm Regularization</title><categories>stat.ML cs.LG</categories><comments>Fixed technical issues. Generalized some results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of non-asymptotic estimation error and structured statistical
recovery based on norm regularized regression, such as Lasso, needs to consider
four aspects: the norm, the loss function, the design matrix, and the noise
model. This paper presents generalizations of such estimation error analysis on
all four aspects compared to the existing literature. We characterize the
restricted error set where the estimation error vector lies, establish
relations between error sets for the constrained and regularized problems, and
present an estimation error bound applicable to any norm. Precise
characterizations of the bound is presented for isotropic as well as
anisotropic subGaussian design matrices, subGaussian noise models, and convex
loss functions, including least squares and generalized linear models. Generic
chaining and associated results play an important role in the analysis. A key
result from the analysis is that the sample complexity of all such estimators
depends on the Gaussian width of a spherical cap corresponding to the
restricted error set. Further, once the number of samples $n$ crosses the
required sample complexity, the estimation error decreases as
$\frac{c}{\sqrt{n}}$, where $c$ depends on the Gaussian width of the unit norm
ball.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02298</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02298</id><created>2015-05-09</created><updated>2015-10-31</updated><authors><author><keyname>Bakst</keyname><forenames>Alexander</forenames></author><author><keyname>Jhala</keyname><forenames>Ranjit</forenames></author></authors><title>Predicate Abstraction for Linked Data Structures</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Alias Refinement Types (ART), a new approach to the verification
of correctness properties of linked data structures. While there are many
techniques for checking that a heap-manipulating program adheres to its
specification, they often require that the programmer annotate the behavior of
each procedure, for example, in the form of loop invariants and pre- and
post-conditions. Predicate abstraction would be an attractive abstract domain
for performing invariant inference, existing techniques are not able to reason
about the heap with enough precision to verify functional properties of data
structure manipulating programs. In this paper, we propose a technique that
lifts predicate abstraction to the heap by factoring the analysis of data
structures into two orthogonal components: (1) Alias Types, which reason about
the physical shape of heap structures, and (2) Refinement Types, which use
simple predicates from an SMT decidable theory to capture the logical or
semantic properties of the structures. We prove ART sound by translating types
into separation logic assertions, thus translating typing derivations in ART
into separation logic proofs. We evaluate ART by implementing a tool that
performs type inference for an imperative language, and empirically show, using
a suite of data-structure benchmarks, that ART requires only 21% of the
annotations needed by other state-of-the-art verification techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02305</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02305</id><created>2015-05-09</created><authors><author><keyname>Lumsdaine</keyname><forenames>Robin L.</forenames></author><author><keyname>Rockmore</keyname><forenames>Daniel N.</forenames></author><author><keyname>Foti</keyname><forenames>Nicholas</forenames></author><author><keyname>Leibon</keyname><forenames>Gregory</forenames></author><author><keyname>Farmer</keyname><forenames>J. Doyne</forenames></author></authors><title>The Intrafirm Complexity of Systemically Important Financial
  Institutions</title><categories>q-fin.GN cs.SI physics.soc-ph</categories><comments>33 pages, 7 Figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In November, 2011, the Financial Stability Board, in collaboration with the
International Monetary Fund, published a list of 29 &quot;systemically important
financial institutions&quot; (SIFIs). This designation reflects a concern that the
failure of any one of them could have dramatic negative consequences for the
global economy and is based on &quot;their size, complexity, and systemic
interconnectedness&quot;. While the characteristics of &quot;size&quot; and &quot;systemic
interconnectedness&quot; have been the subject of a good deal of quantitative
analysis, less attention has been paid to measures of a firm's &quot;complexity.&quot; In
this paper we take on the challenges of measuring the complexity of a financial
institution and to that end explore the use of the structure of an individual
firm's control hierarchy as a proxy for institutional complexity. The control
hierarchy is a network representation of the institution and its subsidiaries.
We show that this mathematical representation (and various associated metrics)
provides a consistent way to compare the complexity of firms with often very
disparate business models and as such may provide the foundation for
determining a SIFI designation. By quantifying the level of complexity of a
firm, our approach also may prove useful should firms need to reduce their
level of complexity either in response to business or regulatory needs. Using a
data set containing the control hierarchies of many of the designated SIFIs, we
find that in the past two years, these firms have decreased their level of
complexity, perhaps in response to regulatory requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02310</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02310</id><created>2015-05-09</created><updated>2015-11-25</updated><authors><author><keyname>Ganti</keyname><forenames>Radha K.</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>Asymptotics and Approximation of the SIR Distribution in General
  Cellular Networks</title><categories>cs.IT cs.NI cs.SI math.IT math.PR</categories><comments>Accepted at IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has recently been observed that the SIR distributions of a variety of
cellular network models and transmission techniques look very similar in shape.
As a result, they are well approximated by a simple horizontal shift (or gain)
of the distribution of the most tractable model, the Poisson point process
(PPP). To study and explain this behavior, this paper focuses on general
single-tier network models with nearest-base station association and studies
the asymptotic gain both at 0 and at infinity.
  We show that the gain at 0 is determined by the so-called mean
interference-to-signal ratio (MISR) between the PPP and the network model under
consideration, while the gain at infinity is determined by the expected
fading-to-interference ratio (EFIR).
  The analysis of the MISR is based on a novel type of point process, the
so-called relative distance process, which is a one-dimensional point process
on the unit interval [0,1] that fully determines the SIR. A comparison of the
gains at 0 and infinity shows that the gain at 0 indeed provides an excellent
approximation for the entire SIR distribution. Moreover, the gain is mostly a
function of the network geometry and barely depends on the path loss exponent
and the fading. The results are illustrated using several examples of repulsive
point processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02318</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02318</id><created>2015-05-09</created><updated>2015-07-07</updated><authors><author><keyname>Kullmann</keyname><forenames>Oliver</forenames></author><author><keyname>Zhao</keyname><forenames>Xishun</forenames></author></authors><title>Parameters for minimal unsatisfiability: Smarandache primitive numbers
  and full clauses</title><categories>cs.DM cs.LO math.CO</categories><comments>19 pages; second version with more explanations and examples</comments><msc-class>68R99 (Primary), 03B05, 05D99 (Secondary)</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a new bridge between propositional logic and elementary number
theory. The main objects are &quot;minimally unsatisfiable clause-sets&quot;, short
&quot;MUs&quot;, unsatisfiable conjunctive normal forms rendered satisfiable by
elimination of any clause. In other words, irredundant coverings of the boolean
hypercube by subcubes. The main parameter for MUs is the &quot;deficiency&quot; k, the
difference between the number of clauses and the number of variables (the
difference between the number of elements in the covering and the dimension of
the hypercube), and the fundamental fact is that k &gt;= 1 holds.
  A &quot;full clause&quot; in an MU contains all variables (corresponding to a singleton
in the covering). We show the lower bound S_2(k) &lt;= FCM(k), where FCM(k) is the
maximal number of full clauses in MUs of deficiency k, while S_2(k) is the
smallest n such that 2^k divides n!.
  The proof rests on two methods: On the logic-combinatorial side, applying
subsumption resolution and its inverse, a fundamental method since Boole in
1854 introduced the &quot;expansion method&quot;. On the arithmetical side, analysing
certain recursions, combining an application-specific recursion with a
recursion from the field of meta-Fibonacci sequences (indeed S_2 equals twice
the Conolly sequence).
  A further tool is the consideration of unsatisfiable &quot;hitting clause-sets&quot;
(UHITs), special cases of MUs, which correspond to the partitions of the
boolean hypercube by subcubes; they are also known as orthogonal or disjoint
DNF tautologies. We actually show the sharper lower bound S_2(k) &lt;= FCH(k),
where FCH(k) is the maximal number of full clauses in UHITs of deficiency k. We
conjecture that for all k holds S_2(k) = FCH(k), which would establish a
surprising connection between the extremal combinatorics of (un)satisfiability
and elementary number theory.
  We apply the lower bound to analyse the structure of MUs and UHITs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02322</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02322</id><created>2015-05-09</created><authors><author><keyname>Lempi&#xe4;inen</keyname><forenames>Tuomo</forenames></author></authors><title>Ability to Count Is Worth $\Theta(\Delta)$ Rounds</title><categories>cs.DC cs.CC cs.DM</categories><comments>23 pages, 6 figures</comments><msc-class>68Q05 (Primary) 68Q15, 68Q17, 68Q85, 05C78 (Secondary)</msc-class><acm-class>F.1.1; F.1.3; F.4.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hella et al. (PODC 2012, Distributed Computing 2015) identified seven
different models of distributed computing - one of which is the port-numbering
model - and provided a complete classification of their computational power
relative to each other. However, one of their simulation results involves an
additive overhead of $2\Delta-2$ communication rounds, and it was not clear, if
this is actually optimal. In this paper we give a positive answer: there is a
matching linear-in-$\Delta$ lower bound. This closes the final gap in our
understanding of the models, with respect to the number of communication
rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02324</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02324</id><created>2015-05-09</created><updated>2015-09-06</updated><authors><author><keyname>Hasnat</keyname><forenames>Md. Abul</forenames></author><author><keyname>Velcin</keyname><forenames>Julien</forenames></author><author><keyname>Bonnevay</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Jacques</keyname><forenames>Julien</forenames></author></authors><title>Simultaneous Clustering and Model Selection for Multinomial
  Distribution: A Comparative Study</title><categories>cs.LG stat.ME stat.ML</categories><comments>Accepted in the International Symposium on Intelligent Data Analysis
  (IDA 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study different discrete data clustering methods, which use
the Model-Based Clustering (MBC) framework with the Multinomial distribution.
Our study comprises several relevant issues, such as initialization, model
estimation and model selection. Additionally, we propose a novel MBC method by
efficiently combining the partitional and hierarchical clustering techniques.
We conduct experiments on both synthetic and real data and evaluate the methods
using accuracy, stability and computation time. Our study identifies
appropriate strategies to be used for discrete data analysis with the MBC
methods. Moreover, our proposed method is very competitive w.r.t. clustering
accuracy and better w.r.t. stability and computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02325</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02325</id><created>2015-05-09</created><authors><author><keyname>Khouzani</keyname><forenames>MHR</forenames></author><author><keyname>Mardziel</keyname><forenames>Piotr</forenames></author><author><keyname>Cid</keyname><forenames>Carlos</forenames></author><author><keyname>Srivatsa</keyname><forenames>Mudhakar</forenames></author></authors><title>Picking vs. Guessing Secrets: A Game-Theoretic Analysis (Technical
  Report)</title><categories>cs.GT cs.CR math.OC</categories><comments>This manuscript is the extended version of our conference paper:
  &quot;Picking vs. Guessing Secrets: A Game-Theoretic Analysis&quot;, in 28th IEEE
  Computer Security Foundations Symposium (CSF 2015), Verona, Italy, July 2015</comments><msc-class>91A05, 91A80</msc-class><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Choosing a hard-to-guess secret is a prerequisite in many security
applications. Whether it is a password for user authentication or a secret key
for a cryptographic primitive, picking it requires the user to trade-off
usability costs with resistance against an adversary: a simple password is
easier to remember but is also easier to guess; likewise, a shorter
cryptographic key may require fewer computational and storage resources but it
is also easier to attack. A fundamental question is how one can optimally
resolve this trade-off. A big challenge is the fact that an adversary can also
utilize the knowledge of such usability vs. security trade-offs to strengthen
its attack. In this paper, we propose a game-theoretic framework for analyzing
the optimal trade-offs in the face of strategic adversaries. We consider two
types of adversaries: those limited in their number of tries, and those that
are ruled by the cost of making individual guesses. For each type, we derive
the mutually-optimal decisions as Nash Equilibria, the strategically
pessimistic decisions as maximin, and optimal commitments as Strong Stackelberg
Equilibria of the game. We establish that when the adversaries are faced with a
capped number of guesses, the user's optimal trade-off is a uniform
randomization over a subset of the secret domain. On the other hand, when the
attacker strategy is ruled by the cost of making individual guesses, Nash
Equilibria may completely fail to provide the user with any level of security,
signifying the crucial role of credible commitment for such cases. We
illustrate our results using numerical examples based on real-world samples and
discuss some policy implications of our work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02343</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02343</id><created>2015-05-10</created><authors><author><keyname>Zhao</keyname><forenames>Qibin</forenames></author><author><keyname>Zhang</keyname><forenames>Liqing</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Bayesian Sparse Tucker Models for Dimension Reduction and Tensor
  Completion</title><categories>cs.LG cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tucker decomposition is the cornerstone of modern machine learning on
tensorial data analysis, which have attracted considerable attention for
multiway feature extraction, compressive sensing, and tensor completion. The
most challenging problem is related to determination of model complexity (i.e.,
multilinear rank), especially when noise and missing data are present. In
addition, existing methods cannot take into account uncertainty information of
latent factors, resulting in low generalization performance. To address these
issues, we present a class of probabilistic generative Tucker models for tensor
decomposition and completion with structural sparsity over multilinear latent
space. To exploit structural sparse modeling, we introduce two group sparsity
inducing priors by hierarchial representation of Laplace and Student-t
distributions, which facilitates fully posterior inference. For model learning,
we derived variational Bayesian inferences over all model (hyper)parameters,
and developed efficient and scalable algorithms based on multilinear
operations. Our methods can automatically adapt model complexity and infer an
optimal multilinear rank by the principle of maximum lower bound of model
evidence. Experimental results and comparisons on synthetic, chemometrics and
neuroimaging data demonstrate remarkable performance of our models for
recovering ground-truth of multilinear rank and missing entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02348</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02348</id><created>2015-05-10</created><updated>2015-05-18</updated><authors><author><keyname>Shamrani</keyname><forenames>Mohammed</forenames></author><author><keyname>Major</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Waldisp&#xfc;hl</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Evolution by Computational Selection</title><categories>cs.SI physics.soc-ph q-bio.MN</categories><comments>11 pages, 2 figures, 3 tables, 1 theorem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complexity-theoretic approach to studying biological networks is proposed.
A simple graph representation is used where molecules (DNA, RNA, proteins and
chemicals) are vertices and relations between them are directed and signed
(promotional (+) or inhibitory (-)) edges. Based on this model, the problem of
network evolution (NE) is defined formally as an optimization problem and
subsequently proven to be fundamentally hard (NP-hard) by means of reduction
from the Knapsack problem (KP). Second, for empirical validation, various
biological networks of experimentally-validated interactions are compared
against randomly generated networks with varying degree distributions. An NE
instance is created using a given real or synthetic (random) network. After
being reverse-reduced to a KP instance, each NE instance is fed to a KP solver
and the average achieved knapsack value-to-weight ratio is recorded from
multiple rounds of simulated evolutionary pressure. The results show that
biological networks (and synthetic networks of similar degree distribution)
achieve the highest ratios at maximal evolutionary pressure and minimal error
tolerance conditions. The more distant (in degree distribution) a synthetic
network is from biological networks the lower its achieved ratio. The results
shed light on how computational intractability has shaped the evolution of
biological networks into their current topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02358</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02358</id><created>2015-05-10</created><authors><author><keyname>Vaidhiyan</keyname><forenames>Nidhin Koshy</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Active Search with a Cost for Switching Actions</title><categories>cs.IT math.IT</categories><comments>8 pages. Presented at 2015 Information Theory and Applications
  Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active Sequential Hypothesis Testing (ASHT) is an extension of the classical
sequential hypothesis testing problem with controls. Chernoff (Ann. Math.
Statist., 1959) proposed a policy called Procedure A and showed its asymptotic
optimality as the cost of sampling was driven to zero. In this paper we study a
further extension where we introduce costs for switching of actions. We show
that a modification of Chernoff's Procedure A, one that we call Sluggish
Procedure A, is asymptotically optimal even with switching costs. The growth
rate of the total cost, as the probability of false detection is driven to
zero, and as a switching parameter of the Sluggish Procedure A is driven down
to zero, is the same as that without switching costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02362</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02362</id><created>2015-05-10</created><authors><author><keyname>Vaidhiyan</keyname><forenames>Nidhin Koshy</forenames></author><author><keyname>Arun</keyname><forenames>S. P.</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Neural dissimilarity indices that predict oddball detection in behaviour</title><categories>cs.IT math.IT</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuroscientists have recently shown that images that are difficult to find in
visual search elicit similar patterns of firing across a population of recorded
neurons. The $L^{1}$ distance between firing rate vectors associated with two
images was strongly correlated with the inverse of decision time in behaviour.
But why should decision times be correlated with $L^{1}$ distance? What is the
decision-theoretic basis? In our decision theoretic formulation, we modeled
visual search as an active sequential hypothesis testing problem with switching
costs. Our analysis suggests an appropriate neuronal dissimilarity index which
correlates equally strongly with the inverse of decision time as the $L^{1}$
distance. We also consider a number of other possibilities such as the relative
entropy (Kullback-Leibler divergence) and the Chernoff entropy of the firing
rate distributions. A more stringent test of equality of means, which would
have provided a strong backing for our modeling fails for our proposed as well
as the other already discussed dissimilarity indices. However, test statistics
from the equality of means test, when used to rank the indices in terms of
their ability to explain the observed results, places our proposed
dissimilarity index at the top followed by relative entropy, Chernoff entropy
and the $L^{1}$ indices. Computations of the different indices requires an
estimate of the relative entropy between two Poisson point processes. An
estimator is developed and is shown to have near unbiased performance for
almost all operating regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02368</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02368</id><created>2015-05-10</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Sharif</keyname><forenames>Bayan S.</forenames></author></authors><title>Solutions to Integrals Involving the Marcum Q-Function and Applications</title><categories>cs.IT math.IT</categories><comments>15 Pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel analytic solutions are derived for integrals that involve the
generalized Marcum Q-function, exponential functions and arbitrary powers.
Simple closed-form expressions are also derived for the specific cases of the
generic integrals. The offered expressions are both convenient and versatile,
which is particularly useful in applications relating to natural sciences and
engineering, including wireless cpmmunications and signal processing. To this
end, they are employed in the derivation of the channel capacity for fixed rate
and channel inversion in the case of correlated multipath fading and switched
diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02371</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02371</id><created>2015-05-10</created><updated>2015-08-02</updated><authors><author><keyname>Kullmann</keyname><forenames>Oliver</forenames></author><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames></author></authors><title>Computing maximal autarkies with few and simple oracle queries</title><categories>cs.LO cs.DM cs.DS</categories><comments>18 pages; second version with editorial changes, to appear in LNCS
  for Theory and Applications of Satisfiability Testing - SAT 2015</comments><msc-class>68R99 (Primary) 03B05, 05D99 (Secondary)</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the algorithmic task of computing a maximal autarky for a
clause-set F, i.e., a partial assignment which satisfies every clause of F it
touches, and where this property is destroyed by adding any non-empty set of
further assignments. We employ SAT solvers as oracles, using various
capabilities. Using the standard SAT oracle, log_2(n(F)) oracle calls suffice,
where n(F) is the number of variables, but the drawback is that (translated)
cardinality constraints are employed, which makes this approach less efficient
in practice. Using an extended SAT oracle, motivated by the capabilities of
modern SAT solvers, we show how to compute maximal autarkies with 2 n(F)^{1/2}
simpler oracle calls. This novel algorithm combines the previous two main
approaches, based on the autarky-resolution duality and on SAT translations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02372</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02372</id><created>2015-05-10</created><updated>2015-05-30</updated><authors><author><keyname>Zakablukov</keyname><forenames>Dmitry</forenames></author></authors><title>On Asymptotic Gate Complexity and Depth of Reversible Circuits With
  Additional Memory</title><categories>cs.CC cs.ET</categories><comments>In English, 19 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:1504.06876</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible logic can be used in various research areas, e.g. quantum
computation, cryptography and signal processing. In this paper we study
reversible logic circuits with additional inputs, consisting of NOT, CNOT and
2-CNOT gates. We consider a set $F(n,q)$ of all transformations $\mathbb Z_2^n
\to \mathbb Z_2^n$ that can be implemented by reversible circuits with $(n+q)$
inputs. An analogue of Lupanov's method for the synthesis of reversible logic
circuits with additional memory is described. We prove upper asymptotic bounds
for the Shannon gate complexity function $L(n,q)$ and the depth function
$D(n,q)$ in case of $q &gt; 0$: $L(n,q_0) \lesssim 2^n$ if $q_0 \sim n 2^{n-o(n)}$
and $D(n,q_1) \lesssim 3n$ if $q_1 \sim 2^n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02377</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02377</id><created>2015-05-10</created><authors><author><keyname>Liao</keyname><forenames>Renjie</forenames></author><author><keyname>Shi</keyname><forenames>Jianping</forenames></author><author><keyname>Ma</keyname><forenames>Ziyang</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Jia</keyname><forenames>Jiaya</forenames></author></authors><title>Bounded-Distortion Metric Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric learning aims to embed one metric space into another to benefit tasks
like classification and clustering. Although a greatly distorted metric space
has a high degree of freedom to fit training data, it is prone to overfitting
and numerical inaccuracy. This paper presents {\it bounded-distortion metric
learning} (BDML), a new metric learning framework which amounts to finding an
optimal Mahalanobis metric space with a bounded-distortion constraint. An
efficient solver based on the multiplicative weights update method is proposed.
Moreover, we generalize BDML to pseudo-metric learning and devise the
semidefinite relaxation and a randomized algorithm to approximately solve it.
We further provide theoretical analysis to show that distortion is a key
ingredient for stability and generalization ability of our BDML algorithm.
Extensive experiments on several benchmark datasets yield promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02385</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02385</id><created>2015-05-10</created><authors><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>Lin</keyname><forenames>Pin-Hsun</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Energy Efficiency in Secure Multi-Antenna Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of resource allocation in multiple-antenna wiretap channels is
investigated, wherein a malicious user tries to eavesdrop the communication
between two legitimate users. Both multiple input single output single-antenna
eavesdropper (MISO-SE) and multiple input multiple output multiple-antenna
eavesdropper (MIMO-ME) systems are considered. Unlike most papers dealing with
physical layer security, the focus of the resource allocation process here is
not to maximize the secrecy capacity, but rather to maximize the energy
efficiency of the system. Two fractional energy-efficient metrics are
introduced, namely the ratios between the system secrecy capacity and the
consumed power, and between the system secret-key rate and the consumed power.
Both performance metrics are measured in bit/Joule, and result in non-concave
fractional optimization problems, which are tackled by fractional programming
theory and sequential convex optimization. For both performance metrics, the
energy-efficient resource allocation is carried out considering both perfect as
well as statistical channel state information (CSI) as to the channel from the
legitimate transmitter to the eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02388</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02388</id><created>2015-05-10</created><authors><author><keyname>Abdelnasser</keyname><forenames>Heba</forenames></author><author><keyname>Harras</keyname><forenames>Khaled A.</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>UbiBreathe: A Ubiquitous non-Invasive WiFi-based Breathing Estimator</title><categories>cs.OH</categories><comments>Accepted for publication in MobiHoc 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monitoring breathing rates and patterns helps in the diagnosis and potential
avoidance of various health problems. Current solutions for respiratory
monitoring, however, are usually invasive and/or limited to medical facilities.
In this paper, we propose a novel respiratory monitoring system, UbiBreathe,
based on ubiquitous off-the-shelf WiFi-enabled devices. Our experiments show
that the received signal strength (RSS) at a WiFi-enabled device held on a
person's chest is affected by the breathing process. This effect extends to
scenarios when the person is situated on the line-of-sight (LOS) between the
access point and the device, even without holding it. UbiBreathe leverages
these changes in the WiFi RSS patterns to enable ubiquitous non-invasive
respiratory rate estimation, as well as apnea detection.
  We propose the full architecture and design for UbiBreathe, incorporating
various modules that help reliably extract the hidden breathing signal from a
noisy WiFi RSS. The system handles various challenges such as noise
elimination, interfering humans, sudden user movements, as well as detecting
abnormal breathing situations. Our implementation of UbiBreathe using
off-the-shelf devices in a wide range of environmental conditions shows that it
can estimate different breathing rates with less than 1 breaths per minute
(bpm) error. In addition, UbiBreathe can detect apnea with more than 96%
accuracy in both the device-on-chest and hands-free scenarios. This highlights
its suitability for a new class of anywhere respiratory monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02394</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02394</id><created>2015-05-10</created><authors><author><keyname>Stepanov</keyname><forenames>S. Y.</forenames></author></authors><title>Processing heterogeneous data space measurement subpolar territories to
  formulate stochastic models assessment geohazards</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article describes the development of a knowledge base of forecasting the
state of the individual parameters geoenvironment circumpolar territory using
satellite data. The paper provides an analysis of the parameters characterizing
the state of polar territory. Select a model and method for processing geodata.
Developing an information system of formation of the knowledge base, taking
into account development trends (predicting) the values of these parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02399</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02399</id><created>2015-05-10</created><authors><author><keyname>Weng</keyname><forenames>Lilian</forenames></author><author><keyname>Karsai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>Attention on Weak Ties in Social and Communication Networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Granovetter's weak tie theory of social networks is built around two central
hypotheses. The first states that strong social ties carry the large majority
of interaction events; the second maintains that weak social ties, although
less active, are often relevant for the exchange of especially important
information (e.g., about potential new jobs in Granovetter's work). While
several empirical studies have provided support for the first hypothesis, the
second has been the object of far less scrutiny. A possible reason is that it
involves notions relative to the nature and importance of the information that
are hard to quantify and measure, especially in large scale studies. Here, we
search for empirical validation of both Granovetter's hypotheses. We find clear
empirical support for the first. We also provide empirical evidence and a
quantitative interpretation for the second. We show that attention, measured as
the fraction of interactions devoted to a particular social connection, is high
on weak ties --- possibly reflecting the postulated informational purposes of
such ties --- but also on very strong ties. Data from online social media and
mobile communication reveal network-dependent mixtures of these two effects on
the basis of a platform's typical usage. Our results establish a clear
relationships between attention, importance, and strength of social links, and
could lead to improved algorithms to prioritize social media content
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02402</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02402</id><created>2015-05-10</created><authors><author><keyname>Ponomarev</keyname><forenames>Anton</forenames></author></authors><title>Reduction-Based Robustness Analysis of Linear Predictor Feedback for
  Distributed Input Delays</title><categories>math.OC cs.SY</categories><journal-ref>IEEE Transactions on Automatic Control, 2016, vol. 61, no. 2, pp.
  468-472</journal-ref><doi>10.1109/TAC.2015.2437520</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lyapunov-Krasovskii approach is applied to parameter- and delay-robustness
analysis of the feedback suggested by Manitius and Olbrot for a linear
time-invariant system with distributed input delay. A functional is designed
based on Artstein's system reduction technique. It depends on the norms of the
reduction-transformed plant state and original actuator state. The functional
is used to prove that the feedback is stabilizing when there is a slight
mismatch in the system matrices and delay values between the plant and
controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02403</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02403</id><created>2015-05-10</created><updated>2015-05-12</updated><authors><author><keyname>Abbes</keyname><forenames>Samy</forenames></author></authors><title>Representation of traces by vectors of words</title><categories>cs.DM</categories><comments>Withdrawn since the results are known, published in the 1985 RAIRO
  paper by Cori and Perrin (thanks to A. Muscholl for pointing that out to me)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that every trace monoid is isomorphic to a sub-monoid of a monoid of
word vectors. It provides a concrete representation of the elements of a trace
monoid as processes associated with a resource sharing mechanism. We illustrate
this representation by obtaining some results on the ordering structure of the
left divisibility relation on trace monoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02405</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02405</id><created>2015-05-10</created><authors><author><keyname>Neves</keyname><forenames>Miguel</forenames></author><author><keyname>Martins</keyname><forenames>Ruben</forenames></author><author><keyname>Janota</keyname><forenames>Mikol&#xe1;&#x161;</forenames></author><author><keyname>Lynce</keyname><forenames>In&#xea;s</forenames></author><author><keyname>Manquinho</keyname><forenames>Vasco</forenames></author></authors><title>Exploiting Resolution-based Representations for MaxSAT Solving</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most recent MaxSAT algorithms rely on a succession of calls to a SAT solver
in order to find an optimal solution. In particular, several algorithms take
advantage of the ability of SAT solvers to identify unsatisfiable subformulas.
Usually, these MaxSAT algorithms perform better when small unsatisfiable
subformulas are found early. However, this is not the case in many problem
instances, since the whole formula is given to the SAT solver in each call. In
this paper, we propose to partition the MaxSAT formula using a resolution-based
graph representation. Partitions are then iteratively joined by using a
proximity measure extracted from the graph representation of the formula. The
algorithm ends when only one partition remains and the optimal solution is
found. Experimental results show that this new approach further enhances a
state of the art MaxSAT solver to optimally solve a larger set of industrial
problem instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02406</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02406</id><created>2015-05-10</created><authors><author><keyname>Argerich</keyname><forenames>Luis</forenames></author></authors><title>EntropyWalker, a Fast Algorithm for Small Community Detection in Large
  Graphs</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents a very simple algorithm for overlaping
community-detection in large graphs under constraints such as the minimum and
maximum number of members allowed. The algorithm is based on the simulation of
random walks and measures the entropy of each random walk to detect the
discovery of a community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02408</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02408</id><created>2015-05-10</created><authors><author><keyname>Neves</keyname><forenames>Miguel</forenames></author><author><keyname>Lynce</keyname><forenames>In&#xea;s</forenames></author><author><keyname>Manquinho</keyname><forenames>Vasco</forenames></author></authors><title>DistMS: A Non-Portfolio Distributed Solver for Maximum Satisfiability</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most successful parallel SAT and MaxSAT solvers follow a portfolio
approach, where each thread applies a different algorithm (or the same
algorithm configured differently) to solve a given problem instance. The main
goal of building a portfolio is to diversify the search process being carried
out by each thread. As soon as one thread finishes, the instance can be deemed
solved. In this paper we present a new open source distributed solver for
MaxSAT solving that addresses two issues commonly found in multicore parallel
solvers, namely memory contention and scalability. Preliminary results show
that our non-portfolio distributed MaxSAT solver outperforms its sequential
version and is able to solve more instances as the number of processes
increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02414</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02414</id><created>2015-05-10</created><authors><author><keyname>Chessa</keyname><forenames>Michela</forenames></author><author><keyname>Grossklags</keyname><forenames>Jens</forenames></author><author><keyname>Loiseau</keyname><forenames>Patrick</forenames></author></authors><title>A Game-Theoretic Study on Non-Monetary Incentives in Data Analytics
  Projects with Privacy Implications</title><categories>cs.GT cs.CR cs.CY</categories><msc-class>91A80, 91A10</msc-class><acm-class>K.4.1; K.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of personal information contributed by individuals to digital
repositories such as social network sites has grown substantially. The
existence of this data offers unprecedented opportunities for data analytics
research in various domains of societal importance including medicine and
public policy. The results of these analyses can be considered a public good
which benefits data contributors as well as individuals who are not making
their data available. At the same time, the release of personal information
carries perceived and actual privacy risks to the contributors. Our research
addresses this problem area. In our work, we study a game-theoretic model in
which individuals take control over participation in data analytics projects in
two ways: 1) individuals can contribute data at a self-chosen level of
precision, and 2) individuals can decide whether they want to contribute at all
(or not). From the analyst's perspective, we investigate to which degree the
research analyst has flexibility to set requirements for data precision, so
that individuals are still willing to contribute to the project, and the
quality of the estimation improves. We study this tradeoff scenario for
populations of homogeneous and heterogeneous individuals, and determine Nash
equilibria that reflect the optimal level of participation and precision of
contributions. We further prove that the analyst can substantially increase the
accuracy of the analysis by imposing a lower bound on the precision of the data
that users can reveal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02417</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02417</id><created>2015-05-10</created><updated>2015-10-19</updated><authors><author><keyname>Toulis</keyname><forenames>Panos</forenames></author><author><keyname>Tran</keyname><forenames>Dustin</forenames></author><author><keyname>Airoldi</keyname><forenames>Edoardo M.</forenames></author></authors><title>Towards stability and optimality in stochastic gradient descent</title><categories>stat.ME cs.LG stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative procedures for parameter estimation based on stochastic gradient
descent allow the estimation to scale to massive data sets. However, in both
theory and practice, they suffer from numerical instability. Moreover, they are
statistically inefficient as estimators of the true parameter value. To address
these two issues, we propose a new iterative procedure termed averaged implicit
stochastic gradient descent (AI-SGD). For statistical efficiency, AISGD employs
averaging of the iterates, which achieves the optimal Cram\'{e}r-Rao bound
under strong convexity, i.e., it is an optimal unbiased estimator of the true
parameter value. For numerical stability, AISGD employs an implicit update at
each iteration, which is related to proximal operators in optimization. In
practice, AISGD achieves competitive performance with state-of-the-art
procedures. Furthermore, it is more stable than averaging procedures that do
not employ proximal operators, and is simpler to implement than procedures that
do employ proximal operators but require careful tuning of several
hyperparameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02419</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02419</id><created>2015-05-10</created><updated>2015-09-14</updated><authors><author><keyname>Gormley</keyname><forenames>Matthew R.</forenames></author><author><keyname>Yu</keyname><forenames>Mo</forenames></author><author><keyname>Dredze</keyname><forenames>Mark</forenames></author></authors><title>Improved Relation Extraction with Feature-Rich Compositional Embedding
  Models</title><categories>cs.CL cs.AI cs.LG</categories><comments>12 pages for EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compositional embedding models build a representation (or embedding) for a
linguistic structure based on its component word embeddings. We propose a
Feature-rich Compositional Embedding Model (FCM) for relation extraction that
is expressive, generalizes to new domains, and is easy-to-implement. The key
idea is to combine both (unlexicalized) hand-crafted features with learned word
embeddings. The model is able to directly tackle the difficulties met by
traditional compositional embeddings models, such as handling arbitrary types
of sentence annotations and utilizing global information for composition. We
test the proposed model on two relation extraction tasks, and demonstrate that
our model outperforms both previous compositional models and traditional
feature rich models on the ACE 2005 relation extraction task, and the SemEval
2010 relation classification task. The combination of our model and a
log-linear classifier with hand-crafted features gives state-of-the-art
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02420</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02420</id><created>2015-05-10</created><authors><author><keyname>Guiraldelli</keyname><forenames>Ricardo Henrique Gracini</forenames></author><author><keyname>Manca</keyname><forenames>Vincenzo</forenames></author></authors><title>The Computational Universality of Metabolic Computing</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  System and synthetic biology are rapidly evolving systems, but both lack
tools such as those used in engineering environments to shift the their focus
from the design of parts (details) to the design of systems (behaviors); to
aggravate, there are insufficient theoretical justifications on the
computational limits of biological systems. To diminish these deficiencies, we
present theoretical results over the Turing-equivalence of metabolic systems,
defines rules for translations of algorithms into metabolic P systems and
presents a software tool to assist the task in an automatic way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02425</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02425</id><created>2015-05-10</created><authors><author><keyname>Heilman</keyname><forenames>Michael</forenames></author><author><keyname>Sagae</keyname><forenames>Kenji</forenames></author></authors><title>Fast Rhetorical Structure Theory Discourse Parsing</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, There has been a variety of research on discourse parsing,
particularly RST discourse parsing. Most of the recent work on RST parsing has
focused on implementing new types of features or learning algorithms in order
to improve accuracy, with relatively little focus on efficiency, robustness, or
practical use. Also, most implementations are not widely available. Here, we
describe an RST segmentation and parsing system that adapts models and feature
sets from various previous work, as described below. Its accuracy is near
state-of-the-art, and it was developed to be fast, robust, and practical. For
example, it can process short documents such as news articles or essays in less
than a second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02428</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02428</id><created>2015-05-10</created><authors><author><keyname>Kulesz</keyname><forenames>Daniel</forenames></author><author><keyname>Toth</keyname><forenames>Fabian</forenames></author><author><keyname>Beck</keyname><forenames>Fabian</forenames></author></authors><title>Live Inspection of Spreadsheets</title><categories>cs.SE</categories><comments>In Proceedings of the 2nd Workshop on Software Engineering Methods in
  Spreadsheets (http://spreadsheetlab.org/sems15/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing approaches for detecting anomalies in spreadsheets can help to
discover faults, but they are often applied too late in the spreadsheet
lifecycle. By contrast, our approach detects anomalies immediately whenever
users change their spreadsheets. This live inspection approach has been
implemented as part of the Spreadsheet Inspection Framework, enabling the tool
to visually report findings without disturbing the users' workflow. An advanced
list representation allows users to keep track of the latest findings,
prioritize open problems, and check progress on solving the issues. Results
from a first user study indicate that users find the approach useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02433</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02433</id><created>2015-05-10</created><updated>2015-05-22</updated><authors><author><keyname>Fan</keyname><forenames>Miao</forenames></author><author><keyname>Zhou</keyname><forenames>Qiang</forenames></author><author><keyname>Abel</keyname><forenames>Andrew</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author><author><keyname>Grishman</keyname><forenames>Ralph</forenames></author></authors><title>Probabilistic Belief Embedding for Knowledge Base Completion</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1503.08155</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contributes a novel embedding model which measures the probability
of each belief $\langle h,r,t,m\rangle$ in a large-scale knowledge repository
via simultaneously learning distributed representations for entities ($h$ and
$t$), relations ($r$), and the words in relation mentions ($m$). It facilitates
knowledge completion by means of simple vector operations to discover new
beliefs. Given an imperfect belief, we can not only infer the missing entities,
predict the unknown relations, but also tell the plausibility of the belief,
just leveraging the learnt embeddings of remaining evidences. To demonstrate
the scalability and the effectiveness of our model, we conduct experiments on
several large-scale repositories which contain millions of beliefs from
WordNet, Freebase and NELL, and compare it with other cutting-edge approaches
via competing the performances assessed by the tasks of entity inference,
relation prediction and triplet classification with respective metrics.
Extensive experimental results show that the proposed model outperforms the
state-of-the-arts with significant improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02434</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02434</id><created>2015-05-10</created><authors><author><keyname>Dai</keyname><forenames>Zhenwen</forenames></author><author><keyname>Hensman</keyname><forenames>James</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil</forenames></author></authors><title>Spike and Slab Gaussian Process Latent Variable Models</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gaussian process latent variable model (GP-LVM) is a popular approach to
non-linear probabilistic dimensionality reduction. One design choice for the
model is the number of latent variables. We present a spike and slab prior for
the GP-LVM and propose an efficient variational inference procedure that gives
a lower bound of the log marginal likelihood. The new model provides a more
principled approach for selecting latent dimensions than the standard way of
thresholding the length-scale parameters. The effectiveness of our approach is
demonstrated through experiments on real and simulated data. Further, we extend
multi-view Gaussian processes that rely on sharing latent dimensions (known as
manifold relevance determination) with spike and slab priors. This allows a
more principled approach for selecting a subset of the latent space for each
view of data. The extended model outperforms the previous state-of-the-art when
applied to a cross-modal multimedia retrieval task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02435</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02435</id><created>2015-05-10</created><authors><author><keyname>D'Angelo</keyname><forenames>Gabriele</forenames></author><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author></authors><title>Cloud for Gaming</title><categories>cs.DC cs.MM</categories><acm-class>C.2.4; I.6.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud for Gaming refers to the use of cloud computing technologies to build
large-scale gaming infrastructures, with the goal of improving scalability and
responsiveness, improve the user's experience and enable new business models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02438</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02438</id><created>2015-05-10</created><updated>2015-11-24</updated><authors><author><keyname>Tsogkas</keyname><forenames>S.</forenames></author><author><keyname>Kokkinos</keyname><forenames>I.</forenames></author><author><keyname>Papandreou</keyname><forenames>G.</forenames></author><author><keyname>Vedaldi</keyname><forenames>A.</forenames></author></authors><title>Deep Learning for Semantic Part Segmentation with High-Level Guidance</title><categories>cs.CV</categories><comments>11 pages (including references), 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the task of segmenting an object into its parts, or
semantic part segmentation. We start by adapting a state-of-the-art semantic
segmentation system to this task, and show that a combination of a
fully-convolutional Deep CNN system coupled with Dense CRF labelling provides
excellent results for a broad range of object categories. Still, this approach
remains agnostic to high-level constraints between object parts. We introduce
such prior information by means of the Restricted Boltzmann Machine, adapted to
our task and train our model in an discriminative fashion, as a hidden CRF,
demonstrating that prior information can yield additional improvements. We also
investigate the performance of our approach ``in the wild'', without
information concerning the objects' bounding boxes, using an object detector to
guide a multi-scale segmentation scheme. We evaluate the performance of our
approach on the Penn-Fudan and LFW datasets for the tasks of pedestrian parsing
and face labelling respectively. We show superior performance with respect to
competitive methods that have been extensively engineered on these benchmarks,
as well as realistic qualitative results on part segmentation, even for
occluded or deformable objects. We also provide quantitative and extensive
qualitative results on three classes from the PASCAL Parts dataset. Finally, we
show that our multi-scale segmentation scheme can boost accuracy, recovering
segmentations for finer parts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02441</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02441</id><created>2015-05-10</created><updated>2015-05-13</updated><authors><author><keyname>Liu</keyname><forenames>Weimo</forenames></author><author><keyname>Rahman</keyname><forenames>Md Farhadur</forenames></author><author><keyname>Thirumuruganathan</keyname><forenames>Saravanan</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Aggregate Estimations over Location Based Services</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location based services (LBS) have become very popular in recent years. They
range from map services (e.g., Google Maps) that store geographic locations of
points of interests, to online social networks (e.g., WeChat, Sina Weibo,
FourSquare) that leverage user geographic locations to enable various
recommendation functions. The public query interfaces of these services may be
abstractly modeled as a kNN interface over a database of two dimensional points
on a plane: given an arbitrary query point, the system returns the k points in
the database that are nearest to the query point. In this paper we consider the
problem of obtaining approximate estimates of SUM and COUNT aggregates by only
querying such databases via their restrictive public interfaces. We distinguish
between interfaces that return location information of the returned tuples
(e.g., Google Maps), and interfaces that do not return location information
(e.g., Sina Weibo). For both types of interfaces, we develop aggregate
estimation algorithms that are based on novel techniques for precisely
computing or approximately estimating the Voronoi cell of tuples. We discuss a
comprehensive set of real-world experiments for testing our algorithms,
including experiments on Google Maps, WeChat, and Sina Weibo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02444</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02444</id><created>2015-05-10</created><authors><author><keyname>Mazowiecki</keyname><forenames>Filip</forenames></author><author><keyname>Ochremiak</keyname><forenames>Joanna</forenames></author><author><keyname>Witkowski</keyname><forenames>Adam</forenames></author></authors><title>Eliminating Recursion from Monadic Datalog Programs on Trees</title><categories>cs.LO cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of eliminating recursion from monadic datalog programs
on trees with an infinite set of labels. We show that the boundedness problem,
i.e., determining whether a datalog program is equivalent to some nonrecursive
one is undecidable but the decidability is regained if the descendant relation
is disallowed. Under similar restrictions we obtain decidability of the problem
of equivalence to a given nonrecursive program. We investigate the connection
between these two problems in more detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02445</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02445</id><created>2015-05-10</created><updated>2015-08-25</updated><authors><author><keyname>Massara</keyname><forenames>Guido Previde</forenames></author><author><keyname>Di Matteo</keyname><forenames>T.</forenames></author><author><keyname>Aste</keyname><forenames>Tomaso</forenames></author></authors><title>Network Filtering for Big Data: Triangulated Maximally Filtered Graph</title><categories>cs.DS cond-mat.stat-mech cs.IR</categories><comments>16 pages, 7 Figures, 2 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a network-filtering method, the Triangulated Maximally Filtered
Graph (TMFG), that provides an approximate solution to the Weighted Maximal
Planar Graph problem. The underlying idea of TMFG consists in building a
triangulation that maximizes a score function associated with the amount of
information retained by the network. TMFG uses as weights any arbitrary
similarity measure to arrange data into a meaningful network structure that can
be used for clustering, community detection and modeling. The method is fast,
adaptable and scalable to very large datasets, it allows online updating and
learning as new data can be inserted and deleted with combinations of local and
non-local moves. TMFG permits readjustments of the network in consequence of
changes in the strength of the similarity measure. The method is based on local
topological moves and can therefore take advantage of parallel and GPUs
computing. We discuss how this network-filtering method can be used intuitively
and efficiently for big data studies and its significance from an
information-theoretic perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02449</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02449</id><created>2015-05-10</created><authors><author><keyname>Raggi</keyname><forenames>Daniel</forenames></author><author><keyname>Bundy</keyname><forenames>Alan</forenames></author><author><keyname>Grov</keyname><forenames>Gudmund</forenames></author><author><keyname>Pease</keyname><forenames>Alison</forenames></author></authors><title>Automating change of representation for proofs in discrete mathematics</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation determines how we can reason about a specific problem.
Sometimes one representation helps us find a proof more easily than others.
Most current automated reasoning tools focus on reasoning within one
representation. There is, therefore, a need for the development of better tools
to mechanise and automate formal and logically sound changes of representation.
  In this paper we look at examples of representational transformations in
discrete mathematics, and show how we have used Isabelle's Transfer tool to
automate the use of these transformations in proofs. We give a brief overview
of a general theory of transformations that we consider appropriate for
thinking about the matter, and we explain how it relates to the Transfer
package. We show our progress towards developing a general tactic that
incorporates the automatic search for representation within the proving
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02462</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02462</id><created>2015-05-10</created><updated>2015-06-20</updated><authors><author><keyname>Kiwaki</keyname><forenames>Taichi</forenames></author></authors><title>Soft-Deep Boltzmann Machines</title><categories>cs.NE cs.LG stat.ML</categories><comments>Major revision after bug fixes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a layered Boltzmann machine (BM) that can better exploit the
advantages of a distributed representation. It is widely believed that deep BMs
(DBMs) have far greater representational power than its shallow counterpart,
restricted Boltzmann machines (RBMs). However, this expectation on the
supremacy of DBMs over RBMs has not ever been validated in a theoretical
fashion. In this paper, we provide both theoretical and empirical evidences
that the representational power of DBMs can be actually rather limited in
taking advantages of distributed representations. We propose an approximate
measure for the representational power of a BM regarding to the efficiency of a
distributed representation. With this measure, we show a surprising fact that
DBMs can make inefficient use of distributed representations. Based on these
observations, we propose an alternative BM architecture, which we dub soft-deep
BMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed
representations in terms of the measure. Experiments demonstrate that sDBMs
outperform several state-of-the-art models, including DBMs, in generative tasks
on binarized MNIST and Caltech-101 silhouettes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02463</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02463</id><created>2015-05-10</created><updated>2015-11-04</updated><authors><author><keyname>Li</keyname><forenames>Yaliang</forenames></author><author><keyname>Gao</keyname><forenames>Jing</forenames></author><author><keyname>Meng</keyname><forenames>Chuishi</forenames></author><author><keyname>Li</keyname><forenames>Qi</forenames></author><author><keyname>Su</keyname><forenames>Lu</forenames></author><author><keyname>Zhao</keyname><forenames>Bo</forenames></author><author><keyname>Fan</keyname><forenames>Wei</forenames></author><author><keyname>Han</keyname><forenames>Jiawei</forenames></author></authors><title>A Survey on Truth Discovery</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to information explosion, data for the objects of interest can be
collected from increasingly more sources. However, for the same object, there
usually exist conflicts among the collected multi-source information. To tackle
this challenge, truth discovery, which integrates multi-source noisy
information by estimating the reliability of each source, has emerged as a hot
topic. Several truth discovery methods have been proposed for various
scenarios, and they have been successfully applied in diverse application
domains. In this survey, we focus on providing a comprehensive overview of
truth discovery methods, and summarizing them from different aspects. We also
discuss some future directions of truth discovery research. We hope that this
survey will promote a better understanding of the current progress on truth
discovery, and offer some guidelines on how to apply these approaches in
application domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02469</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02469</id><created>2015-05-10</created><updated>2015-05-24</updated><authors><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author><author><keyname>Abeliuk</keyname><forenames>Andres</forenames></author><author><keyname>Berbeglia</keyname><forenames>Franco</forenames></author><author><keyname>Berbeglia</keyname><forenames>Gerardo</forenames></author></authors><title>On the Optimality and Predictability of Cultural Markets with Social
  Influence</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social influence is ubiquitous in cultural markets, from book recommendations
in Amazon, to song popularities in iTunes and the ranking of newspaper articles
in the online edition of the New York Times to mention only a few. Yet social
influence is often presented in a bad light, often because it supposedly
increases market unpredictability.
  Here we study a model of trial-offer markets, in which participants try
products and later decide whether to purchase. We consider a simple policy
which ranks the products by quality when presenting them to market
participants. We show that, in this setting, market efficiency always benefits
from social influence. Moreover, we prove that the market converges almost
surely to a monopoly for the product of highest quality, making the market both
predictable and asymptotically optimal. Computational experiments confirm that
the quality ranking policy identifies &quot;blockbusters&quot; in reasonable time,
outperforms other policies, and is highly predictable. These results indicate
that social influence does not necessarily increase market unpredicatibility.
The outcome really depends on how social influence is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02476</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02476</id><created>2015-05-10</created><updated>2016-01-15</updated><authors><author><keyname>Ma</keyname><forenames>Ling-Ling</forenames></author><author><keyname>Ma</keyname><forenames>Chuang</forenames></author><author><keyname>Zhang</keyname><forenames>Hai-Feng</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Identifying influential spreaders in complex networks based on gravity
  formula</title><categories>physics.soc-ph cs.SI</categories><comments>4 tables and 4 figures, accepted by Physica A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to identify the influential spreaders in social networks is crucial for
accelerating/hindering information diffusion, increasing product exposure,
controlling diseases and rumors, and so on. In this paper, by viewing the
k-shell value of each node as its mass and the shortest path distance between
two nodes as their distance, then inspired by the idea of the gravity formula,
we propose a gravity centrality index to identify the influential spreaders in
complex networks. The comparison between the gravity centrality index and some
well-known centralities, such as degree centrality, betweenness centrality,
closeness centrality, and k-shell centrality, and so forth, indicates that our
method can effectively identify the influential spreaders in real networks as
well as synthetic networks. We also use the classical
Susceptible-Infected-Recovered (SIR) epidemic model to verify the good
performance of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02487</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02487</id><created>2015-05-11</created><authors><author><keyname>Even</keyname><forenames>Caroline</forenames></author><author><keyname>Schutt</keyname><forenames>Andreas</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>A Constraint Programming Approach for Non-Preemptive Evacuation
  Scheduling</title><categories>cs.AI</categories><comments>Submitted to the 21st International Conference on Principles and
  Practice of Constraint Programming (CP 2015). 15 pages + 1 reference page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale controlled evacuations require emergency services to select
evacuation routes, decide departure times, and mobilize resources to issue
orders, all under strict time constraints. Existing algorithms almost always
allow for preemptive evacuation schedules, which are less desirable in
practice. This paper proposes, for the first time, a constraint-based
scheduling model that optimizes the evacuation flow rate (number of vehicles
sent at regular time intervals) and evacuation phasing of widely populated
areas, while ensuring a nonpreemptive evacuation for each residential zone. Two
optimization objectives are considered: (1) to maximize the number of evacuees
reaching safety and (2) to minimize the overall duration of the evacuation.
Preliminary results on a set of real-world instances show that the approach can
produce, within a few seconds, a non-preemptive evacuation schedule which is
either optimal or at most 6% away of the optimal preemptive solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02493</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02493</id><created>2015-05-11</created><updated>2015-06-08</updated><authors><author><keyname>Li</keyname><forenames>Zhengshuo</forenames></author><author><keyname>Guo</keyname><forenames>Qinglai</forenames></author><author><keyname>Sun</keyname><forenames>Hongbin</forenames></author><author><keyname>Wang</keyname><forenames>Jianhui</forenames></author></authors><title>Further Discussions on Sufficient Conditions for Exact Relaxation of
  Complementarity Constraints for Storage-Concerned Economic Dispatch</title><categories>cs.SY</categories><comments>6 pages, 3 figures. Extention work based on the paper that is to
  appear in IEEE Transactions on Power Systems: Z. Li, Q. Guo, H. Sun, J. Wang,
  &quot;Sufficient conditions for exact relaxation of complementarity constraints
  for storage-concerned economic dispatch&quot;. The work presented here with
  possible further improvement and addition is considered to be submitted to
  the IEEE for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Storage-concerned economic dispatch (ED) problems with complementarity
constraints are strongly non-convex and hard to solve because traditional
Karush-Kuhn-Tucker (KKT) conditions do not hold in this condition. In our
recent paper, we proposed a new exact relaxation method which directly removes
the complementarity constraints from the model to make it convex and easier to
solve. This paper further extends our previous study, with more than one group
of sufficient conditions that guarantee the exact relaxation presented, proven
and discussed. This paper may contribute to wider application of the exact
relaxation in storage-concerned ED problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02495</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02495</id><created>2015-05-11</created><authors><author><keyname>Thakur</keyname><forenames>Chetan Singh</forenames></author><author><keyname>Wang</keyname><forenames>Runchun</forenames></author><author><keyname>Afshar</keyname><forenames>Saeed</forenames></author><author><keyname>Hamilton</keyname><forenames>Tara Julia</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>van Schaik</keyname><forenames>Andre</forenames></author></authors><title>An Online Learning Algorithm for Neuromorphic Hardware Implementation</title><categories>cs.NE</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a sign-based online learning (SOL) algorithm for a neuromorphic
hardware framework called Trainable Analogue Block (TAB). The TAB framework
utilises the principles of neural population coding, implying that it encodes
the input stimulus using a large pool of nonlinear neurons. The SOL algorithm
is a simple weight update rule that employs the sign of the hidden layer
activation and the sign of the output error, which is the difference between
the target output and the predicted output. The SOL algorithm is easily
implementable in hardware, and can be used in any artificial neural network
framework that learns weights by minimising a convex cost function. We show
that the TAB framework can be trained for various regression tasks using the
SOL algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02496</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02496</id><created>2015-05-11</created><authors><author><keyname>Wang</keyname><forenames>Liwei</forenames></author><author><keyname>Lee</keyname><forenames>Chen-Yu</forenames></author><author><keyname>Tu</keyname><forenames>Zhuowen</forenames></author><author><keyname>Lazebnik</keyname><forenames>Svetlana</forenames></author></authors><title>Training Deeper Convolutional Networks with Deep Supervision</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most promising ways of improving the performance of deep
convolutional neural networks is by increasing the number of convolutional
layers. However, adding layers makes training more difficult and
computationally expensive. In order to train deeper networks, we propose to add
auxiliary supervision branches after certain intermediate layers during
training. We formulate a simple rule of thumb to determine where these branches
should be added. The resulting deeply supervised structure makes the training
much easier and also produces better classification results on ImageNet and the
recently released, larger MIT Places dataset
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02505</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02505</id><created>2015-05-11</created><authors><author><keyname>Lihua</keyname><forenames>Guo</forenames></author><author><keyname>Chenggan</keyname><forenames>Guo</forenames></author></authors><title>A Two-Layer Local Constrained Sparse Coding Method for Fine-Grained
  Visual Categorization</title><categories>cs.CV</categories><comments>19 pages, 12 figures, 8 tables</comments><msc-class>68T45</msc-class><acm-class>I.4.10</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Fine-grained categories are more difficulty distinguished than generic
categories due to the similarity of inter-class and the diversity of
intra-class. Therefore, the fine-grained visual categorization (FGVC) is
considered as one of challenge problems in computer vision recently. A new
feature learning framework, which is based on a two-layer local constrained
sparse coding architecture, is proposed in this paper. The two-layer
architecture is introduced for learning intermediate-level features, and the
local constrained term is applied to guarantee the local smooth of coding
coefficients. For extracting more discriminative information, local orientation
histograms are the input of sparse coding instead of raw pixels. Moreover, a
quick dictionary updating process is derived to further improve the training
speed. Two experimental results show that our method achieves 85.29% accuracy
on the Oxford 102 flowers dataset and 67.8% accuracy on the CUB-200-2011 bird
dataset, and the performance of our framework is highly competitive with
existing literatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02508</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02508</id><created>2015-05-11</created><authors><author><keyname>Elmir</keyname><forenames>Abir</forenames></author><author><keyname>Elmir</keyname><forenames>Badr</forenames></author><author><keyname>Bounabat</keyname><forenames>Bouchaib</forenames></author></authors><title>Inter organizational System Management for integrated service delivery:
  an Enterprise Architecture Perspective</title><categories>cs.SE</categories><comments>10 pages, 7 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:1110.1277 by other authors</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Volume 12,
  Issue 2, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service sharing is a prominent operating model to support business. Many
large inter-organizational networks have implemented some form of value added
integrated services in order to reach efficiency and to reduce costs
sustainably. Coupling Service orientation with enterprise architecture paradigm
is very important at improving organizational performance through business
process optimization. Indeed, enterprise architecture management is
increasingly discussed because of information system role as part of achieving
the strategic direction of value creation and contribution to economic growth.
Also, system architecture promotes synergy and business efficiency for
inter-organizational collaboration. For this purpose, this work proposes a
review of service oriented enterprise architecture. This review, enumerates
several integrative and collaborative frameworks for integrated service
delivery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02509</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02509</id><created>2015-05-11</created><authors><author><keyname>Wise</keyname><forenames>Ben</forenames></author><author><keyname>Bankes</keyname><forenames>Steven</forenames></author></authors><title>Non-spatial Probabilistic Condorcet Election Methodology</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a class of models for pol/mil/econ bargaining and conflict that is
loosely based on the Median Voter Theorem which has been used with great
success for about 30 years. However, there are fundamental mathematical
limitations to these models. They apply to issues which can be represented on a
single one-dimensional continuum. They represent fundamental group decision
process by a deterministic Condorcet Election: deterministic voting by all
actors, and deterministic outcomes of each vote. This work provides a
methodology for addressing a broader class of problems. The first extension is
to continuous issue sets where the consequences of policies are not
well-described by a distance measure or utility is not monotonic in distance.
The second fundamental extension is to inherently discrete issue sets. Because
the options cannot easily be mapped into a multidimensional space so that the
utility depends on distance, we refer to it as a non-spatial issue set. The
third, but most fundamental, extension is to represent the negotiating process
as a probabilistic Condorcet election (PCE). The actors' generalized voting is
deterministic, but the outcomes of the votes is probabilistic. The PCE provides
the flexibility to make the first two extensions possible; this flexibility
comes at the cost of less precise predictions and more complex validation. The
methodology has been implemented in two proof-of-concept prototypes which
address the subset selection problem of forming a parliament, and strategy
optimization for one-dimensional issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02510</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02510</id><created>2015-05-11</created><updated>2015-11-20</updated><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Stoica</keyname><forenames>Petre</forenames></author></authors><title>Vandermonde Decomposition of Multilevel Toeplitz Matrices with
  Application to Multidimensional Super-Resolution</title><categories>cs.IT math.IT</categories><comments>34 pages, 5 figures, submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Vandermonde decomposition of Toeplitz matrices, discovered by
Carath\'{e}odory and Fej\'{e}r in the 1910s and rediscovered by Pisarenko in
the 1970s, forms the basis of modern subspace methods for 1D frequency
estimation. Many related numerical tools have also been developed for
multidimensional (MD), especially 2D, frequency estimation; however, a
fundamental question has remained unresolved as to whether an analog of the
Vandermonde decomposition holds for multilevel Toeplitz matrices in the MD
case. In this paper, an affirmative answer to this question and a constructive
method for finding the decomposition are provided when the matrix rank is lower
than the dimension of each Toeplitz block. A numerical method for searching for
a decomposition is also proposed when the matrix rank is higher. The new
results are applied to studying MD frequency estimation within the recent
super-resolution framework. A precise formulation of the atomic $\ell_0$ norm
is derived using the Vandermonde decomposition. Practical algorithms for
frequency estimation are proposed based on relaxation techniques. Extensive
numerical simulations are provided to demonstrate the effectiveness of these
algorithms compared to the existing atomic norm method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02532</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02532</id><created>2015-05-11</created><updated>2015-06-17</updated><authors><author><keyname>Huang</keyname><forenames>Ming-Deh A.</forenames></author><author><keyname>Kosters</keyname><forenames>Michiel</forenames></author><author><keyname>Yang</keyname><forenames>Yun</forenames></author><author><keyname>Yeo</keyname><forenames>Sze Ling</forenames></author></authors><title>On the last fall degree of zero-dimensional Weil descent systems</title><categories>math.AC cs.SC</categories><comments>16 pages, changed definition of tau and revised Section 5</comments><msc-class>13P10, 13P15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we will discuss a new, mostly theoretical, method for solving
(zero-dimensional) polynomial systems, which lies in between Gr\&quot;obner basis
computations and the heuristic first fall degree assumption and is not based on
any heuristic. This method relies on the new concept of last fall degree.
  Let $k$ be a finite field of cardinality $q^n$ and let $k'$ be its subfield
of cardinality $q$. Let $\mathcal{F} \subset k[X_0,\ldots,X_{m-1}]$ be a finite
subset generating a zero-dimensional ideal. We give an upper bound of the last
fall degree of the Weil descent system of $\mathcal{F}$, which depends on $q$,
$m$, the last fall degree of $\mathcal{F}$, the degree of $\mathcal{F}$ and the
number of solutions of $\mathcal{F}$, but not on $n$. This shows that such Weil
descent systems can be solved efficiently if $n$ grows. In particular, we apply
these results for multi-HFE and essentially show that multi-HFE is insecure.
  Finally, we discuss that the degree of regularity (or last fall degree) of
Weil descent systems coming from summation polynomials to solve the elliptic
curve discrete logarithm problem might depend on $n$, since such systems
without field equations are not zero-dimensional.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02534</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02534</id><created>2015-05-11</created><authors><author><keyname>Song</keyname><forenames>Tianyu</forenames></author><author><keyname>Kam</keyname><forenames>Pooi-Yuen</forenames></author></authors><title>Robust Data Detection for the Photon-Counting Free-Space Optical System
  with Implicit CSI Acquisition and Background Radiation Compensation</title><categories>cs.IT math.IT</categories><comments>11pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since atmospheric turbulence and pointing errors cause signal intensity
fluctuations and the background radiation surrounding the free-space optical
(FSO) receiver contributes an undesired noisy component, the receiver requires
accurate channel state information (CSI) and background information to adjust
the detection threshold. In most previous studies, for CSI acquisition, pilot
symbols were employed, which leads to a reduction of spectral and energy
efficiency; and an impractical assumption that the background radiation
component is perfectly known was made. In this paper, we develop an efficient
and robust sequence receiver, which acquires the CSI and the background
information implicitly and requires no knowledge about the channel model
information. It is robust since it can automatically estimate the CSI and
background component and detect the data sequence accordingly. Its decision
metric has a simple form and involves no integrals, and thus can be easily
evaluated. A Viterbi-type trellis-search algorithm is adopted to improve the
search efficiency, and a selective-store strategy is adopted to overcome a
potential error floor problem as well as to increase the memory efficiency. To
further simplify the receiver, a decision-feedback symbol-by-symbol receiver is
proposed as an approximation of the sequence receiver. By simulations and
theoretical analysis, we show that the performance of both the sequence
receiver and the symbol-by-symbol receiver, approach that of detection with
perfect knowledge of the CSI and background radiation, as the length of the
window for forming the decision metric increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02536</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02536</id><created>2015-05-11</created><authors><author><keyname>Song</keyname><forenames>Tianyu</forenames></author><author><keyname>Kam</keyname><forenames>Pooi-Yuen</forenames></author></authors><title>Efficient Symbol Detection for the FSO IM/DD System with Automatic and
  Adaptive Threshold Adjustment: The Multi-level PAM Case</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To detect M-ary pulse amplitude modulation signals reliably in an FSO
communication system, the receiver requires accurate knowledge about the
instantaneous channel attenuation on the signal. We derive here an optimum,
symbol-by-symbol receiver that jointly estimates the attenuation with the help
of past detected data symbols and detects the data symbols accordingly. Few
pilot symbols are required, resulting in high spectral efficiency. Detection
can be performed with a very low complexity. From both theoretical analysis and
simulation, we show that as the number of the detected data symbols used for
estimating the channel attenuation increases, the bit error probability of our
receiver approaches that of detection with perfect channel knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02539</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02539</id><created>2015-05-11</created><updated>2015-05-13</updated><authors><author><keyname>Krawczyk</keyname><forenames>Ma&#x142;gorzata J.</forenames></author><author><keyname>del Castillo-Mussot</keyname><forenames>Marcelo</forenames></author><author><keyname>Hern&#xe1;ndez-Ramirez</keyname><forenames>Eric</forenames></author><author><keyname>Naumis</keyname><forenames>Gerardo G.</forenames></author><author><keyname>Ku&#x142;akowski</keyname><forenames>Krzysztof</forenames></author></authors><title>Heider balance, asymmetric ties, and gender segregation</title><categories>physics.soc-ph cs.SI</categories><doi>10.1016/j.physa.2015.07.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To remove a cognitive dissonance in interpersonal relations, people tend to
divide our acquaintances into friendly and hostile parts, both groups
internally friendly and mutually hostile. This process is modeled as an
evolution towards the Heider balance. A set of differential equations have been
proposed and validated (Kulakowski {\it et al}, IJMPC 16 (2005) 707) to model
the Heider dynamics of this social and psychological process. Here we
generalize the model by including the initial asymmetry of the interprersonal
relations and the direct reciprocity effect which removes this asymmetry. Our
model is applied to the data on enmity and friendship in 37 school classes and
4 groups of teachers in M\'exico. For each class, a stable balanced partition
is obtained into two groups. The gender structure of the groups reveals
stronger gender segregation in younger classes, i.e. of age below 12 years, a
fact consistent with other general empirical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02552</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02552</id><created>2015-05-11</created><authors><author><keyname>Perez</keyname><forenames>Guillaume</forenames></author><author><keyname>R&#xe9;gin</keyname><forenames>Jean-Charles</forenames></author></authors><title>Relations between MDDs and Tuples and Dynamic Modifications of MDDs
  based constraints</title><categories>cs.AI</categories><comments>15 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relations between Multi-valued Decision Diagrams (MDD) and
tuples (i.e. elements of the Cartesian Product of variables). First, we improve
the existing methods for transforming a set of tuples, Global Cut Seeds,
sequences of tuples into MDDs. Then, we present some in-place algorithms for
adding and deleting tuples from an MDD. Next, we consider an MDD constraint
which is modified during the search by deleting some tuples. We give an
algorithm which adapts MDD-4R to these dynamic and persistent modifications.
Some experiments show that MDD constraints are competitive with Table
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02558</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02558</id><created>2015-05-11</created><authors><author><keyname>Hertz</keyname><forenames>Alain</forenames></author><author><keyname>Lozin</keyname><forenames>Vadim</forenames></author><author><keyname>Ries</keyname><forenames>Bernard</forenames></author><author><keyname>Zamaraev</keyname><forenames>Victor</forenames></author><author><keyname>de Werra</keyname><forenames>Dominique</forenames></author></authors><title>Dominating induced matchings in graphs containing no long claw</title><categories>cs.DM cs.DS math.CO</categories><msc-class>05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An induced matching $M$ in a graph $G$ is dominating if every edge not in $M$
shares exactly one vertex with an edge in $M$. The dominating induced matching
problem (also known as efficient edge domination) asks whether a graph $G$
contains a dominating induced matching. This problem is generally NP-complete,
but polynomial-time solvable for graphs with some special properties. In
particular, it is solvable in polynomial time for claw-free graphs. In the
present paper, we study this problem for graphs containing no long claw, i.e.
no induced subgraph obtained from the claw by subdividing each of its edges
exactly once. To solve the problem in this class, we reduce it to the following
question: given a graph $G$ and a subset of its vertices, does $G$ contain a
matching saturating all vertices of the subset? We show that this question can
be answered in polynomial time, thus providing a polynomial-time algorithm to
solve the dominating induced matching problem for graphs containing no long
claw.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02568</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02568</id><created>2015-05-11</created><authors><author><keyname>Giotis</keyname><forenames>Ioannis</forenames></author><author><keyname>Kirousis</keyname><forenames>Lefteris</forenames></author><author><keyname>Psaromiligkos</keyname><forenames>Kostas I.</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>An alternative proof for the constructive Asymmetric Lov\'asz Local
  Lemma</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an alternative constructive proof of the Asymmetric Lov\'asz Local
Lemma. Our proof uses the classic algorithmic framework of Moser and the
analysis introduced by Giotis, Kirousis, Psaromiligkos, and Thilikos in &quot;On the
algorithmic Lov\'asz Local Lemma and acyclic edge coloring&quot;, combined with the
work of Bender and Richmond on the multivariable Lagrange Inversion formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02579</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02579</id><created>2015-05-11</created><authors><author><keyname>Abou-Saleh</keyname><forenames>Faris</forenames></author><author><keyname>Cheney</keyname><forenames>James</forenames></author><author><keyname>Gibbons</keyname><forenames>Jeremy</forenames></author><author><keyname>McKinna</keyname><forenames>James</forenames></author><author><keyname>Stevens</keyname><forenames>Perdita</forenames></author></authors><title>Notions of bidirectional computation and entangled state monads</title><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidirectional transformations (bx) support principled consistency maintenance
between data sources. Each data source corresponds to one perspective on a
composite system, manifested by operations to 'get' and 'set' a view of the
whole from that particular perspective. Bx are important in a wide range of
settings, including databases, interactive applications, and model-driven
development. We show that bx are naturally modelled in terms of mutable state;
in particular, the 'set' operations are stateful functions. This leads
naturally to considering bx that exploit other computational effects too, such
as I/O, nondeterminism, and failure, all largely ignored in the bx literature
to date. We present a semantic foundation for symmetric bidirectional
transformations with effects. We build on the mature theory of monadic
encapsulation of effects in functional programming, develop the equational
theory and important combinators for effectful bx, and provide a prototype
implementation in Haskell along with several illustrative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02581</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02581</id><created>2015-05-11</created><authors><author><keyname>Tomczak</keyname><forenames>Jakub Mikolaj</forenames></author></authors><title>Improving neural networks with bunches of neurons modeled by Kumaraswamy
  units: Preliminary study</title><categories>cs.LG cs.NE</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks have recently achieved state-of-the-art results in many
machine learning problems, e.g., speech recognition or object recognition.
Hitherto, work on rectified linear units (ReLU) provides empirical and
theoretical evidence on performance increase of neural networks comparing to
typically used sigmoid activation function. In this paper, we investigate a new
manner of improving neural networks by introducing a bunch of copies of the
same neuron modeled by the generalized Kumaraswamy distribution. As a result,
we propose novel non-linear activation function which we refer to as
Kumaraswamy unit which is closely related to ReLU. In the experimental study
with MNIST image corpora we evaluate the Kumaraswamy unit applied to
single-layer (shallow) neural network and report a significant drop in test
classification error and test cross-entropy in comparison to sigmoid unit, ReLU
and Noisy ReLU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02586</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02586</id><created>2015-05-11</created><authors><author><keyname>Hofmann</keyname><forenames>Johannes</forenames></author><author><keyname>Fey</keyname><forenames>Dietmar</forenames></author><author><keyname>Eitzinger</keyname><forenames>Jan</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Performance analysis of the Kahan-enhanced scalar product on current
  multicore processors</title><categories>cs.PF cs.DC</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the performance characteristics of a numerically enhanced
scalar product (dot) kernel loop that uses the Kahan algorithm to compensate
for numerical errors, and describe efficient SIMD-vectorized implementations on
recent Intel processors. Using low-level instruction analysis and the
execution-cache-memory (ECM) performance model we pinpoint the relevant
performance bottlenecks for single-core and thread-parallel execution, and
predict performance and saturation behavior. We show that the Kahan-enhanced
scalar product comes at almost no additional cost compared to the naive
(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD
vectorization and unrolling, are applied. We also investigate the impact of
architectural changes across four generations of Intel Xeon processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02591</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02591</id><created>2015-05-11</created><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author><author><keyname>Zhang</keyname><forenames>Bowei</forenames></author><author><keyname>Zhang</keyname><forenames>Ningye</forenames></author></authors><title>Linear Time Approximation Schemes for Geometric Maximum Coverage</title><categories>cs.CG</categories><comments>17 pages, 4 figures, to be published in the Proceedings of 21st
  International Computing and Combinatorics Conference(COCOON2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study approximation algorithms for the following geometric version of the
maximum coverage problem: Let P be a set of n weighted points in the plane. We
want to place m a * b rectangles such that the sum of the weights of the points
in P covered by these rectangles is maximized.For any fixed &gt; 0, we present
efficient approximation schemes that can find (1-{\epsilon})-approximation to
the optimal solution.In particular, for m = 1, our algorithm runs in linear
time O(n log( 1/{\epsilon})), improving over the previous result. For m &gt; 1, we
present an algorithm that runs in
O(n/{\epsilon}log(1/{\epsilon})+m(1/{\epsilon})^(O(min(sqrt(m),1/{\epsilon})))
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02595</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02595</id><created>2015-05-11</created><authors><author><keyname>Wang</keyname><forenames>Xiangke</forenames></author><author><keyname>Li</keyname><forenames>Xun</forenames></author><author><keyname>Cong</keyname><forenames>Yirui</forenames></author><author><keyname>Zeng</keyname><forenames>Zhiwen</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiqiang</forenames></author></authors><title>Multi-Agent Distributed Coordination Control: Developments and
  Directions</title><categories>cs.MA cs.SY</categories><comments>28 pages, 8 figures</comments><msc-class>93A14</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the recent developments on distributed coordination control,
especially the consensus and formation control, are summarized with the graph
theory playing a central role, in order to present a cohesive overview of the
multi-agent distributed coordination control, together with brief reviews of
some closely related issues including rendezvous/alignment, swarming/flocking
and containment control.In terms of the consensus problem, the recent results
on consensus for the agents with different dynamics from first-order,
second-order to high-order linear and nonlinear dynamics, under different
communication conditions, such as cases with/without switching communication
topology and varying time-delays, are reviewed, in which the algebraic graph
theory is very useful in the protocol designs, stability proofs and converging
analysis. In terms of the formation control problem, after reviewing the
results of the algebraic graph theory employed in the formation control, we
mainly pay attention to the developments of the rigid and persistent graphs.
With the notions of rigidity and persistence, the formation transformation,
splitting and reconstruction can be completed, and consequently the range-based
formation control laws are designed with the least required information in
order to maintain a formation rigid/persistent. Afterwards, the recent results
on rendezvous/alignment, swarming/flocking and containment control, which are
very closely related to consensus and formation control, are briefly
introduced, in order to present an integrated view of the graph theory used in
the coordination control problem. Finally, towards the practical applications,
some directions possibly deserving investigation in coordination control are
raised as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02597</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02597</id><created>2015-05-11</created><updated>2015-08-25</updated><authors><author><keyname>Blackburn</keyname><forenames>Simon R.</forenames></author></authors><title>Probabilistic existence results for separable codes</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>16 pages. Typos corrected and minor changes since last version.
  Accepted by IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separable codes were defined by Cheng and Miao in 2011, motivated by
applications to the identification of pirates in a multimedia setting.
Combinatorially, $\overline{t}$-separable codes lie somewhere between
$t$-frameproof and $(t-1)$-frameproof codes: all $t$-frameproof codes are
$\overline{t}$-separable, and all $\overline{t}$-separable codes are
$(t-1)$-frameproof. Results for frameproof codes show that (when $q$ is large)
there are $q$-ary $\overline{t}$-separable codes of length $n$ with
approximately $q^{\lceil n/t\rceil}$ codewords, and that no $q$-ary
$\overline{t}$-separable codes of length $n$ can have more than approximately
$q^{\lceil n/(t-1)\rceil}$ codewords.
  The paper provides improved probabilistic existence results for
$\overline{t}$-separable codes when $t\geq 3$. More precisely, for all $t\geq
3$ and all $n\geq 3$, there exists a constant $\kappa$ (depending only on $t$
and $n$) such that there exists a $q$-ary $\overline{t}$-separable code of
length $n$ with at least $\kappa q^{n/(t-1)}$ codewords for all sufficiently
large integers $q$. This shows, in particular, that the upper bound (derived
from the bound on $(t-1)$-frameproof codes) on the number of codewords in a
$\overline{t}$-separable code is realistic.
  The results above are more surprising after examining the situation when
$t=2$. Results due to Gao and Ge show that a $q$-ary $\overline{2}$-separable
code of length $n$ can contain at most $\frac{3}{2}q^{2\lceil
n/3\rceil}-\frac{1}{2}q^{\lceil n/3\rceil}$ codewords, and that codes with at
least $\kappa q^{2n/3}$ codewords exist. So optimal $\overline{2}$-separable
codes behave neither like $2$-frameproof nor $1$-frameproof codes.
  Also, the Gao--Ge bound is strengthened to show that a $q$-ary
$\overline{2}$-separable code of length $n$ can have at most \[ q^{\lceil
2n/3\rceil}+\tfrac{1}{2}q^{\lfloor n/3\rfloor}(q^{\lfloor n/3\rfloor}-1) \]
codewords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02617</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02617</id><created>2015-05-11</created><authors><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Ashikhmin</keyname><forenames>Alexei</forenames></author><author><keyname>Yang</keyname><forenames>Hong</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Marzetta</keyname><forenames>Thomas L.</forenames></author></authors><title>Cell-Free Massive MIMO: Uniformly Great Service For Everyone</title><categories>cs.IT math.IT</categories><comments>SPAWC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the downlink of Cell-Free Massive MIMO systems, where a very
large number of distributed access points (APs) simultaneously serve a much
smaller number of users. Each AP uses local channel estimates obtained from
received uplink pilots and applies conjugate beamforming to transmit data to
the users. We derive a closed-form expression for the achievable rate. This
expression enables us to design an optimal max-min power control scheme that
gives equal quality of service to all users.
  We further compare the performance of the Cell-Free Massive MIMO system to
that of a conventional small-cell network and show that the throughput of the
Cell-Free system is much more concentrated around its median compared to that
of the small-cell system. The Cell-Free Massive MIMO system can provide an
almost $20-$fold increase in 95%-likely per-user throughput, compared with the
small-cell system. Furthermore, Cell-Free systems are more robust to shadow
fading correlation than small-cell systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02619</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02619</id><created>2015-05-11</created><authors><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>A Graph Model for Opportunistic Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements in graph-based analysis and solutions of instantly
decodable network coding (IDNC) trigger the interest to extend them to more
complicated opportunistic network coding (ONC) scenarios, with limited increase
in complexity. In this paper, we design a simple IDNC-like graph model for a
specific subclass of ONC, by introducing a more generalized definition of its
vertices and the notion of vertex aggregation in order to represent the storage
of non-instantly-decodable packets in ONC. Based on this representation, we
determine the set of pairwise vertex adjacency conditions that can populate
this graph with edges so as to guarantee decodability or aggregation for the
vertices of each clique in this graph. We then develop the algorithmic
procedures that can be applied on the designed graph model to optimize any
performance metric for this ONC subclass. A case study on reducing the
completion time shows that the proposed framework improves on the performance
of IDNC and gets very close to the optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02634</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02634</id><created>2015-05-11</created><authors><author><keyname>Sinafar</keyname><forenames>Behzad</forenames></author><author><keyname>Ghiasi</keyname><forenames>Amir Rikhtegar</forenames></author></authors><title>Power and Frequency Control of Induction Furnace Using Fuzzy Logic
  Controller</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new method to control resonance frequency and output
power of induction heating coil. Induction heating coil can be controlled by
single phase sinusoidal pulse width modulation (SPWM) inverter .All electrical
requirements beside magnetic permeability and resistivity variation for
modeling induction heating coil have been considered to make simulations
practical .Control blocks using Fuzzy logic which control both active and
reactive power have been designed .The system modeling and Fuzzy logic
controllers are simulated in MATLAB/SIMULINK and FUZZY LOGIC toolbox .The
results of the simulations show the effectiveness and superiority of this
control system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02637</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02637</id><created>2015-05-11</created><authors><author><keyname>Liu</keyname><forenames>Peizun</forenames></author><author><keyname>Wahl</keyname><forenames>Thomas</forenames></author></authors><title>Unbounded-Thread Reachability via Symbolic Execution and Loop
  Acceleration (Technical Report)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to parameterized reachability for communicating
finite-state threads that formulates the analysis as a satisfiability problem.
In addition to the unbounded number of threads, the main challenge for
SAT/SMT-based reachability methods is the existence of unbounded loops in the
program executed by a thread. We show in this paper how simple loops can be
accelerated without approximation into Presburger arithmetic constraints. The
constraints are obtained via symbolic execution and are satisfiable exactly if
the given program state is reachable. We summarize loops nested inside other
loops using recurrence relations derived from the inner loop's acceleration.
This summary abstracts the loop iteration parameter and may thus
overapproximate. An advantage of our symbolic approach is that the process of
building the Presburger formulas may instantly reveal their unsatisfiability,
before any arithmetic has been performed. We demonstrate the power of this
technique for proving and refuting safety properties of unbounded-thread
programs and other infinite-state transition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02642</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02642</id><created>2015-05-11</created><authors><author><keyname>Ebadi</keyname><forenames>Hamid</forenames></author><author><keyname>Sands</keyname><forenames>David</forenames></author></authors><title>Featherweight PINQ</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differentially private mechanisms enjoy a variety of composition properties.
Leveraging these, McSherry introduced PINQ (SIGMOD 2009), a system empowering
non-experts to construct new differentially private analyses. PINQ is an
LINQ-like API which provides automatic privacy guarantees for all programs
which use it to mediate sensitive data manipulation. In this work we introduce
featherweight PINQ, a formal model capturing the essence of PINQ. We prove that
any program interacting with featherweight PINQ's API is differentially
private.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02648</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02648</id><created>2015-05-08</created><authors><author><keyname>Ahmed</keyname><forenames>Waqar</forenames></author><author><keyname>Hasan</keyname><forenames>Osman</forenames></author></authors><title>Towards Formal Fault Tree Analysis using Theorem Proving</title><categories>cs.LO cs.AI cs.SE</categories><comments>16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault Tree Analysis (FTA) is a dependability analysis technique that has been
widely used to predict reliability, availability and safety of many complex
engineering systems. Traditionally, these FTA-based analyses are done using
paper-and-pencil proof methods or computer simulations, which cannot ascertain
absolute correctness due to their inherent limitations. As a complementary
approach, we propose to use the higher-order-logic theorem prover HOL4 to
conduct the FTA-based analysis of safety-critical systems where accuracy of
failure analysis is a dire need. In particular, the paper presents a
higher-order-logic formalization of generic Fault Tree gates, i.e., AND, OR,
NAND, NOR, XOR and NOT and the formal verification of their failure probability
expressions. Moreover, we have formally verified the generic probabilistic
inclusion-exclusion principle, which is one of the foremost requirements for
conducting the FTA-based failure analysis of any given system. For illustration
purposes, we conduct the FTA-based failure analysis of a solar array that is
used as the main source of power for the Dong Fang Hong-3 (DFH-3) satellite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02651</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02651</id><created>2015-05-06</created><updated>2015-11-04</updated><authors><author><keyname>Fong</keyname><forenames>Brendan</forenames></author><author><keyname>Nava-Kopp</keyname><forenames>Hugo</forenames></author></authors><title>Additive monotones for resource theories of parallel-combinable
  processes with discarding</title><categories>cs.LO cs.IT math.IT quant-ph</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 170-178</journal-ref><doi>10.4204/EPTCS.195.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partitioned process theory, as defined by Coecke, Fritz, and Spekkens, is a
symmetric monoidal category together with an all-object-including symmetric
monoidal subcategory. We think of the morphisms of this category as processes,
and the morphisms of the subcategory as those processes that are freely
executable. Via a construction we refer to as parallel-combinable processes
with discarding, we obtain from this data a partially ordered monoid on the set
of processes, with f &gt; g if one can use the free processes to construct g from
f. The structure of this partial order can then be probed using additive
monotones: order-preserving monoid homomorphisms with values in the real
numbers under addition. We first characterise these additive monotones in terms
of the corresponding partitioned process theory.
  Given enough monotones, we might hope to be able to reconstruct the order on
the monoid. If so, we say that we have a complete family of monotones. In
general, however, when we require our monotones to be additive monotones, such
families do not exist or are hard to compute. We show the existence of complete
families of additive monotones for various partitioned process theories based
on the category of finite sets, in order to shed light on the way such families
can be constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02653</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02653</id><created>2015-05-11</created><authors><author><keyname>Zitouni</keyname><forenames>Rafik</forenames></author><author><keyname>George</keyname><forenames>Laurent</forenames></author><author><keyname>Abouda</keyname><forenames>Yacine</forenames></author></authors><title>A Dynamic Spectrum Access on SDR for IEEE 802.15.4 networks</title><categories>cs.NI</categories><comments>Wireless Innovation Forum SDR 2015</comments><journal-ref>Proceedings of WInnComm 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our paper deals with a Dynamic Spectrum Access (DSA) and its implementation
on a Software Defined Radio (SDR) for IEEE 802.15.4e Networks. The network
nodes select the carrier frequency after Energy-Detection based Spectrum
Sensing (SS). To ensure frequency hoping between two nodes in IEEE 802.15.4e
Network, we propose a synchronization algorithm. We considerate the IEEE
802.15.4e Network is Secondary User (SU), and all other networks are Primary
Users (PUs) in unlicensed 868/915 MHz and 2450 MHz bands of a Cognitive Radio
(CR). However, the algorithm and the energy-sensor have been implemented over
GNU Radio and Universal Software Radio Peripheral (USRP) SDR. In addition, real
packet transmissions have been performed in two cases. In the first case, SU
communicates in static carrier-frequency, while in the second case with the
implemented DSA. For each case, PU transmitter disturbs SU, which calculates
Packet Success Rate (PSR) to measure the robustness of a used DSA. The obtained
PSR is improved by 80\% when the SU accomplished DSA rather than a static
access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02655</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02655</id><created>2015-05-11</created><updated>2015-06-19</updated><authors><author><keyname>Brazdil</keyname><forenames>Tomas</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Kucera</keyname><forenames>Antonin</forenames></author><author><keyname>Novotny</keyname><forenames>Petr</forenames></author></authors><title>Long-Run Average Behaviour of Probabilistic Vector Addition Systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the pattern frequency vector for runs in probabilistic Vector
Addition Systems with States (pVASS). Intuitively, each configuration of a
given pVASS is assigned one of finitely many patterns, and every run can thus
be seen as an infinite sequence of these patterns. The pattern frequency vector
assigns to each run the limit of pattern frequencies computed for longer and
longer prefixes of the run. If the limit does not exist, then the vector is
undefined. We show that for one-counter pVASS, the pattern frequency vector is
defined and takes only finitely many values for almost all runs. Further, these
values and their associated probabilities can be approximated up to an
arbitrarily small relative error in polynomial time. For stable two-counter
pVASS, we show the same result, but we do not provide any upper complexity
bound. As a byproduct of our study, we discover counterexamples falsifying some
classical results about stochastic Petri nets published in the~80s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02656</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02656</id><created>2015-05-11</created><authors><author><keyname>Doreau</keyname><forenames>Henri</forenames></author></authors><title>Distributed Lustre activity tracking</title><categories>cs.DC</categories><comments>International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</comments><proxy>Michael Brim</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous administration tools and techniques require near real time vision of
the activity occurring on a distributed filesystem. The changelog facility
provided by Lustre to address this need suffers limitations in terms of
scalability and flexibility. We have been working on reducing those limitations
by enhancing Lustre itself and developing external tools such as Lustre
ChangeLog Aggregate and Publish (LCAP) proxy. Beyond the ability to distribute
changelog processing, this effort aims at opening new prospectives by making
the changelog stream simpler to leverage for various purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02681</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02681</id><created>2015-05-11</created><updated>2015-05-13</updated><authors><author><keyname>Shen</keyname><forenames>Chih-Ya</forenames></author><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Huang</keyname><forenames>Liang-Hao</forenames></author><author><keyname>Lee</keyname><forenames>Wang-Chien</forenames></author><author><keyname>Chen</keyname><forenames>Ming-Syan</forenames></author></authors><title>Socio-Spatial Group Queries for Impromptu Activity Planning</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development and integration of social networking services and smartphones
have made it easy for individuals to organize impromptu social activities
anywhere and anytime. Main challenges arising in organizing impromptu
activities are mostly due to the requirements of making timely invitations in
accordance with the potential activity locations, corresponding to the
locations of and the relationship among the candidate attendees. Various
combinations of candidate attendees and activity locations create a large
solution space. Thus, in this paper, we propose Multiple Rally-Point Social
Spatial Group Query (MRGQ), to select an appropriate activity location for a
group of nearby attendees with tight social relationships. Although MRGQ is
NP-hard, the number of attendees in practice is usually small enough such that
an optimal solution can be found efficiently. Therefore, we first propose an
Integer Linear Programming optimization model for MRGQ. We then design an
efficient algorithm, called MAGS, which employs effective search space
exploration and pruning strategies to reduce the running time for finding the
optimal solution. We also propose to further optimize efficiency by indexing
the potential activity locations. A user study demonstrates the strength of
using MAGS over manual coordination in terms of both solution quality and
efficiency. Experimental results on real datasets show that our algorithms can
process MRGQ efficiently and significantly outperform other baseline
algorithms, including one based on the commercial parallel optimizer IBM CPLEX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02690</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02690</id><created>2015-05-11</created><updated>2015-05-19</updated><authors><author><keyname>Delporte-Gallet</keyname><forenames>Carole</forenames></author><author><keyname>Fauconnier</keyname><forenames>Hugues</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Ruppert</keyname><forenames>Eric</forenames></author></authors><title>On the Space Complexity of Set Agreement</title><categories>cs.DC</categories><acm-class>C.2.4; D.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-set agreement problem is a generalization of the classical consensus
problem in which processes are permitted to output up to $k$ different input
values. In a system of $n$ processes, an $m$-obstruction-free solution to the
problem requires termination only in executions where the number of processes
taking steps is eventually bounded by $m$. This family of progress conditions
generalizes wait-freedom ($m=n$) and obstruction-freedom ($m=1$). In this
paper, we prove upper and lower bounds on the number of registers required to
solve $m$-obstruction-free $k$-set agreement, considering both one-shot and
repeated formulations. In particular, we show that repeated $k$ set agreement
can be solved using $n+2m-k$ registers and establish a nearly matching lower
bound of $n+m-k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02708</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02708</id><created>2015-05-11</created><authors><author><keyname>Mchedlidze</keyname><forenames>Tamara</forenames></author></authors><title>Simultaneous straight-line drawing of a planar graph and its rectangular
  dual</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A natural way to represent on the plane both a planar graph and its dual is
to follow the definition of the dual, thus, to place vertices inside their
corresponding primal faces, and to draw the dual edges so that they only cross
their corresponding primal edges. The problem of constructing such drawings has
a long tradition when the drawings of both primal and dual are required to be
straight-line. We consider the same problem for a planar graph and its
rectangular dual. We show that the rectangular dual can be resized to host a
planar straight-line drawing of its primal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02710</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02710</id><created>2015-05-11</created><updated>2015-05-15</updated><authors><author><keyname>Bray</keyname><forenames>Nicolas</forenames></author><author><keyname>Pimentel</keyname><forenames>Harold</forenames></author><author><keyname>Melsted</keyname><forenames>P&#xe1;ll</forenames></author><author><keyname>Pachter</keyname><forenames>Lior</forenames></author></authors><title>Near-optimal RNA-Seq quantification</title><categories>q-bio.QM cs.CE cs.DS q-bio.GN</categories><comments>- Added some results (paralog analysis, allele specific expression
  analysis, alignment comparison, accuracy analysis with TPMs) - Switched
  bootstrap analysis to human sample from SEQC-MAQCIII - Provided link to a
  snakefile that allows for reproducibility of all results and figures in the
  paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to RNA-Seq quantification that is near optimal in
speed and accuracy. Software implementing the approach, called kallisto, can be
used to analyze 30 million unaligned paired-end RNA-Seq reads in less than 5
minutes on a standard laptop computer while providing results as accurate as
those of the best existing tools. This removes a major computational bottleneck
in RNA-Seq analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02728</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02728</id><created>2015-05-11</created><authors><author><keyname>Harbi</keyname><forenames>Razen</forenames></author><author><keyname>Abdelaziz</keyname><forenames>Ibrahim</forenames></author><author><keyname>Kalnis</keyname><forenames>Panos</forenames></author><author><keyname>Mamoulis</keyname><forenames>Nikos</forenames></author><author><keyname>Ebrahim</keyname><forenames>Yasser</forenames></author><author><keyname>Sahli</keyname><forenames>Majed</forenames></author></authors><title>Adaptive Partitioning for Very Large RDF Data</title><categories>cs.DB</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed RDF systems partition data across multiple computer nodes
(workers). Some systems perform cheap hash partitioning, which may result in
expensive query evaluation, while others apply heuristics aiming at minimizing
inter-node communication during query evaluation. This requires an expensive
data preprocessing phase, leading to high startup costs for very large RDF
knowledge bases. Apriori knowledge of the query workload has also been used to
create partitions, which however are static and do not adapt to workload
changes; hence, inter-node communication cannot be consistently avoided for
queries that are not favored by the initial data partitioning.
  In this paper, we propose AdHash, a distributed RDF system, which addresses
the shortcomings of previous work. First, AdHash applies lightweight
partitioning on the initial data, that distributes triples by hashing on their
subjects; this renders its startup overhead low. At the same time, the
locality-aware query optimizer of AdHash takes full advantage of the
partitioning to (i)support the fully parallel processing of join patterns on
subjects and (ii) minimize data communication for general queries by applying
hash distribution of intermediate results instead of broadcasting, wherever
possible. Second, AdHash monitors the data access patterns and dynamically
redistributes and replicates the instances of the most frequent ones among
workers. As a result, the communication cost for future queries is drastically
reduced or even eliminated. To control replication, AdHash implements an
eviction policy for the redistributed patterns. Our experiments with synthetic
and real data verify that AdHash (i) starts faster than all existing systems,
(ii) processes thousands of queries before other systems become online, and
(iii) gracefully adapts to the query load, being able to evaluate queries on
billion-scale RDF data in sub-seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02729</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02729</id><created>2015-05-11</created><authors><author><keyname>Verma</keyname><forenames>Nakul</forenames></author><author><keyname>Branson</keyname><forenames>Kristin</forenames></author></authors><title>Sample complexity of learning Mahalanobis distance metrics</title><categories>cs.LG cs.AI stat.ML</categories><comments>26 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric learning seeks a transformation of the feature space that enhances
prediction quality for the given task at hand. In this work we provide
PAC-style sample complexity rates for supervised metric learning. We give
matching lower- and upper-bounds showing that the sample complexity scales with
the representation dimension when no assumptions are made about the underlying
data distribution. However, by leveraging the structure of the data
distribution, we show that one can achieve rates that are fine-tuned to a
specific notion of intrinsic complexity for a given dataset. Our analysis
reveals that augmenting the metric learning optimization criterion with a
simple norm-based regularization can help adapt to a dataset's intrinsic
complexity, yielding better generalization. Experiments on benchmark datasets
validate our analysis and show that regularizing the metric can help discern
the signal even when the data contains high amounts of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02731</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02731</id><created>2015-05-11</created><updated>2015-12-04</updated><authors><author><keyname>Delbracio</keyname><forenames>Mauricio</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Removing Camera Shake via Weighted Fourier Burst Accumulation</title><categories>cs.CV</categories><comments>Errata with respect to published version: Algorithm 1, lines 9 and
  10: w_i is replaced by w^p_i (as was correctly stated in Eq (9))</comments><journal-ref>Image Processing, IEEE Transactions on, Year: 2015, Volume: 24,
  Issue: 11, Pages: 3293 - 3307</journal-ref><doi>10.1109/TIP.2015.2442914</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous recent approaches attempt to remove image blur due to camera shake,
either with one or multiple input images, by explicitly solving an inverse and
inherently ill-posed deconvolution problem. If the photographer takes a burst
of images, a modality available in virtually all modern digital cameras, we
show that it is possible to combine them to get a clean sharp version. This is
done without explicitly solving any blur estimation and subsequent inverse
problem. The proposed algorithm is strikingly simple: it performs a weighted
average in the Fourier domain, with weights depending on the Fourier spectrum
magnitude. The method can be seen as a generalization of the align and average
procedure, with a weighted average, motivated by hand-shake physiology and
theoretically supported, taking place in the Fourier domain. The method's
rationale is that camera shake has a random nature and therefore each image in
the burst is generally blurred differently. Experiments with real camera data,
and extensive comparisons, show that the proposed Fourier Burst Accumulation
(FBA) algorithm achieves state-of-the-art results an order of magnitude faster,
with simplicity for on-board implementation on camera phones. Finally, we also
present experiments in real high dynamic range (HDR) scenes, showing how the
method can be straightforwardly extended to HDR photography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02740</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02740</id><created>2015-05-11</created><updated>2015-09-07</updated><authors><author><keyname>Kongskov</keyname><forenames>Rasmus Dalgas</forenames></author><author><keyname>J&#xf8;rgensen</keyname><forenames>Jakob Sauer</forenames></author><author><keyname>Poulsen</keyname><forenames>Henning Friis</forenames></author><author><keyname>Hansen</keyname><forenames>Per Christian</forenames></author></authors><title>Noise Robustness of a Combined Phase Retrieval and Reconstruction Method
  for Phase-Contrast Tomography</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical reconstruction methods for phase-contrast tomography consist of two
stages: phase retrieval and tomographic reconstruction. A novel algebraic
method combining the two was suggested by Kostenko et al. (Opt. Express, 21,
12185, 2013) and preliminary results demonstrating improved reconstruction
compared to a two-stage method given. Using simulated free-space propagation
experiments with a single sample-detector distance, we thoroughly compare the
novel method with the two-stage method to address limitations of the
preliminary results. We demonstrate that the novel method is substantially more
robust towards noise; our simulations point to a possible reduction in counting
times by an order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02759</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02759</id><created>2015-05-08</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Pajaziti</keyname><forenames>Arianit</forenames></author></authors><title>How Resilient Are Our Societies? Analyses, Models, and Preliminary
  Results</title><categories>cs.OH cs.MA</categories><comments>Paper submitted for publication in the Proc. of SERENE 2015
  (http://serene.disim.univaq.it/2015/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional social organizations such as those for the management of
healthcare and civil defence are the result of designs and realizations that
matched well with an operational context considerably different from the one we
are experiencing today: A simpler world, characterized by a greater amount of
resources to match less users producing lower peaks of requests. The new
context reveals all the fragility of our societies: unmanageability is just
around the corner unless we do not complement the &quot;old recipes&quot; with smarter
forms of social organization. Here we analyze this problem and propose a
refinement to our fractal social organizations as a model for resilient
cyber-physical societies. Evidence to our claims is provided by simulating our
model in terms of multi-agent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02765</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02765</id><created>2015-05-10</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Dropshipping - a new business revolution</title><categories>cs.CY</categories><comments>11 pages, in Russian. ISSN 0131-7652</comments><journal-ref>ECO. 2013. No 2 (192). P. 128-141</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article about the new form of trade in a network the Internet. Advantages of
dropshipping open to the Russian manufacturers and businessmen new horizons in
international trade and marketing. The author considers features of evolution
and a role of dropshipping in electronic commerce of a new millennium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02766</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02766</id><created>2015-05-10</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Features of transformation of marketing in e-commerce</title><categories>cs.CY</categories><comments>4 pages, in Russian</comments><journal-ref>Omsk Scientific Bulletin. 2013. No 1 (115). P. 55-58. ISSN
  1813-8225</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article about influence of e-commerce on transformation of the theory and
practice of marketing. The author considers Internet-marketing as the
independent form of marketing formed under the general laws in new
institutional conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02767</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02767</id><created>2015-05-10</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Marketing features of dropshipping in system of e-commerce</title><categories>cs.CY</categories><comments>13 pages, in Russian. ISSN 2071-3762</comments><journal-ref>Practical marketing. 2013. No 6 (196). P. 15-28.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today dropshipping wins the Internet promptly and transformed to one of the
basic tools of marketing in e-commerce. Marketing features, mechanisms and
value dropshipping in the conditions of network economy of the XXI century
reveal in article. The author carries out the comparative analysis of
institutional development dropshipping in the USA, China and Russia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02776</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02776</id><created>2015-05-11</created><updated>2015-10-19</updated><authors><author><keyname>Brandao</keyname><forenames>Fernando G. S. L.</forenames></author><author><keyname>Cubitt</keyname><forenames>Toby S.</forenames></author><author><keyname>Lucia</keyname><forenames>Angelo</forenames></author><author><keyname>Michalakis</keyname><forenames>Spyridon</forenames></author><author><keyname>Perez-Garcia</keyname><forenames>David</forenames></author></authors><title>Area law for fixed points of rapidly mixing dissipative quantum systems</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>17 pages, 1 figure. Final version</comments><journal-ref>J. Math. Phys. 56, 102202 (2015)</journal-ref><doi>10.1063/1.4932612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove an area law with a logarithmic correction for the mutual information
for fixed points of local dissipative quantum system satisfying a rapid mixing
condition, under either of the following assumptions: the fixed point is pure,
or the system is frustration free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02798</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02798</id><created>2015-05-11</created><authors><author><keyname>Zanibbi</keyname><forenames>Richard</forenames></author><author><keyname>Orakwue</keyname><forenames>Awelemdy</forenames></author></authors><title>Math Search for the Masses: Multimodal Search Interfaces and
  Appearance-Based Retrieval</title><categories>cs.IR</categories><comments>Paper for Invited Talk at 2015 Conference on Intelligent Computer
  Mathematics (July, Washington DC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We summarize math search engines and search interfaces produced by the
Document and Pattern Recognition Lab in recent years, and in particular the min
math search interface and the Tangent search engine. Source code for both
systems are publicly available. &quot;The Masses&quot; refers to our emphasis on creating
systems for mathematical non-experts, who may be looking to define unfamiliar
notation, or browse documents based on the visual appearance of formulae rather
than their mathematical semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02799</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02799</id><created>2015-05-11</created><authors><author><keyname>Pfander</keyname><forenames>G&#xf6;tz E.</forenames></author><author><keyname>Zheltov</keyname><forenames>Pavel</forenames></author></authors><title>Sampling of stochastic operators</title><categories>cs.IT math.FA math.IT</categories><msc-class>Primary 94A20, 94A05, 60G20, 42C15, Secondary 47G99</msc-class><journal-ref>Information Theory, IEEE Transactions on 60.4 (2014): 2359-2372</journal-ref><doi>10.1109/TIT.2014.2301444</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop sampling methodology aimed at determining stochastic operators
that satisfy a support size restriction on the autocorrelation of the operators
stochastic spreading function. The data that we use to reconstruct the operator
(or, in some cases only the autocorrelation of the spreading function) is based
on the response of the unknown operator to a known, deterministic test signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02810</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02810</id><created>2015-05-11</created><authors><author><keyname>Rad</keyname><forenames>Amir Afrasiabi</forenames></author><author><keyname>Flocchini</keyname><forenames>Paola</forenames></author><author><keyname>Gaudet</keyname><forenames>Joanne</forenames></author></authors><title>Tempus Fugit: The Impact of Time in Knowledge Mobilization Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The temporal component of social networks is often neglected in their
analysis, and statistical measures are typically performed on a &quot;static&quot;
representation of the network. As a result, measures of importance (like
betweenness centrality) cannot reveal any temporal role of the entities
involved. Our goal is to start filling this limitation by proposing a form of
temporal betweenness measure, and by using it to analyse a knowledge
mobilization network. We show that this measure, which takes time explicitly
into account, allows us to detect centrality roles that were completely hidden
in the classical statistical analysis. In particular, we uncover nodes whose
static centrality was considered negligible, but whose temporal role is instead
important to accelerate mobilization flow in the network. We also observe the
reverse behaviour by detecting nodes with high static centrality, whose role as
temporal bridges is instead very low. By revealing important temporal roles,
this study is a first step towards a better understanding of the impact of time
in social networks, and opens the road to further investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02818</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02818</id><created>2015-05-11</created><authors><author><keyname>Tsapeli</keyname><forenames>Fani</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Investigating Causality in Human Behavior from Smartphone Sensor Data: A
  Quasi-Experimental Approach</title><categories>cs.CY</categories><journal-ref>EPJ Data Science, 2015</journal-ref><doi>10.1140/epjds/s13688-015-0061-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smartphones have become an indispensable part of our daily life. Their
improved sensing and computing capabilities bring new opportunities for human
behavior monitoring and analysis. Most work so far has been focused on
detecting correlation rather than causation among features extracted from
smartphone data. However, pure correlation analysis does not offer sufficient
understanding of human behavior. Moreover, causation analysis could allow
scientists to identify factors that have a causal effect on health and
well-being issues, such as obesity, stress, depression and so on and suggest
actions to deal with them. Finally, detecting causal relationships in this kind
of observational data is challenging since, in general, subjects cannot be
randomly exposed to an event.
  In this article, we discuss the design, implementation and evaluation of a
generic quasi-experimental framework for conducting causation studies on human
behavior from smartphone data. We demonstrate the effectiveness of our approach
by investigating the causal impact of several factors such as exercise, social
interactions and work on stress level. Our results indicate that exercising and
spending time outside home and working environment have a positive effect on
participants stress level while reduced working hours only slightly impact
stress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02820</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02820</id><created>2015-05-11</created><updated>2015-09-25</updated><authors><author><keyname>Barbay</keyname><forenames>Jeremy</forenames></author><author><keyname>Ochoa</keyname><forenames>Carlos</forenames></author><author><keyname>Perez-Lantero</keyname><forenames>Pablo</forenames></author></authors><title>Refining the Analysis of Divide and Conquer: How and When</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Divide-and-conquer is a central paradigm for the design of algorithms,
through which some fundamental computational problems, such as sorting arrays
and computing convex hulls, are solved in optimal time within
$\Theta(n\log{n})$ in the worst case over instances of size $n$. A finer
analysis of those problems yields complexities within $O(n(1 + \mathcal{H}(n_1,
\dots, n_k))) \subseteq O(n(1{+}\log{k})) \subseteq O(n\log{n})$ in the worst
case over all instances of size $n$ composed of $k$ &quot;easy&quot; fragments of
respective sizes $n_1, \dots, n_k$ summing to $n$, where the entropy function
$\mathcal{H}(n_1, \dots, n_k) = \sum_{i=1}^k{\frac{n_i}{n}}\log{\frac{n}{n_i}}$
measures the &quot;difficulty&quot; of the instance. We consider whether such refined
analysis can be applied to other algorithms based on divide-and-conquer, such
as polynomial multiplication, input-order adaptive computation of convex hulls
in 2D and 3D, and computation of Delaunay triangulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02824</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02824</id><created>2015-05-11</created><updated>2015-07-01</updated><authors><author><keyname>Fern&#xe1;ndez-Duque</keyname><forenames>David</forenames></author></authors><title>Perfectly secure data aggregation via shifted projections</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a general scenario where confidential information is distributed
among a group of agents who wish to share it in such a way that the data
becomes common knowledge among them but an eavesdropper intercepting their
communications would be unable to obtain any of said data. The information is
modelled as a deck of cards dealt among the agents, so that after the
information is exchanged, all of the communicating agents must know the entire
deal, but the eavesdropper must remain ignorant about who holds each card.
  Valentin Goranko and the author previously set up this scenario as the secure
aggregation of distributed information problem and constructed weakly safe
protocols, where given any card $c$, the eavesdropper does not know with
certainty which agent holds $c$. Here we present a perfectly safe protocol,
which does not alter the eavesdropper's perceived probability that any given
agent holds $c$. In our protocol, one of the communicating agents holds a
larger portion of the cards than the rest, but we show how for infinitely many
values of $a$, the number of cards may be chosen so that each of the $m$ agents
holds more than $a$ cards and less than $2m^2a$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02826</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02826</id><created>2015-05-11</created><authors><author><keyname>Hu</keyname><forenames>Xiuli</forenames></author><author><keyname>Hong</keyname><forenames>Pangbei</forenames></author><author><keyname>Li</keyname><forenames>Bing</forenames></author></authors><title>Benefit of Multipath TCP on the Stability of Network</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multipath-TCP receives a lot of attention recently and can potentially
improve quality of service for both private and commercial users. It leverages
the multiple available paths and send packets through all the available paths.
The growing of Mutipath TCP has received a growing interest from both
researchers who publish a growing number of articles on the topic and the
vendors since Apple has decided to use Multipath TCP on its smartphones and
tablets to support the Siri voice recognition application. In this paper, we
study the performance of Multipath TCP from its impact on the stability of the
network. In particular, we study three scenarios, Internet, which is the
largest networks and involves heterogeneous traffic, data center, which is
smaller but has different traffic patterns compared with Internet scale network
and wireless network, whose energy consumption also needs to be considered. Our
study shows that stability is affected but not seriouly for Internet and
wireless network, but datacenter network stability is seriously affected due to
its bursty traffic pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02829</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02829</id><created>2015-05-11</created><authors><author><keyname>Dias</keyname><forenames>Elis&#xe2;ngela Silva</forenames></author><author><keyname>Castonguay</keyname><forenames>Diane</forenames></author></authors><title>Polynomial enumeration of chordless cycles on cyclically orientable
  graphs</title><categories>cs.DS cs.DM</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a finite undirected simple graph, a chordless cycle is an induced subgraph
which is a cycle. A graph is called cyclically orientable if it admits an
orientation in which every chordless cycle is cyclically oriented. We propose
an algorithm to enumerate all chordless cycles of such a graph. Compared to
other similar algorithms, the proposed algorithm have the advantage of finding
each chordless cycle only once in time complexity $\mathcal{O}(n^2)$ in the
input size, where $n$ is the number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02830</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02830</id><created>2015-05-11</created><authors><author><keyname>Liu</keyname><forenames>Yun-Ching</forenames></author><author><keyname>Tsuruoka</keyname><forenames>Yoshimasa</forenames></author></authors><title>Adapting Improved Upper Confidence Bounds for Monte-Carlo Tree Search</title><categories>cs.AI</categories><comments>To appear in the 14th International Conference on Advances in
  Computer Games (ACG 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The UCT algorithm, which combines the UCB algorithm and Monte-Carlo Tree
Search (MCTS), is currently the most widely used variant of MCTS. Recently, a
number of investigations into applying other bandit algorithms to MCTS have
produced interesting results. In this research, we will investigate the
possibility of combining the improved UCB algorithm, proposed by Auer et al.
(2010), with MCTS. However, various characteristics and properties of the
improved UCB algorithm may not be ideal for a direct application to MCTS.
Therefore, some modifications were made to the improved UCB algorithm, making
it more suitable for the task of game tree search. The Mi-UCT algorithm is the
application of the modified UCB algorithm applied to trees. The performance of
Mi-UCT is demonstrated on the games of $9\times 9$ Go and $9\times 9$ NoGo, and
has shown to outperform the plain UCT algorithm when only a small number of
playouts are given, and rougly on the same level when more playouts are
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02847</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02847</id><created>2015-05-11</created><authors><author><keyname>O'Callaghan</keyname><forenames>Patrick H. D.</forenames></author></authors><title>Minimal conditions for parametric continuity of a utility representation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependence on the parameter is continuous when perturbations of the parameter
preserves strict preference for one alternative over another. We characterise
this property via a utility function over alternatives that depends
continuously on the parameter. The class of parameter spaces where such a
representation is guaranteed to exist is also identified. When the parameter is
the type or belief of a player, these results have implications for Bayesian
and psychological games. When alternatives are discrete, the representation is
jointly continuous and an extension of Berge's theorem of the maximum yields a
continuous value function. We apply this result to generalise a standard
consumer choice problem where parameters are price-wealth vectors. When the
parameter space is lexicographically ordered, a novel application to
reference-dependent preferences is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02850</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02850</id><created>2015-05-11</created><authors><author><keyname>Lu</keyname><forenames>Xiaotao</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Buffer-Aided Relay Selection Algorithms for Physical-Layer Security in
  Wireless Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures in WSA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the use of buffer-aided relays, linear precoding
techniques and multiple antennas for physical-layer security in wireless
networks. We develop relay selection algorithms to improve the secrecy-rate
performance of cooperative multi-user multiple-antenna wireless networks. In
particular, we propose a novel finite buffer-aided relay selection algorithm
that employs the maximum likelihood (ML) criterion to select sets of relays
which fully exploit the flexibility offered by relay nodes equipped with
buffers. Numerical results show the benefits of the proposed techniques as
compared to prior art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02851</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02851</id><created>2015-05-11</created><authors><author><keyname>Kaddoum</keyname><forenames>Georges</forenames></author><author><keyname>El-Hajjar</keyname><forenames>Mohammed</forenames></author></authors><title>Analysis of Network Coding Schemes for Differential Chaos Shift Keying
  Communication System</title><categories>cs.IT math.IT</categories><comments>Submitted for potential publication (journal paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we design network coding schemes for Differential Chaos Shift
Keying (DCSK) modulation. In this work, non-coherent chaos-based communication
system is used due to its simplicity and robustness to multipath propagation
effects, while dispensing with any channel state information knowledge at the
receiver. We propose a relay network using network coding and DCSK, where we
first present a Physical layer Network Coding (PNC) scheme with two users,
$\mathcal{A}$ and $\mathcal{B}$, sharing the same spreading code and bandwidth,
while synchronously transmitting their signals to the relay node $\mathcal{R}$.
We show that the main drawback of this design in multipath channels is the high
level of interference in the resultant signal, which severely degrades the
system performance. Hence, in order to address this problem, we propose two
coding schemes, which separate the users' signals in the frequency or the time
domains. We show also in this paper that the performance of the Analog Network
Coding (ANC) with DCSK modulation suffers from the same interference problem as
the PNC scheme. We present the analytical bit error rate performance for
multipath Rayleigh fading channel for the different scenarios and we analyse
these schemes in terms of complexity, throughput and link spectral efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02855</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02855</id><created>2015-05-11</created><updated>2015-10-02</updated><authors><author><keyname>Barbay</keyname><forenames>J&#xe9;r&#xe9;my</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author><author><keyname>Rojas-Ledesma</keyname><forenames>Javiel</forenames></author></authors><title>Adaptive Computation of the Klee's Measure in High Dimensions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The KLEE'S MESURE of $n$ axis-parallel boxes in $\mathbb{R}^d$ is the volume
of their union. It can be computed in time within $O(n^{d/2})$ in the worst
case. We describe three techniques to boost its computation: one based on some
type of &quot;degeneracy'' of the input, and two ones on the inherent &quot;easiness'' of
the structure of the input. The first technique benefits from instances where
the MAXIMA of the input is of small size $h$, and yields a solution running in
time within $O(n\log^{2d-2}{h}+ h^{d/2}) \subseteq O(n^{d/2}$). The second
technique takes advantage of instances where no $d$-dimensional axis-aligned
hyperplane intersects more than $k$ boxes in some dimension, and yields a
solution running in time within $O(n \log n + n k^{(d-2)/2}) \subseteq
O(n^{d/2})$. The third technique takes advantage of instances where the
\emph{intersection graph} of the input has small treewidth $\omega$. It yields
an algorithm running in time within $O(n^4\omega \log \omega + n (\omega \log
\omega)^{d/2})$ in general, and in time within $O(n \log n + n \omega ^{d/2})$
if an optimal tree decomposition of the intersection graph is given. We show
how to combine these techniques in an algorithm which takes advantage of all
three configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02862</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02862</id><created>2015-05-11</created><updated>2016-01-15</updated><authors><author><keyname>Cheng</keyname><forenames>Fan</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>A Numerical Study on the Wiretap Network with a Simple Network Topology</title><categories>cs.IT math.IT</categories><comments>Submitted to IT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a security problem on a simple wiretap network,
consisting of a source node S, a destination node D, and an intermediate node
R. The intermediate node connects the source and the destination nodes via a
set of noiseless parallel channels, with sizes $n_1$ and $n_2$, respectively. A
message $M$ is to be sent from S to D. The information in the network may be
eavesdropped by a set of wiretappers. The wiretappers cannot communicate with
one another. Each wiretapper can access a subset of channels, called a wiretap
set. All the chosen wiretap sets form a wiretap pattern. A random key $K$ is
generated at S and a coding scheme on $(M, K)$ is employed to protect $M$. We
define two decoding classes at D: In Class-I, only $M$ is required to be
recovered and in Class-II, both $M$ and $K$ are required to be recovered. The
objective is to minimize $H(K)/H(M)$ {for a given wiretap pattern} under the
perfect secrecy constraint. The first question we address is whether routing is
optimal on this simple network. By enumerating all the wiretap patterns on the
Class-I/II $(3,3)$ networks and harnessing the power of Shannon-type
inequalities, we find that gaps exist between the bounds implied by routing and
the bounds implied by Shannon-type inequalities for a small fraction~($&lt;2\%$)
of all the wiretap patterns. The second question we investigate is the
following: What is $\min H(K)/H(M)$ for the remaining wiretap patterns where
gaps exist? We study some simple wiretap patterns and find that their Shannon
bounds (i.e., the lower bound induced by Shannon-type inequalities) can be
achieved by linear codes, which means routing is not sufficient even for the
($3$, $3$) network. For some complicated wiretap patterns, we study the
structures of linear coding schemes under the assumption that they can achieve
the corresponding Shannon bounds....
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02865</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02865</id><created>2015-05-11</created><updated>2015-12-17</updated><authors><author><keyname>Cowan</keyname><forenames>Wesley</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>Asymptotic Behavior of Minimal-Exploration Allocation Policies: Almost
  Sure, Arbitrarily Slow Growing Regret</title><categories>stat.ML cs.LG</categories><msc-class>62L10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to provide further understanding into the
structure of the sequential allocation (&quot;stochastic multi-armed bandit&quot;, or
MAB) problem by establishing probability one finite horizon bounds and
convergence rates for the sample (or &quot;pseudo&quot;) regret associated with two
simple classes of allocation policies $\pi$.
  For any slowly increasing function $g$, subject to mild regularity
constraints, we construct two policies (the $g$-Forcing, and the $g$-Inflated
Sample Mean) that achieve a measure of regret of order $ O(g(n))$ almost surely
as $n \to \infty$, bound from above and below. Additionally, almost sure upper
and lower bounds on the remainder term are established. In the constructions
herein, the function $g$ effectively controls the &quot;exploration&quot; of the
classical &quot;exploration/exploitation&quot; tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02867</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02867</id><created>2015-05-11</created><authors><author><keyname>Mathy</keyname><forenames>Charles</forenames></author><author><keyname>Derbinsky</keyname><forenames>Nate</forenames></author><author><keyname>Bento</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Rosenthal</keyname><forenames>Jonathan</forenames></author><author><keyname>Yedidia</keyname><forenames>Jonathan</forenames></author></authors><title>The Boundary Forest Algorithm for Online Supervised and Unsupervised
  Learning</title><categories>cs.LG cs.DS cs.IR stat.ML</categories><comments>7 pages, 4 figs, 1 page supp. info</comments><journal-ref>Proc. of the 29th AAAI Conference on Artificial Intelligence
  (AAAI), 2864-2870. Austin, TX, USA. (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new instance-based learning algorithm called the Boundary
Forest (BF) algorithm, that can be used for supervised and unsupervised
learning. The algorithm builds a forest of trees whose nodes store previously
seen examples. It can be shown data points one at a time and updates itself
incrementally, hence it is naturally online. Few instance-based algorithms have
this property while being simultaneously fast, which the BF is. This is crucial
for applications where one needs to respond to input data in real time. The
number of children of each node is not set beforehand but obtained from the
training procedure, which makes the algorithm very flexible with regards to
what data manifolds it can learn. We test its generalization performance and
speed on a range of benchmark datasets and detail in which settings it
outperforms the state of the art. Empirically we find that training time scales
as O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N
the amount of data,
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02870</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02870</id><created>2015-05-12</created><authors><author><keyname>Brenner</keyname><forenames>Eliot</forenames></author><author><keyname>Sontag</keyname><forenames>David</forenames></author></authors><title>Incorporating Type II Error Probabilities from Independence Tests into
  Score-Based Learning of Bayesian Network Structure</title><categories>cs.LG stat.ML</categories><comments>118 pages, 13 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new consistent scoring function for structure learning of Bayesian
networks. In contrast to traditional approaches to score-based structure
learning, such as BDeu or MDL, the complexity penalty that we propose is
data-dependent and is given by the probability that a conditional independence
test correctly shows that an edge cannot exist. What really distinguishes this
new scoring function from earlier work is that it has the property of becoming
computationally easier to maximize as the amount of data increases. We prove a
polynomial sample complexity result, showing that maximizing this score is
guaranteed to correctly learn a structure with no false edges and a
distribution close to the generating distribution, whenever there exists a
Bayesian network which is a perfect map for the data generating distribution.
Although the new score can be used with any search algorithm, in our related
UAI 2013 paper [BS13], we have given empirical results showing that it is
particularly effective when used together with a linear programming relaxation
approach to Bayesian network structure learning. The present paper contains all
details of the proofs of the finite-sample complexity results in [BS13] as well
as detailed explanation of the computation of the certain error probabilities
called beta-values, whose precomputation and tabulation is necessary for the
implementation of the algorithm in [BS13].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02871</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02871</id><created>2015-05-12</created><authors><author><keyname>Buehler</keyname><forenames>Edward A.</forenames></author><author><keyname>Paulson</keyname><forenames>Joel A.</forenames></author><author><keyname>Akhavan</keyname><forenames>Ali</forenames></author><author><keyname>Mesbah</keyname><forenames>Ali</forenames></author></authors><title>Lyapunov-based Stochastic Nonlinear Model Predictive Control: Shaping
  the State Probability Density Functions</title><categories>math.OC cs.SY</categories><comments>Submitted to the 54th IEEE Conference on Decision and Control, Osaka,
  Japan, December 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic uncertainties in complex dynamical systems lead to variability of
system states, which can in turn degrade the closed-loop performance. This
paper presents a stochastic model predictive control approach for a class of
nonlinear systems with unbounded stochastic uncertainties. The control approach
aims to shape probability density function of the stochastic states, while
satisfying input and joint state chance constraints. Closed-loop stability is
ensured by designing a stability constraint in terms of a stochastic control
Lyapunov function, which explicitly characterizes stability in a probabilistic
sense. The Fokker-Planck equation is used for describing the dynamic evolution
of the states' probability density functions. Complete characterization of
probability density functions using the Fokker-Planck equation allows for
shaping the states' density functions as well as direct computation of joint
state chance constraints. The closed-loop performance of the stochastic control
approach is demonstrated using a continuous stirred-tank reactor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02875</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02875</id><created>2015-05-12</created><authors><author><keyname>Su</keyname><forenames>Sihong</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author></authors><title>On the Systematic Constructions of Rotation Symmetric Bent Functions
  with Any Possible Algebraic Degrees</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the literature, few constructions of $n$-variable rotation symmetric bent
functions have been presented, which either have restriction on $n$ or have
algebraic degree no more than $4$. In this paper, for any even integer
$n=2m\ge2$, a first systemic construction of $n$-variable rotation symmetric
bent functions, with any possible algebraic degrees ranging from $2$ to $m$, is
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02878</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02878</id><created>2015-05-12</created><updated>2015-05-16</updated><authors><author><keyname>Hashimoto</keyname><forenames>Kodai</forenames></author><author><keyname>Unno</keyname><forenames>Hiroshi</forenames></author></authors><title>Refinement Type Inference via Horn Constraint Optimization</title><categories>cs.PL</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method for inferring refinement types of higher-order
functional programs. The main advantage of the proposed method is that it can
infer maximally preferred (i.e., Pareto optimal) refinement types with respect
to a user-specified preference order. The flexible optimization of refinement
types enabled by the proposed method paves the way for interesting
applications, such as inferring most-general characterization of inputs for
which a given program satisfies (or violates) a given safety (or termination)
property. Our method reduces such a type optimization problem to a Horn
constraint optimization problem by using a new refinement type system that can
flexibly reason about non-determinism in programs. Our method then solves the
constraint optimization problem by repeatedly improving a current solution
until convergence via template-based invariant generation. We have implemented
a prototype inference system based on our method, and obtained promising
results in preliminary experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02884</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02884</id><created>2015-05-12</created><authors><author><keyname>Liang</keyname><forenames>Po-Huei</forenames></author><author><keyname>Yang</keyname><forenames>Jiann-Min</forenames></author></authors><title>Evaluation of Two-Level Load Balancing Framework in Cloud Environment</title><categories>cs.NI cs.DC</categories><comments>11 pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 7, No 2, April 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With technological advancements and constant changes of Internet, cloud
computing has been today's trend. With the lower cost and convenience of cloud
computing services, users have increasingly put their Web resources and
information in the cloud environment. The availability and reliability of the
client systems will become increasingly important. Today cloud applications
slightest interruption, the impact will be significant for users. It is an
important issue that how to ensure reliability and stability of the cloud
sites. Load balancing would be one good solution. This paper presents a
framework for global server load balancing of the Web sites in a cloud with
two-level load balancing model. The proposed framework is intended for adapting
an open-source load-balancing system and the framework allows the network
service provider to deploy a load balancer in different data centers
dynamically while the customers need more load balancers for increasing the
availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02890</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02890</id><created>2015-05-12</created><updated>2015-08-25</updated><authors><author><keyname>Graham</keyname><forenames>Ben</forenames></author></authors><title>Sparse 3D convolutional neural networks</title><categories>cs.CV</categories><comments>BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have implemented a convolutional neural network designed for processing
sparse three-dimensional input data. The world we live in is three dimensional
so there are a large number of potential applications including 3D object
recognition and analysis of space-time objects. In the quest for efficiency, we
experiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02891</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02891</id><created>2015-05-12</created><authors><author><keyname>Elsayed</keyname><forenames>Abdelrahman</forenames></author><author><keyname>Mokhtar</keyname><forenames>Hoda M. O.</forenames></author><author><keyname>Ismail</keyname><forenames>Osama</forenames></author></authors><title>Ontology Based Document Clustering Using MapReduce</title><categories>cs.DB cs.IR</categories><comments>12 page</comments><journal-ref>The International Journal of Database Management Systems (IJDMS),
  April 2015, Volume 7, Number 2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, document clustering is considered as a data intensive task due to
the dramatic, fast increase in the number of available documents. Nevertheless,
the features that represent those documents are also too large. The most common
method for representing documents is the vector space model, which represents
document features as a bag of words and does not represent semantic relations
between words. In this paper we introduce a distributed implementation for the
bisecting k-means using MapReduce programming model. The aim behind our
proposed implementation is to solve the problem of clustering intensive data
documents. In addition, we propose integrating the WordNet ontology with
bisecting k-means in order to utilize the semantic relations between words to
enhance document clustering results. Our presented experimental results show
that using lexical categories for nouns only enhances internal evaluation
measures of document clustering; and decreases the documents features from
thousands to tens features. Our experiments were conducted using Amazon Elastic
MapReduce to deploy the Bisecting k-means algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02896</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02896</id><created>2015-05-12</created><authors><author><keyname>Nam</keyname><forenames>Junyoung</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Ko</keyname><forenames>Young-Jo</forenames></author><author><keyname>Ha</keyname><forenames>Jeongseok</forenames></author></authors><title>On the Role of Transmit Correlation Diversity in Multiuser MIMO Systems</title><categories>cs.IT math.IT</categories><comments>38 pages, 8 figures, submitted to IEEE Transaction on Information
  Theory. arXiv admin note: substantial text overlap with arXiv:1401.7114</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation across transmit antennas, in multiple antenna systems (MIMO), has
been studied in various scenarios and has been shown to be detrimental or
provide benefits depending on the particular system and underlying assumptions.
In this paper, we investigate the effect of transmit correlation on the
capacity of the Gaussian MIMO broadcast channel (BC), with a particular
interest in the large-scale array (or massive MIMO) regime. To this end, we
introduce a new type of diversity, referred to as transmit correlation
diversity, which captures the fact that the channel vectors of different users
may have different, and often nearly mutually orthogonal, large-scale channel
eigen-directions. In particular, when taking the cost of downlink training
properly into account, transmit correlation diversity can yield significant
capacity gains in all regimes of interest. Our analysis shows that the system
multiplexing gain can be increased by a factor up to $\lfloor{M}/{r}\rfloor$,
where $M$ is the number of antennas and $r\le M$ is the common rank of the
users transmit correlation matrices, with respect to standard schemes that are
agnostic of the transmit correlation and treat the channels as if they were
isotropically distributed. Thus, this new form of diversity reveals itself as a
valuable &quot;new resource&quot; in multiuser communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02898</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02898</id><created>2015-05-12</created><authors><author><keyname>Singh</keyname><forenames>Bikramjit</forenames></author><author><keyname>Koufos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author></authors><title>Coordination protocol for inter-operator spectrum sharing based on
  spectrum usage favors</title><categories>cs.NI</categories><comments>Published in proceedings of 23rd edition of European Conference on
  Networks and Communications (EuCNC), Bologna, Jun. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, mobile network operators are allocated spectrum bands on an
exclusive basis. While this approach facilitates interference control, it may
also result in low spectrum utilization efficiency. Inter-operator spectrum
sharing is a potential method to enhance spectrum utilization. In order to
realize it, a protocol to coordinate the actions of operators is needed. We
propose a spectrum sharing protocol which is distributed in nature, it does not
require operator-specific information exchange and it incurs minimal
communication overhead between the operators. Operators are still free to
decide whether they share spectrum or not as the protocol is based on the book
keeping of spectrum usage favors, asked and received by the operators. We show
that operators can enhance their QoS in comparison with traditional orthogonal
spectrum allocation while also maintaining reciprocity i.e. no operator
benefits over the other in the long run. We demonstrate the usability of the
proposed protocol in an indoor deployment scenario with frequent network load
variations as expected to have in small cell deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02903</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02903</id><created>2015-05-12</created><updated>2016-02-01</updated><authors><author><keyname>Karpuk</keyname><forenames>David</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author></authors><title>Locally Diverse Constellations from the Special Orthogonal Group</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To optimize rotated, multidimensional constellations over a single-input,
single-output Rayleigh fading channel, a family of rotation matrices is
constructed for all dimensions which are a power of 2. This family is a
one-parameter subgroup of the group of rotation matrices, and is located using
a gradient descent scheme on this Lie group. The parameter defining the family
is chosen to optimize the cutoff rate of the constellation. The optimal
rotation parameter is computed explicitly for low signal-to-noise ratios.
  These rotations outperform full-diversity algebraic rotations in terms of
cutoff rate at low SNR (signal-to-noise ratio) and bit error rate at high SNR
in dimension $n = 4$. However, a QAM (quadrature amplitude modulation)
constellation rotated by such a matrix lacks full diversity, in contrast with
the conventional wisdom that good signal sets exhibit full diversity. A new
notion of diversity, referred to as local diversity, is introduced to attempt
to account for this behavior. Roughly, a locally fully diverse constellation is
fully diverse only in small neighborhoods. A local variant of the minimum
product distance is also introduced and is shown experimentally to be a
superior predictor of constellation performance than the minimum product
distance in dimension $n = 4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02905</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02905</id><created>2015-05-12</created><authors><author><keyname>Azfar</keyname><forenames>Abdullah</forenames></author><author><keyname>Choo</keyname><forenames>Kim-Kwang Raymond</forenames></author><author><keyname>Liu</keyname><forenames>Lin</forenames></author></authors><title>Forensic Taxonomy of Popular Android mHealth Apps</title><categories>cs.CY</categories><comments>Proceedings of 21st Americas Conference on Information Systems (AMCIS
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile health applications (or mHealth apps, as they are commonly known) are
increasingly popular with both individual end users and user groups such as
physicians. Due to their ability to access, store and transmit personally
identifiable and sensitive information (e.g. geolocation information and
personal details), they are potentially an important source of evidentiary
materials in digital investigations. In this paper, we examine 40 popular
Android mHealth apps. Based on our findings, we propose a taxonomy
incorporating artefacts of forensic interest to facilitate the timely
collection and analysis of evidentiary materials from mobile devices involving
the use of such apps. Artefacts of forensic interest recovered include user
details and email addresses, chronology of user locations and food habits. We
are also able to recover user credentials (e.g. user password and four-digit
app login PIN number), locate user profile pictures and identify timestamp
associated with the location of a user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02906</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02906</id><created>2015-05-12</created><authors><author><keyname>Farnden</keyname><forenames>Jody</forenames></author><author><keyname>Martini</keyname><forenames>Ben</forenames></author><author><keyname>Choo</keyname><forenames>Kim-Kwang Raymond</forenames></author></authors><title>Privacy Risks in Mobile Dating Apps</title><categories>cs.CY</categories><comments>Proceedings of 21st Americas Conference on Information Systems (AMCIS
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dating apps for mobile devices, one popular GeoSocial app category, are
growing increasingly popular. These apps encourage the sharing of more personal
information than conventional social media apps, including continuous location
data. However, recent high profile incidents have highlighted the privacy risks
inherent in using these apps. In this paper, we present a case study utilizing
forensic techniques on nine popular proximity-based dating apps in order to
determine the types of data that can be recovered from user devices. We recover
a number of data types from these apps that raise concerns about user privacy.
For example, we determine that chat messages could be recovered in at least
half of the apps examined and, in some cases, the details of any users that had
been discovered nearby could also be extracted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02908</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02908</id><created>2015-05-12</created><authors><author><keyname>Rahman</keyname><forenames>Nurul Hidayah Ab</forenames></author><author><keyname>Choo</keyname><forenames>Kim-Kwang Raymond</forenames></author></authors><title>Factors Influencing the Adoption of Cloud Incident Handling Strategy: A
  Preliminary Study in Malaysia</title><categories>cs.CY</categories><comments>Proceedings of 21st Americas Conference on Information Systems (AMCIS
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study seeks to understand the factors influencing the adoption of an
incident handling strategy by organisational cloud service users. We propose a
conceptual model that draws upon the Situation Awareness (SA) model and
Protection Motivation Theory (PMT) to guide this research. 40 organisational
cloud service users in Malaysia were surveyed. We also conduct face-to-face
interviews with participants from four of the organisations. Findings from the
study indicate that four PMT factors (Perceived Vulnerability, Self-Efficacy,
Response Efficacy, and Perceived Severity) have a significantly influence on
the adoption of cloud incident handling strategy within the organisations. We,
therefore, suggest a successful adoption cloud incident handling strategy by
organisational cloud service users involves the nexus between these four PMT
factors. We also outline future research required to validate the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02910</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02910</id><created>2015-05-12</created><updated>2016-02-23</updated><authors><author><keyname>Tolstikhin</keyname><forenames>Ilya</forenames></author><author><keyname>Zhivotovskiy</keyname><forenames>Nikita</forenames></author><author><keyname>Blanchard</keyname><forenames>Gilles</forenames></author></authors><title>Permutational Rademacher Complexity: a New Complexity Measure for
  Transductive Learning</title><categories>stat.ML cs.LG</categories><comments>Corrected error in Inequality (1)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transductive learning considers situations when a learner observes $m$
labelled training points and $u$ unlabelled test points with the final goal of
giving correct answers for the test points. This paper introduces a new
complexity measure for transductive learning called Permutational Rademacher
Complexity (PRC) and studies its properties. A novel symmetrization inequality
is proved, which shows that PRC provides a tighter control over expected
suprema of empirical processes compared to what happens in the standard i.i.d.
setting. A number of comparison results are also provided, which show the
relation between PRC and other popular complexity measures used in statistical
learning theory, including Rademacher complexity and Transductive Rademacher
Complexity (TRC). We argue that PRC is a more suitable complexity measure for
transductive learning. Finally, these results are combined with a standard
concentration argument to provide novel data-dependent risk bounds for
transductive learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02911</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02911</id><created>2015-05-12</created><authors><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Resource Allocation in Full-Duplex Communications for Future Wireless
  Networks</title><categories>cs.IT math.IT</categories><comments>20 pages, 7 figures, accepated in IEEE Wireless Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent significant progress in realizing full-duplex~(FD) systems has
opened up a promising avenue for improving quality of service (QoS) and quality
of experience (QoE) in future wireless networks. There is an urgent need to
address the diverse set of challenges regarding different aspects of FD network
design, theory, and development. In addition to the self-interference
cancelation signal processing algorithms, network protocols such as resource
management are also essential in the practical design and implementation of FD
wireless networks. This article aims to present the latest development and
future directions of resource allocation in different full duplex systems by
exploring the network resources in different domains, including power, space,
frequency, and device dimensions. Four representative application scenarios are
considered: FD MIMO networks, FD cooperative networks, FD OFDMA cellular
networks, and FD heterogeneous networks. Resource management problems and novel
algorithms in these systems are presented, and key open research directions are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02921</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02921</id><created>2015-05-12</created><updated>2015-11-18</updated><authors><author><keyname>Bianco</keyname><forenames>Simone</forenames></author><author><keyname>Ciocca</keyname><forenames>Gianluigi</forenames></author><author><keyname>Schettini</keyname><forenames>Raimondo</forenames></author></authors><title>How Far Can You Get By Combining Change Detection Algorithms?</title><categories>cs.CV</categories><comments>Submitted to IEEE Transactions on Image Processing</comments><acm-class>I.4.8; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate how state-of-the-art change detection algorithms
can be combined and used to create a more robust change algorithm leveraging
their individual peculiarities. We exploited Genetic Programming (GP) to
automatically select the best algorithms, combine them in different ways, and
perform the most suitable post-processing operations on the outputs of the
algorithms. In particular, algorithms' combination and post-processing
operations are achieved with unary, binary and $n$-ary functions embedded into
the GP framework. Using different experimental settings for combining existing
algorithms we obtained different GP solutions that we termed IUTIS (In Unity
There Is Strength). These solutions are then compared against state-of-the-art
change detection algorithms on the video sequences and ground truth annotations
of the ChandeDetection.net (CDNET 2014) challenge. Results demonstrate that
using GP, our solutions are able to outperform all the considered single
state-of-the-art change detection algorithms, as well as other combination
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02934</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02934</id><created>2015-05-12</created><updated>2015-07-12</updated><authors><author><keyname>Shlezinger</keyname><forenames>Nir</forenames></author><author><keyname>Dabora</keyname><forenames>Ron</forenames></author></authors><title>On the Capacity of Narrowband PLC Channels</title><categories>cs.IT math.IT</categories><comments>46 pages, 4 figures. This work was presented in part at the IEEE
  International Conference on Communications (ICC), June 2015</comments><journal-ref>IEEE Transactions on Communications, Vol. 63, No. 4, Apr. 2015,
  pp. 1191-1201</journal-ref><doi>10.1109/TCOMM.2015.2408318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power line communications (PLC) is the central communications technology for
the realization of smart power grids. As the designated band for smart grid
communications is the narrowband (NB) power line channel, NB-PLC has been
receiving substantial attention in recent years. Narrowband power line channels
are characterized by cyclic short-term variations of the channel transfer
function (CTF) and strong noise with periodic statistics. In this work,
modeling the CTF as a linear periodically time-varying filter and the noise as
an additive cyclostationary Gaussian process, we derive the capacity of
discrete-time NB-PLC channels. As part of the capacity derivation, we
characterize the capacity achieving transmission scheme, which leads to a
practical code construction that approaches capacity. The capacity derived in
this work is numerically evaluated for several NB-PLC channel configurations
taken from previous works, and the results show that the optimal scheme
achieves a substantial rate gain over a previously proposed ad-hoc scheme. This
gain is due to optimally accounting for the periodic properties of the channel
and the noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02947</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02947</id><created>2015-05-12</created><authors><author><keyname>Ohara</keyname><forenames>Katsuyoshi</forenames></author><author><keyname>Takayama</keyname><forenames>Nobuki</forenames></author></authors><title>Pfaffian Systems of A-Hypergeometric Systems II --- Holonomic Gradient
  Method</title><categories>cs.SC math.CA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give two efficient methods to derive Pfaffian systems for A-hypergeometric
systems for the application to the holonomic gradient method for statistics. We
utilize the Hilbert driven Buchberger algorithm and Macaulay type matrices in
the two methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02950</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02950</id><created>2015-05-12</created><authors><author><keyname>Morelli</keyname><forenames>M.</forenames></author><author><keyname>Moretti</keyname><forenames>M.</forenames></author></authors><title>A Robust Scheme for PSS Detection and Integer Frequency Offset Recovery
  in LTE Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Before establishing a communication link in a cellular network, the user
terminal must activate a synchronization procedure called initial cell search
in order to acquire specific information about the serving base station. To
accomplish this task, the primary synchronization signal (PSS) and secondary
synchronization signal (SSS) are periodically transmitted in the downlink of a
Long Term Evolution (LTE) network.
  Since SSS detection can be performed only after successful identification of
the primary signal, in this work we present a robust scheme for joint PSS
detection, sector index identification and integer frequency offset (IFO)
recovery in an LTE system. The proposed algorithm relies on the maximum
likelihood (ML) estimation criterion and exploits a suitable reduced-rank
representation of the channel frequency response to take multipath distortions
into account. We show that some PSS detection methods that were originally
introduced through heuristic reasoning can be derived from our ML framework by
selecting an appropriate model for the channel gains over the PSS subcarriers.
  Numerical simulations indicate that the proposed scheme can be effectively
applied in the presence of severe multipath propagation, where existing
alternatives provide unsatisfactory performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02951</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02951</id><created>2015-05-12</created><authors><author><keyname>Sousa</keyname><forenames>Diogo G.</forenames></author><author><keyname>Dias</keyname><forenames>Ricardo J.</forenames></author><author><keyname>Ferreira</keyname><forenames>Carla</forenames></author><author><keyname>Louren&#xe7;o</keyname><forenames>Jo&#xe3;o M.</forenames></author></authors><title>Preventing Atomicity Violations with Contracts</title><categories>cs.DC cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software developers are expected to protect concurrent accesses to shared
regions of memory with some mutual exclusion primitive that ensures atomicity
properties to a sequence of program statements. This approach prevents data
races but may fail to provide all necessary correctness properties.The
composition of correlated atomic operations without further synchronization may
cause atomicity violations. Atomic violations may be avoided by grouping the
correlated atomic regions in a single larger atomic scope. Concurrent programs
are particularly prone to atomicity violations when they use services provided
by third party packages or modules, since the programmer may fail to identify
which services are correlated. In this paper we propose to use contracts for
concurrency, where the developer of a module writes a set of contract terms
that specify which methods are correlated and must be executed in the same
atomic scope. These contracts are then used to verify the correctness of the
main program with respect to the usage of the module(s). If a contract is well
defined and complete, and the main program respects it, then the program is
safe from atomicity violations with respect to that module. We also propose a
static analysis based methodology to verify contracts for concurrency that we
applied to some real-world software packages. The bug we found in Tomcat 6.0
was immediately acknowledged and corrected by its development team.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02956</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02956</id><created>2015-05-12</created><authors><author><keyname>Kim</keyname><forenames>Minho</forenames></author><author><keyname>Jung</keyname><forenames>Sang Yeob</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Sum-Rate Maximizing Cell Association via Dual-Connectivity</title><categories>cs.NI</categories><comments>10 pages, 5 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a dual-connectivity (DC) profile allocation algorithm, in
which a central macro base station (MBS) is underlaid with randomly scattered
small base stations (SBSs), operating on different carrier frequencies. We
introduce two dual-connectivity profiles and the differences among them. We
utilize the characteristics of dual-connectivity profiles and their applying
scenarios to reduce feasible combination set to consider. Algorithm analysis
and numerical results verify that our proposed algorithm achieve the optimal
algorithm's performance within 5 \% gap with quite low complexity up to $10^6$
times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02973</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02973</id><created>2015-05-12</created><authors><author><keyname>Psomakelis</keyname><forenames>Evangelos</forenames></author><author><keyname>Tserpes</keyname><forenames>Konstantinos</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Dimosthenis</forenames></author><author><keyname>Varvarigou</keyname><forenames>Theodora</forenames></author></authors><title>Comparing methods for Twitter Sentiment Analysis</title><categories>cs.CL cs.IR cs.SI</categories><comments>5 pages, 1 figure, 6th Conference on Knowledge Discovery and
  Information Retrieval 2014, Rome, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work extends the set of works which deal with the popular problem of
sentiment analysis in Twitter. It investigates the most popular document
(&quot;tweet&quot;) representation methods which feed sentiment evaluation mechanisms. In
particular, we study the bag-of-words, n-grams and n-gram graphs approaches and
for each of them we evaluate the performance of a lexicon-based and 7
learning-based classification algorithms (namely SVM, Na\&quot;ive Bayesian
Networks, Logistic Regression, Multilayer Perceptrons, Best-First Trees,
Functional Trees and C4.5) as well as their combinations, using a set of 4451
manually annotated tweets. The results demonstrate the superiority of
learning-based methods and in particular of n-gram graphs approaches for
predicting the sentiment of tweets. They also show that the combinatory
approach has impressive effects on n-grams, raising the confidence up to 83.15%
on the 5-Grams, using majority vote and a balanced dataset (equal number of
positive, negative and neutral tweets for training). In the n-gram graph cases
the improvement was small to none, reaching 94.52% on the 4-gram graphs, using
Orthodromic distance and a threshold of 0.001.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02977</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02977</id><created>2015-05-12</created><authors><author><keyname>Kardara</keyname><forenames>Magdalini</forenames></author><author><keyname>Kalogirou</keyname><forenames>Vasilis</forenames></author><author><keyname>Papaoikonomou</keyname><forenames>Athanasios</forenames></author><author><keyname>Varvarigou</keyname><forenames>Theodora</forenames></author><author><keyname>Tserpes</keyname><forenames>Konstantinos</forenames></author></authors><title>SocIoS API: A data aggregator for accessing user generated content from
  online social networks</title><categories>cs.SE cs.SI</categories><comments>15th International Conference on Web Information System Engineering
  (WISE 2014), Thessaloniki, Greece, 12-14 October 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following the boost in popularity of online social networks, both enterprises
and researchers looked for ways to access the social dynamics information and
user generated content residing in these spaces. This endeavor, however,
presented several challenges caused by the heterogeneity of data and the lack
of a common way to access them. The SocIoS framework tries to address these
challenges by providing tools that operate on top of multiple popular social
networks allowing uniform access to their data. It provides a single access
point for aggregating data and functionality from the networks, as well as a
set of analytical tools for exploiting them. In this paper we present the
SocIoS API, an abstraction layer on top of the social networks exposing
operations that encapsulate the functionality of their APIs. Currently, the
component provides support for seven social networks and is flexible enough to
allow for the seamless addition of more.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02982</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02982</id><created>2015-05-12</created><authors><author><keyname>Shi</keyname><forenames>Baoguang</forenames></author><author><keyname>Yao</keyname><forenames>Cong</forenames></author><author><keyname>Zhang</keyname><forenames>Chengquan</forenames></author><author><keyname>Guo</keyname><forenames>Xiaowei</forenames></author><author><keyname>Huang</keyname><forenames>Feiyue</forenames></author><author><keyname>Bai</keyname><forenames>Xiang</forenames></author></authors><title>Automatic Script Identification in the Wild</title><categories>cs.CV</categories><comments>5 pages, 7 figures, submitted to ICDAR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid increase of transnational communication and cooperation,
people frequently encounter multilingual scenarios in various situations. In
this paper, we are concerned with a relatively new problem: script
identification at word or line levels in natural scenes. A large-scale dataset
with a great quantity of natural images and 10 types of widely used languages
is constructed and released. In allusion to the challenges in script
identification in real-world scenarios, a deep learning based algorithm is
proposed. The experiments on the proposed dataset demonstrate that our
algorithm achieves superior performance, compared with conventional image
classification methods, such as the original CNN architecture and LLC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02983</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02983</id><created>2015-05-12</created><authors><author><keyname>Martins</keyname><forenames>Rodrigo S. V.</forenames></author><author><keyname>Panario</keyname><forenames>Daniel</forenames></author></authors><title>On the Heuristic of Approximating Polynomials over Finite Fields by
  Random Mappings</title><categories>cs.DM math.NT</categories><comments>Partial results presented in the 13th Conference of the Canadian
  Number Theory Association (CNTA XIII), Ottawa, Canada, 2014. Complete work
  presented in Workshop on Finite Fields, Combinatorics and Cryptographical
  Applications, Florianopolis, Brazil, 2015</comments><msc-class>12Y05, 11T99</msc-class><doi>10.1142/S1793042116501219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of iterations of functions over a finite field and the
corresponding functional graphs is a growing area of research with connections
to cryptography. The behaviour of such iterations is frequently approximated by
what is know as the Brent-Pollard heuristic, where one treats functions as
random mappings. We aim at understanding this heuristic and focus on the
expected rho length of a node of the functional graph of a polynomial over a
finite field. Since the distribution of indegrees (preimage sizes) of a class
of functions appears to play a central role in its average rho length, we
survey the known results for polynomials over finite fields giving new proofs
and improving one of the cases for quartic polynomials. We discuss the
effectiveness of the heuristic for many classes of polynomials by comparing our
experimental results with the known estimates for random mapping models defined
by different restrictions on their distribution of indegrees. We prove that the
distribution of indegrees of general polynomials and mappings have similar
asymptotic properties, including the same asymptotic average coalescence. The
combination of these results and our experiments suggests that these
polynomials behave like random mappings, extending a heuristic that was known
only for degree $2$. We show numerically that the behaviour of Chebyshev
polynomials of degree $d \geq 2$ over finite fields present a sharp contrast
when compared to other polynomials in their respective classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02984</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02984</id><created>2015-05-12</created><updated>2015-11-19</updated><authors><author><keyname>Daskin</keyname><forenames>Anmer</forenames></author></authors><title>Quantum Eigenvalue Estimation for Irreducible Non-negative Matrices</title><categories>quant-ph cs.DS</categories><comments>A few typos are corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum phase estimation algorithm has been successfully adapted as a sub
frame of many other algorithms applied to a wide variety of applications in
different fields. However, the requirement of a good approximate eigenvector
given as an input to the algorithm hinders the application of the algorithm to
the problems where we do not have any prior knowledge about the eigenvector.
  In this paper, we show that the principal eigenvalue of an irreducible
non-negative operator can be determined by using an equal superposition initial
state in the phase estimation algorithm. This removes the necessity of the
existence of an initial good approximate eigenvector. Moreover, we show that
the success probability of the algorithm is related to the closeness of the
operator to a stochastic matrix. Therefore, we draw an estimate for the success
probability by using the variance of the column sums of the operator. This
provides a priori information which can be used to know the success probability
of the algorithm beforehand for the non-negative matrices and apply the
algorithm only in cases when the estimated probability reasonably high.
Finally, we discuss the possible applications and show the results for random
symmetric matrices and 3-local Hamiltonians with non-negative off-diagonal
elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02985</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02985</id><created>2015-05-12</created><authors><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Cooley</keyname><forenames>Oliver</forenames></author><author><keyname>Kang</keyname><forenames>Mihyun</forenames></author><author><keyname>Skubch</keyname><forenames>Kathrin</forenames></author></authors><title>The minimum bisection in the planted bisection model</title><categories>cs.DM math.CO math.PR</categories><msc-class>05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the planted bisection model a random graph $G(n,p_+,p_- )$ with $n$
vertices is created by partitioning the vertices randomly into two classes of
equal size (up to $\pm1$). Any two vertices that belong to the same class are
linked by an edge with probability $p_+$ and any two that belong to different
classes with probability $p_- &lt;p_+$ independently. The planted bisection model
has been used extensively to benchmark graph partitioning algorithms. If
$p_{\pm} =2d_{\pm} /n$ for numbers $0\leq d_- &lt;d_+ $ that remain fixed as
$n\to\infty$, then w.h.p. the ``planted'' bisection (the one used to construct
the graph) will not be a minimum bisection. In this paper we derive an
asymptotic formula for the minimum bisection width under the assumption that
$d_+ -d_- &gt;c\sqrt{d_+ \ln d_+ }$ for a certain constant $c&gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02992</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02992</id><created>2015-05-12</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Large-Scale MIMO Relaying Techniques for Physical Layer Security: AF or
  DF?</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1401.3049</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a large scale multiple input multiple output
(LS-MIMO) relaying system, where an information source sends the message to its
intended destination aided by an LS-MIMO relay, while a passive eavesdropper
tries to intercept the information forwarded by the relay. The advantage of a
large scale antenna array is exploited to improve spectral efficiency and
enhance wireless security. In particular, the challenging issue incurred by
short-distance interception is well addressed. Under very practical
assumptions, i.e., no eavesdropper channel state information (CSI) and
imperfect legitimate CSI at the relay, this paper gives a thorough secrecy
performance analysis and comparison of two classic relaying techniques, i.e.,
amplify-and-forward (AF) and decode-and-forward (DF). Furthermore, asymptotical
analysis is carried out to provide clear insights on the secrecy performance
for such an LS-MIMO relaying system. We show that under large transmit powers,
AF is a better choice than DF from the perspectives of both secrecy performance
and implementation complexity, and prove that there exits an optimal transmit
power at medium regime that maximizes the secrecy outage capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02993</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02993</id><created>2015-05-12</created><authors><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Fu</keyname><forenames>Zhiguo</forenames></author><author><keyname>Guo</keyname><forenames>Heng</forenames></author><author><keyname>Williams</keyname><forenames>Tyson</forenames></author></authors><title>A Holant Dichotomy: Is the FKT Algorithm Universal?</title><categories>cs.CC cs.DS</categories><comments>128 pages, 36 figures</comments><msc-class>68Q17</msc-class><acm-class>F.1.3; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a complexity dichotomy for complex-weighted Holant problems with an
arbitrary set of symmetric constraint functions on Boolean variables. This
dichotomy is specifically to answer the question: Is the FKT algorithm under a
holographic transformation a \emph{universal} strategy to obtain
polynomial-time algorithms for problems over planar graphs that are intractable
in general? This dichotomy is a culmination of previous ones, including those
for Spin Systems, Holant, and #CSP. A recurring theme has been that a
holographic reduction to FKT is a universal strategy. Surprisingly, for planar
Holant, we discover new planar tractable problems that are not expressible by a
holographic reduction to FKT.
  In previous work, an important tool was a dichotomy for #CSP^d, which denotes
#CSP where every variable appears a multiple of d times. However its proof
violates planarity. We prove a dichotomy for planar #CSP^2. We apply this
planar #CSP^2 dichotomy in the proof of the planar Holant dichotomy.
  As a special case of our new planar tractable problems, counting perfect
matchings (#PM) over k-uniform hypergraphs is polynomial-time computable when
the incidence graph is planar and k &gt;= 5. The same problem is #P-hard when k=3
or k=4, which is also a consequence of our dichotomy. When k=2, it becomes #PM
over planar graphs and is tractable again. More generally, over hypergraphs
with specified hyperedge sizes and the same planarity assumption, #PM is
polynomial-time computable if the greatest common divisor of all hyperedge
sizes is at least 5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.02997</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.02997</id><created>2015-05-12</created><authors><author><keyname>Gattami</keyname><forenames>Ather</forenames></author></authors><title>Optimal Data and Training Symbol Ratio for Communication over Uncertain
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of determining the power ratio between the training
symbols and data symbols in order to maximize the channel capacity for
transmission over uncertain channels with a channel estimate available at both
the transmitter and receiver. The receiver makes an estimate of the channel by
using a known sequence of training symbols. This channel estimate is then
transmitted back to the transmitter. The capacity that the transceiver
maximizes is the worst case capacity, in the sense that given a noise
covariance, the transceiver maximizes the minimal capacity over all
distributions of the measurement noise under a fixed covariance matrix known at
both the transmitter and receiver. We give an exact expression of the channel
capacity as a function of the channel covariance matrix, and the number of
training symbols used during a coherence time interval. This expression
determines the number of training symbols that need to be used by finding the
optimal integer number of training symbols that maximize the channel capacity.
As a bi-product, we show that linear filters are optimal at both the
transmitter and receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03001</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03001</id><created>2015-05-12</created><updated>2015-12-20</updated><authors><author><keyname>Shwartz</keyname><forenames>Ofer</forenames></author><author><keyname>Nadler</keyname><forenames>Boaz</forenames></author></authors><title>Detecting the large entries of a sparse covariance matrix in
  sub-quadratic time</title><categories>stat.CO cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The covariance matrix of a $p$-dimensional random variable is a fundamental
quantity in data analysis. Given $n$ i.i.d. observations, it is typically
estimated by the sample covariance matrix, at a computational cost of
$O(np^{2})$ operations. When $n,p$ are large, this computation may be
prohibitively slow. Moreover, in several contemporary applications, the
population matrix is approximately sparse, and only its few large entries are
of interest. This raises the following question, at the focus of our work:
Assuming approximate sparsity of the covariance matrix, can its large entries
be detected much faster, say in sub-quadratic time, without explicitly
computing all its $p^{2}$ entries? In this paper, we present and theoretically
analyze two randomized algorithms that detect the large entries of an
approximately sparse sample covariance matrix using only $O(np\text{ poly log }
p)$ operations. Furthermore, assuming sparsity of the population matrix, we
derive sufficient conditions on the underlying random variable and on the
number of samples $n$, for the sample covariance matrix to satisfy our
approximate sparsity requirements. Finally, we illustrate the performance of
our algorithms via several simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03002</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03002</id><created>2015-05-12</created><authors><author><keyname>Palovics</keyname><forenames>Robert</forenames></author><author><keyname>Daroczy</keyname><forenames>Balint</forenames></author><author><keyname>Benczur</keyname><forenames>Andras</forenames></author><author><keyname>Pap</keyname><forenames>Julia</forenames></author><author><keyname>Ermann</keyname><forenames>Leonardo</forenames></author><author><keyname>Phan</keyname><forenames>Samuel</forenames></author><author><keyname>Chepelianskii</keyname><forenames>Alexei D.</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Statistical analysis of NOMAO customer votes for spots of France</title><categories>physics.soc-ph cs.IR cs.SI</categories><comments>10 pages, 12 figs</comments><journal-ref>Eur. Phys. J. B. v.88, p.194 (2015)</journal-ref><doi>10.1140/epjb/e2015-60357-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the statistical properties of votes of customers for spots of
France collected by the startup company NOMAO. The frequencies of votes per
spot and per customer are characterized by a power law distributions which
remain stable on a time scale of a decade when the number of votes is varied by
almost two orders of magnitude. Using the computer science methods we explore
the spectrum and the eigenvalues of a matrix containing user ratings to
geolocalized items. Eigenvalues nicely map to large towns and regions but show
certain level of instability as we modify the interpretation of the underlying
matrix. We evaluate imputation strategies that provide improved prediction
performance by reaching geographically smooth eigenvectors. We point on
possible links between distribution of votes and the phenomenon of
self-organized criticality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03006</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03006</id><created>2015-05-12</created><updated>2016-02-12</updated><authors><author><keyname>Cavalcante</keyname><forenames>Renato L. G.</forenames></author><author><keyname>Shen</keyname><forenames>Yuxiang</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>Slawomir</forenames></author></authors><title>Elementary Properties of Positive Concave Mappings with Applications to
  Network Planning and Optimization</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE Transactions on Signal Processing (accepted for publication)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents novel methods for computing fixed points of positive
concave mappings and for characterizing the existence of fixed points. These
methods are particularly important in planning and optimization tasks in
wireless networks. For example, previous studies have shown that the
feasibility of a network design can be quickly evaluated by computing the fixed
point of a concave mapping that is constructed based on many environmental and
network control parameters such as the position of base stations, channel
conditions, and antenna tilts. To address this and more general problems, given
a positive concave mapping, we show two alternative but equivalent ways to
construct a matrix that is guaranteed to have spectral radius strictly smaller
than one if the mapping has a fixed point. This matrix is then used to build a
new mapping that preserves the fixed point of the original positive concave
mapping. We show that the standard fixed point iterations using the new mapping
converges faster than the standard iterations applied to the original concave
mapping. As exemplary applications of the proposed methods, we consider the
problems of power and load planning in networks based on the orthogonal
frequency division multiple access (OFDMA) technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03008</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03008</id><created>2015-05-12</created><authors><author><keyname>Fiala</keyname><forenames>Dalibor</forenames></author><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>&#x17d;itnik</keyname><forenames>Slavko</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Do PageRank-based author rankings outperform simple citation counts?</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>28 pages, 5 figures, 6 tables</comments><journal-ref>J. Infometr. 9(2), 334-348 (2015)</journal-ref><doi>10.1016/j.joi.2015.02.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic indicators of a researcher's productivity and impact are still the
number of publications and their citation counts. These metrics are clear,
straightforward, and easy to obtain. When a ranking of scholars is needed, for
instance in grant, award, or promotion procedures, their use is the fastest and
cheapest way of prioritizing some scientists over others. However, due to their
nature, there is a danger of oversimplifying scientific achievements.
Therefore, many other indicators have been proposed including the usage of the
PageRank algorithm known for the ranking of webpages and its modifications
suited to citation networks. Nevertheless, this recursive method is
computationally expensive and even if it has the advantage of favouring
prestige over popularity, its application should be well justified,
particularly when compared to the standard citation counts. In this study, we
analyze three large datasets of computer science papers in the categories of
artificial intelligence, software engineering, and theory and methods and apply
12 different ranking methods to the citation networks of authors. We compare
the resulting rankings with self-compiled lists of outstanding researchers
selected as frequent editorial board members of prestigious journals in the
field and conclude that there is no evidence of PageRank-based methods
outperforming simple citation counts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03014</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03014</id><created>2015-05-12</created><authors><author><keyname>Baltrunas</keyname><forenames>Linas</forenames></author><author><keyname>Church</keyname><forenames>Karen</forenames></author><author><keyname>Karatzoglou</keyname><forenames>Alexandros</forenames></author><author><keyname>Oliver</keyname><forenames>Nuria</forenames></author></authors><title>Frappe: Understanding the Usage and Perception of Mobile App
  Recommendations In-The-Wild</title><categories>cs.IR</categories><report-no>11</report-no><acm-class>H.3.3; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a real world deployment of a context-aware mobile app
recommender system (RS) called Frappe. Utilizing a hybrid-approach, we
conducted a large-scale app market deployment with 1000 Android users combined
with a small-scale local user study involving 33 users. The resulting usage
logs and subjective feedback enabled us to gather key insights into (1)
context-dependent app usage and (2) the perceptions and experiences of
end-users while interacting with context-aware mobile app recommendations.
While Frappe performs very well based on usage-centric evaluation metrics
insights from the small-scale study reveal some negative user experiences. Our
results point to a number of actionable lessons learned specifically related to
designing, deploying and evaluating mobile context-aware RS in-the-wild with
real users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03015</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03015</id><created>2015-05-12</created><authors><author><keyname>Paolucci</keyname><forenames>Pier Stanislao</forenames></author><author><keyname>Ammendola</keyname><forenames>Roberto</forenames></author><author><keyname>Biagioni</keyname><forenames>Andrea</forenames></author><author><keyname>Frezza</keyname><forenames>Ottorino</forenames></author><author><keyname>Cicero</keyname><forenames>Francesca Lo</forenames></author><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Martinelli</keyname><forenames>Michele</forenames></author><author><keyname>Pastorelli</keyname><forenames>Elena</forenames></author><author><keyname>Simula</keyname><forenames>Francesco</forenames></author><author><keyname>Vicini</keyname><forenames>Piero</forenames></author></authors><title>Power, Energy and Speed of Embedded and Server Multi-Cores applied to
  Distributed Simulation of Spiking Neural Networks: ARM in NVIDIA Tegra vs
  Intel Xeon quad-cores</title><categories>cs.DC q-bio.NC</categories><comments>4 pages, 1 table</comments><acm-class>C.2.4; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short note regards a comparison of instantaneous power, total energy
consumption, execution time and energetic cost per synaptic event of a spiking
neural network simulator (DPSNN-STDP) distributed on MPI processes when
executed either on an embedded platform (based on a dual socket quad-core ARM
platform) or a server platform (INTEL-based quad-core dual socket platform). We
also compare the measure with those reported by leading custom and semi-custom
designs: TrueNorth and SpiNNaker. In summary, we observed that: 1- we spent 2.2
micro-Joule per simulated event on the &quot;embedded platform&quot;, approx. 4.4 times
lower than what was spent by the &quot;server platform&quot;; 2- the instantaneous power
consumption of the &quot;embedded platform&quot; was 14.4 times better than the &quot;server&quot;
one; 3- the server platform is a factor 3.3 faster. The &quot;embedded platform&quot; is
made of NVIDIA Jetson TK1 boards, interconnected by Ethernet, each mounting a
Tegra K1 chip including a quad-core ARM Cortex-A15 at 2.3GHz. The &quot;server
platform&quot; is based on dual-socket quad-core Intel Xeon CPUs (E5620 at 2.4GHz).
The measures were obtained with the DPSNN-STDP simulator (Distributed Simulator
of Polychronous Spiking Neural Network with synaptic Spike Timing Dependent
Plasticity) developed by INFN, that already proved its efficient scalability
and execution speed-up on hundreds of similar &quot;server&quot; cores and MPI processes,
applied to neural nets composed of several billions of synapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03020</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03020</id><created>2015-05-12</created><authors><author><keyname>Munuera</keyname><forenames>Carlos</forenames></author><author><keyname>Olaya-Le&#xf3;n</keyname><forenames>Wilson</forenames></author></authors><title>An Introduction to Algebraic Geometry codes</title><categories>cs.IT math.AG math.IT</categories><msc-class>94B27, 14G50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an introduction to the theory of algebraic geometry codes.
Starting from evaluation codes and codes from order and weight functions,
special attention is given to one-point codes and, in particular, to the family
of Castle codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03036</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03036</id><created>2015-05-12</created><authors><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Hogg</keyname><forenames>David W.</forenames></author><author><keyname>Wang</keyname><forenames>Dun</forenames></author><author><keyname>Foreman-Mackey</keyname><forenames>Daniel</forenames></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author><author><keyname>Simon-Gabriel</keyname><forenames>Carl-Johann</forenames></author><author><keyname>Peters</keyname><forenames>Jonas</forenames></author></authors><title>Removing systematic errors for exoplanet search via latent causes</title><categories>stat.ML astro-ph.EP astro-ph.IM cs.LG</categories><comments>Extended version of a paper appearing in the Proceedings of the 32nd
  International Conference on Machine Learning, Lille, France, 2015</comments><acm-class>G.3; I.2.6; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for removing the effect of confounders in order to
reconstruct a latent quantity of interest. The method, referred to as
half-sibling regression, is inspired by recent work in causal inference using
additive noise models. We provide a theoretical justification and illustrate
the potential of the method in a challenging astronomy application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03044</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03044</id><created>2015-05-12</created><authors><author><keyname>Hamon</keyname><forenames>Ronan</forenames></author><author><keyname>Borgnat</keyname><forenames>Pierre</forenames></author><author><keyname>Flandrin</keyname><forenames>Patrick</forenames></author><author><keyname>Robardet</keyname><forenames>C&#xe9;line</forenames></author></authors><title>Duality between Temporal Networks and Signals: Extraction of the
  Temporal Network Structures</title><categories>cs.SI cs.DM physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework to track the structure of temporal networks with a
signal processing approach. The method is based on the duality between networks
and signals using a multidimensional scaling technique. This enables a study of
the network structure using frequency patterns of the corresponding signals. An
extension is proposed for temporal networks, thereby enabling a tracking of the
network structure over time. A method to automatically extract the most
significant frequency patterns and their activation coefficients over time is
then introduced, using nonnegative matrix factorization of the temporal
spectra. The framework, inspired by audio decomposition, allows transforming
back these frequency patterns into networks, to highlight the evolution of the
underlying structure of the network over time. The effectiveness of the method
is first evidenced on a toy example, prior being used to study a temporal
network of face-to-face contacts. The extraction of sub-networks highlights
significant structures decomposed on time intervals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03046</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03046</id><created>2015-05-12</created><updated>2015-09-15</updated><authors><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Liu</keyname><forenames>Jiamin</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Seff</keyname><forenames>Ari</forenames></author><author><keyname>Cherry</keyname><forenames>Kevin</forenames></author><author><keyname>Kim</keyname><forenames>Lauren</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Improving Computer-aided Detection using Convolutional Neural Networks
  and Random View Aggregation</title><categories>cs.CV</categories><comments>2D vs 2.5D vs 3D inputs and comparison to other standard classifiers
  such as SVM have been addressed by more experimentation and two completely
  new sections and figures. Results and Discussions have been updated
  accordingly</comments><doi>10.1109/TMI.2015.2482920</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Automated computer-aided detection (CADe) in medical imaging has been an
important tool in clinical practice and research. State-of-the-art methods
often show high sensitivities but at the cost of high false-positives (FP) per
patient rates. We design a two-tiered coarse-to-fine cascade framework that
first operates a candidate generation system at sensitivities of $\sim$100% but
at high FP levels. By leveraging existing CAD systems, coordinates of regions
or volumes of interest (ROI or VOI) for lesion candidates are generated in this
step and function as input for a second tier, which is our focus in this study.
In this second stage, we generate $N$ 2D (two-dimensional) or 2.5D views via
sampling through scale transformations, random translations and rotations with
respect to each ROI's centroid coordinates. These random views are used to
train deep convolutional neural network (ConvNet) classifiers. In testing, the
trained ConvNets are employed to assign class (e.g., lesion, pathology)
probabilities for a new set of $N$ random views that are then averaged at each
ROI to compute a final per-candidate classification probability. This second
tier behaves as a highly selective process to reject difficult false positives
while preserving high sensitivities. The methods are evaluated on three
different data sets with different numbers of patients: 59 patients for
sclerotic metastases detection, 176 patients for lymph node detection, and
1,186 patients for colonic polyp detection. Experimental results show the
ability of ConvNets to generalize well to different medical imaging CADe
applications and scale elegantly to various data sets. Our proposed methods
improve CADe performance markedly in all cases. CADe sensitivities improved
from 57% to 70%, from 43% to 77% and from 58% to 75% at 3 FPs per patient for
sclerotic metastases, lymph nodes and colonic polyps, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03048</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03048</id><created>2015-05-12</created><authors><author><keyname>Arfaoui</keyname><forenames>Ghada</forenames><affiliation>LIFO</affiliation></author><author><keyname>Lalande</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>INRIA - SUPELEC, LIFO</affiliation></author><author><keyname>Traor&#xe9;</keyname><forenames>Jacques</forenames><affiliation>LIFO</affiliation></author><author><keyname>Desmoulins</keyname><forenames>Nicolas</forenames><affiliation>LIFO</affiliation></author><author><keyname>Berthom&#xe9;</keyname><forenames>Pascal</forenames><affiliation>LIFO</affiliation></author><author><keyname>Gharout</keyname><forenames>Sa&#xef;d</forenames></author></authors><title>A Practical Set-Membership Proof for Privacy-Preserving NFC Mobile
  Ticketing</title><categories>cs.CR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To ensure the privacy of users in transport systems, researchers are working
on new protocols providing the best security guarantees while respecting
functional requirements of transport operators. In this paper, we design a
secure NFC m-ticketing protocol for public transport that preserves users'
anonymity and prevents transport operators from tracing their customers' trips.
To this end, we introduce a new practical set-membership proof that does not
require provers nor verifiers (but in a specific scenario for verifiers) to
perform pairing computations. It is therefore particularly suitable for our
(ticketing) setting where provers hold SIM/UICC cards that do not support such
costly computations. We also propose several optimizations of Boneh-Boyen type
signature schemes, which are of independent interest, increasing their
performance and efficiency during NFC transactions. Our m-ticketing protocol
offers greater flexibility compared to previous solutions as it enables the
post-payment and the off-line validation of m-tickets. By implementing a
prototype using a standard NFC SIM card, we show that it fulfils the stringent
functional requirement imposed by transport operators whilst using strong
security parameters. In particular, a validation can be completed in 184.25 ms
when the mobile is switched on, and in 266.52 ms when the mobile is switched
off or its battery is flat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03049</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03049</id><created>2015-05-12</created><authors><author><keyname>Jankowski</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Michalski</keyname><forenames>Rados&#x142;aw</forenames></author><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Utz</keyname><forenames>Sonja</forenames></author></authors><title>Knowledge Acquisition from Social Platforms Based on Network
  Distributions Fitting</title><categories>cs.SI</categories><journal-ref>Computers in Human Behavior, 2014, 12</journal-ref><doi>10.1016/j.chb.2014.12.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The uniqueness of online social networks makes it possible to implement new
methods that increase the quality and effectiveness of research processes.
While surveys are one of the most important tools for research, the
representativeness of selected online samples is often a challenge and the
results are hardly generalizable. An approach based on surveys with
representativeness targeted at network measure distributions is proposed and
analysed in this paper. Its main goal is to focus not only on sample
representativeness in terms of demographic attributes, but also to follow the
measures distributions within main network. The approach presented has many
application areas related to online research, sampling a network for the
evaluation of collaborative learning processes, and candidate selection for
training purposes with the ability to distribute information within a social
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03052</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03052</id><created>2015-05-12</created><authors><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>Pralat</keyname><forenames>Pawel</forenames></author><author><keyname>Roshanbin</keyname><forenames>Elham</forenames></author></authors><title>Burning graphs - a probabilistic perspective</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a graph parameter that was recently introduced, the
burning number, focusing on a few probabilistic aspects of the problem. The
original burning number is revisited and analyzed for binomial random graphs
G(n,p), random geometric graphs, and the Cartesian product of paths. Moreover,
new variants of the burning number are introduced in which a burning sequence
of vertices is selected according to some probabilistic rules. We analyze these
new graph parameters for paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03055</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03055</id><created>2015-05-12</created><authors><author><keyname>Rozewski</keyname><forenames>Przemyslaw</forenames></author><author><keyname>Jankowski</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Brodka</keyname><forenames>Piotr</forenames></author><author><keyname>Michalski</keyname><forenames>Radoslaw</forenames></author></authors><title>Knowledge workers collaborative learning behavior modeling in an
  organizational social network</title><categories>cs.CY cs.SI</categories><journal-ref>Computers in Human Behavior, 2015, 01</journal-ref><doi>10.1016/j.chb.2014.12.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computations related to learning processes within an organizational social
network area require some network model preparation and specific algorithms in
order to implement human behaviors in simulated environments. The proposals in
this research model of collaborative learning in an organizational social
network are based on knowledge resource distribution through the establishment
of a knowledge flow. The nodes, which represent knowledge workers, contain
information about workers social and cognitive abilities. Moreover, the workers
are described by their set of competences, their skill level, and the
collaborative learning behavior that can be detected through knowledge flow
analysis. The proposed approach assumes that an increase in workers competence
is a result of collaborative learning. In other words, collaborative learning
can be analyzed as a process of knowledge flow that is being broadcast in a
network. In order to create a more effective organizational social network for
co-learning, the authors found the best strategies for knowledge facilitator,
knowledge collector, and expert roles allocation. Special attention is paid to
the process of knowledge flow in the community of practice. Acceleration within
the community of practice happens when knowledge flows more effectively between
community members. The presented procedure makes it possible to add new ties to
the community of practice in order to influence community members competences.
Both the proposed allocation and acceleration approaches were confirmed through
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03057</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03057</id><created>2015-05-12</created><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>M&#xf6;nich</keyname><forenames>Ullrich J.</forenames></author></authors><title>Strong Divergence for System Approximations</title><categories>cs.IT math.CV math.FA math.IT</categories><comments>Preprint accepted for publication in Problems of Information
  Transmission. The material in this paper was presented in part at the 2015
  IEEE International Conference on Acoustics, Speech, and Signal Processing</comments><msc-class>94A20 (Primary), 30D10, 94A15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the approximation of stable linear time-invariant
systems, like the Hilbert transform, by sampling series for bandlimited
functions in the Paley-Wiener space $\mathcal{PW}_{\pi}^{1}$. It is known that
there exist systems and functions such that the approximation process is weakly
divergent, i.e., divergent for certain subsequences. Here we strengthen this
result by proving strong divergence, i.e., divergence for all subsequences.
Further, in case of divergence, we give the divergence speed. We consider
sampling at Nyquist rate as well as oversampling with adaptive choice of the
kernel. Finally, connections between strong divergence and the Banach-Steinhaus
theorem, which is not powerful enough to prove strong divergence, are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03060</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03060</id><created>2015-05-12</created><authors><author><keyname>Crafa</keyname><forenames>Silvia</forenames></author><author><keyname>Tronchin</keyname><forenames>Luca</forenames></author></authors><title>Actors vs Shared Memory: two models at work on Big Data application
  frameworks</title><categories>cs.DC</categories><acm-class>C.2.4; D.1.3; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims at analyzing how two different concurrency models, namely the
shared memory model and the actor model, can influence the development of
applications that manage huge masses of data, distinctive of Big Data
applications. The paper compares the two models by analyzing a couple of
concrete projects based on the MapReduce and Bulk Synchronous Parallel
algorithmic schemes. Both projects are doubly implemented on two concrete
platforms: Akka Cluster and Managed X10. The result is both a conceptual
comparison of models in the Big Data Analytics scenario, and an experimental
analysis based on concrete executions on a cluster platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03062</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03062</id><created>2015-05-12</created><authors><author><keyname>Kirthi</keyname><forenames>Krishnamurthy</forenames></author></authors><title>Binary GH Sequences for Multiparty Communication</title><categories>cs.CR</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates cross correlation properties of sequences derived
from GH sequences modulo p, where p is a prime number and presents comparison
with cross correlation properties of pseudo noise sequences. For GH sequences
modulo prime, a binary random sequence B(n) is constructed, based on whether
the period is p-1 (or a divisor) or 2p+2 (or a divisor). We show that B(n)
sequences have much less peak cross correlation compared to PN sequence
fragments obtained from the same generator. Potential applications of these
sequences to cryptography are sketched.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03065</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03065</id><created>2015-05-10</created><updated>2016-02-15</updated><authors><author><keyname>Aghasi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Iraei</keyname><forenames>Rouhollah Mousavi</forenames></author><author><keyname>Naeemi</keyname><forenames>Azad</forenames></author><author><keyname>Afshari</keyname><forenames>Ehsan</forenames></author></authors><title>Smart Detector Cell: A Scalable All-Spin Circuit for Low Power
  Non-Boolean Pattern Recognition</title><categories>cs.ET cond-mat.mes-hall</categories><comments>This article is accepted to appear in IEEE Transactions on
  Nanotechnology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new circuit for non-Boolean recognition of binary images.
Employing all-spin logic (ASL) devices, we design logic comparators and
non-Boolean decision blocks for compact and efficient computation. By
manipulation of fan-in number in different stages of the circuit, the structure
can be extended for larger training sets or larger images. Operating based on
the mainly similarity idea, the system is capable of constructing a mean image
and compare it with a separate input image within a short decision time. Taking
advantage of the non-volatility of ASL devices, the proposed circuit is capable
of hybrid memory/logic operation. Compared with existing CMOS pattern
recognition circuits, this work achieves a smaller footprint, lower power
consumption, faster decision time and a lower operational voltage. To the best
of our knowledge, this is the first fully spin-based complete pattern
recognition circuit demonstrated using spintronic devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03072</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03072</id><created>2015-05-12</created><authors><author><keyname>Falgas-Ravry</keyname><forenames>Victor</forenames></author><author><keyname>Markstr&#xf6;m</keyname><forenames>Klas</forenames></author><author><keyname>Verstra&#xeb;te</keyname><forenames>Jacques</forenames></author></authors><title>Full subgraphs</title><categories>math.CO cs.DM</categories><comments>16 pages; a first version of this paper appeared in the
  Mittag--Leffler Institute's preprint series</comments><msc-class>05C35, 05C70, 05D10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be an $n$-vertex graph with edge-density $p$. Following Erd\H{o}s, \L
uczak and Spencer, an $m$-vertex subgraph H of G is called full if it has
minimum degree at least $p(m - 1)$. Let $f(G)$ denote the order of a largest
full subgraph of $G$, and let $f_p(n)$ denote the minimum of $f(G)$ over all
$n$-vertex graphs $G$ with edge-density $p$.
  In this paper we show that for $p$: $n^{-2/3} &lt;p&lt; 1-n^{-1/5}$, the function
$f_p(n)$ is of order at least $(1-p)^{1/3} n^{2/3}$, improving on an earlier
lower bound of Erd\H{o}s, \L uczak and Spencer in the case $p=1/2$. Moreover we
show that this bound is tight: for infinitely many $p$ near the elements of
$\{\frac{1}{2}, \frac{2}{3}, \frac{3}{4}, \ldots \}$ we have $f_p(n)= \Theta
(n^{2/3})$
  As an ingredient of the proof, we show that every graph $G$ on $n$ vertices
has a subgraph $H$ on $m$ vertices with $\frac{n}{r}-1&lt; m &lt;\frac{n}{r}+2$ such
that for every every vertex $v$ in $V(H)$ the degree of $v$ in $H$ is at least
$\frac{1}{r}$ times its degree in $G$. Finally, we discuss full subgraphs of
random and pseudorandom graphs, and introduce several open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03078</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03078</id><created>2015-05-12</created><authors><author><keyname>Ghafari</keyname><forenames>Zeinab</forenames></author><author><keyname>Arian</keyname><forenames>Taha</forenames></author><author><keyname>Analoui</keyname><forenames>Morteza</forenames></author></authors><title>SFAMSS: a secure framework for atm machines via secret sharing</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  As ATM applications deploy for a banking system, the need to secure
communications will become critical. However, multicast protocols do not fit
the point-to-point model of most network security protocols which were designed
with unicast communications in mind. In recent years, we have seen the
emergence and the growing of ATMs (Automatic Teller Machines) in banking
systems. Many banks are extending their activity and increasing transactions by
using ATMs. ATM will allow them to reach more customers in a cost effective way
and to make their transactions fast and efficient. However, communicating in
the network must satisfy integrity, privacy, confidentiality, authentication
and non-repudiation. Many frameworks have been implemented to provide security
in communication and transactions. In this paper, we analyze ATM communication
protocol and propose a novel framework for ATM systems that allows entities
communicate in a secure way without using a lot of storage. We describe the
architecture and operation of SFAMSS in detail. Our framework is implemented
with Java and the software architecture, and its components are studied in
detailed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03081</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03081</id><created>2015-05-12</created><authors><author><keyname>Elmadany</keyname><forenames>AbdelRahim A.</forenames></author><author><keyname>Abdou</keyname><forenames>Sherif M.</forenames></author><author><keyname>Gheith</keyname><forenames>Mervat</forenames></author></authors><title>Turn Segmentation into Utterances for Arabic Spontaneous Dialogues and
  Instance Messages</title><categories>cs.CL</categories><journal-ref>International Journal on Natural Language Computing (IJNLC) Vol.
  4, No.2,April 2015</journal-ref><doi>10.5121/ijnlc.2015.4208</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text segmentation task is an essential processing task for many of Natural
Language Processing (NLP) such as text summarization, text translation,
dialogue language understanding, among others. Turns segmentation considered
the key player in dialogue understanding task for building automatic
Human-Computer systems. In this paper, we introduce a novel approach to turn
segmentation into utterances for Egyptian spontaneous dialogues and Instance
Messages (IM) using Machine Learning (ML) approach as a part of automatic
understanding Egyptian spontaneous dialogues and IM task. Due to the lack of
Egyptian dialect dialogue corpus the system evaluated by our corpus includes
3001 turns, which are collected, segmented, and annotated manually from
Egyptian call-centers. The system achieves F1 scores of 90.74% and accuracy of
95.98%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03084</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03084</id><created>2015-05-12</created><authors><author><keyname>Elmadany</keyname><forenames>AbdelRahim A.</forenames></author><author><keyname>Abdou</keyname><forenames>Sherif M.</forenames></author><author><keyname>Gheith</keyname><forenames>Mervat</forenames></author></authors><title>A Survey of Arabic Dialogues Understanding for Spontaneous Dialogues and
  Instant Message</title><categories>cs.CL</categories><journal-ref>International Journal on Natural Language Computing (IJNLC) Vol.
  4, No.2,April 2015</journal-ref><doi>10.5121/ijnlc.2015.4206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building dialogues systems interaction has recently gained considerable
attention, but most of the resources and systems built so far are tailored to
English and other Indo-European languages. The need for designing systems for
other languages is increasing such as Arabic language. For this reasons, there
are more interest for Arabic dialogue acts classification task because it a key
player in Arabic language understanding to building this systems. This paper
surveys different techniques for dialogue acts classification for Arabic. We
describe the main existing techniques for utterances segmentations and
classification, annotation schemas, and test corpora for Arabic dialogues
understanding that have introduced in the literature
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03085</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03085</id><created>2015-05-12</created><authors><author><keyname>Lunando</keyname><forenames>Edwin</forenames></author><author><keyname>Purwarianti</keyname><forenames>Ayu</forenames></author></authors><title>Indonesian Social Media Sentiment Analysis With Sarcasm Detection</title><categories>cs.CL</categories><comments>4 pages; 3 figures</comments><doi>10.1109/ICACSIS.2013.6761575</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sarcasm is considered one of the most difficult problem in sentiment
analysis. In our ob-servation on Indonesian social media, for cer-tain topics,
people tend to criticize something using sarcasm. Here, we proposed two
additional features to detect sarcasm after a common sentiment analysis is
conducted. The features are the negativity information and the number of
interjection words. We also employed translated SentiWordNet in the sentiment
classification. All the classifications were conducted with machine learning
algorithms. The experimental results showed that the additional features are
quite effective in the sarcasm detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03090</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03090</id><created>2015-05-12</created><authors><author><keyname>Zhong</keyname><forenames>Yu</forenames></author></authors><title>Efficient Similarity Indexing and Searching in High Dimensions</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient indexing and searching of high dimensional data has been an area of
active research due to the growing exploitation of high dimensional data and
the vulnerability of traditional search methods to the curse of dimensionality.
This paper presents a new approach for fast and effective searching and
indexing of high dimensional features using random partitions of the feature
space. Experiments on both handwritten digits and 3-D shape descriptors have
shown the proposed algorithm to be highly effective and efficient in indexing
and searching real data sets of several hundred dimensions. We also compare its
performance to that of the state-of-the-art locality sensitive hashing
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03093</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03093</id><created>2015-05-12</created><authors><author><keyname>Pinheiro</keyname><forenames>Manuel</forenames></author><author><keyname>Alves</keyname><forenames>J. L.</forenames></author></authors><title>A new Level-set based Protocol for Accurate Bone Segmentation from CT
  Imaging</title><categories>physics.med-ph cs.CV</categories><comments>11 pages, 24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work it is proposed a medical image segmentation pipeline for
accurate bone segmentation from CT imaging. It is a two-step methodology, with
a pre-segmentation step and a segmentation refinement step. First, the user
performs a rough segmenting of the desired region of interest. Next, a fully
automatic refinement step is applied to the pre-segmented data. The automatic
segmentation refinement is composed by several sub-stpng, namely image
deconvolution, image cropping and interpolation. The user-defined
pre-segmentation is then refined over the deconvolved, cropped, and up-sampled
version of the image. The algorithm is applied in the segmentation of CT images
of a composite femur bone, reconstructed with different reconstruction
protocols. Segmentation outcomes are validated against a gold standard model
obtained with coordinate measuring machine Nikon Metris LK V20 with a digital
line scanner LC60-D that guarantees an accuracy of 28 $\mu m$. High sub-pixel
accuracy models were obtained for all tested Datasets. The algorithm is able to
produce high quality segmentation of the composite femur regardless of the
surface meshing strategy used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03097</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03097</id><created>2015-05-12</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Mohjazi</keyname><forenames>Lina</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Al-Qutayri</keyname><forenames>Mahmoud</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Energy Detection of Unknown Signals over Cascaded Fading Channels</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy detection is a favorable mechanism in several applications relating to
the identification of deterministic unknown signals such as in radar systems
and cognitive radio communications. The present work quantifies the detrimental
effects of cascaded multipath fading on energy detection and investigates the
corresponding performance capability. A novel analytic solution is firstly
derived for a generic integral that involves a product of the Meijer
$G-$function, the Marcum $Q-$function and arbitrary power terms. This solution
is subsequently employed in the derivation of an exact closed-form expression
for the average probability of detection of unknown signals over $N$*Rayleigh
channels. The offered results are also extended to the case of square-law
selection, which is a relatively simple and effective diversity method. It is
shown that the detection performance is considerably degraded by the number of
cascaded channels and that these effects can be effectively mitigated by a
non-substantial increase of diversity branches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03101</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03101</id><created>2015-05-12</created><updated>2015-09-15</updated><authors><author><keyname>Mero&#xf1;o-Pe&#xf1;uela</keyname><forenames>Albert</forenames></author><author><keyname>Gu&#xe9;ret</keyname><forenames>Christophe</forenames></author><author><keyname>Schlobach</keyname><forenames>Stefan</forenames></author></authors><title>Release Early, Release Often: Predicting Change in Versioned Knowledge
  Organization Systems on the Web</title><categories>cs.AI</categories><comments>16 pages, 6 figures, ISWC 2015 conference pre-print The paper has
  been withdrawn due to significant overlap with a subsequent paper submitted
  to a conference for review</comments><acm-class>I.2.4; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Semantic Web is built on top of Knowledge Organization Systems (KOS)
(vocabularies, ontologies, concept schemes) that provide a structured,
interoperable and distributed access to Linked Data on the Web. The maintenance
of these KOS over time has produced a number of KOS version chains: subsequent
unique version identifiers to unique states of a KOS. However, the release of
new KOS versions pose challenges to both KOS publishers and users. For
publishers, updating a KOS is a knowledge intensive task that requires a lot of
manual effort, often implying deep deliberation on the set of changes to
introduce. For users that link their datasets to these KOS, a new version
compromises the validity of their links, often creating ramifications. In this
paper we describe a method to automatically detect which parts of a Web KOS are
likely to change in a next version, using supervised learning on past versions
in the KOS version chain. We use a set of ontology change features to model and
predict change in arbitrary Web KOS. We apply our method on 139 varied datasets
systematically retrieved from the Semantic Web, obtaining robust results at
correctly predicting change. To illustrate the accuracy, genericity and domain
independence of the method, we study the relationship between its effectiveness
and several characterizations of the evaluated datasets, finding that
predictors like the number of versions in a chain and their release frequency
have a fundamental impact in predictability of change in Web KOS. Consequently,
we argue for adopting a release early, release often philosophy in Web KOS
development cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03105</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03105</id><created>2015-05-12</created><authors><author><keyname>Ibrahim</keyname><forenames>Hossam S.</forenames></author><author><keyname>Abdou</keyname><forenames>Sherif M.</forenames></author><author><keyname>Gheith</keyname><forenames>Mervat</forenames></author></authors><title>Sentiment Analysis For Modern Standard Arabic And Colloquial</title><categories>cs.CL</categories><comments>International Journal on Natural Language Computing (IJNLC) Vol. 4,
  No.2,April 2015</comments><doi>10.5121/ijnlc.2015.4207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of social media such as blogs and social networks has fueled
interest in sentiment analysis. With the proliferation of reviews, ratings,
recommendations and other forms of online expression, online opinion has turned
into a kind of virtual currency for businesses looking to market their
products, identify new opportunities and manage their reputations, therefore
many are now looking to the field of sentiment analysis. In this paper, we
present a feature-based sentence level approach for Arabic sentiment analysis.
Our approach is using Arabic idioms/saying phrases lexicon as a key importance
for improving the detection of the sentiment polarity in Arabic sentences as
well as a number of novels and rich set of linguistically motivated features
contextual Intensifiers, contextual Shifter and negation handling), syntactic
features for conflicting phrases which enhance the sentiment classification
accuracy. Furthermore, we introduce an automatic expandable wide coverage
polarity lexicon of Arabic sentiment words. The lexicon is built with
gold-standard sentiment words as a seed which is manually collected and
annotated and it expands and detects the sentiment orientation automatically of
new sentiment words using synset aggregation technique and free online Arabic
lexicons and thesauruses. Our data focus on modern standard Arabic (MSA) and
Egyptian dialectal Arabic tweets and microblogs (hotel reservation, product
reviews, etc.). The experimental results using our resources and techniques
with SVM classifier indicate high performance levels, with accuracies of over
95%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03110</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03110</id><created>2015-05-12</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Garg</keyname><forenames>Ankit</forenames></author><author><keyname>Ko</keyname><forenames>Young Kun</forenames></author><author><keyname>Mao</keyname><forenames>Jieming</forenames></author><author><keyname>Touchette</keyname><forenames>Dave</forenames></author></authors><title>Near-optimal bounds on bounded-round quantum communication complexity of
  disjointness</title><categories>cs.CC cs.IT math.IT quant-ph</categories><comments>41 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a near optimal round-communication tradeoff for the two-party
quantum communication complexity of disjointness. For protocols with $r$
rounds, we prove a lower bound of $\tilde{\Omega}(n/r + r)$ on the
communication required for computing disjointness of input size $n$, which is
optimal up to logarithmic factors. The previous best lower bound was
$\Omega(n/r^2 + r)$ due to Jain, Radhakrishnan and Sen [JRS03]. Along the way,
we develop several tools for quantum information complexity, one of which is a
lower bound for quantum information complexity in terms of the generalized
discrepancy method. As a corollary, we get that the quantum communication
complexity of any boolean function $f$ is at most $2^{O(QIC(f))}$, where
$QIC(f)$ is the prior-free quantum information complexity of $f$ (with error
$1/3$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03111</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03111</id><created>2015-05-12</created><updated>2015-08-24</updated><authors><author><keyname>R&#xf3;th</keyname><forenames>&#xc1;goston</forenames></author></authors><title>Control point based exact description of curves and surfaces in extended
  Chebyshev spaces</title><categories>math.NA cs.GR</categories><comments>24 pages, 7 figures, 2 appendices, 2 listings (some new materials
  have been added)</comments><msc-class>65D17, 68U07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extended Chebyshev spaces that also comprise the constants represent large
families of functions that can be used in real-life modeling or engineering
applications that also involve important (e.g. transcendental) integral or
rational curves and surfaces. Concerning computer aided geometric design, the
unique normalized B-bases of such vector spaces ensure optimal shape preserving
properties, important evaluation or subdivision algorithms and useful shape
parameters. Therefore, we propose global explicit formulas for the entries of
those transformation matrices that map these normalized B-bases to the
traditional (or ordinary) bases of the underlying vector spaces. Then, we also
describe general and ready to use control point configurations for the exact
representation of those traditional integral parametric curves and (hybrid)
surfaces that are specified by coordinate functions given as (products of
separable) linear combinations of ordinary basis functions. The obtained
results are also extended to the control point and weight based exact
description of the rational counterpart of these integral parametric curves and
surfaces. The universal applicability of our methods is presented through
polynomial, trigonometric, hyperbolic or mixed extended Chebyshev vector
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03116</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03116</id><created>2015-05-12</created><authors><author><keyname>Krupke</keyname><forenames>Dominik</forenames></author><author><keyname>Ernestus</keyname><forenames>Maximilian</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author></authors><title>Distributed Cohesive Control for Robot Swarms: Maintaining Good
  Connectivity in the Presence of Exterior Forces</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a number of powerful local mechanisms for maintaining a dynamic
swarm of robots with limited capabilities and information, in the presence of
external forces and permanent node failures. We propose a set of local
continuous algorithms that together produce a generalization of a Euclidean
Steiner tree. At any stage, the resulting overall shape achieves a good
compromise between local thickness, global connectivity, and flexibility to
further continuous motion of the terminals. The resulting swarm behavior scales
well, is robust against node failures, and performs close to the best known
approximation bound for a corresponding centralized static optimization
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03128</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03128</id><created>2015-05-12</created><authors><author><keyname>Krupke</keyname><forenames>Dominik</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>McLurkin</keyname><forenames>James</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author></authors><title>A Parallel Distributed Strategy for Arraying a Scattered Robot Swarm</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of organizing a scattered group of $n$ robots in
two-dimensional space, with geometric maximum distance $D$ between robots. The
communication graph of the swarm is connected, but there is no central
authority for organizing it. We want to arrange them into a sorted and
equally-spaced array between the robots with lowest and highest label, while
maintaining a connected communication network.
  In this paper, we describe a distributed method to accomplish these goals,
without using central control, while also keeping time, travel distance and
communication cost at a minimum. We proceed in a number of stages (leader
election, initial path construction, subtree contraction, geometric
straightening, and distributed sorting), none of which requires a central
authority, but still accomplishes best possible parallelization. The overall
arraying is performed in $O(n)$ time, $O(n^2)$ individual messages, and $O(nD)$
travel distance. Implementation of the sorting and navigation use communication
messages of fixed size, and are a practical solution for large populations of
low-cost robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03132</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03132</id><created>2015-05-12</created><updated>2015-08-13</updated><authors><author><keyname>Gribanov</keyname><forenames>Dmitry</forenames></author><author><keyname>Veselov</keyname><forenames>Sergey</forenames></author></authors><title>On integer programing with bounded determinants</title><categories>cs.CG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A$ be an $(m \times n)$ integral matrix, and let $P=\{ x : A x \leq b\}$
be an $n$-dimensional polytope. The width of $P$ is defined as $ w(P)=min\{
x\in \mathbb{Z}^n\setminus\{0\} :\: max_{x \in P} x^\top u - min_{x \in P}
x^\top v \}$. Let $\Delta(A)$ and $\delta(A)$ denote the greatest and the
smallest absolute values of a determinant among all $r(A) \times r(A)$
sub-matrices of $A$, where $r(A)$ is the rank of a matrix $A$.
  We prove that if every $r(A) \times r(A)$ sub-matrix of $A$ has a determinant
equal to $\pm \Delta(A)$ or $0$ and $w(P)\ge (\Delta(A)-1)(n+1)$, then $P$
contains $n$ affine independent integer points. Also we have similar results
for the case of \emph{$k$-modular} matrices. The matrix $A$ is called
\emph{totally $k$-modular} if every square sub-matrix of $A$ has a determinant
in the set $\{0,\, \pm k^r :\: r \in \mathbb{N} \}$.
  When $P$ is a simplex and $w(P)\ge \delta(A)-1$, we describe a polynomial
time algorithm for finding an integer point in $P$.
  Finally we show that if $A$ is \emph{almost unimodular}, then integer program
$\max \{c^\top x :\: x \in P \cap \mathbb{Z}^n \}$ can be solved in polynomial
time. The matrix $A$ is called \emph{almost unimodular} if $\Delta(A) \leq 2$
and any $(r(A)-1)\times(r(A)-1)$ sub-matrix has a determinant from the set
$\{0,\pm 1\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03159</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03159</id><created>2015-05-12</created><updated>2015-12-17</updated><authors><author><keyname>Zhang</keyname><forenames>Ziyu</forenames></author><author><keyname>Schwing</keyname><forenames>Alexander G.</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author></authors><title>Monocular Object Instance Segmentation and Depth Ordering with CNNs</title><categories>cs.CV</categories><comments>International Conference on Computer Vision (ICCV), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we tackle the problem of instance-level segmentation and depth
ordering from a single monocular image. Towards this goal, we take advantage of
convolutional neural nets and train them to directly predict instance-level
segmentations where the instance ID encodes the depth ordering within image
patches. To provide a coherent single explanation of an image we develop a
Markov random field which takes as input the predictions of convolutional
neural nets applied at overlapping patches of different resolutions, as well as
the output of a connected component algorithm. It aims to predict accurate
instance-level segmentation and depth ordering. We demonstrate the
effectiveness of our approach on the challenging KITTI benchmark and show good
performance on both tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03190</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03190</id><created>2015-05-12</created><authors><author><keyname>Choromanski</keyname><forenames>Krzysztof</forenames></author></authors><title>Efficient data hashing with structured binary embeddings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present here new mechanisms for hashing data via binary embeddings.
Contrary to most of the techniques presented before, the embedding matrix of
our mechanism is highly structured. That enables us to perform hashing more
efficiently and use less memory. What is crucial and nonintuitive is the fact
that imposing structured mechanism does not affect the quality of the produced
hash. To the best of our knowledge, we are the first to give strong theoretical
guarantees of the proposed binary hashing method by proving the efficiency of
the mechanism for several classes of structured projection matrices. As a
corollary, we obtain binary hashing mechanisms with strong concentration
results for circulant and Topelitz matrices. Our approach is however much more
general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03193</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03193</id><created>2015-05-12</created><authors><author><keyname>Garcia</keyname><forenames>Nil</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander M.</forenames></author><author><keyname>Coulon</keyname><forenames>Martial</forenames></author><author><keyname>Dabin</keyname><forenames>Jason A.</forenames></author></authors><title>High Precision TOA-based Direct Localization of Multiple Sources in
  Multipath</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization of radio frequency sources over multipath channels is a
difficult problem arising in applications such as outdoor or indoor gelocation.
Common approaches that combine ad-hoc methods for multipath mitigation with
indirect localization relying on intermediary parameters such as
time-of-arrivals, time difference of arrivals or received signal strengths,
provide limited performance. This work models the localization of known
waveforms over unknown multipath channels in a sparse framework, and develops a
direct approach in which multiple sources are localized jointly, directly from
observations obtained at distributed sources. The proposed approach exploits
channel properties that enable to distinguish line-of-sight (LOS) from non-LOS
signal paths. Theoretical guarantees are established for correct recovery of
the sources' locations by atomic norm minimization. A second-order cone-based
algorithm is developed to produce the optimal atomic decomposition, and it is
shown to produce high accuracy location estimates over complex scenes, in which
sources are subject to diverse multipath conditions, including lack of LOS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03205</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03205</id><created>2015-05-12</created><updated>2015-05-14</updated><authors><author><keyname>Taisho</keyname><forenames>Tsukamoto</forenames></author><author><keyname>Kanji</keyname><forenames>Tanaka</forenames></author></authors><title>Leveraging Image based Prior for Visual Place Recognition</title><categories>cs.CV</categories><comments>8 pages, 6 figures, preprint. Accepted for publication in MVA2015
  (oral presentation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we propose a novel scene descriptor for visual place
recognition. Unlike popular bag-of-words scene descriptors which rely on a
library of vector quantized visual features, our proposed descriptor is based
on a library of raw image data, such as publicly available photo collections
from Google StreetView and Flickr. The library images need not to be associated
with spatial information regarding the viewpoint and orientation of the scene.
As a result, these images are cheaper than the database images; in addition,
they are readily available. Our proposed descriptor directly mines the image
library to discover landmarks (i.e., image patches) that suitably match an
input query/database image. The discovered landmarks are then compactly
described by their pose and shape (i.e., library image ID, bounding boxes) and
used as a compact discriminative scene descriptor for the input image. We
evaluate the effectiveness of our scene description framework by comparing its
performance to that of previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03209</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03209</id><created>2015-05-12</created><authors><author><keyname>Zhang</keyname><forenames>Haijun</forenames></author><author><keyname>Jiang</keyname><forenames>Chunxiao</forenames></author><author><keyname>Hu</keyname><forenames>Rose Qingyang</forenames></author><author><keyname>Qian</keyname><forenames>Yi</forenames></author></authors><title>Self-Organization in Disaster Resilient Heterogeneous Small Cell
  Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>to appear in IEEE Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous small cell networks with overlay femtocells and macrocell is a
promising solution for future heterogeneous wireless cellular communications.
However, great resilience is needed in heterogeneous small cells in case of
accidents, attacks and natural disasters. In this article, we first describe
the network architecture of disaster resilient heterogeneous small cell
networks (DRHSCNs), where several self-organization inspired approaches are
applied. Based on the proposed resilient heterogeneous small cell network
architecture, self-configuring (power, physical cell ID and neighbor cell list
self-configuration) and self-optimizing (coverage and capacity optimization and
mobility robustness optimization) techniques are investigated in the DRHSCN.
Simulation results show that self-configuration and self-optimization can
effectively improve the performance of the deployment and operation of the
small cell networks in disaster scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03213</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03213</id><created>2015-05-12</created><authors><author><keyname>Lin</keyname><forenames>Cheng-Wei</forenames></author><author><keyname>Jang</keyname><forenames>Jae-Won</forenames></author><author><keyname>Ghosh</keyname><forenames>Swaroop</forenames></author></authors><title>Schmitt-Trigger-based Recycling Sensor and Robust and High-Quality PUFs
  for Counterfeit IC Detection</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Schmitt-Trigger (ST) based recycling sensor that are tailored to
amplify the aging mechanisms and detect fine grained recycling (minutes to
seconds). We exploit the susceptibility of ST to process variations to realize
high-quality arbiter PUF. Conventional SRAM PUF suffer from environmental
fluctuation-induced bit flipping. We propose 8T SRAM PUF with a back-to-back
PMOS latch to improve robustness by 4X. We also propose a low-power 7T SRAM
with embedded Magnetic Tunnel Junction (MTJ) devices to enhance the robustness
(2.3X to 20X).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03214</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03214</id><created>2015-05-12</created><authors><author><keyname>Zhong</keyname><forenames>Lin-Feng</forenames></author><author><keyname>Liu</keyname><forenames>Jian-Guo</forenames></author><author><keyname>Shang</keyname><forenames>Ming-Sheng</forenames></author></authors><title>Iterative resource allocation based on propagation feature of node for
  identifying the influential nodes</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 5 figures</comments><doi>10.1016/j.physleta.2015.05.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Identification of the influential nodes in networks is one of the most
promising domains. In this paper, we present an improved iterative resource
allocation (IIRA) method by considering the centrality information of neighbors
and the influence of spreading rate for a target node. Comparing with the
results of the Susceptible Infected Recovered (SIR) model for four real
networks, the IIRA method could identify influential nodes more accurately than
the tradition IRA method. Specially, in the Erdos network, the Kendall's tau
could be enhanced 23\% when the spreading rate is 0.12. In the Protein network,
the Kendall's tau could be enhanced 24\% when the spreading rate is 0.08.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03218</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03218</id><created>2015-05-12</created><authors><author><keyname>Kapovich</keyname><forenames>Ilya</forenames></author></authors><title>Musings on generic-case complexity</title><categories>cs.CC math.GR math.LO</categories><comments>9 pages, no figures</comments><msc-class>Primary 03D15, 68Q15 Secondary 20F, 68Q17, 68Q25, 94A</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a more general definition of generic-case complexity, based on
using a random process for generating inputs of an algorithm and using the time
needed to generate an input as a way of measuring the size of that input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03227</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03227</id><created>2015-05-12</created><authors><author><keyname>Wang</keyname><forenames>Keze</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Lu</keyname><forenames>Jiangbo</forenames></author><author><keyname>Li</keyname><forenames>Chenglong</forenames></author><author><keyname>Shi</keyname><forenames>Keyang</forenames></author></authors><title>PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance
  Contrast Measures with Edge-Preserving Coherence</title><categories>cs.CV</categories><comments>14 pages, 14 figures, 1 table, to appear in IEEE Transactions on
  Image Processing</comments><msc-class>68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by recent vision and graphics applications such as image segmentation
and object recognition, computing pixel-accurate saliency values to uniformly
highlight foreground objects becomes increasingly important. In this paper, we
propose a unified framework called PISA, which stands for Pixelwise Image
Saliency Aggregating various bottom-up cues and priors. It generates spatially
coherent yet detail-preserving, pixel-accurate and fine-grained saliency, and
overcomes the limitations of previous methods which use homogeneous
superpixel-based and color only treatment. PISA aggregates multiple saliency
cues in a global context such as complementary color and structure contrast
measures with their spatial priors in the image domain. The saliency confidence
is further jointly modeled with a neighborhood consistence constraint into an
energy minimization formulation, in which each pixel will be evaluated with
multiple hypothetical saliency levels. Instead of using global discrete
optimization methods, we employ the cost-volume filtering technique to solve
our formulation, assigning the saliency levels smoothly while preserving the
edge-aware structure details. In addition, a faster version of PISA is
developed using a gradient-driven image sub-sampling strategy to greatly
improve the runtime efficiency while keeping comparable detection accuracy.
Extensive experiments on a number of public datasets suggest that PISA
convincingly outperforms other state-of-the-art approaches. In addition, with
this work we also create a new dataset containing $800$ commodity images for
evaluating saliency detection. The dataset and source code of PISA can be
downloaded at http://vision.sysu.edu.cn/project/PISA/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03229</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03229</id><created>2015-05-12</created><authors><author><keyname>Sato</keyname><forenames>Ikuro</forenames></author><author><keyname>Nishimura</keyname><forenames>Hiroki</forenames></author><author><keyname>Yokoi</keyname><forenames>Kensuke</forenames></author></authors><title>APAC: Augmented PAttern Classification with Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks have been exhibiting splendid accuracies in many of
visual pattern classification problems. Many of the state-of-the-art methods
employ a technique known as data augmentation at the training stage. This paper
addresses an issue of decision rule for classifiers trained with augmented
data. Our method is named as APAC: the Augmented PAttern Classification, which
is a way of classification using the optimal decision rule for augmented data
learning. Discussion of methods of data augmentation is not our primary focus.
We show clear evidences that APAC gives far better generalization performance
than the traditional way of class prediction in several experiments. Our
convolutional neural network model with APAC achieved a state-of-the-art
accuracy on the MNIST dataset among non-ensemble classifiers. Even our
multilayer perceptron model beats some of the convolutional models with
recently invented stochastic regularization techniques on the CIFAR-10 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03231</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03231</id><created>2015-05-12</created><authors><author><keyname>ElSherief</keyname><forenames>Mai</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author><author><keyname>Zahran</keyname><forenames>Ahmed</forenames></author><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>An Information-theoretic Model for Knowledge Sharing in Opportunistic
  Social Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we establish fundamental limits on the performance of knowledge
sharing in opportunistic social net- works. In particular, we introduce a novel
information-theoretic model to characterize the performance limits of knowledge
sharing policies. Towards this objective, we first introduce the notions of
knowledge gain and its upper bound, knowledge gain limit, per user. Second, we
characterize these quantities for a number of network topologies and sharing
policies. This work constitutes a first step towards defining and
characterizing the performance limits and trade-offs associated with knowledge
sharing in opportunistic social networks. Finally, we present nu- merical
results characterizing the cumulative knowledge gain over time and its upper
bound, using publicly available smartphone data. The results confirm the key
role of the proposed model to motivate future research in this ripe area of
research as well as new knowledge sharing policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03235</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03235</id><created>2015-05-13</created><authors><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author></authors><title>Optimizing Ad Allocation in Social Advertising</title><categories>cs.SI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social advertising (or social promotion) is an effective approach that
produces a significant cascade of adoption through influence in the online
social networks. The goal of this work is to optimize the ad allocation from
the platform's perspective. On the one hand, the platform would like to
maximize revenue earned from each advertiser by exposing their ads to as many
people as possible, one the other hand, the platform wants to reduce
free-riding to ensure the truthfulness of the advertiser. To access this
tradeoff, we adopt the concept of \emph{regret} \citep{viral2015social} to
measure the performance of an ad allocation scheme. In particular, we study two
social advertising problems: \emph{budgeted social advertising problem} and
\emph{unconstrained social advertising problem}. In the first problem, we aim
at selecting a set of seeds for each advertiser that minimizes the regret while
setting budget constraints on the attention cost; in the second problem, we
propose to optimize a linear combination of the regret and attention costs. We
prove that both problems are NP-hard, and then develop a constant factor
approximation algorithm for each problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03236</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03236</id><created>2015-05-13</created><authors><author><keyname>Jensi</keyname><forenames>R.</forenames></author><author><keyname>Jiji</keyname><forenames>G. Wiselin</forenames></author></authors><title>Hybrid data clustering approach using K-Means and Flower Pollination
  Algorithm</title><categories>cs.LG cs.IR cs.NE</categories><comments>11 pages, Journal. Advanced Computational Intelligence: An
  International Journal (ACII), Vol.2, No.2, April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data clustering is a technique for clustering set of objects into known
number of groups. Several approaches are widely applied to data clustering so
that objects within the clusters are similar and objects in different clusters
are far away from each other. K-Means, is one of the familiar center based
clustering algorithms since implementation is very easy and fast convergence.
However, K-Means algorithm suffers from initialization, hence trapped in local
optima. Flower Pollination Algorithm (FPA) is the global optimization
technique, which avoids trapping in local optimum solution. In this paper, a
novel hybrid data clustering approach using Flower Pollination Algorithm and
K-Means (FPAKM) is proposed. The proposed algorithm results are compared with
K-Means and FPA on eight datasets. From the experimental results, FPAKM is
better than FPA and K-Means.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03239</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03239</id><created>2015-05-13</created><authors><author><keyname>Hegde</keyname><forenames>Sarika</forenames></author><author><keyname>Achary</keyname><forenames>K. K.</forenames></author><author><keyname>Shetty</keyname><forenames>Surendra</forenames></author></authors><title>Feature selection using Fisher's ratio technique for automatic speech
  recognition</title><categories>cs.CL</categories><comments>in International Journal on Cybernetics &amp; Informatics (IJCI) Vol. 4,
  No. 2, April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic Speech Recognition involves mainly two steps; feature extraction
and classification . Mel Frequency Cepstral Coefficient is used as one of the
prominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC
coefficients is used as the feature vector in the classification step. But the
question is whether the same or improved classification accuracy can be
achieved by using a subset of 12 MFCC as feature vector. In this paper,
Fisher's ratio technique is used for selecting a subset of 12 MFCC coefficients
that contribute more in discriminating a pattern. The selected coefficients are
used in classification with Hidden Markov Model algorithm. The classification
accuracies that we get by using 12 coefficients and by using the selected
coefficients are compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03246</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03246</id><created>2015-05-13</created><authors><author><keyname>Koong</keyname><forenames>Kok-Leong</forenames></author><author><keyname>Haw</keyname><forenames>Su-Cheng</forenames></author><author><keyname>Soon</keyname><forenames>Lay-Ki</forenames></author><author><keyname>Subramaniam</keyname><forenames>Samini</forenames></author></authors><title>Prefix-based Labeling Annotation for Effective XML Fragmentation</title><categories>cs.DB</categories><comments>12 pages, invited extension from conference paper. International
  Journal of Computer Science &amp; Information Technology (IJCSIT), Vol 7, No 2,
  April 2015</comments><acm-class>H.2.4</acm-class><doi>10.5121/ijcsit.2015.7209</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  XML is gradually employed as a standard of data exchange in web environment
since its inception in the 90s until present. It serves as a data exchange
between systems and other applications. Meanwhile the data volume has grown
substantially in the web and thus effective methods of storing and retrieving
these data is essential. One recommended way is physically or virtually
fragments the large chunk of data and distributes the fragments into different
nodes. Fragmentation design of XML document contains of two parts:
fragmentation operation and fragmentation method. The three fragmentation
operations are Horizontal, Vertical and Hybrid. It determines how the XML
should be fragmented. This paper aims to give an overview on the fragmentation
design consideration and subsequently, propose a fragmentation technique using
number addressing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03257</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03257</id><created>2015-05-13</created><authors><author><keyname>Yi</keyname><forenames>Xinyang</forenames></author><author><keyname>Wang</keyname><forenames>Zhaoran</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Liu</keyname><forenames>Han</forenames></author></authors><title>Optimal linear estimation under unknown nonlinear transform</title><categories>stat.ML cs.IT math.IT</categories><comments>25 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear regression studies the problem of estimating a model parameter
$\beta^* \in \mathbb{R}^p$, from $n$ observations
$\{(y_i,\mathbf{x}_i)\}_{i=1}^n$ from linear model $y_i = \langle
\mathbf{x}_i,\beta^* \rangle + \epsilon_i$. We consider a significant
generalization in which the relationship between $\langle \mathbf{x}_i,\beta^*
\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear,
noninvertible, as well as unknown. This model is known as the single-index
model in statistics, and, among other things, it represents a significant
generalization of one-bit compressed sensing. We propose a novel spectral-based
estimation procedure and show that we can recover $\beta^*$ in settings (i.e.,
classes of link function $f$) where previous algorithms fail. In general, our
algorithm requires only very mild restrictions on the (unknown) functional
relationship between $y_i$ and $\langle \mathbf{x}_i,\beta^* \rangle$. We also
consider the high dimensional setting where $\beta^*$ is sparse ,and introduce
a two-stage nonconvex framework that addresses estimation challenges in high
dimensional regimes where $p \gg n$. For a broad class of link functions
between $\langle \mathbf{x}_i,\beta^* \rangle$ and $y_i$, we establish minimax
lower bounds that demonstrate the optimality of our estimators in both the
classical and high dimensional regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03259</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03259</id><created>2015-05-13</created><updated>2015-11-16</updated><authors><author><keyname>Meng</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Ji-Feng</forenames></author></authors><title>Coordination Over Multi-Agent Networks With Unmeasurable States and
  Finite-Level Quantization</title><categories>cs.SY math.OC</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, the coordination of linear discrete-time multi-agent systems
over digital networks is investigated with unmeasurable states in agents'
dynamics. The quantized-observer based communication protocols and Certainty
Equivalence principle based control protocols are proposed to characterize the
inter-agent communication and the cooperative control in an integrative
framework. By investigating the structural and asymptotic properties of the
equations of stabilization and estimation errors nonlinearly coupled by the
finite-level quantization scheme, some necessary conditions and sufficient
conditions are given for the existence of such communication and control
protocols to ensure the inter-agent state observation and cooperative
stabilization. It is shown that these conditions come down to the simultaneous
stabilizability and the detectability of the dynamics of agents and the
structure of the communication network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03263</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03263</id><created>2015-05-13</created><authors><author><keyname>Prasanna</keyname><forenames>B. T.</forenames></author><author><keyname>Akki</keyname><forenames>C. B.</forenames></author></authors><title>A Comparative Study of Homomorphic and Searchable Encryption Schemes for
  Cloud Computing</title><categories>cs.CR</categories><comments>in IJASCSE Volume 4, Issue 5 (May 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a popular distributed network and utility model based
technology. Since in cloud the data is outsourced to third parties, the
protection of confidentiality and privacy of user data becomes important.
Different methods for securing the data in cloud have been proposed by
researchers including but not limited to Oblivious RAM, Searchable Encryption,
Functional Encryption, Homomorphic Encryption etc. This paper focuses on
Searchable and Homomorphic Encryption methods. Finally, a comparative study of
these two efficient cloud cryptographic methods has been carried out and given
here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03273</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03273</id><created>2015-05-13</created><updated>2015-11-05</updated><authors><author><keyname>Au&#xdf;erlechner</keyname><forenames>Simon</forenames></author><author><keyname>Jacobs</keyname><forenames>Swen</forenames></author><author><keyname>Khalimov</keyname><forenames>Ayrat</forenames></author></authors><title>Tight Cutoffs for Guarded Protocols with Fairness</title><categories>cs.LO</categories><comments>Accepted for publication at VMCAI 2016. Extended version, revised
  after conference reviews</comments><acm-class>F.3.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Guarded protocols were introduced in a seminal paper by Emerson and Kahlon
(2000), and describe systems of processes whose transitions are enabled or
disabled depending on the existence of other processes in certain local states.
We study parameterized model checking and synthesis of guarded protocols, both
aiming at formal correctness arguments for systems with any number of
processes. Cutoff results reduce reasoning about systems with an arbitrary
number of processes to systems of a determined, fixed size. Our work stems from
the observation that existing cutoff results for guarded protocols i) are
restricted to closed systems, and ii) are of limited use for liveness
properties because reductions do not preserve fairness. We close these gaps and
obtain new cutoff results for open systems with liveness properties under
fairness assumptions. Furthermore, we obtain cutoffs for the detection of
global and local deadlocks, which are of paramount importance in synthesis.
Finally, we prove tightness or asymptotic tightness for the new cutoffs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03279</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03279</id><created>2015-05-13</created><authors><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author><author><keyname>Boshkoska</keyname><forenames>Biljana Mileva</forenames></author><author><keyname>Kastrin</keyname><forenames>Andrej</forenames></author><author><keyname>Levnaji&#x107;</keyname><forenames>Zoran</forenames></author></authors><title>Quantifying the consistency of scientific databases</title><categories>cs.DL cs.SI physics.data-an physics.soc-ph</categories><comments>20 pages, 5 figures, 4 tables</comments><journal-ref>PLoS ONE 10(5), e0127390 (2015)</journal-ref><doi>10.1371/journal.pone.0127390</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Science is a social process with far-reaching impact on our modern society.
In the recent years, for the first time we are able to scientifically study the
science itself. This is enabled by massive amounts of data on scientific
publications that is increasingly becoming available. The data is contained in
several databases such as Web of Science or PubMed, maintained by various
public and private entities. Unfortunately, these databases are not always
consistent, which considerably hinders this study. Relying on the powerful
framework of complex networks, we conduct a systematic analysis of the
consistency among six major scientific databases. We found that identifying a
single &quot;best&quot; database is far from easy. Nevertheless, our results indicate
appreciable differences in mutual consistency of different databases, which we
interpret as recipes for future bibliometric studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03281</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03281</id><created>2015-05-13</created><authors><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author></authors><title>Codes for Partially Stuck-at Memory Cells</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study a new model of defect memory cells, called partially
stuck-at memory cells, which is motivated by the behavior of multi-level cells
in non-volatile memories such as flash memories and phase change memories. If a
cell can store the $q$ levels $0, 1, \dots, q-1$, we say that it is partially
stuck-at level $s$, where $1 \leq s \leq q-1$, if it can only store values
which are at least $s$. We follow the common setup where the encoder knows the
positions and levels of the partially stuck-at cells whereas the decoder does
not.
  Our main contribution in the paper is the study of codes for masking $u$
partially stuck-at cells. We first derive lower and upper bounds on the
redundancy of such codes. The upper bounds are based on two trivial
constructions. We then present three code constructions over an alphabet of
size $q$, by first considering the case where the cells are partially stuck-at
level $s=1$. The first construction works for $u&lt;q$ and is asymptotically
optimal if $u+1$ divides $q$. The second construction uses the reduced row
Echelon form of matrices to generate codes for the case $u\geq q$, and the
third construction solves the case of arbitrary $u$ by using codes which mask
binary stuck-at cells. We then show how to generalize all constructions to
arbitrary stuck levels. Furthermore, we study the dual defect model in which
cells cannot reach higher levels, and show that codes for partially stuck-at
cells can be used to mask this type of defects as well. Lastly, we analyze the
capacity of the partially stuck-at memory channel and study how far our
constructions are from the capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03303</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03303</id><created>2015-05-13</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Dropshipping - alternative infrastructure of sales and promotion</title><categories>cs.CY</categories><comments>14 pages, in Russian. ISSN 1028-5849</comments><journal-ref>Marketing in Russia and Abroad. 2012. No 1 (87). P. 90-104</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An article about the transformation of the theory and practice of marketing
in terms of e-commerce and network economy. The author considers Internet
Marketing as an independent marketing communication in a virtual environment.
The main thesis of the article: virtual environment determines the
transformation of marketing, changing methods, priorities and structure not
only practice, but also the theory of marketing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03305</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03305</id><created>2015-05-13</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Transformation of marketing in the e-commerce</title><categories>cs.CY</categories><comments>13 pages, in Russian. ISSN 2071-3762</comments><journal-ref>Practical marketing. 2013. No 1 (191). P. 4-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article is about transformation of the theory and practice of marketing
in the conditions of e-commerce and network economy. The author considers
Internet-marketing as an independent kind of marketing in the virtual
communicative environment. The basic thesis of the article: the virtual
environment defines marketing transformation, changing methods, priorities and
structure at practice and then theories of marketing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03308</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03308</id><created>2015-05-13</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author><author><keyname>Karpov</keyname><forenames>Valery</forenames></author></authors><title>Network Internet-communications as an instrument of marketing</title><categories>cs.CY</categories><comments>12 pages, in Russian. ISSN 2071-3762</comments><journal-ref>Practical marketing. 2013. No 2 (192). P. 32-39</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article is about the features of application of network
Internet-communications for advancement of the goods. Wide development of
Internet technologies has transformed social communications into the
independent tool of marketing. Authors classify and analyze possibilities of
use of network Internet-communications in the marketing environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03309</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03309</id><created>2015-05-13</created><updated>2015-12-07</updated><authors><author><keyname>Gattami</keyname><forenames>Ather</forenames></author><author><keyname>Ringh</keyname><forenames>Emil</forenames></author><author><keyname>Karlsson</keyname><forenames>Johan</forenames></author></authors><title>Time Localization and Capacity of Faster-Than-Nyquist Signaling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider communication over the bandwidth limited analog
white Gaussian noise channel using non-orthogonal pulses. In particular, we
consider non-orthogonal transmission by signaling samples at a rate higher than
the Nyquist rate. Using the faster-than-Nyquist (FTN) framework, Mazo showed
that one may transmit symbols carried by sinc pulses at a higher rate than that
dictated by Nyquist without loosing bit error rate. However, as we will show in
this paper, such pulses are not necessarily well localized in time. In fact,
assuming that signals in the FTN framework are well localized in time, one can
construct a signaling scheme that violates the Shannon capacity bound. We also
show directly that FTN signals are in general not well localized in time.
Therefore, the results of Mazo do not imply that one can transmit more data per
time unit without degrading performance in terms of error probability.
  We also consider FTN signaling in the case of pulses that are different from
the sinc pulses. We show that one can use a precoding scheme of low complexity
to remove the inter-symbol interference. This leads to the possibility of
increasing the number of transmitted samples per time unit and compensate for
spectral inefficiency due to signaling at the Nyquist rate of the non sinc
pulses. We demonstrate the power of the precoding scheme by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03315</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03315</id><created>2015-05-13</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Innovative forms of sales in the e-commerce</title><categories>cs.CY</categories><comments>12 pages, in Russian. ISSN 2071-3762</comments><journal-ref>Practical marketing. 2013. No 4 (194). P. 23-34</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article about transformation of methods of remote sales in the conditions of
e-commerce. The author classifies and analyzes features of new methods of
electronic sales in the virtual environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03326</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03326</id><created>2015-05-13</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>New economy: from crisis of dot-coms to virtual business</title><categories>cs.CY</categories><comments>13 pages, in Russian. ISSN 0204-3653</comments><journal-ref>Information resources of Russia. 2013. No 2 (132). P. 27-32</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article about objective regularity and mechanisms of formation of network
economy in the conditions of crisis of {\guillemotleft}new
economy{\guillemotright}. The network economy is considered as a basis of a
following business cycle. The author analyzes features of institutional
transformation of economic relations under the influence of development of
cloudy technologies and the virtual organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03329</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03329</id><created>2015-05-13</created><authors><author><keyname>Milosevic</keyname><forenames>Jelena</forenames></author><author><keyname>Ferrante</keyname><forenames>Alberto</forenames></author><author><keyname>Malek</keyname><forenames>Miroslaw</forenames></author></authors><title>A general practitioner or a specialist for your infected smartphone?</title><categories>cs.CR</categories><comments>2 pages poster, IEEE Symposium on Security and Privacy, San Jose,
  USA, May 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With explosive growth in the number of mobile devices, the mobile malware is
rapidly spreading as well, and the number of encountered malware families is
increasing. Existing solutions, which are mainly based on one malware detector
running on the phone or in the cloud, are no longer effective. Main problem
lies in the fact that it might be impossible to create a unique mobile malware
detector that would be able to detect different malware families with high
accuracy, being at the same time lightweight enough not to drain battery
quickly and fast enough to give results of detection promptly. The proposed
approach to mobile malware detection is analogous to general practitioner
versus specialist approach to dealing with a medical problem. Similarly to a
general practitioner that, based on indicative symptoms identifies potential
illnesses and sends the patient to an appropriate specialist, our detection
system distinguishes among symptoms representing different malware families
and, once the symptoms are detected, it triggers specific analyses. A system
monitoring application operates in the same way as a general practitioner. It
is able to distinguish between different symptoms and trigger appropriate
detection mechanisms. As an analogy to different specialists, an ensemble of
detectors, each of which specifically trained for a particular malware family,
is used. The main challenge of the approach is to define representative
symptoms of different malware families and train detectors accordingly to them.
The main goal of the poster is to foster discussion on the most representative
symptoms of different malware families and to discuss initial results in this
area obtained by using Malware Genome project dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03331</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03331</id><created>2015-05-13</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Fikadu</keyname><forenames>Mulugeta K.</forenames></author><author><keyname>Ho-Van</keyname><forenames>Khuong</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>The Area Under a Receiver Operating Characteristic Curve Over
  Nakagami$-q$ Fading Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to the analysis of the performance of energy detection
based spectrum sensing in the presence of enriched fading conditions which are
distinct for the large number of multipath components and the lack of a
dominant components. This type of fading conditions are characterized
efficiently by the well known Nakagami${-}q$ or Hoyt distribution and the
proposed analysis is carried out in the context of the area under the receiver
operating characteristics (ROC) curve (AUC). Unlike the widely used probability
of detection metric, the AUC is a single metric and has been shown to be rather
capable of evaluating the performance of a detector in applications relating to
cognitive radio, radar systems and biomedical engineering, among others. Based
on this, novel analytic expressions are derived for the average AUC and its
complementary metric, average CAUC, for both integer and fractional values of
the involved time-bandwidth product. The derived expressions have a tractable
algebraic representation which renders them convenient to handle both
analytically and numerically. Based on this, they are employed in analyzing the
behavior of energy detection based spectrum sensing over enriched fading
conditions for different severity scenarios, which demonstrates that the
performance of energy detectors is, as expected, closely related to the value
of the fading parameter $q$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03332</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03332</id><created>2015-05-13</created><authors><author><keyname>Ebongue</keyname><forenames>Jean Louis Fendji Kedieng</forenames></author><author><keyname>Thron</keyname><forenames>Christopher</forenames></author><author><keyname>Nlong</keyname><forenames>Jean Michel</forenames></author></authors><title>Mesh Router Nodes placement in Rural Wireless Mesh Networks</title><categories>cs.NI</categories><comments>8 pages, 4 figures, Conference CARI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of placement of mesh router nodes in Wireless Mesh Networks is
known to be a NP hard problem. In this paper, the problem is addressed under a
constraint of network model tied to rural regions where we usually observe low
density and sparse population. We consider the area to cover as decomposed into
a set of elementary areas which can be required or optional in terms of
coverage and where a node can be placed or not. We propose an effective
algorithm to ensure the coverage. This algorithm is based on metropolis
approach. We evaluated the proposed algorithm on an instance network. A close
to 100 percent coverage with an optimal number of routers showed the efficiency
of our approach for the mesh router node placement problem
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03334</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03334</id><created>2015-05-13</created><updated>2015-11-03</updated><authors><author><keyname>Fran&#xe7;ois</keyname><forenames>Nathana&#xeb;l</forenames></author><author><keyname>Magniez</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>de Rougemont</keyname><forenames>Michel</forenames></author><author><keyname>Serre</keyname><forenames>Olivier</forenames></author></authors><title>Streaming Property Testing of Visibly Pushdown Languages</title><categories>cs.DS cs.CC</categories><comments>23 pages. Major modifications in the presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of language recognition, we demonstrate the superiority of
streaming property testers against streaming algorithms and property testers,
when they are not combined. Initiated by Feigenbaum et al., a streaming
property tester is a streaming algorithm recognizing a language under the
property testing approximation: it must distinguish inputs of the language from
those that are $\varepsilon$-far from it, while using the smallest possible
memory (rather than limiting its number of input queries).
  Our main result is a streaming $\varepsilon$-property tester for visibly
pushdown languages (VPL) with one-sided error using memory space
$\mathrm{poly}((\log n) / \varepsilon)$.
  This constructions relies on a (non-streaming) property tester for weighted
regular languages based on a previous tester by Alon et al. We provide a simple
application of this tester for streaming testing special cases of instances of
VPL that are already hard for both streaming algorithms and property testers.
  Our main algorithm is a combination of an original simulation of visibly
pushdown automata using a stack with small height but possible items of linear
size. In a second step, those items are replaced by small sketches. Those
sketches relies on a notion of suffix-sampling we introduce. This sampling is
the key idea connecting our streaming tester algorithm to property testers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03337</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03337</id><created>2015-05-13</created><authors><author><keyname>Koliander</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Pichler</keyname><forenames>Georg</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author></authors><title>Entropy and Source Coding for Integer-Dimensional Singular Random
  Variables</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><msc-class>94A17, 94A29</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entropy and differential entropy are important quantities in information
theory. A tractable extension to singular random variables-which are neither
discrete nor continuous-has not been available so far. Here, we present such an
extension for the practically relevant class of integer-dimensional singular
random variables. The proposed entropy definition contains the entropy of
discrete random variables and the differential entropy of continuous random
variables as special cases. We show that it transforms in a natural manner
under Lipschitz functions, and that it is invariant under unitary
transformations. We define joint entropy and conditional entropy for
integer-dimensional singular random variables, and we show that the proposed
entropy conveys useful expressions of the mutual information. As first
applications of our entropy definition, we present a result on the minimal
expected codeword length of quantized integer-dimensional singular sources and
a Shannon lower bound for integer-dimensional singular sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03340</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03340</id><created>2015-05-13</created><updated>2015-08-03</updated><authors><author><keyname>Balyo</keyname><forenames>Tomas</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Sinz</keyname><forenames>Carsten</forenames></author></authors><title>HordeSat: A Massively Parallel Portfolio SAT Solver</title><categories>cs.LO</categories><comments>Accepted for SAT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple yet successful approach to parallel satisfiability (SAT) solving is
to run several different (a portfolio of) SAT solvers on the input problem at
the same time until one solver finds a solution. The SAT solvers in the
portfolio can be instances of a single solver with different configuration
settings. Additionally the solvers can exchange information usually in the form
of clauses. In this paper we investigate whether this approach is applicable in
the case of massively parallel SAT solving. Our solver is intended to run on
clusters with thousands of processors, hence the name HordeSat. HordeSat is a
fully distributed portfolio-based SAT solver with a modular design that allows
it to use any SAT solver that implements a given interface. HordeSat has a
decentralized design and features hierarchical parallelism with interleaved
communication and search. We experimentally evaluated it using all the
benchmark problems from the application tracks of the 2011 and 2014
International SAT Competitions. The experiments demonstrate that HordeSat is
scalable up to hundreds or even thousands of processors achieving significant
speedups especially for hard instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03343</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03343</id><created>2015-05-13</created><updated>2015-12-01</updated><authors><author><keyname>Sin'ya</keyname><forenames>Ryoma</forenames></author></authors><title>Zero-One Law for Regular Languages and Semigroups with Zero</title><categories>cs.FL</categories><comments>See more recent paper arXiv:1509.07209</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A regular language has the zero-one law if its asymptotic density converges
to either zero or one. We prove that the class of all zero-one languages is
closed under Boolean operations and quotients. Moreover, we prove that a
regular language has the zero-one law if and only if its syntactic monoid has a
zero element. Our proof gives both algebraic and automata characterisation of
the zero-one law for regular languages, and it leads the following two
corollaries: (i) There is an O(n log n) algorithm for testing whether a given
regular language has the zero-one law. (ii) The Boolean closure of existential
first-order logic over finite words has the zero-one law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03344</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03344</id><created>2015-05-13</created><authors><author><keyname>George</keyname><forenames>Anjith</forenames></author><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>A Framework for Fast Face and Eye Detection</title><categories>cs.CV</categories><comments>5 pages , 10 figures,</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face detection is an essential step in many computer vision applications like
surveillance, tracking, medical analysis, facial expression analysis etc.
Several approaches have been made in the direction of face detection. Among
them, Haar-like features based method is a robust method. In spite of the
robustness, Haar - like features work with some limitations. However, with some
simple modifications in the algorithm, its performance can be made faster and
more robust. The present work refers to the increase in speed of operation of
the original algorithm by down sampling the frames and its analysis with
different scale factors. It also discusses the detection of tilted faces using
an affine transformation of the input image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03346</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03346</id><created>2015-05-13</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Tsiftsis</keyname><forenames>Theodoros A.</forenames></author><author><keyname>Brychkov</keyname><forenames>Yury A.</forenames></author><author><keyname>Freear</keyname><forenames>Steven</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Analytic Solutions to a Useful Marcum $Q{-}$Function Based Integral</title><categories>cs.IT math.IT</categories><comments>16 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents analytic solutions for a useful integral in wireless
communications, which involves the Marcum $Q{-}$function in combination with an
exponential function and arbitrary power terms. The derived expressions have a
rather simple algebraic representation which renders them convenient both
analytically and computationally. Furthermore, they can be useful in wireless
communications and particularly in the context of cognitive radio
communications and radar systems, where this integral is often encountered. To
this end, we derive novel expressions for the probability of detection in
energy detection based spectrum sensing over $\eta{-}\mu$ fading channels.
These expressions are given in closed-form and are subsequently employed in
analyzing the effects of generalised multipath fading conditions in cognitive
radio systems. As expected, it is shown that the detector is highly dependent
upon the severity of fading conditions as even slight variation of the fading
parameters affect the corresponding performance significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03352</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03352</id><created>2015-05-13</created><authors><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>George</keyname><forenames>Anjith</forenames></author><author><keyname>Happy</keyname><forenames>S. L.</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>A Vision Based System for Monitoring the Loss of Attention in Automotive
  Drivers</title><categories>cs.CV</categories><comments>14 pages, 24 figures Journal article</comments><journal-ref>Intelligent Transportation Systems, IEEE Transactions on 14, no. 4
  (2013): 1825-1838</journal-ref><doi>10.1109/TITS.2013.2271052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On board monitoring of the alertness level of an automotive driver has been a
challenging research in transportation safety and management. In this paper, we
propose a robust real time embedded platform to monitor the loss of attention
of the driver during day as well as night driving conditions. The PERcentage of
eye CLOSure (PERCLOS) has been used as the indicator of the alertness level. In
this approach, the face is detected using Haar like features and tracked using
a Kalman Filter. The Eyes are detected using Principal Component Analysis (PCA)
during day time and the block Local Binary Pattern (LBP) features during night.
Finally the eye state is classified as open or closed using Support Vector
Machines(SVM). In plane and off plane rotations of the drivers face have been
compensated using Affine and Perspective Transformation respectively.
Compensation in illumination variation is carried out using Bi Histogram
Equalization (BHE). The algorithm has been cross validated using brain signals
and finally been implemented on a Single Board Computer (SBC) having Intel Atom
processor, 1 GB RAM, 1.66 GHz clock, x86 architecture, Windows Embedded XP
operating system. The system is found to be robust under actual driving
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03356</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03356</id><created>2015-05-13</created><updated>2015-06-08</updated><authors><author><keyname>Fayaz</keyname><forenames>Seyed K.</forenames></author><author><keyname>Tobioka</keyname><forenames>Yoshiaki</forenames></author><author><keyname>Chaki</keyname><forenames>Sagar</forenames></author><author><keyname>Sekar</keyname><forenames>Vyas</forenames></author></authors><title>Scalable Testing of Context-Dependent Policies over Stateful Data Planes
  with Armstrong</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network operators today spend significant manual effort in ensuring and
checking that the network meets their intended policies. While recent work in
network verification has made giant strides to reduce this effort, they focus
on simple reachability properties and cannot handle context-dependent policies
(e.g., how many connections has a host spawned) that operators realize using
stateful network functions (NFs). Together, these introduce new expressiveness
and scalability challenges that fall outside the scope of existing network
verification mechanisms. To address these challenges, we present Armstrong, a
system that enables operators to test if network with stateful data plane
elements correctly implements a given context-dependent policy. Our design
makes three key contributions to address expressiveness and scalability: (1) An
abstract I/O unit for modeling network I/O that encodes policy-relevant context
information; (2) A practical representation of complex NFs via an ensemble of
finite state machines abstraction; and (3) A scalable application of symbolic
execution to tackle state space explosion. We demonstrate that Armstrong is
several orders of magnitude faster than existing mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03357</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03357</id><created>2015-05-13</created><updated>2015-12-11</updated><authors><author><keyname>Homolya</keyname><forenames>Mikl&#xf3;s</forenames></author><author><keyname>Ham</keyname><forenames>David A.</forenames></author></authors><title>A parallel edge orientation algorithm for quadrilateral meshes</title><categories>cs.MS cs.CG</categories><comments>Second revision: minor changes</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One approach to achieving correct finite element assembly is to ensure that
the local orientation of facets relative to each cell in the mesh is consistent
with the global orientation of that facet. Rognes et al. have shown how to
achieve this for any mesh composed of simplex elements, and deal.II contains a
serial algorithm to construct a consistent orientation of any quadrilateral
mesh of an orientable manifold.
  The core contribution of this paper is the extension of this algorithm for
distributed memory parallel computers, which facilitates its seamless
application as part of a parallel simulation system.
  Furthermore, our analysis establishes a link between the well-known
Union-Find algorithm and the construction of a consistent orientation of a
quadrilateral mesh. As a result, existing work on the parallelisation of the
Union-Find algorithm can be easily adapted to construct further parallel
algorithms for mesh orientations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03358</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03358</id><created>2015-05-13</created><updated>2015-05-15</updated><authors><author><keyname>Schifanella</keyname><forenames>Rossano</forenames></author><author><keyname>Redi</keyname><forenames>Miriam</forenames></author><author><keyname>Aiello</keyname><forenames>Luca</forenames></author></authors><title>An Image is Worth More than a Thousand Favorites: Surfacing the Hidden
  Beauty of Flickr Pictures</title><categories>cs.SI cs.CV cs.CY cs.MM</categories><comments>ICWSM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of attention in social media tend to obey power laws. Attention
concentrates on a relatively small number of popular items and neglecting the
vast majority of content produced by the crowd. Although popularity can be an
indication of the perceived value of an item within its community, previous
research has hinted to the fact that popularity is distinct from intrinsic
quality. As a result, content with low visibility but high quality lurks in the
tail of the popularity distribution. This phenomenon can be particularly
evident in the case of photo-sharing communities, where valuable photographers
who are not highly engaged in online social interactions contribute with
high-quality pictures that remain unseen. We propose to use a computer vision
method to surface beautiful pictures from the immense pool of
near-zero-popularity items, and we test it on a large dataset of
creative-commons photos on Flickr. By gathering a large crowdsourced ground
truth of aesthetics scores for Flickr images, we show that our method retrieves
photos whose median perceived beauty score is equal to the most popular ones,
and whose average is lower by only 1.5%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03359</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03359</id><created>2015-05-13</created><authors><author><keyname>Matias</keyname><forenames>J. Nathan</forenames></author><author><keyname>Johnson</keyname><forenames>Amy</forenames></author><author><keyname>Boesel</keyname><forenames>Whitney Erin</forenames></author><author><keyname>Keegan</keyname><forenames>Brian</forenames></author><author><keyname>Friedman</keyname><forenames>Jaclyn</forenames></author><author><keyname>DeTar</keyname><forenames>Charlie</forenames></author></authors><title>Reporting, Reviewing, and Responding to Harassment on Twitter</title><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When people experience harassment online, from individual threats or
invective to coordinated campaigns of harassment, they have the option to
report the harassers and content to the platform where the harassment has
occurred. Platforms then evaluate harassment reports against terms of use and
other policies to decide whether to remove content or take action against the
alleged harasser--or not. On Twitter, harassing accounts can be deleted
entirely, suspended (with content made unavailable pending appeal or specific
changes), or sent a warning. Some platforms, including Twitter and YouTube,
grant authorized reporters or trusted flaggers special privileges to identify
and report inappropriate content on behalf of others. In November 2014, Twitter
granted Women, Action, and the Media (WAM!) this authorized reporter status. In
three weeks, WAM! reviewers assessed 811 incoming reports of harassment and
escalated 161 reports to Twitter, ultimately seeing Twitter carry out 70
account suspensions, 18 warnings, and one deleted account. This document
presents findings from this three-week project; it draws on both quantitative
and qualitative methods. Findings focus on the people reporting and receiving
harassment, the kinds of harassment that were reported, Twitter's response to
harassment reports, the process of reviewing harassment reports, and challenges
for harassment reporting processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03365</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03365</id><created>2015-05-13</created><authors><author><keyname>Kim</keyname><forenames>Wonsik</forenames></author><author><keyname>Lee</keyname><forenames>Kyoung Mu</forenames></author></authors><title>MRF Optimization by Graph Approximation</title><categories>cs.CV</categories><comments>CVPR2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph cuts-based algorithms have achieved great success in energy
minimization for many computer vision applications. These algorithms provide
approximated solutions for multi-label energy functions via move-making
approach. This approach fuses the current solution with a proposal to generate
a lower-energy solution. Thus, generating the appropriate proposals is
necessary for the success of the move-making approach. However, not much
research efforts has been done on the generation of &quot;good&quot; proposals,
especially for non-metric energy functions. In this paper, we propose an
application-independent and energy-based approach to generate &quot;good&quot; proposals.
With these proposals, we present a graph cuts-based move-making algorithm
called GA-fusion (fusion with graph approximation-based proposals). Extensive
experiments support that our proposal generation is effective across different
classes of energy functions. The proposed algorithm outperforms others both on
real and synthetic problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03374</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03374</id><created>2015-05-13</created><updated>2015-11-03</updated><authors><author><keyname>Pallister</keyname><forenames>James</forenames></author><author><keyname>Kerrison</keyname><forenames>Steve</forenames></author><author><keyname>Morse</keyname><forenames>Jeremy</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Data dependent energy modelling for worst case energy consumption
  analysis</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the impact of operand values upon instruction level
energy models of embedded processors, to explore whether the requirements for
safe worst case energy consumption (WCEC) analysis can be met. WCEC is similar
to worst case execution time (WCET) analysis, but seeks to determine whether a
task can be completed within an energy budget rather than within a deadline.
Existing energy models that underpin such analysis typically use energy
measurements from random input data, providing average or otherwise unbounded
estimates not necessarily suitable for worst case analysis.
  We examine energy consumption distributions of two benchmarks under a range
of input data on two cache-less embedded architectures, AVR and XS1-L. We find
that the worst case can be predicted with a distribution created from random
data. We propose a model to obtain energy distributions for instruction
sequences that can be composed, enabling WCEC analysis on program basic blocks.
Data dependency between instructions is also examined, giving a case where
dependencies create a bimodal energy distribution. The worst case energy
prediction remains safe. We conclude that worst-case energy models based on a
probabilistic approach are suitable for safe WCEC analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03389</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03389</id><created>2015-05-13</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Laws governing development of institutionalization of e-commerce</title><categories>cs.CY</categories><comments>8 pages, in Russian. ISSN 0204-3653</comments><journal-ref>Information resources of Russia. 2013. No 5 (135). P. 19-24</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article about objective laws of formation of social and economic institutes
in system of electronic commerce. Rapid development of Internet technologies
became the reason of deep institutional transformation of economic relations.
The author analyzes value transaction costs as motive power of formation of new
economic institutes in network economy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03398</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03398</id><created>2015-05-13</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Priorities of institutional regulation of e-commerce: Russia and world
  trends</title><categories>cs.CY</categories><comments>14 pages, in Russian. ISSN 2073-2872</comments><journal-ref>National interests: priorities and security. 2013. No 42 (231). P.
  11-22</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-commerce is gradually transformed from a version of trading activity to
independent branch of global network economy which cannot be ignored. The
Russian Federation is in the lead in the CIS on development of e-commerce, but
lags behind world leaders in institutionalization of e-commerce. Problems of
state regulation of e-commerce in Russia are analyzed in article, ways of their
decision are offered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03399</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03399</id><created>2015-05-13</created><authors><author><keyname>Li</keyname><forenames>Yanjun</forenames></author><author><keyname>Lee</keyname><forenames>Kiryung</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Identifiability in Blind Deconvolution with Subspace or Sparsity
  Constraints</title><categories>cs.IT math.IT</categories><comments>17 pages, 3 figures. Some of these results will be presented at SPARS
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind deconvolution (BD), the resolution of a signal and a filter given their
convolution, arises in many applications. Without further constraints, BD is
ill-posed. In practice, subspace or sparsity constraints have been imposed to
reduce the search space, and have shown some empirical success. However,
existing theoretical analysis on uniqueness in BD is rather limited. As an
effort to address the still mysterious question, we derive sufficient
conditions under which two vectors can be uniquely identified from their
circular convolution, subject to subspace or sparsity constraints. These
sufficient conditions provide the first algebraic sample complexities for BD.
We first derive a sufficient condition that applies to almost all bases or
frames. For blind deconvolution of vectors in $\mathbb{C}^n$, with two subspace
constraints of dimensions $m_1$ and $m_2$, the required sample complexity is
$n\geq m_1m_2$. Then we impose a sub-band structure on one basis, and derive a
sufficient condition that involves a relaxed sample complexity $n\geq
m_1+m_2-1$, which we show to be optimal. We present the extensions of these
results to BD with sparsity constraints or mixed constraints, with the sparsity
level replacing the subspace dimension. The cost for the unknown support in
this case is an extra factor of 2 in the sample complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03402</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03402</id><created>2015-05-13</created><authors><author><keyname>Edelsbrunner</keyname><forenames>Herbert</forenames></author><author><keyname>Iglesias-Ham</keyname><forenames>Mabel</forenames></author><author><keyname>Kurlin</keyname><forenames>Vitaliy</forenames></author></authors><title>Relaxed Disk Packing</title><categories>cs.CG</categories><comments>8 pages =&gt; 5 pages of main text plus 3 pages in appendix. Submitted
  to CCCG 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by biological questions, we study configurations of equal-sized
disks in the Euclidean plane that neither pack nor cover. Measuring the quality
by the probability that a random point lies in exactly one disk, we show that
the regular hexagonal grid gives the maximum among lattice configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03404</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03404</id><created>2015-05-13</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Institutionalization of the payment environment of e-commerce in Russia</title><categories>cs.CY</categories><comments>15 pages, in Russian. ISSN 2073-4484</comments><journal-ref>Financial analytics: science and experience. 2014. No 2 (188). P.
  8-19</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article about problems of Institutionalization of the payment environment of
e-commerce. The author analyzes tendencies of development of payment institutes
and prospect of creation of the International financial Centre in Russia on the
basis of primary development of e-commerce in spheres B2C and C2C. The author
proves, that in the conditions of globalization of the consumer markets the
analysis of a role and a place of transboundary private payments in structure
of National payment system of the Russian Federation become especially current.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03406</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03406</id><created>2015-05-13</created><authors><author><keyname>Aydin</keyname><forenames>Nuh</forenames></author><author><keyname>Asamov</keyname><forenames>Tsvetan</forenames></author></authors><title>A Database of $\mathbb{Z}_4$ Codes</title><categories>cs.IT math.IT</categories><journal-ref>Journal of Combinatorics, Information and System Sciences, Vol. 34
  No. 1-4 Comb, 2009, p: 1-12</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been much research on codes over $\mathbb{Z}_4$, sometimes called
quaternary codes, for over a decade. Yet, no database is available for best
known quaternary codes. This work introduces a new database for quaternary
codes. It also presents a new search algorithm called genetic code search
(GCS), as well as new quaternary codes obtained by existing and new search
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03410</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03410</id><created>2015-05-13</created><updated>2015-12-03</updated><authors><author><keyname>Fercoq</keyname><forenames>Olivier</forenames></author><author><keyname>Gramfort</keyname><forenames>Alexandre</forenames></author><author><keyname>Salmon</keyname><forenames>Joseph</forenames></author></authors><title>Mind the duality gap: safer rules for the Lasso</title><categories>stat.ML cs.LG math.OC stat.CO</categories><comments>erratum to ICML 2015, &quot;The authors would like to thanks Jalal Fadili
  and Jingwei Liang for helping clarifying some misleading statements on the
  equicorrelation set&quot;</comments><msc-class>68Uxx, 49N15, 62Jxx, 68Q32, 62-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Screening rules allow to early discard irrelevant variables from the
optimization in Lasso problems, or its derivatives, making solvers faster. In
this paper, we propose new versions of the so-called $\textit{safe rules}$ for
the Lasso. Based on duality gap considerations, our new rules create safe test
regions whose diameters converge to zero, provided that one relies on a
converging solver. This property helps screening out more variables, for a
wider range of regularization parameter values. In addition to faster
convergence, we prove that we correctly identify the active sets (supports) of
the solutions in finite time. While our proposed strategy can cope with any
solver, its performance is demonstrated using a coordinate descent algorithm
particularly adapted to machine learning use cases. Significant computing time
reductions are obtained with respect to previous safe rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03412</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03412</id><created>2015-05-13</created><authors><author><keyname>Kaluzhsky</keyname><forenames>Mikhail</forenames></author></authors><title>Transformation of chains of deliveries in network economy: priorities
  and prospects</title><categories>cs.CY</categories><comments>14 pages, in Russian. ISSN 2073-2872</comments><journal-ref>National interests: priorities and security. 2014. No 3 (240). P.
  53-64</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development of network economy changes the institutional maintenance of
economic relations. On change to traditional channels of distribution virtual
networks of distribution of production come. In article features and mechanisms
of transformation of chains deliveries in e-commerce reveal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03421</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03421</id><created>2015-05-13</created><updated>2016-02-10</updated><authors><author><keyname>Mizrahi</keyname><forenames>Tal</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>Time4: Time for SDN</title><categories>cs.NI</categories><comments>This report is an extended version of &quot;Software Defined Networks:
  It's About Time&quot;, which was accepted to IEEE INFOCOM 2016. A preliminary
  version of this report was published in arXiv in May, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of Software Defined Networks (SDN), there is growing interest
in dynamic and centralized traffic engineering, where decisions about
forwarding paths are taken dynamically from a network-wide perspective.
Frequent path reconfiguration can significantly improve the network
performance, but should be handled with care, so as to minimize disruptions
that may occur during network updates.
  In this paper we introduce Time4, an approach that uses accurate time to
coordinate network updates. Time4 is a powerful tool in softwarized
environments, that can be used for various network update scenarios.
Specifically, we characterize a set of update scenarios called flow swaps, for
which Time4 is the optimal update approach, yielding less packet loss than
existing update approaches. We define the lossless flow allocation problem, and
formally show that in environments with frequent path allocation, scenarios
that require simultaneous changes at multiple network devices are inevitable.
  We present the design, implementation, and evaluation of a Time4-enabled
OpenFlow prototype. The prototype is publicly available as open source. Our
work includes an extension to the OpenFlow protocol that has been adopted by
the Open Networking Foundation (ONF), and is now included in OpenFlow 1.5. Our
experimental results show the significant advantages of Time4 compared to other
network update approaches, and demonstrate an SDN use case that is infeasible
without Time4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03424</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03424</id><created>2015-05-13</created><updated>2015-08-11</updated><authors><author><keyname>Barak</keyname><forenames>Boaz</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Raghavendra</keyname><forenames>Prasad</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Aravindan</forenames></author><author><keyname>Witmer</keyname><forenames>David</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>Beating the random assignment on constraint satisfaction problems of
  bounded degree</title><categories>cs.CC cs.DS</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for any odd $k$ and any instance of the Max-kXOR constraint
satisfaction problem, there is an efficient algorithm that finds an assignment
satisfying at least a $\frac{1}{2} + \Omega(1/\sqrt{D})$ fraction of
constraints, where $D$ is a bound on the number of constraints that each
variable occurs in. This improves both qualitatively and quantitatively on the
recent work of Farhi, Goldstone, and Gutmann (2014), which gave a
\emph{quantum} algorithm to find an assignment satisfying a $\frac{1}{2} +
\Omega(D^{-3/4})$ fraction of the equations.
  For arbitrary constraint satisfaction problems, we give a similar result for
&quot;triangle-free&quot; instances; i.e., an efficient algorithm that finds an
assignment satisfying at least a $\mu + \Omega(1/\sqrt{D})$ fraction of
constraints, where $\mu$ is the fraction that would be satisfied by a uniformly
random assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03437</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03437</id><created>2015-05-13</created><authors><author><keyname>Calafiore</keyname><forenames>Giuseppe</forenames></author><author><keyname>Carlone</keyname><forenames>Luca</forenames></author><author><keyname>Dellaert</keyname><forenames>Frank</forenames></author></authors><title>Pose Graph Optimization in the Complex Domain: Lagrangian Duality,
  Conditions For Zero Duality Gap, and Optimal Solutions</title><categories>cs.RO</categories><comments>53 pages</comments><msc-class>68W01, 68W40, 68W25, 49K30</msc-class><acm-class>I.2.9; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pose Graph Optimization (PGO) is the problem of estimating a set of poses
from pairwise relative measurements. PGO is a nonconvex problem, and currently
no known technique can guarantee the computation of an optimal solution. In
this paper, we show that Lagrangian duality allows computing a globally optimal
solution, under certain conditions that are satisfied in many practical cases.
Our first contribution is to frame the PGO problem in the complex domain. This
makes analysis easier and allows drawing connections with the recent literature
on unit gain graphs. Exploiting this connection we prove non-trival results
about the spectrum of the matrix underlying the problem. The second
contribution is to formulate and analyze the dual problem in the complex
domain. Our analysis shows that the duality gap is connected to the number of
eigenvalues of the penalized pose graph matrix, which arises from the solution
of the dual. We prove that if this matrix has a single eigenvalue in zero, then
(i) the duality gap is zero, (ii) the primal PGO problem has a unique solution,
and (iii) the primal solution can be computed by scaling an eigenvector of the
penalized pose graph matrix. The third contribution is algorithmic: we exploit
the dual problem and propose an algorithm that computes a guaranteed optimal
solution for PGO when the penalized pose graph matrix satisfies the Single Zero
Eigenvalue Property (SZEP). We also propose a variant that deals with the case
in which the SZEP is not satisfied. The fourth contribution is a numerical
analysis. Empirical evidence shows that in the vast majority of cases (100% of
the tests under noise regimes of practical robotics applications) the penalized
pose graph matrix does satisfy the SZEP, hence our approach allows computing
the global optimal solution. Finally, we report simple counterexamples in which
the duality gap is nonzero, and discuss open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03445</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03445</id><created>2015-05-13</created><authors><author><keyname>Tan</keyname><forenames>Guangning</forenames></author><author><keyname>Nedialkov</keyname><forenames>Ned S.</forenames></author><author><keyname>Pryce</keyname><forenames>John D.</forenames></author></authors><title>Symbolic-numeric methods for improving structural analysis of
  differential-algebraic equation systems</title><categories>cs.SC cs.NA</categories><comments>technical report, 84 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems of differential-algebraic equations (DAEs) are generated routinely by
simulation and modeling environments such as Modelica and MapleSim. Before a
simulation starts and a numerical solution method is applied, some kind of
structural analysis is performed to determine the structure and the index of a
DAE. Structural analysis methods serve as a necessary preprocessing stage, and
among them, Pantelides's algorithm is widely used.
  Recently Pryce's $\Sigma$-method is becoming increasingly popular, owing to
its straightforward approach and capability of analyzing high-order systems.
Both methods are equivalent in the sense that when one succeeds, producing a
nonsingular system Jacobian, the other also succeeds, and the two give the same
structural index.
  Although provably successful on fairly many problems of interest, the
structural analysis methods can fail on some simple, solvable DAEs and give
incorrect structural information including the index. In this report, we focus
on the $\Sigma$-method. We investigate its failures, and develop two
symbolic-numeric conversion methods for converting a DAE, on which the
$\Sigma$-method fails, to an equivalent problem on which this method succeeds.
Aimed at making structural analysis methods more reliable, our conversion
methods exploit structural information of a DAE, and require a symbolic tool
for their implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03446</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03446</id><created>2015-05-13</created><authors><author><keyname>Vasisht</keyname><forenames>Deepak</forenames></author><author><keyname>Kumar</keyname><forenames>Swarun</forenames></author><author><keyname>Katabi</keyname><forenames>Dina</forenames></author></authors><title>Sub-Nanosecond Time of Flight on Commercial Wi-Fi Cards</title><categories>cs.NI cs.ET</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-of-flight, i.e., the time incurred by a signal to travel from
transmitter to receiver, is perhaps the most intuitive way to measure distances
using wireless signals. It is used in major positioning systems such as GPS,
RADAR, and SONAR. However, attempts at using time-of-flight for indoor
localization have failed to deliver acceptable accuracy due to fundamental
limitations in measuring time on Wi-Fi and other RF consumer technologies.
While the research community has developed alternatives for RF-based indoor
localization that do not require time-of-flight, those approaches have their
own limitations that hamper their use in practice. In particular, many existing
approaches need receivers with large antenna arrays while commercial Wi-Fi
nodes have two or three antennas. Other systems require fingerprinting the
environment to create signal maps. More fundamentally, none of these methods
support indoor positioning between a pair of Wi-Fi devices
without~third~party~support.
  In this paper, we present a set of algorithms that measure the time-of-flight
to sub-nanosecond accuracy on commercial Wi-Fi cards. We implement these
algorithms and demonstrate a system that achieves accurate device-to-device
localization, i.e. enables a pair of Wi-Fi devices to locate each other without
any support from the infrastructure, not even the location of the access
points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03447</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03447</id><created>2015-05-13</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Van</keyname><forenames>Khuong Ho-</forenames></author><author><keyname>Anh</keyname><forenames>Tuan Dang</forenames></author><author><keyname>Quoc</keyname><forenames>Hung Dinh</forenames></author></authors><title>Useful Results for Computing the Nuttall${-}Q$ and Incomplete Toronto
  Special Functions</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to the derivation of novel analytic results for special
functions which are particularly useful in wireless communication theory.
Capitalizing on recently reported series representations for the Nuttall
$Q{-}$function and the incomplete Toronto function, we derive closed-form upper
bounds for the corresponding truncation error of these series as well as
closed-form upper bounds that under certain cases become accurate
approximations. The derived expressions are tight and their algebraic
representation is rather convenient to handle analytically and numerically.
Given that the Nuttall${-}Q$ and incomplete Toronto functions are not built-in
in popular mathematical software packages, the proposed results are
particularly useful in computing these functions when employed in applications
relating to natural sciences and engineering, such as wireless communication
over fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03449</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03449</id><created>2015-05-13</created><authors><author><keyname>Singla</keyname><forenames>Ankit</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>Balakrishnan</forenames></author><author><keyname>Godfrey</keyname><forenames>P. Brighten</forenames></author><author><keyname>Maggs</keyname><forenames>Bruce</forenames></author></authors><title>Towards a Speed of Light Internet</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In principle, a network can transfer data at nearly the speed of light.
Today's Internet, however, is much slower: our measurements show that latencies
are typically more than one, and often more than two orders of magnitude larger
than the lower bound implied by the speed of light. Closing this gap would not
only add value to today's Internet applications, but might also open the door
to exciting new applications. Thus, we propose a grand challenge for the
networking research community: building a speed-of-light Internet. Towards
addressing this goal, we begin by investigating the causes of latency inflation
in the Internet across the network stack. Our analysis reveals that while
protocol overheads, which have dominated the community's attention, are indeed
important, infrastructural inefficiencies are a significant and under-explored
problem. Thus, we propose a radical, yet surprisingly low-cost approach to
mitigating latency inflation at the lowest layers and building a nearly
speed-of-light Internet infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03460</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03460</id><created>2015-05-13</created><authors><author><keyname>Flint</keyname><forenames>Ian</forenames></author><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Privault</keyname><forenames>Nicolas</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author></authors><title>Performance Analysis of Ambient RF Energy Harvesting with Repulsive
  Point Process Modeling</title><categories>cs.NI</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ambient RF (Radio Frequency) energy harvesting technique has recently been
proposed as a potential solution to provide proactive energy replenishment for
wireless devices. This paper aims to analyze the performance of a battery-free
wireless sensor powered by ambient RF energy harvesting using a stochastic
geometry approach. Specifically, we consider the point-to-point uplink
transmission of a wireless sensor in a stochastic geometry network, where
ambient RF sources, such as mobile transmit devices, access points and base
stations, are distributed as a Ginibre alpha-determinantal point process (DPP).
The DPP is able to capture repulsion among points, and hence, it is more
general than the Poisson point process (PPP). We analyze two common receiver
architectures: separated receiver and time-switching architectures. For each
architecture, we consider the scenarios with and without co-channel
interference for information transmission. We derive the expectation of the RF
energy harvesting rate in closed form and also compute its variance. Moreover,
we perform a worst-case study which derives the upper bound of both power and
transmission outage probabilities. Additionally, we provide guidelines on the
setting of optimal time-switching coefficient in the case of the time-switching
architecture. Numerical results verify the correctness of the analysis and show
various tradeoffs between parameter setting. Lastly, we prove that the sensor
is more efficient when the distribution of the ambient sources exhibits
stronger repulsion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03463</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03463</id><created>2015-05-13</created><authors><author><keyname>Perrault</keyname><forenames>Andrew</forenames></author><author><keyname>Drummond</keyname><forenames>Joanna</forenames></author><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author></authors><title>Exploring Strategy-Proofness, Uniqueness, and Pareto Optimality for the
  Stable Matching Problem with Couples</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Stable Matching Problem with Couples (SMP-C) is a ubiquitous real-world
extension of the stable matching problem (SMP) involving complementarities.
Although SMP can be solved in polynomial time, SMP-C is NP-Complete. Hence, it
is not clear which, if any, of the theoretical results surrounding the
canonical SMP problem apply in this setting. In this paper, we use a
recently-developed SAT encoding to solve SMP-C exactly. This allows us to
enumerate all stable matchings for any given instance of SMP-C. With this tool,
we empirically evaluate some of the properties that have been hypothesized to
hold for SMP-C.
  We take particular interest in investigating if, as the size of the market
grows, the percentage of instances with unique stable matchings also grows.
While we did not find this trend among the random problem instances we sampled,
we did find that the percentage of instances with an resident optimal matching
seems to more closely follow the trends predicted by previous conjectures. We
also define and investigate resident Pareto optimal stable matchings, finding
that, even though this is important desideratum for the deferred acceptance
style algorithms previously designed to solve SMP-C, they do not always find
one.
  We also investigate strategy-proofness for SMP-C, showing that even if only
one stable matching exists, residents still have incentive to misreport their
preferences. However, if a problem has a resident optimal stable matching, we
show that residents cannot manipulate via truncation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03469</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03469</id><created>2015-05-13</created><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames></author><author><keyname>Guerraoui</keyname><forenames>Rachid</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Petit</keyname><forenames>Franck</forenames></author><author><keyname>Sens</keyname><forenames>Pierre</forenames></author></authors><title>The Weakest Failure Detector for Eventual Consistency</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In its classical form, a consistent replicated service requires all replicas
to witness the same evolution of the service state. Assuming a message-passing
environment with a majority of correct processes, the necessary and sufficient
information about failures for implementing a general state machine replication
scheme ensuring consistency is captured by the {\Omega} failure detector. This
paper shows that in such a message-passing environment, {\Omega} is also the
weakest failure detector to implement an eventually consistent replicated
service, where replicas are expected to agree on the evolution of the service
state only after some (a priori unknown) time. In fact, we show that {\Omega}
is the weakest to implement eventual consistency in any message-passing
environment, i.e., under any assumption on when and where failures might occur.
Ensuring (strong) consistency in any environment requires, in addition to
{\Omega}, the quorum failure detector {\Sigma}. Our paper thus captures, for
the first time, an exact computational difference be- tween building a
replicated state machine that ensures consistency and one that only ensures
eventual consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03474</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03474</id><created>2015-05-13</created><authors><author><keyname>Caron</keyname><forenames>Pascal</forenames></author><author><keyname>Luque</keyname><forenames>Jean-Gabriel</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author><author><keyname>Patrou</keyname><forenames>Bruno</forenames></author></authors><title>State complexity of catenation combined with a boolean operation: a
  unified approach</title><categories>cs.FL math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the state complexity of catenation combined with
symmetric difference. First, an upper bound is computed using some combinatoric
tools. Then, this bound is shown to be tight by giving a witness for it.
Moreover, we relate this work with the study of state complexity for two other
combinations: catenation with union and catenation with intersection. And we
extract a unified approach which allows to obtain the state complexity of any
combination involving catenation and a binary boolean operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03476</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03476</id><created>2015-05-13</created><authors><author><keyname>Cui</keyname><forenames>Zehan</forenames></author><author><keyname>Lu</keyname><forenames>Tianyue</forenames></author><author><keyname>Pan</keyname><forenames>Haiyang</forenames></author><author><keyname>Mckee</keyname><forenames>Sally A.</forenames></author><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author></authors><title>Twin-Load: Building a Scalable Memory System over the Non-Scalable
  Interface</title><categories>cs.AR</categories><comments>submitted to PACT15</comments><acm-class>B.3; C.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Commodity memory interfaces have difficulty in scaling memory capacity to
meet the needs of modern multicore and big data systems. DRAM device density
and maximum device count are constrained by technology, package, and signal in-
tegrity issues that limit total memory capacity. Synchronous DRAM protocols
require data to be returned within a fixed latency, and thus memory extension
methods over commodity DDRx interfaces fail to support scalable topologies.
Current extension approaches either use slow PCIe interfaces, or require
expensive changes to the memory interface, which limits commercial
adoptability. Here we propose twin-load, a lightweight asynchronous memory
access mechanism over the synchronous DDRx interface. Twin-load uses two
special loads to accomplish one access request to extended memory, the first
serves as a prefetch command to the DRAM system, and the second asynchronously
gets the required data. Twin-load requires no hardware changes on the processor
side and only slight soft- ware modifications. We emulate this system on a
prototype to demonstrate the feasibility of our approach. Twin-load has
comparable performance to NUMA extended memory and outperforms a page-swapping
PCIe-based system by several orders of magnitude. Twin-load thus enables
instant capacity increases on commodity platforms, but more importantly, our
architecture opens opportunities for the design of novel, efficient, scalable,
cost-effective memory subsystems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03489</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03489</id><created>2015-05-13</created><authors><author><keyname>Boyat</keyname><forenames>Ajay Kumar</forenames><affiliation>Research Scholar, Department of Electronics Telecomm and Computer Engineering, Military College of Tele Communication Engineering, Military Head Quartar of War</affiliation></author><author><keyname>Joshi</keyname><forenames>Brijendra Kumar</forenames><affiliation>Professor, Department of Electronics Telecomm and Computer Engineering, Military College of Tele Communication Engineering, Military Head Quartar of War</affiliation></author></authors><title>A Review Paper: Noise Models in Digital Image Processing</title><categories>cs.CV</categories><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.6, No.2, April 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noise is always presents in digital images during image acquisition, coding,
transmission, and processing steps. Noise is very difficult to remove it from
the digital images without the prior knowledge of noise model. That is why,
review of noise models are essential in the study of image denoising
techniques. In this paper, we express a brief overview of various noise models.
These noise models can be selected by analysis of their origin. In this way, we
present a complete and quantitative analysis of noise models available in
digital images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03491</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03491</id><created>2015-05-13</created><updated>2015-08-27</updated><authors><author><keyname>Kumar</keyname><forenames>Siddhartha</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Andriyanova</keyname><forenames>Iryna</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author></authors><title>A Family of Erasure Correcting Codes with Low Repair Bandwidth and Low
  Repair Complexity</title><categories>cs.IT math.IT</categories><comments>Accepted, will appear in the proceedings of Globecom 2015 (Selected
  Areas in Communications: Data Storage)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the construction of a new family of erasure correcting codes for
distributed storage that yield low repair bandwidth and low repair complexity.
The construction is based on two classes of parity symbols. The primary goal of
the first class of symbols is to provide good erasure correcting capability,
while the second class facilitates node repair, reducing the repair bandwidth
and the repair complexity. We compare the proposed codes with other codes
proposed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03493</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03493</id><created>2015-05-13</created><updated>2015-05-14</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Modified Hausdorff Fractal Dimension (MHFD)</title><categories>cs.CV</categories><comments>15 pages, 4 figures, 2 algorithms. Working Paper WP-RFM-15-02,
  (version: 150507)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hausdorff fractal dimension has been a fast-to-calculate method to
estimate complexity of fractal shapes. In this work, a modified version of this
fractal dimension is presented in order to make it more robust when applied in
estimating complexity of non-fractal images. The modified Hausdorff fractal
dimension stands on two features that weaken the requirement of presence of a
shape and also reduce the impact of the noise possibly presented in the input
image. The new algorithm has been evaluated on a set of images of different
character with promising performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03504</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03504</id><created>2015-05-13</created><updated>2016-02-16</updated><authors><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author><author><keyname>Zheng</keyname><forenames>Wei-Mou</forenames></author></authors><title>Loop-corrected belief propagation for lattice spin models</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CV</categories><comments>11 pages, minor changes with new references added. Final version as
  published in EPJB</comments><journal-ref>European Physical Journal B 88: 336 (2015)</journal-ref><doi>10.1140/epjb/e2015-60485-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief propagation (BP) is a message-passing method for solving probabilistic
graphical models. It is very successful in treating disordered models (such as
spin glasses) on random graphs. On the other hand, finite-dimensional lattice
models have an abundant number of short loops, and the BP method is still far
from being satisfactory in treating the complicated loop-induced correlations
in these systems. Here we propose a loop-corrected BP method to take into
account the effect of short loops in lattice spin models. We demonstrate,
through an application to the square-lattice Ising model, that loop-corrected
BP improves over the naive BP method significantly. We also implement
loop-corrected BP at the coarse-grained region graph level to further boost its
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03505</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03505</id><created>2015-05-13</created><authors><author><keyname>Patrone</keyname><forenames>Aniello Raffale</forenames></author><author><keyname>Scherzer</keyname><forenames>Otmar</forenames></author></authors><title>On a spatial-temporal decomposition of the optical flow</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the first variational spatial-temporal decomposition
algorithm for computation of the optical flow of a dynamic sequence. We
consider several applications, such as the extraction of temporal motion
patterns of different scales and motion detection in dynamic sequences under
varying illumination conditions, such as they appear for instance in
psychological flickering experiments. In order to take into account variable
illumination conditions we review the derivation, and modify, the optical flow
equation. Concerning the numerical implementation, we propose a relaxation
approach for the adapted model such that the resulting optimality condition is
an integro-differential equation, which is numerically solved by a fixed point
iteration. For comparison purposes we use the standard time dependent optical
flow algorithm from Weickert-Schn\&quot;orr, which in contrast to our method,
constitutes in solving a spatial-temporal differential equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03509</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03509</id><created>2015-05-13</created><authors><author><keyname>Di Luna</keyname><forenames>Giuseppe Antonio</forenames></author><author><keyname>Baldoni</keyname><forenames>Roberto</forenames></author></authors><title>Investigating the Cost of Anonymity on Dynamic Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the difficulty of counting nodes in a synchronous
dynamic network where nodes share the same identifier, they communicate by
using a broadcast with unlimited bandwidth and, at each synchronous round,
network topology may change. To count in such setting, it has been shown that
the presence of a leader is necessary. We focus on a particularly interesting
subset of dynamic networks, namely \textit{Persistent Distance} - ${\cal
G}($PD$)_{h}$, in which each node has a fixed distance from the leader across
rounds and such distance is at most $h$. In these networks the dynamic diameter
$D$ is at most $2h$. We prove the number of rounds for counting in ${\cal
G}($PD$)_{2}$ is at least logarithmic with respect to the network size $|V|$.
Thanks to this result, we show that counting on any dynamic anonymous network
with $D$ constant w.r.t. $|V|$ takes at least $D+ \Omega(\text{log}\, |V| )$
rounds where $\Omega(\text{log}\, |V|)$ represents the additional cost to be
payed for handling anonymity. At the best of our knowledge this is the fist non
trivial, i.e. different from $\Omega(D)$, lower bounds on counting in anonymous
interval connected networks with broadcast and unlimited bandwith.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03532</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03532</id><created>2015-05-13</created><authors><author><keyname>Wu</keyname><forenames>Lingfei</forenames></author><author><keyname>Wu</keyname><forenames>Kesheng</forenames></author><author><keyname>Sim</keyname><forenames>Alex</forenames></author><author><keyname>Churchill</keyname><forenames>Michael</forenames></author><author><keyname>Choi</keyname><forenames>Jong Y.</forenames></author><author><keyname>Stathopoulos</keyname><forenames>Andreas</forenames></author><author><keyname>Chang</keyname><forenames>Cs</forenames></author><author><keyname>Klasky</keyname><forenames>Scott</forenames></author></authors><title>Towards Real-Time Detection and Tracking of Blob-Filaments in Fusion
  Plasma Big Data</title><categories>cs.DC cs.CE cs.DS physics.plasm-ph</categories><comments>14 pages, 40 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic fusion could provide an inexhaustible, clean, and safe solution to
the global energy needs. The success of magnetically-confined fusion reactors
demands steady-state plasma confinement which is challenged by the
blob-filaments driven by the edge turbulence. Real-time analysis can be used to
monitor the progress of fusion experiments and prevent catastrophic events.
However, terabytes of data are generated over short time periods in fusion
experiments. Timely access to and analyzing this amount of data demands
properly responding to extreme scale computing and big data challenges. In this
paper, we apply outlier detection techniques to effectively tackle the fusion
blob detection problem on extremely large parallel machines. We present a
real-time region outlier detection algorithm to efficiently find blobs in
fusion experiments and simulations. In addition, we propose an efficient scheme
to track the movement of region outliers over time. We have implemented our
algorithms with hybrid MPI/OpenMP and demonstrated the accuracy and efficiency
of the proposed blob detection and tracking methods with a set of data from the
XGC1 fusion simulation code. Our tests illustrate that we can achieve linear
time speedup and complete blob detection in two or three milliseconds using
Edison, a Cray XC30 system at NERSC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03537</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03537</id><created>2015-05-13</created><authors><author><keyname>Habibulla</keyname><forenames>Yusupjan</forenames></author><author><keyname>Zhao</keyname><forenames>Jin-Hua</forenames></author><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author></authors><title>The Directed Dominating Set Problem: Generalized Leaf Removal and Belief
  Propagation</title><categories>physics.soc-ph cond-mat.dis-nn cs.DM cs.DS</categories><comments>11 pages, 3 figures in EPS format</comments><journal-ref>Lecture Notes in Computer Science 9130, 78--88 (2015)</journal-ref><doi>10.1007/978-3-319-19647-3_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A minimum dominating set for a digraph (directed graph) is a smallest set of
vertices such that each vertex either belongs to this set or has at least one
parent vertex in this set. We solve this hard combinatorial optimization
problem approximately by a local algorithm of generalized leaf removal and by a
message-passing algorithm of belief propagation. These algorithms can construct
near-optimal dominating sets or even exact minimum dominating sets for random
digraphs and also for real-world digraph instances. We further develop a core
percolation theory and a replica-symmetric spin glass theory for this problem.
Our algorithmic and theoretical results may facilitate applications of
dominating sets to various network problems involving directed interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03540</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03540</id><created>2015-05-13</created><updated>2015-10-05</updated><authors><author><keyname>Havaei</keyname><forenames>Mohammad</forenames></author><author><keyname>Davy</keyname><forenames>Axel</forenames></author><author><keyname>Warde-Farley</keyname><forenames>David</forenames></author><author><keyname>Biard</keyname><forenames>Antoine</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Pal</keyname><forenames>Chris</forenames></author><author><keyname>Jodoin</keyname><forenames>Pierre-Marc</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author></authors><title>Brain Tumor Segmentation with Deep Neural Networks</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a fully automatic brain tumor segmentation method
based on Deep Neural Networks (DNNs). The proposed networks are tailored to
glioblastomas (both low and high grade) pictured in MR images. By their very
nature, these tumors can appear anywhere in the brain and have almost any kind
of shape, size, and contrast. These reasons motivate our exploration of a
machine learning solution that exploits a flexible, high capacity DNN while
being extremely efficient. Here, we give a description of different model
choices that we've found to be necessary for obtaining competitive performance.
We explore in particular different architectures based on Convolutional Neural
Networks (CNN), i.e. DNNs specifically adapted to image data.
  We present a novel CNN architecture which differs from those traditionally
used in computer vision. Our CNN exploits both local features as well as more
global contextual features simultaneously. Also, different from most
traditional uses of CNNs, our networks use a final layer that is a
convolutional implementation of a fully connected layer which allows a 40 fold
speed up. We also describe a 2-phase training procedure that allows us to
tackle difficulties related to the imbalance of tumor labels. Finally, we
explore a cascade architecture in which the output of a basic CNN is treated as
an additional source of information for a subsequent CNN. Results reported on
the 2013 BRATS test dataset reveal that our architecture improves over the
currently published state-of-the-art while being over 30 times faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03555</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03555</id><created>2015-05-13</created><authors><author><keyname>Reiffers-Masson</keyname><forenames>Alexandre</forenames></author><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Pricing Agreement between Service and Content Providers: A Net
  Neutrality Issue</title><categories>cs.CY cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Net Neutrality issue has been at the center of debate worldwide lately.
Some countries have established laws so that principles of Net Neutrality are
respected, the Netherlands being the latest country in Europe. Among the
questions that have been discussed in these debates are whether to allow
agreements between service and content providers, i.e. to allow some
preferential treatment by an operator to traffic from some subset of providers.
Our goal in this paper is to analyze the impact of non neutral pricing and
agreements on the Internet users and on the content providers. Each one of
several Internet users have to decide in which way to split their demand among
several content providers. The cost for an Internet user depends on whether the
content providers have an agreement with the Internet Service Provider in which
the Internet user is connected to. In addition, the requests coming from users
depend on the preference of the consumer in the different CP. We model the
choice of how to split the demands and the pricing aspects faced by the content
providers as a hierarchical game model composed of a congestion game at the
lower level and a noncooperative pricing game at the upper level. We show that
agreement between providers have a positive impact on the equilibrium
performance of the Internet users. We further show that at equilibrium, the
content provider on the contrary, does not benefit from the agreement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03561</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03561</id><created>2015-05-13</created><updated>2015-06-03</updated><authors><author><keyname>Song</keyname><forenames>Linqi</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>Content-type coding</title><categories>cs.IT math.IT</categories><comments>Netcod</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is motivated by the observation that, in many cases, we do not
need to serve specific messages, but rather, any message within a content-type.
Content-type traffic pervades a host of applications today, ranging from search
engines and recommender networks to newsfeeds and advertisement networks. The
paper asks a novel question: if there are benefits in designing network and
channel codes specifically tailored to content-type requests. It provides three
examples of content-type formulations to argue that, indeed in some cases we
can have significant such benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03564</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03564</id><created>2015-05-13</created><authors><author><keyname>Uteshev</keyname><forenames>Alexei Yu.</forenames></author></authors><title>Some Analytics for Steiner Minimal Tree Problem for Four Terminals</title><categories>cs.CG</categories><comments>19 pages, 7 figures</comments><msc-class>51E10, 51N20</msc-class><acm-class>F.2.2; G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the coordinates of four terminals in the Euclidean plane we present
explicit formulas for Steiner point coordinates for Steiner minimal tree
problem. We utilize the obtained formulas for evaluation of the influence of
terminal coordinates on the loci of Steiner points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03566</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03566</id><created>2015-05-13</created><updated>2016-01-28</updated><authors><author><keyname>Shakeri</keyname><forenames>Moein</forenames></author><author><keyname>Zhang</keyname><forenames>Hong</forenames></author></authors><title>COROLA: A Sequential Solution to Moving Object Detection Using Low-rank
  Approximation</title><categories>cs.CV cs.RO</categories><comments>37 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting moving objects from a video sequence and estimating the background
of each individual image are fundamental issues in many practical applications
such as visual surveillance, intelligent vehicle navigation, and traffic
monitoring. Recently, some methods have been proposed to detect moving objects
in a video via low-rank approximation and sparse outliers where the background
is modeled with the computed low-rank component of the video and the foreground
objects are detected as the sparse outliers in the low-rank approximation. All
of these existing methods work in a batch manner, preventing them from being
applied in real time and long duration tasks. In this paper, we present an
online sequential framework, namely contiguous outliers representation via
online low-rank approximation (COROLA), to detect moving objects and learn the
background model at the same time. We also show that our model can detect
moving objects with a moving camera. Our experimental evaluation uses simulated
data and real public datasets and demonstrates the superior performance of
COROLA in terms of both accuracy and execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03578</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03578</id><created>2015-05-13</created><authors><author><keyname>Borji</keyname><forenames>Ali</forenames></author><author><keyname>Feng</keyname><forenames>Mengyang</forenames></author><author><keyname>Lu</keyname><forenames>Huchuan</forenames></author></authors><title>Vanishing Point Attracts Eye Movements in Scene Free-viewing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eye movements are crucial in understanding complex scenes. By predicting
where humans look in natural scenes, we can understand how they percieve scenes
and priotriaze information for further high-level processing. Here, we study
the effect of a particular type of scene structural information known as
vanishing point and show that human gaze is attracted to vanishing point
regions. We then build a combined model of traditional saliency and vanishing
point channel that outperforms state of the art saliency models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03579</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03579</id><created>2015-05-13</created><updated>2016-01-06</updated><authors><author><keyname>Salsano</keyname><forenames>Stefano</forenames></author><author><keyname>Ventre</keyname><forenames>Pier Luigi</forenames></author><author><keyname>Lombardo</keyname><forenames>Francesco</forenames></author><author><keyname>Siracusano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Gerola</keyname><forenames>Matteo</forenames></author><author><keyname>Salvadori</keyname><forenames>Elio</forenames></author><author><keyname>Santuari</keyname><forenames>Michele</forenames></author><author><keyname>Campanella</keyname><forenames>Mauro</forenames></author><author><keyname>Prete</keyname><forenames>Luca</forenames></author></authors><title>Hybrid IP/SDN networking: open implementation and experiment management
  tools</title><categories>cs.NI</categories><comments>Accepted for publication in IEEE Transaction of Network and Service
  Management - December 2015 http://dx.doi.org/10.1109/TNSM.2015.2507622</comments><doi>10.1109/TNSM.2015.2507622</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of SDN in large-scale IP provider networks is still an open
issue and different solutions have been suggested so far. In this paper we
propose a hybrid approach that allows the coexistence of traditional IP routing
with SDN based forwarding within the same provider domain. The solution is
called OSHI - Open Source Hybrid IP/SDN networking as we have fully implemented
it combining and extending Open Source software. We discuss the OSHI system
architecture and the design and implementation of advanced services like Pseudo
Wires and Virtual Switches. In addition, we describe a set of Open Source
management tools for the emulation of the proposed solution using either the
Mininet emulator or distributed physical testbeds. We refer to this suite of
tools as Mantoo (Management tools). Mantoo includes an extensible web-based
graphical topology designer, which provides different layered network &quot;views&quot;
(e.g. from physical links to service relationships among nodes). The suite can
validate an input topology, automatically deploy it over a Mininet emulator or
a distributed SDN testbed and allows access to emulated nodes by opening
consoles in the web GUI. Mantoo provides also tools to evaluate the performance
of the deployed nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03580</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03580</id><created>2015-05-13</created><authors><author><keyname>Mota</keyname><forenames>Francisco</forenames></author></authors><title>Splitting Root-Locus Plot into Algebraic Plane Curves</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how to split the Root Locus plot for an irreducible
rational transfer function into several individual algebraic plane curves, like
lines, circles, conics, etc. To achieve this goal we use results of a previous
paper of the author to represent the Root Locus as an algebraic variety
generated by an ideal over a polynomial ring, and whose primary decomposion
allow us to isolate the planes curves that composes the Root Locus. As a
by-product, using the concept of duality in projective algebraic geometry, we
show how to obtain the dual curve of each plane curve that composes the Root
Locus and unite them to obtain what we denominate the &quot;Algebraic Dual Root
Locus&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03581</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03581</id><created>2015-05-13</created><authors><author><keyname>Borji</keyname><forenames>Ali</forenames></author><author><keyname>Itti</keyname><forenames>Laurent</forenames></author></authors><title>CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Saliency modeling has been an active research area in computer vision for
about two decades. Existing state of the art models perform very well in
predicting where people look in natural scenes. There is, however, the risk
that these models may have been overfitting themselves to available small scale
biased datasets, thus trapping the progress in a local minimum. To gain a
deeper insight regarding current issues in saliency modeling and to better
gauge progress, we recorded eye movements of 120 observers while they freely
viewed a large number of naturalistic and artificial images. Our stimuli
includes 4000 images; 200 from each of 20 categories covering different types
of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor,
Outdoor, Jumbled, Random, and Line drawings. We analyze some basic properties
of this dataset and compare some successful models. We believe that our dataset
opens new challenges for the next generation of saliency models and helps
conduct behavioral studies on bottom-up visual attention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03587</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03587</id><created>2015-05-13</created><authors><author><keyname>Alikhani</keyname><forenames>Malihe</forenames></author><author><keyname>Kjos-Hanssen</keyname><forenames>Bj&#xf8;rn</forenames></author><author><keyname>Pakravan</keyname><forenames>Amirarsalan</forenames></author><author><keyname>Saadat</keyname><forenames>Babak</forenames></author></authors><title>Pricing complexity options</title><categories>q-fin.PR cs.CC cs.FL math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider options that pays the complexity deficiency of a sequence of up
and down ticks of a stock upon exercise. We study the price of European and
American versions of this option numerically for automatic complexity, and
theoretically for Kolmogorov complexity. We also consider the case of run
complexity, which is a restricted form of automatic complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03597</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03597</id><created>2015-05-13</created><authors><author><keyname>Ohn-Bar</keyname><forenames>Eshed</forenames></author><author><keyname>Trivedi</keyname><forenames>M. M.</forenames></author></authors><title>Looking outside of the Box: Object Detection and Localization with
  Multi-scale Patterns</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection and localization of objects at multiple scales often involves
sliding a single scale template in order to score windows at different scales
independently. Nonetheless, multi-scale visual information at a given image
location is highly correlated. This fundamental insight allows us to generalize
the traditional multi-scale sliding window technique by jointly considering
image features at all scales in order to detect and localize objects. Two
max-margin approaches are studied for learning the multi-scale templates and
leveraging the highly structured multi-scale information which would have been
ignored if a single-scale template was used. The multi-scale formulation is
shown to significantly improve general detection performance (measured on the
PASCAL VOC dataset). The experimental analysis shows the method to be effective
with different visual features, both HOG and CNN. Surprisingly, for a given
window in a specific scale, visual information from windows at the same image
location but other scales (`out-of-scale' information) contains most of the
discriminative information for detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03615</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03615</id><created>2015-05-14</created><authors><author><keyname>Chuang</keyname><forenames>Ming</forenames></author><author><keyname>Kazhdan</keyname><forenames>Michael</forenames></author></authors><title>A Connectivity-Aware Multi-level Finite-Element System for Solving
  Laplace-Beltrami Equations</title><categories>cs.GR</categories><comments>This work was done when the first author was a PhD student at Johns
  Hopkins University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on octree-based finite-element systems has developed a multigrid
solver for Poisson equations on meshes. While the idea of defining a regularly
indexed function space has been successfully used in a number of applications,
it has also been noted that the richness of the function space is limited
because the function values can be coupled across locally disconnected regions.
In this work, we show how to enrich the function space by introducing functions
that resolve the coupling while still preserving the nesting hierarchy that
supports multigrid. A spectral analysis reveals the superior quality of the
resulting Laplace-Beltrami operator and applications to surface flow
demonstrate that our new solver more efficiently converges to the correct
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03635</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03635</id><created>2015-05-14</created><updated>2015-07-06</updated><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Faggian</keyname><forenames>Claudia</forenames></author><author><keyname>Valiron</keyname><forenames>Benoit</forenames></author><author><keyname>Yoshimizu</keyname><forenames>Akira</forenames></author></authors><title>Parallelism and Synchronization in an Infinitary Context (Long Version)</title><categories>cs.LO</categories><comments>40 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study multitoken interaction machines in the context of a very expressive
logical system with exponentials, fixpoints and synchronization. The advantage
of such machines is to provide models in the style of the Geometry of
Interaction, i.e., an interactive semantics which is close to low-level
implementation. On the one hand, we prove that despite the inherent complexity
of the framework, interaction is guaranteed to be deadlock free. On the other
hand, the resulting logical system is powerful enough to embed PCF and to
adequately model its behaviour, both when call-by-name and when call-by-value
evaluation are considered. This is not the case for single-token stateless
interactive machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03638</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03638</id><created>2015-05-14</created><authors><author><keyname>Crubill&#xe9;</keyname><forenames>Rapha&#xeb;lle</forenames></author><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author></authors><title>Metric Reasoning about $\lambda$-Terms: the Affine Case (Long Version)</title><categories>cs.LO</categories><comments>46 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Terms of Church's $\lambda$-calculus can be considered equivalent along many
different definitions, but context equivalence is certainly the most direct and
universally accepted one. If the underlying calculus becomes probabilistic,
however, equivalence is too discriminating: terms which have totally unrelated
behaviours are treated the same as terms which behave very similarly. We study
the problem of evaluating the distance between affine $\lambda$-terms. The most
natural definition for it, namely a natural generalisation of context
equivalence, is shown to be characterised by a notion of trace distance, and to
be bounded from above by a coinductively defined distance based on the
Kantorovich metric on distributions. A different, again fully-abstract,
tuple-based notion of trace distance is shown to be able to handle nontrivial
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03640</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03640</id><created>2015-05-14</created><authors><author><keyname>Mousavi</keyname><forenames>Seyed Hamed</forenames></author><author><keyname>Haghighat</keyname><forenames>Javad</forenames></author><author><keyname>Hamouda</keyname><forenames>Walaa</forenames></author><author><keyname>Dastbasteh</keyname><forenames>Reza</forenames></author></authors><title>A Subset Selection Algorithm for Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>21 Pages, 7 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges facing wireless sensor networks (WSNs) is the
limited power resources available at small sensor nodes. It is therefore
desired to reduce the power consumption of sensors while keeping the distortion
between the source information and its estimate at the fusion centre (FC) below
a specific threshold. In this paper, given the channel state information at the
FC, we propose a subset selection algorithm of sensor nodes to reduce the
average transmission power of the WSN. We assume the channels between the
source and the sensors to be correlated fading channels, modeled by the
Gilbert-Elliott model. We show that when these channels are known at the FC, a
subset of sensors can be selected by the FC such that the received observations
from this subset is sufficient to estimate the source information at the FC
while maintaining the distortion between source information and its estimate
below a specific threshold. Through analyses, we find the probability
distribution of the size of this subset and provide results to evaluate the
power efficiency of our proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03653</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03653</id><created>2015-05-14</created><authors><author><keyname>Mizrahi</keyname><forenames>Tal</forenames></author><author><keyname>Saat</keyname><forenames>Efi</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>Timed Consistent Network Updates</title><categories>cs.NI</categories><comments>This technical report is an extended version of the paper &quot;Timed
  Consistent Network Updates&quot;, which was accepted to the ACM SIGCOMM Symposium
  on SDN Research (SOSR) '15, Santa Clara, CA, US, June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network updates such as policy and routing changes occur frequently in
Software Defined Networks (SDN). Updates should be performed consistently,
preventing temporary disruptions, and should require as little overhead as
possible. Scalability is increasingly becoming an essential requirement in SDN.
In this paper we propose to use time-triggered network updates to achieve
consistent updates. Our proposed solution requires lower overhead than existing
update approaches, without compromising the consistency during the update. We
demonstrate that accurate time enables far more scalable consistent updates in
SDN than previously available. In addition, it provides the SDN programmer with
fine-grained control over the tradeoff between consistency and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03654</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03654</id><created>2015-05-14</created><updated>2015-11-29</updated><authors><author><keyname>Sonoda</keyname><forenames>Sho</forenames></author><author><keyname>Murata</keyname><forenames>Noboru</forenames></author></authors><title>Neural Network with Unbounded Activation Functions is Universal
  Approximator</title><categories>cs.NE cs.LG math.FA</categories><comments>under review; first revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an investigation of the approximation property of neural
networks with unbounded activation functions, such as the rectified linear unit
(ReLU), which is the new de-facto standard of deep learning. The ReLU network
can be analyzed by the ridgelet transform with respect to Lizorkin
distributions. By showing three reconstruction formulas by using the Fourier
slice theorem, the Radon transform, and Parseval's relation, it is shown that a
neural network with unbounded activation functions still satisfies the
universal approximation property. As an additional consequence, the ridgelet
transform, or the backprojection filter in the Radon domain, is what the
network learns after backpropagation. Subject to a constructive admissibility
condition, the trained network can be obtained by simply discretizing the
ridgelet transform, without backpropagation. Numerical examples not only
support the consistency of the admissibility condition but also imply that some
non-admissible cases result in low-pass filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03662</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03662</id><created>2015-05-14</created><updated>2015-08-06</updated><authors><author><keyname>Dias</keyname><forenames>Gabriel Martins</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Oechsner</keyname><forenames>Simon</forenames></author></authors><title>Predicting Occupancy Trends in Barcelona's Bicycle Service Stations
  Using Open Data</title><categories>cs.AI cs.CY</categories><comments>7 pages, 7 figures, 1 table, accepted to SAI Intelligent Systems
  Conference 2015</comments><msc-class>68-06</msc-class><acm-class>I.2.M</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2008, the CEO of the company that manages and maintains the public bicycle
service in Barcelona recognized that one may not expect to always find a place
to leave the rented bike nearby their destination, similarly to the case when,
driving a car, people may not find a parking lot. In this work, we make
predictions about the statuses of the stations of the public bicycle service in
Barcelona. We show that it is feasible to correctly predict nearly half of the
times when the stations are either completely full of bikes or completely
empty, up to 2 days before they actually happen. That is, users might avoid
stations at times when they could not return a bicycle that they have rented
before, or when they would not find a bike to rent. To achieve that, we apply
the Random Forest algorithm to classify the status of the stations and improve
the lifetime of the models using publicly available data, such as information
about the weather forecast. Finally, we expect that the results of the
predictions can be used to improve the quality of the service and make it more
reliable for the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03671</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03671</id><created>2015-05-14</created><authors><author><keyname>Stock</keyname><forenames>Wolfgang G.</forenames></author></authors><title>Informetric Analyses of Knowledge Organization Systems (KOSs)</title><categories>cs.DL</categories><comments>20 pp in C. R. Sugimoto (Ed.), Theories of Informetrics and Scholarly
  Communication. Berlin, Germany: De Gruyter, 2015</comments><msc-class>68T30</msc-class><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A knowledge organization system (KOS) is made up of concepts and semantic
relations between the concepts which represent a knowledge domain
terminologically. We distinguish between five approaches to KOSs:
nomenclatures, classification systems, thesauri, ontologies and, as a
borderline case of KOSs, folksonomies. The research question of this paper is:
How can we informetrically analyze the effectiveness of KOSs? Quantitative
informetric measures and indicators allow for the description, for comparative
analyses as well as for evaluation of KOSs and their quality. We describe the
state of the art of KOS evaluation. Most of the evaluation studies found in the
literature are about ontologies. We introduce measures of the structure of KOSs
(e.g., groundedness, tangledness, fan-out factor, or granularity) and
indicators of KOS quality (completeness, consistency, overlap, and use).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03681</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03681</id><created>2015-05-14</created><authors><author><keyname>Gottlieb</keyname><forenames>Lee-Ad</forenames></author></authors><title>A light metric spanner</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has long been known that $d$-dimensional Euclidean point sets admit
$(1+\epsilon)$-stretch spanners with lightness $W_E = \epsilon^{-O(d)}$, that
is total edge weight at most $W_E$ times the weight of the minimum spaning tree
of the set [DHN93]. Whether or not a similar result holds for metric spaces
with low doubling dimension has remained an important open problem, and has
resisted numerous attempts at resolution. In this paper, we resolve the
question in the affirmative, and show that doubling spaces admit
$(1+\epsilon)$-stretch spanners with lightness $W_D =
(ddim/\epsilon)^{O(ddim)}$.
  Important in its own right, our result also implies a much faster
polynomial-time approximation scheme for the traveling salesman problemin
doubling metric spaces, improving upon the bound presented in [BGK-12].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03682</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03682</id><created>2015-05-14</created><updated>2015-05-26</updated><authors><author><keyname>Li</keyname><forenames>Xueru</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author></authors><title>Massive MIMO with Multi-cell MMSE Processing: Exploiting All Pilots for
  Interference Suppression</title><categories>cs.IT math.IT</categories><comments>31 pages, 9 figures, submitted to IEEE Trans. Wireless Commun</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new state-of-the-art multi-cell MMSE scheme is proposed for
massive MIMO networks, which includes an uplink MMSE detector and a downlink
MMSE precoder. The main novelty is that it exploits all available pilots for
interference suppression. Specifically, let $K$ and $B$ denote the number of
users per cell and the number of orthogonal pilot sequences in the network,
respectively, where $\beta = B/K$ is the pilot reuse factor. Then our
multi-cell MMSE scheme utilizes all $B$ channel directions, that can be
estimated locally at each base station, to actively suppress both intra-cell
and inter-cell interference. The proposed scheme is particularly practical and
general, since power control for the pilot and payload, imperfect channel
estimation and arbitrary pilot allocation are all accounted for. Simulations
show that significant spectral efficiency (SE) gains are obtained over the
single-cell MMSE scheme and the multi-cell ZF, particularly for large $\beta$
and/or $K$. Furthermore, large-scale approximations of the uplink and downlink
SINRs are derived, which are asymptotically tight in the large-system limit.
The approximations are easy to compute and very accurate even for small system
dimensions. Using these SINR approximations, a low-complexity power control
algorithm is also proposed to maximize the sum SE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03689</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03689</id><created>2015-05-14</created><authors><author><keyname>Zanin</keyname><forenames>Massimiliano</forenames></author></authors><title>On alternative formulations of the small-world metric in complex
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small-world networks, i.e. networks displaying both a high clustering
coefficient and a small characteristic path length, are obliquitous in nature.
Since their identification, the &quot;small-worldness&quot; metric, as proposed by
Humphries and Gurney, has frequently been used to detect such structural
property in real-world complex networks, to a large extent in the study of
brain dynamics. Here I discuss several of its drawbacks, including its lack of
definition in disconnected networks and the impossibility of assessing a
statistical significance; and present different alternative formulations to
overcome these difficulties, validated through the phenospaces representing a
set of 48 real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03700</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03700</id><created>2015-05-14</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Fikadu</keyname><forenames>Mulugeta K.</forenames></author><author><keyname>Ho-Van</keyname><forenames>Khuong</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Sensing of Unknown Signals over Weibull Fading Conditions</title><categories>cs.IT math.IT</categories><comments>16 pages. arXiv admin note: text overlap with arXiv:1505.03331</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy detection is a widely used method of spectrum sensing in cognitive
radio and Radio Detection And Ranging (RADAR) systems. This paper is devoted to
the analytical evaluation of the performance of an energy detector over Weibull
fading channels. This is a flexible fading model that has been shown capable of
providing accurate characterization of multipath fading in, e.g., typical
cellular radio frequency range of 800${/}$900 MHz. A novel analytic expression
for the corresponding average probability of detection is derived in a simple
algebraic representation which renders it convenient to handle both
analytically and numerically. As expected, the performance of the detector is
highly dependent upon the severity of fading as even small variation of the
fading parameters affect significantly the value of the average probability of
detection. This appears to be particularly the case in severe fading
conditions. The offered results are useful in evaluating the effect of
multipath fading in energy detection-based cognitive radio communication
systems and therefore they can be used in quantifying the associated trade-offs
between sensing performance and energy efficiency in cognitive radio networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03703</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03703</id><created>2015-05-14</created><authors><author><keyname>Gan</keyname><forenames>Yanhai</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Dong</keyname><forenames>Junyu</forenames></author><author><keyname>Zhong</keyname><forenames>Guoqiang</forenames></author></authors><title>A PCA-Based Convolutional Network</title><categories>cs.LG cs.CV cs.NE</categories><comments>8 pages,5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel unsupervised deep learning model, called
PCA-based Convolutional Network (PCN). The architecture of PCN is composed of
several feature extraction stages and a nonlinear output stage. Particularly,
each feature extraction stage includes two layers: a convolutional layer and a
feature pooling layer. In the convolutional layer, the filter banks are simply
learned by PCA. In the nonlinear output stage, binary hashing is applied. For
the higher convolutional layers, the filter banks are learned from the feature
maps that were obtained in the previous stage. To test PCN, we conducted
extensive experiments on some challenging tasks, including handwritten digits
recognition, face recognition and texture classification. The results show that
PCN performs competitive with or even better than state-of-the-art deep
learning models. More importantly, since there is no back propagation for
supervised finetuning, PCN is much more efficient than existing deep networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03717</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03717</id><created>2015-05-14</created><authors><author><keyname>B&#xe9;rczi</keyname><forenames>Krist&#xf3;f</forenames></author><author><keyname>Bern&#xe1;th</keyname><forenames>Attila</forenames></author><author><keyname>Vizer</keyname><forenames>M&#xe1;t&#xe9;</forenames></author></authors><title>A note on $\mathtt{V}$-free $2$-matchings</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a conjecture of Liang [Y.-C. Liang. {\em Anti-magic labeling of
graphs}. PhD thesis, National Sun Yat-sen University, 2013.], we introduce a
restricted path packing problem in bipartite graphs that we call a
$\mathtt{V}$-free $2$-matching. We verify the conjecture through a weakening of
the hypergraph matching problem. We close the paper by showing that it is
NP-complete to decide whether one of the color classes of a bipartite graph can
be covered by a $\mathtt{V}$-free $2$-matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03718</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03718</id><created>2015-05-14</created><updated>2015-09-14</updated><authors><author><keyname>&#xc0;lvarez</keyname><forenames>Carme</forenames></author><author><keyname>Blesa</keyname><forenames>Maria</forenames></author><author><keyname>Duch</keyname><forenames>Amalia</forenames></author><author><keyname>Messegu&#xe9;</keyname><forenames>Arnau</forenames></author><author><keyname>Serna</keyname><forenames>Maria</forenames></author></authors><title>Stars and Celebrities: A Network Creation Game</title><categories>cs.GT</categories><comments>20 pages, 1 figure</comments><msc-class>91A06, 91A43</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Celebrity games, a new model of network creation games is introduced. The
specific features of this model are that players have different celebrity
weights and that a critical distance is taken into consideration. The aim of
any player is to be close (at distance less than critical) to the others,
mainly to those with high celebrity weights. The cost of each player depends on
the cost of establishing direct links to other players and on the sum of the
weights of those players at a distance greater than the critical distance. We
show that celebrity games always have pure Nash equilibria and we characterize
the family of subgames having connected Nash equilibria, the so called star
celebrity games. Exact bounds for the PoA of non star celebrity games and a
bound of $O(n/\beta + \beta)$ for star celebrity games are provided.
  The upper bound on the PoA can be tightened when restricted to particular
classes of Nash equilibria graphs. We show that the upper bound is $O(n/\beta)$
in the case of 2-edge-connected graphs and 2 in the case of trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03736</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03736</id><created>2015-05-14</created><authors><author><keyname>Chitchyan</keyname><forenames>Ruzanna</forenames></author><author><keyname>Noppen</keyname><forenames>Joost</forenames></author><author><keyname>Groher</keyname><forenames>Iris</forenames></author></authors><title>Sustainability in Software Product Lines: Report on Discussion Panel at
  SPLC 2014</title><categories>cs.SE</categories><comments>4 pages, notes on panel held at Software Product Lines Conference -
  SPLC 2014</comments><acm-class>D.2.9; K.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sustainability (defined as 'the capacity to keep up') encompasses a wide set
of aims: ranging from energy efficient software products (environmental
sustainability), reduction of software development and maintenance costs
(economic sustainability), to employee and end-user wellbeing (social
sustainability). In this report we explore the role that sustainability plays
in software product line engineering (SPL). The report is based on the
'Sustainability in Software Product Lines' panel held at SPLC 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03737</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03737</id><created>2015-05-14</created><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Schweitzer</keyname><forenames>Pascal</forenames></author></authors><title>Isomorphism Testing for Graphs of Bounded Rank Width</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm that, for every fixed k, decides isomorphism of graphs
of rank width at most k in polynomial time. As the clique width of a graph is
bounded in terms of its rank width, we also obtain a polynomial time
isomorphism test for graph classes of bounded clique width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03738</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03738</id><created>2015-05-11</created><authors><author><keyname>Grun</keyname><forenames>Casey</forenames></author><author><keyname>Sarma</keyname><forenames>Karthik</forenames></author><author><keyname>Wolfe</keyname><forenames>Brian</forenames></author><author><keyname>Shin</keyname><forenames>Seung Woo</forenames></author><author><keyname>Winfree</keyname><forenames>Erik</forenames></author></authors><title>A domain-level DNA strand displacement reaction enumerator allowing
  arbitrary non-pseudoknotted secondary structures</title><categories>cs.CE cs.ET q-bio.MN</categories><comments>Accepted for oral presentation at Verification of Engineered
  Molecular Devices and Programs (VEMDP), July 17, 2014, Vienna, Austria. 29
  pages, conference version. (Revised and expanded journal version is in
  preparation.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DNA strand displacement systems have proven themselves to be fertile
substrates for the design of programmable molecular machinery and circuitry.
Domain-level reaction enumerators provide the foundations for molecular
programming languages by formalizing DNA strand displacement mechanisms and
modeling interactions at the &quot;domain&quot; level - one level of abstraction above
models that explicitly describe DNA strand sequences. Unfortunately, the
most-developed models currently only treat pseudo-linear DNA structures, while
many systems being experimentally and theoretically pursued exploit a much
broader range of secondary structure configurations. Here, we describe a new
domain-level reaction enumerator that can handle arbitrary non-pseudoknotted
secondary structures and reaction mechanisms including association and
dissociation, 3-way and 4-way branch migration, and direct as well as remote
toehold activation. To avoid polymerization that is inherent when considering
general structures, we employ a time-scale separation technique that holds in
the limit of low concentrations. This also allows us to &quot;condense&quot; the detailed
reactions by eliminating fast transients, with provable guarantees of
correctness for the set of reactions and their kinetics. We hope that the new
reaction enumerator will be used in new molecular programming languages,
compilers, and tools for analysis and verification that treat a wider variety
of mechanisms of interest to experimental and theoretical work. We have
implemented this enumerator in Python, and it is included in the DyNAMiC
Workbench Integrated Development Environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03758</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03758</id><created>2015-05-14</created><authors><author><keyname>Ho-Van</keyname><forenames>Khuong</forenames></author><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Que</keyname><forenames>Son Vo</forenames></author><author><keyname>Anh</keyname><forenames>Tuan Dang</forenames></author><author><keyname>Quang</keyname><forenames>Thai Pham</forenames></author><author><keyname>Hong</keyname><forenames>Lien Pham</forenames></author></authors><title>Bit-Error-Rate Analysis of Underlay Relay Cognitive Networks with
  Channel Estimation Errors</title><categories>cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper evaluates the bit error rate (BER) performance of underlay relay
cognitive networks with decode-and-forward (DF) relays in arbitrary number of
hops over Rayleigh fading with channel estimation errors. In order to
facilitate the performance evaluation analytically we derive a novel exact
closed-form representation for the corresponding BER which is validated through
extensive comparisons with results from Monte-Carlo simulations. The proposed
expression involved well known elementary and special functions which render
its computational realization rather simple and straightforward. As a result,
the need for laborious, energy exhaustive and time-consuming computer
simulations can be ultimately omitted. Numerous results illustrate that the
performance of underlay relay cognitive networks is, as expected, significantly
degraded by channel estimation errors and that is highly dependent upon of both
the network topology and the number of hops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03759</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03759</id><created>2015-05-14</created><updated>2015-05-30</updated><authors><author><keyname>Zhou</keyname><forenames>Keren</forenames></author><author><keyname>Niu</keyname><forenames>Guocheng</forenames></author><author><keyname>Zhang</keyname><forenames>Wuzhao</forenames></author><author><keyname>Li</keyname><forenames>Xueqi</forenames></author><author><keyname>Liu</keyname><forenames>Wenqin</forenames></author></authors><title>Parse Concurrent Data Structures: BST as an Example</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing concurrent data structures should follow some basic rules. By
separating the algorithms into two phases, we present guidelines for scalable
data structures, with a analysis model based on the Amadal's law. To the best
of our knowledge, we are the first to formalize a practical model for measuring
concurrent structures' speedup. We also build some edge-cutting BSTs following
our principles, testing them under different workloads. The result provides
compelling evidence to back the our guidelines, and shows that our theory is
useful for reasoning the varied speedup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03772</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03772</id><created>2015-05-14</created><updated>2015-10-02</updated><authors><author><keyname>Gao</keyname><forenames>Chao</forenames></author><author><keyname>Ma</keyname><forenames>Zongming</forenames></author><author><keyname>Zhang</keyname><forenames>Anderson Y.</forenames></author><author><keyname>Zhou</keyname><forenames>Harrison H.</forenames></author></authors><title>Achieving Optimal Misclassification Proportion in Stochastic Block Model</title><categories>math.ST cs.SI stat.ME stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is a fundamental statistical problem in network data
analysis. Many algorithms have been proposed to tackle this problem. Most of
these algorithms are not guaranteed to achieve the statistical optimality of
the problem, while procedures that achieve information theoretic limits for
general parameter spaces are not computationally tractable. In this paper, we
present a computationally feasible two-stage method that achieves optimal
statistical performance in misclassification proportion for stochastic block
model under weak regularity conditions. Our two-stage procedure consists of a
generic refinement step that can take a wide range of weakly consistent
community detection procedures as initializer, to which the refinement stage
applies and outputs a community assignment achieving optimal misclassification
proportion with high probability. The practical effectiveness of the new
algorithm is demonstrated by competitive numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03774</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03774</id><created>2015-05-14</created><authors><author><keyname>Levi</keyname><forenames>Retsef</forenames></author><author><keyname>Shi</keyname><forenames>Cong</forenames></author></authors><title>Dynamic Allocation Problems in Loss Network Systems with Advanced
  Reservation</title><categories>math.PR cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of well-known dynamic resource allocation models in loss
network systems with advanced reservation. The most important performance
measure in any loss network system is to compute its blocking probability,
i.e., the probability of an arriving customer in equilibrium finds a fully
utilized system (thereby getting rejected by the system). In this paper, we
derive upper bounds on the asymptotic blocking probabilities for such systems
in high-volume regimes. There have been relatively few results on loss network
systems with advanced reservation due to its inherent complexity. The
theoretical results find applications in a wide class of revenue management
problems in systems with reusable resources and advanced reservation, e.g.,
hotel room, car rental and workforce management. We propose a simple control
policy called the improved class selection policy (ICSP) based on solving a
continuous knapsack problem, similar in spirit to the one proposed in Levi and
Radovanovic (2010). Using our results derived for loss network systems with
advanced reservation, we show the ICSP performs asymptotically near-optimal in
high-volume regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03776</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03776</id><created>2015-05-14</created><authors><author><keyname>Alvarez</keyname><forenames>Raquel</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Sentiment cascades in the 15M movement</title><categories>cs.SI physics.soc-ph</categories><comments>EPJ Data Science vol 4 (2015) (forthcoming)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent grassroots movements have suggested that online social networks might
play a key role in their organization, as adherents have a fast, many-to-many,
communication channel to help coordinate their mobilization. The structure and
dynamics of the networks constructed from the digital traces of protesters have
been analyzed to some extent recently. However, less effort has been devoted to
the analysis of the semantic content of messages exchanged during the protest.
Using the data obtained from a microblogging service during the brewing and
active phases of the 15M movement in Spain, we perform the first large scale
test of theories on collective emotions and social interaction in collective
actions. Our findings show that activity and information cascades in the
movement are larger in the presence of negative collective emotions and when
users express themselves in terms related to social content. At the level of
individual participants, our results show that their social integration in the
movement, as measured through social network metrics, increases with their
level of engagement and of expression of negativity. Our findings show that
non-rational factors play a role in the formation and activity of social
movements through online media, having important consequences for viral
spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03779</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03779</id><created>2015-05-14</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Freear</keyname><forenames>Steven</forenames></author></authors><title>A Generalized Non-Linear Composite Fading Model</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to the formulation and derivation of the
$\alpha{-}\kappa{-}\mu{/}$gamma distribution which corresponds to a physical
fading model. The proposed distribution is composite and is constituted by the
$\alpha{-}\kappa{-}\mu$ non-linear generalized multipath model and the gamma
shadowing model. It also constitute the basis for deriving the
$\alpha{-}\kappa{-}\mu$ \textit{Extreme}${/}$gamma model which accounts for
non-linear severe multipath and shadowing effects and also includes the more
widely known $\alpha{-}\mu$ and $\kappa{-}\mu$ models which includes as special
cases the Rice, Weibull, Nakagami-$m$ and Rayleigh distributions. The derived
models provide accurate characterisation of the simultaneous occurrence of
multipath fading and shadowing effects. This is achieved thanks to the
remarkable flexibility of their named parameters which have been shown to
render them capable of providing good fittings to experimental data associated
with realistic communication scenarios. This is also evident by the fact that
they include as special cases the widely known composite fading models such as
the recently reported $\kappa{-}\mu{/}$gamma model and the novel
$\alpha{-}\mu{/}$gamma model. Novel analytic expressions are derived for the
corresponding probability density function of these distributions which are
expressed in a convenient algebraic form and can be efficiently utilized in the
derivation of numerous vital measures in investigations related to the analytic
performance evaluation of digital communications over composite
multipath${/}$shadowing fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03783</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03783</id><created>2015-05-14</created><authors><author><keyname>Cocho</keyname><forenames>Germinal</forenames></author><author><keyname>Flores</keyname><forenames>Jorge</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Pineda</keyname><forenames>Carlos</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Sergio</forenames></author></authors><title>Rank diversity of languages: Generic behavior in computational
  linguistics</title><categories>cs.CL</categories><journal-ref>PLoS ONE 10(4): e0121898 (2015)</journal-ref><doi>10.1371/journal.pone.0121898</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical studies of languages have focused on the rank-frequency
distribution of words. Instead, we introduce here a measure of how word ranks
change in time and call this distribution \emph{rank diversity}. We calculate
this diversity for books published in six European languages since 1800, and
find that it follows a universal lognormal distribution. Based on the mean and
standard deviation associated with the lognormal distribution, we define three
different word regimes of languages: &quot;heads&quot; consist of words which almost do
not change their rank in time, &quot;bodies&quot; are words of general use, while &quot;tails&quot;
are comprised by context-specific words and vary their rank considerably in
time. The heads and bodies reflect the size of language cores identified by
linguists for basic communication. We propose a Gaussian random walk model
which reproduces the rank variation of words in time and thus the diversity.
Rank diversity of words can be understood as the result of random variations in
rank, where the size of the variation depends on the rank itself. We find that
the core size is similar for all languages studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03791</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03791</id><created>2015-05-14</created><authors><author><keyname>Accattoli</keyname><forenames>Beniamino</forenames></author><author><keyname>Coen</keyname><forenames>Claudio Sacerdoti</forenames></author></authors><title>On the Relative Usefulness of Fireballs</title><categories>cs.LO</categories><comments>Technical report for the LICS 2015 submission with the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In CSL-LICS 2014, Accattoli and Dal Lago showed that there is an
implementation of the ordinary (i.e. strong, pure, call-by-name)
$\lambda$-calculus into models like RAM machines which is polynomial in the
number of $\beta$-steps, answering a long-standing question. The key ingredient
was the use of a calculus with useful sharing, a new notion whose complexity
was shown to be polynomial, but whose implementation was not explored. This
paper, meant to be complementary, studies useful sharing in a call-by-value
scenario and from a practical point of view. We introduce the Fireball
Calculus, a natural extension of call-by-value to open terms for which the
problem is as hard as for the ordinary lambda-calculus. We present three
results. First, we adapt the solution of Accattoli and Dal Lago, improving the
meta-theory of useful sharing. Then, we refine the picture by introducing the
GLAMoUr, a simple abstract machine implementing the Fireball Calculus extended
with useful sharing. Its key feature is that usefulness of a step is
tested---surprisingly---in constant time. Third, we provide a further
optimization that leads to an implementation having only a linear overhead with
respect to the number of $\beta$-steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03795</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03795</id><created>2015-05-14</created><authors><author><keyname>Abdul-Rahman</keyname><forenames>Houssam</forenames></author><author><keyname>Chernov</keyname><forenames>Nikolai</forenames></author></authors><title>Fast and numerically stable circle fit</title><categories>cs.CV</categories><comments>16 pages</comments><journal-ref>Journal of Mathematical Imaging and Vision June 2014, Volume 49,
  Issue 2, pp 289-295</journal-ref><doi>10.1007/s10851-013-0461-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new algorithm for fitting circles that does not have drawbacks
commonly found in existing circle fits. Our fit achieves ultimate accuracy (to
machine precision), avoids divergence, and is numerically stable even when
fitting circles get arbitrary large. Lastly, our algorithm takes less than 10
iterations to converge, on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03796</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03796</id><created>2015-05-14</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Freear</keyname><forenames>Steven</forenames></author></authors><title>Novel Expressions for the Rice $Ie{-}$Function and the Incomplete
  Lipschitz-Hankel Integrals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents novel analytic expressions for the Rice $Ie{-}$function,
$Ie(k,x)$, and the incomplete Lipschitz-Hankel Integrals (ILHIs) of the
modified Bessel function of the first kind, $Ie_{m,n}(a,z)$. Firstly, an exact
infinite series and an accurate polynomial approximation are derived for the
$Ie(k ,x)$ function which are valid for all values of $k$. Secondly, an exact
closed-form expression is derived for the $Ie_{m,n}(a,z)$ integrals for the
case that $n$ is an odd multiple of $1/2$ and subsequently an infinite series
and a tight polynomial approximation which are valid for all values of $m$ and
$n$. Analytic upper bounds are also derived for the corresponding truncation
errors of the derived series'. Importantly, these bounds are expressed in
closed-form and are particularly tight while they straightforwardly indicate
that a remarkable accuracy is obtained by truncating each series after a small
number of terms. Furthermore, the offered expressions have a convenient
algebraic representation which renders them easy to handle both analytically
and numerically. As a result, they can be considered as useful mathematical
tools that can be efficiently utilized in applications related to the
analytical performance evaluation of classical and modern digital communication
systems over fading environments, among others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03799</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03799</id><created>2015-05-14</created><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Musco</keyname><forenames>Cameron</forenames></author><author><keyname>Radeva</keyname><forenames>Tsvetomira</forenames></author><author><keyname>Lynch</keyname><forenames>Nancy</forenames></author></authors><title>Distributed House-Hunting in Ant Colonies</title><categories>cs.DC</categories><comments>To appear in PODC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the study of the ant colony house-hunting problem from a
distributed computing perspective. When an ant colony's nest becomes unsuitable
due to size constraints or damage, the colony must relocate to a new nest. The
task of identifying and evaluating the quality of potential new nests is
distributed among all ants. The ants must additionally reach consensus on a
final nest choice and the full colony must be transported to this single new
nest. Our goal is to use tools and techniques from distributed computing theory
in order to gain insight into the house-hunting process.
  We develop a formal model for the house-hunting problem inspired by the
behavior of the Temnothorax genus of ants. We then show a \Omega(log n) lower
bound on the time for all n ants to agree on one of k candidate nests. We also
present two algorithms that solve the house-hunting problem in our model. The
first algorithm solves the problem in optimal O(log n) time but exhibits some
features not characteristic of natural ant behavior. The second algorithm runs
in O(k log n) time and uses an extremely simple and natural rule for each ant
to decide on the new nest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03804</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03804</id><created>2015-05-14</created><updated>2015-07-30</updated><authors><author><keyname>Cody</keyname><forenames>Emily M.</forenames></author><author><keyname>Reagan</keyname><forenames>Andrew J.</forenames></author><author><keyname>Mitchell</keyname><forenames>Lewis</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author></authors><title>Climate change sentiment on Twitter: An unsolicited public opinion poll</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>11 pages, 10 figures</comments><doi>10.1371/journal.pone.0136092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The consequences of anthropogenic climate change are extensively debated
through scientific papers, newspaper articles, and blogs. Newspaper articles
may lack accuracy, while the severity of findings in scientific papers may be
too opaque for the public to understand. Social media, however, is a forum
where individuals of diverse backgrounds can share their thoughts and opinions.
As consumption shifts from old media to new, Twitter has become a valuable
resource for analyzing current events and headline news. In this research, we
analyze tweets containing the word &quot;climate&quot; collected between September 2008
and July 2014. Through use of a previously developed sentiment measurement tool
called the Hedonometer, we determine how collective sentiment varies in
response to climate change news, events, and natural disasters. We find that
natural disasters, climate bills, and oil-drilling can contribute to a decrease
in happiness while climate rallies, a book release, and a green ideas contest
can contribute to an increase in happiness. Words uncovered by our analysis
suggest that responses to climate change news are predominately from climate
change activists rather than climate change deniers, indicating that Twitter is
a valuable resource for the spread of climate change awareness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03819</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03819</id><created>2015-05-14</created><authors><author><keyname>Teodoro</keyname><forenames>George</forenames></author><author><keyname>Kurc</keyname><forenames>Tahsin</forenames></author><author><keyname>Andrade</keyname><forenames>Guilherme</forenames></author><author><keyname>Kong</keyname><forenames>Jun</forenames></author><author><keyname>Ferreira</keyname><forenames>Renato</forenames></author><author><keyname>Saltz</keyname><forenames>Joel</forenames></author></authors><title>Performance Analysis and Efficient Execution on Systems with multi-core
  CPUs, GPUs and MICs</title><categories>cs.DC</categories><comments>22 pages, 12 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We carry out a comparative performance study of multi-core CPUs, GPUs and
Intel Xeon Phi (Many Integrated Core - MIC) with a microscopy image analysis
application. We experimentally evaluate the performance of computing devices on
core operations of the application. We correlate the observed performance with
the characteristics of computing devices and data access patterns, computation
complexities, and parallelization forms of the operations. The results show a
significant variability in the performance of operations with respect to the
device used. The performances of operations with regular data access are
comparable or sometimes better on a MIC than that on a GPU. GPUs are more
efficient than MICs for operations that access data irregularly, because of the
lower bandwidth of the MIC for random data accesses. We propose new
performance-aware scheduling strategies that consider variabilities in
operation speedups. Our scheduling strategies significantly improve application
performance compared to classic strategies in hybrid configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03823</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03823</id><created>2015-05-14</created><updated>2015-08-04</updated><authors><author><keyname>Fan</keyname><forenames>Miao</forenames></author><author><keyname>Zhou</keyname><forenames>Qiang</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author></authors><title>Distant Supervision for Entity Linking</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity linking is an indispensable operation of populating knowledge
repositories for information extraction. It studies on aligning a textual
entity mention to its corresponding disambiguated entry in a knowledge
repository. In this paper, we propose a new paradigm named distantly supervised
entity linking (DSEL), in the sense that the disambiguated entities that belong
to a huge knowledge repository (Freebase) are automatically aligned to the
corresponding descriptive webpages (Wiki pages). In this way, a large scale of
weakly labeled data can be generated without manual annotation and fed to a
classifier for linking more newly discovered entities. Compared with
traditional paradigms based on solo knowledge base, DSEL benefits more via
jointly leveraging the respective advantages of Freebase and Wikipedia.
Specifically, the proposed paradigm facilitates bridging the disambiguated
labels (Freebase) of entities and their textual descriptions (Wikipedia) for
Web-scale entities. Experiments conducted on a dataset of 140,000 items and
60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyze
the feature performance and improve the F1-measure to 0.545.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03824</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03824</id><created>2015-05-14</created><updated>2015-10-28</updated><authors><author><keyname>Iacovacci</keyname><forenames>Jacopo</forenames></author><author><keyname>Wu</keyname><forenames>Zhihao</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author></authors><title>Mesoscopic Structures Reveal the Network Between the Layers of Multiplex
  Datasets</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 7 figures</comments><journal-ref>Phys. Rev. E 92, 042806 (2015)</journal-ref><doi>10.1103/PhysRevE.92.042806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplex networks describe a large variety of complex systems, whose
elements (nodes) can be connected by different types of interactions forming
different layers (networks) of the multiplex. Multiplex networks include social
networks, transportation networks or biological networks in the cell or in the
brain. Extracting relevant information from these networks is of crucial
importance for solving challenging inference problems and for characterizing
the multiplex networks microscopic and mesoscopic structure. Here we propose an
information theory method to extract the network between the layers of
multiplex datasets, forming a &quot;network of networks&quot;. We build an indicator
function, based on the entropy of network ensembles, to characterize the
mesoscopic similarities between the layers of a multiplex network and we use
clustering techniques to characterize the communities present in this network
of networks. We apply the proposed method to study the Multiplex Collaboration
Network formed by scientists collaborating on different subjects and publishing
in the Americal Physical Society (APS) journals. The analysis of this dataset
reveals the interplay between the collaboration networks and the organization
of knowledge in physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03825</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03825</id><created>2015-05-14</created><authors><author><keyname>Kwak</keyname><forenames>Suha</forenames></author><author><keyname>Cho</keyname><forenames>Minsu</forenames></author><author><keyname>Laptev</keyname><forenames>Ivan</forenames></author><author><keyname>Ponce</keyname><forenames>Jean</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>Unsupervised Object Discovery and Tracking in Video Collections</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of automatically localizing dominant objects
as spatio-temporal tubes in a noisy collection of videos with minimal or even
no supervision. We formulate the problem as a combination of two complementary
processes: discovery and tracking. The first one establishes correspondences
between prominent regions across videos, and the second one associates
successive similar object regions within the same video. Interestingly, our
algorithm also discovers the implicit topology of frames associated with
instances of the same object class across different videos, a role normally
left to supervisory information in the form of class labels in conventional
image and video understanding methods. Indeed, as demonstrated by our
experiments, our method can handle video collections featuring multiple object
classes, and substantially outperforms the state of the art in colocalization,
even though it tackles a broader problem with much less supervision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03832</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03832</id><created>2015-05-14</created><authors><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Singh</keyname><forenames>Nikhil</forenames></author><author><keyname>Kwitt</keyname><forenames>Roland</forenames></author><author><keyname>Vasconcelos</keyname><forenames>Nuno</forenames></author><author><keyname>Niethammer</keyname><forenames>Marc</forenames></author></authors><title>Parametric Regression on the Grassmannian</title><categories>cs.CV</categories><comments>14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of fitting parametric curves on the Grassmann manifold
for the purpose of intrinsic parametric regression. As customary in the
literature, we start from the energy minimization formulation of linear
least-squares in Euclidean spaces and generalize this concept to general
nonflat Riemannian manifolds, following an optimal-control point of view. We
then specialize this idea to the Grassmann manifold and demonstrate that it
yields a simple, extensible and easy-to-implement solution to the parametric
regression problem. In fact, it allows us to extend the basic geodesic model to
(1) a time-warped variant and (2) cubic splines. We demonstrate the utility of
the proposed solution on different vision problems, such as shape regression as
a function of age, traffic-speed estimation and crowd-counting from
surveillance video clips. Most notably, these problems can be conveniently
solved within the same framework without any specifically-tailored steps along
the processing pipeline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03840</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03840</id><created>2015-05-14</created><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Chen</keyname><forenames>Yutong</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Non-unique games over compact groups and orientation estimation in
  cryo-EM</title><categories>cs.CV cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{G}$ be a compact group and let $f_{ij} \in L^2(\mathcal{G})$.
We define the Non-Unique Games (NUG) problem as finding $g_1,\dots,g_n \in
\mathcal{G}$ to minimize $\sum_{i,j=1}^n f_{ij} \left( g_i g_j^{-1}\right)$. We
devise a relaxation of the NUG problem to a semidefinite program (SDP) by
taking the Fourier transform of $f_{ij}$ over $\mathcal{G}$, which can then be
solved efficiently. The NUG framework can be seen as a generalization of the
little Grothendieck problem over the orthogonal group and the Unique Games
problem and includes many practically relevant problems, such as the maximum
likelihood estimator} to registering bandlimited functions over the unit sphere
in $d$-dimensions and orientation estimation in cryo-Electron Microscopy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03851</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03851</id><created>2015-05-14</created><authors><author><keyname>Steele</keyname><forenames>Guy L.</forenames><suffix>Jr.</suffix><affiliation>Oracle Labs</affiliation></author><author><keyname>Tristan</keyname><forenames>Jean-Baptiste</forenames><affiliation>Oracle Labs</affiliation></author></authors><title>Using Butterfly-Patterned Partial Sums to Optimize GPU Memory Accesses
  for Drawing from Discrete Distributions</title><categories>cs.DC</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a technique for drawing values from discrete distributions, such
as sampling from the random variables of a mixture model, that avoids computing
a complete table of partial sums of the relative probabilities. A table of
alternate (&quot;butterfly-patterned&quot;) form is faster to compute, making better use
of coalesced memory accesses. From this table, complete partial sums are
computed on the fly during a binary search. Measurements using an NVIDIA Titan
Black GPU show that for a sufficiently large number of clusters or topics (K &gt;
200), this technique alone more than doubles the speed of a latent Dirichlet
allocation (LDA) application already highly tuned for GPU execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03852</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03852</id><created>2015-05-14</created><authors><author><keyname>Carayol</keyname><forenames>Arnaud</forenames></author><author><keyname>Haddad</keyname><forenames>Axel</forenames></author><author><keyname>Serre</keyname><forenames>Olivier</forenames></author></authors><title>Counting Branches in Trees Using Games</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study finite automata running over infinite binary trees. A run of such an
automaton is usually said to be accepting if all its branches are accepting. In
this article, we relax the notion of accepting run by allowing a certain
quantity of rejecting branches.
  More precisely we study the following criteria for a run to be accepting: -
it contains at most finitely (resp countably) many rejecting branches; - it
contains infinitely (resp uncountably) many accepting branches; - the set of
accepting branches is topologically &quot;big&quot;.
  In all situations we provide a simple acceptance game that later permits to
prove that the languages accepted by automata with cardinality constraints are
always $\omega$-regular. In the case (ii) where one counts accepting branches
it leads to new proofs (without appealing to logic) of an old result of
Beauquier and Niwinski.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03854</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03854</id><created>2015-05-14</created><authors><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Sitko</keyname><forenames>Izabela</forenames></author><author><keyname>Combes</keyname><forenames>Remi Tachet des</forenames></author><author><keyname>Hawelka</keyname><forenames>Bartosz</forenames></author><author><keyname>Arias</keyname><forenames>Juan Murillo</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Cities through the Prism of People's Spending Behavior</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific studies of society increasingly rely on digital traces produced by
various aspects of human activity. In this paper, we use a relatively
unexplored source of data, anonymized records of bank card transactions
collected in Spain by a big European bank, in order to propose a new
classification scheme of cities based on the economic behavior of their
residents. First, we study how individual spending behavior is qualitatively
and quantitatively affected by various factors such as customer's age, gender,
and size of a home city. We show that, similar to other socioeconomic urban
quantities, individual spending activity exhibits a statistically significant
superlinear scaling with city size. With respect to the general trends, we
quantify the distinctive signature of each city in terms of residents' spending
behavior, independently from the effects of scale and demographic
heterogeneity. Based on the comparison of city signatures, we build a novel
classification of cities across Spain in three categories. That classification
is, with few exceptions, stable over different ways of city definition and
connects with a meaningful socioeconomic interpretation. Furthermore, it
appears to be related with the ability of cities to attract foreign visitors,
which is a particularly remarkable finding given that the classification was
based exclusively on the behavioral patterns of city residents. This highlights
the far-reaching applicability of the presented classification approach and its
ability to discover patterns that go beyond the quantities directly involved in
it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03873</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03873</id><created>2015-05-14</created><authors><author><keyname>Tang</keyname><forenames>Kevin</forenames></author><author><keyname>Paluri</keyname><forenames>Manohar</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author></authors><title>Improving Image Classification with Location Context</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the widespread availability of cellphones and cameras that have GPS
capabilities, it is common for images being uploaded to the Internet today to
have GPS coordinates associated with them. In addition to research that tries
to predict GPS coordinates from visual features, this also opens up the door to
problems that are conditioned on the availability of GPS coordinates. In this
work, we tackle the problem of performing image classification with location
context, in which we are given the GPS coordinates for images in both the train
and test phases. We explore different ways of encoding and extracting features
from the GPS coordinates, and show how to naturally incorporate these features
into a Convolutional Neural Network (CNN), the current state-of-the-art for
most image classification and recognition problems. We also show how it is
possible to simultaneously learn the optimal pooling radii for a subset of our
features within the CNN framework. To evaluate our model and to help promote
research in this area, we identify a set of location-sensitive concepts and
annotate a subset of the Yahoo Flickr Creative Commons 100M dataset that has
GPS coordinates with these concepts, which we make publicly available. By
leveraging location context, we are able to achieve almost a 7% gain in mean
average precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03883</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03883</id><created>2015-05-14</created><updated>2015-09-14</updated><authors><author><keyname>Chen</keyname><forenames>Jian-Jia</forenames></author><author><keyname>Huang</keyname><forenames>Wen-Hung</forenames></author><author><keyname>Liu</keyname><forenames>Cong</forenames></author></authors><title>k2Q: A Quadratic-Form Response Time and Schedulability Analysis
  Framework for Utilization-Based Analysis</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1505.02155;
  text overlap with arXiv:1501.07084</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a general response-time analysis and
schedulability-test framework, called k2Q. It provides automatic constructions
of closed-form quadratic bounds or utilization bounds for a wide range of
applications in real-time systems under fixed-priority scheduling. The key of
the framework is a k-point schedulability test or a k-point response time
analysis that is based on the utilizations and the execution times of
higher-priority tasks. We show the generality of k2Q by applying it to several
different task models. Specifically, we achieve many new results in
uniprocessor and multiprocessor scheduling for not only traditional sporadic
task models, but also some more advanced and expressive task models.
  In the past, exponential-time schedulability tests were typically not
recommended and most of time ignored, as this requires very high complexity. We
have successfully shown in this paper that exponential-time schedulability
tests may lead to good polynomial-time tests (almost automatically) by using
the k2Q framework. Analogously, a similar concept by testing only k points with
a different formulation has been studied by us in another framework, called
k2U, which provides hyperbolic bounds or utilization bounds based on a
different formulation of schedulability test. With the quadratic and hyperbolic
forms, k2Q and k2U frameworks can be used to provide many quantitive features
to be measured, like the total utilization bounds, speed-up factors, etc., not
only for uniprocessor scheduling but also for multiprocessor scheduling. These
frameworks can be viewed as a &quot;blackbox&quot; interface for schedulability tests and
response-time analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03891</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03891</id><created>2015-05-14</created><authors><author><keyname>Sanchez</keyname><forenames>Adrian A.</forenames></author></authors><title>Task-Based Optimization of Computed Tomography Imaging Systems</title><categories>physics.med-ph cs.CV</categories><comments>Doctoral Dissertation - University of Chicago</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this thesis is to provide a framework for the use of task-based
metrics of image quality to aid in the design, implementation, and evaluation
of CT image reconstruction algorithms and CT systems in general. We support the
view that task-based metrics of image quality can be useful in guiding the
algorithm design and implementation process in order to yield images of
objectively superior quality and higher utility for a given task. Further, we
believe that metrics such as the Hotelling observer (HO) SNR can be used as
summary scalar metrics of image quality for the evaluation of images produced
by novel reconstruction algorithms. In this work, we aim to construct a concise
and versatile formalism for image reconstruction algorithm design,
implementation, and assessment. The bulk of the work focuses on linear
analytical algorithms, specifically the ubiquitous filtered back-projection
(FBP) algorithm. However, due to the demonstrated importance of
optimization-based algorithms in a wide variety of CT applications, we devote
one chapter to the characterization of noise properties in TV-based iterative
reconstruction, as the understanding of image statistics in optimization-based
reconstruction is the limiting factor in applying HO metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03897</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03897</id><created>2015-05-14</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Freear</keyname><forenames>Steven</forenames></author></authors><title>Closed-Form Bounds for the Rice $Ie$-Function</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted in the derivation of novel upper and lower bounds for
the Rice $Ie$-function. These bounds are expressed in closed-form and are shown
to be quite tight. This is particularly evident by the fact that for a certain
range of parameter values, the derived lower bound virtually behaves as a
remarkably accurate approximation. As a result, the offered expressions can be
considered useful mathematical tools that can be efficiently employed in
various analytical studies related to natural sciences and engineering. To this
effect, they can be sufficiently applied in the area of digital communications
over fading channels for the derivation of explicit representations for vital
performance measures such as bit and symbol error probability, among others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03898</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03898</id><created>2015-05-14</created><authors><author><keyname>Huang</keyname><forenames>Xiaolin</forenames></author><author><keyname>Shi</keyname><forenames>Lei</forenames></author><author><keyname>Yan</keyname><forenames>Ming</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author></authors><title>Pinball Loss Minimization for One-bit Compressive Sensing</title><categories>cs.IT math.IT math.NA math.OC stat.ML</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The one-bit quantization can be implemented by one single comparator, which
operates at low power and a high rate. Hence one-bit compressive sensing
(\emph{1bit-CS}) becomes very attractive in signal processing. When the
measurements are corrupted by noise during signal acquisition and transmission,
1bit-CS is usually modeled as minimizing a loss function with a sparsity
constraint. The existing loss functions include the hinge loss and the linear
loss. Though 1bit-CS can be regarded as a binary classification problem because
a one-bit measurement only provides the sign information, the choice of the
hinge loss over the linear loss in binary classification is not true for
1bit-CS. Many experiments show that the linear loss performs better than the
hinge loss for 1bit-CS. Motivated by this observation, we consider the pinball
loss, which provides a bridge between the hinge loss and the linear loss. Using
this bridge, two 1bit-CS models and two corresponding algorithms are proposed.
Pinball loss iterative hard thresholding improves the performance of the binary
iterative hard theresholding proposed in [6] and is suitable for the case when
the sparsity of the true signal is given. Elastic-net pinball support vector
machine generalizes the passive model proposed in [11] and is suitable for the
case when the sparsity of the true signal is not given. A fast dual coordinate
ascent algorithm is proposed to solve the elastic-net pinball support vector
machine problem, and its convergence is proved. The numerical experiments
demonstrate that the pinball loss, as a trade-off between the hinge loss and
the linear loss, improves the existing 1bit-CS models with better performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03899</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03899</id><created>2015-05-14</created><authors><author><keyname>Sung</keyname><forenames>Jean</forenames></author><author><keyname>Krupa</keyname><forenames>Sebastian</forenames></author><author><keyname>Fishberg</keyname><forenames>Andrew</forenames></author><author><keyname>Spjut</keyname><forenames>Josef</forenames></author></authors><title>An Approach to Data Prefetching Using 2-Dimensional Selection Criteria</title><categories>cs.AR</categories><comments>4 pages, 5 figures, submitted to Second Data Prefetching Championship</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach to data memory prefetching which augments the standard
prefetch buffer with selection criteria based on performance and usage pattern
of a given instruction. This approach is built on top of a pattern matching
based prefetcher, specifically one which can choose between a stream, a stride,
or a stream followed by a stride. We track the most recently called
instructions to make a decision on the quantity of data to prefetch next. The
decision is based on the frequency with which these instructions are called and
the hit/miss rate of the prefetcher. In our approach, we separate the amount of
data to prefetch into three categories: a high degree, a standard degree and a
low degree. We ran tests on different values for the high prefetch degree,
standard prefetch degree and low prefetch degree to determine that the most
optimal combination was 1, 4, 8 lines respectively. The 2 dimensional selection
criteria improved the performance of the prefetcher by up to 9.5% over the
first data prefetching championship winner. Unfortunately performance also fell
by as much as 14%, but remained similar on average across all of the benchmarks
we tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03906</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03906</id><created>2015-05-14</created><authors><author><keyname>Dziugaite</keyname><forenames>Gintare Karolina</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Training generative neural networks via Maximum Mean Discrepancy
  optimization</title><categories>stat.ML cs.LG</categories><comments>10 pages, to appear in Uncertainty in Artificial Intelligence (UAI)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider training a deep neural network to generate samples from an
unknown distribution given i.i.d. data. We frame learning as an optimization
minimizing a two-sample test statistic---informally speaking, a good generator
network produces samples that cause a two-sample test to fail to reject the
null hypothesis. As our two-sample test statistic, we use an unbiased estimate
of the maximum mean discrepancy, which is the centerpiece of the nonparametric
kernel two-sample test proposed by Gretton et al. (2012). We compare to the
adversarial nets framework introduced by Goodfellow et al. (2014), in which
learning is a two-player game between a generator network and an adversarial
discriminator network, both trained to outwit the other. From this perspective,
the MMD statistic plays the role of the discriminator. In addition to empirical
comparisons, we prove bounds on the generalization error incurred by optimizing
the empirical MMD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03917</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03917</id><created>2015-05-14</created><authors><author><keyname>Schewtschenko</keyname><forenames>Jascha A.</forenames></author></authors><title>General Riemannian SOM</title><categories>cs.NE</categories><comments>175 pages, 46 figures, Diploma thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kohonen's Self-Organizing Maps (SOMs) have proven to be a successful
data-reduction method to identify the intrinsic lower-dimensional sub-manifold
of a data set that is scattered in the higher-dimensional feature space.
Motivated by the possibly non-Euclidian nature of the feature space and of the
intrinsic geometry of the data set, we extend the definition of classic SOMs to
obtain the General Riemannian SOM (GRiSOM). We additionally provide an
implementation as a proof-of-concept for geometries with constant curvature. We
furthermore perform the analytic and numerical analysis of the stability limits
of certain (GRi)SOM configurations covering the different possible regular
tessellation of the map space in each geometry. A deviation between the
numerical and analytic stability limit has been observed for the square and
hexagonal Euclidean maps for very small neighbourhoods in the map space as well
as agreement in case of longer-ranged relations between the map nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03924</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03924</id><created>2015-05-14</created><updated>2016-03-07</updated><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Haghtalab</keyname><forenames>Nika</forenames></author><author><keyname>White</keyname><forenames>Colin</forenames></author></authors><title>$k$-center Clustering under Perturbation Resilience</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-center problem is a canonical and long-studied facility location and
clustering problem with many applications in both its symmetric and asymmetric
forms. Both versions of the problem have tight approximation factors on worst
case instances: a $2$-approximation for symmetric $k$-center and an
$O(\log^*(k))$-approximation for the asymmetric version.
  In this work, we go beyond the worst case and provide strong positive results
both for the asymmetric and symmetric $k$-center problems under a very natural
input stability (promise) condition called $\alpha$-perturbation resilience
(Bilu &amp; Linial 2012) , which states that the optimal solution does not change
under any $\alpha$-factor perturbation to the input distances. We show that by
assuming 2-perturbation resilience, the exact solution for the asymmetric
$k$-center problem can be found in polynomial time. To our knowledge, this is
the first problem that is hard to approximate to any constant factor in the
worst case, yet can be optimally solved in polynomial time under perturbation
resilience for a constant value of $\alpha$. Furthermore, we prove our result
is tight by showing symmetric $k$-center under $(2-\epsilon)$-perturbation
resilience is hard unless $NP=RP$. This is the first tight result for any
problem under perturbation resilience, i.e., this is the first time the exact
value of $\alpha$ for which the problem switches from being NP-hard to
efficiently computable has been found.
  Our results illustrate a surprising relationship between symmetric and
asymmetric $k$-center instances under perturbation resilience. Unlike
approximation ratio, for which symmetric $k$-center is easily solved to a
factor of $2$ but asymmetric $k$-center cannot be approximated to any constant
factor, both symmetric and asymmetric $k$-center can be solved optimally under
resilience to 2-perturbations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03931</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03931</id><created>2015-05-14</created><authors><author><keyname>Klinge</keyname><forenames>Titus H.</forenames></author><author><keyname>Lathrop</keyname><forenames>James I.</forenames></author><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author></authors><title>Robust Biomolecular Finite Automata</title><categories>cs.CC cs.ET cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a uniform method for translating an arbitrary
nondeterministic finite automaton (NFA) into a deterministic mass action
biochemical reaction network (BRN) that simulates it. The BRN receives its
input as a continuous time signal consisting of concentrations of chemical
species that vary to represent the NFA's input string in a natural way. The BRN
exploits the inherent parallelism of chemical kinetics to simulate the NFA in
real time with a number of chemical species that is linear in the number of
states of the NFA. We prove that the simulation is correct and that it is
robust with respect to perturbations of the input signal, the initial
concentrations of species, the output (decision), and the rate constants of the
reactions of the BRN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03932</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03932</id><created>2015-05-14</created><authors><author><keyname>Crocetti</keyname><forenames>Giancarlo</forenames></author><author><keyname>Coakley</keyname><forenames>Michael</forenames></author><author><keyname>Dressner</keyname><forenames>Phil</forenames></author><author><keyname>Kellum</keyname><forenames>Wanda</forenames></author><author><keyname>Lamin</keyname><forenames>Tamba</forenames></author></authors><title>Using Ensemble Models in the Histological Examination of Tissue
  Abnormalities</title><categories>cs.CV cs.CE cs.LG</categories><comments>4 pages, 4 tables, 3 figures. Proceedings of 12th Annual Research
  Day, 2014 - Pace University</comments><acm-class>H.2.8; I.5.3; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification models for the automatic detection of abnormalities on
histological samples do exists, with an active debate on the cost associated
with false negative diagnosis (underdiagnosis) and false positive diagnosis
(overdiagnosis). Current models tend to underdiagnose, failing to recognize a
potentially fatal disease.
  The objective of this study is to investigate the possibility of
automatically identifying abnormalities in tissue samples through the use of an
ensemble model on data generated by histological examination and to minimize
the number of false negative cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03934</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03934</id><created>2015-05-14</created><authors><author><keyname>Crocetti</keyname><forenames>Giancarlo</forenames></author></authors><title>Textual Spatial Cosine Similarity</title><categories>cs.IR</categories><comments>4 pages, 4 tables. Proceedings of 12th Annual Research Day, 2014 -
  Pace University</comments><acm-class>I.2.7; H.3.3; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dealing with document similarity many methods exist today, like cosine
similarity. More complex methods are also available based on the semantic
analysis of textual information, which are computationally expensive and rarely
used in the real time feeding of content as in enterprise-wide search
environments. To address these real-time constraints, we developed a new
measure of document similarity called Textual Spatial Cosine Similarity, which
is able to detect similitude at the semantic level using word placement
information contained in the document. We will see in this paper that two
degenerate cases exist for this model, which coincide with Cosine Similarity on
one side and with a paraphrasing detection model to the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03941</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03941</id><created>2015-05-14</created><authors><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author></authors><title>Mismatched Estimation in Large Linear Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the excess mean square error (EMSE) above the minimum mean square
error (MMSE) in large linear systems where the posterior mean estimator (PME)
is evaluated with a postulated prior that differs from the true prior of the
input signal. We focus on large linear systems where the measurements are
acquired via an independent and identically distributed random matrix, and are
corrupted by additive white Gaussian noise (AWGN). The relationship between the
EMSE in large linear systems and EMSE in scalar channels is derived, and closed
form approximations are provided. Our analysis is based on the decoupling
principle, which links scalar channels to large linear system analyses.
Numerical examples demonstrate that our closed form approximations are
accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03946</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03946</id><created>2015-05-14</created><updated>2015-10-15</updated><authors><author><keyname>Liang</keyname><forenames>Chulong</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author></authors><title>Block Markov Superposition Transmission of RUN Codes</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a simple procedure to construct (decodable) good
codes with any given alphabet (of moderate size) for any given (rational) code
rate to achieve any given target error performance (of interest) over additive
white Gaussian noise (AWGN) channels. We start with constructing codes over
groups for any given code rates. This can be done in an extremely simple way if
we ignore the error performance requirement for the time being. Actually, this
can be satisfied by repetition (R) codes and uncoded (UN) transmission along
with time-sharing technique. The resulting codes are simply referred to as RUN
codes for convenience. The encoding/decoding algorithms for RUN codes are
almost trivial. In addition, the performance can be easily analyzed. It is not
difficult to imagine that a RUN code usually performs far away from the
corresponding Shannon limit. Fortunately, the performance can be improved as
required by spatially coupling the RUN codes via block Markov superposition
transmission (BMST), resulting in the BMST-RUN codes. Simulation results show
that the BMST-RUN codes perform well (within one dB away from Shannon limits)
for a wide range of code rates and outperform the BMST with bit-interleaved
coded modulation (BMST-BICM) scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03950</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03950</id><created>2015-05-14</created><authors><author><keyname>Fan</keyname><forenames>Jie</forenames></author></authors><title>Logics of Strong Noncontingency</title><categories>cs.LO</categories><comments>28 pages</comments><msc-class>03B45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by Hintikka's treatment of question embedding verbs in [8] and the
variations of noncontingency operator, we propose a logic with strong
noncontingency operator $\blacktriangle$ as the only primitive modality. A
proposition is strongly noncontingent, if no matter whether it is true or
false, it does it necessarily; otherwise, it is weakly contingent. This logic
is not a normal modal logic, since
$\blacktriangle(\phi\to\psi)\to(\blacktriangle\phi\to\blacktriangle\psi)$ is
invalid. We compare the relative expressivity of this logic and other logics,
such as standard modal logic, noncontingency logic, and logic of essence and
accident, and investigate its frame definability. Apart from these results, we
also propose a suitable notion of bisimulation for the logic of strong
noncontingency, based on which we characterize this logic within modal logic
and within first-order logic. We also axiomatize the logic of strong
noncontingency over various frame classes. Our work is also related to the
treatment of agreement operator in [10].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03953</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03953</id><created>2015-05-14</created><updated>2015-11-07</updated><authors><author><keyname>Jha</keyname><forenames>Susmit</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>A Theory of Formal Synthesis via Inductive Learning</title><categories>cs.AI cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal synthesis is the process of generating a program satisfying a
high-level specification. In recent times, effective formal synthesis methods
have been proposed based on the use of inductive learning. We refer to this
class of methods that learn programs from examples as formal inductive
synthesis. In this paper, we present a theoretical framework for formal
inductive synthesis. We discuss how formal inductive synthesis differs from
traditional machine learning. We then describe oracle-guided inductive
synthesis (OGIS), a class of synthesizers that operate by iteratively querying
an oracle. An instance of OGIS that has had much practical impact is
counterexample-guided inductive synthesis (CEGIS). We present a theoretical
characterization of CEGIS for learning any program that computes a recursive
language. In particular, we analyze the relative power of CEGIS variants where
the types of counterexamples generated by the oracle varies. We also consider
the impact of bounded versus unbounded memory available to the learning
algorithm. In the special case where the universe of candidate programs is
finite, we relate the speed of convergence to the notion of teaching dimension
studied in machine learning theory. Altogether, the results of the paper take a
first step towards a theoretical foundation for the emerging field of formal
inductive synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03961</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03961</id><created>2015-05-15</created><updated>2015-07-14</updated><authors><author><keyname>Ruderman</keyname><forenames>Michael</forenames></author></authors><title>Computationally efficient formulation of relay operator for Preisach
  hysteresis modeling</title><categories>cs.SY</categories><comments>4 pages, 6 figures in IEEE Transactions on Magnetics, 2015</comments><doi>10.1109/TMAG.2015.2455517</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algebraic expression for the Preisach hysteron, which is a non-ideal
(delayed) relay operator, is formulated for a computationally efficient
real-time implementation. This allows representing the classical scalar
Preisach hysteresis model as a summation of a large number of weighted
hysterons which computation can be accomplished in parallel. The latter makes
possible an efficient FPGA or ASIC realization of the scalar Preisach
hysteresis model that can be useful for multiple applications. The signal flow
which specifies the model implementation is provided in form of the block
diagram. The proposed computation of Preisach hysterons, aggregated to the
entire Preisach hysteresis model, is evaluated numerically and on a real-time
hardware platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03962</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03962</id><created>2015-05-15</created><updated>2015-11-20</updated><authors><author><keyname>Loebl</keyname><forenames>Martin</forenames></author></authors><title>Deciding 4-colorability of planar triangulations</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show, without using the Four Color Theorem, that for each planar
triangulation, the number of its proper vertex colorings by 4 colors is a
determinant and thus can be calculated in a polynomial time. In particular, we
can efficiently decide if the number is non-zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03977</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03977</id><created>2015-05-15</created><updated>2016-01-27</updated><authors><author><keyname>Elmesbahi</keyname><forenames>Jelloul</forenames></author><author><keyname>Errami</keyname><forenames>Ahmed</forenames></author><author><keyname>Khaldoun</keyname><forenames>Mohammed</forenames></author><author><keyname>Bouattane</keyname><forenames>Omar</forenames></author></authors><title>A wide diversity of 3D surfaces Generator using a new implicit function</title><categories>cs.GR</categories><comments>This second submission replaces the first following submission at:
  arXiv:1505.03977 It completes this later by adding a second model to complete
  the first one. It corresponds to particular surfaces (Lego cubes) and the
  other remained results (about 500 3D surfaces). This is to avoid huge file
  size for our submission and to proof the effectiveness of our model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper a new family of implicit function for synthesizing a
wide variety of 3D surfaces. The basis of this family consists of the usual
functions that are: the function rectangular pulses, the function saw-tooth
pulses, the function of triangular pulses, the staircase function and the power
function. By combining these common functions, named constituent functions, in
one implicit function and by varying some parameters of this function we can
synthesize a wide variety of 3D surfaces with the possibility to set their
deformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03978</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03978</id><created>2015-05-15</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Freear</keyname><forenames>Steven</forenames></author></authors><title>On the $\eta-\mu$/gamma and the $\lambda-\mu$/gamma Composite
  Distributions</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1505.03779</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to the formulation and derivation of the
$\eta{-}\mu{/}$gamma and $\lambda{-}\mu{/}$gamma distributions which correspond
to physical fading models. These distributions are composite and are based on
the $\eta-\mu$ and $\lambda-\mu$ generalized multipath models, respectively,
and the gamma shadowing model. Novel analytic expressions are derived for the
corresponding envelope probability density functions. Importantly, the proposed
models provide accurate characterisation of the simultaneous occurrence of
multipath fading and shadowing effects which is achieved thanks to the
remarkable flexibility offered by their parameters that render them capable of
providing good fittings to experimental data associated with realistic
communication scenarios. This is additionally justified by the fact that they
include as special cases the widely known fading models such as Hoyt/gamma,
Nakagami-m/gamma and Rayleigh/gamma. As a result, they can be meaningfully
utilized in various analytical studies related to the performance evaluation of
digital communications over composite multipath/shadowing fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03984</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03984</id><created>2015-05-15</created><authors><author><keyname>Zhang</keyname><forenames>Xiaoming</forenames></author><author><keyname>Li</keyname><forenames>Zhoujun</forenames></author><author><keyname>Wang</keyname><forenames>Senzhang</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Lv</keyname><forenames>Xueqiang</forenames></author></authors><title>Location Prediction of Social Images via Generative Model</title><categories>cs.IR</categories><comments>8 pages</comments><acm-class>H.3.1</acm-class><doi>10.1145/2671188.2749308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vast amount of geo-tagged social images has attracted great attention in
research of predicting location using the plentiful content of images, such as
visual content and textual description. Most of the existing researches use the
text-based or vision-based method to predict location. There still exists a
problem: how to effectively exploit the correlation between different types of
content as well as their geographical distributions for location prediction. In
this paper, we propose to predict image location by learning the latent
relation between geographical location and multiple types of image content. In
particularly, we propose a geographical topic model GTMI (geographical topic
model of social image) to integrate multiple types of image content as well as
the geographical distributions, In GTMI, image topic is modeled on both text
vocabulary and visual feature. Each region has its own distribution over topics
and hence has its own language model and vision pattern. The location of a new
image is estimated based on the joint probability of image content and
similarity measure on topic distribution between images. Experiment results
demonstrate the performance of location prediction based on GTMI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03989</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03989</id><created>2015-05-15</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Freear</keyname><forenames>Steven</forenames></author></authors><title>Solutions to the Incomplete Toronto Function and Incomplete
  Lipschitz-Hankel Integrals</title><categories>cs.IT math.IT</categories><comments>12 pages. arXiv admin note: text overlap with arXiv:1112.4076</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides novel analytic expressions for the incomplete Toronto
function, $T_{B}(m,n,r)$, and the incomplete Lipschitz-Hankel Integrals of the
modified Bessel function of the first kind, $Ie_{m,n}(a,z)$. These expressions
are expressed in closed-form and are valid for the case that $n$ is an odd
multiple of $1/2$, i.e. $n \pm 0.5\in\mathbb{N}$. Capitalizing on these, tight
upper and lower bounds are subsequently proposed for both $T_{B}(m,n,r)$
function and $Ie_{m,n}(a,z)$ integrals. Importantly, all new representations
are expressed in closed-form whilst the proposed bounds are shown to be rather
tight. To this effect, they can be effectively exploited in various analytical
studies related to wireless communication theory. Indicative applications
include, among others, the performance evaluation of digital communications
over fading channels and the information-theoretic analysis of multiple-input
multiple-output systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03991</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03991</id><created>2015-05-15</created><authors><author><keyname>Ayuev</keyname><forenames>Boris I.</forenames></author><author><keyname>Davydov</keyname><forenames>Viktor V.</forenames></author><author><keyname>Erokhin</keyname><forenames>Petr M.</forenames></author></authors><title>Nonlinear-Programming-Based Model of Power System Marginal States:
  Theoretical Substantiation</title><categories>cs.SY math.OC</categories><comments>9 pages, 1 figure, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to maintain the security of power system at an appropriate level and
at low cost, it is essential to accurately assess the steady-state stability
limits and power flow feasibility boundaries, i.e., the power system marginal
states (MS). This paper is devoted to creation and theoretical substantiation
of the MS model based on nonlinear programming (NLP-MS model), its research to
reveal MS properties which promote better MS understanding, to evolution of the
theory of power systems and MS, to elaboration of more effective algorithms of
MS problem solution. The proposed NLP-MS model is universal and allows to
determine and to take into account various MS, including the MS in a given
direction of power change, the closest MS, a moving the power system state into
a power flow feasibility region, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03996</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03996</id><created>2015-05-15</created><authors><author><keyname>Criado</keyname><forenames>Natalia</forenames></author><author><keyname>Such</keyname><forenames>Jose M.</forenames></author></authors><title>Norm Monitoring under Partial Action Observability</title><categories>cs.MA cs.AI</categories><doi>10.1109/TCYB.2015.2513430</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of using norms for controlling multi-agent systems, a vitally
important question that has not yet been addressed in the literature is the
development of mechanisms for monitoring norm compliance under partial action
observability. This paper proposes the reconstruction of unobserved actions to
tackle this problem. In particular, we formalise the problem of reconstructing
unobserved actions, and propose an information model and algorithms for
monitoring norms under partial action observability using two different
processes for reconstructing unobserved actions. Our evaluation shows that
reconstructing unobserved actions increases significantly the number of norm
violations and fulfilments detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.03998</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.03998</id><created>2015-05-15</created><authors><author><keyname>Chhun</keyname><forenames>Sophea</forenames></author><author><keyname>Cherifi</keyname><forenames>Chantal</forenames></author><author><keyname>Moalla</keyname><forenames>Nejib</forenames></author><author><keyname>Ouzrout</keyname><forenames>Yacine</forenames></author></authors><title>A multi-criteria service selection algorithm for business process
  requirements</title><categories>cs.SE</categories><comments>15 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The selection of the most appropriate Web services to realize business tasks
still remain an open issue. We propose a multi-criteria algorithm for efficient
service selection. Web services and their QoS values are stored in a Web
service ontology (WSOnto) and business processes are modeled with the BPMN2.0
specifications. Our algorithm performs an instance-based ontology matching
between the WSOnto and the business process ontology. The business context,
functional properties and QoS values of Web services are considered. The
algorithm computes the variation of QoS values over times. This strategy allows
better accurate Web services ranking relevant to a user's request.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04000</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04000</id><created>2015-05-15</created><authors><author><keyname>Celani</keyname><forenames>Fabio</forenames></author></authors><title>Spacecraft Attitude Stabilization with Piecewise-constant Magnetic
  Dipole Moment</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1411.2756</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In actual implementations of magnetic control laws for spacecraft attitude
stabilization, the time in which Earth magnetic field is measured must be
separated from the time in which magnetic dipole moment is generated. The
latter separation translates into the constraint of being able to genere only
piecewise-constant magnetic dipole moment. In this work we present attitude
stabilization laws using only magnetic actuators that take into account of the
latter aspect. Both a state feedback and an output feedback are presented, and
it is shown that the proposed design allows for a systematic selection of the
sampling period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04005</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04005</id><created>2015-05-15</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Freear</keyname><forenames>Steven</forenames></author></authors><title>Tight Approximations for the Two Dimensional Gaussian $Q-$function</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is the derivation of two approximated expressions for
the two dimensional Gaussian Q-function, $Q(x,y;\rho)$. These expressions are
highly accurate and are expressed in closed-form. Furthermore, their algebraic
representation is relatively simple and therefore, convenient to handle both
analytically and numerically. This feature is particularly useful for two
reasons: firstly because it renders the derived expressions useful mathematical
tools that can be utilized in numerous analytic performance evaluation studies
in digital communications under fading; secondly because the two dimensional
Gaussian Q-function is neither tabulated nor a built-in function in popular
mathematical software packages such as $Maple$, $Mathematica$ and $Matlab$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04019</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04019</id><created>2015-05-15</created><updated>2015-09-17</updated><authors><author><keyname>Brankovic</keyname><forenames>Ljiljana</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Costas S.</forenames></author><author><keyname>Kundu</keyname><forenames>Ritu</forenames></author><author><keyname>Mohamed</keyname><forenames>Manal</forenames></author><author><keyname>Pissis</keyname><forenames>Solon P.</forenames></author><author><keyname>Vayani</keyname><forenames>Fatima</forenames></author></authors><title>Linear-Time Superbubble Identification Algorithm for Genome Assembly</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DNA sequencing is the process of determining the exact order of the
nucleotide bases of an individual's genome in order to catalogue sequence
variation and understand its biological implications. Whole-genome sequencing
techniques produce masses of data in the form of short sequences known as
reads. Assembling these reads into a whole genome constitutes a major
algorithmic challenge. Most assembly algorithms utilize de Bruijn graphs
constructed from reads for this purpose. A critical step of these algorithms is
to detect typical motif structures in the graph caused by sequencing errors and
genome repeats, and filter them out; one such complex subgraph class is a
so-called superbubble. In this paper, we propose an O(n+m)-time algorithm to
detect all superbubbles in a directed acyclic graph with n nodes and m
(directed) edges, improving the best-known O(m log m)-time algorithm by Sung et
al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04026</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04026</id><created>2015-05-15</created><authors><author><keyname>Happy</keyname><forenames>S L</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>Automatic Facial Expression Recognition Using Features of Salient Facial
  Patches</title><categories>cs.CV</categories><journal-ref>IEEE Transactions on Affective Computing, vol. 6, no. 1, pp. 1-12,
  2015</journal-ref><doi>10.1109/TAFFC.2014.2386334</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extraction of discriminative features from salient facial patches plays a
vital role in effective facial expression recognition. The accurate detection
of facial landmarks improves the localization of the salient patches on face
images. This paper proposes a novel framework for expression recognition by
using appearance features of selected facial patches. A few prominent facial
patches, depending on the position of facial landmarks, are extracted which are
active during emotion elicitation. These active patches are further processed
to obtain the salient patches which contain discriminative features for
classification of each pair of expressions, thereby selecting different facial
patches as salient for different pair of expression classes. One-against-one
classification method is adopted using these features. In addition, an
automated learning-free facial landmark detection technique has been proposed,
which achieves similar performances as that of other state-of-art landmark
detection methods, yet requires significantly less execution time. The proposed
method is found to perform well consistently in different resolutions, hence,
providing a solution for expression recognition in low resolution images.
Experiments on CK+ and JAFFE facial expression databases show the effectiveness
of the proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04028</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04028</id><created>2015-05-15</created><updated>2015-05-20</updated><authors><author><keyname>Kayaoglu</keyname><forenames>Mehmet</forenames></author><author><keyname>Topcu</keyname><forenames>Berkay</forenames></author><author><keyname>Uludag</keyname><forenames>Umut</forenames></author></authors><title>Biometric Matching and Fusion System for Fingerprints from Non-Distal
  Phalanges</title><categories>cs.CV</categories><comments>22 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Market research indicates that fingerprints are still the most popular
biometric modality for personal authentication. Even with the onset of new
modalities (e.g. vein matching), many applications within different domains
(e-ID, banking, border control...) and geographies rely on fingerprints
obtained from the distal phalanges (a.k.a. sections, digits) of the human hand
structure. Motivated by the problem of poor quality distal fingerprint images
affecting a non-trivial portion of the population (which decreases associated
authentication accuracy), we designed and tested a multifinger, multiphalanx
fusion scheme, that combines minutiae matching scores originating from
non-distal (ie. middle and proximal) phalanges based on (i) simple sum fusion,
(ii) NFIQ image-quality-based fusion, and (iii) phalanx-type-based fusion.
Utilizing a medium-size (50 individuals, 400 unique fingers, 1600 distinct
images) database collected in our laboratory with a commercial optical
fingerprint sensor, and a commercial minutiae extractor &amp; matcher (without any
modification), allowed us to simulate a real-world fingerprint authentication
setting. Detailed analyses including ROC curves with statistical confidence
intervals show that the proposed system can be a viable alternative for cases
where (i) distal phalanx images are not usable (e.g. due to missing digits, or
low quality finger surface due to manual labor), and (ii) switching to a new
biometric modality (e.g. iris) is not possible due to economical or
infrastructure limits. Further, we show that when distal phalanx images are in
fact usable, combining them with images from other phalanges increases accuracy
as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04030</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04030</id><created>2015-05-15</created><authors><author><keyname>Happy</keyname><forenames>S. L.</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>Robust Facial Expression Classification Using Shape and Appearance
  Features</title><categories>cs.CV</categories><comments>Proceedings of 8th International Conference of Advances in Pattern
  Recognition, 2015</comments><doi>10.1109/ICAPR.2015.7050661</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial expression recognition has many potential applications which has
attracted the attention of researchers in the last decade. Feature extraction
is one important step in expression analysis which contributes toward fast and
accurate expression recognition. This paper represents an approach of combining
the shape and appearance features to form a hybrid feature vector. We have
extracted Pyramid of Histogram of Gradients (PHOG) as shape descriptors and
Local Binary Patterns (LBP) as appearance features. The proposed framework
involves a novel approach of extracting hybrid features from active facial
patches. The active facial patches are located on the face regions which
undergo a major change during different expressions. After detection of facial
landmarks, the active patches are localized and hybrid features are calculated
from these patches. The use of small parts of face instead of the whole face
for extracting features reduces the computational cost and prevents the
over-fitting of the features for classification. By using linear discriminant
analysis, the dimensionality of the feature is reduced which is further
classified by using the support vector machine (SVM). The experimental results
on two publicly available databases show promising accuracy in recognizing all
expression classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04033</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04033</id><created>2015-05-15</created><authors><author><keyname>Czenky</keyname><forenames>M&#xe1;rta</forenames></author></authors><title>An Examination of the Effectiveness of Teaching Data Modelling Concepts</title><categories>cs.CY cs.DB</categories><acm-class>H.2.1; K.3.2</acm-class><journal-ref>International Journal of Database Management Systems ( IJDMS )
  Vol.7, No.2, April 2015</journal-ref><doi>10.5121/ijdms.2015.7202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effective teaching of data modelling concepts is very important; it
constitutes the fundament of database planning methods and the handling of
databases with the help of database management lan-guages, typically SQL. We
examined three courses. The students of two courses prepared for the exam by
solving tests, while the students of the third course prepared by solving tasks
from a printed exercise book. The number of task for the second course was 2.5
times more than the number of task for the first course. The main purpose of
our examination was to determine the effectiveness of the teaching of data
modelling concepts, and to decide if there is a significant difference between
the results of the three courses. According to our examination, with increasing
the number of test tasks and with the use of exercise book, the results became
significantly better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04036</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04036</id><created>2015-05-15</created><authors><author><keyname>Gawryluk</keyname><forenames>Krzysztof</forenames></author><author><keyname>Karpiuk</keyname><forenames>Tomasz</forenames></author><author><keyname>Gajda</keyname><forenames>Mariusz</forenames></author><author><keyname>Rzazewski</keyname><forenames>Kazimierz</forenames></author><author><keyname>Brewczyk</keyname><forenames>Miroslaw</forenames></author></authors><title>Extended Split Operator method as an efficient way for computing
  dynamics of a spinor F=1 Bose-Einstein condensate</title><categories>cs.CE cond-mat.quant-gas</categories><comments>13pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a very efficient way of solving a spinor version of a
time-dependent Gross-Pitaevskii equation (GP). The method we use is a
modification of the well known Split Operator Method (SOM) - applied to the
case of $F=1$ spinor Bose-Einstein condensates. We focus here on $F=1$ case for
an educational purpose only - the extension to higher values of $F$ is
straightforward. Our extension of the SOM to the spinor version has many
advantages: it is fast, stable, and keeps constant all the physical constraints
(constants of motion) at very high level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04041</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04041</id><created>2015-05-15</created><authors><author><keyname>Singh</keyname><forenames>Bikramjit</forenames></author></authors><title>Repeated Games for Inter-operator Spectrum Sharing</title><categories>cs.NI</categories><comments>Master Thesis, Published on 25/8/2014, Aalto University
  (https://aaltodoc.aalto.fi/handle/123456789/13922)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As wireless communication becomes an ever-more evolving and pervasive part of
the existing world, system capacity and Quality of Service (QoS) provisioning
are becoming more critically evident. In order to improve system capacity and
QoS, it is mandatory that we pay closer attention to operational bandwidth
efficiency issues. We address this issue for two operators' spectrum sharing in
the same geographical area.
  We model and analyze interactions between the competitive operators
coexisting in the same frequency band as a strategic noncooperative game, where
the operators simultaneously share the spectrum dynamically as per their
relative requirement. If resources are allocated in a conventional way (static
orthogonal allocation), spectrum utilization becomes inefficient when there is
load asymmetry between the operators and low inter-operator interference.
  Theoretically, operators can share resources in a cooperative manner, but
pragmatically they are reluctant to reveal their network information to
competitors. By using game theory, we design a distributed implementation, in
which self-interested operators play strategies and contend for the spectrum
resources in a noncooperative manner. We have proposed two game theoretic
approaches in the thesis, one using a virtual carrier price; and the other
based on a mutual history of favors. The former approach takes into account a
penalty proportional to spectrum usage in its utility function, whereas in the
latter, operators play strategies based on their history of interactions, i.e.,
how well the other behaved in the past. Finally, based on the simulations, we
assess the performance of the proposed game theoretic approaches in comparison
to existing conventional allocations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04055</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04055</id><created>2015-05-15</created><authors><author><keyname>Happy</keyname><forenames>S L</forenames></author><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>George</keyname><forenames>Anjith</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>A Video Database of Human Faces under Near Infra-Red Illumination for
  Human Computer Interaction Aplications</title><categories>cs.CV</categories><journal-ref>IEEE Proceedings of 4th International Conference on Intelligent
  Human Computer Interaction, 2012</journal-ref><doi>10.1109/IHCI.2012.6481868</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human Computer Interaction (HCI) is an evolving area of research for coherent
communication between computers and human beings. Some of the important
applications of HCI as reported in literature are face detection, face pose
estimation, face tracking and eye gaze estimation. Development of algorithms
for these applications is an active field of research. However, availability of
standard database to validate such algorithms is insufficient. This paper
discusses the creation of such a database created under Near Infra-Red (NIR)
illumination. NIR illumination has gained its popularity for night mode
applications since prolonged exposure to Infra-Red (IR) lighting may lead to
many health issues. The database contains NIR videos of 60 subjects in
different head orientations and with different facial expressions, facial
occlusions and illumination variation. This new database can be a very valuable
resource for development and evaluation of algorithms on face detection, eye
detection, head tracking, eye gaze tracking etc. in NIR lighting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04058</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04058</id><created>2015-05-15</created><authors><author><keyname>Happy</keyname><forenames>S. L.</forenames></author><author><keyname>George</keyname><forenames>Anjith</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>A Real Time Facial Expression Classification System Using Local Binary
  Patterns</title><categories>cs.CV</categories><comments>IEEE Proceedings of 4th International Conference on Intelligent Human
  Computer Interaction, 2012</comments><doi>10.1109/IHCI.2012.6481802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial expression analysis is one of the popular fields of research in human
computer interaction (HCI). It has several applications in next generation user
interfaces, human emotion analysis, behavior and cognitive modeling. In this
paper, a facial expression classification algorithm is proposed which uses Haar
classifier for face detection purpose, Local Binary Patterns (LBP) histogram of
different block sizes of a face image as feature vectors and classifies various
facial expressions using Principal Component Analysis (PCA). The algorithm is
implemented in real time for expression classification since the computational
complexity of the algorithm is small. A customizable approach is proposed for
facial expression analysis, since the various expressions and intensity of
expressions vary from person to person. The system uses grayscale frontal face
images of a person to classify six basic emotions namely happiness, sadness,
disgust, fear, surprise and anger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04073</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04073</id><created>2015-05-15</created><authors><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Safe Screening for Multi-Task Feature Learning with Multiple Data
  Matrices</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task feature learning (MTFL) is a powerful technique in boosting the
predictive performance by learning multiple related
classification/regression/clustering tasks simultaneously. However, solving the
MTFL problem remains challenging when the feature dimension is extremely large.
In this paper, we propose a novel screening rule---that is based on the dual
projection onto convex sets (DPC)---to quickly identify the inactive
features---that have zero coefficients in the solution vectors across all
tasks. One of the appealing features of DPC is that: it is safe in the sense
that the detected inactive features are guaranteed to have zero coefficients in
the solution vectors across all tasks. Thus, by removing the inactive features
from the training phase, we may have substantial savings in the computational
cost and memory usage without sacrificing accuracy. To the best of our
knowledge, it is the first screening rule that is applicable to sparse models
with multiple data matrices. A key challenge in deriving DPC is to solve a
nonconvex problem. We show that we can solve for the global optimum efficiently
via a properly chosen parametrization of the constraint set. Moreover, DPC has
very low computational cost and can be integrated with any existing solvers. We
have evaluated the proposed DPC rule on both synthetic and real data sets. The
experiments indicate that DPC is very effective in identifying the inactive
features---especially for high dimensional data---which leads to a speedup up
to several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04077</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04077</id><created>2015-05-15</created><updated>2015-07-27</updated><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Shu</keyname><forenames>Panpan</forenames></author><author><keyname>Zhu</keyname><forenames>Yu-Xiao</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Dynamics of social contagions with limited contact capacity</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 6 figures</comments><report-no>CHAOS 25, 103102 (2015)</report-no><doi>10.1063/1.4929761</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individuals are always limited by some inelastic resources, such as time and
energy, which restrict them to dedicate to social interaction and limit their
contact capacity. Contact capacity plays an important role in dynamics of
social contagions, which so far has eluded theoretical analysis. In this paper,
we first propose a non-Markovian model to understand the effects of contact
capacity on social contagions, in which each individual can only contact and
transmit the information to a finite number of neighbors. We then develop a
heterogeneous edge-based compartmental theory for this model, and a remarkable
agreement with simulations is obtained. Through theory and simulations, we find
that enlarging the contact capacity makes the network more fragile to behavior
spreading. Interestingly, we find that both the continuous and discontinuous
dependence of the final adoption size on the information transmission
probability can arise. And there is a crossover phenomenon between the two
types of dependence. More specifically, the crossover phenomenon can be induced
by enlarging the contact capacity only when the degree exponent is above a
critical degree exponent, while the the final behavior adoption size always
grows continuously for any contact capacity when degree exponent is below the
critical degree exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04085</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04085</id><created>2015-05-15</created><authors><author><keyname>Shah</keyname><forenames>Parikshit</forenames></author><author><keyname>Rao</keyname><forenames>Nikhil</forenames></author><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author></authors><title>Optimal Low-Rank Tensor Recovery from Separable Measurements: Four
  Contractions Suffice</title><categories>stat.ML cs.IT cs.LG math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensors play a central role in many modern machine learning and signal
processing applications. In such applications, the target tensor is usually of
low rank, i.e., can be expressed as a sum of a small number of rank one
tensors. This motivates us to consider the problem of low rank tensor recovery
from a class of linear measurements called separable measurements. As specific
examples, we focus on two distinct types of separable measurement mechanisms
(a) Random projections, where each measurement corresponds to an inner product
of the tensor with a suitable random tensor, and (b) the completion problem
where measurements constitute revelation of a random set of entries. We present
a computationally efficient algorithm, with rigorous and order-optimal sample
complexity results (upto logarithmic factors) for tensor recovery. Our method
is based on reduction to matrix completion sub-problems and adaptation of
Leurgans' method for tensor decomposition. We extend the methodology and sample
complexity results to higher order tensors, and experimentally validate our
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04086</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04086</id><created>2015-05-15</created><updated>2015-05-18</updated><authors><author><keyname>Rokos</keyname><forenames>Georgios</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard</forenames></author><author><keyname>Kelly</keyname><forenames>Paul H J</forenames></author></authors><title>A Fast and Scalable Graph Coloring Algorithm for Multi-core and
  Many-core Architectures</title><categories>cs.DC</categories><comments>To appear in the proceedings of Euro Par 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Irregular computations on unstructured data are an important class of
problems for parallel programming. Graph coloring is often an important
preprocessing step, e.g. as a way to perform dependency analysis for safe
parallel execution. The total run time of a coloring algorithm adds to the
overall parallel overhead of the application whereas the number of colors used
determines the amount of exposed parallelism. A fast and scalable coloring
algorithm using as few colors as possible is vital for the overall parallel
performance and scalability of many irregular applications that depend upon
runtime dependency analysis.
  Catalyurek et al. have proposed a graph coloring algorithm which relies on
speculative, local assignment of colors. In this paper we present an improved
version which runs even more optimistically with less thread synchronization
and reduced number of conflicts compared to Catalyurek et al.'s algorithm. We
show that the new technique scales better on multi-core and many-core systems
and performs up to 1.5x faster than its predecessor on graphs with high-degree
vertices, while keeping the number of colors at the same near-optimal levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04093</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04093</id><created>2015-05-15</created><authors><author><keyname>Schlesinger</keyname><forenames>M.</forenames></author><author><keyname>Vodolazskiy</keyname><forenames>E.</forenames></author><author><keyname>Yakovenko</keyname><forenames>V.</forenames></author></authors><title>Frechet similarity of closed polygonal curves</title><categories>cs.CG</categories><comments>13 pages. arXiv admin note: text overlap with arXiv:1409.4613</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article analyzes similarity of closed polygonal curves with respect to
the Frechet metric, which is stronger than the well-known Hausdorff metric and
therefore is more appropriate in some applications. An algorithm is described
that determines whether the Frechet distance between two closed polygonal
curves with m and n vertices is less than a given number. The algorithm takes
O(mn) time whereas the previously known algorithms take O(mn log(mn)) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04094</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04094</id><created>2015-05-15</created><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Lichtenwalter</keyname><forenames>Ryan N.</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Evaluating Link Prediction Methods</title><categories>cs.IR cs.DB cs.SI</categories><acm-class>H.2.8</acm-class><doi>10.1007/s10115-014-0789-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction is a popular research area with important applications in a
variety of disciplines, including biology, social science, security, and
medicine. The fundamental requirement of link prediction is the accurate and
effective prediction of new links in networks. While there are many different
methods proposed for link prediction, we argue that the practical performance
potential of these methods is often unknown because of challenges in the
evaluation of link prediction, which impact the reliability and reproducibility
of results. We describe these challenges, provide theoretical proofs and
empirical examples demonstrating how current methods lead to questionable
conclusions, show how the fallacy of these conclusions is illuminated by
methods we propose, and develop recommendations for consistent, standard, and
applicable evaluation metrics. We also recommend the use of precision-recall
threshold curves and associated areas in lieu of receiver operating
characteristic curves due to complications that arise from extreme imbalance in
the link prediction classification problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04095</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04095</id><created>2015-05-15</created><authors><author><keyname>Morales</keyname><forenames>A. J.</forenames></author><author><keyname>Borondo</keyname><forenames>J.</forenames></author><author><keyname>Losada</keyname><forenames>J. C.</forenames></author><author><keyname>Benito</keyname><forenames>R. M.</forenames></author></authors><title>Measuring Political Polarization: Twitter shows the two sides of
  Venezuela</title><categories>physics.soc-ph cs.CY cs.SI</categories><journal-ref>Chaos 25, 033114 (2015)</journal-ref><doi>10.1063/1.4913758</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We say that a population is perfectly polarized when divided in two groups of
the same size and opposite opinions. In this paper, we propose a methodology to
study and measure the emergence of polarization from social interactions. We
begin by proposing a model to estimate opinions in which a minority of
influential individuals propagate their opinions through a social network. The
result of the model is an opinion probability density function. Next, we
propose an index to quantify the extent to which the resulting distribution is
polarized. Finally, we apply the proposed methodology to a Twitter conversation
about the late Venezuelan president, Hugo Ch\'avez, finding a good agreement
between our results and offline data. Hence, we show that our methodology can
detect different degrees of polarization, depending on the structure of the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04097</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04097</id><created>2015-05-15</created><authors><author><keyname>Hong</keyname><forenames>Charmgil</forenames></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author></authors><title>MCODE: Multivariate Conditional Outlier Detection</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Outlier detection aims to identify unusual data instances that deviate from
expected patterns. The outlier detection is particularly challenging when
outliers are context dependent and when they are defined by unusual
combinations of multiple outcome variable values. In this paper, we develop and
study a new conditional outlier detection approach for multivariate outcome
spaces that works by (1) transforming the conditional detection to the outlier
detection problem in a new (unconditional) space and (2) defining outlier
scores by analyzing the data in the new space. Our approach relies on the
classifier chain decomposition of the multi-dimensional classification problem
that lets us transform the output space into a probability vector, one
probability for each dimension of the output space. Outlier scores applied to
these transformed vectors are then used to detect the outliers. Experiments on
multiple multi-dimensional classification problems with the different outlier
injection rates show that our methodology is robust and able to successfully
identify outliers when outliers are either sparse (manifested in one or very
few dimensions) or dense (affecting multiple dimensions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04098</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04098</id><created>2015-05-15</created><authors><author><keyname>Hauser</keyname><forenames>Kris</forenames></author><author><keyname>Zhou</keyname><forenames>Yilun</forenames></author></authors><title>Asymptotically Optimal Planning by Feasible Kinodynamic Planning in
  State-Cost Space</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an equivalence between feasible kinodynamic planning and
optimal kinodynamic planning, in that any optimal planning problem can be
transformed into a series of feasible planning problems in a state-cost space
whose solutions approach the optimum. This transformation gives rise to a
meta-algorithm that produces an asymptotically optimal planner, given any
feasible kinodynamic planner as a subroutine. The meta-algorithm is proven to
be asymptotically optimal, and a formula is derived relating expected running
time and solution suboptimality. It is directly applicable to a wide range of
optimal planning problems because it does not resort to the use of steering
functions or numerical boundary-value problem solvers. On a set of benchmark
problems, it is demonstrated to perform, using the EST and RRT algorithms as
subroutines, at a superior or comparable level to related planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04103</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04103</id><created>2015-05-15</created><authors><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>A splitting scheme to solve an equation for fractional powers of
  elliptic operators</title><categories>cs.NA math.NA</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An equation containing a fractional power of an elliptic operator of second
order is studied for Dirichlet boundary conditions. Finite difference
approximations in space are employed. The proposed numerical algorithm is based
on solving an auxiliary Cauchy problem for a pseudo-parabolic equation.
Unconditionally stable vector additive schemes (splitting schemes) are
constructed. Numerical results for a model problem in a rectangle calculated
using the splitting with respect to spatial variables are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04107</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04107</id><created>2015-05-15</created><authors><author><keyname>Kaladzavi</keyname><forenames>Guidedi</forenames></author><author><keyname>Diallo</keyname><forenames>Papa Fary</forenames></author><author><keyname>Kolyang</keyname></author><author><keyname>Lo</keyname><forenames>Moussa</forenames></author></authors><title>OntoSOC: Sociocultural Knowledge Ontology</title><categories>cs.AI</categories><comments>8 pages, 5 figures, 2 tables</comments><journal-ref>IJWesT Vol. 6, No. 2 (2015)</journal-ref><doi>10.5121/ijwest.2015.6201</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a sociocultural knowledge ontology (OntoSOC) modeling
approach. OntoSOC modeling approach is based on Engestrom Human Activity Theory
(HAT). That Theory allowed us to identify fundamental concepts and
relationships between them. The top-down precess has been used to define
differents sub-concepts. The modeled vocabulary permits us to organise data, to
facilitate information retrieval by introducing a semantic layer in social web
platform architecture, we project to implement. This platform can be considered
as a collective memory and Participative and Distributed Information System
(PDIS) which will allow Cameroonian communities to share an co-construct
knowledge on permanent organized activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04112</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04112</id><created>2015-05-15</created><authors><author><keyname>Warrender</keyname><forenames>Jennifer D.</forenames></author><author><keyname>Lord</keyname><forenames>Phillip</forenames></author></authors><title>How, What and Why to test an ontology</title><categories>cs.AI cs.CE</categories><comments>4 pages, accepted at Bio-Ontologies 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology development relates to software development in that they both
involve the production of formal computational knowledge. It is possible,
therefore, that some of the techniques used in software engineering could also
be used for ontologies; for example, in software engineering testing is a
well-established process, and part of many different methodologies.
  The application of testing to ontologies, therefore, seems attractive. The
Karyotype Ontology is developed using the novel Tawny-OWL library. This
provides a fully programmatic environment for ontology development, which
includes a complete test harness.
  In this paper, we describe how we have used this harness to build an
extensive series of tests as well as used a commodity continuous integration
system to link testing deeply into our development process; this environment,
is applicable to any OWL ontology whether written using Tawny-OWL or not.
Moreover, we present a novel analysis of our tests, introducing a new
classification of what our different tests are. For each class of test, we
describe why we use these tests, also by comparison to software tests. We
believe that this systematic comparison between ontology and software
development will help us move to a more agile form of ontology development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04114</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04114</id><created>2015-05-15</created><authors><author><keyname>Warrender</keyname><forenames>Jennifer D.</forenames></author><author><keyname>Lord</keyname><forenames>Phillip</forenames></author></authors><title>Scaffolding the Mitochondrial Disease Ontology from extant knowledge
  sources</title><categories>cs.CE</categories><comments>5 pages, 1 figure, accepted at ICBO 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bio-medical ontologies can contain a large number of concepts. Often many of
these concepts are very similar to each other, and similar or identical to
concepts found in other bio-medical databases. This presents both a challenge
and opportunity: maintaining many similar concepts is tedious and fastidious
work, which could be substantially reduced if the data could be derived from
pre-existing knowledge sources. In this paper, we describe how we have achieved
this for an ontology of the mitochondria using our novel ontology development
environment, the Tawny-OWL library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04117</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04117</id><created>2015-05-15</created><authors><author><keyname>Kovashka</keyname><forenames>Adriana</forenames></author><author><keyname>Grauman</keyname><forenames>Kristen</forenames></author></authors><title>Discovering Attribute Shades of Meaning with the Crowd</title><categories>cs.CV</categories><comments>Published in the International Journal of Computer Vision (IJCV),
  January 2015. The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11263-014-0798-1</comments><journal-ref>International Journal of Computer Vision 1573-1405 (2015,
  Springer)</journal-ref><doi>10.1007/s11263-014-0798-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To learn semantic attributes, existing methods typically train one
discriminative model for each word in a vocabulary of nameable properties.
However, this &quot;one model per word&quot; assumption is problematic: while a word
might have a precise linguistic definition, it need not have a precise visual
definition. We propose to discover shades of attribute meaning. Given an
attribute name, we use crowdsourced image labels to discover the latent factors
underlying how different annotators perceive the named concept. We show that
structure in those latent factors helps reveal shades, that is, interpretations
for the attribute shared by some group of annotators. Using these shades, we
train classifiers to capture the primary (often subtle) variants of the
attribute. The resulting models are both semantic and visually precise. By
catering to users' interpretations, they improve attribute prediction accuracy
on novel images. Shades also enable more successful attribute-based image
search, by providing robust personalized models for retrieving multi-attribute
query results. They are widely applicable to tasks that involve describing
visual content, such as zero-shot category learning and organization of photo
collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04123</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04123</id><created>2015-05-15</created><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Pe&#xf1;a</keyname><forenames>Javier</forenames></author></authors><title>Margins, Kernels and Non-linear Smoothed Perceptrons</title><categories>cs.LG cs.AI cs.NA math.OC</categories><comments>17 pages, published in the proceedings of the International
  Conference on Machine Learning, 2014</comments><journal-ref>Ramdas, Aaditya, and Javier Pena. &quot;Margins, kernels and non-linear
  smoothed perceptrons.&quot; Proceedings of the 31st International Conference on
  Machine Learning (ICML-14). 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the problem of finding a non-linear classification function that
lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of
view (finding a perfect separator when one exists) and the dual point of view
(giving a certificate of non-existence), with special focus on generalizations
of two classical schemes - the Perceptron (primal) and Von-Neumann (dual)
algorithms.
  We cast our problem as one of maximizing the regularized normalized
hard-margin ($\rho$) in an RKHS and %use the Representer Theorem to rephrase it
in terms of a Mahalanobis dot-product/semi-norm associated with the kernel's
(normalized and signed) Gram matrix. We derive an accelerated smoothed
algorithm with a convergence rate of $\tfrac{\sqrt {\log n}}{\rho}$ given $n$
separable points, which is strikingly similar to the classical kernelized
Perceptron algorithm whose rate is $\tfrac1{\rho^2}$. When no such classifier
exists, we prove a version of Gordan's separation theorem for RKHSs, and give a
reinterpretation of negative margins. This allows us to give guarantees for a
primal-dual algorithm that halts in $\min\{\tfrac{\sqrt n}{|\rho|},
\tfrac{\sqrt n}{\epsilon}\}$ iterations with a perfect separator in the RKHS if
the primal is feasible or a dual $\epsilon$-certificate of near-infeasibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04134</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04134</id><created>2015-05-15</created><updated>2015-05-18</updated><authors><author><keyname>Rokos</keyname><forenames>Georgios</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard J.</forenames></author><author><keyname>Kelly</keyname><forenames>Paul H. J.</forenames></author></authors><title>An Interrupt-Driven Work-Sharing For-Loop Scheduler</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a parallel for-loop scheduler which is based on
work-stealing principles but runs under a completely cooperative scheme. POSIX
signals are used by idle threads to interrupt left-behind workers, which in
turn decide what portion of their workload can be given to the requester. We
call this scheme Interrupt-Driven Work-Sharing (IDWS). This article describes
how IDWS works, how it can be integrated into any POSIX-compliant OpenMP
implementation and how a user can manually replace OpenMP parallel for-loops
with IDWS in existing POSIX-compliant C++ applications. Additionally, we
measure its performance using both a synthetic benchmark with varying
distributions of workload across the iteration space and a real-life
application on Sandy Bridge and Xeon Phi systems. Regardless the workload
distribution and the underlying hardware, IDWS is always the best or among the
best-performing strategies, providing a good all-around solution to the
scheduling-choice dilemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04137</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04137</id><created>2015-05-15</created><authors><author><keyname>Ramaswamy</keyname><forenames>Harish G.</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Agarwal</keyname><forenames>Shivani</forenames></author></authors><title>Consistent Algorithms for Multiclass Classification with a Reject Option</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of $n$-class classification ($n\geq 2$), where the
classifier can choose to abstain from making predictions at a given cost, say,
a factor $\alpha$ of the cost of misclassification. Designing consistent
algorithms for such $n$-class classification problems with a `reject option' is
the main goal of this paper, thereby extending and generalizing previously
known results for $n=2$. We show that the Crammer-Singer surrogate and the one
vs all hinge loss, albeit with a different predictor than the standard argmax,
yield consistent algorithms for this problem when $\alpha=\frac{1}{2}$. More
interestingly, we design a new convex surrogate that is also consistent for
this problem when $\alpha=\frac{1}{2}$ and operates on a much lower dimensional
space ($\log(n)$ as opposed to $n$). We also generalize all three surrogates to
be consistent for any $\alpha\in[0, \frac{1}{2}]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04140</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04140</id><created>2015-05-15</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author><author><keyname>Kauffman</keyname><forenames>A. N.</forenames></author></authors><title>Efficient Multiplex for Band-Limited Channels: Galois-Field Division
  Multiple Access</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, in: Workshop on Coding and Cryptography, INRIA,
  1999, Paris. pp.235-241. arXiv admin note: text overlap with arXiv:1502.05881</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new Efficient-bandwidth code-division-multiple-access (CDMA) for
band-limited channels is introduced which is based on finite field transforms.
A multilevel code division multiplex exploits orthogonality properties of
nonbinary sequences defined over a complex finite field. Galois-Fourier
transforms contain some redundancy and just cyclotomic coefficients are needed
to be transmitted yielding compact spectrum requirements. The primary advantage
of such schemes regarding classical multiplex is their better spectral
efficiency. This paper estimates the \textit{bandwidth compactness factor}
relatively to Time Division Multiple Access TDMA showing that it strongly
depends on the alphabet extension. These multiplex schemes termed Galois
Division Multiplex (GDM) are based on transforms for which there exists fast
algorithms. They are also convenient from the implementation viewpoint since
they can be implemented by a Digital Signal Processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04141</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04141</id><created>2015-05-15</created><updated>2015-05-18</updated><authors><author><keyname>Kovashka</keyname><forenames>Adriana</forenames></author><author><keyname>Parikh</keyname><forenames>Devi</forenames></author><author><keyname>Grauman</keyname><forenames>Kristen</forenames></author></authors><title>WhittleSearch: Interactive Image Search with Relative Attribute Feedback</title><categories>cs.CV</categories><comments>Published in the International Journal of Computer Vision (IJCV),
  April 2015. The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11263-015-0814-0</comments><journal-ref>International Journal of Computer Vision, 1573-1405 (2015,
  Springer)</journal-ref><doi>10.1007/s11263-015-0814-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel mode of feedback for image search, where a user describes
which properties of exemplar images should be adjusted in order to more closely
match his/her mental model of the image sought. For example, perusing image
results for a query &quot;black shoes&quot;, the user might state, &quot;Show me shoe images
like these, but sportier.&quot; Offline, our approach first learns a set of ranking
functions, each of which predicts the relative strength of a nameable attribute
in an image (e.g., sportiness). At query time, the system presents the user
with a set of exemplar images, and the user relates them to his/her target
image with comparative statements. Using a series of such constraints in the
multi-dimensional attribute space, our method iteratively updates its relevance
function and re-ranks the database of images. To determine which exemplar
images receive feedback from the user, we present two variants of the approach:
one where the feedback is user-initiated and another where the feedback is
actively system-initiated. In either case, our approach allows a user to
efficiently &quot;whittle away&quot; irrelevant portions of the visual feature space,
using semantic language to precisely communicate her preferences to the system.
We demonstrate our technique for refining image search for people, products,
and scenes, and we show that it outperforms traditional binary relevance
feedback in terms of search speed and accuracy. In addition, the ordinal nature
of relative attributes helps make our active approach efficient -- both
computationally for the machine when selecting the reference images, and for
the user by requiring less user interaction than conventional passive and
active methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04142</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04142</id><created>2015-05-15</created><authors><author><keyname>Burgos</keyname><forenames>Andres C.</forenames></author><author><keyname>Polani</keyname><forenames>Daniel</forenames></author></authors><title>An informational study of the evolution of codes and of emerging
  concepts in populations of agents</title><categories>cs.MA</categories><comments>Accepted in Artificial Life Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of the evolution of a code within a structured
population of agents. The agents try to maximise their information about their
environment by acquiring information from the outputs of other agents in the
population. A naive use of information-theoretic methods would assume that
every agent knows how to &quot;interpret&quot; the information offered by other agents.
However, this assumes that one &quot;knows&quot; which other agents one observes, and
thus which code they use. In our model, however, we wish to preclude that: it
is not clear which other agents an agent is observing, and the resulting usable
information is therefore influenced by the universality of the code used and by
which agents an agent is &quot;listening&quot; to. We further investigate whether an
agent who does not directly perceive the environment can distinguish states by
observing other agents' outputs. For this purpose, we consider a population of
different types of agents &quot;talking&quot; about different concepts, and try to
extract new ones by considering their outputs only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04143</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04143</id><created>2015-05-15</created><authors><author><keyname>Bristow</keyname><forenames>Hilton</forenames></author><author><keyname>Valmadre</keyname><forenames>Jack</forenames></author><author><keyname>Lucey</keyname><forenames>Simon</forenames></author></authors><title>Dense Semantic Correspondence where Every Pixel is a Classifier</title><categories>cs.CV</categories><comments>ICCV 2015 Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining dense semantic correspondences across objects and scenes is a
difficult problem that underpins many higher-level computer vision algorithms.
Unlike canonical dense correspondence problems which consider images that are
spatially or temporally adjacent, semantic correspondence is characterized by
images that share similar high-level structures whose exact appearance and
geometry may differ.
  Motivated by object recognition literature and recent work on rapidly
estimating linear classifiers, we treat semantic correspondence as a
constrained detection problem, where an exemplar LDA classifier is learned for
each pixel. LDA classifiers have two distinct benefits: (i) they exhibit higher
average precision than similarity metrics typically used in correspondence
problems, and (ii) unlike exemplar SVM, can output globally interpretable
posterior probabilities without calibration, whilst also being significantly
faster to train.
  We pose the correspondence problem as a graphical model, where the unary
potentials are computed via convolution with the set of exemplar classifiers,
and the joint potentials enforce smoothly varying correspondence assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04148</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04148</id><created>2015-05-15</created><authors><author><keyname>van de Belt</keyname><forenames>Jonathan</forenames></author><author><keyname>Ahmadi</keyname><forenames>Hamed</forenames></author><author><keyname>Doyle</keyname><forenames>Linda E.</forenames></author><author><keyname>Sallent</keyname><forenames>Oriol</forenames></author></authors><title>A Prioritised Traffic Embedding Mechanism enabling a Public Safety
  Virtual Operator</title><categories>cs.NI</categories><comments>Accepted in VTC Fall 2015, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public Protection and Distaster Relief (PPDR) services can benefit greatly
from the availability of mobile broadband communications in disaster and
emergency scenarios. While undoubtedly offering full control and reliability,
dedicated networks for PPDR have resulted in high operating costs and a lack of
innovation in comparison to the commercial domain. Driven by the many benefits
of broadband communications, PPDR operators are increasingly interested in
adopting mainstream commercial technologies such as Long Term Evolution (LTE)
in favour of expensive, dedicated narrow-band networks.
  In addition, the emergence of virtualization for wireless networks offers a
new model for sharing infrastructure between several operators in a flexible
and customizable manner. In this context, we propose a virtual Public Safety
(PS) operator that relies on shared infrastructure of commercial LTE networks
to deliver services to its users. We compare several methods of allocating
spectrum resources between virtual operators at peak times and examine how this
influences differing traffic services. We show that it is possible to provide
services to the PS users reliably during both normal and emergency operation,
and examine the impact on the commercial operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04150</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04150</id><created>2015-05-15</created><authors><author><keyname>Wang</keyname><forenames>Zhipeng</forenames></author><author><keyname>Cai</keyname><forenames>Mingbo</forenames></author></authors><title>Reinforcement Learning applied to Single Neuron</title><categories>cs.AI cs.NE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper extends the reinforcement learning ideas into the multi-agents
system, which is far more complicated than the previously studied single-agent
system. We studied two different multi-agents systems. One is the
fully-connected neural network consists of multiple single neurons. Another one
is the simplified mechanical arm system which is controlled by multiple
neurons. We suppose that each neuron is like an agent and it can do Gibbs
sampling of the posterior probability of stimulus features. The policy is
optimized in a way that the cumulative global rewards are maximized. The
algorithm for the second system is based on the same idea but we incorporate
the physics model into the constraints. The simulation results show that for
the first system our algorithm converges well. For the second system it does
not converge well in a reasonable simulation time length. In summary, we took
the initial endeavor to study the reinforcement learning for multi-agents
system. The computational complexity is always an issue and significant amount
of works have to be done in order to better understand the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04157</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04157</id><created>2015-05-08</created><authors><author><keyname>S.</keyname><forenames>Vijaykumar</forenames></author><author><keyname>G.</keyname><forenames>Saravanakumar S.</forenames></author><author><keyname>Balamurugan</keyname><forenames>M.</forenames></author></authors><title>Unique Sense: Smart Computing Prototype</title><categories>cs.OH</categories><comments>6 Pages</comments><journal-ref>Elsevier Procedia Computer Science 50, 2015 Pages 223-228</journal-ref><doi>10.1016/j.procs.2015.04.056</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Unique sense: Smart computing prototype is a part of unique sense computing
architecture, which delivers alternate solution for todays computing
architecture. This computing is one step towards future generation needs, which
brings extended support to the ubiquitous environment. This smart computing
prototype is the light weight compact architecture which is designed to satisfy
all the needs of this society. The proposed solution is based on the hybrid
combination of cutting edge technologies and techniques from the various
layers. In addition it achieves low cost architecture and eco-friendly to meet
all the levels of peoples needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04186</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04186</id><created>2015-05-15</created><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Freear</keyname><forenames>Steven</forenames></author></authors><title>On the $\kappa$-$\mu$/Gamma Generalized Multipath/ Shadowing Fading
  Distribution</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1505.03779,
  arXiv:1505.03978</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to the formulation and derivation of the
$\kappa-\mu$/gamma distribution which corresponds to A physical fading model.
This distribution is composite and is based on the well known $\kappa-\mu$
generalized multipath model and the gamma shadowing model. A special case of
the derived model constitutes the $\kappa-\mu$ Extreme/gamma model which
accounts for severe multipath and shadowing effects. These models provide
accurate characterisation of the simultaneous occurrence of multipath fading
and shadowing effects. This is achieved thanks to the remarkable flexibility of
their named parameters which render them capable of providing good fittings to
experimental data associated with realistic communication scenarios. This is
additionally justified by the fact that they include as special cases the
widely known composite fading models such as Rice/gamma, Nakagami-m/gamma and
Rayleigh/gamma. Novel analytic expressions are derived for the envelope and
power probability density function of these distributions which are expressed
in a relatively simple algebraic form which is convenient to handle both
analytically and numerically. As a result, they can be meaningfully utilized in
the derivation of numerous vital measures in investigations related to the
analytic performance evaluation of digital communications over composite
multipath/shadowing fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04197</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04197</id><created>2015-05-15</created><authors><author><keyname>Elmadany</keyname><forenames>AbdelRahim A.</forenames></author><author><keyname>Abdou</keyname><forenames>Sherif M.</forenames></author><author><keyname>Gheith</keyname><forenames>Mervat</forenames></author></authors><title>Arabic Inquiry-Answer Dialogue Acts Annotation Schema</title><categories>cs.CL</categories><comments>IOSR Journal of Engineering (IOSRJEN),Vol. 04, Issue 12 (December
  2014),V2. arXiv admin note: text overlap with arXiv:1505.03084</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an annotation schema as part of an effort to create a manually
annotated corpus for Arabic dialogue language understanding including spoken
dialogue and written &quot;chat&quot; dialogue for inquiry-answer domain. The proposed
schema handles mainly the request and response acts that occurs frequently in
inquiry-answer debate conversations expressing request services, suggests, and
offers. We applied the proposed schema on 83 Arabic inquiry-answer dialogues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04198</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04198</id><created>2015-05-15</created><authors><author><keyname>Besser</keyname><forenames>Bert</forenames></author><author><keyname>Poloczek</keyname><forenames>Matthias</forenames></author></authors><title>Greedy Matching: Guarantees and Limitations</title><categories>cs.DS</categories><comments>29 pages, 8 figures</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since Tinhofer proposed the MinGreedy algorithm for maximum cardinality
matching in 1984, several experimental studies found the randomized algorithm
to perform excellently for various classes of random graphs and benchmark
instances. In contrast, only few analytical results are known. We show that
MinGreedy cannot improve on the trivial approximation ratio 1/2 whp., even for
bipartite graphs. Our hard inputs seem to require a small number of high-degree
nodes.
  This motivates an investigation of greedy algorithms on graphs with maximum
degree D: We show that MinGreedy achieves a (D-1)/(2D-3)-approximation for
graphs with D=3 and for D-regular graphs, and a guarantee of (D-1/2)/(2D-2) for
graphs with maximum degree D. Interestingly, our bounds even hold for the
deterministic MinGreedy that breaks all ties arbitrarily.
  Moreover, we investigate the limitations of the greedy paradigm, using the
model of priority algorithms introduced by Borodin, Nielsen, and Rackoff. We
study deterministic priority algorithms and prove a
(D-1)/(2D-3)-inapproximability result for graphs with maximum degree D; thus,
these greedy algorithms do not achieve a 1/2+eps-approximation and in
particular the 2/3-approximation obtained by the deterministic MinGreedy for
D=3 is optimal in this class. For k-uniform hypergraphs we show a tight
1/k-inapproximability bound. We also study fully randomized priority algorithms
and give a 5/6-inapproximability bound. Thus, they cannot compete with matching
algorithms of other paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04202</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04202</id><created>2015-05-15</created><updated>2015-09-06</updated><authors><author><keyname>Boyle</keyname><forenames>Bradford D.</forenames></author><author><keyname>Ren</keyname><forenames>Jie</forenames></author><author><keyname>Walsh</keyname><forenames>John MacLaren</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>Interactive Scalar Quantization for Distributed Resource Allocation</title><categories>cs.IT math.IT</categories><comments>31 pages, 9 figures. Submitted on 2015-05-15 to IEEE Transactions on
  Signal Processing. Revised 2015-09-06</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many resource allocation problems, a centralized controller needs to award
some resource to a user selected from a collection of distributed users with
the goal of maximizing the utility the user would receive from the resource.
This can be modeled as the controller computing an extremum of the distributed
users' utilities. The overhead rate necessary to enable the controller to
reproduce the users' local state can be prohibitively high. An approach to
reduce this overhead is interactive communication wherein rate savings are
achieved by tolerating an increase in delay. In this paper, we consider the
design of a simple achievable scheme based on successive refinements of scalar
quantization at each user. The optimal quantization policy is computed via a
dynamic program and we demonstrate that tolerating a small increase in delay
can yield significant rate savings. We then consider two simpler quantization
policies to investigate the scaling properties of the rate-delay trade-offs.
Using a combination of these simpler policies, the performance of the optimal
policy can be closely approximated with lower computational costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04207</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04207</id><created>2015-05-15</created><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Drobniak</keyname><forenames>Szymon</forenames></author><author><keyname>Moore</keyname><forenames>Sean</forenames></author></authors><title>Towards a Systematic View on Cybersecurity Ecology</title><categories>cs.CR</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current network security systems are progressively showing their limitations.
One credible estimate is that only about 45% of new threats are detected.
Therefore it is vital to find a new direction that cybersecurity development
should follow. We argue that the next generation of cybersecurity systems
should seek inspiration from nature. This approach has been used before in a
first generation of cybersecurity systems; however, since then cyber threats
and environment have evolved significantly, and accordingly the
first-generation systems have lost their effectiveness. A next generation of
bio-inspired cybersecurity research is emerging, but progress is hindered by
the lack of a framework for mapping biological security systems to their cyber
analogies. In this paper, using terminology and concepts from biology, we
describe a cybersecurity ecology and a framework that may be used to
systematically research and develop bio-inspired cybersecurity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04211</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04211</id><created>2015-05-15</created><authors><author><keyname>Loverich</keyname><forenames>John</forenames></author></authors><title>Discontinuous Piecewise Polynomial Neural Networks</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An artificial neural network is presented where each link is represented by a
grid of weights defining a series of piecewise polynomial functions with
discontinuities between each polynomial. The polynomial order ranges from first
to fifth order. The unit averages the input values from each link. A back
propagation technique that works with discontinuous link functions and
activation functions is presented. The use of discontinuous links function
means that only a subset of the network is active for a given input and so only
a subset of the network is trained for a particular training example. Unit
dropout is used for regularization and a parameter free weight update is used.
Better performance is obtained by moving from piecewise linear links to
piecewise quadratic and higher order links. The algorithm is tested on the
MNIST data set, using multiple autoencoders, with good results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04214</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04214</id><created>2015-05-15</created><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Algorithmic Connections Between Active Learning and Stochastic Convex
  Optimization</title><categories>cs.LG cs.AI math.OC stat.ML</categories><comments>15 pages, published in proceedings of Algorithmic Learning Theory,
  Springer Berlin Heidelberg, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interesting theoretical associations have been established by recent papers
between the fields of active learning and stochastic convex optimization due to
the common role of feedback in sequential querying mechanisms. In this paper,
we continue this thread in two parts by exploiting these relations for the
first time to yield novel algorithms in both fields, further motivating the
study of their intersection. First, inspired by a recent optimization algorithm
that was adaptive to unknown uniform convexity parameters, we present a new
active learning algorithm for one-dimensional thresholds that can yield minimax
rates by adapting to unknown noise parameters. Next, we show that one can
perform $d$-dimensional stochastic minimization of smooth uniformly convex
functions when only granted oracle access to noisy gradient signs along any
coordinate instead of real-valued gradients, by using a simple randomized
coordinate descent procedure where each line search can be solved by
$1$-dimensional active learning, provably achieving the same error convergence
rate as having the entire real-valued gradient. Combining these two parts
yields an algorithm that solves stochastic convex optimization of uniformly
convex and smooth functions using only noisy gradient signs by repeatedly
performing active learning, achieves optimal rates and is adaptive to all
unknown convexity and smoothness parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04215</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04215</id><created>2015-05-15</created><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>An Analysis of Active Learning With Uniform Feature Noise</title><categories>stat.ML cs.AI cs.LG math.ST stat.TH</categories><comments>24 pages, 2 figures, published in the proceedings of the 17th
  International Conference on Artificial Intelligence and Statistics (AISTATS),
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In active learning, the user sequentially chooses values for feature $X$ and
an oracle returns the corresponding label $Y$. In this paper, we consider the
effect of feature noise in active learning, which could arise either because
$X$ itself is being measured, or it is corrupted in transmission to the oracle,
or the oracle returns the label of a noisy version of the query point. In
statistics, feature noise is known as &quot;errors in variables&quot; and has been
studied extensively in non-active settings. However, the effect of feature
noise in active learning has not been studied before. We consider the
well-known Berkson errors-in-variables model with additive uniform noise of
width $\sigma$.
  Our simple but revealing setting is that of one-dimensional binary
classification setting where the goal is to learn a threshold (point where the
probability of a $+$ label crosses half). We deal with regression functions
that are antisymmetric in a region of size $\sigma$ around the threshold and
also satisfy Tsybakov's margin condition around the threshold. We prove minimax
lower and upper bounds which demonstrate that when $\sigma$ is smaller than the
minimiax active/passive noiseless error derived in \cite{CN07}, then noise has
no effect on the rates and one achieves the same noiseless rates. For larger
$\sigma$, the \textit{unflattening} of the regression function on convolution
with uniform noise, along with its local antisymmetry around the threshold,
together yield a behaviour where noise \textit{appears} to be beneficial. Our
key result is that active learning can buy significant improvement over a
passive strategy even in the presence of feature noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04216</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04216</id><created>2015-05-15</created><authors><author><keyname>Amarilli</keyname><forenames>Antoine</forenames></author><author><keyname>Benedikt</keyname><forenames>Michael</forenames></author></authors><title>Finite Open-World Query Answering with Number Restrictions (Extended
  Version)</title><categories>cs.LO cs.DB</categories><comments>59 pages. To appear in LICS 2015. Extended version including proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open-world query answering is the problem of deciding, given a set of facts,
conjunction of constraints, and query, whether the facts and constraints imply
the query. This amounts to reasoning over all instances that include the facts
and satisfy the constraints. We study finite open-world query answering (FQA),
which assumes that the underlying world is finite and thus only considers the
finite completions of the instance. The major known decidable cases of FQA
derive from the following: the guarded fragment of first-order logic, which can
express referential constraints (data in one place points to data in another)
but cannot express number restrictions such as functional dependencies; and the
guarded fragment with number restrictions but on a signature of arity only two.
In this paper, we give the first decidability results for FQA that combine both
referential constraints and number restrictions for arbitrary signatures: we
show that, for unary inclusion dependencies and functional dependencies, the
finiteness assumption of FQA can be lifted up to taking the finite implication
closure of the dependencies. Our result relies on new techniques to construct
finite universal models of such constraints, for any bound on the maximal query
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04220</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04220</id><created>2015-05-15</created><authors><author><keyname>Semiari</keyname><forenames>Omid</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Context-Aware Small Cell Networks: How Social Metrics Improve Wireless
  Resource Allocation</title><categories>cs.IT cs.GT math.IT</categories><comments>Submitted to the IEEE Transaction on Wireless Communications</comments><doi>10.1109/TWC.2015.2444385</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel approach for optimizing and managing resource
allocation in wireless small cell networks (SCNs) with device-to-device (D2D)
communication is proposed. The proposed approach allows to jointly exploit both
the wireless and social context of wireless users for optimizing the overall
allocation of resources and improving traffic offload in SCNs. This
context-aware resource allocation problem is formulated as a matching game in
which user equipments (UEs) and resource blocks (RBs) rank one another, based
on utility functions that capture both wireless and social metrics. Due to
social interrelations, this game is shown to belong to a class of matching
games with peer effects. To solve this game, a novel, selforganizing algorithm
is proposed, using which UEs and RBs can interact to decide on their desired
allocation. The proposed algorithm is then proven to converge to a two-sided
stable matching between UEs and RBs. The properties of the resulting stable
outcome are then studied and assessed. Simulation results using real social
data show that clustering of socially connected users allows to offload a
substantially larger amount of traffic than the conventional context-unaware
approach. These results show that exploiting social context has high practical
relevance in saving resources on the wireless links and on the backhaul.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04224</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04224</id><created>2015-05-15</created><updated>2015-08-26</updated><authors><author><keyname>Mendes</keyname><forenames>Hammurabi</forenames></author><author><keyname>Herlihy</keyname><forenames>Maurice</forenames></author></authors><title>Tight Bounds for Connectivity and Set Agreement in Byzantine Synchronous
  Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that the protocol complex of a Byzantine synchronous
system can remain $(k - 1)$-connected for up to $\lceil t/k \rceil$ rounds,
where $t$ is the maximum number of Byzantine processes, and $t \ge k \ge 1$.
Protocol complex connectivity is important since a $(k - 1)$-connected protocol
complex does not admit solutions for the $k$-set agreement problem. In
crash-failure systems, the connectivity upper bound is $\lfloor t/k \rfloor$
rounds, therefore, in Byzantine systems, the ambiguity in the communication can
potentially persist for one extra round, delaying the solution to $k$-set
agreement and other related problems.
  Additionally, we show that our connectivity bound is tight, at least when $n
+ 1$, the number of processes, is suitably large compared to $t$. We solve a
formulation of $k$-set agreement appropriate for Byzantine systems in $\lceil
t/k \rceil + 1$ rounds. Essentially, we see that Byzantine failures can
potentially require us one extra round to handle ambiguity, and, in specific
cases, at most that.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04229</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04229</id><created>2015-05-15</created><updated>2015-10-06</updated><authors><author><keyname>Borjian</keyname><forenames>Setareh</forenames></author><author><keyname>Galle</keyname><forenames>Virgile</forenames></author><author><keyname>Manshadi</keyname><forenames>Vahideh H.</forenames></author><author><keyname>Barnhart</keyname><forenames>Cynthia</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author></authors><title>Container Relocation Problem: Approximation, Asymptotic, and Incomplete
  Information</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Container Relocation Problem (CRP) is concerned with finding a sequence
of moves of containers that minimizes the number of relocations needed to
retrieve all containers respecting a given order of retrieval. While the
problem is known to be NP-hard, certain algorithms such as the A* search and
heuristics perform reasonably well on many instances of the problem. In this
paper, we first focus on the A* search algorithm, and analyze lower and upper
bounds that are easy to compute and can be used to prune nodes. Our analysis
sheds light on which bounds result in fast computation within a given
approximation gap. We present extensive simulation results that improve upon
our theoretical analysis, and further show that our method finds the optimum
solution on most instances of medium-size bays. On &quot;hard&quot; instances, our method
finds an approximate solution with a small gap and within a time frame that is
fast for practical applications. We also study the average-case asymptotic
behavior of the CRP where the number of columns grows. We calculate the
expected number of relocations in the limit, and show that the optimum number
of relocations converges to a simple and intuitive lower-bound. We further
study the CRP with incomplete information by relaxing the assumption that the
order of retrieval of all containers are initially known. This assumption is
particularly unrealistic in ports without an appointment system. We assume that
the retrieval order of a subset of containers is known initially and the
retrieval order of the remaining containers is observed later at a given
specific time. Before this time, we assume a probabilistic distribution on the
retrieval order of unknown containers. We combine the A* algorithm with
sampling technique to solve this two-stage stochastic optimization problem. We
show that our algorithm is fast and the error due to sampling and pruning is
reasonably small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04235</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04235</id><created>2015-05-15</created><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author></authors><title>Triangulating planar graphs while keeping the pathwidth small</title><categories>cs.DM math.CO</categories><comments>To appear (without the appendix) at WG 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any simple planar graph can be triangulated, i.e., we can add edges to it,
without adding multi-edges, such that the result is planar and all faces are
triangles. In this paper, we study the problem of triangulating a planar graph
without increasing the pathwidth by much.
  We show that if a planar graph has pathwidth $k$, then we can triangulate it
so that the resulting graph has pathwidth $O(k)$ (where the factors are 1, 8
and 16 for 3-connected, 2-connected and arbitrary graphs). With similar
techniques, we also show that any outer-planar graph of pathwidth $k$ can be
turned into a maximal outer-planar graph of pathwidth at most $4k+4$. The
previously best known result here was $16k+15$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04243</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04243</id><created>2015-05-16</created><authors><author><keyname>Freund</keyname><forenames>Robert M.</forenames></author><author><keyname>Grigas</keyname><forenames>Paul</forenames></author><author><keyname>Mazumder</keyname><forenames>Rahul</forenames></author></authors><title>A New Perspective on Boosting in Linear Regression via Subgradient
  Optimization and Relatives</title><categories>math.ST cs.LG math.OC stat.ML stat.TH</categories><msc-class>62J05, 62J07, 90C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze boosting algorithms in linear regression from a new
perspective: that of modern first-order methods in convex optimization. We show
that classic boosting algorithms in linear regression, namely the incremental
forward stagewise algorithm (FS$_\varepsilon$) and least squares boosting
(LS-Boost($\varepsilon$)), can be viewed as subgradient descent to minimize the
loss function defined as the maximum absolute correlation between the features
and residuals. We also propose a modification of FS$_\varepsilon$ that yields
an algorithm for the Lasso, and that may be easily extended to an algorithm
that computes the Lasso path for different values of the regularization
parameter. Furthermore, we show that these new algorithms for the Lasso may
also be interpreted as the same master algorithm (subgradient descent), applied
to a regularized version of the maximum absolute correlation loss function. We
derive novel, comprehensive computational guarantees for several boosting
algorithms in linear regression (including LS-Boost($\varepsilon$) and
FS$_\varepsilon$) by using techniques of modern first-order methods in convex
optimization. Our computational guarantees inform us about the statistical
properties of boosting algorithms. In particular they provide, for the first
time, a precise theoretical description of the amount of data-fidelity and
regularization imparted by running a boosting algorithm with a prespecified
learning rate for a fixed but arbitrary number of iterations, for any dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04252</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04252</id><created>2015-05-16</created><updated>2015-05-27</updated><authors><author><keyname>Lin</keyname><forenames>Tianyi</forenames></author><author><keyname>Ma</keyname><forenames>Shiqian</forenames></author><author><keyname>Zhang</keyname><forenames>Shuzhong</forenames></author></authors><title>Global Convergence of Unmodified 3-Block ADMM for a Class of Convex
  Minimization Problems</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The alternating direction method of multipliers (ADMM) has been successfully
applied to solve structured convex optimization problems due to its superior
practical performance. The convergence properties of the 2-block ADMM have been
studied extensively in the literature. Specifically, it has been proven that
the 2-block ADMM globally converges for any penalty parameter $\gamma&gt;0$. In
this sense, the 2-block ADMM allows the parameter to be free, i.e., there is no
need to restrict the value for the parameter when implementing this algorithm
in order to ensure convergence. However, for the 3-block ADMM, Chen et al.
recently constructed a counter-example showing that it can diverge if no
further condition is imposed. The existing results on studying further
sufficient conditions on guaranteeing the convergence of the 3-block ADMM
usually require $\gamma$ to be smaller than a certain bound, which is usually
either difficult to compute or too small to make it a practical algorithm. In
this paper, we show that the 3-block ADMM still globally converges with any
penalty parameter $\gamma&gt;0$ when applied to solve a class of commonly
encountered problems to be called regularized least squares decomposition
(RLSD) in this paper, which covers many important applications in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04260</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04260</id><created>2015-05-16</created><authors><author><keyname>Cuculo</keyname><forenames>Vittorio</forenames></author><author><keyname>Lanzarotti</keyname><forenames>Raffaella</forenames></author><author><keyname>Boccignone</keyname><forenames>Giuseppe</forenames></author></authors><title>The color of smiling: computational synaesthesia of facial expressions</title><categories>cs.CV</categories><comments>Submitted to: 18th International Conference on Image Analysis and
  Processing (ICIAP 2015), 7-11 September 2015, Genova, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note gives a preliminary account of the transcoding or rechanneling
problem between different stimuli as it is of interest for the natural
interaction or affective computing fields. By the consideration of a simple
example, namely the color response of an affective lamp to a sensed facial
expression, we frame the problem within an information- theoretic perspective.
A full justification in terms of the Information Bottleneck principle promotes
a latent affective space, hitherto surmised as an appealing and intuitive
solution, as a suitable mediator between the different stimuli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04265</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04265</id><created>2015-05-16</created><authors><author><keyname>Veitas</keyname><forenames>Viktoras</forenames><affiliation>Weaver</affiliation></author><author><keyname>Weinbaum</keyname><forenames>David</forenames><affiliation>Weaver</affiliation></author></authors><title>Cognitive Development of the Web</title><categories>cs.AI</categories><comments>Working paper, 22 pages, 2 figures</comments><report-no>ECCO working paper 2015-02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sociotechnological system is a system constituted of human individuals
and their artifacts: technological artifacts, institutions, conceptual and
representational systems, worldviews, knowledge systems, culture and the whole
biosphere as a volutionary niche. In our view the sociotechnological system as
a super-organism is shaped and determined both by the characteristics of the
agents involved and the characteristics emergent in their interactions at
multiple scales. Our approach to sociotechnological dynamics will maintain a
balance between perspectives: the individual and the collective. Accordingly,
we analyze dynamics of the Web as a sociotechnological system made of people,
computers and digital artifacts (Web pages, databases, search engines, etc.).
Making sense of the sociotechnological system while being part of it, is also a
constant interplay between pragmatic and value based approaches. The first is
focusing on the actualities of the system while the second highlights the
observer's projections. In our attempt to model sociotechnological dynamics and
envision its future, we take special care to make explicit our values as part
of the analysis. In sociotechnological systems with a high degree of
reflexivity (coupling between the perception of the system and the system's
behavior), highlighting values is of critical importance. In this essay, we
choose to see the future evolution of the web as facilitating a basic value,
that is, continuous open-ended intelligence expansion. By that we mean that we
see intelligence expansion as the determinant of the 'greater good' and 'well
being' of both of individuals and collectives at all scales. Our working
definition of intelligence here is the progressive process of sense-making of
self, other, environment and universe. Intelligence expansion, therefore, means
an increasing ability of sense-making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04274</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04274</id><created>2015-05-16</created><authors><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Uno</keyname><forenames>Yushi</forenames></author></authors><title>Threes!, Fives, 1024!, and 2048 are Hard</title><categories>cs.CC cs.GT</categories><comments>14 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the computational complexity of the popular computer games
Threes!, 1024!, 2048 and many of their variants. For most known versions
expanded to an m x n board, we show that it is NP-hard to decide whether a
given starting position can be played to reach a specific (constant) tile
value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04282</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04282</id><created>2015-05-16</created><authors><author><keyname>Desbief</keyname><forenames>Simon</forenames></author><author><keyname>Kyndiah</keyname><forenames>Adrica</forenames></author><author><keyname>Guerin</keyname><forenames>David</forenames></author><author><keyname>Gentili</keyname><forenames>Denis</forenames></author><author><keyname>Murgia</keyname><forenames>Mauro</forenames></author><author><keyname>Lenfant</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Alibart</keyname><forenames>Fabien</forenames></author><author><keyname>Cramer</keyname><forenames>Tobias</forenames></author><author><keyname>Biscarini</keyname><forenames>Fabio</forenames></author><author><keyname>Vuillaume</keyname><forenames>Dominique</forenames></author></authors><title>Low voltage and time constant organic synapse-transistor</title><categories>cond-mat.mes-hall cs.ET</categories><comments>Full paper with supporting information</comments><journal-ref>Organic Electronics 21, 47-53 (2015)</journal-ref><doi>10.1016/j.orgel.2015.02.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on an artificial synapse, an organic synapse-transistor (synapstor)
working at 1 volt and with a typical response time in the range 100-200 ms.
This device (also called NOMFET, Nanoparticle Organic Memory Field Effect
Transistor) combines a memory and a transistor effect in a single device. We
demonstrate that short-term plasticity (STP), a typical synaptic behavior, is
observed when stimulating the device with input spikes of 1 volt. Both
significant facilitating and depressing behaviors of this artificial synapse
are observed with a relative amplitude of about 50% and a dynamic response &lt;
200 ms. From a series of in-situ experiments, i.e. measuring the
current-voltage characteristic curves in-situ and in real time, during the
growth of the pentacene over a network of gold nanoparticles, we elucidate
these results by analyzing the relationship between the organic film morphology
and the transport properties. This synapstor works at a low energy of about 2
nJ/spike. We discuss the implications of these results for the development of
neuro-inspired computing architectures and interfacing with biological neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04286</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04286</id><created>2015-05-16</created><authors><author><keyname>Commin</keyname><forenames>Harry</forenames></author></authors><title>Robust Real-time Extraction of Fiducial Facial Feature Points using
  Haar-like Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore methods of robustly extracting fiducial facial
feature points - an important process for numerous facial image processing
tasks. We consider various methods to first detect face, then facial features
and finally salient facial feature points. Colour-based models are analysed and
their overall unsuitability for this task is summarised. The bulk of the report
is then dedicated to proposing a learning-based method centred on the
Viola-Jones algorithm. The specific difficulties and considerations relating to
feature point detection are laid out in this context and a novel approach is
established to address these issues. On a sequence of clear and unobstructed
face images, our proposed system achieves average detection rates of over 90%.
Then, using a more varied sample dataset, we identify some possible areas for
future development of our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04307</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04307</id><created>2015-05-16</created><updated>2015-08-19</updated><authors><author><keyname>Arapostathis</keyname><forenames>Ari</forenames></author><author><keyname>Pang</keyname><forenames>Guodong</forenames></author></authors><title>Ergodic Diffusion Control of Multiclass Multi-Pool Networks in the
  Halfin-Whitt Regime</title><categories>math.PR cs.SY math.OC</categories><comments>32 pages</comments><msc-class>60K25, 68M20, 90B22, 90B36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Markovian multiclass multi-pool networks with heterogeneous
server pools, each consisting of many statistically identical parallel servers,
where the bipartite graph of customer classes and server pools forms a tree.
Customers form their own queue and are served in the first-come first-served
discipline, and can abandon while waiting in queue. Service rates are both
class and pool dependent. The objective is to study the limiting diffusion
control problems under the long run average (ergodic) cost criteria in the
Halfin--Whitt regime. Two formulations of ergodic diffusion control problems
are considered: (i) both queueing and idleness costs are minimized, and (ii)
only the queueing cost is minimized while a constraint is imposed upon the
idleness of all server pools. We develop a recursive leaf elimination algorithm
that enables us to obtain an explicit representation of the drift for the
controlled diffusions. Consequently, we show that for the limiting controlled
diffusions, there always exists a stationary Markov control under which the
diffusion process is geometrically ergodic. The framework developed in our
earlier work is extended to address a broad class of ergodic diffusion control
problems with constraints. We show that that the unconstrained and constrained
problems are well posed, and we characterize the optimal stationary Markov
controls via HJB equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04308</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04308</id><created>2015-05-16</created><updated>2015-06-25</updated><authors><author><keyname>Glacet</keyname><forenames>Christian</forenames></author><author><keyname>Miller</keyname><forenames>Avery</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Time vs. Information Tradeoffs for Leader Election in Anonymous Trees</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The leader election task calls for all nodes of a network to agree on a
single node. If the nodes of the network are anonymous, the task of leader
election is formulated as follows: every node $v$ of the network must output a
simple path, coded as a sequence of port numbers, such that all these paths end
at a common node, the leader. In this paper, we study deterministic leader
election in anonymous trees.
  Our aim is to establish tradeoffs between the allocated time $\tau$ and the
amount of information that has to be given $\textit{a priori}$ to the nodes to
enable leader election in time $\tau$ in all trees for which leader election in
this time is at all possible. Following the framework of $\textit{algorithms
with advice}$, this information (a single binary string) is provided to all
nodes at the start by an oracle knowing the entire tree. The length of this
string is called the $\textit{size of advice}$. For an allocated time $\tau$,
we give upper and lower bounds on the minimum size of advice sufficient to
perform leader election in time $\tau$.
  We consider $n$-node trees of diameter $diam \leq D$. While leader election
in time $diam$ can be performed without any advice, for time $diam-1$ we give
tight upper and lower bounds of $\Theta (\log D)$. For time $diam-2$ we give
tight upper and lower bounds of $\Theta (\log D)$ for even values of $diam$,
and tight upper and lower bounds of $\Theta (\log n)$ for odd values of $diam$.
For the time interval $[\beta \cdot diam, diam-3]$ for constant $\beta &gt;1/2$,
we prove an upper bound of $O(\frac{n\log n}{D})$ and a lower bound of
$\Omega(\frac{n}{D})$, the latter being valid whenever $diam$ is odd or when
the time is at most $diam-4$. Finally, for time $\alpha \cdot diam$ for any
constant $\alpha &lt;1/2$ (except for the case of very small diameters), we give
tight upper and lower bounds of $\Theta (n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04313</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04313</id><created>2015-05-16</created><authors><author><keyname>Luuk</keyname><forenames>Erkki</forenames></author></authors><title>A type-theoretical approach to Universal Grammar</title><categories>cs.CL math.LO</categories><msc-class>03</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of Universal Grammar (UG) as the hypothetical linguistic structure
shared by all human languages harkens back at least to the 13th century. The
best known modern elaborations of the idea are due to Chomsky. Following a
devastating critique from theoretical, typological and field linguistics, these
elaborations, the idea of UG itself and the more general idea of language
universals stand untenable and are largely abandoned. The proposal tackles the
hypothetical contents of UG using dependent and polymorphic type theory in a
framework very different from the Chomskyan ones. We introduce a type logic for
a precise, universal and parsimonious representation of natural language
morphosyntax and compositional semantics. The logic handles grammatical
ambiguity (with polymorphic types), selectional restrictions and diverse kinds
of anaphora (with dependent types), and features a partly universal set of
morphosyntactic types (by the Curry-Howard isomorphism).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04324</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04324</id><created>2015-05-16</created><updated>2015-12-17</updated><authors><author><keyname>de Moura</keyname><forenames>Leonardo</forenames></author><author><keyname>Avigad</keyname><forenames>Jeremy</forenames></author><author><keyname>Kong</keyname><forenames>Soonho</forenames></author><author><keyname>Roux</keyname><forenames>Cody</forenames></author></authors><title>Elaboration in Dependent Type Theory</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To be usable in practice, interactive theorem provers need to provide
convenient and efficient means of writing expressions, definitions, and proofs.
This involves inferring information that is often left implicit in an ordinary
mathematical text, and resolving ambiguities in mathematical expressions. We
refer to the process of passing from a quasi-formal and partially-specified
expression to a completely precise formal one as elaboration. We describe an
elaboration algorithm for dependent type theory that has been implemented in
the Lean theorem prover. Lean's elaborator supports higher-order unification,
type class inference, ad hoc overloading, insertion of coercions, the use of
tactics, and the computational reduction of terms. The interactions between
these components are subtle and complex, and the elaboration algorithm has been
carefully designed to balance efficiency and usability. We describe the central
design goals, and the means by which they are achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04330</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04330</id><created>2015-05-16</created><authors><author><keyname>Heunen</keyname><forenames>Chris</forenames></author><author><keyname>Karvonen</keyname><forenames>Martti</forenames></author></authors><title>Reversible monadic computing</title><categories>cs.LO math.CT</categories><comments>19 pages</comments><journal-ref>Proceedings MFPS, Electronic Notes in Theoretical Computer Science
  319:217--237, 2015</journal-ref><doi>10.1016/j.entcs.2015.12.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend categorical semantics of monadic programming to reversible
computing, by considering monoidal closed dagger categories: the dagger gives
reversibility, whereas closure gives higher-order expressivity. We demonstrate
that Frobenius monads model the appropriate notion of coherence between the
dagger and closure by reinforcing Cayley's theorem; by proving that effectful
computations (Kleisli morphisms) are reversible precisely when the monad is
Frobenius; by characterizing the largest reversible subcategory of
Eilenberg-Moore algebras; and by identifying the latter algebras as
measurements in our leading example of quantum computing. Strong Frobenius
monads are characterized internally by Frobenius monoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04339</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04339</id><created>2015-05-16</created><authors><author><keyname>Mhaske</keyname><forenames>Swapnil</forenames></author><author><keyname>Uliana</keyname><forenames>David</forenames></author><author><keyname>Kee</keyname><forenames>Hojin</forenames></author><author><keyname>Ly</keyname><forenames>Tai</forenames></author><author><keyname>Aziz</keyname><forenames>Ahsan</forenames></author><author><keyname>Spasojevic</keyname><forenames>Predrag</forenames></author></authors><title>A 2.48Gb/s QC-LDPC Decoder Implementation on the NI USRP-2953R</title><categories>cs.AR</categories><comments>3 figures, 5 pages. arXiv admin note: text overlap with
  arXiv:1503.02986</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing data rates expected to be of the order of Gb/s for future
wireless systems directly impact the throughput requirements of the modulation
and coding subsystems of the physical layer. In an effort to design a suitable
channel coding solution for 5G wireless systems, in this brief we present a
massively-parallel 2.48Gb/s Quasi-Cyclic Low-Density Parity-Check (QC-LDPC)
decoder implementation operating at 200MHz on the NI USRP-2953R, on a single
FPGA. The high-level description of the entire massively-parallel decoder was
translated to a Hardware Description Language (HDL), namely VHDL, using the
algorithmic compiler in the National Instruments LabVIEW Communication System
Design Suite (CSDS) in approximately 2 minutes. This implementation not only
demonstrates the scalability of our decoder architecture but also, the rapid
prototyping capability of the LabVIEW CSDS tools. As per our knowledge, at the
time of writing this paper, this is the fastest implementation of a standard
compliant QC-LDPC decoder on a USRP using an algorithmic compiler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04340</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04340</id><created>2015-05-16</created><authors><author><keyname>Li</keyname><forenames>Ruipeng</forenames></author><author><keyname>Xi</keyname><forenames>Yuanzhe</forenames></author><author><keyname>Saad</keyname><forenames>Yousef</forenames></author></authors><title>Schur Complement based domain decomposition preconditioners with
  Low-rank corrections</title><categories>cs.NA math.NA</categories><report-no>ys-2014-3</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a robust preconditioner for general sparse symmetric
matrices, that is based on low-rank approximations of the Schur complement in a
Domain Decomposition (DD) framework. In this &quot;Schur Low Rank&quot; (SLR)
preconditioning approach, the coefficient matrix is first decoupled by DD, and
then a low-rank correction is exploited to compute an approximate inverse of
the Schur complement associated with the interface points. The method avoids
explicit formation of the Schur complement matrix. We show the feasibility of
this strategy for a model problem, and conduct a detailed spectral analysis for
the relationship between the low-rank correction and the quality of the
preconditioning. Numerical experiments on general matrices illustrate the
robustness and efficiency of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04341</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04341</id><created>2015-05-16</created><updated>2015-05-29</updated><authors><author><keyname>Li</keyname><forenames>Ruipeng</forenames></author><author><keyname>Saad</keyname><forenames>Yousef</forenames></author></authors><title>Low-rank correction methods for algebraic domain decomposition
  preconditioners</title><categories>cs.NA math.NA</categories><report-no>ys-2014-4</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a parallel preconditioning method for distributed sparse
linear systems, based on an approximate inverse of the original matrix, that
adopts a general framework of distributed sparse matrices and exploits the
domain decomposition method and low-rank corrections. The domain decomposition
approach decouples the matrix and once inverted, a low-rank approximation is
applied by exploiting the Sherman-Morrison-Woodbury formula, which yields two
variants of the preconditioning methods. The low-rank expansion is computed by
the Lanczos procedure with reorthogonalizations. Numerical experiments indicate
that, when combined with Krylov subspace accelerators, this preconditioner can
be efficient and robust for solving symmetric sparse linear systems.
Comparisons with other distributed-memory preconditioning methods are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04342</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04342</id><created>2015-05-16</created><updated>2016-02-24</updated><authors><author><keyname>Clark</keyname><forenames>Eric M.</forenames></author><author><keyname>Williams</keyname><forenames>Jake Ryland</forenames></author><author><keyname>Jones</keyname><forenames>Chris A.</forenames></author><author><keyname>Galbraith</keyname><forenames>Richard A.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Sifting Robotic from Organic Text: A Natural Language Approach for
  Detecting Automation on Twitter</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter, a popular social media outlet, has evolved into a vast source of
linguistic data, rich with opinion, sentiment, and discussion. Due to the
increasing popularity of Twitter, its perceived potential for exerting social
influence has led to the rise of a diverse community of automatons, commonly
referred to as bots. These inorganic and semi-organic Twitter entities can
range from the benevolent (e.g., weather-update bots, help-wanted-alert bots)
to the malevolent (e.g., spamming messages, advertisements, or radical
opinions). Existing detection algorithms typically leverage meta-data (time
between tweets, number of followers, etc.) to identify robotic accounts. Here,
we present a powerful classification scheme that exclusively uses the natural
language text from organic users to provide a criterion for identifying
accounts posting automated messages. Since the classifier operates on text
alone, it is flexible and may be applied to any textual data beyond the
Twitter-sphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04343</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04343</id><created>2015-05-16</created><authors><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Provably Correct Active Sampling Algorithms for Matrix Column Subset
  Selection with Missing Data</title><categories>stat.ML cs.LG</categories><comments>32 pages. A short version titled &quot;column subset selection with
  missing data via active sampling&quot; appeared in International Conference on AI
  and Statistics (AISTATS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of matrix column subset selection, which selects a
subset of columns from an input matrix such that the input can be well
approximated by the span of the selected columns. Column subset selection has
been applied to numerous real-world data applications such as population
genetics summarization, electronic circuits testing and recommendation systems.
In many applications the complete data matrix is unavailable and one needs to
select representative columns by inspecting only a small portion of the input
matrix. In this paper we propose the first provably correct column subset
selection algorithms for partially observed data matrices. Our proposed
algorithms exhibit different merits and drawbacks in terms of statistical
accuracy, computational efficiency, sample complexity and sampling schemes,
which provides a nice exploration of the tradeoff between these desired
properties for column subset selection. The proposed methods employ the idea of
feedback driven sampling and are inspired by several sampling schemes
previously introduced for low-rank matrix approximation tasks [DMM08, FKV04,
DV06, KS14]. Our analysis shows that two of the proposed algorithms enjoy a
relative error bound, which is preferred for column subset selection and matrix
approximation purposes. We also demonstrate through both theoretical and
empirical analysis the power of feedback driven sampling compared to uniform
random sampling on input matrices with highly correlated columns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04344</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04344</id><created>2015-05-16</created><updated>2016-02-02</updated><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Naves</keyname><forenames>Humberto</forenames></author><author><keyname>Sudakov</keyname><forenames>Benny</forenames></author></authors><title>On the maximum quartet distance between phylogenetic trees</title><categories>cs.DM math.CO q-bio.PE</categories><comments>arXiv admin note: text overlap with arXiv:1203.2723</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conjecture of Bandelt and Dress states that the maximum quartet distance
between any two phylogenetic trees on $n$ leaves is at most $(\frac 23
+o(1))\binom{n}{4}$. Using the machinery of flag algebras we improve the
currently known bounds regarding this conjecture, in particular we show that
the maximum is at most $(0.69 +o(1))\binom{n}{4}$. We also give further
evidence that the conjecture is true by proving that the maximum distance
between caterpillar trees is at most $(\frac 23 +o(1))\binom{n}{4}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04357</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04357</id><created>2015-05-17</created><authors><author><keyname>Howard</keyname><forenames>Gerard David</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Gale</keyname><forenames>Ella</forenames></author></authors><title>Evolving Spiking Networks with Variable Resistive Memories</title><categories>cs.NE</categories><comments>27 pages</comments><journal-ref>Evolutionary Computation, Spring 2014, Vol. 22, No. 1, Pages
  79-103 Posted Online February 7, 2014</journal-ref><doi>10.1162/EVCO_a_00103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuromorphic computing is a brainlike information processing paradigm that
requires adaptive learning mechanisms. A spiking neuro-evolutionary system is
used for this purpose; plastic resistive memories are implemented as synapses
in spiking neural networks. The evolutionary design process exploits parameter
self-adaptation and allows the topology and synaptic weights to be evolved for
each network in an autonomous manner. Variable resistive memories are the focus
of this research; each synapse has its own conductance profile which modifies
the plastic behaviour of the device and may be altered during evolution. These
variable resistive networks are evaluated on a noisy robotic dynamic-reward
scenario against two static resistive memories and a system containing standard
connections only. Results indicate that the extra behavioural degrees of
freedom available to the networks incorporating variable resistive memories
enable them to outperform the comparative synapse types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04364</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04364</id><created>2015-05-17</created><authors><author><keyname>Yang</keyname><forenames>Kai-Fu</forenames></author><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Li</keyname><forenames>Chao-Yi</forenames></author><author><keyname>Li</keyname><forenames>Yong-Jie</forenames></author></authors><title>Salient Structure Detection by Context-Guided Visual Search</title><categories>cs.CV</categories><comments>13 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the task of salient structure (SS) detection to unify the
saliency-related tasks like fixation prediction, salient object detection, and
other detection of structures of interest. In this study, we propose a unified
framework for SS detection by modeling the two-pathway-based guided search
strategy of biological vision. Firstly, context-based spatial prior (CBSP) is
extracted based on the layout of edges in the given scene along a fast visual
pathway, called non-selective pathway. This is a rough and non-selective
estimation of the locations where the potential SSs present. Secondly, another
flow of local feature extraction is executed in parallel along the selective
pathway. Finally, Bayesian inference is used to integrate local cues guided by
CBSP, and to predict the exact locations of SSs in the input scene. The
proposed model is invariant to size and features of objects. Experimental
results on four datasets (two fixation prediction datasets and two salient
object datasets) demonstrate that our system achieves competitive performance
for SS detection (i.e., both the tasks of fixation prediction and salient
object detection) comparing to the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04365</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04365</id><created>2015-05-17</created><authors><author><keyname>Arif</keyname><forenames>M. Fareed</forenames></author><author><keyname>Menc&#xed;a</keyname><forenames>Carlos</forenames></author><author><keyname>Marques-Silva</keyname><forenames>Joao</forenames></author></authors><title>Efficient MUS Enumeration of Horn Formulae with Applications to Axiom
  Pinpointing</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enumeration of minimal unsatisfiable subsets (MUSes) finds a growing
number of practical applications, that includes a wide range of diagnosis
problems. As a concrete example, the problem of axiom pinpointing in the EL
family of description logics (DLs) can be modeled as the enumeration of the
group-MUSes of Horn formulae. In turn, axiom pinpointing for the EL family of
DLs finds important applications, such as debugging medical ontologies, of
which SNOMED CT is the best known example. The main contribution of this paper
is to develop an efficient group-MUS enumerator for Horn formulae, HGMUS, that
finds immediate application in axiom pinpointing for the EL family of DLs. In
the process of developing HGMUS, the paper also identifies performance
bottlenecks of existing solutions. The new algorithm is shown to outperform all
alternative approaches when the problem domain targeted by group-MUS
enumeration of Horn formulae is axiom pinpointing for the EL family of DLs,
with a representative suite of examples taken from different medical
ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04366</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04366</id><created>2015-05-17</created><authors><author><keyname>Noh</keyname><forenames>Hyeonwoo</forenames></author><author><keyname>Hong</keyname><forenames>Seunghoon</forenames></author><author><keyname>Han</keyname><forenames>Bohyung</forenames></author></authors><title>Learning Deconvolution Network for Semantic Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel semantic segmentation algorithm by learning a
deconvolution network. We learn the network on top of the convolutional layers
adopted from VGG 16-layer net. The deconvolution network is composed of
deconvolution and unpooling layers, which identify pixel-wise class labels and
predict segmentation masks. We apply the trained network to each proposal in an
input image, and construct the final semantic segmentation map by combining the
results from all proposals in a simple manner. The proposed algorithm mitigates
the limitations of the existing methods based on fully convolutional networks
by integrating deep deconvolution network and proposal-wise prediction; our
segmentation method typically identifies detailed structures and handles
objects in multiple scales naturally. Our network demonstrates outstanding
performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy
(72.5%) among the methods trained with no external data through ensemble with
the fully convolutional network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04368</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04368</id><created>2015-05-17</created><authors><author><keyname>Oizumi</keyname><forenames>Masafumi</forenames></author><author><keyname>Amari</keyname><forenames>Shun-ichi</forenames></author><author><keyname>Yanagawa</keyname><forenames>Toru</forenames></author><author><keyname>Fujii</keyname><forenames>Naotaka</forenames></author><author><keyname>Tsuchiya</keyname><forenames>Naotsugu</forenames></author></authors><title>Measuring integrated information from the decoding perspective</title><categories>q-bio.NC cs.IT math.IT</categories><journal-ref>PLoS Comput Biol 12(1), e1004654, 2016</journal-ref><doi>10.1371/journal.pcbi.1004654</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accumulating evidence indicates that the capacity to integrate information in
the brain is a prerequisite for consciousness. Integrated Information Theory
(IIT) of consciousness provides a mathematical approach to quantifying the
information integrated in a system, called integrated information, $\Phi$.
Integrated information is defined theoretically as the amount of information a
system generates as a whole, above and beyond the sum of the amount of
information its parts independently generate. IIT predicts that the amount of
integrated information in the brain should reflect levels of consciousness.
Empirical evaluation of this theory requires computing integrated information
from neural data acquired from experiments, although difficulties with using
the original measure $\Phi$ precludes such computations. Although some
practical measures have been previously proposed, we found that these measures
fail to satisfy the theoretical requirements as a measure of integrated
information. Measures of integrated information should satisfy the lower and
upper bounds as follows: The lower bound of integrated information should be 0
when the system does not generate information (no information) or when the
system comprises independent parts (no integration). The upper bound of
integrated information is the amount of information generated by the whole
system and is realized when the amount of information generated independently
by its parts equals to 0. Here we derive the novel practical measure $\Phi^*$
by introducing a concept of mismatched decoding developed from information
theory. We show that $\Phi^*$ is properly bounded from below and above, as
required, as a measure of integrated information. We derive the analytical
expression $\Phi^*$ under the Gaussian assumption, which makes it readily
applicable to experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04369</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04369</id><created>2015-05-17</created><authors><author><keyname>Xu</keyname><forenames>Lin</forenames></author><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Shrinkage degree in $L_2$-re-scale boosting for regression</title><categories>cs.LG</categories><comments>11 pages, 27 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Re-scale boosting (RBoosting) is a variant of boosting which can essentially
improve the generalization performance of boosting learning. The key feature of
RBoosting lies in introducing a shrinkage degree to re-scale the ensemble
estimate in each gradient-descent step. Thus, the shrinkage degree determines
the performance of RBoosting.
  The aim of this paper is to develop a concrete analysis concerning how to
determine the shrinkage degree in $L_2$-RBoosting. We propose two feasible ways
to select the shrinkage degree. The first one is to parameterize the shrinkage
degree and the other one is to develope a data-driven approach of it. After
rigorously analyzing the importance of the shrinkage degree in $L_2$-RBoosting
learning, we compare the pros and cons of the proposed methods. We find that
although these approaches can reach the same learning rates, the structure of
the final estimate of the parameterized approach is better, which sometimes
yields a better generalization capability when the number of sample is finite.
With this, we recommend to parameterize the shrinkage degree of
$L_2$-RBoosting. To this end, we present an adaptive parameter-selection
strategy for shrinkage degree and verify its feasibility through both
theoretical analysis and numerical verification.
  The obtained results enhance the understanding of RBoosting and further give
guidance on how to use $L_2$-RBoosting for regression tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04373</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04373</id><created>2015-05-17</created><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>Evolutionary Cost-sensitive Extreme Learning Machine and Subspace
  Extension</title><categories>cs.CV</categories><comments>13 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional extreme learning machines solve a Moore-Penrose generalized
inverse of hidden layer activated matrix and analytically determine the output
weights to achieve generalized performance, by assuming the same loss from
different types of misclassification. The assumption may not hold in
cost-sensitive recognition tasks, such as face recognition based access control
system, where misclassifying a stranger as a family member and allowed to enter
the house may result in more serious disaster than misclassifying a family
member as a stranger and not allowed to enter. Though recent cost-sensitive
learning can reduce the total loss with a given cost matrix that quantifies how
severe one type of mistake against another type of mistake, in many realistic
cases the cost matrix is unknown to users. Motivated by these concerns, this
paper proposes an evolutionary cost-sensitive extreme learning machine
(ECSELM), with the following merits: 1) to our best knowledge, it is the first
proposal of cost-sensitive ELM; 2) it well addresses the open issue of how to
define the cost matrix in cost-sensitive learning tasks; 3) an evolutionary
backtracking search algorithm is induced for adaptive cost matrix optimization.
Extensively, an ECSLDA method is generalized by coupling with cost-sensitive
subspace learning. Experiments in a variety of cost-sensitive tasks well
demonstrate the efficiency and effectiveness of the proposed approaches,
specifically, 5%~10% improvements in classification are obtained on several
datasets compared with ELMs; the computational efficiency is also 10 times
faster than cost-sensitive subspace learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04382</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04382</id><created>2015-05-17</created><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>Robust Visual Knowledge Transfer via EDA</title><categories>cs.CV</categories><comments>15 pages,7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of visual knowledge adaptation by leveraging labeled
patterns from the source domain and a very limited number of labeled instances
in target domain to learn a robust classifier for visual categorization. We
introduce a new semi-supervised cross-domain network learning framework,
referred to as Extreme Domain Adaptation (EDA), that allows us to
simultaneously learn a category transformation and an extreme classifier by
minimizing the L(2,1)-norm of the output weights and the learning error, in
which the network output weights can be analytically determined. The unlabeled
target data, as useful knowledge, is also learned as a fidelity term by
minimizing the matching error between the extreme classifier and a base
classifier to guarantee the stability during cross domain learning, into which
many existing classifiers can be readily incorporated as base classifiers.
Additionally, a manifold regularization with Laplacian graph is incorporated
into EDA, such that it is beneficial to semi-supervised learning. Under the
EDA, we also propose an extensive model learned with multiple views.
Experiments on three visual data sets for video event recognition and object
recognition, respectively, demonstrate that our EDA outperforms existing
cross-domain learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04383</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04383</id><created>2015-05-17</created><updated>2015-07-27</updated><authors><author><keyname>Allen</keyname><forenames>Sarah R.</forenames></author><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Witmer</keyname><forenames>David</forenames></author></authors><title>How to refute a random CSP</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a $k$-ary predicate over a finite alphabet. Consider a random
CSP$(P)$ instance $I$ over $n$ variables with $m$ constraints. When $m \gg n$
the instance $I$ will be unsatisfiable with high probability, and we want to
find a refutation - i.e., a certificate of unsatisfiability. When $P$ is the
$3$-ary OR predicate, this is the well studied problem of refuting random
$3$-SAT formulas, and an efficient algorithm is known only when $m \gg
n^{3/2}$. Understanding the density required for refutation of other predicates
is important in cryptography, proof complexity, and learning theory.
Previously, it was known that for a $k$-ary predicate, having $m \gg n^{\lceil
k/2 \rceil}$ constraints suffices for refutation. We give a criterion for
predicates that often yields efficient refutation algorithms at much lower
densities. Specifically, if $P$ fails to support a $t$-wise uniform
distribution, then there is an efficient algorithm that refutes random CSP$(P)$
instances $I$ whp when $m \gg n^{t/2}$. Indeed, our algorithm will &quot;somewhat
strongly&quot; refute $I$, certifying $\mathrm{Opt}(I) \leq 1-\Omega_k(1)$, if $t =
k$ then we get the strongest possible refutation, certifying $\mathrm{Opt}(I)
\leq \mathrm{E}[P] + o(1)$. This last result is new even in the context of
random $k$-SAT. Regarding the optimality of our $m \gg n^{t/2}$ requirement,
prior work on SDP hierarchies has given some evidence that efficient refutation
of random CSP$(P)$ may be impossible when $m \ll n^{t/2}$. Thus there is an
indication our algorithm's dependence on $m$ is optimal for every $P$, at least
in the context of SDP hierarchies. Along these lines, we show that our
refutation algorithm can be carried out by the $O(1)$-round SOS SDP hierarchy.
Finally, as an application of our result, we falsify assumptions used to show
hardness-of-learning results in recent work of Daniely, Linial, and
Shalev-Shwartz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04385</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04385</id><created>2015-05-17</created><authors><author><keyname>Samarasinghe</keyname><forenames>Prasanga</forenames></author><author><keyname>Abhayapala</keyname><forenames>Thushara</forenames></author><author><keyname>Poletti</keyname><forenames>Mark</forenames></author><author><keyname>Betlehem</keyname><forenames>Terence</forenames></author></authors><title>An Efficient Parameterization of the Room Transfer Function</title><categories>cs.SD</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an efficient parameterization of the Room Transfer
Function (RTF). Typically, the RTF rapidly varies with varying source and
receiver positions, hence requires an impractical number of point to point
measurements to characterize a given room. Therefore, we derive a novel RTF
parameterization that is robust to both receiver and source variations with the
following salient features: (i) The parameterization is given in terms of a
modal expansion of 3D basis functions. (ii) The aforementioned modal expansion
can be truncated at a finite number of modes given that the source and receiver
locations are from two sizeable spatial regions, which are arbitrarily
distributed. (iii) The parameter weights/coefficients are independent of the
source/receiver positions. Therefore, a finite set of coefficients is shown to
be capable of accurately calculating the RTF between any two arbitrary points
from a predefined spatial region where the source(s) lie and a pre-defined
spatial region where the receiver(s) lie. A practical method to measure the RTF
coefficients is also provided, which only requires a single microphone unit and
a single loudspeaker unit, given that the room characteristics remain
stationary over time. The accuracy of the above parameterization is verified
using appropriate simulation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04388</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04388</id><created>2015-05-17</created><authors><author><keyname>Evans</keyname><forenames>William S.</forenames></author><author><keyname>Liotta</keyname><forenames>Giuseppe</forenames></author><author><keyname>Montecchiani</keyname><forenames>Fabrizio</forenames></author></authors><title>Simultaneous Visibility Representations of Plane st-graphs Using
  L-shapes</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\langle G_r,G_b \rangle$ be a pair of plane $st$-graphs with the same
vertex set $V$. A simultaneous visibility representation with L-shapes of
$\langle G_r,G_b \rangle$ is a pair of bar visibility representations
$\langle\Gamma_r,\Gamma_b\rangle$ such that, for every vertex $v \in V$,
$\Gamma_r(v)$ and $\Gamma_b(v)$ are a horizontal and a vertical segment, which
share an end-point. In other words, every vertex is drawn as an $L$-shape,
every edge of $G_r$ is a vertical visibility segment, and every edge of $G_b$
is a horizontal visibility segment. Also, no two L-shapes intersect each other.
An L-shape has four possible rotations, and we assume that each vertex is given
a rotation for its L-shape as part of the input. Our main results are: (i) a
characterization of those pairs of plane $st$-graphs admitting such a
representation, (ii) a cubic time algorithm to recognize them, and (iii) a
linear time drawing algorithm if the test is positive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04394</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04394</id><created>2015-05-17</created><updated>2015-09-02</updated><authors><author><keyname>Xiang</keyname><forenames>Ju</forenames></author><author><keyname>Hu</keyname><forenames>Ke</forenames></author><author><keyname>Hu</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Li</keyname><forenames>Jian-Ming</forenames></author></authors><title>Analysis and perturbation of degree correlation in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 4 figures</comments><journal-ref>EPL, 111 (2015) 48003</journal-ref><doi>10.1209/0295-5075/111/48003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Degree correlation is an important topological property common to many
real-world networks. In this paper, the statistical measures for characterizing
the degree correlation in networks are investigated analytically. We give an
exact proof of the consistency for the statistical measures, reveal the general
linear relation in the degree correlation, which provide a simple and
interesting perspective on the analysis of the degree correlation in complex
networks. By using the general linear analysis, we investigate the perturbation
of the degree correlation in complex networks caused by the addition of few
nodes and the rich club. The results show that the assortativity of homogeneous
networks such as the ER graphs is easily to be affected strongly by the simple
structural changes, while it has only slight variation for heterogeneous
networks with broad degree distribution such as the scale-free networks.
Clearly, the homogeneous networks are more sensitive for the perturbation than
the heterogeneous networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04399</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04399</id><created>2015-05-17</created><authors><author><keyname>Yoon</keyname><forenames>Jangho</forenames></author><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author></authors><title>Elastic Routing in Wireless Ad Hoc Networks With Directional Antennas</title><categories>cs.IT math.IT</categories><comments>21 pages, 10 figures, Submitted to IEEE Transactions on Mobile
  Computing. Part of this paper was presented at the IEEE International
  Symposium on Information Theory, Honolulu, HI, June/July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Throughput scaling laws of an ad hoc network equipping directional antennas
at each node are analyzed. More specifically, this paper considers a general
framework in which the beam width of each node can scale at an arbitrary rate
relative to the number of nodes. We introduce an elastic routing protocol,
which enables to increase per-hop distance elastically according to the beam
width, while maintaining an average signal-to-interference-and-noise ratio at
each receiver as a constant. We then identify fundamental operating regimes
characterized according to the beam width scaling and analyze throughput
scaling laws for each of the regimes. The elastic routing is shown to achieve a
much better throughput scaling law than that of the conventional
nearest-neighbor multihop for all operating regimes. The gain comes from the
fact that more source--destination pairs can be simultaneously activated as the
beam width becomes narrower, which eventually leads to a linear throughput
scaling law. In addition, our framework is applied to a hybrid network
consisting of both wireless ad hoc nodes and infrastructure nodes. As a result,
in the hybrid network, we analyze a further improved throughput scaling law and
identify the operating regime where the use of directional antennas is
beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04401</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04401</id><created>2015-05-17</created><authors><author><keyname>Fass</keyname><forenames>Sebastian</forenames></author><author><keyname>Turner</keyname><forenames>Kevin</forenames></author></authors><title>The quantitative and qualitative content analysis of marketing
  literature for innovative information systems: the Aldrich Archive</title><categories>cs.CY</categories><comments>Published at arXiv on May 17th 2015, 11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Aldrich Archive is a collection of technical and marketing material
covering the period from 1977 to 2000; the physical documents are in the
process of being digitised and made available on the internet. The Aldrich
Archive includes contemporaneous case studies of end-user computer systems that
were used for marketing purposes. This paper analyses these case studies of
innovative information systems 1980 - 1990 using a quantitative and qualitative
content analysis. The major aim of this research paper is to find out how
innovative information systems were marketed in the decade from 1980 to 1990.
The paper uses a double-step content analysis and does not focus on one method
of content analysis only. The reason for choosing this approach is to combine
the advantages of both quantitative and qualitative content analysis. The
results of the quantitative content analysis indicated that the focus of the
marketing material would be on information management / information supply. But
the qualitative analysis revealed that the focus is on monetary advantages. The
strong focus on monetary advantages of information technology seems typical for
the 1980s and 1990s. In 1987, Robert Solow stated you can see the computer age
everywhere but in the productivity statistics. This paradox caused a lot of
discussion: since the introduction of the IT productivity paradox the business
value of information technology has been the topic of many debates by
practitioners as well as by academics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04402</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04402</id><created>2015-05-17</created><authors><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author><author><keyname>Blum</keyname><forenames>Rick S.</forenames></author></authors><title>An Electrical Structure-Based Approach to PMU Placement in the Electric
  Power Grid</title><categories>cs.SY</categories><comments>8 pages, submitted to IEEE Transactions on Smart Grid. arXiv admin
  note: text overlap with arXiv:1309.1300</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The phasor measurement unit (PMU) placement problem is revisited by taking
into account a stronger characterization of the electrical connectedness
between various buses in the grid. To facilitate this study, the placement
problem is approached from the perspective of the \emph{electrical structure}
which, unlike previous work on PMU placement, accounts for the sensitivity
between power injections and nodal phase angle differences between various
buses in the power network. The problem is formulated as a binary integer
program with the objective to minimize the number of PMUs for complete network
observability in the absence of zero injection measurements. The implication of
the proposed approach on static state estimation and fault detection algorithms
incorporating PMU measurements is analyzed. Results show a significant
improvement in the performance of estimation and detection schemes by employing
the electrical structure-based PMU placement compared to its topological
counterpart. In light of recent advances in the electrical structure of the
grid, our study provides a more realistic perspective of PMU placement in the
electric power grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04406</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04406</id><created>2015-05-17</created><updated>2015-12-08</updated><authors><author><keyname>Bach</keyname><forenames>Stephen H.</forenames></author><author><keyname>Broecheler</keyname><forenames>Matthias</forenames></author><author><keyname>Huang</keyname><forenames>Bert</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author></authors><title>Hinge-Loss Markov Random Fields and Probabilistic Soft Logic</title><categories>cs.LG cs.AI stat.ML</categories><comments>Expanded and added empirical results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental challenge in developing high-impact machine learning
technologies is balancing the ability to model rich, structured domains with
the ability to scale to big data. Many important problem areas are both richly
structured and large scale, from social and biological networks, to knowledge
graphs and the Web, to images, video, and natural language. In this paper, we
introduce two new formalisms for modeling structured data, distinguished from
previous approaches by their ability to both capture rich structure and scale
to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new
kind of probabilistic graphical model that generalizes different approaches to
convex inference. We unite three approaches from the randomized algorithms,
probabilistic graphical models, and fuzzy logic communities, showing that all
three lead to the same inference objective. We then derive HL-MRFs by
generalizing this unified objective. The second new formalism, probabilistic
soft logic (PSL), is a probabilistic programming language that makes HL-MRFs
easy to define using a syntax based on first-order logic. We next introduce an
algorithm for inferring most-probable variable assignments (MAP inference) that
is much more scalable than general-purpose convex optimization software,
because it uses message passing to take advantage of sparse dependency
structures. We then show how to learn the parameters of HL-MRFs. The learned
HL-MRFs are as accurate as analogous discrete models, but much more scalable.
Together, these algorithms enable HL-MRFs and PSL to model rich, structured
data at scales not previously possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04409</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04409</id><created>2015-05-17</created><authors><author><keyname>Alur</keyname><forenames>Rajeev</forenames></author><author><keyname>Raghothaman</keyname><forenames>Mukund</forenames></author><author><keyname>Stergiou</keyname><forenames>Christos</forenames></author><author><keyname>Tripakis</keyname><forenames>Stavros</forenames></author><author><keyname>Udupa</keyname><forenames>Abhishek</forenames></author></authors><title>Automatic Completion of Distributed Protocols with Symmetry</title><categories>cs.FL cs.LO</categories><comments>Full version of paper presented at CAV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed protocol is typically modeled as a set of communicating
processes, where each process is described as an extended state machine along
with fairness assumptions, and its correctness is specified using safety and
liveness requirements. Designing correct distributed protocols is a challenging
task. Aimed at simplifying this task, we allow the designer to leave some of
the guards and updates to state variables in the description of extended state
machines as unknown functions. The protocol completion problem then is to find
interpretations for these unknown functions while guaranteeing correctness. In
many distributed protocols, process behaviors are naturally symmetric, and
thus, synthesized expressions are further required to obey symmetry
constraints. Our counterexample-guided synthesis algorithm consists of
repeatedly invoking two phases. In the first phase, candidates for unknown
expressions are generated using the SMT solver Z3. This phase requires
carefully orchestrating constraints to enforce the desired symmetry in
read/write accesses. In the second phase, the resulting completed protocol is
checked for correctness using a custom-built model checker that handles
fairness assumptions, safety and liveness requirements, and exploits symmetry.
When model checking fails, our tool examines a set of counterexamples to
safety/liveness properties to generate constraints on unknown functions that
must be satisfied by subsequent completions. For evaluation, we show that our
prototype is able to automatically discover interesting missing details in
distributed protocols for mutual exclusion, self stabilization, and cache
coherence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04417</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04417</id><created>2015-05-17</created><updated>2016-01-10</updated><authors><author><keyname>Inggs</keyname><forenames>Gordon</forenames></author><author><keyname>Thomas</keyname><forenames>David B.</forenames></author><author><keyname>Luk</keyname><forenames>Wayne</forenames></author></authors><title>A Domain Specific Approach to High Performance Heterogeneous Computing</title><categories>cs.DC cs.CE</categories><comments>14 pages, preprint draft, major revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Users of heterogeneous computing systems face two problems: firstly, in
understanding the trade-off relationships between the observable
characteristics of their applications, such as latency and quality of the
result, and secondly, how to exploit knowledge of these characteristics to
allocate work to distributed computing platforms efficiently. A domain specific
approach addresses both of these problems. By considering a subset of
operations or functions, models of the observable characteristics or domain
metrics may be formulated in advance, and populated at run-time for particular
task instances. These metric models can then be used to express the allocation
of work as a constrained integer program, which can be solved using heuristics,
machine learning or Mixed Integer Linear Programming (MILP) frameworks.
  These claims are illustrated using the example domain of derivatives pricing
in computational finance, with the domain metrics of workload latency or
makespan and pricing accuracy. For a large, varied workload of 128
Black-Scholes and Heston model-based option pricing tasks, running upon a
diverse array of 16 Multicore CPUs, GPUs and FPGAs platforms, predictions made
by models of both the makespan and accuracy are generally within 10% of the
run-time performance. When these models are used as inputs to machine learning
and MILP-based workload allocation approaches, a latency improvement of up to
24 and 270 times over the heuristic approach is seen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04420</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04420</id><created>2015-05-17</created><authors><author><keyname>de Lhoneux</keyname><forenames>Miryam</forenames></author></authors><title>CCG Parsing and Multiword Expressions</title><categories>cs.CL</categories><comments>MSc thesis, The University of Edinburgh, 2014, School of Informatics,
  MSc Artificial Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis presents a study about the integration of information about
Multiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar
(CCG). We build on previous work which has shown the benefit of adding
information about MWEs to syntactic parsing by implementing a similar pipeline
with CCG parsing. More specifically, we collapse MWEs to one token in training
and test data in CCGbank, a corpus which contains sentences annotated with CCG
derivations. Our collapsing algorithm however can only deal with MWEs when they
form a constituent in the data which is one of the limitations of our approach.
  We study the effect of collapsing training and test data. A parsing effect
can be obtained if collapsed data help the parser in its decisions and a
training effect can be obtained if training on the collapsed data improves
results. We also collapse the gold standard and show that our model
significantly outperforms the baseline model on our gold standard, which
indicates that there is a training effect. We show that the baseline model
performs significantly better on our gold standard when the data are collapsed
before parsing than when the data are collapsed after parsing which indicates
that there is a parsing effect. We show that these results can lead to improved
performance on the non-collapsed standard benchmark although we fail to show
that it does so significantly. We conclude that despite the limited settings,
there are noticeable improvements from using MWEs in parsing. We discuss ways
in which the incorporation of MWEs into parsing can be improved and hypothesize
that this will lead to more substantial results.
  We finally show that turning the MWE recognition part of the pipeline into an
experimental part is a useful thing to do as we obtain different results with
different recognizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04424</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04424</id><created>2015-05-17</created><authors><author><keyname>Haloi</keyname><forenames>Mrinal</forenames></author></authors><title>Improved Microaneurysm Detection using Deep Neural Networks</title><categories>cs.CV</categories><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel microaneurysm (MA) detection for early
dieabetic ratinopathy screening using color fundus images. Since MA usually the
first lesions to appear as a indicator of diabetic retinopathy, accurate
detection of MA is necessary for treatment. Each pixel of the image is
classified as either MA or non-MA using deep neural network with dropout
training procedure using maxout activation function. No preprocessing step or
manual feature extraction is required. Substantial improvements over standard
MA detection method based on pipeline of preprocessing, feature extraction,
classification followed by postprocessing is achieved. The presented method is
evaluated in publicly available Retinopathy Online Challenge (ROC) and
Diaretdb1v2 database and achieved state-of-the-art accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04427</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04427</id><created>2015-05-17</created><authors><author><keyname>Lan</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Yao</keyname><forenames>Dezhong</forenames></author><author><keyname>Lin</keyname><forenames>Ming</forenames></author><author><keyname>Yu</keyname><forenames>Shoou-I</forenames></author><author><keyname>Hauptmann</keyname><forenames>Alexander</forenames></author></authors><title>The Best of Both Worlds: Combining Data-independent and Data-driven
  Approaches for Action Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the success of data-driven convolutional neural networks (CNNs)
in object recognition on static images, researchers are working hard towards
developing CNN equivalents for learning video features. However, learning video
features globally has proven to be quite a challenge due to its high
dimensionality, the lack of labelled data and the difficulty in processing
large-scale video data. Therefore, we propose to leverage effective techniques
from both data-driven and data-independent approaches to improve action
recognition system.
  Our contribution is three-fold. First, we propose a two-stream Stacked
Convolutional Independent Subspace Analysis (ConvISA) architecture to show that
unsupervised learning methods can significantly boost the performance of
traditional local features extracted from data-independent models. Second, we
demonstrate that by learning on video volumes detected by Improved Dense
Trajectory (IDT), we can seamlessly combine our novel local descriptors with
hand-crafted descriptors. Thus we can utilize available feature enhancing
techniques developed for hand-crafted descriptors. Finally, similar to
multi-class classification framework in CNNs, we propose a training-free
re-ranking technique that exploits the relationship among action classes to
improve the overall performance. Our experimental results on four benchmark
action recognition datasets show significantly improved performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04437</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04437</id><created>2015-05-17</created><authors><author><keyname>Gray</keyname><forenames>Norman</forenames></author></authors><title>Xoxa: a lightweight approach to normalizing and signing XML</title><categories>cs.CR</categories><comments>For submission to 'Software: Practice and Experience'</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptographically signing XML, and normalizing it prior to signing, are
forbiddingly intricate problems in the general case. This is largely because of
the complexities of the XML Information Set. We can define a more aggressive
normalization, which dispenses with distinctions and features which are
unimportant in a large class of cases, and thus define a straightforwardly
implementable and portable signature framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04449</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04449</id><created>2015-05-17</created><authors><author><keyname>Ebongue</keyname><forenames>Jean Louis Fendji Kedieng</forenames></author></authors><title>Rethinking Network Connectivity in Rural Communities in Cameroon</title><categories>cs.CY</categories><comments>10 pages, 10 figures, 1 table, conference IST-Africa 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To bridge the digital divide between the urban and rural regions, the
government of Cameroon has launched the Multipurpose Community Telecentres
(MCT). But this project does not seems to sustain the local development. The
aim of this study is threefold: to determine the ICT penetration in rural
Cameroon and Internet adoption; to evaluate the impact of MCTs in rural region
and to provide some recommendations for both the network planning and the
development of suitable services. The study considers two rural communities in
Cameroon. The results show that despite low incomes, and MCTs that are missing
their goal, there is some readiness of local populations to welcome ICT
projects in order to improve their daily life and activities. To design
sustainable ICT projects for those regions, we provide some recommendations
from network design to business strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04467</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04467</id><created>2015-05-17</created><authors><author><keyname>Devlin</keyname><forenames>Jacob</forenames></author><author><keyname>Gupta</keyname><forenames>Saurabh</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Mitchell</keyname><forenames>Margaret</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author></authors><title>Exploring Nearest Neighbor Approaches for Image Captioning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a variety of nearest neighbor baseline approaches for image
captioning. These approaches find a set of nearest neighbor images in the
training set from which a caption may be borrowed for the query image. We
select a caption for the query image by finding the caption that best
represents the &quot;consensus&quot; of the set of candidate captions gathered from the
nearest neighbor images. When measured by automatic evaluation metrics on the
MS COCO caption evaluation server, these approaches perform as well as many
recent approaches that generate novel captions. However, human studies show
that a method that generates novel captions is still preferred over the nearest
neighbor approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04474</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04474</id><created>2015-05-17</created><authors><author><keyname>Gupta</keyname><forenames>Saurabh</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Visual Semantic Role Labeling</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the problem of Visual Semantic Role Labeling:
given an image we want to detect people doing actions and localize the objects
of interaction. Classical approaches to action recognition either study the
task of action classification at the image or video clip level or at best
produce a bounding box around the person doing the action. We believe such an
output is inadequate and a complete understanding can only come when we are
able to associate objects in the scene to the different semantic roles of the
action. To enable progress towards this goal, we annotate a dataset of 16K
people instances in 10K images with actions they are doing and associate
objects in the scene with different semantic roles for each action. Finally, we
provide a set of baseline algorithms for this task and analyze error modes
providing directions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04488</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04488</id><created>2015-05-17</created><authors><author><keyname>Ren</keyname><forenames>Jian</forenames></author><author><keyname>Ding</keyname><forenames>Wanxing</forenames></author></authors><title>Exploring the Self-enhanced Mechanism of Interactive Advertising
  Phenomenon---Based on the Research of Three Cases</title><categories>cs.CY</categories><comments>16 pages,10 figures, The International Journal of Multimedia &amp; its
  Application,2015</comments><doi>10.5121/ijma.2015.7202</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Under the background of the new media era with the rapid development of
interactive advertising, this paper used case study method based on the summary
of the research of the communication effect of interactive advertising from
both domestic and foreign academia. This paper divided interactive advertising
into three types to examine ---- interactive ads on official website,
interactive ads based on SNS and interactive ads based on mobile media.
Furthermore, this paper induced and summarized a self-enhanced dissemination
mechanism of the interactive advertising, including three parts which are micro
level, meso level and macro level mechanism, micro level embodies core
interaction, inner interaction and outer interaction which reveal the whole
process of interact with contents, with people and with computer, and the
communication approach and spread speed shown in meso level which is
self-fission-type spread, finally in macro level the communication effect of IA
achieved the spiral increasing. In a word, this article enriches research
procedure of the interactive advertising communication effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04497</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04497</id><created>2015-05-17</created><authors><author><keyname>Daswani</keyname><forenames>Mayank</forenames></author><author><keyname>Leike</keyname><forenames>Jan</forenames></author></authors><title>A Definition of Happiness for Reinforcement Learning Agents</title><categories>cs.AI</categories><comments>AGI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is happiness for reinforcement learning agents? We seek a formal
definition satisfying a list of desiderata. Our proposed definition of
happiness is the temporal difference error, i.e. the difference between the
value of the obtained reward and observation and the agent's expectation of
this value. This definition satisfies most of our desiderata and is compatible
with empirical research on humans. We state several implications and discuss
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04502</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04502</id><created>2015-05-17</created><authors><author><keyname>Chen</keyname><forenames>Gengjie</forenames></author><author><keyname>St-Charles</keyname><forenames>Pierre-Luc</forenames></author><author><keyname>Bouachir</keyname><forenames>Wassim</forenames></author><author><keyname>Joeisseint</keyname><forenames>Thomas</forenames></author><author><keyname>Bilodeau</keyname><forenames>Guillaume-Alexandre</forenames></author><author><keyname>Bergevin</keyname><forenames>Robert</forenames></author></authors><title>Reproducible Evaluation of Pan-Tilt-Zoom Tracking</title><categories>cs.CV</categories><comments>This is an extended version of the 2015 ICIP paper &quot;Reproducible
  Evaluation of Pan-Tilt-Zoom Tracking&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking with a Pan-Tilt-Zoom (PTZ) camera has been a research topic in
computer vision for many years. However, it is very difficult to assess the
progress that has been made on this topic because there is no standard
evaluation methodology. The difficulty in evaluating PTZ tracking algorithms
arises from their dynamic nature. In contrast to other forms of tracking, PTZ
tracking involves both locating the target in the image and controlling the
motors of the camera to aim it so that the target stays in its field of view.
This type of tracking can only be performed online. In this paper, we propose a
new evaluation framework based on a virtual PTZ camera. With this framework,
tracking scenarios do not change for each experiment and we are able to
replicate online PTZ camera control and behavior including camera positioning
delays, tracker processing delays, and numerical zoom. We tested our evaluation
framework with the Camshift tracker to show its viability and to establish
baseline results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04511</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04511</id><created>2015-05-18</created><authors><author><keyname>Schilling</keyname><forenames>Simon J.</forenames></author></authors><title>Contribution to Temporal Fault Tree Analysis without Modularization and
  Transformation into the State Space</title><categories>cs.CE cs.LO</categories><comments>Translation into English of the german doctoral thesis &quot;Beitrag zur
  dynamischen Fehlerbaumanalyse ohne Modulbildung und zustandsbasierte
  Erweiterungen&quot; of Dr. Ing. Simon J. Schilling at the Bergische Universit\&quot;at
  Wuppertal
  (http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:de:hbz:468-20100070).
  This translation is licensed under a Creative Commons Attribution-ShareAlike
  4.0 License</comments><report-no>urn:nbn:de:hbz:468-20100070</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background:
  Fault tree analysis (FTA) is a well established method for qualitative as
well as probabilistic reliability and safety analysis. As a Boolean model it
does not support modelling of dynamic effects like sequence dependencies
between fault events. This work describes a method that allows consideration of
sequence dependencies without transformations into state-space.
  Concept:
  The new temporal fault tree analysis (TFTA) described in this work extends
the Boolean FTA. The TFTA is based on a new temporal logic which adds a concept
of time to the Boolean logic and algebra. This allows modelling of temporal
relationships between events using two new temporal operators (PAND and SAND).
With a set of temporal logic rules, a given temporal term may be simplified to
its temporal disjunctive normal form (TDNF) which is similar to the Boolean DNF
but includes event sequencies. In TDNF the top event's temporal system function
may be reduced to a list of minimal cutset sequences (MCSS). These allow
qualitative analyses similar to Boolean cutset analysis in normal FTA.
Furthermore the TFTA may also be used for probabilistic analyses without using
state-space models.
  Results:
  One significant aspect of the new TFTA described in this work is the
possibility to take sequence dependencies into account for qualitative and
probabilistic analyses without state-space transformations. Among others, this
allows for modelling of event sequencies at all levels within a fault tree, a
real qualitative analysis similar to the FTA's cutset analysis, and
quantification of sequence dependencies within the same model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04512</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04512</id><created>2015-05-18</created><authors><author><keyname>Shoari</keyname><forenames>Arian</forenames></author><author><keyname>Seyedi</keyname><forenames>Alireza</forenames></author></authors><title>On the Existence of an MVU Estimator for Target Localization with
  Censored, Noise Free Binary Detectors</title><categories>cs.IT math.IT</categories><comments>25 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of target localization with censored noise free binary detectors
is considered. In this setting only the detecting sensors report their
locations to the fusion center. It is proven that if the radius of detection is
not known to the fusion center, a minimum variance unbiased (MVU) estimator
does not exist. Also it is shown that when the radius is known the center of
mass of the possible target region is the MVU estimator. In addition, a
sub-optimum estimator is introduced whose performance is close to the MVU
estimator but is preferred computationally. Furthermore, minimal sufficient
statistics have been provided, both when the detection radius is known and when
it is not. Simulations confirmed that the derived MVU estimator outperforms
several heuristic location estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04514</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04514</id><created>2015-05-18</created><authors><author><keyname>Halldorsson</keyname><forenames>Magnus M.</forenames></author><author><keyname>Holzer</keyname><forenames>Stephan</forenames></author><author><keyname>Lynch</keyname><forenames>Nancy</forenames></author></authors><title>A Local Broadcast Layer for the SINR Network Model</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first algorithm that implements an abstract MAC (absMAC) layer
in the Signal-to-Interference-plus-Noise-Ratio (SINR) wireless network model.
We first prove that efficient SINR implementations are not possible for the
standard absMAC specification. We modify that specification to an &quot;approximate&quot;
version that better suits the SINR model. We give an efficient algorithm to
implement the modified specification, and use it to derive efficient algorithms
for higher-level problems of global broadcast and consensus.
  In particular, we show that the absMAC progress property has no efficient
implementation in terms of the SINR strong connectivity graph $G_{1-\epsilon}$,
which contains edges between nodes of distance at most $(1-\epsilon)$ times the
transmission range, where $\epsilon&gt;0$ is a small constant that can be chosen
by the user. This progress property bounds the time until a node is guaranteed
to receive some message when at least one of its neighbors is transmitting.
  To overcome this limitation, we introduce the slightly weaker notion of
approximate progress into the absMAC specification. We provide a fast
implementation of the modified specification, based on decomposing a known
algorithm into local and global parts. We analyze our algorithm in terms of
local parameters such as node degrees, rather than global parameters such as
the overall number of nodes. A key contribution is our demonstration that such
a local analysis is possible even in the presence of global interference.
  Our absMAC algorithm leads to several new, efficient algorithms for solving
higher-level problems in the SINR model. Namely, by combining our algorithm
with known high-level algorithms, we obtain an improved algorithm for global
single-message broadcast in the SINR model, and the first efficient algorithm
for multi-message broadcast in that model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04515</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04515</id><created>2015-05-18</created><authors><author><keyname>Rao</keyname><forenames>Vishwas</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>A Time-parallel Approach to Strong-constraint Four-dimensional
  Variational Data Assimilation</title><categories>cs.NA math.NA</categories><comments>22 Pages</comments><report-no>CSL TR-18-2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A parallel-in-time algorithm based on an augmented Lagrangian approach is
proposed to solve four-dimensional variational (4D-Var) data assimilation
problems. The assimilation window is divided into multiple sub-intervals that
allows to parallelize cost function and gradient computations. Solution
continuity equations across interval boundaries are added as constraints. The
augmented Lagrangian approach leads to a different formulation of the
variational data assimilation problem than weakly constrained 4D-Var. A
combination of serial and parallel 4D-Vars to increase performance is also
explored. The methodology is illustrated on data assimilation problems with
Lorenz-96 and the shallow water models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04518</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04518</id><created>2015-05-18</created><authors><author><keyname>Marriott</keyname><forenames>Chris</forenames></author><author><keyname>Chebib</keyname><forenames>Jobran</forenames></author></authors><title>Emergence-focused design in complex system simulation</title><categories>q-bio.PE cs.AI cs.NE</categories><comments>European Conference on Artificial Life 2015 - York, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emergence is a phenomenon taken for granted in science but also still not
well understood. We have developed a model of artificial genetic evolution
intended to allow for emergence on genetic, population and social levels. We
present the details of the current state of our environment, agent, and
reproductive models. In developing our models we have relied on a principle of
using non-linear systems to model as many systems as possible including
mutation and recombination, gene-environment interaction, agent metabolism,
agent survival, resource gathering and sexual reproduction. In this paper we
review the genetic dynamics that have emerged in our system including
genotype-phenotype divergence, genetic drift, pseudogenes, and gene
duplication. We conclude that emergence-focused design in complex system
simulation is necessary to reproduce the multilevel emergence seen in the
natural world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04527</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04527</id><created>2015-05-18</created><authors><author><keyname>Ibrahim</keyname><forenames>Noha</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Fr&#xe9;not</keyname><forenames>St&#xe9;phane</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>Semantic Service Substitution in Pervasive Environments</title><categories>cs.SE</categories><proxy>ccsd</proxy><journal-ref>International Journal of Services, Economics and Management
  (IJSEM), Inderscience, 2014, 6 (4), pp.283-309</journal-ref><doi>10.1504/IJSEM.2014.068244</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computing infrastructure where everything is a service offers many new
system and application possibilities. Among the main challenges, however, is
the issue of service substitution for the application execution in such
heterogeneous environments. An application would like to continue to execute
even when a service disappears, or it would like to benefit from the
environment by using better services with better QoS when possible. In this
article, we define a generic service model and describe the equivalence
relations between services considering the functionalities they propose and
their non functional QoS properties. We define semantic equivalence relations
between services and equivalence degree between non functional QoS properties.
Using these relations we propose semantic substitution mechanisms upon the
appearance and disappearance of services that fits the application needs. We
developed a prototype as a proof of concept and evaluated its efficiency over a
real use case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04532</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04532</id><created>2015-05-18</created><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author></authors><title>Large System Analysis of Cognitive Radio Network via Partially-Projected
  Regularized Zero-Forcing Precoding</title><categories>cs.IT math.IT</categories><comments>32 pages, 6 figures. IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a cognitive radio (CR) network in which a
secondary multiantenna base station (BS) attempts to communicate with multiple
secondary users (SUs) using the radio frequency spectrum that is originally
allocated to multiple primary users (PUs). Here, we employ partially-projected
regularized zero-forcing (PP-RZF) precoding to control the amount of
interference at the PUs and to minimize inter-SUs interference. The PP-RZF
precoding partially projects the channels of the SUs into the null space of the
channels from the secondary BS to the PUs. The regularization parameter and the
projection control parameter are used to balance the transmissions to the PUs
and the SUs. However, the search for the optimal parameters, which can maximize
the ergodic sum-rate of the CR network, is a demanding process because it
involves Monte-Carlo averaging. Then, we derive a deterministic expression for
the ergodic sum-rate achieved by the PP-RZF precoding using recent advancements
in large dimensional random matrix theory. The deterministic equivalent enables
us to efficiently determine the two critical parameters in the PP-RZF precoding
because no Monte-Carlo averaging is required. Several insights are also
obtained through the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04533</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04533</id><created>2015-05-18</created><authors><author><keyname>&#x10c;ern&#xfd;</keyname><forenames>Pavol</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund M.</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Radhakrishna</keyname><forenames>Arjun</forenames></author><author><keyname>Ryzhyk</keyname><forenames>Leonid</forenames></author><author><keyname>Samanta</keyname><forenames>Roopsha</forenames></author><author><keyname>Tarrach</keyname><forenames>Thorsten</forenames></author></authors><title>From Non-preemptive to Preemptive Scheduling using Synchronization
  Synthesis</title><categories>cs.PL</categories><comments>Liss is published as open-source at
  https://github.com/thorstent/Liss, Computer Aided Verification 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a computer-aided programming approach to concurrency. The approach
allows programmers to program assuming a friendly, non-preemptive scheduler,
and our synthesis procedure inserts synchronization to ensure that the final
program works even with a preemptive scheduler. The correctness specification
is implicit, inferred from the non-preemptive behavior. Let us consider
sequences of calls that the program makes to an external interface. The
specification requires that any such sequence produced under a preemptive
scheduler should be included in the set of such sequences produced under a
non-preemptive scheduler. The solution is based on a finitary abstraction, an
algorithm for bounded language inclusion modulo an independence relation, and
rules for inserting synchronization. We apply the approach to device-driver
programming, where the driver threads call the software interface of the device
and the API provided by the operating system. Our experiments demonstrate that
our synthesis method is precise and efficient, and, since it does not require
explicit specifications, is more practical than the conventional approach based
on user-provided assertions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04542</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04542</id><created>2015-05-18</created><authors><author><keyname>Ishii</keyname><forenames>Daisuke</forenames></author><author><keyname>Yoshizoe</keyname><forenames>Kazuki</forenames></author><author><keyname>Suzumura</keyname><forenames>Toyotaro</forenames></author></authors><title>Scalable Parallel Numerical Constraint Solver Using Global Load
  Balancing</title><categories>cs.DC cs.AI</categories><comments>To be presented at X10'15 Workshop</comments><acm-class>D.1.3</acm-class><doi>10.1145/2771774.2771776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scalable parallel solver for numerical constraint satisfaction
problems (NCSPs). Our parallelization scheme consists of homogeneous worker
solvers, each of which runs on an available core and communicates with others
via the global load balancing (GLB) method. The parallel solver is implemented
with X10 that provides an implementation of GLB as a library. In experiments,
several NCSPs from the literature were solved and attained up to 516-fold
speedup using 600 cores of the TSUBAME2.5 supercomputer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04546</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04546</id><created>2015-05-18</created><updated>2016-02-16</updated><authors><author><keyname>Yamauchi</keyname><forenames>Yukiko</forenames></author><author><keyname>Uehara</keyname><forenames>Taichi</forenames></author><author><keyname>Kijima</keyname><forenames>Shuji</forenames></author><author><keyname>Yamashita</keyname><forenames>Masafumi</forenames></author></authors><title>Plane Formation by Synchronous Mobile Robots in the Three Dimensional
  Euclidean Space</title><categories>cs.DC cs.RO</categories><msc-class>68Q85, 68W15</msc-class><acm-class>C.2.4; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creating a swarm of mobile computing entities frequently called robots,
agents or sensor nodes, with self-organization ability is a contemporary
challenge in distributed computing. Motivated by this, we investigate the plane
formation problem that requires a swarm of robots moving in the three
dimensional Euclidean space to land on a common plane. The robots are fully
synchronous and endowed with visual perception. But they do not have
identifiers, nor access to the global coordinate system, nor any means of
explicit communication with each other. Though there are plenty of results on
the agreement problem for robots in the two dimensional plane, for example, the
point formation problem, the pattern formation problem, and so on, this is the
first result for robots in the three dimensional space. This paper presents a
necessary and sufficient condition for fully-synchronous robots to solve the
plane formation problem that does not depend on obliviousness i.e., the
availability of local memory at robots. An implication of the result is
somewhat counter-intuitive: The robots cannot form a plane from most of the
semi-regular polyhedra, while they can form a plane from every regular
polyhedron (except a regular icosahedron), whose symmetry is usually considered
to be higher than any semi-regular polyhedrdon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04548</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04548</id><created>2015-05-18</created><authors><author><keyname>Milford</keyname><forenames>Michael</forenames></author><author><keyname>Kim</keyname><forenames>Hanme</forenames></author><author><keyname>Mangan</keyname><forenames>Michael</forenames></author><author><keyname>Leutenegger</keyname><forenames>Stefan</forenames></author><author><keyname>Stone</keyname><forenames>Tom</forenames></author><author><keyname>Webb</keyname><forenames>Barbara</forenames></author><author><keyname>Davison</keyname><forenames>Andrew</forenames></author></authors><title>Place Recognition with Event-based Cameras and a Neural Implementation
  of SeqSLAM</title><categories>cs.RO cs.CV</categories><comments>Paper accepted for presentation at the &quot;Innovative Sensing for
  Robotics: Focus on Neuromorphic Sensors&quot; workshop at the 2015 IEEE
  International Conference on Robotics and Automation, 8 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-based cameras offer much potential to the fields of robotics and
computer vision, in part due to their large dynamic range and extremely high
&quot;frame rates&quot;. These attributes make them, at least in theory, particularly
suitable for enabling tasks like navigation and mapping on high speed robotic
platforms under challenging lighting conditions, a task which has been
particularly challenging for traditional algorithms and camera sensors. Before
these tasks become feasible however, progress must be made towards adapting and
innovating current RGB-camera-based algorithms to work with event-based
cameras. In this paper we present ongoing research investigating two distinct
approaches to incorporating event-based cameras for robotic navigation: the
investigation of suitable place recognition / loop closure techniques, and the
development of efficient neural implementations of place recognition techniques
that enable the possibility of place recognition using event-based cameras at
very high frame rates using neuromorphic computing hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04557</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04557</id><created>2015-05-18</created><updated>2015-06-17</updated><authors><author><keyname>M&#xfc;ller</keyname><forenames>Martin Klaus</forenames></author><author><keyname>Taranetz</keyname><forenames>Martin</forenames></author><author><keyname>Rupp</keyname><forenames>Markus</forenames></author></authors><title>Providing Current and Future Cellular Services to High Speed Trains</title><categories>cs.NI</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand for a broadband wireless connection is nowadays no longer limited
to stationary situations, but also required while traveling. Therefore, there
exist combined efforts to provide wireless access also on High Speed Trains
(HSTs), in order to add to the attractiveness of this means for transportation.
Installing an additional relay on the train, to facilitate the communication,
is an approach that has already been extensively discussed in literature. The
possibility of a direct communication between the base station and the
passenger has been neglected until now, despite it having numerous advantages.
Therefore, a comparison between these two opposing approaches is presented in
this paper, accompanied by a detailed discussion of the related aspects. The
focus is set on the feasibility of the direct link approach, including
simulation results. Further technical issues are also presented, especially
regarding the interdependencies of the different aspects and providing a view
of mobile- and train-operators on the topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04560</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04560</id><created>2015-05-18</created><authors><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author><author><keyname>Patranabis</keyname><forenames>Sikhar</forenames></author><author><keyname>Goyal</keyname><forenames>Pawan</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author></authors><title>On the Formation of Circles in Co-authorship Networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 8 figures, 1 table, 21st ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining, Sydney, Australia, Aug 10-13, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of an overwhelmingly large amount of bibliographic
information including citation and co-authorship data makes it imperative to
have a systematic approach that will enable an author to organize her own
personal academic network profitably. An effective method could be to have
one's co-authorship network arranged into a set of &quot;circles&quot;, which has been a
recent practice for organizing relationships (e.g., friendship) in many online
social networks.
  In this paper, we propose an unsupervised approach to automatically detect
circles in an ego network such that each circle represents a densely knit
community of researchers. Our model is an unsupervised method which combines a
variety of node features and node similarity measures. The model is built from
a rich co-authorship network data of more than 8 hundred thousand authors. In
the first level of evaluation, our model achieves 13.33% improvement in terms
of overlapping modularity compared to the best among four state-of-the-art
community detection methods. Further, we conduct a task-based evaluation -- two
basic frameworks for collaboration prediction are considered with the circle
information (obtained from our model) included in the feature set. Experimental
results show that including the circle information detected by our model
improves the prediction performance by 9.87% and 15.25% on average in terms of
AUC (Area under the ROC) and P rec@20 (Precision at Top 20) respectively
compared to the case, where the circle information is not present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04563</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04563</id><created>2015-05-18</created><authors><author><keyname>Graziotin</keyname><forenames>Daniel</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames></author></authors><title>The Affect of Software Developers: Common Misconceptions and
  Measurements</title><categories>cs.SE cs.HC</categories><comments>2 pages. Research note to be presented at the 2015 IEEE/ACM 8th
  International Workshop on Cooperative and Human Aspects of Software
  Engineering (CHASE 2015)</comments><journal-ref>8th Intl. Workshop on Cooperative and Human Aspects of Software
  Engineering (CHASE '15), pp. 123-124, 2015</journal-ref><doi>10.1109/CHASE.2015.23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of affects (i.e., emotions, moods) in the workplace has received a
lot of attention in the last 15 years. Despite the fact that software
development has been shown to be intellectual, creative, and driven by
cognitive activities, and that affects have a deep influence on cognitive
activities, software engineering research lacks an understanding of the affects
of software developers. This note provides (1) common misconceptions of affects
when dealing with job satisfaction, motivation, commitment, well-being, and
happiness; (2) validated measurement instruments for affect measurement; and
(3) our recommendations when measuring the affects of software developers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04565</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04565</id><created>2015-05-18</created><authors><author><keyname>Wildgaard</keyname><forenames>Lorna</forenames></author></authors><title>A critical cluster analysis of 44 indicators of author-level performance</title><categories>cs.DL</categories><comments>28 pages, 7 tables, 2 figures, 2 appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the relationship between author-level bibliometric
indicators and the researchers the &quot;measure&quot;, exemplified across five academic
seniorities and four disciplines. Using cluster methodology, the disciplinary
and seniority appropriateness of author-level indicators is examined.
Publication and citation data for 741 researchers across Astronomy,
Environmental Science, Philosophy and Public Health was collected in Web of
Science (WoS). Forty-four indicators of individual performance were computed
using the data. A two-step cluster analysis using IBM SPSS version 22 was
performed, followed by a risk analysis and ordinal logistic regression to
explore cluster membership. Indicator scores were contextualized using the
individual researcher's curriculum vitae. Four different clusters based on
indicator scores ranked researchers as low, middle, high and extremely high
performers. The results show that different indicators were appropriate in
demarcating ranked performance in different disciplines. In Astronomy the h2
indicator, sum pp top prop in Environmental Science, Q2 in Philosophy and
e-index in Public Health. The regression and odds analysis showed individual
level indicator scores were primarily dependent on the number of years since
the researcher's first publication registered in WoS, number of publications
and number of citations. Seniority classification was secondary therefore no
seniority appropriate indicators were confidently identified. Cluster
methodology proved useful in identifying disciplinary appropriate indicators
providing the preliminary data preparation was thorough but needed to be
supplemented by other analyses to validate the results. A general disconnection
between the performance of the researcher on their curriculum vitae and the
performance of the researcher based on bibliometric indicators was observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04569</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04569</id><created>2015-05-18</created><authors><author><keyname>Sau</keyname><forenames>Suman</forenames></author><author><keyname>Mandal</keyname><forenames>Swagata</forenames></author><author><keyname>Saini</keyname><forenames>Jogender</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Subhasis</forenames></author></authors><title>High speed fault tolerant secure communication for muon chamber using
  fpga based gbt emulator</title><categories>cs.AR</categories><doi>10.1088/1742-6596/664/8/082049</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Compressed Baryonic Matter (CBM) experiment is a part of the Facility for
Antiproton and Ion Research (FAIR) in Darmstadt at the GSI. The CBM experiment
will investigate the highly compressed nuclear matter using nucleus-nucleus
collisions. This experiment will examine heavy-ion collisions in fixed target
geometry and will be able to measure hadrons, electrons and muons. CBM requires
precise time synchronization, compact hardware, radiation tolerance,
self-triggered front-end electronics, efficient data aggregation schemes and
capability to handle high data rate (up to several TB/s). As a part of the
implementation of read out chain of MUCH in India, we have tried to implement
FPGA based emulator of GBTx in India. GBTx is a radiation tolerant ASIC that
can be used to implement multipurpose high speed bidirectional optical links
for high-energy physics (HEP) experiments and is developed by CERN. GBTx will
be used in highly irradiated area and more prone to be affected by multi bit
error. To mitigate this effect instead of single bit error correcting RS code
we have used two bit error correcting (15, 7) BCH code. It will increase the
redundancy which in turn increases the reliability of the coded data. So the
coded data will be less prone to be affected by noise due to radiation. Data
will go from detector to PC through multiple nodes through the communication
channel. In order to make the data communication secure, advanced encryption
standard (AES - a symmetric key cryptography) and RSA (asymmetric key
cryptography) are used after the channel coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04578</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04578</id><created>2015-05-18</created><authors><author><keyname>Diamant</keyname><forenames>Emanuel</forenames></author></authors><title>Advances in Artificial Intelligence: Deep Intentions, Shallow
  Achievements</title><categories>cs.AI q-bio.NC</categories><comments>The paper was submitted to the ICAI'15 conference (Las Vegas, Nevada,
  USA, July 27-30, 2015) and was accepted as a poster presentation. arXiv admin
  note: substantial text overlap with arXiv:1502.04791</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, AI has made a remarkable progress due to recently
revived Deep Learning technology. Deep Learning enables to process large
amounts of data using simplified neuron networks that simulate the way in which
the brain works. At the same time, there is another point of view that posits
that brain is processing information, not data. This duality hampered AI
progress for years. To provide a remedy for this situation, I propose a new
definition of information that considers it as a coupling between two separate
entities - physical information (that implies data processing) and semantic
information (that provides physical information interpretation). In such a
case, intelligence arises as a result of information processing. The paper
points on the consequences of this turn for the AI design philosophy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04581</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04581</id><created>2015-05-18</created><authors><author><keyname>Chen</keyname><forenames>Hong-Yi</forenames></author><author><keyname>David</keyname><forenames>Cristina</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Schrammel</keyname><forenames>Peter</forenames></author><author><keyname>Wachter</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Synthesising Interprocedural Bit-Precise Termination Proofs (extended
  version)</title><categories>cs.SE cs.LO</categories><comments>extended version</comments><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proving program termination is key to guaranteeing absence of undesirable
behaviour, such as hanging programs and even security vulnerabilities such as
denial-of-service attacks. To make termination checks scale to large systems,
interprocedural termination analysis seems essential, which is a largely
unexplored area of research in termination analysis, where most effort has
focussed on difficult single-procedure problems. We present a modular
termination analysis for C programs using template-based interprocedural
summarisation. Our analysis combines a context-sensitive, over-approximating
forward analysis with the inference of under-approximating preconditions for
termination. Bit-precise termination arguments are synthesised over
lexicographic linear ranking function templates. Our experimental results show
that our tool 2LS outperforms state-of-the-art alternatives, and demonstrate
the clear advantage of interprocedural reasoning over monolithic analysis in
terms of efficiency, while retaining comparable precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04585</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04585</id><created>2015-05-18</created><authors><author><keyname>Thai</keyname><forenames>Duy Hoang</forenames></author><author><keyname>Gottschlich</keyname><forenames>Carsten</forenames></author></authors><title>Global Variational Method for Fingerprint Segmentation by Three-part
  Decomposition</title><categories>cs.CV</categories><doi>10.1049/iet-bmt.2015.0010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verifying an identity claim by fingerprint recognition is a commonplace
experience for millions of people in their daily life, e.g. for unlocking a
tablet computer or smartphone. The first processing step after fingerprint
image acquisition is segmentation, i.e. dividing a fingerprint image into a
foreground region which contains the relevant features for the comparison
algorithm, and a background region. We propose a novel segmentation method by
global three-part decomposition (G3PD). Based on global variational analysis,
the G3PD method decomposes a fingerprint image into cartoon, texture and noise
parts. After decomposition, the foreground region is obtained from the non-zero
coefficients in the texture image using morphological processing. The
segmentation performance of the G3PD method is compared to five
state-of-the-art methods on a benchmark which comprises manually marked ground
truth segmentation for 10560 images. Performance evaluations show that the G3PD
method consistently outperforms existing methods in terms of segmentation
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04597</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04597</id><created>2015-05-18</created><authors><author><keyname>Ronneberger</keyname><forenames>Olaf</forenames></author><author><keyname>Fischer</keyname><forenames>Philipp</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>U-Net: Convolutional Networks for Biomedical Image Segmentation</title><categories>cs.CV</categories><comments>conditionally accepted at MICCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is large consent that successful training of deep networks requires
many thousand annotated training samples. In this paper, we present a network
and training strategy that relies on the strong use of data augmentation to use
the available annotated samples more efficiently. The architecture consists of
a contracting path to capture context and a symmetric expanding path that
enables precise localization. We show that such a network can be trained
end-to-end from very few images and outperforms the prior best method (a
sliding-window convolutional network) on the ISBI challenge for segmentation of
neuronal structures in electron microscopic stacks. Using the same network
trained on transmitted light microscopy images (phase contrast and DIC) we won
the ISBI cell tracking challenge 2015 in these categories by a large margin.
Moreover, the network is fast. Segmentation of a 512x512 image takes less than
a second on a recent GPU. The full implementation (based on Caffe) and the
trained networks are available at
http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04604</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04604</id><created>2015-05-18</created><authors><author><keyname>Kluth</keyname><forenames>Stefan</forenames></author><author><keyname>Pia</keyname><forenames>Maria Grazia</forenames></author><author><keyname>Schoerner-Sadenius</keyname><forenames>Thomas</forenames></author><author><keyname>Steinbach</keyname><forenames>Peter</forenames></author></authors><title>How do particle physicists learn the programming concepts they need?</title><categories>physics.ed-ph cs.SE hep-ex physics.comp-ph</categories><comments>8 pages, 2 figures, CHEP2015 proceedings</comments><doi>10.1088/1742-6596/664/6/062048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to read, use and develop code efficiently and successfully is a
key ingredient in modern particle physics. We report the experience of a
training program, identified as &quot;Advanced Programming Concepts&quot;, that
introduces software concepts, methods and techniques to work effectively on a
daily basis in a HEP experiment or other programming intensive fields. This
paper illustrates the principles, motivations and methods that shape the
&quot;Advanced Computing Concepts&quot; training program, the knowledge base that it
conveys, an analysis of the feedback received so far, and the integration of
these concepts in the software development process of the experiments as well
as its applicability to a wider audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04609</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04609</id><created>2015-05-18</created><authors><author><keyname>Lavou&#xe9;</keyname><forenames>&#xc9;lise</forenames></author><author><keyname>Molinari</keyname><forenames>Ga&#xeb;lle</forenames></author><author><keyname>Pri&#xe9;</keyname><forenames>Yannick</forenames></author><author><keyname>Khezami</keyname><forenames>Saf&#xe8;</forenames></author></authors><title>Reflection-in-Action Markers for Reflection-on-Action in
  Computer-Supported Collaborative Learning Settings</title><categories>cs.CY</categories><comments>Accepted for publication by Computers &amp; Education</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We describe an exploratory study on the use of markers set during a
synchronous collaborative interaction (reflection-in-action) for later
construction of reflection reports upon the collaboration that occurred
(reflection-on-action). During two sessions, pairs of students used the Visu
videoconferencing tool for synchronous interaction and marker setting
(positive, negative or free) and then individual report building on the
interaction (using markers or not). A quantitative descriptive analysis was
conducted on the markers put in action, on their use to reflect on action and
on the reflection categories of the sentences in these reports. Results show
that the students (1) used the markers equally as a note-taking and reflection
means during the interaction, (2) used mainly positive markers both to reflect
in and on action; (3) paid more attention in identifying what worked in their
interaction (conservative direction) rather than in planning on how to improve
their group work (progressive direction); (4) used mainly their own markers to
reflect on action, with an increase in the use of their partners's markers in
the second reflection reports; (5) reflected mainly on their partner in the
first reflection reports and more on themselves in the second reports to
justify themselves and to express their satisfaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04617</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04617</id><created>2015-05-18</created><authors><author><keyname>Wang</keyname><forenames>Liping</forenames></author><author><keyname>Chen</keyname><forenames>Songcan</forenames></author></authors><title>Joint Representation Classification for Collective Face Recognition</title><categories>cs.CV math.OC</categories><comments>20 pages, 5 figures, 3 tables; 4 algorithms, 2 lemmas and 2 theorems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representation based classification (SRC) is popularly used in many
applications such as face recognition, and implemented in two steps:
representation coding and classification. For a given set of testing images,
SRC codes every image over the base images as a sparse representation then
classifies it to the class with the least representation error. This scheme
utilizes an individual representation rather than the collective one to
classify such a set of images, doing so obviously ignores the correlation among
the given images. In this paper, a joint representation classification (JRC)
for collective face recognition is proposed. JRC takes the correlation of
multiple images as well as a single representation into account. Under the
assumption that the given face images are generally related to each other, JRC
codes all the testing images over the base images simultaneously to facilitate
recognition. To this end, the testing inputs are aligned into a matrix and the
joint representation coding is formulated to a generalized
$l_{2,q}-l_{2,p}$-minimization problem. To uniformly solve the induced
optimization problems for any $q\in[1,2]$ and $p\in (0,2]$, an iterative
quadratic method (IQM) is developed. IQM is proved to be a strict descent
algorithm with convergence to the optimal solution. Moreover, a more practical
IQM is proposed for large-scale case. Experimental results on three public
databases show that the JRC with practical IQM no only saves much computational
cost but also achieves better performance in collective face recognition than
the state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04618</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04618</id><created>2015-05-18</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>Fractally-organized Connectionist Networks: Conjectures and Preliminary
  Results</title><categories>cs.NE</categories><comments>Draft of an invited paper for PEWET (1st Workshop on PErvasive WEb
  Technologies, trends and challenges),
  http://www.irpps.cnr.it/en/events/call-for-papers-pewet-pervasive-web-technologies-trends-and-challenges</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A strict interpretation of connectionism mandates complex networks of simple
components. The question here is, is this simplicity to be interpreted in
absolute terms? I conjecture that absolute simplicity might not be an essential
attribute of connectionism, and that it may be effectively exchanged with a
requirement for relative simplicity, namely simplicity with respect to the
current organizational level. In this paper I provide some elements to the
analysis of the above question. In particular I conjecture that fractally
organized connectionist networks may provide a convenient means to achive what
Leibniz calls an &quot;art of complication&quot;, namely an effective way to encapsulate
complexity and practically extend the applicability of connectionism to domains
such as sociotechnical system modeling and design. Preliminary evidence to my
claim is brought by considering the design of the software architecture
designed for the telemonitoring service of Flemish project &quot;Little Sister&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04623</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04623</id><created>2015-05-18</created><authors><author><keyname>Catini</keyname><forenames>Roberto</forenames></author><author><keyname>Karamshuk</keyname><forenames>Dmytro</forenames></author><author><keyname>Penner</keyname><forenames>Orion</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author></authors><title>Identifying Geographic Clusters: A Network Analytic Approach</title><categories>physics.soc-ph cs.SI</categories><doi>10.1016/j.respol.2015.01.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there has been a growing interest in the role of networks and
clusters in the global economy. Despite being a popular research topic in
economics, sociology and urban studies, geographical clustering of human
activity has often studied been by means of predetermined geographical units
such as administrative divisions and metropolitan areas. This approach is
intrinsically time invariant and it does not allow one to differentiate between
different activities. Our goal in this paper is to present a new methodology
for identifying clusters, that can be applied to different empirical settings.
We use a graph approach based on k-shell decomposition to analyze world
biomedical research clusters based on PubMed scientific publications. We
identify research institutions and locate their activities in geographical
clusters. Leading areas of scientific production and their top performing
research institutions are consistently identified at different geographic
scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04627</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04627</id><created>2015-05-18</created><authors><author><keyname>Carpentier</keyname><forenames>Alexandra</forenames></author><author><keyname>Valko</keyname><forenames>Michal</forenames></author></authors><title>Simple regret for infinitely many armed bandits</title><categories>cs.LG stat.ML</categories><comments>in 32th International Conference on Machine Learning (ICML 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a stochastic bandit problem with infinitely many arms. In this
setting, the learner has no chance of trying all the arms even once and has to
dedicate its limited number of samples only to a certain number of arms. All
previous algorithms for this setting were designed for minimizing the
cumulative regret of the learner. In this paper, we propose an algorithm aiming
at minimizing the simple regret. As in the cumulative regret setting of
infinitely many armed bandits, the rate of the simple regret will depend on a
parameter $\beta$ characterizing the distribution of the near-optimal arms. We
prove that depending on $\beta$, our algorithm is minimax optimal either up to
a multiplicative constant or up to a $\log(n)$ factor. We also provide
extensions to several important cases: when $\beta$ is unknown, in a natural
setting where the near-optimal arms have a small variance, and in the case of
unknown time horizon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04628</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04628</id><created>2015-05-18</created><authors><author><keyname>Shahzad</keyname><forenames>Faisal</forenames></author><author><keyname>Kreutzer</keyname><forenames>Moritz</forenames></author><author><keyname>Zeiser</keyname><forenames>Thomas</forenames></author><author><keyname>Machado</keyname><forenames>Rui</forenames></author><author><keyname>Pieper</keyname><forenames>Andreas</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Building a fault tolerant application using the GASPI communication
  layer</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is commonly agreed that highly parallel software on Exascale computers
will suffer from many more runtime failures due to the decreasing trend in the
mean time to failures (MTTF). Therefore, it is not surprising that a lot of
research is going on in the area of fault tolerance and fault mitigation.
Applications should survive a failure and/or be able to recover with minimal
cost. MPI is not yet very mature in handling failures, the User-Level Failure
Mitigation (ULFM) proposal being currently the most promising approach is still
in its prototype phase. In our work we use GASPI, which is a relatively new
communication library based on the PGAS model. It provides the missing features
to allow the design of fault-tolerant applications. Instead of introducing
algorithm-based fault tolerance in its true sense, we demonstrate how we can
build on (existing) clever checkpointing and extend applications to allow
integrate a low cost fault detection mechanism and, if necessary, recover the
application on the fly. The aspects of process management, the restoration of
groups and the recovery mechanism is presented in detail. We use a sparse
matrix vector multiplication based application to perform the analysis of the
overhead introduced by such modifications. Our fault detection mechanism causes
no overhead in failure-free cases, whereas in case of failure(s), the failure
detection and recovery cost is of reasonably acceptable order and shows good
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04630</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04630</id><created>2015-05-18</created><updated>2016-01-11</updated><authors><author><keyname>Tang</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyong</forenames></author></authors><title>Recurrent Neural Network Training with Dark Knowledge Transfer</title><categories>stat.ML cs.CL cs.LG cs.NE</categories><comments>ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks (RNNs), particularly long short-term memory (LSTM),
have gained much attention in automatic speech recognition (ASR). Although some
successful stories have been reported, training RNNs remains highly
challenging, especially with limited training data. Recent research found that
a well-trained model can be used as a teacher to train other child models, by
using the predictions generated by the teacher model as supervision. This
knowledge transfer learning has been employed to train simple neural nets with
a complex one, so that the final performance can reach a level that is
infeasible to obtain by regular training. In this paper, we employ the
knowledge transfer learning approach to train RNNs (precisely LSTM) using a
deep neural network (DNN) model as the teacher. This is different from most of
the existing research on knowledge transfer learning, since the teacher (DNN)
is assumed to be weaker than the child (RNN); however, our experiments on an
ASR task showed that it works fairly well: without applying any tricks on the
learning scheme, this approach can train RNNs successfully even with limited
training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04633</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04633</id><created>2015-05-18</created><authors><author><keyname>Lange</keyname><forenames>Michael</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard J.</forenames></author></authors><title>Flexible, Scalable Mesh and Data Management using PETSc DMPlex</title><categories>cs.MS</categories><comments>6 pages, 6 figures, to appear in EASC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing a scientific software stack to meet the needs of the
next-generation of mesh-based simulation demands, not only scalable and
efficient mesh and data management on a wide range of platforms, but also an
abstraction layer that makes it useful for a wide range of application codes.
Common utility tasks, such as file I/O, mesh distribution, and work
partitioning, should be delegated to external libraries in order to promote
code re-use, extensibility and software interoperability. In this paper we
demonstrate the use of PETSc's DMPlex data management API to perform mesh input
and domain partitioning in Fluidity, a large scale CFD application. We
demonstrate that raising the level of abstraction adds new functionality to the
application code, such as support for additional mesh file formats and mesh re-
ordering, while improving simulation startup cost through more efficient mesh
distribution. Moreover, the separation of concerns accomplished through this
interface shifts critical performance and interoperability issues, such as
scalable I/O and file format support, to a widely used and supported open
source community library, improving the sustainability, performance, and
functionality of Fluidity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04636</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04636</id><created>2015-05-18</created><authors><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Andersen</keyname><forenames>Dave G.</forenames></author><author><keyname>Smola</keyname><forenames>Alexander J.</forenames></author></authors><title>Graph Partitioning via Parallel Submodular Approximation to Accelerate
  Distributed Machine Learning</title><categories>cs.DC cs.AI cs.LG</categories><acm-class>I.2.11; I.5.1; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed computing excels at processing large scale data, but the
communication cost for synchronizing the shared parameters may slow down the
overall performance. Fortunately, the interactions between parameter and data
in many problems are sparse, which admits efficient partition in order to
reduce the communication overhead.
  In this paper, we formulate data placement as a graph partitioning problem.
We propose a distributed partitioning algorithm. We give both theoretical
guarantees and a highly efficient implementation. We also provide a highly
efficient implementation of the algorithm and demonstrate its promising results
on both text datasets and social networks. We show that the proposed algorithm
leads to 1.6x speedup of a state-of-the-start distributed machine learning
system by eliminating 90\% of the network communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04637</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04637</id><created>2015-05-18</created><authors><author><keyname>Bahnsen</keyname><forenames>Alejandro Correa</forenames></author><author><keyname>Aouada</keyname><forenames>Djamila</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Ensemble of Example-Dependent Cost-Sensitive Decision Trees</title><categories>cs.LG</categories><comments>13 pages, 6 figures, Submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several real-world classification problems are example-dependent
cost-sensitive in nature, where the costs due to misclassification vary between
examples and not only within classes. However, standard classification methods
do not take these costs into account, and assume a constant cost of
misclassification errors. In previous works, some methods that take into
account the financial costs into the training of different algorithms have been
proposed, with the example-dependent cost-sensitive decision tree algorithm
being the one that gives the highest savings. In this paper we propose a new
framework of ensembles of example-dependent cost-sensitive decision-trees. The
framework consists in creating different example-dependent cost-sensitive
decision trees on random subsamples of the training set, and then combining
them using three different combination approaches. Moreover, we propose two new
cost-sensitive combination approaches; cost-sensitive weighted voting and
cost-sensitive stacking, the latter being based on the cost-sensitive logistic
regression method. Finally, using five different databases, from four
real-world applications: credit card fraud detection, churn modeling, credit
scoring and direct marketing, we evaluate the proposed method against
state-of-the-art example-dependent cost-sensitive techniques, namely,
cost-proportionate sampling, Bayes minimum risk and cost-sensitive decision
trees. The results show that the proposed algorithms have better results for
all databases, in the sense of higher savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04650</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04650</id><created>2015-05-18</created><updated>2015-09-06</updated><authors><author><keyname>Tepper</keyname><forenames>Mariano</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Compressed Nonnegative Matrix Factorization is Fast and Accurate</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) has an established reputation as a
useful data analysis technique in numerous applications. However, its usage in
practical situations is undergoing challenges in recent years. The fundamental
factor to this is the increasingly growing size of the datasets available and
needed in the information sciences. To address this, in this work we propose to
use structured random compression, that is, random projections that exploit the
data structure, for two NMF variants: classical and separable. In separable NMF
(SNMF) the left factors are a subset of the columns of the input matrix. We
present suitable formulations for each problem, dealing with different
representative algorithms within each one. We show that the resulting
compressed techniques are faster than their uncompressed variants, vastly
reduce memory demands, and do not encompass any significant deterioration in
performance. The proposed structured random projections for SNMF allow to deal
with arbitrarily shaped large matrices, beyond the standard limit of
tall-and-skinny matrices, granting access to very efficient computations in
this general setting. We accompany the algorithmic presentation with
theoretical foundations and numerous and diverse examples, showing the
suitability of the proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04657</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04657</id><created>2015-05-18</created><updated>2015-10-25</updated><authors><author><keyname>Vu</keyname><forenames>Phong Minh</forenames></author><author><keyname>Nguyen</keyname><forenames>Tam The</forenames></author><author><keyname>Pham</keyname><forenames>Hung Viet</forenames></author><author><keyname>Nguyen</keyname><forenames>Tung Thanh</forenames></author></authors><title>Mining User Opinions in Mobile App Reviews: A Keyword-based Approach</title><categories>cs.IR cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User reviews of mobile apps often contain complaints or suggestions which are
valuable for app developers to improve user experience and satisfaction.
However, due to the large volume and noisy-nature of those reviews, manually
analyzing them for useful opinions is inherently challenging. To address this
problem, we propose MARK, a keyword-based framework for semi-automated review
analysis. MARK allows an analyst describing his interests in one or some mobile
apps by a set of keywords. It then finds and lists the reviews most relevant to
those keywords for further analysis. It can also draw the trends over time of
those keywords and detect their sudden changes, which might indicate the
occurrences of serious issues. To help analysts describe their interests more
effectively, MARK can automatically extract keywords from raw reviews and rank
them by their associations with negative reviews. In addition, based on a
vector-based semantic representation of keywords, MARK can divide a large set
of keywords into more cohesive subsets, or suggest keywords similar to the
selected ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04658</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04658</id><created>2015-05-18</created><updated>2015-05-18</updated><authors><author><keyname>Li</keyname><forenames>Xueliang</forenames></author><author><keyname>Wei</keyname><forenames>Meiqin</forenames></author></authors><title>A survey of recent results in (generalized) graph entropies</title><categories>cs.IT math.IT</categories><comments>This will appear as a chapter &quot;Graph Entropy: Recent Results and
  Perspectives&quot; in a book: Mathematical Foundations and Applications of Graph
  Entropy</comments><msc-class>94A17, 05C90, 92C42, 92E10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The entropy of a graph was first introduced by Rashevsky \cite{Rashevsky} and
Trucco \cite{Trucco} to interpret as the structural information content of the
graph and serve as a complexity measure. In this paper, we first state a number
of definitions of graph entropy measures and generalized graph entropies. Then
we survey the known results about them from the following three respects:
inequalities and extremal properties on graph entropies, relationships between
graph structures, graph energies, topological indices and generalized graph
entropies, complexity for calculation of graph entropies. Various applications
of graph entropies together with some open problems and conjectures are also
presented for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04661</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04661</id><created>2015-05-18</created><updated>2015-06-19</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Recoverability in quantum information theory</title><categories>quant-ph cond-mat.stat-mech cs.IT hep-th math-ph math.IT math.MP</categories><comments>v5: 26 pages, generalized lower bounds to apply when supp(rho) is
  contained in supp(sigma)</comments><journal-ref>Proceedings of the Royal Society A, vol. 471, no. 2182, page
  20150338 October 2015</journal-ref><doi>10.1098/rspa.2015.0338</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fact that the quantum relative entropy is non-increasing with respect to
quantum physical evolutions lies at the core of many optimality theorems in
quantum information theory and has applications in other areas of physics. In
this work, we establish improvements of this entropy inequality in the form of
physically meaningful remainder terms. One of the main results can be
summarized informally as follows: if the decrease in quantum relative entropy
between two quantum states after a quantum physical evolution is relatively
small, then it is possible to perform a recovery operation, such that one can
perfectly recover one state while approximately recovering the other. This can
be interpreted as quantifying how well one can reverse a quantum physical
evolution. Our proof method is elementary, relying on the method of complex
interpolation, basic linear algebra, and the recently introduced Renyi
generalization of a relative entropy difference. The theorem has a number of
applications in quantum information theory, which have to do with providing
physically meaningful improvements to many known entropy inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04673</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04673</id><created>2015-05-18</created><updated>2015-08-20</updated><authors><author><keyname>Huang</keyname><forenames>Shao-Lun</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Zheng</keyname><forenames>Lizhong</forenames></author></authors><title>Euclidean Information Theory of Networks</title><categories>cs.IT math.IT</categories><comments>to appear in the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the information theoretic framework that was
developed in earlier work to multi-hop network settings. For a given network,
we construct a novel deterministic model that quantifies the ability of the
network in transmitting private and common messages across users. Based on this
model, we formulate a linear optimization problem that explores the throughput
of a multi-layer network, thereby offering the optimal strategy as to what kind
of common messages should be generated in the network to maximize the
throughput. With this deterministic model, we also investigate the role of
feedback for multi-layer networks, from which we identify a variety of
scenarios in which feedback can improve transmission efficiency. Our results
provide fundamental guidelines as to how to coordinate cooperation between
users to enable efficient information exchanges across them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04677</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04677</id><created>2015-05-18</created><authors><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>On sets of graded attribute implications with witnessed non-redundancy</title><categories>cs.AI</categories><msc-class>68P20, 68T30, 03B52</msc-class><acm-class>H.2.8; H.3.3</acm-class><journal-ref>Information Sciences 329 (2016), 434-446</journal-ref><doi>10.1016/j.ins.2015.09.044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study properties of particular non-redundant sets of if-then rules
describing dependencies between graded attributes. We introduce notions of
saturation and witnessed non-redundancy of sets of graded attribute
implications are show that bases of graded attribute implications given by
systems of pseudo-intents correspond to non-redundant sets of graded attribute
implications with saturated consequents where the non-redundancy is witnessed
by antecedents of the contained graded attribute implications. We introduce an
algorithm which transforms any complete set of graded attribute implications
parameterized by globalization into a base given by pseudo-intents.
Experimental evaluation is provided to compare the method of obtaining bases
for general parameterizations by hedges with earlier graph-based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04679</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04679</id><created>2015-05-18</created><authors><author><keyname>Kim</keyname><forenames>Sunghyun</forenames></author><author><keyname>Wang</keyname><forenames>I-Hsiang</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author></authors><title>A Relay Can Increase Degrees of Freedom in Bursty Interference Networks</title><categories>cs.IT math.IT</categories><comments>submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the benefits of relays in multi-user wireless networks with
bursty user traffic, where intermittent data traffic restricts the users to
bursty transmissions. To this end, we study a two-user bursty MIMO Gaussian
interference channel with a relay, where two Bernoulli random states govern the
bursty user traffic. We show that an in-band relay can provide a degrees of
freedom (DoF) gain in this bursty channel. This beneficial role of in-band
relays in the bursty channel is in direct contrast to their role in the
non-bursty channel which is not as significant to provide a DoF gain. More
importantly, we demonstrate that for certain antenna configurations, an in-band
relay can help achieve interference-free performances with increased DoF. We
find the benefits particularly substantial with low data traffic, as the DoF
gain can grow linearly with the number of antennas at the relay. In this work,
we first derive an outer bound from which we obtain a necessary condition for
interference-free DoF performances. Then, we develop a novel scheme that
exploits information of the bursty traffic states to achieve them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04693</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04693</id><created>2015-05-18</created><updated>2016-02-24</updated><authors><author><keyname>Chen</keyname><forenames>Jian-Jia</forenames></author></authors><title>Partitioned Multiprocessor Fixed-Priority Scheduling of Sporadic
  Real-Time Tasks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partitioned multiprocessor scheduling has been widely accepted in academia
and industry to statically assign and partition real-time tasks onto identical
multiprocessor systems. This paper studies fixed-priority partitioned
multiprocessor scheduling for sporadic real-time systems, in which
deadline-monotonic scheduling is applied on each processor. Prior to this
paper, the best known results are by Fisher, Baruah, and Baker with speedup
factors $4-\frac{2}{M}$ and $3-\frac{1}{M}$ for arbitrary-deadline and
constrained-deadline sporadic real-time task systems, respectively, where $M$
is the number of processors. We show that a greedy mapping strategy has a
speedup factor $3-\frac{1}{M}$ when considering task systems with arbitrary
deadlines. Such a factor holds for polynomial-time schedulability tests and
exponential-time (exact) schedulability tests. Moreover, we also improve the
speedup factor to $2.84306$ when considering constrained-deadline task systems.
We also provide tight examples when the fitting strategy in the mapping stage
is arbitrary and $M$ is sufficiently large. For both constrained- and
arbitrary-deadline task systems, the analytical result surprisingly shows that
using exact tests does not gain theoretical benefits (with respect to speedup
factors) for an arbitrary fitting strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04694</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04694</id><created>2015-05-18</created><authors><author><keyname>Rokos</keyname><forenames>Georgios</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard J.</forenames></author><author><keyname>Jensen</keyname><forenames>Kristian Ejlebjerg</forenames></author><author><keyname>Kelly</keyname><forenames>Paul H. J.</forenames></author></authors><title>Thread Parallelism for Highly Irregular Computation in Anisotropic Mesh
  Adaptation</title><categories>cs.DC</categories><comments>To appear in the proceedings of EASC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thread-level parallelism in irregular applications with mutable data
dependencies presents challenges because the underlying data is extensively
modified during execution of the algorithm and a high degree of parallelism
must be realized while keeping the code race-free. In this article we describe
a methodology for exploiting thread parallelism for a class of graph-mutating
worklist algorithms, which guarantees safe parallel execution via processing in
rounds of independent sets and using a deferred update strategy to commit
changes in the underlying data structures. Scalability is assisted by atomic
fetch-and-add operations to create worklists and work-stealing to balance the
shared-memory workload. This work is motivated by mesh adaptation algorithms,
for which we show a parallel efficiency of 60% and 50% on Intel(R) Xeon(R)
Sandy Bridge and AMD Opteron(tm) Magny-Cours systems, respectively, using these
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04713</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04713</id><created>2015-05-18</created><authors><author><keyname>Sheeba</keyname><forenames>G. Merlin</forenames></author><author><keyname>Nachiappan</keyname><forenames>Alamelu</forenames></author><author><keyname>Kumar</keyname><forenames>P. H. Pavan</forenames></author><author><keyname>Prateek</keyname></author></authors><title>Placement Of Energy Aware Wireless Mesh Nodes For E-Learning In Green
  Campuses</title><categories>cs.NI</categories><comments>10 pages,4 figures</comments><journal-ref>International Journal on Cybernetics &amp; Informatics (IJCI) Vol. 4,
  No. 2, April 2015</journal-ref><doi>10.5121/ijci.2015.4218</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Energy efficiency solutions are more vital for Green Mesh Network (GMN)
campuses. Today students are benefited using these e-learning methodologies.
Renewable energies such as solar, wind, hydro has tremendous applications on
energy efficient wireless networks for sustaining the ever growing traffic
demands. One of the major issues in designing a GMN is minimizing the number of
deployed mesh routers and gateways and satisfying the sustainable QOS based
energy constraints. During low traffic periods the mesh routers are switched to
power save or sleep mode. In this paper we have mathematically formulated a
single objective function with multi constraints to optimize the energy. The
objective is to place minimum number of Mesh routers and gateways in a set of
candidate location. The mesh nodes are powered using the solar energy to meet
the traffic demands. Two global optimisation algorithms are compared in this
paper to optimize the energy sustainability, to guarantee seamless
connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04724</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04724</id><created>2015-05-18</created><authors><author><keyname>Attia</keyname><forenames>Ahmed</forenames></author><author><keyname>Rao</keyname><forenames>Vishwas</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>A Hybrid Monte-Carlo Sampling Smoother for Four Dimensional Data
  Assimilation</title><categories>cs.NA stat.CO</categories><comments>33 Pages</comments><report-no>CSL-TR-19-2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper constructs an ensemble-based sampling smoother for
four-dimensional data assimilation using a Hybrid/Hamiltonian Monte-Carlo
approach. The smoother samples efficiently from the posterior probability
density of the solution at the initial time. Unlike the well-known ensemble
Kalman smoother, which is optimal only in the linear Gaussian case, the
proposed methodology naturally accommodates non-Gaussian errors and non-linear
model dynamics and observation operators. Unlike the four-dimensional
variational met\-hod, which only finds a mode of the posterior distribution,
the smoother provides an estimate of the posterior uncertainty. One can use the
ensemble mean as the minimum variance estimate of the state, or can use the
ensemble in conjunction with the variational approach to estimate the
background errors for subsequent assimilation windows. Numerical results
demonstrate the advantages of the proposed method compared to the traditional
variational and ensemble-based smoothing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04726</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04726</id><created>2015-05-18</created><updated>2015-05-28</updated><authors><author><keyname>Ribeiro</keyname><forenames>Pedro</forenames></author></authors><title>Angelic Processes</title><categories>cs.LO</categories><comments>Extended version of PhD thesis submitted to the University of York,
  UK, 868 pages, 10 figures, 7 tables (revised to match print version deposited
  in the White Rose eTheses Online), http://etheses.whiterose.ac.uk/9020/</comments><msc-class>68Q55, 68Q65, 68Q60, 68Q85</msc-class><acm-class>D.2.4; F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the formal modelling of systems, demonic and angelic nondeterminism play
fundamental roles as abstraction mechanisms. The angelic nature of a choice
pertains to the property of avoiding failure whenever possible. As a concept,
angelic choice first appeared in automata theory and Turing machines, where it
can be implemented via backtracking. It has traditionally been studied in the
refinement calculus, and has proved to be useful in a variety of applications
and refinement techniques. Recently it has been studied within relational,
multirelational and higher-order models. It has been employed for modelling
user interactions, game-like scenarios, theorem proving tactics, constraint
satisfaction problems and control systems.
  When the formal modelling of state-rich reactive systems is considered, it
only seems natural that both types of nondeterministic choice should be
considered. However, despite several treatments of angelic nondeterminism in
the context of process algebras, namely Communicating Sequential Processes, the
counterpart to the angelic choice of the refinement calculus has been elusive.
  In this thesis, we develop a semantics in the relational setting of Hoare and
He's Unifying Theories of Programming that enables the characterisation of
angelic nondeterminism in CSP. Since CSP processes are given semantics in the
UTP via designs, that is, pre and postcondition pairs, we first introduce a
theory of angelic designs, and an isomorphic multirelational model, that is
suitable for characterising processes. We then develop a theory of reactive
angelic designs by enforcing the healthiness conditions of CSP. Finally, by
introducing a notion of divergence that can undo the history of events, we
obtain a model where angelic choice avoids divergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04732</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04732</id><created>2015-05-18</created><updated>2016-02-25</updated><authors><author><keyname>Martino</keyname><forenames>L.</forenames></author><author><keyname>Elvira</keyname><forenames>V.</forenames></author><author><keyname>Luengo</keyname><forenames>D.</forenames></author><author><keyname>Corander</keyname><forenames>J.</forenames></author></authors><title>Layered Adaptive Importance Sampling</title><categories>stat.CO cs.LG stat.ML</categories><comments>Statistics and Computing, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo methods represent the &quot;de facto&quot; standard for approximating
complicated integrals involving multidimensional target distributions. In order
to generate random realizations from the target distribution, Monte Carlo
techniques use simpler proposal probability densities to draw candidate
samples. The performance of any such method is strictly related to the
specification of the proposal distribution, such that unfortunate choices
easily wreak havoc on the resulting estimators. In this work, we introduce a
layered (i.e., hierarchical) procedure to generate samples employed within a
Monte Carlo scheme. This approach ensures that an appropriate equivalent
proposal density is always obtained automatically (thus eliminating the risk of
a catastrophic performance), although at the expense of a moderate increase in
the complexity. Furthermore, we provide a general unified importance sampling
(IS) framework, where multiple proposal densities are employed and several IS
schemes are introduced by applying the so-called deterministic mixture
approach. Finally, given these schemes, we also propose a novel class of
adaptive importance samplers using a population of proposals, where the
adaptation is driven by independent parallel or interacting Markov Chain Monte
Carlo (MCMC) chains. The resulting algorithms efficiently combine the benefits
of both IS and MCMC methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04746</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04746</id><created>2015-05-18</created><authors><author><keyname>Davari</keyname><forenames>Somaye</forenames></author><author><keyname>Ghadiri</keyname><forenames>Nasser</forenames></author></authors><title>Spatial database implementation of fuzzy region connection calculus for
  analysing the relationship of diseases</title><categories>cs.DB cs.AI cs.CG</categories><comments>ICEE2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing huge amounts of spatial data plays an important role in many
emerging analysis and decision-making domains such as healthcare, urban
planning, agriculture and so on. For extracting meaningful knowledge from
geographical data, the relationships between spatial data objects need to be
analyzed. An important class of such relationships are topological relations
like the connectedness or overlap between regions. While real-world
geographical regions such as lakes or forests do not have exact boundaries and
are fuzzy, most of the existing analysis methods neglect this inherent feature
of topological relations. In this paper, we propose a method for handling the
topological relations in spatial databases based on fuzzy region connection
calculus (RCC). The proposed method is implemented in PostGIS spatial database
and evaluated in analyzing the relationship of diseases as an important
application domain. We also used our fuzzy RCC implementation for fuzzification
of the skyline operator in spatial databases. The results of the evaluation
show that our method provides a more realistic view of spatial relationships
and gives more flexibility to the data analyst to extract meaningful and
accurate results in comparison with the existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04766</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04766</id><created>2015-05-18</created><authors><author><keyname>Vijayaraghavan</keyname><forenames>Vikram S.</forenames></author><author><keyname>No&#xeb;l</keyname><forenames>Pierre-Andr&#xe9;</forenames></author><author><keyname>Maoz</keyname><forenames>Zeev</forenames></author><author><keyname>D'Souza</keyname><forenames>Raissa M.</forenames></author></authors><title>Quantifying dynamical spillover in co-evolving multiplex networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplex networks (a system of multiple networks that have different types
of links but share a common set of nodes) arise naturally in a wide spectrum of
fields. Theoretical studies show that in such multiplex networks, correlated
edge dynamics between the layers can have a profound effect on dynamical
processes. However, how to extract the correlations from real-world systems is
an outstanding challenge. Here we provide a null model based on Markov chains
to quantify correlations in edge dynamics found in longitudinal data of
multiplex networks. We use this approach on two different data sets: the
network of trade and alliances between nation states, and the email and
co-commit networks between developers of open source software. We establish the
existence of &quot;dynamical spillover&quot; showing the correlated formation (or
deletion) of edges of different types as the system evolves. The details of the
dynamics over time provide insight into potential causal pathways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04771</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04771</id><created>2015-05-18</created><authors><author><keyname>Malmi</keyname><forenames>Eric</forenames></author><author><keyname>Takala</keyname><forenames>Pyry</forenames></author><author><keyname>Toivonen</keyname><forenames>Hannu</forenames></author><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author><author><keyname>Gionis</keyname><forenames>Aristides</forenames></author></authors><title>DopeLearning: A Computational Approach to Rap Lyrics Generation</title><categories>cs.LG cs.AI cs.CL cs.NE</categories><comments>11 pages</comments><acm-class>I.2.7; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Writing rap lyrics requires both creativity, to construct a meaningful and an
interesting story, and lyrical skills, to produce complex rhyme patterns, which
are the cornerstone of a good flow. We present a method for capturing both of
these aspects. Our approach is based on two machine-learning techniques: the
RankSVM algorithm, and a deep neural network model with a novel structure. For
the problem of distinguishing the real next line from a randomly selected one,
we achieve an 82 % accuracy. We employ the resulting prediction method for
creating new rap lyrics by combining lines from existing songs. In terms of
quantitative rhyme density, the produced lyrics outperform best human rappers
by 21 %. The results highlight the benefit of our rhyme density metric and our
innovative predictor of next lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04778</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04778</id><created>2015-05-18</created><authors><author><keyname>Iguchi</keyname><forenames>Takayuki</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Peterson</keyname><forenames>Jesse</forenames></author><author><keyname>Villar</keyname><forenames>Soledad</forenames></author></authors><title>On the tightness of an SDP relaxation of k-means</title><categories>cs.IT cs.DS cs.LG math.IT math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Recently, Awasthi et al. introduced an SDP relaxation of the $k$-means
problem in $\mathbb R^m$. In this work, we consider a random model for the data
points in which $k$ balls of unit radius are deterministically distributed
throughout $\mathbb R^m$, and then in each ball, $n$ points are drawn according
to a common rotationally invariant probability distribution. For any fixed ball
configuration and probability distribution, we prove that the SDP relaxation of
the $k$-means problem exactly recovers these planted clusters with probability
$1-e^{-\Omega(n)}$ provided the distance between any two of the ball centers is
$&gt;2+\epsilon$, where $\epsilon$ is an explicit function of the configuration of
the ball centers, and can be arbitrarily small when $m$ is large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04785</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04785</id><created>2015-05-18</created><authors><author><keyname>Diamant</keyname><forenames>Emanuel</forenames></author></authors><title>Advances in Bioinformatics and Computational Biology: Don't take them
  too seriously anyway</title><categories>cs.CE</categories><comments>The paper was submitted to the BIOCOMP'15 conference (Las Vegas,
  Nevada, USA, July 27-30, 2015) and was accepted as a poster presentation.
  arXiv admin note: text overlap with arXiv:1505.04578</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few decades or so, we witness a paradigm shift in our nature
studies - from a data-processing based computational approach to an
information-processing based cognitive approach. The process is restricted and
often misguided by the lack of a clear understanding about what information is
and how it should be treated in research applications (in general) and in
biological studies (in particular). The paper intend to provide some remedies
for this bizarre situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04803</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04803</id><created>2015-05-18</created><authors><author><keyname>Lee</keyname><forenames>Yong Jae</forenames></author><author><keyname>Grauman</keyname><forenames>Kristen</forenames></author></authors><title>Predicting Important Objects for Egocentric Video Summarization</title><categories>cs.CV</categories><comments>Published in the International Journal of Computer Vision (IJCV),
  January 2015</comments><doi>10.1007/s11263-014-0794-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a video summarization approach for egocentric or &quot;wearable&quot; camera
data. Given hours of video, the proposed method produces a compact storyboard
summary of the camera wearer's day. In contrast to traditional keyframe
selection techniques, the resulting summary focuses on the most important
objects and people with which the camera wearer interacts. To accomplish this,
we develop region cues indicative of high-level saliency in egocentric
video---such as the nearness to hands, gaze, and frequency of occurrence---and
learn a regressor to predict the relative importance of any new region based on
these cues. Using these predictions and a simple form of temporal event
detection, our method selects frames for the storyboard that reflect the key
object-driven happenings. We adjust the compactness of the final summary given
either an importance selection criterion or a length budget; for the latter, we
design an efficient dynamic programming solution that accounts for importance,
visual uniqueness, and temporal displacement. Critically, the approach is
neither camera-wearer-specific nor object-specific; that means the learned
importance metric need not be trained for a given user or context, and it can
predict the importance of objects and people that have never been seen
previously. Our results on two egocentric video datasets show the method's
promise relative to existing techniques for saliency and summarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04813</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04813</id><created>2015-05-18</created><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author></authors><title>What is Learning? A primary discussion about information and
  Representation</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Nowadays, represented by Deep Learning techniques, the field of machine
learning is experiencing unprecedented prosperity and its influence is
demonstrated in academia, industry and civil society. &quot;Intelligent&quot; has become
a label which could not be neglected for most applications; celebrities and
scientists also warned that the development of full artificial intelligence may
spell the end of the human race. It seems that the answer to building a
computer system that could automatically improve with experience is right on
the next corner. While for AI and machine learning researchers, it is a
consensus that we are not anywhere near the core technique which could bring
the Terminator, Number 5 or R2D2 into real life, and there is not even a formal
definition about what is intelligence, or one of its basic properties:
Learning. Therefore, even though researchers know these concerns are not
necessary currently, there is no generalized explanation about why these
concerns are not necessary, and what properties people should take into account
that would make these concerns to be necessary. In this paper, starts from
analysing the relation between information and its representation, a necessary
condition for a model to be a learning model is proposed. This condition and
related future works could be used to verify whether a system is able to learn
or not, and enrich our understanding of learning: one important property of
Intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04824</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04824</id><created>2015-05-18</created><authors><author><keyname>Feyzmahdavian</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Aytekin</keyname><forenames>Arda</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>An Asynchronous Mini-Batch Algorithm for Regularized Stochastic
  Optimization</title><categories>math.OC cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mini-batch optimization has proven to be a powerful paradigm for large-scale
learning. However, the state of the art parallel mini-batch algorithms assume
synchronous operation or cyclic update orders. When worker nodes are
heterogeneous (due to different computational capabilities or different
communication delays), synchronous and cyclic operations are inefficient since
they will leave workers idle waiting for the slower nodes to complete their
computations. In this paper, we propose an asynchronous mini-batch algorithm
for regularized stochastic optimization problems with smooth loss functions
that eliminates idle waiting and allows workers to run at their maximal update
rates. We show that by suitably choosing the step-size values, the algorithm
achieves a rate of the order $O(1/\sqrt{T})$ for general convex regularization
functions, and the rate $O(1/T)$ for strongly convex regularization functions,
where $T$ is the number of iterations. In both cases, the impact of asynchrony
on the convergence rate of our algorithm is asymptotically negligible, and a
near-linear speedup in the number of workers can be expected. Theoretical
results are confirmed in real implementations on a distributed computing
infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04829</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04829</id><created>2015-05-18</created><authors><author><keyname>Chakravorty</keyname><forenames>Jhelum</forenames></author><author><keyname>Mahajan</keyname><forenames>Aditya</forenames></author></authors><title>Fundamental limits of remote estimation of Markov processes under
  communication constraints</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1412.3199</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental limits of remote estimation of Markov processes under
communication constraints are presented. The remote estimation system consists
of a sensor and an estimator. The sensor observes a discrete-time Markov
process, which is a symmetric countable state Markov source or a Gauss-Markov
process. At each time, the sensor either transmits the current state of the
Markov process or does not transmit at all. Communication is noiseless but
costly. The estimator estimates the Markov process based on the transmitted
observations. In such a system, there is a trade-off between communication cost
and estimation accuracy. Two fundamental limits of this trade-off are
characterized for infinite horizon discounted cost and average cost setups.
First, when each transmission is costly, we characterize the minimum achievable
cost of communication plus estimation error. Second, when there is a constraint
on the average number of transmissions, we characterize the minimum achievable
estimation error. Transmission and estimation strategies that achieve these
fundamental limits are also identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04845</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04845</id><created>2015-05-18</created><updated>2015-12-02</updated><authors><author><keyname>Zhou</keyname><forenames>Xiaowei</forenames></author><author><keyname>Zhu</keyname><forenames>Menglong</forenames></author><author><keyname>Daniilidis</keyname><forenames>Kostas</forenames></author></authors><title>Multi-Image Matching via Fast Alternating Minimization</title><categories>cs.CV</categories><comments>In ICCV'2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a global optimization-based approach to jointly
matching a set of images. The estimated correspondences simultaneously maximize
pairwise feature affinities and cycle consistency across multiple images.
Unlike previous convex methods relying on semidefinite programming, we
formulate the problem as a low-rank matrix recovery problem and show that the
desired semidefiniteness of a solution can be spontaneously fulfilled. The
low-rank formulation enables us to derive a fast alternating minimization
algorithm in order to handle practical problems with thousands of features.
Both simulation and real experiments demonstrate that the proposed algorithm
can achieve a competitive performance with an order of magnitude speedup
compared to the state-of-the-art algorithm. In the end, we demonstrate the
applicability of the proposed method to match the images of different object
instances and as a result the potential to reconstruct category-specific object
models from those images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04868</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04868</id><created>2015-05-19</created><authors><author><keyname>Wang</keyname><forenames>Limin</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors</title><categories>cs.CV</categories><comments>IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual features are of vital importance for human action understanding in
videos. This paper presents a new video representation, called
trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits
of both hand-crafted features and deep-learned features. Specifically, we
utilize deep architectures to learn discriminative convolutional feature maps,
and conduct trajectory-constrained pooling to aggregate these convolutional
features into effective descriptors. To enhance the robustness of TDDs, we
design two normalization methods to transform convolutional feature maps,
namely spatiotemporal normalization and channel normalization. The advantages
of our features come from (i) TDDs are automatically learned and contain high
discriminative capacity compared with those hand-crafted features; (ii) TDDs
take account of the intrinsic characteristics of temporal dimension and
introduce the strategies of trajectory-constrained sampling and pooling for
aggregating deep-learned features. We conduct experiments on two challenging
datasets: HMDB51 and UCF101. Experimental results show that TDDs outperform
previous hand-crafted features and deep-learned features. Our method also
achieves superior performance to the state of the art on these datasets (HMDB51
65.9%, UCF101 91.5%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04870</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04870</id><created>2015-05-19</created><updated>2015-10-05</updated><authors><author><keyname>Plummer</keyname><forenames>Bryan A.</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author><author><keyname>Cervantes</keyname><forenames>Chris M.</forenames></author><author><keyname>Caicedo</keyname><forenames>Juan C.</forenames></author><author><keyname>Hockenmaier</keyname><forenames>Julia</forenames></author><author><keyname>Lazebnik</keyname><forenames>Svetlana</forenames></author></authors><title>Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
  Richer Image-to-Sentence Models</title><categories>cs.CV cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Flickr30k dataset has become a standard benchmark for sentence-based
image description. This paper presents Flickr30k Entities, which augments the
158k captions from Flickr30k with 244k coreference chains linking mentions of
the same entities in images, as well as 276k manually annotated bounding boxes
corresponding to each entity. Such annotation is essential for continued
progress in automatic image description and grounded language understanding. We
present experiments demonstrating the usefulness of our annotations for
text-to-image reference resolution, or the task of localizing textual entity
mentions in an image, and for bidirectional image-sentence retrieval. These
experiments confirm that we can further improve the accuracy of
state-of-the-art retrieval methods by training with explicit region-to-phrase
correspondence, but at the same time, they show that accurately inferring this
correspondence given an image and a caption remains really challenging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04873</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04873</id><created>2015-05-19</created><authors><author><keyname>Talker</keyname><forenames>Lior</forenames></author><author><keyname>Moses</keyname><forenames>Yael</forenames></author><author><keyname>Shimshoni</keyname><forenames>Ilan</forenames></author></authors><title>Have a Look at What I See</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for guiding a photographer to rotate her/his smartphone
camera to obtain an image that overlaps with another image of the same scene.
The other image is taken by another photographer from a different viewpoint.
Our method is applicable even when the images do not have overlapping fields of
view. Straightforward applications of our method include sharing attention to
regions of interest for social purposes, or adding missing images to improve
structure for motion results. Our solution uses additional images of the scene,
which are often available since many people use their smartphone cameras
regularly. These images may be available online from other photographers who
are present at the scene. Our method avoids 3D scene reconstruction; it relies
instead on a new representation that consists of the spatial orders of the
scene points on two axes, x and y. This representation allows a sequence of
points to be chosen efficiently and projected onto the photographers images,
using epipolar point transfer. Overlaying these epipolar lines on the live
preview of the camera produces a convenient interface to guide the user. The
method was tested on challenging datasets of images and succeeded in guiding a
photographer from one view to a non-overlapping destination view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04875</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04875</id><created>2015-05-19</created><updated>2015-06-03</updated><authors><author><keyname>Kipnis</keyname><forenames>Alon</forenames></author><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>Indirect Rate-Distortion Function of a Binary i.i.d Source</title><categories>cs.IT math.IT stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The indirect source-coding problem in which a Bernoulli process is compressed
in a lossy manner from its noisy observations is considered. These noisy
observations are obtained by passing the source sequence through a The indirect
source-coding problem in which a Bernoulli process is compressed in a lossy
manner from its noisy observations is considered. These noisy observations are
obtained by passing the source sequence through a binary symmetric channel so
that the channel crossover probability controls the amount of information
available about the source realization at the encoder. We use classic results
in rate-distortion theory to compute an expression of the rate-distortion
function for this model, where the Bernoulli source is not necessarily
symmetric. The indirect rate-distortion function is given in terms of a
solution to a simple equation. In addition, we derive an upper bound on the
indirect rate-distortion function which is given in a closed. These expressions
capture precisely the expected behavior that the noisier the observations, the
smaller the return from increasing bit-rate to reduce distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04880</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04880</id><created>2015-05-19</created><authors><author><keyname>Beiranvand</keyname><forenames>Amin</forenames></author><author><keyname>Ghadiri</keyname><forenames>Nasser</forenames></author></authors><title>ADQUEX: Adaptive Processing of Federated Queries over Linked Data based
  on Tuple Routing</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the distribution of linked data across the web, the methods that
process federated queries through a distributed approach are more attractive to
the users and have gained more prosperity. In distributed processing of
federated queries, we need methods and procedures to execute the query in an
optimal manner. Most of the existing methods perform the optimization task
based on some statistical information, whereas the query processor does not
have precise statistical information about their properties, since the data
sources are autonomous. When precise statistics are not available, the
possibility of wrong estimations will highly increase, and may lead to
inefficient execution of the query at runtime. Another problem of the existing
methods is that in the optimization phase, they assume that runtime conditions
of query execution are stable, while the environment in which federated queries
are executed over linked data is dynamic and non-predictable. By considering
these two problems, there is a great potential for exploiting the federated
query processing techniques in an adaptive manner. In this paper, an adaptive
method is proposed for processing federated queries over linked data, based on
the concept of routing the tuples. The proposed method, named ADQUEX, is able
to execute the query effectively without any prior statistical information.
This method can change the query execution plan at runtime so that less
intermediate results are produced. It can also adapt the execution plan to new
situation if unpredicted network latencies arise. Extensive evaluation of our
method by running real queries over well-known linked datasets shows very good
results especially for complex queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04885</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04885</id><created>2015-05-19</created><authors><author><keyname>Leong</keyname><forenames>Alex S.</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author></authors><title>Kalman Filtering With Relays Over Wireless Fading Channels</title><categories>cs.IT cs.SY math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note studies the use of relays to improve the performance of Kalman
filtering over packet dropping links. Packet reception probabilities are
governed by time-varying fading channel gains, and the sensor and relay
transmit powers. We consider situations with multiple sensors and relays, where
each relay can either forward one of the sensors' measurements to the
gateway/fusion center, or perform a simple linear network coding operation on
some of the sensor measurements. Using an expected error covariance performance
measure, we consider optimal and suboptimal methods for finding the best relay
configuration, and power control problems for optimizing the Kalman filter
performance. Our methods show that significant performance gains can be
obtained through the use of relays, network coding and power control, with at
least 30-40$\%$ less power consumption for a given expected error covariance
specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04887</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04887</id><created>2015-05-19</created><authors><author><keyname>Chen</keyname><forenames>Chiao-En</forenames></author></authors><title>A Near-optimal User Ordering Algorithm for Non-iterative Interference
  Alignment Transceiver Design in MIMO Interfering Broadcast Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) has recently emerged as a promising interference
mitigation technique for interference networks. In this letter, we focus on the
IA non-iterative transceiver design problem in a multiple-input-multiple-output
interfering broadcast channel (MIMO-IBC), and observed that there is previously
unexploited flexibility in different permutations of user ordering. By choosing
a good user ordering for a pre-determined IA inter-channel-interference
allocation, an improved transceiver design can be accomplished. In order to
achieve a more practical performance-complexity tradeoff, a suboptimal user
ordering algorithm is proposed. Simulation shows the proposed suboptimal user
ordering algorithm can achieve near-optimal performance compared to the optimal
ordering while exhibiting only moderate computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04890</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04890</id><created>2015-05-19</created><authors><author><keyname>Limbachiya</keyname><forenames>Dixita</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K.</forenames></author></authors><title>Natural Data Storage: A Review on sending Information from now to then
  via Nature</title><categories>cs.IT cs.ET math.IT</categories><comments>draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital data explosion drives a demand for robust and reliable data storage
medium. Development of better digital storage device to accumulate Zetta bytes
(1 ZB = $10^{21}$ bytes ) of data that will be generated in near future is a
big challenge. Looking at limitations of present day digital storage devices,
it will soon be a big challenge for data scientists to provide reliable.
affordable and dense storage medium. As an alternative, researcher used natural
medium of storage like DNA, bacteria and protein as information storage
systems. This article discuss DNA based information storage system in detail
along with an overview about bacterial and protein data storage systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04891</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04891</id><created>2015-05-19</created><updated>2015-06-14</updated><authors><author><keyname>Tian</keyname><forenames>Fei</forenames></author><author><keyname>Gao</keyname><forenames>Bin</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>Learning Better Word Embedding by Asymmetric Low-Rank Projection of
  Knowledge Graph</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word embedding, which refers to low-dimensional dense vector representations
of natural words, has demonstrated its power in many natural language
processing tasks. However, it may suffer from the inaccurate and incomplete
information contained in the free text corpus as training data. To tackle this
challenge, there have been quite a few works that leverage knowledge graphs as
an additional information source to improve the quality of word embedding.
Although these works have achieved certain success, they have neglected some
important facts about knowledge graphs: (i) many relationships in knowledge
graphs are \emph{many-to-one}, \emph{one-to-many} or even \emph{many-to-many},
rather than simply \emph{one-to-one}; (ii) most head entities and tail entities
in knowledge graphs come from very different semantic spaces. To address these
issues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet
models the relationships between head and tail entities after transforming them
with different low-rank projection matrices. The low-rank projection can allow
non \emph{one-to-one} relationships between entities, while different
projection matrices for head and tail entities allow them to originate in
different semantic spaces. The experimental results demonstrate that ProjectNet
yields more accurate word embedding than previous works, thus leads to clear
improvements in various natural language processing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04897</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04897</id><created>2015-05-19</created><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Bhaskar</keyname><forenames>Umang</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Computing Optimal Tolls in Routing Games without Knowing the Latency
  Functions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following question: in a nonatomic routing game, can the
tolls that induce the minimum latency flow be computed without knowing the
latency functions? Since the latency functions are unknown, we assume we are
given an oracle that has access to the underlying routing game. A query to the
oracle consists of tolls on edges, and the response is the resulting
equilibrium flow. We show that in this model, it is impossible to obtain
optimal tolls. However, if we augment the oracle so that it returns the total
latency of the equilibrium flow induced by the tolls in addition to the flow
itself, then the required tolls can be computed with a polynomial number of
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04901</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04901</id><created>2015-05-19</created><updated>2016-01-11</updated><authors><author><keyname>Goerigk</keyname><forenames>Marc</forenames></author><author><keyname>Sch&#xf6;bel</keyname><forenames>Anita</forenames></author></authors><title>Algorithm Engineering in Robust Optimization</title><categories>math.OC cs.DS</categories><acm-class>G.1.6; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust optimization is a young and emerging field of research having received
a considerable increase of interest over the last decade. In this paper, we
argue that the the algorithm engineering methodology fits very well to the
field of robust optimization and yields a rewarding new perspective on both the
current state of research and open research directions.
  To this end we go through the algorithm engineering cycle of design and
analysis of concepts, development and implementation of algorithms, and
theoretical and experimental evaluation. We show that many ideas of algorithm
engineering have already been applied in publications on robust optimization.
Most work on robust optimization is devoted to analysis of the concepts and the
development of algorithms, some papers deal with the evaluation of a particular
concept in case studies, and work on comparison of concepts just starts. What
is still a drawback in many papers on robustness is the missing link to include
the results of the experiments again in the design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04909</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04909</id><created>2015-05-19</created><authors><author><keyname>Hsu</keyname><forenames>Teng-Cheng</forenames></author><author><keyname>Hong</keyname><forenames>Y. -W. Peter</forenames></author><author><keyname>Wang</keyname><forenames>Tsang-Yi</forenames></author></authors><title>Optimized Random Deployment of Energy Harvesting Sensors for Field
  Reconstruction in Analog and Digital Forwarding Systems</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2015.2449262</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work examines the large-scale deployment of energy harvesting sensors
for the purpose of sensing and reconstruction of a spatially correlated
Gaussian random field. The sensors are powered solely by energy harvested from
the environment and are deployed randomly according to a spatially
nonhomogeneous Poisson point process whose density depends on the energy
arrival statistics at different locations. Random deployment is suitable for
applications that require deployment over a wide and/or hostile area. During an
observation period, each sensor takes a local sample of the random field and
reports the data to the closest data-gathering node if sufficient energy is
available for transmission. The realization of the random field is then
reconstructed at the fusion center based on the reported sensor measurements.
For the purpose of field reconstruction, the sensors should, on the one hand,
be more spread out over the field to gather more informative samples, but
should, on the other hand, be more concentrated at locations with high energy
arrival rates or large channel gains toward the closest data-gathering node.
This tradeoff is exploited in the optimization of the random sensor deployment
in both analog and digital forwarding systems. More specifically, given the
statistics of the energy arrival at different locations and a constraint on the
average number of sensors, the spatially-dependent sensor density and the
energy-aware transmission policy at the sensors are determined for both cases
by minimizing an upper bound on the average mean-square reconstruction error.
The efficacy of the proposed schemes are demonstrated through numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04911</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04911</id><created>2015-05-19</created><updated>2015-12-10</updated><authors><author><keyname>Limasset</keyname><forenames>Antoine</forenames></author><author><keyname>Cazaux</keyname><forenames>Bastien</forenames></author><author><keyname>Rivals</keyname><forenames>Eric</forenames></author><author><keyname>Peterlongo</keyname><forenames>Pierre</forenames></author></authors><title>Read Mapping on de Bruijn graph</title><categories>cs.DS q-bio.GN</categories><comments>BMC Bioinformatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background Next Generation Sequencing (NGS) has dramatically enhanced our
ability to sequence genomes, but not to assemble them. In practice, many
published genome sequences remain in the state of a large set of contigs. Each
contig describes the sequence found along some path of the assembly graph,
however, the set of contigs does not record all the sequence information
contained in that graph. Although many subsequent analyses can be performed
with the set of contigs, one may ask whether mapping reads on the contigs is as
informative as mapping them on the paths of the assembly graph. Currently, one
lacks practical tools to perform mapping on such graphs. Results Here, we
propose a formal definition of mapping on a de Bruijn graph, analyse the
problem complexity which turns out to be NP-complete, and provide a practical
solution.We propose a pipeline called GGMAP (Greedy Graph MAPping). Its novelty
is a procedure to map reads on branching paths of the graph, for which we
designed a heuristic algorithm called BGREAT (de Bruijn Graph REAd mapping
Tool). For the sake of efficiency, BGREAT rewrites a read sequence as a
succession of unitigs sequences. GGMAP can map millions of reads per CPU hour
on a de Bruijn graph built from a large set of human genomic reads.
Surprisingly, results show that up to 22% more reads can be mapped on the graph
but not on the contig set. Conclusions Although mapping reads on a de Bruijn
graph is complex task, our proposal offers a practical solution combining
efficiency with an improved mapping capacity compared to assembly-based mapping
even for complex eukaryotic data. Availability: github.com/Malfoy/BGREAT
Keywords: Read mapping; De bruijn graphs; NGS; NP-completeness
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04918</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04918</id><created>2015-05-19</created><updated>2015-06-07</updated><authors><author><keyname>Adiga</keyname><forenames>Abhijin</forenames></author><author><keyname>Babu</keyname><forenames>Jasine</forenames></author><author><keyname>Chandran</keyname><forenames>L. Sunil</forenames></author></authors><title>Sublinear Approximation Algorithms for Boxicity and Related Problems</title><categories>cs.DM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1201.5958</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boxicity of a graph G(V, E) is the minimum integer k such that G can be
represented as the intersection graph of axis parallel boxes in $\mathbb{R}^k$.
Cubicity is a variant of boxicity, where the axis parallel boxes in the
intersection representation are restricted to be of unit length sides. Deciding
whether boxicity (resp. cubicity) of a graph is at most k is NP-hard, even for
k=2 or 3. Computing these parameters is inapproximable within $O(n^{1 -
\epsilon})$-factor, for any $\epsilon &gt;0$ in polynomial time unless NP=ZPP,
even for many simple graph classes.
  In this paper, we give a polynomial time $\kappa(n)$ factor approximation
algorithm for computing boxicity and a $\kappa(n)\lceil \log \log n\rceil$
factor approximation algorithm for computing the cubicity, where $\kappa(n)
=2\left\lceil\frac{n\sqrt{\log \log n}}{\sqrt{\log n}}\right\rceil$. These o(n)
factor approximation algorithms also produce the corresponding box (resp. cube)
representations. As a special case, this resolves the question paused by
Spinrad about polynomial time construction of o(n) dimensional box
representations for boxicity 2 graphs. Other consequences of our approximation
algorithm include $O(\kappa(n))$ factor approximation algorithms for computing
the following parameters: the partial order dimension of finite posets, the
interval dimension of finite posets, minimum chain cover of bipartite graphs,
threshold dimension of split graphs and Ferrer's dimension of digraphs. Each of
these parameters is inapproximable within an $O(n^{1 - \epsilon})$-factor, for
any $\epsilon &gt;0$ in polynomial time unless NP=ZPP and the algorithms we derive
seem to be the first o(n) factor approximation algorithms known for all these
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04920</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04920</id><created>2015-05-19</created><updated>2015-11-27</updated><authors><author><keyname>Parsegov</keyname><forenames>Sergey E.</forenames></author><author><keyname>Proskurnikov</keyname><forenames>Anton V.</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author><author><keyname>Friedkin</keyname><forenames>Noah E.</forenames></author></authors><title>Novel Multidimensional Models of Opinion Dynamics in Social Networks</title><categories>cs.SY cs.SI math.OC</categories><comments>Preprint is under review in IEEE TAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike many complex networks studied in the literature, social networks
rarely exhibit regular unanimous behavior, or consensus of opinions. This
requires a development of mathematical models that are sufficiently simple to
be examined and capture, at the same time, the complex behavior of real social
groups, where opinions and the actions related to them may form clusters of
different sizes. One such model, proposed in [1], deals with scalar opinions
and extends the idea in [2] of iterative pooling to take into account the
actors' prejudices, caused by some exogenous factors and leading to
disagreement in the final opinions. In this paper, we offer a novel
multidimensional extension, which represents the dynamics of agents' opinions
on several topics, and those topic-specific opinions are interdependent. As
soon as opinions on several topics are affected simultaneously by the same
influence networks, they automatically become related. However, we introduce an
additional relation, interdependent topics, by which the opinions being formed
on one topic are functions of the opinions held on other topics. We examine
rigorous convergence properties of the proposed model and find explicitly the
steady opinions of the agents. Although our model assumes synchronous
communication among the agents, we show that the same final opinion may be
reached &quot;on average&quot; via asynchronous gossip-based protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04922</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04922</id><created>2015-05-19</created><authors><author><keyname>Yang</keyname><forenames>Weixin</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author><author><keyname>Liu</keyname><forenames>Manfei</forenames></author></authors><title>Character-level Chinese Writer Identification using Path Signature
  Feature, DropStroke and Deep CNN</title><categories>cs.CV</categories><comments>5 pages, 4 figures, 2 tables. Manuscript is accepted to appear in
  ICDAR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing online writer-identification systems require that the text
content is supplied in advance and rely on separately designed features and
classifiers. The identifications are based on lines of text, entire paragraphs,
or entire documents; however, these materials are not always available. In this
paper, we introduce a path-signature feature to an end-to-end text-independent
writer-identification system with a deep convolutional neural network (DCNN).
Because deep models require a considerable amount of data to achieve good
performance, we propose a data-augmentation method named DropStroke to enrich
personal handwriting. Experiments were conducted on online handwritten Chinese
characters from the CASIA-OLHWDB1.0 dataset, which consists of 3,866 classes
from 420 writers. For each writer, we only used 200 samples for training and
the remaining 3,666. The results reveal that the path-signature feature is
useful for writer identification, and the proposed DropStroke technique
enhances the generalization and significantly improves performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04925</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04925</id><created>2015-05-19</created><authors><author><keyname>Zhong</keyname><forenames>Zhuoyao</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author><author><keyname>Xie</keyname><forenames>Zecheng</forenames></author></authors><title>High Performance Offline Handwritten Chinese Character Recognition Using
  GoogLeNet and Directional Feature Maps</title><categories>cs.CV</categories><comments>5 pages, 4 figures, 2 tables. Manuscript is accepted to appear in
  ICDAR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Just like its great success in solving many computer vision problems, the
convolutional neural networks (CNN) provided new end-to-end approach to
handwritten Chinese character recognition (HCCR) with very promising results in
recent years. However, previous CNNs so far proposed for HCCR were neither deep
enough nor slim enough. We show in this paper that, a deeper architecture can
benefit HCCR a lot to achieve higher performance, meanwhile can be designed
with less parameters. We also show that the traditional feature extraction
methods, such as Gabor or gradient feature maps, are still useful for enhancing
the performance of CNN. We design a streamlined version of GoogLeNet [13],
which was original proposed for image classification in recent years with very
deep architecture, for HCCR (denoted as HCCR-GoogLeNet). The HCCR-GoogLeNet we
used is 19 layers deep but involves with only 7.26 million parameters.
Experiments were conducted using the ICDAR 2013 offline HCCR competition
dataset. It has been shown that with the proper incorporation with traditional
directional feature maps, the proposed single and ensemble HCCR-GoogLeNet
models achieve new state of the art recognition accuracy of 96.35% and 96.74%,
respectively, outperforming previous best result with significant gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04934</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04934</id><created>2015-05-19</created><updated>2015-08-29</updated><authors><author><keyname>Place</keyname><forenames>Thomas</forenames><affiliation>University de Bordeaux</affiliation></author><author><keyname>Segoufin</keyname><forenames>Luc</forenames><affiliation>INRIA &amp; ENS Cachan</affiliation></author></authors><title>Deciding definability in FO2(&lt;h,&lt;v) on trees</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:5) 2015</journal-ref><doi>10.2168/LMCS-11(3:5)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a decidable characterization of regular forest languages definable
in FO2(&lt;h,&lt;v). By FO2(&lt;h,&lt;v) we refer to the two variable fragment of first
order logic built from the descendant relation and the following sibling
relation. In terms of expressive power it corresponds to a fragment of the
navigational core of XPath that contains modalities for going up to some
ancestor, down to some descendant, left to some preceding sibling, and right to
some following sibling. We also show that our techniques can be applied to
other two variable first-order logics having exactly the same vertical
modalities as FO2(&lt;h,&lt;v) but having different horizontal modalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04935</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04935</id><created>2015-05-19</created><updated>2015-07-06</updated><authors><author><keyname>S&#xee;rbu</keyname><forenames>Alina</forenames></author><author><keyname>Babaoglu</keyname><forenames>Ozalp</forenames></author></authors><title>Towards Data-Driven Autonomics in Data Centers</title><categories>cs.DC cs.AI stat.ML</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continued reliance on human operators for managing data centers is a major
impediment for them from ever reaching extreme dimensions. Large computer
systems in general, and data centers in particular, will ultimately be managed
using predictive computational and executable models obtained through
data-science tools, and at that point, the intervention of humans will be
limited to setting high-level goals and policies rather than performing
low-level operations. Data-driven autonomics, where management and control are
based on holistic predictive models that are built and updated using generated
data, opens one possible path towards limiting the role of operators in data
centers. In this paper, we present a data-science study of a public Google
dataset collected in a 12K-node cluster with the goal of building and
evaluating a predictive model for node failures. We use BigQuery, the big data
SQL platform from the Google Cloud suite, to process massive amounts of data
and generate a rich feature set characterizing machine state over time. We
describe how an ensemble classifier can be built out of many Random Forest
classifiers each trained on these features, to predict if machines will fail in
a future 24-hour window. Our evaluation reveals that if we limit false positive
rates to 5%, we can achieve true positive rates between 27% and 88% with
precision varying between 50% and 72%. We discuss the practicality of including
our predictive model as the central component of a data-driven autonomic
manager and operating it on-line with live data streams (rather than off-line
on data logs). All of the scripts used for BigQuery and classification analyses
are publicly available from the authors' website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04938</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04938</id><created>2015-05-19</created><updated>2015-10-13</updated><authors><author><keyname>Iglesias</keyname><forenames>Jos&#xe9; A.</forenames></author><author><keyname>Kirisits</keyname><forenames>Clemens</forenames></author></authors><title>Convective regularization for optical flow</title><categories>math.OC cs.CV</categories><msc-class>49N45, 68T45, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue that the time derivative in a fixed coordinate frame may not be the
most appropriate measure of time regularity of an optical flow field. Instead,
for a given velocity field $v$ we consider the convective acceleration $v_t +
\nabla v v$ which describes the acceleration of objects moving according to
$v$. Consequently we investigate the suitability of the nonconvex functional
$\|v_t + \nabla v v\|^2_{L^2}$ as a regularization term for optical flow. We
demonstrate that this term acts as both a spatial and a temporal regularizer
and has an intrinsic edge-preserving property. We incorporate it into a
contrast invariant and time-regularized variant of the Horn-Schunck functional,
prove existence of minimizers and verify experimentally that it addresses some
of the problems of basic quadratic models. For the minimization we use an
iterative scheme that approximates the original nonlinear problem with a
sequence of linear ones. We believe that the convective acceleration may be
gainfully introduced in a variety of optical flow models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04941</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04941</id><created>2015-05-19</created><authors><author><keyname>Ma</keyname><forenames>Athen</forenames></author><author><keyname>Mondragon</keyname><forenames>Raul J.</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author></authors><title>Funding shapes the anatomy of scientific research</title><categories>physics.soc-ph cs.SI</categories><comments>Main text: 10 pages with 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research projects are primarily collaborative in nature through internal and
external partnerships, but what role does funding play in their formation?
Here, we examined over 43,000 funded projects in the past three decades,
enabling us to characterise changes in the funding landscape and their impacts
on the underlying collaboration patterns. We observed rising inequality in the
distribution of funding and its effect was most noticeable at the institutional
level in which the leading universities diversified their collaborations and
increasingly became the knowledge brokers. Furthermore, these universities
formed a cohesive core through their close ties, and such reliance appeared to
be a key for their research success, with the elites in the core
over-attracting resources but in turn rewarding in both research breadth and
depth. Our results reveal how collaboration networks undergo previously unknown
adaptive organisation in response to external driving forces, which can have
far-reaching implications for future policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04943</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04943</id><created>2015-05-19</created><authors><author><keyname>Paul</keyname><forenames>Thomas</forenames></author><author><keyname>Puscher</keyname><forenames>Daniel</forenames></author><author><keyname>Strufe</keyname><forenames>Thorsten</forenames></author></authors><title>The User Behavior in Facebook and its Development from 2009 until 2014</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Social Networking is a fascinating phenomena, attracting more than one
billion people. It supports basic human needs such as communication,
socializing with others and reputation building. Thus, an in-depth
understanding of user behavior in Online Social Networks (OSNs) can provide
major insights into human behavior, and impacts design choices of social
platforms and applications. However, researchers have only limited access to
behavioral data. As a consequence of this limitation, user behavior in OSNs as
well as its development in recent years are still not deeply understood.
  In this paper, we present a study about user behavior on the most popular
OSN, Facebook, with 2071 participants from 46 countries. We elaborate how
Facebookers orchestrate the offered functions to achieve individual benefit in
2014 and evaluate user activity changes from 2009 till 2014 to understand the
development of user behavior. Inter alia, we focus on the most important
functionality, the newsfeed, to understand content sharing amongst users. We
(i) yield a better understanding on content sharing and consumption and (ii)
refine behavioral assumptions in the literature to improve the performance of
alternative social platforms. Furthermore, we (iii) contribute evidence to the
discussion of Facebook to be an aging network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04944</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04944</id><created>2015-05-19</created><updated>2015-09-10</updated><authors><author><keyname>Ding</keyname><forenames>Xu</forenames></author><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author><author><keyname>Wang</keyname><forenames>Li-Chun</forenames></author><author><keyname>Zhao</keyname><forenames>Xiaohui</forenames></author></authors><title>Coexisting Success Probability and Throughput of Multi-RAT Wireless
  Networks with Unlicensed Band Access</title><categories>cs.NI cs.IT math.IT</categories><comments>4 pages, 3 figures, letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, the coexisting success probability and throughput of a
wireless network consisting of multiple subnetworks of different radio access
technologies (RATs) is investigated. The coexisting success probability that is
defined as the average of all success probabilities of all subnetworks is found
in closed-form and it will be shown to have the concavity over the number of
channels in the unlicensed band. The optimal deployment densities of all
different RATs access points (APs) that maximize the coexisting success
probability are shown to exist and can be found under the derived constraint on
network parameters. The coexisting throughput is defined as the per-channel sum
of all spectrum efficiencies of all subnetworks and numerical results show that
it is significantly higher than the throughput of the unlicensed band only
accessed by WiFi APs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04947</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04947</id><created>2015-05-19</created><updated>2015-05-26</updated><authors><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author></authors><title>The Mean SIR of Large-Scale Wireless Networks: Its Closed-Form
  Expression and Main Applications</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures, letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a large-scale wireless ad hoc network in which all transmitters form a
homogeneous of Poisson point process, the statistics of the
signal-to-interference ratio (SIR) in prior work is only derived in closed-form
for the case of Rayleigh fading channels. In this letter, the mean SIR is found
in closed-form for general random channel (power) gain, transmission distance
and power control models. According to the derived mean SIR, we first show that
channel gain randomness actually benefits the mean SIR so that the upper bound
on the mean spectrum efficiency increases. Then we show that stochastic power
control and opportunistic scheduling that capture the randomness of channel
gain and transmission distance can significantly not only enhance the mean SIR
but reduce the outage probability. The mean-SIR-based throughput capacity is
proposed and it can be maximized by a unique optimal intensity of transmitters
if the derived supporting set of the intensity exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04952</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04952</id><created>2015-05-19</created><authors><author><keyname>Kalai</keyname><forenames>Gil</forenames></author></authors><title>Some old and new problems in combinatorial geometry I: Around Borsuk's
  problem</title><categories>math.CO cs.CG math.MG</categories><comments>This is a draft of a chapter for &quot;Surveys in Combinatorics 2015,&quot;
  edited by Artur Czumaj, Angelos Georgakopoulos, Daniel Kral, Vadim Lozin, and
  Oleg Pikhurko. The final published version shall be available for purchase
  from Cambridge University Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Borsuk asked in 1933 if every set of diameter 1 in $R^d$ can be covered by
$d+1$ sets of smaller diameter. In 1993, a negative solution, based on a
theorem by Frankl and Wilson, was given by Kahn and Kalai. In this paper I will
present questions related to Borsuk's problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04956</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04956</id><created>2015-05-19</created><updated>2015-10-05</updated><authors><author><keyname>Keuper</keyname><forenames>Janis</forenames></author><author><keyname>Pfreundt</keyname><forenames>Franz-Josef</forenames></author></authors><title>Asynchronous Parallel Stochastic Gradient Descent - A Numeric Core for
  Scalable Distributed Machine Learning Algorithms</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation of a vast majority of machine learning (ML) algorithms
boils down to solving a numerical optimization problem. In this context,
Stochastic Gradient Descent (SGD) methods have long proven to provide good
results, both in terms of convergence and accuracy. Recently, several
parallelization approaches have been proposed in order to scale SGD to solve
very large ML problems. At their core, most of these approaches are following a
map-reduce scheme. This paper presents a novel parallel updating algorithm for
SGD, which utilizes the asynchronous single-sided communication paradigm.
Compared to existing methods, Asynchronous Parallel Stochastic Gradient Descent
(ASGD) provides faster (or at least equal) convergence, close to linear scaling
and stable accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04966</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04966</id><created>2015-05-19</created><authors><author><keyname>Fawzi</keyname><forenames>Alhussein</forenames></author><author><keyname>Sinn</keyname><forenames>Mathieu</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Multi-task additive models with shared transfer functions based on
  dictionary learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Additive models form a widely popular class of regression models which
represent the relation between covariates and response variables as the sum of
low-dimensional transfer functions. Besides flexibility and accuracy, a key
benefit of these models is their interpretability: the transfer functions
provide visual means for inspecting the models and identifying domain-specific
relations between inputs and outputs. However, in large-scale problems
involving the prediction of many related tasks, learning independently additive
models results in a loss of model interpretability, and can cause overfitting
when training data is scarce. We introduce a novel multi-task learning approach
which provides a corpus of accurate and interpretable additive models for a
large number of related forecasting tasks. Our key idea is to share transfer
functions across models in order to reduce the model complexity and ease the
exploration of the corpus. We establish a connection with sparse dictionary
learning and propose a new efficient fitting algorithm which alternates between
sparse coding and transfer function updates. The former step is solved via an
extension of Orthogonal Matching Pursuit, whose properties are analyzed using a
novel recovery condition which extends existing results in the literature. The
latter step is addressed using a traditional dictionary update rule.
Experiments on real-world data demonstrate that our approach compares favorably
to baseline methods while yielding an interpretable corpus of models, revealing
structure among the individual tasks and being more robust when training data
is scarce. Our framework therefore extends the well-known benefits of additive
models to common regression settings possibly involving thousands of tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04969</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04969</id><created>2015-05-19</created><authors><author><keyname>Bourgeois</keyname><forenames>N.</forenames></author><author><keyname>Catellier</keyname><forenames>R.</forenames></author><author><keyname>Denat</keyname><forenames>T.</forenames></author><author><keyname>Paschos</keyname><forenames>V. Th.</forenames></author></authors><title>Average-case complexity of a branch-and-bound algorithm for maximum
  independent set, under the $\mathcal{G}(n,p)$ random model</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study average-case complexity of branch-and-bound for maximum independent
set in random graphs under the $\mathcal{G}(n,p)$ distribution. In this model
every pair $(u,v)$ of vertices belongs to $E$ with probability $p$
independently on the existence of any other edge. We make a precise case
analysis, providing phase transitions between subexponential and exponential
complexities depending on the probability $p$ of the random model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04972</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04972</id><created>2015-05-19</created><updated>2015-11-01</updated><authors><author><keyname>Ryman</keyname><forenames>Arthur</forenames></author></authors><title>Recursion in RDF Data Shape Languages</title><categories>cs.DB cs.AI</categories><comments>31 pages, 2 figures, invited expert contribution to the W3C RDF Data
  Shapes Working Group</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An RDF data shape is a description of the expected contents of an RDF
document (aka graph) or dataset. A major part of this description is the set of
constraints that the document or dataset is required to satisfy. W3C recently
(2014) chartered the RDF Data Shapes Working Group to define SHACL, a standard
RDF data shape language. We refer to the ability to name and reference shape
language elements as recursion. This article provides a precise definition of
the meaning of recursion as used in Resource Shape 2.0. The definition of
recursion presented in this article is largely independent of language-specific
details. We speculate that it also applies to ShEx and to all three of the
current proposals for SHACL. In particular, recursion is not permitted in the
SHACL-SPARQL proposal, but we conjecture that recursion could be added by using
the definition proposed here as a top-level control structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04979</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04979</id><created>2015-05-19</created><updated>2015-08-25</updated><authors><author><keyname>Abbas</keyname><forenames>Syed Mohsin</forenames></author><author><keyname>Fan</keyname><forenames>YouZhe</forenames></author><author><keyname>Chen</keyname><forenames>Ji</forenames></author><author><keyname>Tsui</keyname><forenames>Chi-Ying</forenames></author></authors><title>Low Complexity Belief Propagation Polar Code Decoders</title><categories>cs.IT math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since its invention, polar code has received a lot of attention because of
its capacity-achieving performance and low encoding and decoding complexity.
Successive cancellation decoding (SCD) and belief propagation decoding (BPD)
are two of the most popular approaches for decoding polar codes. SCD is able to
achieve good error-correcting performance and is less computationally expensive
as compared to BPD. However SCDs suffer from long latency and low throughput
due to the serial nature of the successive cancellation algorithm. BPD is
parallel in nature and hence is more attractive for high throughput
applications. However since it is iterative in nature, the required latency and
energy dissipation increases linearly with the number of iterations. In this
work, we borrow the idea of SCD and propose a novel scheme based on
sub-factor-graph freezing to reduce the average number of computations as well
as the average number of iterations required by BPD, which directly translates
into lower latency and energy dissipation. Simulation results show that the
proposed scheme has no performance degradation and achieves significant
reduction in computation complexity over the existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04981</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04981</id><created>2015-05-19</created><updated>2015-06-17</updated><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author><author><keyname>Veloz</keyname><forenames>Tomas</forenames></author></authors><title>A New Fundamental Evidence of Non-Classical Structure in the Combination
  of Natural Concepts</title><categories>cs.AI quant-ph</categories><comments>14 pages. arXiv admin note: substantial text overlap with
  arXiv:1503.04260</comments><doi>10.1098/rsta.2015.0095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recently performed cognitive experiments on conjunctions and negations of
two concepts with the aim of investigating the combination problem of concepts.
Our experiments confirmed the deviations (conceptual vagueness, underextension,
overextension, etc.) from the rules of classical (fuzzy) logic and probability
theory observed by several scholars in concept theory, while our data were
successfully modeled in a quantum-theoretic framework developed by ourselves.
In this paper, we isolate a new, very stable and systematic pattern of
violation of classicality that occurs in concept combinations. In addition, the
strength and regularity of this non-classical effect leads us to believe that
it occurs at a more fundamental level than the deviations observed up to now.
It is our opinion that we have identified a deep non-classical mechanism
determining not only how concepts are combined but, rather, how they are
formed. We show that this effect can be faithfully modeled in a two-sector Fock
space structure, and that it can be exactly explained by assuming that human
thought is the supersposition of two processes, a 'logical reasoning', guided
by 'logic', and a 'conceptual reasoning' guided by 'emergence', and that the
latter generally prevails over the former. All these findings provide a new
fundamental support to our quantum-theoretic approach to human cognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04984</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04984</id><created>2015-05-19</created><authors><author><keyname>Huggins</keyname><forenames>Jonathan H.</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Risk and Regret of Hierarchical Bayesian Learners</title><categories>cs.LG stat.ML</categories><comments>In Proceedings of the 32nd International Conference on Machine
  Learning (ICML 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common statistical practice has shown that the full power of Bayesian methods
is not realized until hierarchical priors are used, as these allow for greater
&quot;robustness&quot; and the ability to &quot;share statistical strength.&quot; Yet it is an
ongoing challenge to provide a learning-theoretically sound formalism of such
notions that: offers practical guidance concerning when and how best to utilize
hierarchical models; provides insights into what makes for a good hierarchical
prior; and, when the form of the prior has been chosen, can guide the choice of
hyperparameter settings. We present a set of analytical tools for understanding
hierarchical priors in both the online and batch learning settings. We provide
regret bounds under log-loss, which show how certain hierarchical models
compare, in retrospect, to the best single model in the model class. We also
show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound
for any bounded loss, a result which may be of independent interest. Risk and
regret bounds for Student's $t$ and hierarchical Gaussian priors allow us to
formalize the concepts of &quot;robustness&quot; and &quot;sharing statistical strength.&quot;
Priors for feature selection are investigated as well. Our results suggest that
the learning-theoretic benefits of using hierarchical priors can often come at
little cost on practical problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04985</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04985</id><created>2015-05-19</created><updated>2015-09-20</updated><authors><author><keyname>Chen</keyname><forenames>Taolue</forenames><affiliation>Middlesex University London</affiliation></author><author><keyname>Fokkink</keyname><forenames>Wan</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames><affiliation>NICTA</affiliation></author></authors><title>On the Axiomatizability of Impossible Futures</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:17) 2015</journal-ref><doi>10.2168/LMCS-11(3:17)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general method is established to derive a ground-complete axiomatization
for a weak semantics from such an axiomatization for its concrete counterpart,
in the context of the process algebra BCCS. This transformation moreover
preserves omega-completeness. It is applicable to semantics at least as coarse
as impossible futures semantics. As an application, ground- and omega-complete
axiomatizations are derived for weak failures, completed trace and trace
semantics. We then present a finite, sound, ground-complete axiomatization for
the concrete impossible futures preorder, which implies a finite, sound,
ground-complete axiomatization for the weak impossible futures preorder. In
contrast, we prove that no finite, sound axiomatization for BCCS modulo
concrete and weak impossible futures equivalence is ground-complete. If the
alphabet of actions is infinite, then the aforementioned ground-complete
axiomatizations are shown to be omega-complete. If the alphabet is finite, we
prove that the inequational theories of BCCS modulo the concrete and weak
impossible futures preorder lack such a finite basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.04996</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.04996</id><created>2015-05-19</created><updated>2016-03-04</updated><authors><author><keyname>Boon</keyname><forenames>Marko</forenames></author><author><keyname>Winands</keyname><forenames>Erik</forenames></author></authors><title>Critically loaded k-limited polling systems</title><categories>math.PR cs.PF</categories><comments>Proceedings of ValueTools 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a two-queue polling model with switch-over times and $k$-limited
service (serve at most $k_i$ customers during one visit period to queue $i$) in
each queue. The major benefit of the $k$-limited service discipline is that it
- besides bounding the cycle time - effectuates prioritization by assigning
different service limits to different queues. System performance is studied in
the heavy-traffic regime, in which one of the queues becomes critically loaded
with the other queue remaining stable. By using a singular-perturbation
technique, we rigorously prove heavy-traffic limits for the joint queue-length
distribution. Moreover, it is observed that an interchange exists among the
first two moments in service and switch-over times such that the HT limits
remain unchanged. Not only do the rigorously proven results readily carry over
to $N$($\geq2$) queue polling systems, but one can also easily relax the
distributional assumptions. The results and insights of this note prove their
worth in the performance analysis of Wireless Personal Area Networks (WPAN) and
mobile networks, where different users compete for access to the shared scarce
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05004</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05004</id><created>2015-05-19</created><updated>2015-08-24</updated><authors><author><keyname>Gasse</keyname><forenames>Maxime</forenames><affiliation>DM2L</affiliation></author><author><keyname>Aussem</keyname><forenames>Alex</forenames><affiliation>DM2L</affiliation></author><author><keyname>Elghazel</keyname><forenames>Haytham</forenames><affiliation>DM2L</affiliation></author></authors><title>An Experimental Comparison of Hybrid Algorithms for Bayesian Network
  Structure Learning</title><categories>stat.ML cs.AI cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1101.5184 by other authors.
  Lecture notes in computer science, springer, 2012, Machine Learning and
  Knowledge Discovery in Databases, 7523, pp.58-73</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-33460-3_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel hybrid algorithm for Bayesian network structure learning,
called Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesian
network and then performs a Bayesian-scoring greedy hill-climbing search to
orient the edges. It is based on a subroutine called HPC, that combines ideas
from incremental and divide-and-conquer constraint-based methods to learn the
parents and children of a target variable. We conduct an experimental
comparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the
most powerful state-of-the-art algorithm for Bayesian network structure
learning, on several benchmarks with various data sizes. Our extensive
experiments show that H2PC outperforms MMHC both in terms of goodness of fit to
new data and in terms of the quality of the network structure itself, which is
closer to the true dependence structure of the data. The source code (in R) of
H2PC as well as all data sets used for the empirical tests are publicly
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05007</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05007</id><created>2015-05-19</created><updated>2016-01-04</updated><authors><author><keyname>Blomstedt</keyname><forenames>Paul</forenames></author><author><keyname>Dutta</keyname><forenames>Ritabrata</forenames></author><author><keyname>Seth</keyname><forenames>Sohan</forenames></author><author><keyname>Brazma</keyname><forenames>Alvis</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author></authors><title>Modelling-based experiment retrieval: A case study with gene expression
  clustering</title><categories>stat.ML cs.IR cs.LG</categories><comments>Updated figures. The final version of this article will appear in
  Bioinformatics (https://bioinformatics.oxfordjournals.org/)</comments><doi>10.1093/bioinformatics/btv762</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Public and private repositories of experimental data are growing
to sizes that require dedicated methods for finding relevant data. To improve
on the state of the art of keyword searches from annotations, methods for
content-based retrieval have been proposed. In the context of gene expression
experiments, most methods retrieve gene expression profiles, requiring each
experiment to be expressed as a single profile, typically of case vs. control.
A more general, recently suggested alternative is to retrieve experiments whose
models are good for modelling the query dataset. However, for very noisy and
high-dimensional query data, this retrieval criterion turns out to be very
noisy as well.
  Results: We propose doing retrieval using a denoised model of the query
dataset, instead of the original noisy dataset itself. To this end, we
introduce a general probabilistic framework, where each experiment is modelled
separately and the retrieval is done by finding related models. For retrieval
of gene expression experiments, we use a probabilistic model called product
partition model, which induces a clustering of genes that show similar
expression patterns across a number of samples. The suggested metric for
retrieval using clusterings is the normalized information distance. Empirical
results finally suggest that inference for the full probabilistic model can be
approximated with good performance using computationally faster heuristic
clustering approaches (e.g. $k$-means). The method is highly scalable and
straightforward to apply to construct a general-purpose gene expression
experiment retrieval method.
  Availability: The method can be implemented using standard clustering
algorithms and normalized information distance, available in many statistical
software packages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05008</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05008</id><created>2015-05-19</created><updated>2015-05-25</updated><authors><author><keyname>Santos</keyname><forenames>Cicero Nogueira dos</forenames></author><author><keyname>Guimar&#xe3;es</keyname><forenames>Victor</forenames></author></authors><title>Boosting Named Entity Recognition with Neural Character Embeddings</title><categories>cs.CL</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most state-of-the-art named entity recognition (NER) systems rely on
handcrafted features and on the output of other NLP tasks such as
part-of-speech (POS) tagging and text chunking. In this work we propose a
language-independent NER system that uses automatically learned features only.
Our approach is based on the CharWNN deep neural network, which uses word-level
and character-level representations (embeddings) to perform sequential
classification. We perform an extensive number of experiments using two
annotated corpora in two different languages: HAREM I corpus, which contains
texts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in
Spanish. Our experimental results shade light on the contribution of neural
character embeddings for NER. Moreover, we demonstrate that the same neural
network which has been successfully applied to POS tagging can also achieve
state-of-the-art results for language-independet NER, using the same
hyperparameters, and without any handcrafted features. For the HAREM I corpus,
CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score
for the total scenario (ten NE classes), and by 7.2 points in the F1 for the
selective scenario (five NE classes).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05022</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05022</id><created>2015-05-19</created><updated>2015-07-17</updated><authors><author><keyname>Inclezan</keyname><forenames>Daniela</forenames></author><author><keyname>Gelfond</keyname><forenames>Michael</forenames></author></authors><title>Modular Action Language ALM</title><categories>cs.LO cs.AI</categories><comments>65 pages, 7 figures. To appear in Theory and Practice of Logic
  Programming (TPLP)</comments><doi>10.1017/S1471068415000095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces a new modular action language, ALM, and illustrates the
methodology of its use. It is based on the approach of Gelfond and Lifschitz
(1993; 1998) in which a high-level action language is used as a front end for a
logic programming system description. The resulting logic programming
representation is used to perform various computational tasks. The methodology
based on existing action languages works well for small and even medium size
systems, but is not meant to deal with larger systems that require structuring
of knowledge. ALM is meant to remedy this problem. Structuring of knowledge in
ALM is supported by the concepts of module (a formal description of a specific
piece of knowledge packaged as a unit), module hierarchy, and library, and by
the division of a system description of ALM into two parts: theory and
structure. A theory consists of one or more modules with a common theme,
possibly organized into a module hierarchy based on a dependency relation. It
contains declarations of sorts, attributes, and properties of the domain
together with axioms describing them. Structures are used to describe the
domain's objects. These features, together with the means for defining classes
of a domain as special cases of previously defined ones, facilitate the
stepwise development, testing, and readability of a knowledge base, as well as
the creation of knowledge representation libraries. To appear in Theory and
Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05025</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05025</id><created>2015-05-19</created><updated>2016-02-12</updated><authors><author><keyname>Bramas</keyname><forenames>Quentin</forenames><affiliation>LINCS, UPMC, LIP6, NPA</affiliation></author><author><keyname>Foreback</keyname><forenames>Dianne</forenames><affiliation>IUF, LINCS, UPMC, LIP6, NPA</affiliation></author><author><keyname>Nesterenko</keyname><forenames>Mikhail</forenames><affiliation>IUF, LINCS, UPMC, LIP6, NPA</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>IUF, LINCS, UPMC, LIP6, NPA</affiliation></author></authors><title>Packet Efficient Implementation of the Omega Failure Detector</title><categories>cs.DC cs.DS cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We assume that a message may be delivered by packets through multiple hops
and investigate the feasibility and efficiency of an implementation of the
Omega Failure Detector under such an assumption.To motivate the study, we prove
that the existence and sustainability of a leader is exponentially more
probable in a multi-hop Omega implementation than in a single-hop one.An
implementation is: \emph{message efficient} if all but finitely many messages
are sent by a single process; \emph{packet efficient} if the number of packets
used to transmit a message in all but finitely many messages is linear w.r.t
the number of processes, packets of different messages may potentially use
different channels, thus the number of used channels is not limited;
\emph{super packet efficient} if the number of channels used by packets to
transmit all but finitely many messages is linear.We present the following
results for deterministic algorithms. If reliability and timeliness of one
message does not correlate with another, i.e., there are no channel reliability
properties, then a packet efficient implementation of Omega is impossible. If
eventuallytimely and fair-lossy channels are considered, we establish necessary
and sufficient conditions for the existence of a message and packet efficient
implementation of Omega. We also prove that the eventuality of timeliness of
channels makes a super packet efficientimplementation of Omega impossible. On
the constructive side, we present and prove correct a deterministic packet
efficient implementation of Omega that matches the necessary conditions we
established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05028</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05028</id><created>2015-05-19</created><updated>2015-07-09</updated><authors><author><keyname>Zimmermann</keyname><forenames>Th&#xe9;o</forenames><affiliation>ENS Paris, PPS</affiliation></author><author><keyname>Herbelin</keyname><forenames>Hugo</forenames><affiliation>PPS, PI.R2</affiliation></author></authors><title>Automatic and Transparent Transfer of Theorems along Isomorphisms in the
  Coq Proof Assistant</title><categories>cs.LO cs.MS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mathematics, it is common practice to have several constructions for the
same objects. Mathematicians will identify them modulo isomorphism and will not
worry later on which construction they use, as theorems proved for one
construction will be valid for all.
  When working with proof assistants, it is also common to see several
data-types representing the same objects. This work aims at making the use of
several isomorphic constructions as simple and as transparent as it can be done
informally in mathematics. This requires inferring automatically the missing
proof-steps.
  We are designing an algorithm which finds and fills these missing proof-steps
and we are implementing it as a plugin for Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05033</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05033</id><created>2015-05-19</created><authors><author><keyname>Aviram</keyname><forenames>Nimrod</forenames></author><author><keyname>Shavitt</keyname><forenames>Yuval</forenames></author></authors><title>Optimizing Dijkstra for real-world performance</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Dijkstra's algorithm to compute the shortest paths in a graph from a
single source node to all other nodes is common practice in industry and
academia. Although the original description of the algorithm advises using a
Fibonacci Heap as its internal queue, it has been noted that in practice, a
binary (or $d$-ary) heap implementation is significantly faster. This paper
introduces an even faster queue design for the algorithm.
  Our experimental results currently put our prototype implementation at about
twice as fast as the Boost implementation of the algorithm on both real-world
and generated large graphs. Furthermore, this preliminary implementation was
written in only a few weeks, by a single programmer. The fact that such an
early prototype compares favorably against Boost, a well-known open source
library developed by expert programmers, gives us reason to believe our design
for the queue is indeed better suited to the problem at hand, and the favorable
time measurements are not a product of any specific implementation technique we
employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05041</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05041</id><created>2015-05-19</created><updated>2015-11-30</updated><authors><author><keyname>Ballico</keyname><forenames>Edoardo</forenames></author><author><keyname>Marcolla</keyname><forenames>Chiara</forenames></author></authors><title>Higher Hamming weights for locally recoverable codes on algebraic curves</title><categories>math.AC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the locally recoverable codes on algebraic curves. In the first part
of this article, we provide a bound of generalized Hamming weight of these
codes. Whereas in the second part, we propose a new family of algebraic
geometric LRC codes, that are LRC codes from Norm-Trace curve. Finally, using
some properties of Hermitian codes, we improve the bounds of distance proposed
in [1] for some Hermitian LRC codes.
  [1] A. Barg, I. Tamo, and S. Vlladut. Locally recoverable codes on algebraic
curves. arXiv preprint arXiv:1501.04904, 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05055</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05055</id><created>2015-03-24</created><updated>2015-08-01</updated><authors><author><keyname>Burstedde</keyname><forenames>Carsten</forenames></author><author><keyname>Isaac</keyname><forenames>Tobin</forenames></author></authors><title>Morton curve segments produce no more than two distinct face-connected
  subdomains</title><categories>cs.CG</categories><comments>16 pages, 9 figures: added non-recursive dimension-independent proofs
  and enumeration algorithm with results</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Morton- or z-curve is one example for a space filling curve: Given a
level of refinement L, it maps the interval [0, 2**dL) one-to-one to a set of
d-dimensional cubes of edge length 2**-L that form a subdivision of the unit
cube. In contrast to the Hilbert curve that is continuous, the Morton curve
produces jumps. We prove that any contiguous subinterval of the curve divides
the unit cube into no more than two face-connected, star-shaped subdomains, for
arbitrary dimensions d &gt; 0. In other words, there can be at most one jump in
the curve that creates a new subdomain. We provide an algorithm for enumerating
how many segments of a given length are continuous and prove a lower bound on
the fraction of segments of a given length that are continuous. This result
should eliminate concerns that the Morton curve may lead to an uncontrolled
fragmentation of the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05063</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05063</id><created>2015-05-19</created><updated>2015-12-18</updated><authors><author><keyname>Miranda</keyname><forenames>Conrado Silva</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando Jos&#xe9;</forenames></author></authors><title>Necessary and Sufficient Conditions for Surrogate Functions of Pareto
  Frontiers and Their Synthesis Using Gaussian Processes</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the necessary and sufficient conditions that surrogate
functions must satisfy to properly define frontiers of non-dominated solutions
in multi-objective optimization problems. These new conditions work directly on
the objective space, thus being agnostic about how the solutions are evaluated.
Therefore, real objectives or user-designed objectives' surrogates are allowed,
opening the possibility of linking independent objective surrogates. To
illustrate the practical consequences of adopting the proposed conditions, we
use Gaussian processes as surrogates endowed with monotonicity soft constraints
and with an adjustable degree of flexibility, and compare them to regular
Gaussian processes and to a frontier surrogate method in the literature that is
the closest to the method proposed in this paper. Results show that the
necessary and sufficient conditions proposed here are finely managed by the
constrained Gaussian process, guiding to high-quality surrogates capable of
suitably synthesizing an approximation to the Pareto frontier in challenging
instances of multi-objective optimization, while an existing approach that does
not take the theory proposed in consideration defines surrogates which greatly
violate the conditions to describe a valid frontier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05072</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05072</id><created>2015-05-19</created><authors><author><keyname>Feuilloley</keyname><forenames>Laurent</forenames></author></authors><title>Brief Announcement : Average Complexity for the LOCAL Model</title><categories>cs.DC</categories><doi>10.1145/2767386.2767446</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A standard model in network synchronised distributed computing is the LOCAL
model. In this model, the processors work in rounds and, in the classic
setting, they know the number of vertices of the network, $n$. Using $n$, they
can compute the number of rounds after which they must all stop and output. It
has been shown recently that for many problems, one can basically remove the
assumption about the knowledge of $n$, without increasing the asymptotic
running time. In this case, it is assumed that different vertices can choose
their final output at different rounds, but continue to transmit messages. In
both models, the measure of the running time is the number of rounds before the
last node outputs. In this brief announcement, the vertices do not have the
knowledge of $n$, and we consider an alternative measure: the average, over the
nodes, of the number of rounds before they output. We prove that the complexity
of a problem can be exponentially smaller with the new measure, but that
Linial's lower bound for colouring still holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05079</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05079</id><created>2015-05-19</created><updated>2015-11-11</updated><authors><author><keyname>Farnsworth</keyname><forenames>Cameron</forenames></author></authors><title>Koszul-Young Flattenings and Symmetric Border Rank of the Determinant</title><categories>cs.CC math.AG</categories><comments>11 pages. Additions to acknowledgements. Accepted to the Journal of
  Algebra</comments><msc-class>68Q17, 14N05</msc-class><doi>10.1016/j.jalgebra.2015.11.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new lower bounds for the symmetric border rank of the n x n
determinant for all n. Further lower bounds are given for the 3 x 3 permanent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05080</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05080</id><created>2015-05-19</created><authors><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Cheung</keyname><forenames>Karen C.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>On the Statistics of Reaction-Diffusion Simulations for Molecular
  Communication</title><categories>physics.chem-ph cs.IT math.IT</categories><comments>6 pages, 1 table, 10 figures. Submitted to the 2nd ACM International
  Conference on Nanoscale Computing and Communication (ACM NANOCOM 2015) on May
  16, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A molecule traveling in a realistic propagation environment can experience
stochastic interactions with other molecules and the environment boundary. The
statistical behavior of some isolated phenomena, such as dilute unbounded
molecular diffusion, are well understood. However, the coupling of multiple
interactions can impede closed-form analysis, such that simulations are
required to determine the statistics. This paper compares the statistics of
molecular reaction-diffusion simulation models from the perspective of
molecular communication systems. Microscopic methods track the location and
state of every molecule, whereas mesoscopic methods partition the environment
into virtual containers that hold molecules. The properties of each model are
described and compared with a hybrid of both models. Simulation results also
assess the accuracy of Poisson and Gaussian approximations of the underlying
Binomial statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05081</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05081</id><created>2015-05-19</created><updated>2015-05-28</updated><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Multiparty Probability Computation and Verification</title><categories>cs.CR</categories><comments>8 pages, 4 figures; Corrected typographical errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiparty computation protocol is described in which the parties can
generate different probability events that is based on the sharing of a single
anonymized random number, and also perform oblivious transfer. A method to
verify the correctness of the procedure, without revealing the random numbers
used by the parties, is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05090</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05090</id><created>2015-05-19</created><updated>2015-11-24</updated><authors><author><keyname>Panferov</keyname><forenames>Eugene</forenames></author></authors><title>A Canonical Password Strength Measure</title><categories>cs.CR</categories><comments>7 pages</comments><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We notice that the &quot;password security&quot; discourse is missing a fundamental
notion of the &quot;password strength&quot;. We propose a canonical measure of password's
strength. We give formal definition of the &quot;guessing attack&quot;, and the
&quot;attacker's strategy&quot;. The measure is based on the assessment of the efficiency
of the best possible guessing attack. Unlike naive password strength
assessments our measure takes into account the attacker's strategy. We argue
strongly against widespread informal assumptions about &quot;strong&quot; and &quot;weak&quot;
passwords, and advise to adopt formal metrics such as proposed one. This paper
does NOT advise you to include &quot;at least three capital letters&quot;, seven
underscores, and a number thirteen in your password.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05106</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05106</id><created>2015-05-19</created><authors><author><keyname>Bae</keyname><forenames>Sang Won</forenames></author><author><keyname>Shin</keyname><forenames>Chan-Su</forenames></author><author><keyname>Vigneron</keyname><forenames>Antoine</forenames></author></authors><title>Improved Bounds for Beacon-Based Coverage and Routing in Simple
  Rectilinear Polygons</title><categories>cs.CG</categories><comments>23 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish tight bounds for beacon-based coverage problems, and improve the
bounds for beacon-based routing problems in simple rectilinear polygons.
Specifically, we show that $\lfloor \frac{n}{6} \rfloor$ beacons are always
sufficient and sometimes necessary to cover a simple rectilinear polygon $P$
with $n$ vertices. We also prove tight bounds for the case where $P$ is
monotone, and we present an optimal linear-time algorithm that computes the
beacon-based kernel of $P$. For the routing problem, we show that $\lfloor
\frac{3n-4}{8} \rfloor - 1$ beacons are always sufficient, and $\lceil
\frac{n}{4}\rceil-1$ beacons are sometimes necessary to route between all pairs
of points in $P$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05114</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05114</id><created>2015-05-19</created><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Candes</keyname><forenames>Emmanuel J.</forenames></author></authors><title>Solving Random Quadratic Systems of Equations Is Nearly as Easy as
  Solving Linear Systems</title><categories>cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fundamental problem of solving quadratic systems of equations
in $n$ variables, where $y_i = |\langle \boldsymbol{a}_i, \boldsymbol{x}
\rangle|^2$, $i = 1, \ldots, m$ and $\boldsymbol{x} \in \mathbb{R}^n$ is
unknown. We propose a novel method, which starting with an initial guess
computed by means of a spectral method, proceeds by minimizing a nonconvex
functional as in the Wirtinger flow approach. There are several key
distinguishing features, most notably, a distinct objective functional and
novel update rules, which operate in an adaptive fashion and drop terms bearing
too much influence on the search direction. These careful selection rules
provide a tighter initial guess, better descent directions, and thus enhanced
practical performance. On the theoretical side, we prove that for certain
unstructured models of quadratic systems, our algorithms return the correct
solution in linear time, i.e. in time proportional to reading the data
$\{\boldsymbol{a}_i\}$ and $\{y_i\}$ as soon as the ratio $m/n$ between the
number of equations and unknowns exceeds a fixed numerical constant. We extend
the theory to deal with noisy systems in which we only have $y_i \approx
|\langle \boldsymbol{a}_i, \boldsymbol{x} \rangle|^2$ and prove that our
algorithms achieve a statistical accuracy, which is nearly un-improvable. We
complement our theoretical study with numerical examples showing that solving
random quadratic systems is both computationally and statistically not much
harder than solving linear systems of the same size---hence the title of this
paper. For instance, we demonstrate empirically that the computational cost of
our algorithm is about four times that of solving a least-squares problem of
the same size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05123</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05123</id><created>2015-05-19</created><updated>2015-06-15</updated><authors><author><keyname>Kumar</keyname><forenames>Santhosh</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author></authors><title>Reed-Muller Codes Achieve Capacity on Erasure Channels</title><categories>cs.IT math.IT</categories><comments>(v2) Added Section V (titled 'Discussion') and a detailed discussion
  of primitive narrow-sense BCH codes (Section IV-C)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new approach to proving that a sequence of
deterministic linear codes achieves capacity on an erasure channel under
maximum a posteriori decoding. Rather than relying on the precise structure of
the codes, this method requires only that the codes are highly symmetric. In
particular, the technique applies to any sequence of linear codes where the
blocklengths are strictly increasing, the code rates converge to a number
between 0 and 1, and the permutation group of each code is doubly transitive.
This also provides a rare example in information theory where symmetry alone
implies near-optimal performance.
  An important consequence of this result is that a sequence of Reed-Muller
codes with increasing blocklength achieves capacity if its code rate converges
to a number between 0 and 1. This possibility has been suggested previously in
the literature but it has only been proven for cases where the limiting code
rate is 0 or 1. Moreover, these results extend naturally to affine-invariant
codes and, thus, to all extended primitive narrow-sense BCH codes. The primary
tools used in the proof are the sharp threshold property for monotone boolean
functions and the area theorem for extrinsic information transfer functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05124</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05124</id><created>2015-05-19</created><authors><author><keyname>Ahmad</keyname><forenames>Imad</forenames></author><author><keyname>Wang</keyname><forenames>Chih-Chun</forenames></author></authors><title>When Locally Repairable Codes Meet Regenerating Codes --- What If Some
  Helpers Are Unavailable</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, to appear in IEEE ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locally repairable codes (LRCs) are ingeniously designed distributed storage
codes with a (usually small) bounded number of helper nodes participating in
repair. Since most existing LRCs assume exact repair and allow full exchange of
the stored data ($\beta=\alpha$), they can be viewed as a generalization of the
traditional erasure codes (ECs) with a much desired feature of local repair.
However, it also means that they lack the features of functional repair and
partial information-exchange ($\beta&lt;\alpha$) in the original regenerating
codes (RCs). Motivated by the significant bandwidth (BW) reduction of RCs over
ECs, existing works by Ahmad et al and by Hollmann studied &quot;locally repairable
regenerating codes (LRRCs)&quot; that simultaneously admit all three features: local
repair, partial information-exchange, and functional repair. Significant BW
reduction was observed.
  One important issue for any local repair schemes (including both LRCs and
LRRCs) is that sometimes designated helper nodes may be temporarily
unavailable, the result of multiple failures, degraded reads, or other network
dynamics. Under the setting of LRRCs with temporary node unavailability, this
work studies the impact of different helper selection methods. It proves, for
the first time in the literature, that with node unavailability, all existing
methods of helper selection, including those used in RCs and LRCs, are strictly
repair-BW suboptimal. For some scenarios, it is necessary to combine LRRCs with
a new helper selection method, termed dynamic helper selection, to achieve
optimal BW. This work also compares the performance of different helper
selection methods and answers the following fundamental question: whether one
method of helper selection is intrinsically better than the other? for various
different scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05135</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05135</id><created>2015-04-27</created><authors><author><keyname>Portnoi</keyname><forenames>Marcos</forenames></author><author><keyname>de Ara&#xfa;jo</keyname><forenames>Rafael Gon&#xe7;alves Bezerra</forenames></author></authors><title>Network Simulator - Vis\~ao Geral da Ferramenta de Simula\c{c}\~ao de
  Redes</title><categories>cs.PF</categories><comments>in Portuguese, Semin\'ario Estudantil de Produ\c{c}\~ao Acad\^emica,
  2002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes NS - Network Simulator, the computer networks simulation
tool. We offer an overview NS, and also analyze its characteristics and
functions. Finally, we present in detail all steps for preparing a simulation
of a simple model in NS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05136</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05136</id><created>2015-04-27</created><authors><author><keyname>Turenne</keyname><forenames>Nicolas</forenames></author></authors><title>A Table-Binning Approach for Visualizing the Past</title><categories>cs.IR cs.DB cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large amounts of data are available due to low-cost and high-capacity data
storage equipments. We propose a data exploration/visualization method for
tabular multi-dimensional, time-varying datasets to present selected items in
their global context. The approach is simple and uses a rank-based
visualization and a pattern matching functionality based on temporal profiles.
Ranking categories can be specified in a flexible way and are used instead of
actual values (value reduction into bins) and plotting it over time in an
unevenly quantized representation. Patterns that emerge are matched against a
set of eight predefined temporal profiles. The graphical summarization of
large-scale temporal data is proposed and applicability is tested qualitatively
on about eight data sets and the approach is compared to classic line plots and
SAX representation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05163</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05163</id><created>2015-05-19</created><updated>2015-11-19</updated><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author><author><keyname>de Oliveira</keyname><forenames>Paulo Murilo Castro</forenames></author></authors><title>Inflexibility and independence: Phase transitions in the majority-rule
  model</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>21 pages, 8 figures, to appear in Physical Review E</comments><journal-ref>Phys. Rev. E 92, 062122 (2015)</journal-ref><doi>10.1103/PhysRevE.92.062122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study opinion formation in a population participating of a
public debate with two distinct choices. We considered three distinct
mechanisms of social interactions and individuals' behavior: conformity,
nonconformity and inflexibility. The conformity is ruled by the majority-rule
dynamics, whereas the nonconformity is introduced in the population as an
independent behavior, implying the failure to attempted group influence.
Finally, the inflexible agents are introduced in the population with a given
density. These individuals present a singular behavior, in a way that their
stubbornness makes them reluctant to change their opinions. We consider these
effects separately and all together, with the aim to analyze the critical
behavior of the system. We performed numerical simulations in some lattice
structures and for distinct population sizes, and our results suggest that the
different formulations of the model undergo order-disorder phase transitions in
the same universality class of the Ising model. Some of our results are
complemented by analytical calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05173</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05173</id><created>2015-05-19</created><updated>2015-12-26</updated><authors><author><keyname>Nithyanand</keyname><forenames>Rishab</forenames></author><author><keyname>Starov</keyname><forenames>Oleksii</forenames></author><author><keyname>Zair</keyname><forenames>Adva</forenames></author><author><keyname>Gill</keyname><forenames>Phillipa</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author></authors><title>Measuring and mitigating AS-level adversaries against Tor</title><categories>cs.CR</categories><comments>Appearing at NDSS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popularity of Tor as an anonymity system has made it a popular target for
a variety of attacks. We focus on traffic correlation attacks, which are no
longer solely in the realm of academic research with recent revelations about
the NSA and GCHQ actively working to implement them in practice.
  Our first contribution is an empirical study that allows us to gain a high
fidelity snapshot of the threat of traffic correlation attacks in the wild. We
find that up to 40% of all circuits created by Tor are vulnerable to attacks by
traffic correlation from Autonomous System (AS)-level adversaries, 42% from
colluding AS-level adversaries, and 85% from state-level adversaries. In
addition, we find that in some regions (notably, China and Iran) there exist
many cases where over 95% of all possible circuits are vulnerable to
correlation attacks, emphasizing the need for AS-aware relay-selection.
  To mitigate the threat of such attacks, we build Astoria--an AS-aware Tor
client. Astoria leverages recent developments in network measurement to perform
path-prediction and intelligent relay selection. Astoria reduces the number of
vulnerable circuits to 2% against AS-level adversaries, under 5% against
colluding AS-level adversaries, and 25% against state-level adversaries. In
addition, Astoria load balances across the Tor network so as to not overload
any set of relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05175</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05175</id><created>2015-05-19</created><authors><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author><author><keyname>Stojanac</keyname><forenames>&#x17d;eljka</forenames></author></authors><title>Tensor theta norms and low rank recovery</title><categories>cs.IT math.IT</categories><comments>28 pages</comments><msc-class>13P10, 15A69, 15A60, 52A41, 90C22, 90C25, 94A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study extensions of compressive sensing and low rank matrix recovery to
the recovery of tensors of low rank from incomplete linear information. While
the reconstruction of low rank matrices via nuclear norm minimization is rather
well-understand by now, almost no theory is available so far for the extension
to higher order tensors due to various theoretical and computational
difficulties arising for tensor decompositions. In fact, nuclear norm
minimization for matrix recovery is a tractable convex relaxation approach, but
the extension of the nuclear norm to tensors is NP-hard to compute. In this
article, we introduce convex relaxations of the tensor nuclear norm which are
computable in polynomial time via semidefinite programming. Our approach is
based on theta bodies, a concept from computational algebraic geometry which is
similar to the one of the better known Lasserre relaxations. We introduce
polynomial ideals which are generated by the second order minors corresponding
to different matricizations of the tensor (where the tensor entries are treated
as variables) such that the nuclear norm ball is the convex hull of the
algebraic variety of the ideal. The theta body of order $k$ for such an ideal
generates a new norm which we call the $\theta_k$-norm. We show that in the
matrix case, these norms reduce to the standard nuclear norm. For tensors of
order three or higher however, we indeed obtain new norms. By providing the
Gr{\&quot;o}bner basis for the ideals, we explicitly give semidefinite programs for
the computation of the $\theta_k$-norm and for the minimization of the
$\theta_k$-norm under an affine constraint. Finally, numerical experiments for
order three tensor recovery via $\theta_1$-norm minimization suggest that our
approach successfully reconstructs tensors of low rank from incomplete linear
(random) measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05186</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05186</id><created>2015-05-18</created><authors><author><keyname>Diamant</keyname><forenames>Emanuel</forenames></author></authors><title>Advances in Computational Biology: A Real Boost or a Wishful Thinking</title><categories>cs.OH</categories><comments>The paper was submitted to the ICCS-2015 conference (Reykjavik,
  Iceland, June 1-3, 2015) and was rejected as irrelevant to the conference
  topic. arXiv admin note: text overlap with arXiv:1411.0054, arXiv:1502.04791,
  arXiv:1505.04785</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational biology is on the verge of a paradigm shift in its research
practice - from a data-based (computational) paradigm to an information-based
(cognitive) paradigm. As in the other research fields, this transition is
impeded by lack of a right understanding about what is actually hidden behind
the term &quot;information&quot;. The paper is intended to clarify this issue and
introduces two new notions of &quot;physical information&quot; and &quot;semantic
information&quot;, which together constitute the term &quot;information&quot;. Some
implications of this introduction are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05187</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05187</id><created>2015-05-19</created><authors><author><keyname>Prakash</keyname><forenames>Abhay</forenames></author><author><keyname>Patel</keyname><forenames>Dhaval</forenames></author></authors><title>Techniques for Deep Query Understanding</title><categories>cs.IR</categories><comments>30 pages, 18 figures, 2 tables, student report</comments><acm-class>H.3.3; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query Understanding concerns about inferring the precise intent of search by
the user with his formulated query, which is challenging because the queries
are often very short and ambiguous. The report discusses the various kind of
queries that can be put to a Search Engine and illustrates the Role of Query
Understanding for return of relevant results. With different advances in
techniques for deep understanding of queries as well as documents, the Search
Technology has witnessed three major era. A lot of interesting real world
examples have been used to illustrate the role of Query Understanding in each
of them. The Query Understanding Module is responsible to correct the mistakes
done by user in the query, to guide him in formulation of query with precise
intent, and to precisely infer the intent of the user query. The report
describes the complete architecture to handle aforementioned three tasks, and
then discusses basic as well as recent advanced techniques for each of the
component, through appropriate papers from reputed conferences and journals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05190</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05190</id><created>2015-05-19</created><authors><author><keyname>Kato</keyname><forenames>Hiroharu</forenames></author><author><keyname>Harada</keyname><forenames>Tatsuya</forenames></author></authors><title>Image Reconstruction from Bag-of-Visual-Words</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this work is to reconstruct an original image from
Bag-of-Visual-Words (BoVW). Image reconstruction from features can be a means
of identifying the characteristics of features. Additionally, it enables us to
generate novel images via features. Although BoVW is the de facto standard
feature for image recognition and retrieval, successful image reconstruction
from BoVW has not been reported yet. What complicates this task is that BoVW
lacks the spatial information for including visual words. As described in this
paper, to estimate an original arrangement, we propose an evaluation function
that incorporates the naturalness of local adjacency and the global position,
with a method to obtain related parameters using an external image database. To
evaluate the performance of our method, we reconstruct images of objects of 101
kinds. Additionally, we apply our method to analyze object classifiers and to
generate novel images via BoVW.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05192</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05192</id><created>2015-05-19</created><updated>2016-01-16</updated><authors><author><keyname>Doersch</keyname><forenames>Carl</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author><author><keyname>Efros</keyname><forenames>Alexei A.</forenames></author></authors><title>Unsupervised Visual Representation Learning by Context Prediction</title><categories>cs.CV</categories><comments>Oral paper at ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work explores the use of spatial context as a source of free and
plentiful supervisory signal for training a rich visual representation. Given
only a large, unlabeled image collection, we extract random pairs of patches
from each image and train a convolutional neural net to predict the position of
the second patch relative to the first. We argue that doing well on this task
requires the model to learn to recognize objects and their parts. We
demonstrate that the feature representation learned using this within-image
context indeed captures visual similarity across images. For example, this
representation allows us to perform unsupervised visual discovery of objects
like cats, people, and even birds from the Pascal VOC 2011 detection dataset.
Furthermore, we show that the learned ConvNet can be used in the R-CNN
framework and provides a significant boost over a randomly-initialized ConvNet,
resulting in state-of-the-art performance among algorithms which use only
Pascal-provided training set annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05193</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05193</id><created>2015-05-19</created><authors><author><keyname>Fisher</keyname><forenames>Jasmin</forenames></author><author><keyname>K&#xf6;ksal</keyname><forenames>Ali Sinan</forenames></author><author><keyname>Piterman</keyname><forenames>Nir</forenames></author><author><keyname>Woodhouse</keyname><forenames>Steven</forenames></author></authors><title>Synthesising Executable Gene Regulatory Networks from Single-cell Gene
  Expression Data</title><categories>cs.CE cs.LO q-bio.MN</categories><comments>Final published version to appear in Computer Aided Verification
  (CAV), Springer, July 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent experimental advances in biology allow researchers to obtain gene
expression profiles at single-cell resolution over hundreds, or even thousands
of cells at once. These single-cell measurements provide snapshots of the
states of the cells that make up a tissue, instead of the population-level
averages provided by conventional high-throughput experiments. This new data
therefore provides an exciting opportunity for computational modelling. In this
paper we introduce the idea of viewing single-cell gene expression profiles as
states of an asynchronous Boolean network, and frame model inference as the
problem of reconstructing a Boolean network from its state space. We then give
a scalable algorithm to solve this synthesis problem. We apply our technique to
both simulated and real data. We first apply our technique to data simulated
from a well established model of common myeloid progenitor differentiation. We
show that our technique is able to recover the original Boolean network rules.
We then apply our technique to a large dataset taken during embryonic
development containing thousands of cell measurements. Our technique
synthesises matching Boolean networks, and analysis of these models yields new
predictions about blood development which our experimental collaborators were
able to verify.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05200</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05200</id><created>2015-05-19</created><updated>2015-09-09</updated><authors><author><keyname>Coregliano</keyname><forenames>Leonardo N.</forenames></author><author><keyname>Parente</keyname><forenames>Roberto F.</forenames></author><author><keyname>Sato</keyname><forenames>Cristiane M.</forenames></author></authors><title>On the maximum density of fixed strongly connected subtournaments</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the density of fixed strongly connected subtournaments on 5 vertices
in large tournaments. We determine the maximum density asymptotically for five
tournaments as well as unique extremal sequences for each tournament. As a
byproduct we also characterize tournaments that are recursive blow-ups of a
3-cycle as tournaments that avoid three specific tournaments of size 5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05208</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05208</id><created>2015-05-19</created><authors><author><keyname>Patel</keyname><forenames>Raajen</forenames></author><author><keyname>Goldstein</keyname><forenames>Thomas A.</forenames></author><author><keyname>Dyer</keyname><forenames>Eva L.</forenames></author><author><keyname>Mirhoseini</keyname><forenames>Azalia</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>oASIS: Adaptive Column Sampling for Kernel Matrix Approximation</title><categories>stat.ML cs.LG</categories><acm-class>G.1.0; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel matrices (e.g. Gram or similarity matrices) are essential for many
state-of-the-art approaches to classification, clustering, and dimensionality
reduction. For large datasets, the cost of forming and factoring such kernel
matrices becomes intractable. To address this challenge, we introduce a new
adaptive sampling algorithm called Accelerated Sequential Incoherence Selection
(oASIS) that samples columns without explicitly computing the entire kernel
matrix. We provide conditions under which oASIS is guaranteed to exactly
recover the kernel matrix with an optimal number of columns selected. Numerical
experiments on both synthetic and real-world datasets demonstrate that oASIS
achieves performance comparable to state-of-the-art adaptive sampling methods
at a fraction of the computational cost. The low runtime complexity of oASIS
and its low memory footprint enable the solution of large problems that are
simply intractable using other adaptive methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05211</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05211</id><created>2015-05-19</created><authors><author><keyname>Bhattacherjee</keyname><forenames>Souvik</forenames></author><author><keyname>Chavan</keyname><forenames>Amit</forenames></author><author><keyname>Huang</keyname><forenames>Silu</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author></authors><title>Principles of Dataset Versioning: Exploring the Recreation/Storage
  Tradeoff</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relative ease of collaborative data science and analysis has led to a
proliferation of many thousands or millions of $versions$ of the same datasets
in many scientific and commercial domains, acquired or constructed at various
stages of data analysis across many users, and often over long periods of time.
Managing, storing, and recreating these dataset versions is a non-trivial task.
The fundamental challenge here is the $storage-recreation\;trade-off$: the more
storage we use, the faster it is to recreate or retrieve versions, while the
less storage we use, the slower it is to recreate or retrieve versions. Despite
the fundamental nature of this problem, there has been a surprisingly little
amount of work on it. In this paper, we study this trade-off in a principled
manner: we formulate six problems under various settings, trading off these
quantities in various ways, demonstrate that most of the problems are
intractable, and propose a suite of inexpensive heuristics drawing from
techniques in delay-constrained scheduling, and spanning tree literature, to
solve these problems. We have built a prototype version management system, that
aims to serve as a foundation to our DATAHUB system for facilitating
collaborative data science. We demonstrate, via extensive experiments, that our
proposed heuristics provide efficient solutions in practical dataset versioning
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05212</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05212</id><created>2015-05-19</created><authors><author><keyname>Tizhoosh</keyname><forenames>Hamid R.</forenames></author></authors><title>Barcode Annotations for Medical Image Retrieval: A Preliminary
  Investigation</title><categories>cs.CV</categories><comments>To be published in proceedings of The IEEE International Conference
  on Image Processing (ICIP 2015), September 27-30, 2015, Quebec City, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to generate and to use barcodes to annotate medical
images and/or their regions of interest such as organs, tumors and tissue
types. A multitude of efficient feature-based image retrieval methods already
exist that can assign a query image to a certain image class. Visual
annotations may help to increase the retrieval accuracy if combined with
existing feature-based classification paradigms. Whereas with annotations we
usually mean textual descriptions, in this paper barcode annotations are
proposed. In particular, Radon barcodes (RBC) are introduced. As well, local
binary patterns (LBP) and local Radon binary patterns (LRBP) are implemented as
barcodes. The IRMA x-ray dataset with 12,677 training images and 1,733 test
images is used to verify how barcodes could facilitate image retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05215</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05215</id><created>2015-05-19</created><authors><author><keyname>Hanneke</keyname><forenames>Steve</forenames></author><author><keyname>Kanade</keyname><forenames>Varun</forenames></author><author><keyname>Yang</keyname><forenames>Liu</forenames></author></authors><title>Learning with a Drifting Target Concept</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning in the presence of a drifting target
concept. Specifically, we provide bounds on the error rate at a given time,
given a learner with access to a history of independent samples labeled
according to a target concept that can change on each round. One of our main
contributions is a refinement of the best previous results for polynomial-time
algorithms for the space of linear separators under a uniform distribution. We
also provide general results for an algorithm capable of adapting to a variable
rate of drift of the target concept. Some of the results also describe an
active learning variant of this setting, and provide bounds on the number of
queries for the labels of points in the sequence sufficient to obtain the
stated bounds on the error rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05216</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05216</id><created>2015-05-19</created><authors><author><keyname>Heydari</keyname><forenames>Ali</forenames></author></authors><title>Convergence Analysis of Policy Iteration</title><categories>cs.SY math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive optimal control of nonlinear dynamic systems with deterministic and
known dynamics under a known undiscounted infinite-horizon cost function is
investigated. Policy iteration scheme initiated using a stabilizing initial
control is analyzed in solving the problem. The convergence of the iterations
and the optimality of the limit functions, which follows from the established
uniqueness of the solution to the Bellman equation, are the main results of
this study. Furthermore, a theoretical comparison between the speed of
convergence of policy iteration versus value iteration is presented. Finally,
the convergence results are extended to the case of multi-step look-ahead
policy iteration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05220</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05220</id><created>2015-05-19</created><updated>2015-08-05</updated><authors><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author><author><keyname>Szybowski</keyname><forenames>J.</forenames></author></authors><title>The key properties of inconsistency indicators for a triad in pairwise
  comparison matrices</title><categories>cs.DM</categories><comments>8 pages, 14 references; creative use of the distance to define
  inconsistency in pairwise comparisons (project originated in 2014 but
  submitted for a review in July 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Processing information, acquired by subjective assessments, involves
inconsistency analysis in most (if not all) applications of which some are of
considerable importance at a national level (see, Koczkodaj/Kulakowski/Ligenza,
Scientometrics, 99(3): 911-926, 2014)A triad inconsistency axiomatization in
pairwise comparisons was informally proposed in Koczkodaj/Szwarc, FUNDAMENTA
INFORMATICAE, 132(4): 485-500, 2014. This study, rectifies it by the use of the
distance and theoretical proofs. Three key properties of the indicator are
presented in this study and illustrated by several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05225</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05225</id><created>2015-05-19</created><authors><author><keyname>Lihua</keyname><forenames>Guo</forenames></author><author><keyname>Fudi</keyname><forenames>Li</forenames></author></authors><title>Image aesthetic evaluation using paralleled deep convolution neural
  network</title><categories>cs.CV cs.MM</categories><comments>7 pages, 6 figures, 9 tables</comments><msc-class>68U10</msc-class><acm-class>I.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image aesthetic evaluation has attracted much attention in recent years.
Image aesthetic evaluation methods heavily depend on the effective aesthetic
feature. Traditional meth-ods always extract hand-crafted features. However,
these hand-crafted features are always designed to adapt particu-lar datasets,
and extraction of them needs special design. Rather than extracting
hand-crafted features, an automati-cally learn of aesthetic features based on
deep convolutional neural network (DCNN) is first adopt in this paper. As we
all know, when the training dataset is given, the DCNN architecture with high
complexity may meet the over-fitting problem. On the other side, the DCNN
architecture with low complexity would not efficiently extract effective
features. For these reasons, we further propose a paralleled convolutional
neural network (PDCNN) with multi-level structures to automatically adapt to
the training dataset. Experimental results show that our proposed PDCNN
architecture achieves better performance than other traditional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05226</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05226</id><created>2015-05-19</created><updated>2015-05-20</updated><authors><author><keyname>Ziad</keyname><forenames>M. Tarek Ibn</forenames></author><author><keyname>Alanwar</keyname><forenames>Amr</forenames></author><author><keyname>Alkabani</keyname><forenames>Yousra</forenames></author><author><keyname>El-Kharashi</keyname><forenames>M. Watheq</forenames></author><author><keyname>Bedour</keyname><forenames>Hassan</forenames></author></authors><title>Homomorphic Data Isolation for Hardware Trojan Protection</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interest in homomorphic encryption/decryption is increasing due to its
excellent security properties and operating facilities. It allows operating on
data without revealing its content. In this work, we suggest using homomorphism
for Hardware Trojan protection. We implement two partial homomorphic designs
based on ElGamal encryption/decryption scheme. The first design is a
multiplicative homomorphic, whereas the second one is an additive homomorphic.
We implement the proposed designs on a low-cost Xilinx Spartan-6 FPGA. Area
utilization, delay, and power consumption are reported for both designs.
Furthermore, we introduce a dual-circuit design that combines the two earlier
designs using resource sharing in order to have minimum area cost. Experimental
results show that our dual-circuit design saves 35% of the logic resources
compared to a regular design without resource sharing. The saving in power
consumption is 20%, whereas the number of cycles needed remains almost the same
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05231</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05231</id><created>2015-05-19</created><authors><author><keyname>Yang</keyname><forenames>Liu</forenames></author><author><keyname>Hanneke</keyname><forenames>Steve</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author></authors><title>Bounds on the Minimax Rate for Estimating a Prior over a VC Class from
  Independent Learning Tasks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimal rates of convergence for estimating a prior distribution
over a VC class from a sequence of independent data sets respectively labeled
by independent target functions sampled from the prior. We specifically derive
upper and lower bounds on the optimal rates under a smoothness condition on the
correct prior, with the number of samples per data set equal the VC dimension.
These results have implications for the improvements achievable via transfer
learning. We additionally extend this setting to real-valued function, where we
establish consistency of an estimator for the prior, and discuss an additional
application to a preference elicitation problem in algorithmic economics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05232</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05232</id><created>2015-05-19</created><authors><author><keyname>Yang</keyname><forenames>Songfan</forenames></author><author><keyname>Ramanan</keyname><forenames>Deva</forenames></author></authors><title>Multi-scale recognition with DAG-CNNs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore multi-scale convolutional neural nets (CNNs) for image
classification. Contemporary approaches extract features from a single output
layer. By extracting features from multiple layers, one can simultaneously
reason about high, mid, and low-level features during classification. The
resulting multi-scale architecture can itself be seen as a feed-forward model
that is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs to
learn a set of multiscale features that can be effectively shared between
coarse and fine-grained classification tasks. While fine-tuning such models
helps performance, we show that even &quot;off-the-self&quot; multiscale features perform
quite well. We present extensive analysis and demonstrate state-of-the-art
classification performance on three standard scene benchmarks (SUN397, MIT67,
and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets,
our results reduce the lowest previously-reported error by 23.9% and 9.5%,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05233</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05233</id><created>2015-05-19</created><updated>2015-09-09</updated><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>Visual Understanding via Multi-Feature Shared Learning with Global
  Consistency</title><categories>cs.CV cs.LG</categories><comments>13 pages,6 figures, this paper is accepted for publication in IEEE
  Transactions on Multimedia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image/video data is usually represented with multiple visual features. Fusion
of multi-source information for establishing the attributes has been widely
recognized. Multi-feature visual recognition has recently received much
attention in multimedia applications. This paper studies visual understanding
via a newly proposed l_2-norm based multi-feature shared learning framework,
which can simultaneously learn a global label matrix and multiple
sub-classifiers with the labeled multi-feature data. Additionally, a group
graph manifold regularizer composed of the Laplacian and Hessian graph is
proposed for better preserving the manifold structure of each feature, such
that the label prediction power is much improved through the semi-supervised
learning with global label consistency. For convenience, we call the proposed
approach Global-Label-Consistent Classifier (GLCC). The merits of the proposed
method include: 1) the manifold structure information of each feature is
exploited in learning, resulting in a more faithful classification owing to the
global label consistency; 2) a group graph manifold regularizer based on the
Laplacian and Hessian regularization is constructed; 3) an efficient
alternative optimization method is introduced as a fast solver owing to the
convex sub-problems. Experiments on several benchmark visual datasets for
multimedia understanding, such as the 17-category Oxford Flower dataset, the
challenging 101-category Caltech dataset, the YouTube &amp; Consumer Videos dataset
and the large-scale NUS-WIDE dataset, demonstrate that the proposed approach
compares favorably with the state-of-the-art algorithms. An extensive
experiment on the deep convolutional activation features also show the
effectiveness of the proposed approach. The code is available on
http://www.escience.cn/people/lei/index.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05240</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05240</id><created>2015-05-20</created><authors><author><keyname>Srivastava</keyname><forenames>Siddharth</forenames></author><author><keyname>Mukherjee</keyname><forenames>Prerana</forenames></author><author><keyname>Lall</keyname><forenames>Brejesh</forenames></author></authors><title>Benchmarking KAZE and MCM for Multiclass Classification</title><categories>cs.CV cs.IR</categories><report-no>v01.0</report-no><acm-class>I.4.7; I.5.4; I.4.8; I.4.9; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel approach for feature generation by
appropriately fusing KAZE and SIFT features. We then use this feature set along
with Minimal Complexity Machine(MCM) for object classification. We show that
KAZE and SIFT features are complementary. Experimental results indicate that an
elementary integration of these techniques can outperform the state-of-the-art
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05241</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05241</id><created>2015-05-20</created><authors><author><keyname>Bates</keyname><forenames>Daniel J.</forenames></author><author><keyname>Hauenstein</keyname><forenames>Jonathan D.</forenames></author><author><keyname>Niemerg</keyname><forenames>Matthew E.</forenames></author><author><keyname>Sottile</keyname><forenames>Frank</forenames></author></authors><title>Software for the Gale transform of fewnomial systems and a Descartes
  rule for fewnomials</title><categories>math.AG cs.MS math.NA</categories><comments>22 pages, 4 figures</comments><msc-class>14P99, 65H10, 65H20</msc-class><acm-class>G.1.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a Descartes'-like bound on the number of positive solutions to a
system of fewnomials that holds when its exponent vectors are not in convex
position and a sign condition is satisfied. This was discovered while
developing algorithms and software for computing the Gale transform of a
fewnomial system, which is our main goal. This software is a component of a
package we are developing for Khovanskii-Rolle continuation, which is a
numerical algorithm to compute the real solutions to a system of fewnomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05251</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05251</id><created>2015-05-20</created><updated>2015-08-06</updated><authors><author><keyname>Moulick</keyname><forenames>Subhayan Roy</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K.</forenames></author></authors><title>Signing Perfect Currency Bonds</title><categories>quant-ph cs.CR</categories><comments>6 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose the idea of a Quantum Cheque Scheme, a cryptographic protocol in
which any legitimate client of a trusted bank can issue a cheque, that cannot
be counterfeited or altered in anyway, and can be verified by a bank or any of
its branches. We formally define a Quantum Cheque and present the first
Unconditionally Secure Quantum Cheque Scheme and show it to be secure against
any no-signaling adversary. The proposed Quantum Cheque Scheme can been
perceived as the quantum analog of Electronic Data Interchange, as an alternate
for current e-Payment Gateways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05253</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05253</id><created>2015-05-20</created><updated>2015-09-09</updated><authors><author><keyname>Feng</keyname><forenames>Jun</forenames></author><author><keyname>Zhou</keyname><forenames>Mantong</forenames></author><author><keyname>Hao</keyname><forenames>Yu</forenames></author><author><keyname>Huang</keyname><forenames>Minlie</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoyan</forenames></author></authors><title>Knowlege Graph Embedding by Flexible Translation</title><categories>cs.CL</categories><comments>This paper has been withdraw by the author due to an error in sec3.1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge graph embedding refers to projecting entities and relations in
knowledge graph into continuous vector spaces. State-of-the-art methods, such
as TransE, TransH, and TransR build embeddings by treating relation as
translation from head entity to tail entity. However, previous models can not
deal with reflexive/one-to-many/many-to-one/many-to-many relations properly, or
lack of scalability and efficiency. Thus, we propose a novel method, flexible
translation, named TransF, to address the above issues. TransF regards relation
as translation between head entity vector and tail entity vector with flexible
magnitude. To evaluate the proposed model, we conduct link prediction and
triple classification on benchmark datasets. Experimental results show that our
method remarkably improve the performance compared with several
state-of-the-art baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05254</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05254</id><created>2015-05-20</created><authors><author><keyname>Hoshen</keyname><forenames>Yedid</forenames></author><author><keyname>Peleg</keyname><forenames>Shmuel</forenames></author></authors><title>Live Video Synopsis for Multiple Cameras</title><categories>cs.CV</categories><comments>To be presented in ICIP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video surveillance cameras generate most of recorded video, and there is far
more recorded video than operators can watch. Much progress has recently been
made using summarization of recorded video, but such techniques do not have
much impact on live video surveillance.
  We assume a camera hierarchy where a Master camera observes the
decision-critical region, and one or more Slave cameras observe regions where
past activity is important for making the current decision. We propose that
when people appear in the live Master camera, the Slave cameras will display
their past activities, and the operator could use past information for
real-time decision making.
  The basic units of our method are action tubes, representing objects and
their trajectories over time. Our object-based method has advantages over frame
based methods, as it can handle multiple people, multiple activities for each
person, and can address re-identification uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05259</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05259</id><created>2015-05-20</created><updated>2016-01-25</updated><authors><author><keyname>Posch</keyname><forenames>Daniel</forenames></author><author><keyname>Rainer</keyname><forenames>Benjamin</forenames></author><author><keyname>Hellwagner</keyname><forenames>Hermann</forenames></author></authors><title>SAF: Stochastic Adaptive Forwarding in Named Data Networking</title><categories>cs.NI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forwarding decisions in classical IP-based networks are predetermined by
routing. This is necessary to avoid loops, inhibiting opportunities to
implement an adaptive and intelligent forwarding plane. Consequently, content
distribution efficiency is reduced due to a lack of inherent multi-path
transmission. In Named Data Networking (NDN) instead, routing shall hold a
supporting role to forwarding, providing sufficient potential to enhance
content dissemination at the forwarding plane. In this paper we design,
implement, and evaluate a novel probability-based forwarding strategy, called
Stochastic Adaptive Forwarding (SAF) for NDN. SAF imitates a self-adjusting
water pipe system, intelligently guiding and distributing Interests through
network crossings circumventing link failures and bottlenecks. Just as real
pipe systems, SAF employs overpressure valves enabling congested nodes to lower
pressure autonomously. Through an implicit feedback mechanism it is ensured
that the traffic fraction forwarded via congested nodes decreases. By
simulations we show that our approach outperforms existing forwarding
strategies in terms of the Interest satisfaction ratio in the majority of the
evaluated scenarios. This is achieved by extensive utilization of NDN's
multipath and content-lookup capabilities without relying on the routing plane.
SAF explores the local environment by redirecting requests that are likely to
be dropped anyway. This enables SAF to identify new paths to the content origin
or to cached replicas, circumventing link failures and resource shortages
without relying on routing updates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05265</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05265</id><created>2015-05-20</created><authors><author><keyname>Corrodi</keyname><forenames>Claudio</forenames></author></authors><title>Modelling and Verifying an Object-Oriented Concurrency Model in GROOVE</title><categories>cs.PL cs.LO cs.SE</categories><comments>124 pages, Master's Thesis at ETH Z\&quot;urich</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SCOOP is a programming model and language that allows concurrent programming
at a high level of abstraction. Several approaches to verifying SCOOP programs
have been proposed in the past, but none of them operate directly on the source
code without modifications or annotations.
  We propose a fully automatic approach to verifying (a subset of) SCOOP
programs by translation to graph-based models. First, we present a graph
transformation based semantics for SCOOP. We present an implementation of the
model in the state-of-the-art model checker GROOVE, which can be used to
simulate programs and verify concurrency and consistency properties, such as
the impossibility of deadlocks occurring or the absence of postcondition
violations. Second, we present a translation tool that operates on SCOOP
program code and generates input for the model. We evaluate our approach by
inspecting a number of programs in the form of case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05269</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05269</id><created>2015-05-20</created><authors><author><keyname>Ranjan</keyname><forenames>Alok</forenames></author><author><keyname>Sahu</keyname><forenames>H. B.</forenames></author><author><keyname>Misra</keyname><forenames>Prasant</forenames></author></authors><title>A Survey Report on Operating Systems for Tiny Networked Sensors</title><categories>cs.OS</categories><comments>12 pages, Submitted to Journal</comments><journal-ref>Journal of Advanced Research in Networking and Communication
  Engineering, Vol(1) issue 1, 2014</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Wireless sensor network (WSN) has attracted researchers worldwide to explore
the research opportunities, with application mainly in health monitoring,
industry automation, battlefields, home automation and environmental
monitoring. A WSN is highly resource constrained in terms of energy,
computation and memory. WSNs deployment ranges from the normal working
environment up to hostile and hazardous environment such as in volcano
monitoring and underground mines. These characteristics of WSNs hold additional
set of challenges in front of the operating system designer. The objective of
this survey is to highlight the features and weakness of the opearting system
available for WSNs, with the focus on the current application demands. The
paper also discusses the operating system design issues in terms of
architecture, programming model, scheduling and memory management and support
for real time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05277</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05277</id><created>2015-05-20</created><authors><author><keyname>Gherekhloo</keyname><forenames>Soheyl</forenames></author><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Cooperation for interference management: A GDoF perspective</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impact of cooperation on interference management is investigated by
studying an elemental wireless network, the so called symmetric interference
relay channel (IRC), from a generalized degrees of freedom (GDoF) perspective.
This is motivated by the fact that the deployment of relays is considered as a
remedy to overcome the bottleneck of current systems in terms of achievable
rates. The focus of this work is on the regime in which the interference link
is weaker than the source-relay link in the IRC. Our approach towards studying
the GDoF goes through the capacity analysis of the linear deterministic IRC
(LD-IRC). New upper bounds on the sum-capacity of the LD-IRC based on
genie-aided approaches are established. These upper bounds together with some
existing upper bounds are achieved by using four novel transmission schemes.
Extending the upper bounds and the transmission schemes to the Gaussian case,
the GDoF of the Gaussian IRC is characterized for the aforementioned regime.
This completes the GDoF results available in the literature for the symmetric
GDoF. It is shown that in the strong interference regime, in contrast to the
IC, the GDoF is not a monotonically increasing function of the interference
level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05286</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05286</id><created>2015-05-20</created><authors><author><keyname>Andreu</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Mayer</keyname><forenames>Stefan</forenames></author><author><keyname>Gutjahr</keyname><forenames>Karlheinz</forenames></author><author><keyname>Ganster</keyname><forenames>Harald</forenames></author></authors><title>Measuring Visibility using Atmospheric Transmission and Digital Surface
  Model</title><categories>cs.CV</categories><comments>Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</comments><report-no>OAGM/2015/11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable and exact assessment of visibility is essential for safe air
traffic. In order to overcome the drawbacks of the currently subjective reports
from human observers, we present an approach to automatically derive visibility
measures by means of image processing. It first exploits image based estimation
of the atmospheric transmission describing the portion of the light that is not
scattered by atmospheric phenomena (e.g., haze, fog, smoke) and reaches the
camera. Once the atmospheric transmission is estimated, a 3D representation of
the vicinity (digital surface model: DMS) is used to compute depth measurements
for the haze-free pixels and then derive a global visibility estimation for the
airport. Results on foggy images demonstrate the validity of the proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05288</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05288</id><created>2015-05-20</created><updated>2015-09-14</updated><authors><author><keyname>Toulouse</keyname><forenames>Michel</forenames></author><author><keyname>Minh</keyname><forenames>Bui Quang</forenames></author><author><keyname>Curtis</keyname><forenames>Philip</forenames></author></authors><title>A consensus based network intrusion detection system</title><categories>cs.CR</categories><comments>Presented at THE 5TH INTERNATIONAL CONFERENCE ON IT CONVERGENCE AND
  SECURITY 2015 IN KUALA LUMPUR, MALAYSIA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network intrusion detection is the process of identifying malicious behaviors
that target a network and its resources. Current systems implementing intrusion
detection processes observe traffic at several data collecting points in the
network but analysis is often centralized or partly centralized. These systems
are not scalable and suffer from the single point of failure, i.e. attackers
only need to target the central node to compromise the whole system. This paper
proposes an anomaly-based fully distributed network intrusion detection system
where analysis is run at each data collecting point using a naive Bayes
classifier. Probability values computed by each classifier are shared among
nodes using an iterative average consensus protocol. The final analysis is
performed redundantly and in parallel at the level of each data collecting
point, thus avoiding the single point of failure issue. We run simulations
focusing on DDoS attacks with several network configurations, comparing the
accuracy of our fully distributed system with a hierarchical one. We also
analyze communication costs and convergence speed during consensus phases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05290</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05290</id><created>2015-05-20</created><authors><author><keyname>Wang</keyname><forenames>Suzhen</forenames></author><author><keyname>Han</keyname><forenames>Sheng</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiguo</forenames></author><author><keyname>Wong</keyname><forenames>Wing Shing</forenames></author></authors><title>Sparsest Error Detection via Sparsity Invariant Transformation based
  $\ell_1$ Minimization</title><categories>stat.ME cs.IT math.IT</categories><comments>20 pages, single column. 7 Figures. To be submitted and under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new method, referred to here as the sparsity invariant
transformation based $\ell_1$ minimization, to solve the $\ell_0$ minimization
problem for an over-determined linear system corrupted by additive sparse
errors with arbitrary intensity. Many previous works have shown that $\ell_1$
minimization can be applied to realize sparse error detection in many
over-determined linear systems. However, performance of this approach is
strongly dependent on the structure of the measurement matrix, which limits
application possibility in practical problems. Here, we present a new approach
based on transforming the $\ell_0$ minimization problem by a linear
transformation that keeps sparsest solutions invariant. We call such a property
a sparsity invariant property (SIP), and a linear transformation with SIP is
referred to as a sparsity invariant transformation (SIT). We propose the
SIT-based $\ell_1$ minimization method by using an SIT in conjunction with
$\ell_1$ relaxation on the $\ell_0$ minimization problem. We prove that for any
over-determined linear system, there always exists a specific class of SIT's
that guarantees a solution to the SIT-based $\ell_1$ minimization is a
sparsest-errors solution. Besides, a randomized algorithm based on Monte Carlo
simulation is proposed to search for a feasible SIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05291</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05291</id><created>2015-05-20</created><updated>2015-09-23</updated><authors><author><keyname>Poon</keyname><forenames>Clarice</forenames></author></authors><title>Structure dependent sampling in compressed sensing: theoretical
  guarantees for tight frames</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of the applications of compressed sensing have been based on variable
density sampling, where certain sections of the sampling coefficients are
sampled more densely. Furthermore, it has been observed that these sampling
schemes are dependent not only on sparsity but also on the sparsity structure
of the underlying signal. This paper extends the result of (Adcock, Hansen,
Poon and Roman, arXiv:1302.0561, 2013) to the case where the sparsifying system
forms a tight frame. By dividing the sampling coefficients into levels, our
main result will describe how the amount of subsampling in each level is
determined by the local coherences between the sampling and sparsifying
operators and the localized level sparsities -- the sparsity in each level
under the sparsifying operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05298</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05298</id><created>2015-05-20</created><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author><author><keyname>Downey</keyname><forenames>Rod G.</forenames></author><author><keyname>McInerney</keyname><forenames>Michael</forenames></author></authors><title>Integer Valued Betting strategies and Turing Degrees</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Betting strategies are often expressed formally as martingales. A martingale
is called integer-valued if each bet must be an integer value. Integer-valued
strategies correspond to the fact that in most betting situations, there is a
minimum amount that a player can bet. According to a well known paradigm,
algorithmic randomness can be founded on the notion of betting strategies. A
real X is called integer-valued random if no effective integer-valued
martingale succeeds on X. It turns out that this notion of randomness has
interesting interactions with genericity and the computably enumerable degrees.
We investigate the computational power of the integer-valued random reals in
terms of standard notions from computability theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05310</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05310</id><created>2015-05-20</created><updated>2015-11-04</updated><authors><author><keyname>Hefny</keyname><forenames>Ahmed</forenames></author><author><keyname>Downey</keyname><forenames>Carlton</forenames></author><author><keyname>Gordon</keyname><forenames>Geoffrey</forenames></author></authors><title>Supervised Learning for Dynamical System Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently there has been substantial interest in spectral methods for learning
dynamical systems. These methods are popular since they often offer a good
tradeoff between computational and statistical efficiency. Unfortunately, they
can be difficult to use and extend in practice: e.g., they can make it
difficult to incorporate prior information such as sparsity or structure. To
address this problem, we present a new view of dynamical system learning: we
show how to learn dynamical systems by solving a sequence of ordinary
supervised learning problems, thereby allowing users to incorporate prior
knowledge via standard techniques such as L1 regularization. Many existing
spectral methods are special cases of this new framework, using linear
regression as the supervised learner. We demonstrate the effectiveness of our
framework by showing examples where nonlinear regression or lasso let us learn
better state representations than plain linear regression does; the correctness
of these instances follows directly from our general analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05312</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05312</id><created>2015-05-20</created><updated>2015-10-30</updated><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>A New Oscillating-Error Technique for Classifiers</title><categories>cs.AI</categories><comments>Error margin values were incorrect and some new test results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new method for reducing the error in a classifier. It
uses an error correction update, but includes the very simple rule of either
adding or subtracting the adjustment, based on whether the variable value is
currently larger or smaller than the desired value. The new neuron can take an
input from each variable or column and adjust it by either adding or
subtracting the difference, on a variable by variable basis. While a
traditional neuron would sum the inputs together and then apply a single
function to that total, this new neuron can change the function for each input
variable. This gives added flexibility to the convergence procedure, where
through a series of transpositions, variables that are far away can continue
towards the desired value, whereas variables that are originally much closer
can oscillate from one side to the other. Tests show that the method can
successfully classify some benchmark datasets. It can also work in a batch
mode, with reduced training times and can be used as part of a neural network
architecture. There are also some updates to an earlier wave shape paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05321</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05321</id><created>2015-05-20</created><authors><author><keyname>Munandar</keyname><forenames>Tb. Ai</forenames></author><author><keyname>Winarko</keyname><forenames>Edi</forenames></author></authors><title>Regional Development Classification Model using Decision Tree Approach</title><categories>cs.CY</categories><comments>6 pages</comments><journal-ref>International Journal of Computer Applications Volume 114, No. 8,
  March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regional development classification is one way to look at differences in
levels of development outcomes. Some frequently used methods are the shift
share, Gain index, the Iindex Williamson and Klassen typology. The development
of science in the field of data mining, offers a new way for regional
development data classification. This study discusses how the decision tree is
used to classify the level of development based on indicators of regional gross
domestic product (GDP). GDP Data Central Java and Banten used in this study.
Before the data is entered into the decision tree forming algorithm, both the
provincial GDP data are classified using Klassen typology. Three decision tree
algorithms, namely J48, NBTRee and REPTree tested in this study using
cross-validation evaluation, then selected one of the best performing
algorithms. The results show that the J48 has a better accuracy rate which is
equal to 85.18% compared to the algorithm NBTRee and REPTree. Testing the model
is done to the six districts / municipalities in the province of Banten, and
shows that there are two districts / cities are still at the development of the
status quadrant relatively underdeveloped regions, namely Kota Tangerang and
Kabupaten Tangerang. As for the Central Java Province, Kendal, Magelang,
Pemalang, Rembang, Semarang and Wonosobo are an area with a quadrant of
development also on the status of the region is relatively underdeveloped.
Classification model that has been developed is able to classify the level of
development fast and easy to enter data directly into the decision tree is
formed. This study can be used as an alternative decision support for policy
makers in order to determine the future direction of development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05322</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05322</id><created>2015-05-20</created><authors><author><keyname>SN</keyname><forenames>Azhari</forenames></author><author><keyname>Munandar</keyname><forenames>Tb. Ai</forenames></author></authors><title>Unsupervised Neural Network-Naive Bayes Model for Grouping Data Regional
  Development Results</title><categories>cs.CY</categories><comments>6 pages</comments><journal-ref>International Journal of Computer Applications, Volume 104, No 15,
  October 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determination quadrant development has an important role in order to
determine the achievement of the development of a district, in terms of the
sector's gross regional domestic product (GDP). The process of determining the
quadrant development typically uses Klassen rules based on its sector GDP. This
study aims to provide a new approach in the conduct of regional development
quadrant clustering using cluster techniques. Clustering is performed based on
the average value of the growth and development of a district contribution
compared with the average value and contribution of the development of the
province based on data in comparison with a year of data to be compared.
Testing models of clustering, performed on a dataset of two provinces, namely
Banten (as a data testing) and Central Java (as the training data), to see the
accuracy of the classification model proposed. The proposed model consists of
two learning methods in it, namely unsupervised (Self Organizing Map / SOM-NN)
method and supervised (Naive Bayess). SOM-NN method is used as a learning
engine to generate training data for the target Class that will be used in the
machine learning Naive Bayess. The results showed the clustering accuracy rate
of the model was 98.1%, while the clustering accuracy rate of the model results
compared to manual analysis shows the accuracy of the typology Klassen smaller,
ie 29.63%. On one side, clustering results of the proposed model is influenced
by the number and keagaraman data sets used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05334</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05334</id><created>2015-05-20</created><authors><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author><author><keyname>Herman</keyname><forenames>M. W.</forenames></author><author><keyname>Orlowski</keyname><forenames>M.</forenames></author></authors><title>Managing Null Entries in Pairwise Comparisons</title><categories>cs.OH</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how to manage null entries in pairwise comparisons matrices.
Although assessments can be imprecise, since subjective criteria are involved,
the classical pairwise comparisons theory expects all of them to be available.
In practice, some experts may not be able (or available) to provide all
assessments. Therefore managing null entries is a necessary extension of the
pairwise comparisons method. It is shown that certain null entries can be
recovered on the basis of the transitivity property which each pairwise
comparisons matrix is expected to satisfy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05335</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05335</id><created>2015-05-20</created><authors><author><keyname>Valmorbida</keyname><forenames>Giorgio</forenames></author><author><keyname>Raman</keyname><forenames>Dhruva</forenames></author><author><keyname>Anderson</keyname><forenames>James</forenames></author></authors><title>Bounds for Input- and State-to-Output Properties of Uncertain Linear
  Systems</title><categories>math.OC cs.SY</categories><comments>To appear in the proceedings of the 8th IFAC Symposium on Robust
  Control Design - ROCOND'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the effect of parametric uncertainty on properties of Linear Time
Invariant systems. Traditional approaches to this problem determine the
worst-case gains of the system over the uncertainty set. Whilst such approaches
are computationally tractable, the upper bound obtained is not necessarily
informative in terms of assessing the influence of the parameters on the system
performance. We present theoretical results that lead to simple, convex
algorithms producing parametric bounds on the $\mathcal{L}_2$-induced
input-to-output and state-to-output gains as a function of the uncertain
parameters. These bounds provide quantitative information about how the
uncertainty affects the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05338</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05338</id><created>2015-05-20</created><authors><author><keyname>Dasgupta</keyname><forenames>Poorna Banerjee</forenames></author></authors><title>Algorithmic Analysis of Edge Ranking and Profiling for MTF Determination
  of an Imaging System</title><categories>cs.CV</categories><comments>3 pages, Published with International Journal of Computer Trends and
  Technology (IJCTT), Volume-23 Number-1, 2015</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V23(1):46-48, May 2015</journal-ref><doi>10.14445/22312803/IJCTT-V23P110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge detection is one of the most principal techniques for detecting
discontinuities in the gray levels of image pixels. The Modulation Transfer
Function (MTF) is one of the main criteria for assessing imaging quality and is
a parameter frequently used for measuring the sharpness of an imaging system.
In order to determine the MTF, it is essential to determine the best edge from
the target image so that an edge profile can be developed and then the line
spread function and hence the MTF, can be computed accordingly. For regular
image sizes, the human visual system is adept enough to identify suitable edges
from the image. But considering huge image datasets, such as those obtained
from satellites, the image size may range in few gigabytes and in such a case,
manual inspection of images for determination of the best suitable edge is not
plausible and hence, edge profiling tasks have to be automated. This paper
presents a novel, yet simple, algorithm for edge ranking and detection from
image data-sets for MTF computation, which is ideal for automation on
vectorised graphical processing units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05343</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05343</id><created>2015-05-20</created><updated>2015-05-21</updated><authors><author><keyname>G&#xf6;bel</keyname><forenames>Johannes</forenames></author><author><keyname>Keeler</keyname><forenames>Paul</forenames></author><author><keyname>Krzesinski</keyname><forenames>Anthony E.</forenames></author><author><keyname>Taylor</keyname><forenames>Peter G.</forenames></author></authors><title>Bitcoin Blockchain Dynamics: the Selfish-Mine Strategy in the Presence
  of Propagation Delay</title><categories>cs.CR</categories><comments>14 pages, 13 Figures. Submitted to a peer-reviewed journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of the `selfish-mine' strategy proposed by Eyal and Sirer, we
study the effect of propagation delay on the evolution of the Bitcoin
blockchain. First, we use a simplified Markov model that tracks the contrasting
states of belief about the blockchain of a small pool of miners and the `rest
of the community' to establish that the use of block-hiding strategies, such as
selfish-mine, causes the rate of production of orphan blocks to increase. Then
we use a spatial Poisson process model to study values of Eyal and Sirer's
parameter $\gamma$, which denotes the proportion of the honest community that
mine on a previously-secret block released by the pool in response to the
mining of a block by the honest community. Finally, we use discrete-event
simulation to study the behaviour of a network of Bitcoin miners, a proportion
of which is colluding in using the selfish-mine strategy, under the assumption
that there is a propagation delay in the communication of information between
miners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05354</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05354</id><created>2015-05-20</created><authors><author><keyname>Yang</keyname><forenames>Weixin</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Xie</keyname><forenames>Zecheng</forenames></author><author><keyname>Feng</keyname><forenames>Ziyong</forenames></author></authors><title>DropSample: A New Training Method to Enhance Deep Convolutional Neural
  Networks for Large-Scale Unconstrained Handwritten Chinese Character
  Recognition</title><categories>cs.CV</categories><comments>18 pages, 8 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the theory of Leitners learning box from the field of psychology,
we propose DropSample, a new method for training deep convolutional neural
networks (DCNNs), and apply it to large-scale online handwritten Chinese
character recognition (HCCR). According to the principle of DropSample, each
training sample is associated with a quota function that is dynamically
adjusted on the basis of the classification confidence given by the DCNN
softmax output. After a learning iteration, samples with low confidence will
have a higher probability of being selected as training data in the next
iteration; in contrast, well-trained and well-recognized samples with very high
confidence will have a lower probability of being involved in the next training
iteration and can be gradually eliminated. As a result, the learning process
becomes more efficient as it progresses. Furthermore, we investigate the use of
domain-specific knowledge to enhance the performance of DCNN by adding a domain
knowledge layer before the traditional CNN. By adopting DropSample together
with different types of domain-specific knowledge, the accuracy of HCCR can be
improved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1,
and ICDAR 2013 online HCCR competition datasets yield outstanding recognition
rates of 97.33%, 97.06%, and 97.51% respectively, all of which are
significantly better than the previous best results reported in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05364</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05364</id><created>2015-05-20</created><authors><author><keyname>Artikis</keyname><forenames>Alexander</forenames></author><author><keyname>Sergot</keyname><forenames>Marek</forenames></author><author><keyname>Paliouras</keyname><forenames>Georgios</forenames></author></authors><title>Reactive Reasoning with the Event Calculus</title><categories>cs.AI</categories><comments>International Workshop on Reactive Concepts in Knowledge
  Representation (ReactKnow 2014), co-located with the 21st European Conference
  on Artificial Intelligence (ECAI 2014). Proceedings of the International
  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),
  pages 9-15, technical report, ISSN 1430-3701, Leipzig University, 2014.
  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562. 2014,1</comments><proxy>J\&quot;org P\&quot;uhrer</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems for symbolic event recognition accept as input a stream of
time-stamped events from sensors and other computational devices, and seek to
identify high-level composite events, collections of events that satisfy some
pattern. RTEC is an Event Calculus dialect with novel implementation and
'windowing' techniques that allow for efficient event recognition, scalable to
large data streams. RTEC can deal with applications where event data arrive
with a (variable) delay from, and are revised by, the underlying sources. RTEC
can update already recognised events and recognise new events when data arrive
with a delay or following data revision. Our evaluation shows that RTEC can
support real-time event recognition and is capable of meeting the performance
requirements identified in a recent survey of event processing use cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05365</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05365</id><created>2015-05-20</created><authors><author><keyname>Beck</keyname><forenames>Harald</forenames></author><author><keyname>Dao-Tran</keyname><forenames>Minh</forenames></author><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Fink</keyname><forenames>Michael</forenames></author></authors><title>Towards Ideal Semantics for Analyzing Stream Reasoning</title><categories>cs.AI</categories><comments>International Workshop on Reactive Concepts in Knowledge
  Representation (ReactKnow 2014), co-located with the 21st European Conference
  on Artificial Intelligence (ECAI 2014). Proceedings of the International
  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),
  pages 17-22, technical report, ISSN 1430-3701, Leipzig University, 2014.
  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562 2014,1</comments><proxy>J\&quot;org P\&quot;uhrer</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of smart applications has drawn interest to logical reasoning over
data streams. Recently, different query languages and stream
processing/reasoning engines were proposed in different communities. However,
due to a lack of theoretical foundations, the expressivity and semantics of
these diverse approaches are given only informally. Towards clear
specifications and means for analytic study, a formal framework is needed to
define their semantics in precise terms. To this end, we present a first step
towards an ideal semantics that allows for exact descriptions and comparisons
of stream reasoning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05366</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05366</id><created>2015-05-20</created><authors><author><keyname>Brewka</keyname><forenames>Gerhard</forenames></author><author><keyname>Ellmauthaler</keyname><forenames>Stefan</forenames></author><author><keyname>P&#xfc;hrer</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Multi-Context Systems for Reactive Reasoning in Dynamic Environments</title><categories>cs.AI</categories><comments>International Workshop on Reactive Concepts in Knowledge
  Representation (ReactKnow 2014), co-located with the 21st European Conference
  on Artificial Intelligence (ECAI 2014). Proceedings of the International
  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),
  pages 23-29, technical report, ISSN 1430-3701, Leipzig University, 2014.
  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562</comments><proxy>J\&quot;org P\&quot;uhrer</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show in this paper how managed multi-context systems (mMCSs) can be turned
into a reactive formalism suitable for continuous reasoning in dynamic
environments. We extend mMCSs with (abstract) sensors and define the notion of
a run of the extended systems. We then show how typical problems arising in
online reasoning can be addressed: handling potentially inconsistent sensor
input, modeling intelligent forms of forgetting, selective integration of
knowledge, and controlling the reasoning effort spent by contexts, like setting
contexts to an idle mode. We also investigate the complexity of some important
related decision problems and discuss different design choices which are given
to the knowledge engineer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05367</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05367</id><created>2015-05-20</created><authors><author><keyname>Ellmauthaler</keyname><forenames>Stefan</forenames></author><author><keyname>P&#xfc;hrer</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Asynchronous Multi-Context Systems</title><categories>cs.AI</categories><comments>International Workshop on Reactive Concepts in Knowledge
  Representation (ReactKnow 2014), co-located with the 21st European Conference
  on Artificial Intelligence (ECAI 2014). Proceedings of the International
  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),
  pages 31-37, technical report, ISSN 1430-3701, Leipzig University, 2014.
  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562</comments><proxy>J\&quot;org P\&quot;uhrer</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present asynchronous multi-context systems (aMCSs), which
provide a framework for loosely coupling different knowledge representation
formalisms that allows for online reasoning in a dynamic environment. Systems
of this kind may interact with the outside world via input and output streams
and may therefore react to a continuous flow of external information. In
contrast to recent proposals, contexts in an aMCS communicate with each other
in an asynchronous way which fits the needs of many application domains and is
beneficial for scalability. The federal semantics of aMCSs renders our
framework an integration approach rather than a knowledge representation
formalism itself. We illustrate the introduced concepts by means of an example
scenario dealing with rescue services. In addition, we compare aMCSs to
reactive multi-context systems and describe how to simulate the latter with our
novel approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05368</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05368</id><created>2015-05-20</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Ricardo</forenames></author><author><keyname>Knorr</keyname><forenames>Matthias</forenames></author><author><keyname>Leite</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>On Minimal Change in Evolving Multi-Context Systems (Preliminary Report)</title><categories>cs.AI</categories><comments>International Workshop on Reactive Concepts in Knowledge
  Representation (ReactKnow 2014), co-located with the 21st European Conference
  on Artificial Intelligence (ECAI 2014). Proceedings of the International
  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),
  pages 47-53, technical report, ISSN 1430-3701, Leipzig University, 2014.
  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562</comments><proxy>J\&quot;org P\&quot;uhrer</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Managed Multi-Context Systems (mMCSs) provide a general framework for
integrating knowledge represented in heterogeneous KR formalisms. However,
mMCSs are essentially static as they were not designed to run in a dynamic
scenario. Some recent approaches, among them evolving Multi-Context Systems
(eMCSs), extend mMCSs by allowing not only the ability to integrate knowledge
represented in heterogeneous KR formalisms, but at the same time to both react
to, and reason in the presence of commonly temporary dynamic observations, and
evolve by incorporating new knowledge. The notion of minimal change is a
central notion in dynamic scenarios, specially in those that admit several
possible alternative evolutions. Since eMCSs combine heterogeneous KR
formalisms, each of which may require different notions of minimal change, the
study of minimal change in eMCSs is an interesting and highly non-trivial
problem. In this paper, we study the notion of minimal change in eMCSs, and
discuss some alternative minimal change criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05373</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05373</id><created>2015-05-20</created><authors><author><keyname>P&#xfc;hrer</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Towards a Simulation-Based Programming Paradigm for AI applications</title><categories>cs.AI</categories><comments>International Workshop on Reactive Concepts in Knowledge
  Representation (ReactKnow 2014), co-located with the 21st European Conference
  on Artificial Intelligence (ECAI 2014). Proceedings of the International
  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),
  pages 55-61, technical report, ISSN 1430-3701, Leipzig University, 2014</comments><proxy>J\&quot;org P\&quot;uhrer</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present initial ideas for a programming paradigm based on simulation that
is targeted towards applications of artificial intelligence (AI). The approach
aims at integrating techniques from different areas of AI and is based on the
idea that simulated entities may freely exchange data and behavioural patterns.
We define basic notions of a simulation-based programming paradigm and show how
it can be used for implementing AI applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05375</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05375</id><created>2015-05-20</created><authors><author><keyname>Thimm</keyname><forenames>Matthias</forenames></author></authors><title>Towards Large-scale Inconsistency Measurement</title><categories>cs.AI</categories><comments>International Workshop on Reactive Concepts in Knowledge
  Representation (ReactKnow 2014), co-located with the 21st European Conference
  on Artificial Intelligence (ECAI 2014). Proceedings of the International
  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),
  pages 63-70, technical report, ISSN 1430-3701, Leipzig University, 2014.
  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562</comments><proxy>J\&quot;org P\&quot;uhrer</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of inconsistency measurement on large knowledge
bases by considering stream-based inconsistency measurement, i.e., we
investigate inconsistency measures that cannot consider a knowledge base as a
whole but process it within a stream. For that, we present, first, a novel
inconsistency measure that is apt to be applied to the streaming case and,
second, stream-based approximations for the new and some existing inconsistency
measures. We conduct an extensive empirical analysis on the behavior of these
inconsistency measures on large knowledge bases, in terms of runtime, accuracy,
and scalability. We conclude that for two of these measures, the approximation
of the new inconsistency measure and an approximation of the contension
inconsistency measure, large-scale inconsistency measurement is feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05388</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05388</id><created>2015-05-20</created><authors><author><keyname>Jha</keyname><forenames>R.</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Rouillier</keyname><forenames>Fabrice</forenames><affiliation>LIP6</affiliation></author><author><keyname>Moroz</keyname><forenames>G.</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Workspace and Singularity analysis of a Delta like family robot</title><categories>cs.RO</categories><comments>4th IFTOMM International Symposium on Robotics and Mechatronics, Jun
  2015, Poitiers, France. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Workspace and joint space analysis are essential steps in describing the task
and designing the control loop of the robot, respectively. This paper presents
the descriptive analysis of a family of delta-like parallel robots by using
algebraic tools to induce an estimation about the complexity in representing
the singularities in the workspace and the joint space. A Gr{\&quot;o}bner based
elimination is used to compute the singularities of the manipulator and a
Cylindrical Algebraic Decomposition algorithm is used to study the workspace
and the joint space. From these algebraic objects, we propose some certified
three dimensional plotting describing the the shape of workspace and of the
joint space which will help the engineers or researchers to decide the most
suited configuration of the manipulator they should use for a given task. Also,
the different parameters associated with the complexity of the serial and
parallel singularities are tabulated, which further enhance the selection of
the different configuration of the manipulator by comparing the complexity of
the singularity equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05401</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05401</id><created>2015-05-20</created><updated>2015-08-13</updated><authors><author><keyname>Baldassi</keyname><forenames>Carlo</forenames></author><author><keyname>Braunstein</keyname><forenames>Alfredo</forenames></author></authors><title>A Max-Sum algorithm for training discrete neural networks</title><categories>cond-mat.dis-nn cs.LG cs.NE</categories><journal-ref>Journal of Statistical Mechanics: Theory and Experiment 2015, no.
  8, P08008</journal-ref><doi>10.1088/1742-5468/2015/08/P08008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient learning algorithm for the problem of training neural
networks with discrete synapses, a well-known hard (NP-complete) discrete
optimization problem. The algorithm is a variant of the so-called Max-Sum (MS)
algorithm. In particular, we show how, for bounded integer weights with $q$
distinct states and independent concave a priori distribution (e.g. $l_{1}$
regularization), the algorithm's time complexity can be made to scale as
$O\left(N\log N\right)$ per node update, thus putting it on par with
alternative schemes, such as Belief Propagation (BP), without resorting to
approximations. Two special cases are of particular interest: binary synapses
$W\in\{-1,1\}$ and ternary synapses $W\in\{-1,0,1\}$ with $l_{0}$
regularization. The algorithm we present performs as well as BP on binary
perceptron learning problems, and may be better suited to address the problem
on fully-connected two-layer networks, since inherent symmetries in two layer
networks are naturally broken using the MS approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05404</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05404</id><created>2015-05-20</created><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>Faulty Successive Cancellation Decoding of Polar Codes for the Binary
  Erasure Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, faulty successive cancellation decoding of polar codes for the
binary erasure channel is studied. To this end, a simple erasure-based fault
model is introduced to represent errors n the decoder and it is shown that,
under this model, polarization does not happen, meaning that fully reliable
communication is not possible at any rate. Furthermore, a lower bound on the
frame error rate of polar codes under faulty SC decoding is provided, which is
then used, along with a well-known upper bound, in order to choose a
blocklength that minimizes the erasure probability under faulty decoding.
Finally, an unequal error protection scheme that can re-enable asymptotically
erasure-free transmission at a small rate loss and by protecting only a
constant fraction of the decoder is proposed. The same scheme is also shown to
significantly improve the finite-length performance of the faulty successive
cancellation decoder wit negligible circuit overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05405</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05405</id><created>2015-05-20</created><authors><author><keyname>Schmalen</keyname><forenames>Laurent</forenames></author><author><keyname>Suikat</keyname><forenames>Detlef</forenames></author><author><keyname>R&#xf6;sener</keyname><forenames>Detlef</forenames></author><author><keyname>Aref</keyname><forenames>Vahid</forenames></author><author><keyname>Leven</keyname><forenames>Andreas</forenames></author><author><keyname>Brink</keyname><forenames>Stephan ten</forenames></author></authors><title>Spatially Coupled Codes and Optical Fiber Communications: An Ideal
  Match?</title><categories>cs.IT math.IT</categories><comments>Invited paper to be presented in the special session on &quot;Signal
  Processing, Coding, and Information Theory for Optical Communications&quot; at
  IEEE SPAWC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we highlight the class of spatially coupled codes and discuss
their applicability to long-haul and submarine optical communication systems.
We first demonstrate how to optimize irregular spatially coupled LDPC codes for
their use in optical communications with limited decoding hardware complexity
and then present simulation results with an FPGA-based decoder where we show
that very low error rates can be achieved and that conventional block-based
LDPC codes can be outperformed. In the second part of the paper, we focus on
the combination of spatially coupled LDPC codes with different demodulators and
detectors, important for future systems with adaptive modulation and for
varying channel characteristics. We demonstrate that SC codes can be employed
as universal, channel-agnostic coding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05407</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05407</id><created>2015-05-20</created><authors><author><keyname>Liang</keyname><forenames>Wei-Jie</forenames></author><author><keyname>Lin</keyname><forenames>Gang-Xuan</forenames></author><author><keyname>Lu</keyname><forenames>Chun-Shien</forenames></author></authors><title>Compressive Sensing of Large-Scale Images: An Assumption-Free Approach</title><categories>cs.MM cs.IT math.IT</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cost-efficient compressive sensing of big media data with fast reconstructed
high-quality results is very challenging. In this paper, we propose a new
large-scale image compressive sensing method, composed of operator-based
strategy in the context of fixed point continuation method and weighted LASSO
with tree structure sparsity pattern. The main characteristic of our method is
free from any assumptions and restrictions. The feasibility of our method is
verified via simulations and comparisons with state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05423</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05423</id><created>2015-05-20</created><authors><author><keyname>Gottschalk</keyname><forenames>Corinna</forenames></author><author><keyname>Peis</keyname><forenames>Britta</forenames></author></authors><title>Submodular Function Maximization on the Bounded Integer Lattice</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of maximizing a submodular function on the bounded
integer lattice. As a direct generalization of submodular set functions, $f:
\{0, \ldots, C\} \rightarrow \mathbb{R}_+$ is submodular, if $f(x) + f(y) \geq
f(x \land y) + f(x \lor y)$ for all $x,y \in \{0, \ldots, C\}$ where $\land$
and $\lor$ denote element-wise minimum and maximum. The objective is finding a
vector $x$ maximizing $f(x)$. In this paper, we present a deterministic
$\frac{1}{3}$-approximation using a framework inspired by Buchbinder et al. We
also provide an example that shows the analysis is tight and yields additional
insight into the possibilities of modifying the algorithm. Moreover, we examine
some structural differences to maximization of submodular set functions which
make our problem harder to solve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05424</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05424</id><created>2015-05-20</created><updated>2015-05-21</updated><authors><author><keyname>Blundell</keyname><forenames>Charles</forenames></author><author><keyname>Cornebise</keyname><forenames>Julien</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author><author><keyname>Wierstra</keyname><forenames>Daan</forenames></author></authors><title>Weight Uncertainty in Neural Networks</title><categories>stat.ML cs.LG</categories><comments>In Proceedings of the 32nd International Conference on Machine
  Learning (ICML 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new, efficient, principled and backpropagation-compatible
algorithm for learning a probability distribution on the weights of a neural
network, called Bayes by Backprop. It regularises the weights by minimising a
compression cost, known as the variational free energy or the expected lower
bound on the marginal likelihood. We show that this principled kind of
regularisation yields comparable performance to dropout on MNIST
classification. We then demonstrate how the learnt uncertainty in the weights
can be used to improve generalisation in non-linear regression problems, and
how this weight uncertainty can be used to drive the exploration-exploitation
trade-off in reinforcement learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05425</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05425</id><created>2015-05-20</created><updated>2016-01-13</updated><authors><author><keyname>Jacobs</keyname><forenames>Christian T.</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard J.</forenames></author><author><keyname>Rees</keyname><forenames>Huw E.</forenames></author><author><keyname>Craig</keyname><forenames>Lorraine</forenames></author></authors><title>Experiences with efficient methodologies for teaching computer
  programming to geoscientists</title><categories>cs.CY</categories><comments>Revised version submitted to the Journal of Geoscience Education.
  Contains 5 figures. Main changes: re-organised the paper such that the final
  version of the course is presented more as a course blueprint; added
  recommendations about how the design could be applied to other courses;
  improved the description of the scoring system; showed how the course
  compares to other courses in the department</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer programming was once thought of as a skill required only by
professional software developers. But today, given the ubiquitous nature of
computation and data science it is quickly becoming necessary for all
scientists and engineers to have at least a basic knowledge of how to program.
Teaching how to program, particularly to those students with little or no
computing background, is well-known to be a difficult task. However, there is
also a wealth of evidence-based teaching practices for teaching programming
skills which can be applied to greatly improve learning outcomes and the
student experience. Adopting these practices naturally gives rise to greater
learning efficiency - this is critical if programming is to be integrated into
an already busy geoscience curriculum. This paper considers an undergraduate
computer programming course, run during the last 5 years in the Department of
Earth Science and Engineering at Imperial College London. The teaching
methodologies that were used each year are discussed alongside the challenges
that were encountered, and how the methodologies affected student performance.
Anonymised student marks and feedback are used to highlight this, and also how
the adjustments made to the course eventually resulted in a highly effective
learning environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05428</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05428</id><created>2015-05-20</created><updated>2015-06-06</updated><authors><author><keyname>Chatouh</keyname><forenames>K.</forenames></author><author><keyname>Guenda</keyname><forenames>K.</forenames></author><author><keyname>Gulliver</keyname><forenames>T. A.</forenames></author><author><keyname>Noui</keyname><forenames>L.</forenames></author></authors><title>Simplex and MacDonald Codes over $R_{q}$</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we introduce the homogeneous weight and homogeneous Gray map
over the ring $R_{q}=\mathbb{F}_{2}[u_{1},u_{2},\ldots,u_{q}]/\left\langle
u_{i}^{2}=0,u_{i}u_{j}=u_{j}u_{i}\right\rangle$ for $q \geq 2$. We also
consider the construction of simplex and MacDonald codes of types $\alpha$ and
$\beta$ over this ring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05435</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05435</id><created>2015-05-20</created><authors><author><keyname>Lee</keyname><forenames>Si-Hyeon</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author></authors><title>Noisy Network Coding with Partial DF</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, to appear in Proc. IEEE ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a noisy network coding integrated with partial
decode-and-forward relaying for single-source multicast discrete memoryless
networks (DMN's). Our coding scheme generalizes the
partial-decode-compress-and-forward scheme (Theorem 7) by Cover and El Gamal.
This is the first time the theorem is generalized for DMN's such that each
relay performs both partial decode-and-forward and compress-and-forward
simultaneously. Our coding scheme simultaneously generalizes both noisy network
coding by Lim, Kim, El Gamal, and Chung and distributed decode-and-forward by
Lim, Kim, and Kim. It is not trivial to combine the two schemes because of
inherent incompatibility in their encoding and decoding strategies. We solve
this problem by sending the same long message over multiple blocks at the
source and at the same time by letting the source find the auxiliary covering
indices that carry information about the message simultaneously over all
blocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05441</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05441</id><created>2015-05-20</created><updated>2016-02-05</updated><authors><author><keyname>Nestmeyer</keyname><forenames>Thomas</forenames></author><author><keyname>Giordano</keyname><forenames>Paolo Robuffo</forenames></author><author><keyname>B&#xfc;lthoff</keyname><forenames>Heinrich H.</forenames></author><author><keyname>Franchi</keyname><forenames>Antonio</forenames></author></authors><title>Decentralized Simultaneous Multi-target Exploration using a Connected
  Network of Multiple Robots</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel decentralized control strategy for a multi-robot
system that enables parallel multi-target exploration while ensuring a
time-varying connected topology in cluttered 3D environments. Flexible
continuous connectivity is guaranteed by building upon a recent connectivity
maintenance method, in which limited range, line-of-sight visibility, and
collision avoidance are taken into account at the same time. Completeness of
the decentralized multi-target exploration algorithm is guaranteed by
dynamically assigning the robots with different motion behaviors during the
exploration task. One major group is subject to a suitable downscaling of the
main traveling force based on the traveling efficiency of the current leader
and the direction alignment between traveling and connectivity force. This
supports the leader in always reaching its current target and, on a larger time
horizon, that the whole team realizes the overall task in finite time.
Extensive Monte~Carlo simulations with a group of several quadrotor UAVs show
the scalability and effectiveness of the proposed method and experiments
validate its practicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05451</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05451</id><created>2015-05-20</created><authors><author><keyname>Sartakhti</keyname><forenames>Javad Salimi</forenames></author><author><keyname>Ghadiri</keyname><forenames>Nasser</forenames></author><author><keyname>Afrabandpey</keyname><forenames>Homayun</forenames></author></authors><title>Fuzzy Least Squares Twin Support Vector Machines</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Least Squares Twin Support Vector Machine (LSTSVM) is an extremely efficient
and fast version of SVM algorithm for binary classification. LSTSVM combines
the idea of Least Squares SVM and Twin SVM in which two non-parallel
hyperplanes are found by solving two systems of linear equations. Although, the
algorithm is very fast and efficient in many classification tasks, it is unable
to cope with two features of real-world problems. First, in many real-world
classification problems, it is almost impossible to assign data points to a
single class. Second, data points in real-world problems may have different
importance. In this study, we propose a novel version of LSTSVM based on fuzzy
concepts to deal with these two characteristics of real-world data. The
algorithm is called Fuzzy LSTSVM (FLSTSVM) which provides more flexibility than
binary classification of LSTSVM. Two models are proposed for the algorithm. In
the first model, a fuzzy membership value is assigned to each data point and
the hyperplanes are optimized based on these fuzzy samples. In the second model
we construct fuzzy hyperplanes to classify data. Finally, we apply our proposed
FLSTSVM to an artificial as well as three real-world datasets. Results
demonstrate that FLSTSVM obtains better performance than SVM and LSTSVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05454</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05454</id><created>2015-05-20</created><authors><author><keyname>Boissonnat</keyname><forenames>Jean-Daniel</forenames></author><author><keyname>Dyer</keyname><forenames>Ramsay</forenames></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames></author></authors><title>A probabilistic approach to reducing the algebraic complexity of
  computing Delaunay triangulations</title><categories>cs.CG</categories><comments>24 pages</comments><msc-class>68W05</msc-class><acm-class>I.3.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Computing Delaunay triangulations in $\mathbb{R}^d$ involves evaluating the
so-called in\_sphere predicate that determines if a point $x$ lies inside, on
or outside the sphere circumscribing $d+1$ points $p_0,\ldots ,p_d$. This
predicate reduces to evaluating the sign of a multivariate polynomial of degree
$d+2$ in the coordinates of the points $x, \, p_0,\, \ldots,\, p_d$. Despite
much progress on exact geometric computing, the fact that the degree of the
polynomial increases with $d$ makes the evaluation of the sign of such a
polynomial problematic except in very low dimensions. In this paper, we propose
a new approach that is based on the witness complex, a weak form of the
Delaunay complex introduced by Carlsson and de Silva. The witness complex
$\mathrm{Wit} (L,W)$ is defined from two sets $L$ and $W$ in some metric space
$X$: a finite set of points $L$ on which the complex is built, and a set $W$ of
witnesses that serves as an approximation of $X$. A fundamental result of de
Silva states that $\mathrm{Wit}(L,W)=\mathrm{Del} (L)$ if $W=X=\mathbb{R}^d$.
In this paper, we give conditions on $L$ that ensure that the witness complex
and the Delaunay triangulation coincide when $W$ is a finite set, and we
introduce a new perturbation scheme to compute a perturbed set $L'$ close to
$L$ such that $\mathrm{Del} (L')= \mathrm{wit} (L', W)$. Our perturbation
algorithm is a geometric application of the Moser-Tardos constructive proof of
the Lov\'asz local lemma. The only numerical operations we use are (squared)
distance comparisons (i.e., predicates of degree 2). The time-complexity of the
algorithm is sublinear in $|W|$. Interestingly, although the algorithm does not
compute any measure of simplex quality, a lower bound on the thickness of the
output simplices can be guaranteed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05459</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05459</id><created>2015-05-20</created><authors><author><keyname>Sarbolandi</keyname><forenames>Hamed</forenames></author><author><keyname>Lefloch</keyname><forenames>Damien</forenames></author><author><keyname>Kolb</keyname><forenames>Andreas</forenames></author></authors><title>Kinect Range Sensing: Structured-Light versus Time-of-Flight Kinect</title><categories>cs.CV</categories><comments>58 pages, 23 figures. Accepted for publication in Computer Vision and
  Image Understanding (CVIU)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the new Kinect One has been issued by Microsoft, providing the next
generation of real-time range sensing devices based on the Time-of-Flight (ToF)
principle. As the first Kinect version was using a structured light approach,
one would expect various differences in the characteristics of the range data
delivered by both devices. This paper presents a detailed and in-depth
comparison between both devices. In order to conduct the comparison, we propose
a framework of seven different experimental setups, which is a generic basis
for evaluating range cameras such as Kinect. The experiments have been designed
with the goal to capture individual effects of the Kinect devices as isolatedly
as possible and in a way, that they can also be adopted, in order to apply them
to any other range sensing device. The overall goal of this paper is to provide
a solid insight into the pros and cons of either device. Thus, scientists that
are interested in using Kinect range sensing cameras in their specific
application scenario can directly assess the expected, specific benefits and
potential problem of either device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05481</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05481</id><created>2015-05-20</created><authors><author><keyname>Si</keyname><forenames>Hongbo</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Appaiah</keyname><forenames>Kumar</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Expansion Coding for Channel and Source Coding</title><categories>cs.IT math.IT</categories><comments>42 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general method of coding over expansion is proposed,which allows one to
reduce the highly non-trivial problems of coding over analog channels and
compressing analog sources to a set of much simpler subproblems, coding over
discrete channels and compressing discrete sources. More specifically, the
focus of this paper is on the additive exponential noise (AEN) channel, and
lossy compression of exponential sources. Taking advantage of the essential
decomposable property of these channels (sources), the proposed expansion
method allows for mapping of these problems to coding over parallel channels
(respectively, sources), where each level is modeled as an independent coding
problem over discrete alphabets. Any feasible solution to the resulting
optimization problem after expansion corresponds to an achievable scheme of the
original problem. Utilizing this mapping, even for the cases where the optimal
solutions are difficult to characterize, it is shown that the expansion coding
scheme still performs well with appropriate choices of parameters. More
specifically, theoretical analysis and numerical results reveal that expansion
coding achieves the capacity of AEN channel in the high SNR regime. It is also
shown that for lossy compression, the achievable rate distortion pair by
expansion coding approaches to the Shannon limit in the low distortion region.
Remarkably, by using capacity-achieving codes with low encoding and decoding
complexity that are originally designed for discrete alphabets, for instance
polar codes, the proposed expansion coding scheme allows for designing
low-complexity analog channel and source codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05489</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05489</id><created>2015-05-20</created><updated>2015-10-19</updated><authors><author><keyname>Almosallam</keyname><forenames>Ibrahim A.</forenames></author><author><keyname>Lindsay</keyname><forenames>Sam N.</forenames></author><author><keyname>Jarvis</keyname><forenames>Matt J.</forenames></author><author><keyname>Roberts</keyname><forenames>Stephen J.</forenames></author></authors><title>A Sparse Gaussian Process Framework for Photometric Redshift Estimation</title><categories>astro-ph.IM astro-ph.GA cs.CV</categories><doi>10.1093/mnras/stv2425</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Accurate photometric redshifts are a lynchpin for many future experiments to
pin down the cosmological model and for studies of galaxy evolution. In this
study, a novel sparse regression framework for photometric redshift estimation
is presented. Simulated and real data from SDSS DR12 were used to train and
test the proposed models. We show that approaches which include careful data
preparation and model design offer a significant improvement in comparison with
several competing machine learning algorithms. Standard implementations of most
regression algorithms have as the objective the minimization of the sum of
squared errors. For redshift inference, however, this induces a bias in the
posterior mean of the output distribution, which can be problematic. In this
paper we directly target minimizing $\Delta z = (z_\textrm{s} -
z_\textrm{p})/(1+z_\textrm{s})$ and address the bias problem via a
distribution-based weighting scheme, incorporated as part of the optimization
objective. The results are compared with other machine learning algorithms in
the field such as Artificial Neural Networks (ANN), Gaussian Processes (GPs)
and sparse GPs. The proposed framework reaches a mean absolute $\Delta z =
0.0026(1+z_\textrm{s})$, over the redshift range of $0 \le z_\textrm{s} \le 2$
on the simulated data, and $\Delta z = 0.0178(1+z_\textrm{s})$ over the entire
redshift range on the SDSS DR12 survey, outperforming the standard ANNz used in
the literature. We also investigate how the relative size of the training set
affects the photometric redshift accuracy. We find that a training set of
\textgreater 30 per cent of total sample size, provides little additional
constraint on the photometric redshifts, and note that our GP formalism
strongly outperforms ANNz in the sparse data regime for the simulated data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05502</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05502</id><created>2015-05-20</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Ricardo</forenames></author><author><keyname>Knorr</keyname><forenames>Matthias</forenames></author><author><keyname>Leite</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Towards Efficient Evolving Multi-Context Systems (Preliminary Report)</title><categories>cs.AI</categories><comments>International Workshop on Reactive Concepts in Knowledge
  Representation (ReactKnow 2014), co-located with the 21st European Conference
  on Artificial Intelligence (ECAI 2014). Proceedings of the International
  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),
  pages 39-45, technical report, ISSN 1430-3701, Leipzig University, 2014.
  http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-150562 . arXiv admin note:
  substantial text overlap with arXiv:1505.05368</comments><proxy>J\&quot;org P\&quot;uhrer</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Managed Multi-Context Systems (mMCSs) provide a general framework for
integrating knowledge represented in heterogeneous KR formalisms. Recently,
evolving Multi-Context Systems (eMCSs) have been introduced as an extension of
mMCSs that add the ability to both react to, and reason in the presence of
commonly temporary dynamic observations, and evolve by incorporating new
knowledge. However, the general complexity of such an expressive formalism may
simply be too high in cases where huge amounts of information have to be
processed within a limited short amount of time, or even instantaneously. In
this paper, we investigate under which conditions eMCSs may scale in such
situations and we show that such polynomial eMCSs can be applied in a practical
use case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05531</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05531</id><created>2015-05-20</created><authors><author><keyname>Aisenberg</keyname><forenames>James</forenames></author><author><keyname>Bonet</keyname><forenames>Maria Luisa</forenames></author><author><keyname>Buss</keyname><forenames>Sam</forenames></author><author><keyname>Cr&#xe3;ciun</keyname><forenames>Adrian</forenames></author><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author></authors><title>Short Proofs of the Kneser-Lov\'asz Coloring Principle</title><categories>math.LO cs.LO</categories><comments>This is a paper to appear in ICALP 2015, plus two appendices</comments><msc-class>03F20, 05C15</msc-class><acm-class>F.4.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the propositional translations of the Kneser-Lov\'asz theorem
have polynomial size extended Frege proofs and quasi-polynomial size Frege
proofs. We present a new counting-based combinatorial proof of the
Kneser-Lov\'asz theorem that avoids the topological arguments of prior proofs
for all but finitely many cases for each k. We introduce a miniaturization of
the octahedral Tucker lemma, called the truncated Tucker lemma: it is open
whether its propositional translations have (quasi-)polynomial size Frege or
extended Frege proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05537</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05537</id><created>2015-05-20</created><authors><author><keyname>Khalili</keyname><forenames>Mohsen</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Polycarpou</keyname><forenames>Marios M.</forenames></author><author><keyname>Parisini</keyname><forenames>Thomas</forenames></author><author><keyname>Cao</keyname><forenames>Yongcan</forenames></author></authors><title>Distributed Adaptive Fault-Tolerant Control of Uncertain Multi-Agent
  Systems</title><categories>cs.SY cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an adaptive fault-tolerant control (FTC) scheme for a
class of nonlinear uncertain multi-agent systems. A local FTC scheme is
designed for each agent using local measurements and suitable information
exchanged between neighboring agents. Each local FTC scheme consists of a fault
diagnosis module and a reconfigurable controller module comprised of a baseline
controller and two adaptive fault-tolerant controllers activated after fault
detection and after fault isolation, respectively. Under certain assumptions,
the closed-loop system's stability and leader-follower consensus properties are
rigorously established under different modes of the FTC system, including the
time-period before possible fault detection, between fault detection and
possible isolation, and after fault isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05549</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05549</id><created>2015-05-20</created><authors><author><keyname>Prezioso</keyname><forenames>M.</forenames></author><author><keyname>Merrikh-Bayat</keyname><forenames>F.</forenames></author><author><keyname>Hoskins</keyname><forenames>B.</forenames></author><author><keyname>Likharev</keyname><forenames>K.</forenames></author><author><keyname>Strukov</keyname><forenames>D.</forenames></author></authors><title>Self-Adaptive Spike-Time-Dependent Plasticity of Metal-Oxide Memristors</title><categories>cond-mat.other cs.ET</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metal-oxide memristors have emerged as promising candidates for hardware
implementation of artificial synapses - the key components of high-performance,
analog neuromorphic networks - due to their excellent scaling prospects. Since
some advanced cognitive tasks require spiking neuromorphic networks, which
explicitly model individual neural pulses (spikes) in biological neural
systems, it is crucial for memristive synapses to support the
spike-time-dependent plasticity (STDP), which is believed to be the primary
mechanism of Hebbian adaptation. A major challenge for the STDP implementation
is that, in contrast to some simplistic models of the plasticity, the
elementary change of a synaptic weight in an artificial hardware synapse
depends not only on the pre-synaptic and post-synaptic signals, but also on the
initial weight (memristor's conductance) value. Here we experimentally
demonstrate, for the first time, STDP protocols that ensure self-adaptation of
the average memristor conductance, making the plasticity stable, i.e.
insensitive to the initial state of the devices. The experiments have been
carried out with 200-nm Al2O3/TiO2-x memristors integrated into 12x12
crossbars. The experimentally observed self-adaptive STDP behavior has been
complemented with numerical modeling of weight dynamics in a simple system with
a leaky-integrate-and-fire neuron with a random spike-train input, using a
compact model of memristor plasticity, fitted for quantitatively correct
description of our memristors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05561</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05561</id><created>2015-05-20</created><updated>2016-03-02</updated><authors><author><keyname>Arpit</keyname><forenames>Devansh</forenames></author><author><keyname>Zhou</keyname><forenames>Yingbo</forenames></author><author><keyname>Ngo</keyname><forenames>Hung</forenames></author><author><keyname>Govindaraju</keyname><forenames>Venu</forenames></author></authors><title>Why Regularized Auto-Encoders learn Sparse Representation?</title><categories>stat.ML cs.CV cs.LG</categories><comments>8 pages of content, 1 page of reference, 4 pages of supplementary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse Distributed representation is the key to learning useful features in
deep learning algorithms not just because it is an efficient mode of data
representation, but more importantly, because it captures the generation
process of most real world data. Although a number of regularized auto-encoders
(AE) enforce sparsity explicitly in their learned representation while others
don't, there has been little formal analysis on what encourages sparsity in
these models in general. Therefore, our objective here is to formally study
this general problem for regularized auto-encoders. We show the properties of
both regularization and activation function that play an important role in
encouraging sparsity. We provide sufficient conditions on both these criteria
and show that multiple popular models-- eg. De-noising and Contractive auto
encoders-- and activations-- eg. Rectified Linear and Sigmoid-- satisfy these
conditions; thus explaining sparsity in their learned representation. Our
theoretical and empirical analysis together, throws light on the properties of
regularization/activation that are conducive to sparsity, but also brings
together a number of existing auto-encoder models and activation functions
under a unified analytical framework thereby yielding deeper insights into
unsupervised representation learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05570</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05570</id><created>2015-05-20</created><authors><author><keyname>Zahir</keyname><forenames>Saboor</forenames></author><author><keyname>Pak</keyname><forenames>John</forenames></author><author><keyname>Singh</keyname><forenames>Jatinder</forenames></author><author><keyname>Pawlick</keyname><forenames>Jeffrey</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>Protection and Deception: Discovering Game Theory and Cyber Literacy
  through a Novel Board Game Experience</title><categories>cs.CY cs.CR cs.GT</categories><comments>Submitted to 2015 USENIX Summit on Gaming, Games, and Gamification in
  Security Education</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber literacy merits serious research attention because it addresses a
confluence of specialization and generalization; cybersecurity is often
conceived of as approachable only by a technological intelligentsia, yet its
interdependent nature demands education for a broad population. Therefore,
educational tools should lead participants to discover technical knowledge in
an accessible and attractive framework. In this paper, we present Protection
and Deception (P&amp;G), a novel two-player board game. P&amp;G has three main
contributions. First, it builds cyber literacy by giving participants
&quot;hands-on&quot; experience with game pieces that have the capabilities of
cyber-attacks such as worms, masquerading attacks/spoofs, replay attacks, and
Trojans. Second, P&amp;G teaches the important game-theoretic concepts of
asymmetric information and resource allocation implicitly and non-obtrusively
through its game play. Finally, it strives for the important objective of
security education for underrepresented minorities and people without explicit
technical experience. We tested P&amp;G at a community center in Manhattan with
middle- and high school students, and observed enjoyment and increased cyber
literacy along with suggestions for improvement of the game. Together with
these results, our paper also presents images of the attractive board design
and 3D printed game pieces, together with a Monte-Carlo analysis that we used
to ensure a balanced gaming experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05571</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05571</id><created>2015-05-20</created><authors><author><keyname>Neal</keyname><forenames>Radford M.</forenames></author></authors><title>Fast exact summation using small and large superaccumulators</title><categories>cs.NA cs.DC stat.CO</categories><acm-class>G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present two new methods for exactly summing a set of floating-point
numbers, and then correctly rounding to the nearest floating-point number.
Higher accuracy than simple summation (rounding after each addition) is
important in many applications, such as finding the sample mean of data. Exact
summation also guarantees identical results with parallel and serial
implementations, since the exact sum is independent of order. The new methods
use variations on the concept of a &quot;superaccumulator&quot; - a large fixed-point
number that can exactly represent the sum of any reasonable number of
floating-point values. One method uses a &quot;small&quot; superaccumulator with
sixty-seven 64-bit chunks, each with 32-bit overlap with the next chunk,
allowing carry propagation to be done infrequently. The small superaccumulator
is used alone when summing a small number of terms. For big summations, a
&quot;large&quot; superaccumulator is used as well. It consists of 4096 64-bit chunks,
one for every possible combination of exponent bits and sign bit, plus counts
of when each chunk needs to be transferred to the small superaccumulator. To
add a term to the large superaccumulator, only a single chunk and its
associated count need to be updated, which takes very few instructions if
carefully implemented. On modern 64-bit processors, exactly summing a large
array using this combination of large and small superaccumulators takes less
than twice the time of simple, inexact, ordered summation, with a serial
implementation. A parallel implementation using a small number of processor
cores can be expected to perform exact summation of large arrays at a speed
that reaches the limit imposed by memory bandwidth. Some common methods that
attempt to improve accuracy without being exact may therefore be pointless, at
least for large summations, since they are slower than computing the sum
exactly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05572</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05572</id><created>2015-05-20</created><authors><author><keyname>Wiggins</keyname><forenames>Paul A.</forenames></author><author><keyname>LaMont</keyname><forenames>Colin H.</forenames></author></authors><title>The development of an information criterion for Change-Point Analysis</title><categories>physics.data-an cs.LG stat.ML</categories><comments>10 pages + supplement. 5 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change-point analysis is a flexible and computationally tractable tool for
the analysis of times series data from systems that transition between discrete
states and whose observables are corrupted by noise. The change-point algorithm
is used to identify the time indices (change points) at which the system
transitions between these discrete states. We present a unified
information-based approach to testing for the existence of change points. This
new approach reconciles two previously disparate approaches to Change-Point
Analysis (frequentist and information-based) for testing transitions between
states. The resulting method is statistically principled, parameter and prior
free and widely applicable to a wide range of change-point problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05576</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05576</id><created>2015-05-20</created><updated>2016-02-26</updated><authors><author><keyname>Yang</keyname><forenames>Shudi</forenames></author><author><keyname>Yao</keyname><forenames>Zheng-An</forenames></author></authors><title>The Complete Weight Enumerator of Several Cyclic Codes</title><categories>cs.IT math.IT</categories><comments>18 pages</comments><msc-class>11T71, 94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes have attracted a lot of research interest for decades. In this
paper, for an odd prime $p$, we propose a general strategy to compute the
complete weight enumerator of cyclic codes via the value distribution of the
corresponding exponential sums. As applications of this general strategy, we
determine the complete weight enumerator of several $p$-ary cyclic codes and
give some examples to illustrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05579</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05579</id><created>2015-05-20</created><authors><author><keyname>Mohamed</keyname><forenames>Ehab Mahmoud</forenames></author><author><keyname>Sakaguchi</keyname><forenames>Kei</forenames></author><author><keyname>Sampei</keyname><forenames>Seiichi</forenames></author></authors><title>Millimeter Wave Beamforming Based on WiFi Fingerprinting in Indoor
  Environment</title><categories>cs.NI</categories><comments>6 pages, 9 Figures, 1 Table, ICC workshops 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter Wave (mm-w), especially the 60 GHz band, has been receiving much
attention as a key enabler for the 5G cellular networks. Beamforming (BF) is
tremendously used with mm-w transmissions to enhance the link quality and
overcome the channel impairments. The current mm-w BF mechanism, proposed by
the IEEE 802.11ad standard, is mainly based on exhaustive searching the best
transmit (TX) and receive (RX) antenna beams. This BF mechanism requires a very
high setup time, which makes it difficult to coordinate a multiple number of
mm-w Access Points (APs) in mobile channel conditions as a 5G requirement. In
this paper, we propose a mm-w BF mechanism, which enables a mm-w AP to estimate
the best beam to communicate with a User Equipment (UE) using statistical
learning. In this scheme, the fingerprints of the UE WiFi signal and mm-w best
beam identification (ID) are collected in an offline phase on a grid of
arbitrary learning points (LPs) in target environments. Therefore, by just
comparing the current UE WiFi signal with the pre-stored UE WiFi fingerprints,
the mm-w AP can immediately estimate the best beam to communicate with the UE
at its current position. The proposed mm-w BF can estimate the best beam, using
a very small setup time, with a comparable performance to the exhaustive search
BF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05580</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05580</id><created>2015-05-20</created><authors><author><keyname>Farag</keyname><forenames>Hossam M.</forenames></author><author><keyname>Mohamed</keyname><forenames>Ehab Mahmoud</forenames></author></authors><title>Soft Decision Cooperative Spectrum Sensing Based Upon Noise Uncertainty
  Estimation</title><categories>cs.IT math.IT</categories><comments>6 Pages, 5 Figures, ICC workshops 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum Sensing (SS) constitutes the most critical task i n Cognitive Radio
(CR) systems for Primary User (PU) detection. Cooperative Spectrum Sensing
(CSS) is introduced to enhance the detection reliability of the PU in fading
environments. In this paper, we propose a soft decision based CSS algorithm
using energy detection by taking into account the noise uncertainty effect. In
the proposed algorithm, two threshold levels are utilized based on predicting
the current PU activity, which can be successfully expected using a simple
successive averaging process with time. The two threshold levels are evaluated
based on estimating the noise uncertainty factor. In addition, they are toggled
in a dynamic manner to compensate the noise uncertainty effect and to increase
the probability of detection and decrease the probability of false alarm.
Theoretical analysis is performed on the proposed algorithm to evaluate its
enhanced false alarm and detection probabilities over the conventional soft
decision CSS using different combining schemes. In addition, simulation results
show the high efficiency of the proposed scheme compared to the conventional
soft decision CSS, with high computational complexity enhancements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05586</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05586</id><created>2015-05-20</created><authors><author><keyname>Kipnis</keyname><forenames>Alon</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>The Distortion Rate Function of Cyclostationary Gaussian Processes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general expression for the distortion rate function (DRF) of a
cyclostationary Gaussian process in terms of its spectral properties is
derived. This DRF is given by water-filling over the eigenvalues of a spectral
density matrix associated with the polyphase components of the process. We use
this expression to derive, in a closed form, the DRF of processes obtained by
two important modulation techniques. We first show that the DRF of a
bandlimited narrow-band Gaussian stationary process modulated by a
deterministic sine-wave equals the DRF of the baseband process. We then show
that the DRF of a Gaussian stationary process modulated by a train of pulses is
given by reverse waterfilling over a PSD which is an aliased version of the
energy of the pulse and the base process. Moreover, this last result is used to
derive the indirect distortion-rate function of a stationary Gaussian process
given its sub-Nyquist samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05590</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05590</id><created>2015-05-20</created><authors><author><keyname>Liu</keyname><forenames>Yong-Jin</forenames></author><author><keyname>Xu</keyname><forenames>Chun-Xu</forenames></author><author><keyname>Fan</keyname><forenames>Dian</forenames></author><author><keyname>He</keyname><forenames>Ying</forenames></author></authors><title>Constructing Intrinsic Delaunay Triangulations from the Dual of Geodesic
  Voronoi Diagrams</title><categories>cs.CG cs.GR</categories><comments>32 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intrinsic Delaunay triangulation (IDT) is a fundamental data structure in
computational geometry and computer graphics. However, except for some
theoretical results, such as existence and uniqueness, little progress has been
made towards computing IDT on simplicial surfaces. To date the only way for
constructing IDTs is the edge-flipping algorithm, which iteratively flips the
non-Delaunay edge to be locally Delaunay. Although the algorithm is
conceptually simple and guarantees to stop in finite steps, it has no known
time complexity. Moreover, the edge-flipping algorithm may produce non-regular
triangulations, which contain self-loops and/or faces with only two edges. In
this paper, we propose a new method for constructing IDT on manifold triangle
meshes. Based on the duality of geodesic Voronoi diagrams, our method can
guarantee the resultant IDTs are regular. Our method has a theoretical
worst-case time complexity $O(n^2\log n)$ for a mesh with $n$ vertices. We
observe that most real-world models are far from their Delaunay triangulations,
thus, the edge-flipping algorithm takes many iterations to fix the non-Delaunay
edges. In contrast, our method is non-iterative and insensitive to the number
of non-Delaunay edges. Empirically, it runs in linear time $O(n)$ on real-world
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05599</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05599</id><created>2015-05-21</created><updated>2015-07-08</updated><authors><author><keyname>Bodwin</keyname><forenames>Greg</forenames></author><author><keyname>Williams</keyname><forenames>Virginia Vassilevska</forenames></author></authors><title>Better Distance Preservers and Additive Spanners</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We make improvements to the upper bounds on several popular types of distance
preserving graph sketches. These sketches are all various restrictions of the
{\em additive pairwise spanner} problem, in which one is given an undirected
unweighted graph $G$, a set of node pairs $P$, and an error allowance $+\beta$,
and one must construct a sparse subgraph $H$ satisfying $\delta_H(u, v) \le
\delta_G(u, v) + \beta$ for all $(u, v) \in P$.
  The first part of our paper concerns {\em pairwise distance preservers},
which make the restriction $\beta=0$ (i.e. distances must be preserved {\em
exactly}). Our main result here is an upper bound of $|H| = O(n^{2/3}|P|^{2/3}
+ n|P|^{1/3})$ when $G$ is undirected and unweighted. This improves on existing
bounds whenever $|P| = \omega(n^{3/4})$, and it is the first such improvement
in the last ten years.
  We then devise a new application of distance preservers to graph clustering
algorithms, and we apply this algorithm to {\em subset spanners}, which require
$P = S \times S$ for some node subset $S$, and {\em (standard) spanners}, which
require $P = V \times V$. For both of these objects, our construction
generalizes the best known bounds when the error allowance is constant, and we
obtain the strongest polynomial error/sparsity tradeoff that has yet been
reported (in fact, for subset spanners, ours is the {\em first} nontrivial
construction that enjoys improved sparsity from a polynomial error allowance).
  We leave open a conjecture that $O(n^{2/3}|P|^{2/3} + n)$ pairwise distance
preservers are possible for undirected unweighted graphs. Resolving this
conjecture in the affirmative would improve and simplify our upper bounds for
all the graph sketches mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05601</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05601</id><created>2015-05-21</created><authors><author><keyname>Happy</keyname><forenames>S L</forenames></author><author><keyname>Chatterjee</keyname><forenames>Swarnadip</forenames></author><author><keyname>Sheet</keyname><forenames>Debdoot</forenames></author></authors><title>Unsupervised Segmentation of Overlapping Cervical Cell Cytoplasm</title><categories>cs.CV</categories><comments>2 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Overlapping of cervical cells and poor contrast of cell cytoplasm are the
major issues in accurate detection and segmentation of cervical cells. An
unsupervised cell segmentation approach is presented here. Cell clump
segmentation was carried out using the extended depth of field (EDF) image
created from the images of different focal planes. A modified Otsu method with
prior class weights is proposed for accurate segmentation of nuclei from the
cell clumps. The cell cytoplasm was further segmented from cell clump depending
upon the number of nucleus detected in that cell clump. Level set model was
used for cytoplasm segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05610</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05610</id><created>2015-05-21</created><authors><author><keyname>Zhang</keyname><forenames>Wenkai</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author></authors><title>Extended fast search clustering algorithm: widely density clusters, no
  density peaks</title><categories>cs.DS</categories><comments>18 pages, 10 figures, DBDM 2015</comments><doi>10.5121/csit.2015.50701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CFSFDP (clustering by fast search and find of density peaks) is recently
developed density-based clustering algorithm. Compared to DBSCAN, it needs less
parameters and is computationally cheap for its non-iteration. Alex. at al have
demonstrated its power by many applications. However, CFSFDP performs not well
when there are more than one density peak for one cluster, what we name as &quot;no
density peaks&quot;. In this paper, inspired by the idea of a hierarchical
clustering algorithm CHAMELEON, we propose an extension of CFSFDP,E_CFSFDP, to
adapt more applications. In particular, we take use of original CFSFDP to
generating initial clusters first, then merge the sub clusters in the second
phase. We have conducted the algorithm to several data sets, of which, there
are &quot;no density peaks&quot;. Experiment results show that our approach outperforms
the original one due to it breaks through the strict claim of data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05612</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05612</id><created>2015-05-21</created><updated>2015-11-02</updated><authors><author><keyname>Gao</keyname><forenames>Haoyuan</forenames></author><author><keyname>Mao</keyname><forenames>Junhua</forenames></author><author><keyname>Zhou</keyname><forenames>Jie</forenames></author><author><keyname>Huang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Are You Talking to a Machine? Dataset and Methods for Multilingual Image
  Question Answering</title><categories>cs.CV cs.CL cs.LG</categories><comments>Dataset released on the project page, see
  http://idl.baidu.com/FM-IQA.html ; NIPS 2015 camera ready version</comments><acm-class>I.2.6; I.2.7; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the mQA model, which is able to answer questions
about the content of an image. The answer can be a sentence, a phrase or a
single word. Our model contains four components: a Long Short-Term Memory
(LSTM) to extract the question representation, a Convolutional Neural Network
(CNN) to extract the visual representation, an LSTM for storing the linguistic
context in an answer, and a fusing component to combine the information from
the first three components and generate the answer. We construct a Freestyle
Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate
our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese
question-answer pairs and their English translations. The quality of the
generated answers of our mQA model on this dataset is evaluated by human judges
through a Turing Test. Specifically, we mix the answers provided by humans and
our model. The human judges need to distinguish our model from the human. They
will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the
quality of the answer. We propose strategies to monitor the quality of this
evaluation process. The experiments show that in 64.7% of cases, the human
judges cannot distinguish our model from humans. The average score is 1.454
(1.918 for human). The details of this work, including the FM-IQA dataset, can
be found on the project page: http://idl.baidu.com/FM-IQA.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05613</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05613</id><created>2015-05-21</created><authors><author><keyname>de Vries</keyname><forenames>Christopher M.</forenames></author><author><keyname>De Vine</keyname><forenames>Lance</forenames></author><author><keyname>Geva</keyname><forenames>Shlomo</forenames></author><author><keyname>Nayak</keyname><forenames>Richi</forenames></author></authors><title>Parallel Streaming Signature EM-tree: A Clustering Algorithm for Web
  Scale Applications</title><categories>cs.IR cs.AI cs.DC</categories><comments>11 pages, WWW 2015</comments><acm-class>H.3.3; I.5.3; D.1.3</acm-class><doi>10.1145/2736277.2741111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of the web presents an unsolved problem of automatically
analyzing billions of pages of natural language. We introduce a scalable
algorithm that clusters hundreds of millions of web pages into hundreds of
thousands of clusters. It does this on a single mid-range machine using
efficient algorithms and compressed document representations. It is applied to
two web-scale crawls covering tens of terabytes. ClueWeb09 and ClueWeb12
contain 500 and 733 million web pages and were clustered into 500,000 to
700,000 clusters. To the best of our knowledge, such fine grained clustering
has not been previously demonstrated. Previous approaches clustered a sample
that limits the maximum number of discoverable clusters. The proposed EM-tree
algorithm uses the entire collection in clustering and produces several orders
of magnitude more clusters than the existing algorithms. Fine grained
clustering is necessary for meaningful clustering in massive collections where
the number of distinct topics grows linearly with collection size. These
fine-grained clusters show an improved cluster quality when assessed with two
novel evaluations using ad hoc search relevance judgments and spam
classifications for external validation. These evaluations solve the problem of
assessing the quality of clusters where categorical labeling is unavailable and
unfeasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05625</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05625</id><created>2015-05-21</created><updated>2015-05-22</updated><authors><author><keyname>Cheng</keyname><forenames>Chih-Hong</forenames></author><author><keyname>Guelfirat</keyname><forenames>Tuncay</forenames></author><author><keyname>Messinger</keyname><forenames>Christian</forenames></author><author><keyname>Schmitt</keyname><forenames>Johannes</forenames></author><author><keyname>Schnelte</keyname><forenames>Matthias</forenames></author><author><keyname>Weber</keyname><forenames>Peter</forenames></author></authors><title>Semantic Degrees for Industrie 4.0</title><categories>cs.SE</categories><comments>Timestamp of work-in-progress; the paper has been circulated within
  standardization units</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under the context of Industrie 4.0 (I4.0), future production systems provide
balanced operations between manufacturing flexibility and efficiency, realized
in an autonomous, horizontal, and decentralized item-level production control
framework. Structured interoperability via precise formulations on an
appropriate degree is crucial to achieve engineering efficiency in the system
life cycle. However, selecting the degree of formalization can be challenging,
as it crucially depends on the desired common understanding (semantic degree)
between multiple parties. In this paper, we categorize different semantic
degrees and map a set of technologies in industrial automation to their
associated degrees. Furthermore, we created guidelines to assist engineers
selecting appropriate semantic degrees in their design. We applied these
guidelines on publically available scenarios to examine the validity of the
approach, and identified semantic elements over internally developed use cases
targeting semantically-enabled plug-and-produce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05628</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05628</id><created>2015-05-21</created><updated>2015-10-02</updated><authors><author><keyname>Johnsen</keyname><forenames>Trygve</forenames></author><author><keyname>Shiromoto</keyname><forenames>Keisuke</forenames></author><author><keyname>Verdure</keyname><forenames>Hugues</forenames></author></authors><title>A generalization of Kung's theorem</title><categories>cs.IT cs.DM math.IT</categories><msc-class>94B05, 05E40</msc-class><doi>10.1007/s10623-015-0139-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a generalization of Kung's theorem on critical exponents of linear
codes over a finite field, in terms of sums of extended weight polynomials of
linear codes. For all i=k+1,...,n, we give an upper bound on the smallest
integer m such that there exist m codewords whose union of supports has
cardinality at least i.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05629</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05629</id><created>2015-05-21</created><updated>2016-02-12</updated><authors><author><keyname>Trac&#xe0;</keyname><forenames>Stefano</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Regulating Greed Over Time</title><categories>stat.ML cs.LG</categories><comments>Theorems, proofs, and experimental results</comments><msc-class>68Q32, 68T05, 91A60, 91A20</msc-class><acm-class>F.2.0; G.1.6; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In retail, there are predictable yet dramatic time-dependent patterns in
customer behavior, such as periodic changes in the number of visitors, or
increases in visitors just before major holidays (e.g., Christmas). The current
paradigm of multi-armed bandit analysis does not take these known patterns into
account, which means that despite the firm theoretical foundation of these
methods, they are fundamentally flawed when it comes to real applications. This
work provides a remedy that takes the time-dependent patterns into account, and
we show how this remedy is implemented in the UCB and {\epsilon}-greedy
methods. In the corrected methods, exploitation (greed) is regulated over time,
so that more exploitation occurs during higher reward periods, and more
exploration occurs in periods of low reward. In order to understand why regret
is reduced with the corrected methods, we present a set of bounds that provide
insight into why we would want to exploit during periods of high reward, and
discuss the impact on regret. Our proposed methods have excellent performance
in experiments, and were inspired by a high-scoring entry in the Exploration
and Exploitation 3 contest using data from Yahoo! Front Page. That entry
heavily used time-series methods to regulate greed over time, which was
substantially more effective than other contextual bandit methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05630</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05630</id><created>2015-05-21</created><authors><author><keyname>Bodwin</keyname><forenames>Greg</forenames></author><author><keyname>Williams</keyname><forenames>Virginia Vassilevska</forenames></author></authors><title>Very Sparse Additive Spanners and Emulators</title><categories>cs.DS</categories><comments>Very Sparse Additive Spanners and Emulators. 6th ITCS, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain new upper bounds on the additive distortion for graph emulators and
spanners on relatively few edges. We introduce a new subroutine called &quot;strip
creation,&quot; and we combine this subroutine with several other ideas to obtain
the following results:
  \item Every graph has a spanner on $O(n^{1+\epsilon})$ edges with
$\tilde{O}(n^{1/2 - \epsilon/2})$ additive distortion, for arbitrary
$\epsilon\in [0,1]$. \item Every graph has an emulator on $\tilde{O}(n^{1 +
\epsilon})$ edges with $\tilde{O}(n^{1/3 - 2\epsilon/3})$ additive distortion
whenever $\epsilon \in [0, \frac{1}{5}]$. \item Every graph has a spanner on
$\tilde{O}(n^{1 + \epsilon})$ edges with $\tilde{O}(n^{2/3 - 5\epsilon/3})$
additive distortion whenever $\epsilon \in [0, \frac{1}{4}]$.
  Our first spanner has the new best known asymptotic edge-error tradeoff for
additive spanners whenever $\epsilon \in [0, \frac{1}{7}]$. Our second spanner
has the new best tradeoff whenever $\epsilon \in [\frac{1}{7}, \frac{3}{17}]$.
Our emulator has the new best asymptotic edge-error tradeoff whenever $\epsilon
\in [0, \frac{1}{5}]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05637</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05637</id><created>2015-05-21</created><updated>2015-10-31</updated><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Pemantle</keyname><forenames>Robin</forenames></author></authors><title>Corruption Detection on Networks</title><categories>math.CO cs.DS cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of corruption detection on networks. In this model
each vertex of a directed graph can be either truthful or corrupt. Each vertex
reports about the types (truthful or corrupt) of all his out-neighbors. If he
is truthful, he reports the truth, whereas if he is corrupt he reports
adversarially. This model, considered by Preparata, Metze, and Chien (1967)
motivated by the desire to identify the faulty components of a digital system
by having the other components checking them, became known as the PMC model.
The main known results for this model characterize networks in which {\em all}
corrupt (that is, faulty) vertices can be identified, when there is a known
upper bound on their number. We are interested in the investigation of networks
in which {\em most} of the corrupt vertices can be identified. We show that the
main relevant parameter here is graph expansion. This implies that in contrast
to the known results about the PMC model that imply that in order to identify
all corrupt vertices when their number is $t$ all indegrees have to be at least
$t$, there are bounded degree graphs in which almost all corrupt and almost all
truthful vertices can be identified, whenever there is a majority of truthful
vertices. We also show that expansion is necessary for obtaining such a
corruption detection and discuss algorithms and the computational hardness of
the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05641</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05641</id><created>2015-05-21</created><authors><author><keyname>Su</keyname><forenames>Hao</forenames></author><author><keyname>Qi</keyname><forenames>Charles R.</forenames></author><author><keyname>Li</keyname><forenames>Yangyan</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas</forenames></author></authors><title>Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with
  Rendered 3D Model Views</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object viewpoint estimation from 2D images is an essential task in computer
vision. However, two issues hinder its progress: scarcity of training data with
viewpoint annotations, and a lack of powerful features. Inspired by the growing
availability of 3D models, we propose a framework to address both issues by
combining render-based image synthesis and CNNs. We believe that 3D models have
the potential in generating a large number of images of high variation, which
can be well exploited by deep CNN with a high learning capacity. Towards this
goal, we propose a scalable and overfit-resistant image synthesis pipeline,
together with a novel CNN specifically tailored for the viewpoint estimation
task. Experimentally, we show that the viewpoint estimation from our pipeline
can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05642</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05642</id><created>2015-05-21</created><authors><author><keyname>Mahalakshmi</keyname><forenames>J.</forenames></author><author><keyname>Durairajan</keyname><forenames>C.</forenames></author></authors><title>On the $Z_q$-MacDonald Code and its Weight Distribution of dimension 3</title><categories>cs.IT math.IT</categories><msc-class>Primary: 94B05, Secondary: 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we determine the parameters of $\mathbb{Z}_q$-MacDonald Code
of dimension k for any positive integer $q \geq 2.$ Further, we have obtained
the weight distribution of $\mathbb{Z}_q$-MacDonald code of dimension 3 and
furthermore, we have given the weight distribution of $\mathbb{Z}_q$-Simplex
code of dimension 3 for any positive integer $q \geq 2.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05643</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05643</id><created>2015-05-21</created><authors><author><keyname>Aldoma</keyname><forenames>Aitor</forenames></author><author><keyname>Prankl</keyname><forenames>Johann</forenames></author><author><keyname>Svejda</keyname><forenames>Alexander</forenames></author><author><keyname>Vincze</keyname><forenames>Markus</forenames></author></authors><title>Object Modelling with a Handheld RGB-D Camera</title><categories>cs.CV</categories><comments>Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</comments><report-no>OAGM/2015/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a flexible system to reconstruct 3D models of objects
captured with an RGB-D sensor. A major advantage of the method is that our
reconstruction pipeline allows the user to acquire a full 3D model of the
object. This is achieved by acquiring several partial 3D models in different
sessions that are automatically merged together to reconstruct a full model. In
addition, the 3D models acquired by our system can be directly used by
state-of-the-art object instance recognition and object tracking modules,
providing object-perception capabilities for different applications, such as
human-object interaction analysis or robot grasping. The system does not impose
constraints in the appearance of objects (textured, untextured) nor in the
modelling setup (moving camera with static object or a turn-table setup). The
proposed reconstruction system has been used to model a large number of objects
resulting in metrically accurate and visually appealing 3D models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05644</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05644</id><created>2015-05-21</created><authors><author><keyname>Louca</keyname><forenames>Marianna</forenames></author><author><keyname>Vogiatzakis</keyname><forenames>Ioannis N.</forenames></author><author><keyname>Moustakas</keyname><forenames>Aristides</forenames></author></authors><title>Modelling the combined effects of land use and climatic changes:
  coupling bioclimatic modelling with markov-chain cellular automata in a case
  study in Cyprus</title><categories>q-bio.PE cs.CY cs.DM stat.AP</categories><comments>to appear (in press in Ecological Informatics (2015))</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two endemic plant species in the Mediterranean island of Cyprus, Crocus
cyprius and Ophrys kotschyi, were used as a case study. We have coupled climate
change scenarios, and land use change models with species distribution models.
Future land use scenarios were modelled by initially calculating the rate of
current land use changes between two time snapshots (2000 and 2006) on the
island, and based on these transition probabilities markov-chain cellular
automata were used to generate future land use changes for 2050. Climate change
scenarios A1B, A2, B1 and B2A were derived from the IPCC reports. Species
climatic preferences were derived from their current distributions using
classification trees while habitats preferences were derived from the Red Data
Book of the Flora of Cyprus. A bioclimatic model for Crocus cyprius was built
using mean temperature of wettest quarter, max temperature of warmest month and
precipitation seasonality, while for Ophrys kotchyi the bioclimatic model was
built using precipitation of wettest month, mean temperature of warmest
quarter, isothermality, precipitation of coldest quarter, and annual
precipitation. Sequentially, simulation scenarios were performed regarding
future species distributions by accounting climate alone and both climate and
land use changes. The distribution of the two species resulting from the
bioclimatic models was then filtered by future land use changes, providing the
species projected potential distribution. The species projected potential
distribution varies depending on the type and scenario used, but many of both
species current sites/locations are projected to be outside their future
potential distribution. Our results demonstrate the importance of including
both land use and climatic changes in predictive species modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05646</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05646</id><created>2015-05-21</created><authors><author><keyname>Bourke</keyname><forenames>Timothy</forenames><affiliation>INRIA</affiliation></author><author><keyname>van Glabbeek</keyname><forenames>Robert J.</forenames><affiliation>NICTA</affiliation></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames><affiliation>NICTA</affiliation></author></authors><title>A mechanized proof of loop freedom of the (untimed) AODV routing
  protocol</title><categories>cs.NI cs.LO</categories><comments>The Isabelle/HOL source files, and a full proof document, are
  available in the Archive of Formal Proofs, at
  http://afp.sourceforge.net/entries/AODV.shtml</comments><journal-ref>Proc. Automated Technology for Verification and Analysis, ATVA
  2014 (F. Cassez and J.-F. Raskin, eds.), LNCS 8837, Springer, 2014, pp. 47-63</journal-ref><doi>10.1007/978-3-319-11936-6_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ad hoc On-demand Distance Vector (AODV) routing protocol allows the nodes
in a Mobile Ad hoc Network (MANET) or a Wireless Mesh Network (WMN) to know
where to forward data packets. Such a protocol is 'loop free' if it never leads
to routing decisions that forward packets in circles. This paper describes the
mechanization of an existing pen-and-paper proof of loop freedom of AODV in the
interactive theorem prover Isabelle/HOL. The mechanization relies on a novel
compositional approach for lifting invariants to networks of nodes. We exploit
the mechanization to analyse several improvements of AODV and show that
Isabelle/HOL can re-establish most proof obligations automatically and identify
exactly the steps that are no longer valid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05655</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05655</id><created>2015-05-21</created><authors><author><keyname>Banerjee</keyname><forenames>Poorna</forenames></author><author><keyname>Dave</keyname><forenames>Amit</forenames></author></authors><title>GPGPU Based Parallelized Client-Server Framework for Providing High
  Performance Computation Support</title><categories>cs.DC</categories><comments>6 pages, Published with International Journal of Computer Science &amp;
  Technology (IJCST)- Vol 4 Issue 1 Jan - March 2013</comments><journal-ref>International Journal of Computer Science and Technology (IJCST)
  V4(1):508-513, Jan - March 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel data processing has become indispensable for processing applications
involving huge data sets. This brings into focus the Graphics Processing Units
(GPUs) which emphasize on many-core computing. With the advent of General
Purpose GPUs (GPGPU), applications not directly associated with graphics
operations can also harness the computation capabilities of GPUs. Hence, it
would be beneficial if the computing capabilities of a given GPGPU could be
task optimized and made available. This paper describes a client-server
framework in which users can choose a processing task and submit large
data-sets for processing to a remote GPGPU and receive the results back, using
well defined interfaces. The framework provides extensibility in terms of the
number and type of tasks that the client can choose or submit for processing at
the remote GPGPU server machine, with complete transparency to the underlying
hardware and operating systems. Parallelization of user-submitted tasks on the
GPGPU has been achieved using NVIDIA Compute Unified Device Architecture
(CUDA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05657</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05657</id><created>2015-05-21</created><authors><author><keyname>Guille</keyname><forenames>Adrien</forenames></author><author><keyname>Favre</keyname><forenames>Cecile</forenames></author></authors><title>Event detection, tracking, and visualization in Twitter: a
  mention-anomaly-based approach</title><categories>cs.SI</categories><comments>17 pages</comments><journal-ref>Social Network Analysis and Mining, vol. 5, iss. 1, 2015</journal-ref><doi>10.1007/s13278-015-0258-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ever-growing number of people using Twitter makes it a valuable source of
timely information. However, detecting events in Twitter is a difficult task,
because tweets that report interesting events are overwhelmed by a large volume
of tweets on unrelated topics. Existing methods focus on the textual content of
tweets and ignore the social aspect of Twitter. In this paper we propose MABED
(i.e. mention-anomaly-based event detection), a novel statistical method that
relies solely on tweets and leverages the creation frequency of dynamic links
(i.e. mentions) that users insert in tweets to detect significant events and
estimate the magnitude of their impact over the crowd. MABED also differs from
the literature in that it dynamically estimates the period of time during which
each event is discussed, rather than assuming a predefined fixed duration for
all events. The experiments we conducted on both English and French Twitter
data show that the mention-anomaly-based approach leads to more accurate event
detection and improved robustness in presence of noisy Twitter content.
Qualitatively speaking, we find that MABED helps with the interpretation of
detected events by providing clear textual descriptions and precise temporal
descriptions. We also show how MABED can help understanding users' interest.
Furthermore, we describe three visualizations designed to favor an efficient
exploration of the detected events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05663</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05663</id><created>2015-05-21</created><authors><author><keyname>Pouget-Abadie</keyname><forenames>Jean</forenames></author><author><keyname>Horel</keyname><forenames>Thibaut</forenames></author></authors><title>Inferring Graphs from Cascades: A Sparse Recovery Framework</title><categories>cs.SI cs.LG stat.ML</categories><comments>Full version of the ICML paper with the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Network Inference problem, one seeks to recover the edges of an
unknown graph from the observations of cascades propagating over this graph. In
this paper, we approach this problem from the sparse recovery perspective. We
introduce a general model of cascades, including the voter model and the
independent cascade model, for which we provide the first algorithm which
recovers the graph's edges with high probability and $O(s\log m)$ measurements
where $s$ is the maximum degree of the graph and $m$ is the number of nodes.
Furthermore, we show that our algorithm also recovers the edge weights (the
parameters of the diffusion process) and is robust in the context of
approximate sparsity. Finally we prove an almost matching lower bound of
$\Omega(s\log\frac{m}{s})$ and validate our approach empirically on synthetic
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05667</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05667</id><created>2015-05-21</created><authors><author><keyname>Zhu</keyname><forenames>Chenxi</forenames></author><author><keyname>Qiu</keyname><forenames>Xipeng</forenames></author><author><keyname>Chen</keyname><forenames>Xinchi</forenames></author><author><keyname>Huang</keyname><forenames>Xuanjing</forenames></author></authors><title>A Re-ranking Model for Dependency Parser with Recursive Convolutional
  Neural Network</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the problem to model all the nodes (words or
phrases) in a dependency tree with the dense representations. We propose a
recursive convolutional neural network (RCNN) architecture to capture syntactic
and compositional-semantic representations of phrases and words in a dependency
tree. Different with the original recursive neural network, we introduce the
convolution and pooling layers, which can model a variety of compositions by
the feature maps and choose the most informative compositions by the pooling
layers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list
of candidate dependency parsing trees. The experiments show that RCNN is very
effective to improve the state-of-the-art dependency parsing on both English
and Chinese datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05695</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05695</id><created>2015-05-21</created><updated>2015-06-16</updated><authors><author><keyname>Antu&#xf1;a</keyname><forenames>Laura</forenames></author><author><keyname>Araiza-Illan</keyname><forenames>Dejanira</forenames></author><author><keyname>Campos</keyname><forenames>S&#xe9;rgio</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Symmetry Reduction Enables Model Checking of More Complex Emergent
  Behaviours of Swarm Navigation Algorithms</title><categories>cs.RO</categories><comments>Accepted for presentation in Towards Autonomous Robotic Systems
  (TAROS) 2015, Liverpool, UK</comments><doi>10.1007/978-3-319-22416-9_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergent global behaviours of robotic swarms are important to achieve
their navigation task goals. These emergent behaviours can be verified to
assess their correctness, through techniques like model checking. Model
checking exhaustively explores all possible behaviours, based on a discrete
model of the system, such as a swarm in a grid. A common problem in model
checking is the state-space explosion that arises when the states of the model
are numerous. We propose a novel implementation of symmetry reduction, in the
form of encoding navigation algorithms relatively with respect to a reference,
based on the symmetrical properties of swarms in grids. We applied the relative
encoding to a swarm navigation algorithm, Alpha, modelled for the NuSMV model
checker. A comparison of the state-space and verification results with an
absolute (or global) and a relative encoding of the Alpha algorithm highlights
the advantages of our approach, allowing model checking larger grid sizes and
number of robots, and consequently, verifying more complex emergent behaviours.
For example, a property was verified for a grid with 3 robots and a maximum
allowed size of 8x8 cells in a global encoding, whereas this size was increased
to 16x16 using a relative encoding. Also, the time to verify a property for a
swarm of 3 robots in a 6x6 grid was reduced from almost 10 hours to only 7
minutes. Our approach is transferable to other swarm navigation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05697</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05697</id><created>2015-05-21</created><authors><author><keyname>Barenboim</keyname><forenames>Leonid</forenames></author><author><keyname>Elkin</keyname><forenames>Michael</forenames></author><author><keyname>Gavoille</keyname><forenames>Cyril</forenames></author></authors><title>A Fast Network-Decomposition Algorithm and its Applications to
  Constant-Time Distributed Computation</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partition $(C_1,C_2,...,C_q)$ of $G = (V,E)$ into clusters of strong
(respectively, weak) diameter $d$, such that the supergraph obtained by
contracting each $C_i$ is $\ell$-colorable is called a strong (resp., weak)
$(d, \ell)$-network-decomposition. Network-decompositions were introduced in a
seminal paper by Awerbuch, Goldberg, Luby and Plotkin in 1989. Awerbuch et al.
showed that strong $(exp\{O(\sqrt{ \log n \log \log n})\}, exp\{O(\sqrt{ \log n
\log \log n})\})$-network-decompositions can be computed in distributed
deterministic time $exp\{O(\sqrt{ \log n \log \log n})\}$.
  The result of Awerbuch et al. was improved by Panconesi and Srinivasan in
1992: in the latter result $d = \ell = exp\{O(\sqrt{\log n})\}$, and the
running time is $exp\{O(\sqrt{\log n})\}$ as well. Much more recently Barenboim
(2012) devised a distributed randomized constant-time algorithm for computing
strong network decompositions with $d = O(1)$. However, the parameter $\ell$ in
his result is $O(n^{1/2 + \epsilon})$.
  In this paper we drastically improve the result of Barenboim and devise a
distributed randomized constant-time algorithm for computing strong $(O(1),
O(n^{\epsilon}))$-network-decompositions. As a corollary we derive a
constant-time randomized $O(n^{\epsilon})$-approximation algorithm for the
distributed minimum coloring problem, improving the previously best-known
$O(n^{1/2 + \epsilon})$ approximation guarantee. We also derive other improved
distributed algorithms for a variety of problems.
  Most notably, for the extremely well-studied distributed minimum dominating
set problem currently there is no known deterministic polylogarithmic-time
algorithm. We devise a {deterministic} polylogarithmic-time approximation
algorithm for this problem, addressing an open problem of Lenzen and
Wattenhofer (2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05699</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05699</id><created>2015-05-21</created><updated>2015-05-26</updated><authors><author><keyname>Araiza-Illan</keyname><forenames>Dejanira</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author><author><keyname>Richards</keyname><forenames>Arthur</forenames></author></authors><title>Verification of Control Systems Implemented in Simulink with Assertion
  Checks and Theorem Proving: A Case Study</title><categories>cs.SY cs.SE</categories><comments>Accepted, waiting for publication. European Control Conference, July
  2015, Linz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the verification of control systems implemented in
Simulink. The goal is to ensure that high-level requirements on control
performance, like stability, are satisfied by the Simulink diagram. A two stage
process is proposed. First, the high-level requirements are decomposed into
specific parametrized sub-requirements and implemented as assertions in
Simulink. Second, the verification takes place. On one hand, the
sub-requirements are verified through assertion checks in simulation. On the
other hand, according to their scope, some of the sub-requirements are verified
through assertion checks in simulation, and others via automatic theorem
proving over an ideal mathematical model of the diagram. We compare performing
only assertion checks against the use of theorem proving, to highlight the
advantages of the latter. Theorem proving performs verification by computing a
mathematical proof symbolically, covering the entire state space of the
variables. An automatic translation tool from Simulink to the language of the
theorem proving tool Why3 is also presented. The paper demonstrates our
approach by verifying the stability of a simple discrete linear system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05717</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05717</id><created>2015-05-21</created><authors><author><keyname>S&#xf8;rensen</keyname><forenames>Jesper H.</forenames></author><author><keyname>de Carvalho</keyname><forenames>Elisabeth</forenames></author></authors><title>Pilot Decontamination Through Pilot Sequence Hopping in Massive MIMO
  Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns wireless cellular networks applying massive multiple-input
multiple-output (MIMO) technology. In such a system, the base station in a
given cell is equipped with a very large number (hundreds or even thousands) of
antennas and serves multiple users. Estimation of the channel from the base
station to each user is performed at the base station using an uplink pilot
sequence. Such a channel estimation procedure suffers from pilot contamination.
Orthogonal pilot sequences are used in a given cell but, due to the shortage of
orthogonal sequences, the same pilot sequences must be reused in neighboring
cells, causing pilot contamination. The solution presented in this paper
suppresses pilot contamination, without the need for coordination among cells.
Pilot sequence hopping is performed at each transmission slot, which provides a
randomization of the pilot contamination. Using a modified Kalman filter, it is
shown that such randomized contamination can be significantly suppressed.
Comparisons with conventional estimation methods show that the mean squared
error can be lowered as much as an order of magnitude at low mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05723</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05723</id><created>2015-05-21</created><authors><author><keyname>Zliobaite</keyname><forenames>Indre</forenames></author></authors><title>On the relation between accuracy and fairness in binary classification</title><categories>cs.LG cs.AI</categories><comments>Accepted for presentation to the 2nd workshop on Fairness,
  Accountability, and Transparency in Machine Learning (http://www.fatml.org/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our study revisits the problem of accuracy-fairness tradeoff in binary
classification. We argue that comparison of non-discriminatory classifiers
needs to account for different rates of positive predictions, otherwise
conclusions about performance may be misleading, because accuracy and
discrimination of naive baselines on the same dataset vary with different rates
of positive predictions. We provide methodological recommendations for sound
comparison of non-discriminatory classifiers, and present a brief theoretical
and empirical analysis of tradeoffs between accuracy and non-discrimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05726</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05726</id><created>2015-05-21</created><authors><author><keyname>S&#xf8;rensen</keyname><forenames>Jesper H.</forenames></author><author><keyname>de Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Massive MIMO for Crowd Scenarios: A Solution Based on Random Access</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach to intra-cell pilot contamination in
crowded massive MIMO scenarios. The approach relies on two essential properties
of a massive MIMO system, namely near-orthogonality between user channels and
near-stability of channel powers. Signal processing techniques that take
advantage of these properties allow us to view a set of contaminated pilot
signals as a graph code on which iterative belief propagation can be performed.
This makes it possible to decontaminate pilot signals and increase the
throughput of the system. The proposed solution exhibits high performance with
large improvements over the conventional method. The improvements come at the
price of an increased error rate, although this effect is shown to decrease
significantly for increasing number of antennas at the base station.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05735</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05735</id><created>2015-05-21</created><authors><author><keyname>Hanif</keyname><forenames>Muhammad Fainan</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharmalingam</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>A Minorization-Maximization Method for Optimizing Sum Rate in
  Non-Orthogonal Multiple Access Systems</title><categories>cs.IT math.IT</categories><comments>Submitted for journal publication</comments><journal-ref>IEEE Transactions on Signal Processing, vol.64, no.1, pp.76-88,
  Jan.1, 2016</journal-ref><doi>10.1109/TSP.2015.2480042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-orthogonal multiple access (NOMA) systems have the potential to deliver
higher system throughput, compared to contemporary orthogonal multiple access
techniques. For a linearly precoded multiple-input multiple-output (MISO)
system, we study the downlink sum rate maximization problem, when the NOMA
principles are applied. Being a non-convex and intractable optimization
problem,we resort to approximate it with a minorization-maximization algorithm
(MMA), which is a widely used tool in statistics. In each step of the MMA, we
solve a second-order cone program, such that the feasibility set in each step
contains that of the previous one, and is always guaranteed to be a subset of
the feasibility set of the original problem. It should be noted that the
algorithm takes a few iterations to converge. Furthermore, we study the
conditions under which the achievable rates maximization can be further
simplified to a low complexity design problem, and we compute the probability
of occurrence of this event. Numerical examples are conducted to show a
comparison of the proposed approach against conventional multiple access
systems. NOMA is reported to provide better spectral and power efficiency with
a polynomial time computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05736</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05736</id><created>2015-05-21</created><authors><author><keyname>Kang</keyname><forenames>Suwon</forenames></author><author><keyname>Han</keyname><forenames>Sungmin</forenames></author><author><keyname>Cho</keyname><forenames>Seungik</forenames></author><author><keyname>Jang</keyname><forenames>Donghyuk</forenames></author><author><keyname>Choi</keyname><forenames>Hyuk</forenames></author><author><keyname>Choi</keyname><forenames>Ji-Woong</forenames></author></authors><title>High Speed CAN Transmission Scheme Supporting Data Rate of over 100 Mbps</title><categories>cs.IT math.IT</categories><comments>7 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the number of electronic components in the car increases, the requirement
for the higher data transmission scheme among them is on the sharp rise.
Controller area network (CAN) has been widely adopted to support the in-car
communications needs but the data rate is far below what other schemes such as
Ethernet and optical fibers can offer. A new scheme for enhancing the speed of
CAN network has been proposed, where carrier modulated signal is introduced on
top of the existing CAN signal whereby the data rate can be enhanced over
100Mbps. The proposed scheme is compatible with the existing CAN network and
accordingly enables seamless upgrade of the existing network to support high
speed demand using CAN protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05737</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05737</id><created>2015-05-14</created><updated>2015-07-10</updated><authors><author><keyname>Tran</keyname><forenames>Ngoc Mai</forenames></author><author><keyname>Yu</keyname><forenames>Josephine</forenames></author></authors><title>Product-Mix Auctions and Tropical Geometry</title><categories>cs.GT math.CO q-fin.EC</categories><comments>23 pages, 2 figures. Exposition improved and typographical errors
  fixed. Proof of Proposition 8 is split into two lemmas</comments><msc-class>91B26, 14T05, 52B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent and ongoing work, Baldwin and Klemperer explored a connection
between tropical geometry and economics. They gave a sufficient condition for
the existence of competitive equilibrium in product-mix auctions of indivisible
goods. This result, which we call the Unimodularity Theorem, can also be traced
back to the work of Danilov, Koshevoy, and Murota. We introduce auction theory
for a mathematical audience and give two simple proofs of the Unimodularity
Theorem --- one via tropical and discrete geometry and one via integer
programming. We also discuss connections to Walrasian economies, stable
matching with transferable utilities, the theory of discrete convex analysis,
and some open problems on lattice polytopes such as the Oda conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05740</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05740</id><created>2015-05-21</created><authors><author><keyname>Lerouge</keyname><forenames>Julien</forenames></author><author><keyname>Abu-Aisheh</keyname><forenames>Zeina</forenames></author><author><keyname>Raveaux</keyname><forenames>Romain</forenames></author><author><keyname>H&#xe9;roux</keyname><forenames>Pierre</forenames></author><author><keyname>Adam</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Graph edit distance : a new binary linear programming formulation</title><categories>cs.DS cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph edit distance (GED) is a powerful and flexible graph matching paradigm
that can be used to address different tasks in structural pattern recognition,
machine learning, and data mining. In this paper, some new binary linear
programming formulations for computing the exact GED between two graphs are
proposed. A major strength of the formulations lies in their genericity since
the GED can be computed between directed or undirected fully attributed graphs
(i.e. with attributes on both vertices and edges). Moreover, a relaxation of
the domain constraints in the formulations provides efficient lower bound
approximations of the GED. A complete experimental study comparing the proposed
formulations with 4 state-of-the-art algorithms for exact and approximate graph
edit distances is provided. By considering both the quality of the proposed
solution and the efficiency of the algorithms as performance criteria, the
results show that none of the compared methods dominates the others in the
Pareto sense. As a consequence, faced to a given real-world problem, a
trade-off between quality and efficiency has to be chosen w.r.t. the
application constraints. In this context, this paper provides a guide that can
be used to choose the appropriate method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05747</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05747</id><created>2015-05-21</created><authors><author><keyname>Leibfried</keyname><forenames>Thomas</forenames></author><author><keyname>Mchedlidze</keyname><forenames>Tamara</forenames></author><author><keyname>Meyer-H&#xfc;bner</keyname><forenames>Nico</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author><author><keyname>Wegner</keyname><forenames>Franziska</forenames></author></authors><title>Operating Power Grids with Few Flow Control Buses</title><categories>cs.SY</categories><comments>extended version of an ACM e-Energy 2015 poster/workshop paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future power grids will offer enhanced controllability due to the increased
availability of power flow control units (FACTS). As the installation of
control units in the grid is an expensive investment, we are interested in
using few controllers to achieve high controllability. In particular, two
questions arise: How many flow control buses are necessary to obtain globally
optimal power flows? And if fewer flow control buses are available, what can we
achieve with them? Using steady state IEEE benchmark data sets, we explore
experimentally that already a small number of controllers placed at certain
grid buses suffices to achieve globally optimal power flows. We present a
graph-theoretic explanation for this behavior. To answer the second question we
perform a set of experiments that explore the existence and costs of feasible
power flow solutions at increased loads with respect to the number of flow
control buses in the grid. We observe that adding a small number of flow
control buses reduces the flow costs and extends the existence of feasible
solutions at increased load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05753</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05753</id><created>2015-05-21</created><authors><author><keyname>Shcherbatyi</keyname><forenames>Iaroslav</forenames></author><author><keyname>Bulling</keyname><forenames>Andreas</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>GazeDPM: Early Integration of Gaze Information in Deformable Part Models</title><categories>cs.CV cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of works explore collaborative human-computer systems in
which human gaze is used to enhance computer vision systems. For object
detection these efforts were so far restricted to late integration approaches
that have inherent limitations, such as increased precision without increase in
recall. We propose an early integration approach in a deformable part model,
which constitutes a joint formulation over gaze and visual data. We show that
our GazeDPM method improves over the state-of-the-art DPM baseline by 4% and a
recent method for gaze-supported object detection by 3% on the public POET
dataset. Our approach additionally provides introspection of the learnt models,
can reveal salient image structures, and allows us to investigate the interplay
between gaze attracting and repelling areas, the importance of view-specific
models, as well as viewers' personal biases in gaze patterns. We finally study
important practical aspects of our approach, such as the impact of using
saliency maps instead of real fixations, the impact of the number of fixations,
as well as robustness to gaze estimation error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05755</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05755</id><created>2015-05-21</created><authors><author><keyname>Anane</keyname><forenames>Rajoua</forenames></author><author><keyname>Raoof</keyname><forenames>Kosai</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>On the Evaluation of GMSK Scheme with ECC Techniques in Wireless Sensor
  Network</title><categories>cs.NI cs.IT math.IT</categories><comments>12 pages, International Journal of Wireless &amp; Mobile Networks (IJWMN)
  Vol. 7, No. 2 April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor nodes are powered by batteries, for which replacement is very
difficult. That is why, optimization of energy consumption is a major objective
in the area of wireless sensor networks (WSNs).On the other hand, noisy channel
has a prominent influence on the reliability of data transmission. Therefore,
an energy efficient transmission strategy should be considered on the
communication process of wireless nodes in order to obtain optimal energy
network consumption. Indeed, the choice of suitable modulation format with the
proper Error Correcting code (ECC) played a great responsibility to obtain
better energy conservation.In this work, we aim to evaluate the performance
analysis of Gaussian Minimum Shift Keying (GMSK) modulation with several
combinations of coding strategies using various analysis metrics such as Bit
Error Rate (BER), energy consumption.Through extensive simulation, we disclose
that he gain achieved through GMSK modulation with suitable channel coding
mechanism is promising to obtain reliable communication and energy conservation
in WSN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05768</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05768</id><created>2015-05-21</created><updated>2015-05-25</updated><authors><author><keyname>Merelli</keyname><forenames>Emanuela</forenames></author><author><keyname>Rucco</keyname><forenames>Matteo</forenames></author><author><keyname>Sloot</keyname><forenames>Peter</forenames></author><author><keyname>Tesei</keyname><forenames>Luca</forenames></author></authors><title>Topological characterization of S[B] systems: From data to models of
  complexity</title><categories>cs.ET</categories><comments>26 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a methodology for deriving a model of a complex
system by exploiting the information extracted from Topological Data Analysis.
Central to our approach is the S[B] paradigm in which a complex system is
represented by a two-level model. One level, the structural S one, is derived
using the newly introduced quantitative concept of Persistent Entropy. The
other level, the behavioral B one, is characterized by a network of interacting
computational agents described by a Higher Dimensional Automaton. The
methodology yields also a representation of the evolution of the derived
two-level model as a Persistent Entropy Automaton. The presented methodology is
applied to a real case study, the Idiotypic Network of the mammal immune
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05769</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05769</id><created>2015-05-21</created><authors><author><keyname>Misra</keyname><forenames>Ishan</forenames></author><author><keyname>Shrivastava</keyname><forenames>Abhinav</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author></authors><title>Watch and Learn: Semi-Supervised Learning of Object Detectors from
  Videos</title><categories>cs.CV</categories><comments>To appear in CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a semi-supervised approach that localizes multiple unknown object
instances in long videos. We start with a handful of labeled boxes and
iteratively learn and label hundreds of thousands of object instances. We
propose criteria for reliable object detection and tracking for constraining
the semi-supervised learning process and minimizing semantic drift. Our
approach does not assume exhaustive labeling of each object instance in any
single frame, or any explicit annotation of negative data. Working in such a
generic setting allow us to tackle multiple object instances in video, many of
which are static. In contrast, existing approaches either do not consider
multiple object instances per video, or rely heavily on the motion of the
objects present. The experiments demonstrate the effectiveness of our approach
by evaluating the automatically labeled data on a variety of metrics like
quality, coverage (recall), diversity, and relevance to training an object
detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05770</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05770</id><created>2015-05-21</created><updated>2015-06-22</updated><authors><author><keyname>Rezende</keyname><forenames>Danilo Jimenez</forenames></author><author><keyname>Mohamed</keyname><forenames>Shakir</forenames></author></authors><title>Variational Inference with Normalizing Flows</title><categories>stat.ML cs.AI cs.LG stat.CO stat.ME</categories><comments>Proceedings of the 32nd International Conference on Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The choice of approximate posterior distribution is one of the core problems
in variational inference. Most applications of variational inference employ
simple families of posterior approximations in order to allow for efficient
inference, focusing on mean-field or other simple structured approximations.
This restriction has a significant impact on the quality of inferences made
using variational methods. We introduce a new approach for specifying flexible,
arbitrarily complex and scalable approximate posterior distributions. Our
approximations are distributions constructed through a normalizing flow,
whereby a simple initial density is transformed into a more complex one by
applying a sequence of invertible transformations until a desired level of
complexity is attained. We use this view of normalizing flows to develop
categories of finite and infinitesimal flows and provide a unified view of
approaches for constructing rich posterior approximations. We demonstrate that
the theoretical advantages of having posteriors that better match the true
posterior, combined with the scalability of amortized variational approaches,
provides a clear improvement in performance and applicability of variational
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05772</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05772</id><created>2015-05-21</created><authors><author><keyname>Hernandez</keyname><forenames>Bernardo</forenames><affiliation>PhD student at the Department of Automatic Control and Systems Engineering, The University of Sheffield</affiliation></author><author><keyname>Trodden</keyname><forenames>Paul</forenames><affiliation>Department of Automatic Control and Systems Engineering, The University of Sheffield</affiliation></author></authors><title>Persistently Exciting Tube MPC</title><categories>cs.SY math.OC</categories><comments>Intended to be submitted to the American Control Conference 2016</comments><acm-class>I.2.8; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach to deal with the dual problem of system
identification and regulation. The main feature consists of breaking the
control input to the system into a regulator part and a persistently exciting
part. The former is used to regulate the plant using a robust MPC formulation,
in which the latter is treated as a bounded additive disturbance. The
identification process is executed by a simple recursive least squares
algorithm. In order to guarantee sufficient excitation for the identification,
an additional non-convex constraint is enforced over the persistently exciting
part.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05775</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05775</id><created>2015-05-21</created><authors><author><keyname>Chen</keyname><forenames>Zhengchuan</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Chen</keyname><forenames>Chen</forenames></author></authors><title>Network Coding Tree Algorithm for Multiple Access System</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Network coding is famous for significantly improving the throughput of
networks. The successful decoding of the network coded data relies on some side
information of the original data. In that framework, independent data flows are
usually first decoded and then network coded by relay nodes. If appropriate
signal design is adopted, physical layer network coding is a natural way in
wireless networks. In this work, a network coding tree algorithm which enhances
the efficiency of the multiple access system (MAS) is presented. For MAS,
existing works tried to avoid the collisions while collisions happen frequently
under heavy load. By introducing network coding to MAS, our proposed algorithm
achieves a better performance of throughput and delay. When multiple users
transmit signal in a time slot, the mexed signals are saved and used to jointly
decode the collided frames after some component frames of the network coded
frame are received. Splitting tree structure is extended to the new algorithm
for collision solving. The throughput of the system and average delay of frames
are presented in a recursive way. Besides, extensive simulations show that
network coding tree algorithm enhances the system throughput and decreases the
average frame delay compared with other algorithms. Hence, it improves the
system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05779</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05779</id><created>2015-05-21</created><updated>2016-02-14</updated><authors><author><keyname>Huhta</keyname><forenames>O.</forenames></author><author><keyname>Shrestha</keyname><forenames>P.</forenames></author><author><keyname>Udar</keyname><forenames>S.</forenames></author><author><keyname>Juuti</keyname><forenames>M.</forenames></author><author><keyname>Saxena</keyname><forenames>N.</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author></authors><title>Pitfalls in Designing Zero-Effort Deauthentication: Opportunistic Human
  Observation Attacks</title><categories>cs.CR</categories><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deauthentication is an important component of any authentication system. The
widespread use of computing devices in daily life has underscored the need for
zero-effort deauthentication schemes. However, the quest for eliminating user
effort may lead to hidden security flaws in the authentication schemes. As a
case in point, we investigate a prominent zero-effort deauthentication scheme,
called ZEBRA, which provides an interesting and a useful solution to a
difficult problem as demonstrated in the original paper. We identify a subtle
incorrect assumption in its adversary model that leads to a fundamental design
flaw. We exploit this to break the scheme with a class of attacks that are much
easier for a human to perform in a realistic adversary model, compared to the
na\&quot;ive attacks studied in the ZEBRA paper. For example, one of our main
attacks, where the human attacker has to opportunistically mimic only the
victim's keyboard typing activity at a nearby terminal, is significantly more
successful compared to the na\&quot;ive attack that requires mimicking keyboard and
mouse activities as well as keyboard-mouse movements. Further, by understanding
the design flaws in ZEBRA as cases of tainted input, we show that we can draw
on well-understood design principles to improve ZEBRA's security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05788</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05788</id><created>2015-05-21</created><authors><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author><author><keyname>Pan</keyname><forenames>Ye</forenames></author><author><keyname>Zhou</keyname><forenames>Tianxiong</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>An Empirical Study on Display Ad Impression Viewability Measurements</title><categories>cs.HC cs.IR</categories><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Display advertising normally charges advertisers for every single ad
impression. Specifically, if an ad in a webpage has been loaded in the browser,
an ad impression is counted. However, due to the position and size of the ad
slot, lots of ads are actually not viewed but still measured as impressions and
charged. These fraud ad impressions indeed undermine the efficacy of display
advertising. A perfect ad impression viewability measurement should match what
the user has really viewed with a short memory. In this paper, we conduct
extensive investigations on display ad impression viewability measurements on
dimensions of ad creative displayed pixel percentage and exposure time to find
which measurement provides the most accurate ad impression counting. The
empirical results show that the most accurate measurement counts one ad
impression if more than 75% of the ad creative pixels have been exposed for at
least 2 continuous seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05794</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05794</id><created>2015-05-21</created><updated>2015-05-31</updated><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author><author><keyname>Weinstein</keyname><forenames>Omri</forenames></author></authors><title>An Improved Upper Bound for the Most Informative Boolean Function
  Conjecture</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose $X$ is a uniformly distributed $n$-dimensional binary vector and $Y$
is obtained by passing $X$ through a binary symmetric channel with crossover
probability $\alpha$. A recent conjecture by Courtade and Kumar postulates that
$I(f(X);Y)\leq 1-h(\alpha)$ for any Boolean function $f$. So far, the best
known upper bound was $I(f(X);Y)\leq (1-2\alpha)^2$. In this paper, we derive a
new upper bound that holds for all balanced functions, and improves upon the
best known bound for all $\tfrac{1}{3}&lt;\alpha&lt;\tfrac{1}{2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05798</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05798</id><created>2015-05-21</created><authors><author><keyname>Ammar</keyname><forenames>Haitham Bou</forenames></author><author><keyname>Tutunov</keyname><forenames>Rasul</forenames></author><author><keyname>Eaton</keyname><forenames>Eric</forenames></author></authors><title>Safe Policy Search for Lifelong Reinforcement Learning with Sublinear
  Regret</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lifelong reinforcement learning provides a promising framework for developing
versatile agents that can accumulate knowledge over a lifetime of experience
and rapidly learn new tasks by building upon prior knowledge. However, current
lifelong learning methods exhibit non-vanishing regret as the amount of
experience increases and include limitations that can lead to suboptimal or
unsafe control policies. To address these issues, we develop a lifelong policy
gradient learner that operates in an adversarial set- ting to learn multiple
tasks online while enforcing safety constraints on the learned policies. We
demonstrate, for the first time, sublinear regret for lifelong policy search,
and validate our algorithm on several benchmark dynamical systems and an
application to quadrotor control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05800</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05800</id><created>2015-05-21</created><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author></authors><title>Complexity Theoretic Limitations on Learning Halfspaces</title><categories>cs.CC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of agnostically learning halfspaces which is defined by
a fixed but unknown distribution $\mathcal{D}$ on $\mathbb{Q}^n\times \{\pm
1\}$. We define $\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D})$ as the least error
of a halfspace classifier for $\mathcal{D}$. A learner who can access
$\mathcal{D}$ has to return a hypothesis whose error is small compared to
$\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D})$.
  Using the recently developed method of the author, Linial and Shalev-Shwartz
we prove hardness of learning results under a natural assumption on the
complexity of refuting random $K$-$\mathrm{XOR}$ formulas. We show that no
efficient learning algorithm has non-trivial worst-case performance even under
the guarantees that $\mathrm{Err}_{\mathrm{HALF}}(\mathcal{D}) \le \eta$ for
arbitrarily small constant $\eta&gt;0$, and that $\mathcal{D}$ is supported in
$\{\pm 1\}^n\times \{\pm 1\}$. Namely, even under these favorable conditions
its error must be $\ge \frac{1}{2}-\frac{1}{n^c}$ for every $c&gt;0$. In
particular, no efficient algorithm can achieve a constant approximation ratio.
Under a stronger version of the assumption (where $K$ can be poly-logarithmic
in $n$), we can take $\eta = 2^{-\log^{1-\nu}(n)}$ for arbitrarily small
$\nu&gt;0$. Interestingly, this is even stronger than the best known lower bounds
(Arora et. al. 1993, Feldamn et. al. 2006, Guruswami and Raghavendra 2006) for
the case that the learner is restricted to return a halfspace classifier (i.e.
proper learning).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05819</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05819</id><created>2015-02-24</created><authors><author><keyname>Patrascu</keyname><forenames>Vasile</forenames></author></authors><title>New HSL Distance Based Colour Clustering Algorithm</title><categories>cs.CV</categories><comments>The 24th Midwest Artificial Intelligence and Cognitive Sciences
  Conference (MAICS 2013), pp. 85-92, New Albany, Indiana. USA, April 13-14,
  2013</comments><doi>10.13140/2.1.4990.8007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define a distance for the HSL colour system. Next, the
proposed distance is used for a fuzzy colour clustering algorithm construction.
The presented algorithm is related to the well-known fuzzy c-means algorithm.
Finally, the clustering algorithm is used as colour reduction method. The
obtained experimental results are presented to demonstrate the effectiveness of
our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05821</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05821</id><created>2015-05-21</created><authors><author><keyname>Amid</keyname><forenames>Ehsan</forenames></author><author><keyname>Dikmen</keyname><forenames>Onur</forenames></author><author><keyname>Oja</keyname><forenames>Erkki</forenames></author></authors><title>Optimizing the Information Retrieval Trade-off in Data Visualization
  Using $\alpha$-Divergence</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data visualization is one of the major applications of nonlinear
dimensionality reduction. From the information retrieval perspective, the
quality of a visualization can be evaluated by considering the extent that the
neighborhood relation of each data point is maintained while the number of
unrelated points that are retrieved is minimized. This property can be
quantified as a trade-off between the mean precision and mean recall of the
visualization. While there have been some approaches to formulate the
visualization objective directly as a weighted sum of the precision and recall,
there is no systematic way to determine the optimal trade-off between these two
nor a clear interpretation of the optimal value. In this paper, we investigate
the properties of $\alpha$-divergence for information visualization, focusing
our attention on a particular range of $\alpha$ values. We show that the
minimization of the new cost function corresponds to maximizing a geometric
mean between precision and recall, parameterized by $\alpha$. Contrary to some
earlier methods, no hand-tuning is needed, but we can rigorously estimate the
optimal value of $\alpha$ for a given input data. For this, we provide a
statistical framework using a novel distribution called Exponential Divergence
with Augmentation (EDA). By the extensive set of experiments, we show that the
optimal value of $\alpha$, obtained by EDA corresponds to the optimal trade-off
between the precision and recall for a given data distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05825</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05825</id><created>2015-05-21</created><authors><author><keyname>Husfeldt</keyname><forenames>Thore</forenames></author></authors><title>Graph colouring algorithms</title><categories>cs.DS</categories><comments>Thore Husfeldt, Graph colouring algorithms. Chapter 13 of Topics in
  Chromatic Graph Theory, L. W. Beineke and Robin J. Wilson (eds.),
  Encyclopedia of Mathematics and its Applications 156, Cambridge University
  Press, ISBN 978-1-107-03350-4, 2015, pp. 277-303</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter presents an introduction to graph colouring algorithms. The
focus is on vertex-colouring algorithms that work for general classes of graphs
with worst-case performance guarantees in a sequential model of computation.
The presentation aims to demonstrate the breadth of available techniques and is
organized by algorithmic paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05831</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05831</id><created>2015-05-21</created><authors><author><keyname>Kudekar</keyname><forenames>Shrinivas</forenames></author><author><keyname>Mondelli</keyname><forenames>Marco</forenames></author><author><keyname>&#x15e;a&#x15f;o&#x11f;lu</keyname><forenames>Eren</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>Reed-Muller Codes Achieve Capacity on the Binary Erasure Channel under
  MAP Decoding</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that Reed-Muller codes achieve capacity under maximum a posteriori
bit decoding for transmission over the binary erasure channel for all rates $0
&lt; R &lt; 1$. The proof is generic and applies to other codes with sufficient
amount of symmetry as well. The main idea is to combine the following
observations: (i) monotone functions experience a sharp threshold behavior,
(ii) the extrinsic information transfer (EXIT) functions are monotone, (iii)
Reed--Muller codes are 2-transitive and thus the EXIT functions associated with
their codeword bits are all equal, and (iv) therefore the Area Theorem for the
average EXIT functions implies that RM codes' threshold is at channel capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05832</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05832</id><created>2015-05-21</created><authors><author><keyname>Deshmukh</keyname><forenames>Jyotirmoy V.</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Prabhu</keyname><forenames>Vinayak S.</forenames></author></authors><title>Quantifying Conformance using the Skorokhod Metric (full version)</title><categories>cs.SY cs.LO</categories><comments>Full version of CAV 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conformance testing problem for dynamical systems asks, given two
dynamical models (e.g., as Simulink diagrams), whether their behaviors are
&quot;close&quot; to each other. In the semi-formal approach to conformance testing, the
two systems are simulated on a large set of tests, and a metric, defined on
pairs of real-valued, real-timed trajectories, is used to determine a lower
bound on the distance. We show how the Skorkhod metric on continuous dynamical
systems can be used as the foundation for conformance testing of complex
dynamical models. The Skorokhod metric allows for both state value mismatches
and timing distortions, and is thus well suited for checking conformance
between idealized models of dynamical systems and their implementations. We
demonstrate the robustness of the system conformance quantification by proving
a \emph{transference theorem}: trajectories close under the Skorokhod metric
satisfy &quot;close&quot; logical properties. Specifically, we show the result for the
timed linear time logic \TLTL augmented with a rich class of temporal and
spatial constraint predicates. We provide a window-based streaming algorithm to
compute the Skorokhod metric, and use it as a basis for a conformance testing
tool for Simulink. We experimentally demonstrate the effectiveness of our tool
in finding discrepant behaviors on a set of control system benchmarks,
including an industrial challenge problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05836</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05836</id><created>2015-05-21</created><updated>2015-11-23</updated><authors><author><keyname>Chavali</keyname><forenames>Neelima</forenames></author><author><keyname>Agrawal</keyname><forenames>Harsh</forenames></author><author><keyname>Mahendru</keyname><forenames>Aroma</forenames></author><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author></authors><title>Object-Proposal Evaluation Protocol is 'Gameable'</title><categories>cs.CV</categories><comments>15 pages, 11 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object proposals have quickly become the de-facto pre-processing step in a
number of vision pipelines (for object detection, object discovery, and other
tasks). Their performance is usually evaluated on partially annotated datasets.
In this paper, we argue that the choice of using a partially annotated dataset
for evaluation of object proposals is problematic -- as we demonstrate via a
thought experiment, the evaluation protocol is 'gameable', in the sense that
progress under this protocol does not necessarily correspond to a &quot;better&quot;
category independent object proposal algorithm.
  To alleviate this problem, we: (1) Introduce a nearly-fully annotated version
of PASCAL VOC dataset, which serves as a test-bed to check if object proposal
techniques are overfitting to a particular list of categories. (2) Perform an
exhaustive evaluation of object proposal methods on our introduced nearly-fully
annotated PASCAL dataset and perform cross-dataset generalization experiments;
and (3) Introduce a diagnostic experiment to detect the bias capacity in an
object proposal algorithm. This tool circumvents the need to collect a densely
annotated dataset, which can be expensive and cumbersome to collect. Finally,
we plan to release an easy-to-use toolbox which combines various publicly
available implementations of object proposal algorithms which standardizes the
proposal generation and evaluation so that new methods can be added and
evaluated on different datasets. We hope that the results presented in the
paper will motivate the community to test the category independence of various
object proposal methods by carefully choosing the evaluation protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05840</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05840</id><created>2015-05-21</created><authors><author><keyname>Pradhan</keyname><forenames>Tapan</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author><author><keyname>Kabi</keyname><forenames>Bibek</forenames></author></authors><title>Comparative Evaluation of Symmetric SVD Algorithms for Real-time Face
  and Eye Tracking</title><categories>math.NA cs.NA</categories><comments>20 pages, 4 figures, book chapter</comments><journal-ref>Springer Berlin Heidelberg. (2013) 323-340</journal-ref><doi>10.1007/978-3-642-30232-9_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation of singular value decomposition (SVD) has been a topic of concern
by many numerical linear algebra researchers. Fast SVD has been a very
effective tool in computer vision in a number of aspects, such as: face
recognition, eye tracking etc. At the present state of the art fast and
fixed-point power efficient SVD algorithm needs to be developed for real-time
embedded computing. The work in this paper is the genesis of an attempt to
build an on-board real-time face and eye tracking system for human drivers to
detect loss of attention due to drowsiness or fatigue. A major function of this
on-board system is quick customization. This is carried out when a new driver
comes in. The face and eye images are recorded while instructing the driver for
making specific poses. The eigen faces and eigen eyes are generated at several
resolution levels and stored in the on-board computer. The discriminating eigen
space of face and eyes are determined and stored in the on-board flash memory
for detection and tracking of face and eyes and classification of eyes (open or
closed) as well. Therefore, fast SVD of image covariance matrix at various
levels of resolution needs to be carried out to generate the eigen database. As
a preliminary step, we review the existing symmetric SVD algorithms and
evaluate their feasibility for such an application. In this article, we compare
the performance of (1) Jacobi's, (2) Hestenes', (3) Golub-Kahan, (4)
Tridiagonalization and Symmetric QR iteration and (5) Tridiagonalization and
Divide and Conquer algorithms. A case study has been demonstrated as an
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05841</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05841</id><created>2015-05-21</created><authors><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Strauss</keyname><forenames>Benjamin</forenames></author></authors><title>Translation Memory Retrieval Methods</title><categories>cs.CL</categories><comments>9 pages, 6 tables, 3 figures; appeared in Proceedings of the 14th
  Conference of the European Chapter of the Association for Computational
  Linguistics, April 2014</comments><acm-class>I.2.7</acm-class><journal-ref>In Proceedings of the 14th Conference of the European Chapter of
  the Association for Computational Linguistics, pages 202-210, Gothenburg,
  Sweden, April 2014. Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Translation Memory (TM) systems are one of the most widely used translation
technologies. An important part of TM systems is the matching algorithm that
determines what translations get retrieved from the bank of available
translations to assist the human translator. Although detailed accounts of the
matching algorithms used in commercial systems can't be found in the
literature, it is widely believed that edit distance algorithms are used. This
paper investigates and evaluates the use of several matching algorithms,
including the edit distance algorithm that is believed to be at the heart of
most modern commercial TM systems. This paper presents results showing how well
various matching algorithms correlate with human judgments of helpfulness
(collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithm
based on weighted n-gram precision that can be adjusted for translator length
preferences consistently returns translations judged to be most helpful by
translators for multiple domains and language pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1505.05842</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1505.05842</id><created>2015-05-21</created><authors><author><keyname>Taranetz</keyname><forenames>Martin</forenames></author><author><keyname>Rupp</keyname><forenames>Markus</forenames></author></authors><title>A Circular Interference Model for Asymmetric Aggregate Interference</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scaling up the number of base stations per unit area is one of the major
trends in mobile cellular systems of the fourth (4G)- and fifth generation
(5G), making it increasingly difficult to characterize aggregate interference
statistics with system models of low complexity. This paper proposes a new
circular interference model that aggregates given interferer deployments to
power profiles along circles. The model accurately preserves the original
interference statistics while considerably reducing the amount of relevant
interferers. In comparison to common approaches from stochastic geometry, it
enables to characterize cell-center- and cell-edge users, and preserves effects
that are otherwise concealed by spatial averaging. To enhance the analysis of
given power profiles and to validate the accuracy of the circular model, a new
finite sum representation for the sum of Gamma random variables with
integer-valued shape parameter is introduced. The approach allows to decompose
the distribution of the aggregate interference into the contributions of the
individual interferers. Such knowledge is particularly expedient for the
application of base station coordination- and cooperation schemes. Moreover,
the proposed approach enables to accurately predict the corresponding
signal-to-interference-ratio- and rate statistics.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="77000" completeListSize="102538">1122234|78001</resumptionToken>
</ListRecords>
</OAI-PMH>
